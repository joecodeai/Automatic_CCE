[
    {
        "gold": {
            "text": [
                "This paper considers the reading comprehension task in which some discrete-reasoning abilities are needed to correctly answer questions.",
                "Specifically, we focus on a new reading comprehension dataset called DROP #TARGET_REF , which requires Discrete Reasoning Over the content of Paragraphs to obtain the final answer.",
                "Unlike previous benchmarks such as CNN/DM (#REF) and SQuAD (#REF) that have been well solved #REF) , DROP is substantially more challenging in three ways.",
                "First, the answers to 1 https://github.com/huminghao16/MTMSN the questions involve a wide range of types such as numbers, dates, or text strings.",
                "Therefore, various kinds of prediction strategies are required to successfully find the answers."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This paper considers the reading comprehension task in which some discrete-reasoning abilities are needed to correctly answer questions.\n sent1: Specifically, we focus on a new reading comprehension dataset called DROP #TARGET_REF , which requires Discrete Reasoning Over the content of Paragraphs to obtain the final answer.\n sent2: Unlike previous benchmarks such as CNN/DM (#REF) and SQuAD (#REF) that have been well solved #REF) , DROP is substantially more challenging in three ways.\n sent3: First, the answers to 1 https://github.com/huminghao16/MTMSN the questions involve a wide range of types such as numbers, dates, or text strings.\n sent4: Therefore, various kinds of prediction strategies are required to successfully find the answers.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Based on the representations, we employ a multi-type answer predictor that is able to produce four answer types: (1) span from the text; (2) arithmetic expression; (3) count number; (4) negation on numbers ( §3.2).",
                "Following #TARGET_REF , we first predict the answer type of a given passage-question pair, and then adopt individual prediction strategies.",
                "To support multispan extraction ( §3.3), the model explicitly predicts the number of answer spans.",
                "It then outputs non-overlapped spans until the specific amount is reached.",
                "Moreover, we do not directly use the arithmetic expression that possesses the maximum probability, but instead re-rank several expression candidates that are decoded by beam search to further confirm the prediction ( §3.4)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Based on the representations, we employ a multi-type answer predictor that is able to produce four answer types: (1) span from the text; (2) arithmetic expression; (3) count number; (4) negation on numbers ( §3.2).\n sent1: Following #TARGET_REF , we first predict the answer type of a given passage-question pair, and then adopt individual prediction strategies.\n sent2: To support multispan extraction ( §3.3), the model explicitly predicts the number of answer spans.\n sent3: It then outputs non-overlapped spans until the specific amount is reached.\n sent4: Moreover, we do not directly use the arithmetic expression that possesses the maximum probability, but instead re-rank several expression candidates that are decoded by beam search to further confirm the prediction ( §3.4).\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Rather than restricting the answer to always be a span of text, the discrete-reasoning reading comprehension task involves different answer types (e.g., number, date, span of text).",
                "Following #TARGET_REF , we design a multi-type answer predictor to selectively produce different kinds of answers such as span, count number, and arithmetic expression.",
                "To further increase answer coverage, we propose adding a new answer type to support logical negation.",
                "Moreover, unlike prior work that separately predicts passage spans and question spans, our approach directly extracts spans from the input sequence.",
                "Answer type prediction Inspired by the Augmented QANet model (#REF) , we use the contextualized token representations from the last four blocks (H L−3 , ..., H L ) as the inputs to our answer predictor, which are denoted as M 0 , M 1 , M 2 , M 3 , respectively."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Rather than restricting the answer to always be a span of text, the discrete-reasoning reading comprehension task involves different answer types (e.g., number, date, span of text).\n sent1: Following #TARGET_REF , we design a multi-type answer predictor to selectively produce different kinds of answers such as span, count number, and arithmetic expression.\n sent2: To further increase answer coverage, we propose adding a new answer type to support logical negation.\n sent3: Moreover, unlike prior work that separately predicts passage spans and question spans, our approach directly extracts spans from the input sequence.\n sent4: Answer type prediction Inspired by the Augmented QANet model (#REF) , we use the contextualized token representations from the last four blocks (H L−3 , ..., H L ) as the inputs to our answer predictor, which are denoted as M 0 , M 1 , M 2 , M 3 , respectively.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "To further increase answer coverage, we propose adding a new answer type to support logical negation.",
                "Moreover, unlike prior work that separately predicts passage spans and question spans, our approach directly extracts spans from the input sequence.",
                "Answer type prediction Inspired by the Augmented QANet model #TARGET_REF , we use the contextualized token representations from the last four blocks (H L−3 , ..., H L ) as the inputs to our answer predictor, which are denoted as M 0 , M 1 , M 2 , M 3 , respectively.",
                "To predict the answer type, we first split the representation M 2 into a question representation Q 2 and a passage representation P 2 according to the index of intermediate [SEP] token.",
                "Then the model computes two vectors h Q 2 and h P 2 that summarize the question and passage information respectively:"
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: To further increase answer coverage, we propose adding a new answer type to support logical negation.\n sent1: Moreover, unlike prior work that separately predicts passage spans and question spans, our approach directly extracts spans from the input sequence.\n sent2: Answer type prediction Inspired by the Augmented QANet model #TARGET_REF , we use the contextualized token representations from the last four blocks (H L−3 , ..., H L ) as the inputs to our answer predictor, which are denoted as M 0 , M 1 , M 2 , M 3 , respectively.\n sent3: To predict the answer type, we first split the representation M 2 into a question representation Q 2 and a passage representation P 2 according to the index of intermediate [SEP] token.\n sent4: Then the model computes two vectors h Q 2 and h P 2 that summarize the question and passage information respectively:\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Since DROP does not indicate the answer type but only provides the answer string, we therefore adopt the weakly supervised annotation scheme, as suggested in #REF; #TARGET_REF .",
                "We find all possible annotations that point to the gold answer, including matching spans, arithmetic expressions, correct count numbers, negation operations, and the number of spans.",
                "We use simple rules to search over all mentioned numbers to find potential negations.",
                "That is, if 100 minus a number is equal to the answer, then a negation occurs on this number.",
                "Besides, we only search the addition/subtraction of three numbers at most due to the exponential search space."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Since DROP does not indicate the answer type but only provides the answer string, we therefore adopt the weakly supervised annotation scheme, as suggested in #REF; #TARGET_REF .\n sent1: We find all possible annotations that point to the gold answer, including matching spans, arithmetic expressions, correct count numbers, negation operations, and the number of spans.\n sent2: We use simple rules to search over all mentioned numbers to find potential negations.\n sent3: That is, if 100 minus a number is equal to the answer, then a negation occurs on this number.\n sent4: Besides, we only search the addition/subtraction of three numbers at most due to the exponential search space.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Dataset We consider the reading comprehension benchmark that requires Discrete Reasoning Over Paragraphs (DROP) #TARGET_REF prehensive understanding of the context as well as the ability of numerical reasoning are required.",
                "Model settings We build our model upon two publicly available uncased versions of BERT: BERT BASE and BERT LARGE 2 , and refer readers to #REF for details on model sizes.",
                "We use Adam optimizer with a learning rate of 3e-5 and warmup over the first 5% steps to train.",
                "The maximum number of epochs is set to 10 for base models and 5 for large models, while the batch size is 12 or 24 respectively.",
                "A dropout probability of 0.1 is used unless stated otherwise."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Dataset We consider the reading comprehension benchmark that requires Discrete Reasoning Over Paragraphs (DROP) #TARGET_REF prehensive understanding of the context as well as the ability of numerical reasoning are required.\n sent1: Model settings We build our model upon two publicly available uncased versions of BERT: BERT BASE and BERT LARGE 2 , and refer readers to #REF for details on model sizes.\n sent2: We use Adam optimizer with a learning rate of 3e-5 and warmup over the first 5% steps to train.\n sent3: The maximum number of epochs is set to 10 for base models and 5 for large models, while the batch size is 12 or 24 respectively.\n sent4: A dropout probability of 0.1 is used unless stated otherwise.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Despite their success, these datasets only require shallow pattern matching and simple logical reasoning, thus being well solved #REF) .",
                "Recently, #TARGET_REF released a new benchmark named DROP that demands discrete reasoning as well as deeper paragraph understanding to find the answers.",
                "#REF introduced a dataset consisting of different types of mathematics problems to focuses on mathematical computation.",
                "We choose to work on DROP to test both the numerical reasoning and linguistic comprehension abilities.",
                "Neural reading models Previous neural reading models, such as BiDAF (#REF) , R-Net (#REF) , QANet (#REF) , Reinforced Mreader (#REF) , are usually designed to extract a continuous span of text as the answer."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Despite their success, these datasets only require shallow pattern matching and simple logical reasoning, thus being well solved #REF) .\n sent1: Recently, #TARGET_REF released a new benchmark named DROP that demands discrete reasoning as well as deeper paragraph understanding to find the answers.\n sent2: #REF introduced a dataset consisting of different types of mathematics problems to focuses on mathematical computation.\n sent3: We choose to work on DROP to test both the numerical reasoning and linguistic comprehension abilities.\n sent4: Neural reading models Previous neural reading models, such as BiDAF (#REF) , R-Net (#REF) , QANet (#REF) , Reinforced Mreader (#REF) , are usually designed to extract a continuous span of text as the answer.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Then we compute the probabilities of the starting and ending indices of the answer span from the input sequence as:",
                "where ⊗ denotes the outer product between the vector g and each token representation in M.",
                "Arithmetic expression In order to model the process of performing addition or subtraction among multiple numbers mentioned in the passage, we assign a three-way categorical variable (plus, minus, or zero) for each number to indicate its sign, similar to #TARGET_REF .",
                "As a result, an arithmetic expression that has a number as the final answer can be obtained and easily evaluated.",
                "Specifically, for each number mentioned in the passage, we gather its corresponding representation from the concatenation of M 2 and M 3 , eventually yielding U = (u 1 , ..., u N ) ∈ R N ×2 * D where N numbers exist."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: Then we compute the probabilities of the starting and ending indices of the answer span from the input sequence as:\n sent1: where ⊗ denotes the outer product between the vector g and each token representation in M.\n sent2: Arithmetic expression In order to model the process of performing addition or subtraction among multiple numbers mentioned in the passage, we assign a three-way categorical variable (plus, minus, or zero) for each number to indicate its sign, similar to #TARGET_REF .\n sent3: As a result, an arithmetic expression that has a number as the final answer can be obtained and easily evaluated.\n sent4: Specifically, for each number mentioned in the passage, we gather its corresponding representation from the concatenation of M 2 and M 3 , eventually yielding U = (u 1 , ..., u N ) ∈ R N ×2 * D where N numbers exist.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Baselines Following the implementation of Augmented QANet (NAQANet) #TARGET_REF , we introduce a similar baseline called Augmented BERT (NABERT).",
                "The main difference is that we replace the encoder of QANet (#REF) with the pre-trained Transformer blocks (#REF) .",
                "Moreover, it also supports the prediction of various answer types such as span, arithmetic expression, and count number."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: Baselines Following the implementation of Augmented QANet (NAQANet) #TARGET_REF , we introduce a similar baseline called Augmented BERT (NABERT).\n sent1: The main difference is that we replace the encoder of QANet (#REF) with the pre-trained Transformer blocks (#REF) .\n sent2: Moreover, it also supports the prediction of various answer types such as span, arithmetic expression, and count number.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Polysynthetic languages pose a challenge for morphological analysis due to the rootmorpheme complexity and to the word class \"squish\".",
                "In addition, many of these polysynthetic languages are low-resource.",
                "We propose unsupervised approaches for morphological segmentation of low-resource polysynthetic languages based on Adaptor Grammars (AG) #TARGET_REF .",
                "We experiment with four languages from the Uto-Aztecan family.",
                "Our AG-based approaches outperform other unsupervised approaches and show promise when compared to supervised methods, outperforming them on two of the four languages."
            ],
            "label": [
                "USE",
                "EXTENDS"
            ]
        },
        "input": "sent0: Polysynthetic languages pose a challenge for morphological analysis due to the rootmorpheme complexity and to the word class \"squish\".\n sent1: In addition, many of these polysynthetic languages are low-resource.\n sent2: We propose unsupervised approaches for morphological segmentation of low-resource polysynthetic languages based on Adaptor Grammars (AG) #TARGET_REF .\n sent3: We experiment with four languages from the Uto-Aztecan family.\n sent4: Our AG-based approaches outperform other unsupervised approaches and show promise when compared to supervised methods, outperforming them on two of the four languages.\n",
        "output": "{\"label\": [\"USE\", \"EXTENDS\"], \"context\": [\"sent2\", \"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Adaptor Grammars (AGs) are nonparametric Bayesian models that generalize probabilistic context free grammars (PCFG), and have proven to be successful for unsupervised morphological segmentation, where a PCFG is a morphological grammar that specifies word structure (#REF; #REF; #REF #REF .",
                "Our main goal is to examine the success of Adaptor Grammars for unsupervised morphological segmentation when applied to polysynthetic languages, where the morphology is synthetically complex (not simply agglutinative), and where resources are minimal.",
                "We use the datasets introduced by #REF in an unsupervised fashion (unsegmented words).",
                "We design several AG learning setups: 1) use the best-on-average AG setup from #TARGET_REF ; 2) optimize for language using just the small training vocabulary (unsegmented) and dev vocabulary (segmented) from #REF ; 3) approximate the effect of having some linguistic knowledge; 4) learn from all languages at once and 5) add additional unsupervised data for NH and WX (Section 3).",
                "We show that the AG-based approaches outperform other unsupervised methods -M orf essor (#REF) and M orphoChain (#REF) ) -, and that for two of the languages (NH and YN), the best AG-based approaches outperform the best supervised methods (Section 4)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Adaptor Grammars (AGs) are nonparametric Bayesian models that generalize probabilistic context free grammars (PCFG), and have proven to be successful for unsupervised morphological segmentation, where a PCFG is a morphological grammar that specifies word structure (#REF; #REF; #REF #REF .\n sent1: Our main goal is to examine the success of Adaptor Grammars for unsupervised morphological segmentation when applied to polysynthetic languages, where the morphology is synthetically complex (not simply agglutinative), and where resources are minimal.\n sent2: We use the datasets introduced by #REF in an unsupervised fashion (unsegmented words).\n sent3: We design several AG learning setups: 1) use the best-on-average AG setup from #TARGET_REF ; 2) optimize for language using just the small training vocabulary (unsegmented) and dev vocabulary (segmented) from #REF ; 3) approximate the effect of having some linguistic knowledge; 4) learn from all languages at once and 5) add additional unsupervised data for NH and WX (Section 3).\n sent4: We show that the AG-based approaches outperform other unsupervised methods -M orf essor (#REF) and M orphoChain (#REF) ) -, and that for two of the languages (NH and YN), the best AG-based approaches outperform the best supervised methods (Section 4).\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "For morphological segmentation, a PCFG is a morphological grammar that specifies word structure, where AGs learn latent tree structures given a list of words.",
                "In this paper, we experiment with the grammars and the learning setups proposed by #TARGET_REF , which we outline briefly below.",
                "Grammars.",
                "We use the nine grammars from Eskander et al. (2016 Eskander et al. ( , 2018 that were designed based on three dimensions: 1) how the grammar models word structure (e.g., prefix-stem-suffix vs. morphemes), 2) the level of abstraction in nonterminals (e.g., compounds, morphemes and submorphemes) and 3) how the output boundaries are specified (see Table 2 for a sample grammars).",
                "For example, the PrStSu+SM grammar models the Table 2 : Sample grammar setups used by Eskander et al. (2018 Eskander et al. ( , 2016 ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: For morphological segmentation, a PCFG is a morphological grammar that specifies word structure, where AGs learn latent tree structures given a list of words.\n sent1: In this paper, we experiment with the grammars and the learning setups proposed by #TARGET_REF , which we outline briefly below.\n sent2: Grammars.\n sent3: We use the nine grammars from Eskander et al. (2016 Eskander et al. ( , 2018 that were designed based on three dimensions: 1) how the grammar models word structure (e.g., prefix-stem-suffix vs. morphemes), 2) the level of abstraction in nonterminals (e.g., compounds, morphemes and submorphemes) and 3) how the output boundaries are specified (see Table 2 for a sample grammars).\n sent4: For example, the PrStSu+SM grammar models the Table 2 : Sample grammar setups used by Eskander et al. (2018 Eskander et al. ( , 2016 .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "In this paper, we experiment with the grammars and the learning setups proposed by #REF , which we outline briefly below.",
                "Grammars.",
                "We use the nine grammars from #TARGET_REF Eskander et al. ( , 2018 that were designed based on three dimensions: 1) how the grammar models word structure (e.g., prefix-stem-suffix vs. morphemes), 2) the level of abstraction in nonterminals (e.g., compounds, morphemes and submorphemes) and 3) how the output boundaries are specified (see Table 2 for a sample grammars).",
                "For example, the PrStSu+SM grammar models the Table 2 : Sample grammar setups used by Eskander et al. (2018 Eskander et al. ( , 2016 .",
                "Compound = Upper level representation of the word as a sequence of compounds; Morph = affix/morpheme representation as a sequence of morphemes."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In this paper, we experiment with the grammars and the learning setups proposed by #REF , which we outline briefly below.\n sent1: Grammars.\n sent2: We use the nine grammars from #TARGET_REF Eskander et al. ( , 2018 that were designed based on three dimensions: 1) how the grammar models word structure (e.g., prefix-stem-suffix vs. morphemes), 2) the level of abstraction in nonterminals (e.g., compounds, morphemes and submorphemes) and 3) how the output boundaries are specified (see Table 2 for a sample grammars).\n sent3: For example, the PrStSu+SM grammar models the Table 2 : Sample grammar setups used by Eskander et al. (2018 Eskander et al. ( , 2016 .\n sent4: Compound = Upper level representation of the word as a sequence of compounds; Morph = affix/morpheme representation as a sequence of morphemes.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Grammars.",
                "We use the nine grammars from Eskander et al. (2016 Eskander et al. ( , 2018 that were designed based on three dimensions: 1) how the grammar models word structure (e.g., prefix-stem-suffix vs. morphemes), 2) the level of abstraction in nonterminals (e.g., compounds, morphemes and submorphemes) and 3) how the output boundaries are specified (see Table 2 for a sample grammars).",
                "For example, the PrStSu+SM grammar models the Table 2 : Sample grammar setups used by Eskander et al. (2018 #TARGET_REF .",
                "Compound = Upper level representation of the word as a sequence of compounds; Morph = affix/morpheme representation as a sequence of morphemes.",
                "SubMorph (SM) = Lower level representation of characters as a sequence of sub-morphemes. \"+\" denotes one or more."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Grammars.\n sent1: We use the nine grammars from Eskander et al. (2016 Eskander et al. ( , 2018 that were designed based on three dimensions: 1) how the grammar models word structure (e.g., prefix-stem-suffix vs. morphemes), 2) the level of abstraction in nonterminals (e.g., compounds, morphemes and submorphemes) and 3) how the output boundaries are specified (see Table 2 for a sample grammars).\n sent2: For example, the PrStSu+SM grammar models the Table 2 : Sample grammar setups used by Eskander et al. (2018 #TARGET_REF .\n sent3: Compound = Upper level representation of the word as a sequence of compounds; Morph = affix/morpheme representation as a sequence of morphemes.\n sent4: SubMorph (SM) = Lower level representation of characters as a sequence of sub-morphemes. \"+\" denotes one or more.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The boundaries in the output are based on the prefix, stem and suffix levels.",
                "Learning Settings.",
                "The input to the learner is a grammar and a vocabulary of unsegmented words.",
                "We consider the three learning settings in #TARGET_REF : Standard, Scholarseeded Knowledge and Cascaded.",
                "The Standard setting is language-independent and fully unsupervised, while in the Scholar-seeded-Knowledge setting, some linguistic knowledge (in the form of affixes taken from grammar books) is seeded into the grammar trees before learning takes place."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The boundaries in the output are based on the prefix, stem and suffix levels.\n sent1: Learning Settings.\n sent2: The input to the learner is a grammar and a vocabulary of unsegmented words.\n sent3: We consider the three learning settings in #TARGET_REF : Standard, Scholarseeded Knowledge and Cascaded.\n sent4: The Standard setting is language-independent and fully unsupervised, while in the Scholar-seeded-Knowledge setting, some linguistic knowledge (in the form of affixes taken from grammar books) is seeded into the grammar trees before learning takes place.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "We experimented with several setups using AGs for unsupervised segmentation.",
                "Language-Independent Morphological Segmenter.",
                "LIMS is the best-on-average AG setup obtained by #TARGET_REF when trained on six languages (English, German, Finnish, Estonian, Turkish and Zulu), which is the Cascaded PrStSu+SM configuration.",
                "We use this AG setup for each of the four languages.",
                "We refer to this system as AG LIM S ."
            ],
            "label": [
                "USE",
                "BACKGROUND"
            ]
        },
        "input": "sent0: We experimented with several setups using AGs for unsupervised segmentation.\n sent1: Language-Independent Morphological Segmenter.\n sent2: LIMS is the best-on-average AG setup obtained by #TARGET_REF when trained on six languages (English, German, Finnish, Estonian, Turkish and Zulu), which is the Cascaded PrStSu+SM configuration.\n sent3: We use this AG setup for each of the four languages.\n sent4: We refer to this system as AG LIM S .\n",
        "output": "{\"label\": [\"USE\", \"BACKGROUND\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "We refer to this system as AG LIM S .",
                "Best AG Configuration per Language.",
                "In this experimental setup, we consider all nine grammars from #TARGET_REF using both the Standard and the Cascaded approaches and choosing the one that is best for each polysynthetic language by training on the training set and evaluating on the development set.",
                "We denote this system as AG BestL .",
                "Using Seeded Knowledge."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We refer to this system as AG LIM S .\n sent1: Best AG Configuration per Language.\n sent2: In this experimental setup, we consider all nine grammars from #TARGET_REF using both the Standard and the Cascaded approaches and choosing the one that is best for each polysynthetic language by training on the training set and evaluating on the development set.\n sent3: We denote this system as AG BestL .\n sent4: Using Seeded Knowledge.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Unsupervised approaches based on Adaptor Grammars show promise for morphological segmentation of low-resource polysynthetic languages.",
                "We worked with the AG grammars developed by #TARGET_REF Eskander et al. ( , 2018 for languages that are not polysynthetic.",
                "We showed that even when using these approaches and very little data, we can obtain encouraging results, and that using additional unsupervised data is a promising path."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Unsupervised approaches based on Adaptor Grammars show promise for morphological segmentation of low-resource polysynthetic languages.\n sent1: We worked with the AG grammars developed by #TARGET_REF Eskander et al. ( , 2018 for languages that are not polysynthetic.\n sent2: We showed that even when using these approaches and very little data, we can obtain encouraging results, and that using additional unsupervised data is a promising path.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "In this experimental setup, we consider all nine grammars from #REF using both the Standard and the Cascaded approaches and choosing the one that is best for each polysynthetic language by training on the training set and evaluating on the development set.",
                "We denote this system as AG BestL .",
                "Using Seeded Knowledge.",
                "To approximate the effect of Scholar-seeded-Knowledge in #TARGET_REF, we used the training set to derive affixes and use them as scholar-seeded knowledge added to the grammars (before the learning happens).",
                "However, since affixes and stems are not distinguished in the training annotations from #REF , we only consider the first and last morphemes that appear at least five times."
            ],
            "label": [
                "EXTENDS",
                "SIMILARITY"
            ]
        },
        "input": "sent0: In this experimental setup, we consider all nine grammars from #REF using both the Standard and the Cascaded approaches and choosing the one that is best for each polysynthetic language by training on the training set and evaluating on the development set.\n sent1: We denote this system as AG BestL .\n sent2: Using Seeded Knowledge.\n sent3: To approximate the effect of Scholar-seeded-Knowledge in #TARGET_REF, we used the training set to derive affixes and use them as scholar-seeded knowledge added to the grammars (before the learning happens).\n sent4: However, since affixes and stems are not distinguished in the training annotations from #REF , we only consider the first and last morphemes that appear at least five times.\n",
        "output": "{\"label\": [\"EXTENDS\", \"SIMILARITY\"], \"context\": [\"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "We propose an unsupervised approach for morphological segmentation of polysynthetic languages based on Adaptor Grammars (#REF) .",
                "We experiment with four UtoAztecan languages: Mexicanero (MX), Nahuatl (NH), Wixarika (WX) and Yorem Nokki (YN) (#REF) .",
                "Adaptor Grammars (AGs) are nonparametric Bayesian models that generalize probabilistic context free grammars (PCFG), and have proven to be successful for unsupervised morphological segmentation, where a PCFG is a morphological grammar that specifies word structure (#REF; #REF; #TARGET_REF #REF .",
                "Our main goal is to examine the success of Adaptor Grammars for unsupervised morphological segmentation when applied to polysynthetic languages, where the morphology is synthetically complex (not simply agglutinative), and where resources are minimal.",
                "We use the datasets introduced by #REF in an unsupervised fashion (unsegmented words)."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: We propose an unsupervised approach for morphological segmentation of polysynthetic languages based on Adaptor Grammars (#REF) .\n sent1: We experiment with four UtoAztecan languages: Mexicanero (MX), Nahuatl (NH), Wixarika (WX) and Yorem Nokki (YN) (#REF) .\n sent2: Adaptor Grammars (AGs) are nonparametric Bayesian models that generalize probabilistic context free grammars (PCFG), and have proven to be successful for unsupervised morphological segmentation, where a PCFG is a morphological grammar that specifies word structure (#REF; #REF; #TARGET_REF #REF .\n sent3: Our main goal is to examine the success of Adaptor Grammars for unsupervised morphological segmentation when applied to polysynthetic languages, where the morphology is synthetically complex (not simply agglutinative), and where resources are minimal.\n sent4: We use the datasets introduced by #REF in an unsupervised fashion (unsegmented words).\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Table 3 shows the performance of our AG setups on the four languages.",
                "The best AG setup learned for each of the four polysynthetic languages (AG BestL ) is the PrStSu+SM grammar using the Cascaded learning setup.",
                "This is an interesting finding as the Cascaded PrSTSu+SM setup is in fact AG LIM S -the best-on-average AG setup obtained by #TARGET_REF Table 4 : Best AG results compared to supervised approaches from #REF .",
                "Bold indicates best scores.",
                "WX and YN, respectively."
            ],
            "label": [
                "BACKGROUND",
                "SIMILARITY"
            ]
        },
        "input": "sent0: Table 3 shows the performance of our AG setups on the four languages.\n sent1: The best AG setup learned for each of the four polysynthetic languages (AG BestL ) is the PrStSu+SM grammar using the Cascaded learning setup.\n sent2: This is an interesting finding as the Cascaded PrSTSu+SM setup is in fact AG LIM S -the best-on-average AG setup obtained by #TARGET_REF Table 4 : Best AG results compared to supervised approaches from #REF .\n sent3: Bold indicates best scores.\n sent4: WX and YN, respectively.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"SIMILARITY\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Indeed, principled methods are now required that allow us to measure, understand and remove biases in our data in order for these systems to be truly accepted as a prominent part of our lives.",
                "In the domain of text, many modern approaches often begin by embedding the input text data into an embedding space that is used as the first layer in a subsequent deep network [4] , [14] .",
                "These word embeddings have been shown to contain the same biases #TARGET_REF , due to the source data from which they are trained.",
                "In effect, biases from the source data, such as in the differences in representation for men and women, that have been found in many different large-scale studies [5] , [10] , [12] , carry through to the semantic relations in the word embeddings, which become baked into the learning systems that are built on top of them.",
                "In this paper, we make three contributions towards addressing these concerns."
            ],
            "label": [
                "BACKGROUND",
                "MOTIVATION"
            ]
        },
        "input": "sent0: Indeed, principled methods are now required that allow us to measure, understand and remove biases in our data in order for these systems to be truly accepted as a prominent part of our lives.\n sent1: In the domain of text, many modern approaches often begin by embedding the input text data into an embedding space that is used as the first layer in a subsequent deep network [4] , [14] .\n sent2: These word embeddings have been shown to contain the same biases #TARGET_REF , due to the source data from which they are trained.\n sent3: In effect, biases from the source data, such as in the differences in representation for men and women, that have been found in many different large-scale studies [5] , [10] , [12] , carry through to the semantic relations in the word embeddings, which become baked into the learning systems that are built on top of them.\n sent4: In this paper, we make three contributions towards addressing these concerns.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"MOTIVATION\"], \"context\": [\"sent1\", \"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "First we propose a new version of the Word Embedding Association Tests (WEATs) studied in #TARGET_REF , designed to demonstrate and quantify bias in word embeddings, which puts them on a firm foundation by using the Linguistic Inquiry and Word Count (LIWC) lexica [17] to systematically detect and measure embedding biases.",
                "With this improved experimental setting, we find that European-American names are viewed more positively than African-American names, male names are more associated with work while female names are more associated with family, and that the academic disciplines of science and maths are more associated with male terms than the arts, which are more associated with female terms.",
                "Using this new methodology, we then find that there is a gender bias in the way different occupations are represented by the embedding.",
                "Furthermore, we use the latest official employment statistics in the UK, and find that there is a correlation between the ratio of men and women working in different occupation roles and how those roles are associated with gender in the word embeddings.",
                "This suggests that biases in the embeddings reflect biases in the world."
            ],
            "label": [
                "BACKGROUND",
                "MOTIVATION",
                "DIFFERENCES"
            ]
        },
        "input": "sent0: First we propose a new version of the Word Embedding Association Tests (WEATs) studied in #TARGET_REF , designed to demonstrate and quantify bias in word embeddings, which puts them on a firm foundation by using the Linguistic Inquiry and Word Count (LIWC) lexica [17] to systematically detect and measure embedding biases.\n sent1: With this improved experimental setting, we find that European-American names are viewed more positively than African-American names, male names are more associated with work while female names are more associated with family, and that the academic disciplines of science and maths are more associated with male terms than the arts, which are more associated with female terms.\n sent2: Using this new methodology, we then find that there is a gender bias in the way different occupations are represented by the embedding.\n sent3: Furthermore, we use the latest official employment statistics in the UK, and find that there is a correlation between the ratio of men and women working in different occupation roles and how those roles are associated with gender in the word embeddings.\n sent4: This suggests that biases in the embeddings reflect biases in the world.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"MOTIVATION\", \"DIFFERENCES\"], \"context\": [\"sent0\", \"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "In this paper, we conduct three experiments on semantic word embeddings.",
                "We first propose a new version of the Word Embedding Association Tests studied in #TARGET_REF by using the LIWC lexica to systematically detect and measure the biases within the embedding, keeping the tests comparable with the same set of target words.",
                "We further extend this work using additional sets of target words, and compare sentiment across male and female names.",
                "Furthermore, we investigate gender bias in words that represent different occupations, comparing these associations with UK national employment statistics.",
                "In the last experiment, we use orthogonal projections [2] to debias our word embeddings, and measure the reduction in the biases demonstrated in the previous two experiments."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In this paper, we conduct three experiments on semantic word embeddings.\n sent1: We first propose a new version of the Word Embedding Association Tests studied in #TARGET_REF by using the LIWC lexica to systematically detect and measure the biases within the embedding, keeping the tests comparable with the same set of target words.\n sent2: We further extend this work using additional sets of target words, and compare sentiment across male and female names.\n sent3: Furthermore, we investigate gender bias in words that represent different occupations, comparing these associations with UK national employment statistics.\n sent4: In the last experiment, we use orthogonal projections [2] to debias our word embeddings, and measure the reduction in the biases demonstrated in the previous two experiments.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\", \"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Using the list of occupations from the previous section, we compared their association with each of the genders with the ratio of the actual number of men and women working in those roles, as recorded in the official statistics [15] , where 1 indicates only men work in this role, and 0 only women.",
                "We found that there is a strong, significant correlation (ρ = 0.57, p-value < 10 −6 ) between the word embedding association between gender and occupation and the number of people of each gender in the United Kingdom working in those roles.",
                "This supports a similar finding for U.S. employment statistics using an independent set of occupations found in #TARGET_REF ."
            ],
            "label": [
                "BACKGROUND",
                "SIMILARITY"
            ]
        },
        "input": "sent0: Using the list of occupations from the previous section, we compared their association with each of the genders with the ratio of the actual number of men and women working in those roles, as recorded in the official statistics [15] , where 1 indicates only men work in this role, and 0 only women.\n sent1: We found that there is a strong, significant correlation (ρ = 0.57, p-value < 10 −6 ) between the word embedding association between gender and occupation and the number of people of each gender in the United Kingdom working in those roles.\n sent2: This supports a similar finding for U.S. employment statistics using an independent set of occupations found in #TARGET_REF .\n",
        "output": "{\"label\": [\"BACKGROUND\", \"SIMILARITY\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "If we want AI to take a central position in society, we need to be able to detect and remove any source of possible discrimination, to ensure fairness and transparency, and ultimately trust in these learning systems.",
                "Principled methods to measure biases will certainly need to play a central role in this, as will an understanding of the origins of biases, and new developments in methods that can be used to remove biases once detected.",
                "In this paper, we have introduced the LIWC-WEAT, a set of objective tests extending the association tests in #TARGET_REF by using the LIWC lexica to measure bias within word embeddings.",
                "We found bias in both the associations of gender and race, as first described in [3] , while additionally finding that male names have a slightly higher positive association than female names.",
                "Biases found in the embedding were also shown to reflect biases in the real world and the media, where we found a correlation between the number of men and women in an occupation and its association with each set of male and female names."
            ],
            "label": [
                "BACKGROUND",
                "MOTIVATION"
            ]
        },
        "input": "sent0: If we want AI to take a central position in society, we need to be able to detect and remove any source of possible discrimination, to ensure fairness and transparency, and ultimately trust in these learning systems.\n sent1: Principled methods to measure biases will certainly need to play a central role in this, as will an understanding of the origins of biases, and new developments in methods that can be used to remove biases once detected.\n sent2: In this paper, we have introduced the LIWC-WEAT, a set of objective tests extending the association tests in #TARGET_REF by using the LIWC lexica to measure bias within word embeddings.\n sent3: We found bias in both the associations of gender and race, as first described in [3] , while additionally finding that male names have a slightly higher positive association than female names.\n sent4: Biases found in the embedding were also shown to reflect biases in the real world and the media, where we found a correlation between the number of men and women in an occupation and its association with each set of male and female names.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"MOTIVATION\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Principled methods to measure biases will certainly need to play a central role in this, as will an understanding of the origins of biases, and new developments in methods that can be used to remove biases once detected.",
                "In this paper, we have introduced the LIWC-WEAT, a set of objective tests extending the association tests in [3] by using the LIWC lexica to measure bias within word embeddings.",
                "We found bias in both the associations of gender and race, as first described in #TARGET_REF , while additionally finding that male names have a slightly higher positive association than female names.",
                "Biases found in the embedding were also shown to reflect biases in the real world and the media, where we found a correlation between the number of men and women in an occupation and its association with each set of male and female names.",
                "Finally, using a projection algorithm [2] , we were able to reduce the gender bias shown in the embeddings, resulting in a decrease in the difference between associations for all tests based upon gender."
            ],
            "label": [
                "BACKGROUND",
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Principled methods to measure biases will certainly need to play a central role in this, as will an understanding of the origins of biases, and new developments in methods that can be used to remove biases once detected.\n sent1: In this paper, we have introduced the LIWC-WEAT, a set of objective tests extending the association tests in [3] by using the LIWC lexica to measure bias within word embeddings.\n sent2: We found bias in both the associations of gender and race, as first described in #TARGET_REF , while additionally finding that male names have a slightly higher positive association than female names.\n sent3: Biases found in the embedding were also shown to reflect biases in the real world and the media, where we found a correlation between the number of men and women in an occupation and its association with each set of male and female names.\n sent4: Finally, using a projection algorithm [2] , we were able to reduce the gender bias shown in the embeddings, resulting in a decrease in the difference between associations for all tests based upon gender.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"DIFFERENCES\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Taking the list of target European-American and African-American names used in #TARGET_REF , we tested each of them for their associated with the positive and negative emotion concepts found in [17] by using the methodology described by Eq. 3 in Sec. II-B, replacing the short list of words used to originally represent pleasant and unpleasant attribute sets.",
                "Our test found that while both European-American names and African-American names are more associated with positive emotions than negative emotions, the test showed that European-American names are more associated with positive emotions than their African-American counterparts, as shown in Fig. 1a .",
                "This finding supports the association test in [3] , where they also found that European-American names were more pleasant than African-American names."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Taking the list of target European-American and African-American names used in #TARGET_REF , we tested each of them for their associated with the positive and negative emotion concepts found in [17] by using the methodology described by Eq. 3 in Sec. II-B, replacing the short list of words used to originally represent pleasant and unpleasant attribute sets.\n sent1: Our test found that while both European-American names and African-American names are more associated with positive emotions than negative emotions, the test showed that European-American names are more associated with positive emotions than their African-American counterparts, as shown in Fig. 1a .\n sent2: This finding supports the association test in [3] , where they also found that European-American names were more pleasant than African-American names.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "A further test was conducted to find the association between words related to different subject disciplines (e.g. arts, maths, science) with each of the genders using the 'he' and 'she' categories from LIWC [17] .",
                "The results of our test again support the findings of [3] , with Maths and Science terms being more closely associated with males, while Arts terms are more closely associated with females, as shown in Fig. 1b.",
                "3) Association of Gender with Career and Family : Taking the list of target gendered names used in #TARGET_REF , we tested each of them for their associated with the career and family concepts using the categories of 'work' and 'family' found in LIWC [17] .",
                "As shown in Fig. 1c , we found that the set of male names was more associated with the concept of work, while the female names were more associated with family, mirroring the results found in [3] .",
                "Extending this test, we generated a much larger set of male and female target names from an online list of baby names 1 ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: A further test was conducted to find the association between words related to different subject disciplines (e.g. arts, maths, science) with each of the genders using the 'he' and 'she' categories from LIWC [17] .\n sent1: The results of our test again support the findings of [3] , with Maths and Science terms being more closely associated with males, while Arts terms are more closely associated with females, as shown in Fig. 1b.\n sent2: 3) Association of Gender with Career and Family : Taking the list of target gendered names used in #TARGET_REF , we tested each of them for their associated with the career and family concepts using the categories of 'work' and 'family' found in LIWC [17] .\n sent3: As shown in Fig. 1c , we found that the set of male names was more associated with the concept of work, while the female names were more associated with family, mirroring the results found in [3] .\n sent4: Extending this test, we generated a much larger set of male and female target names from an online list of baby names 1 .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Taking the list of target European-American and African-American names used in [3] , we tested each of them for their associated with the positive and negative emotion concepts found in [17] by using the methodology described by Eq. 3 in Sec. II-B, replacing the short list of words used to originally represent pleasant and unpleasant attribute sets.",
                "Our test found that while both European-American names and African-American names are more associated with positive emotions than negative emotions, the test showed that European-American names are more associated with positive emotions than their African-American counterparts, as shown in Fig. 1a .",
                "This finding supports the association test in #TARGET_REF , where they also found that European-American names were more pleasant than African-American names."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: Taking the list of target European-American and African-American names used in [3] , we tested each of them for their associated with the positive and negative emotion concepts found in [17] by using the methodology described by Eq. 3 in Sec. II-B, replacing the short list of words used to originally represent pleasant and unpleasant attribute sets.\n sent1: Our test found that while both European-American names and African-American names are more associated with positive emotions than negative emotions, the test showed that European-American names are more associated with positive emotions than their African-American counterparts, as shown in Fig. 1a .\n sent2: This finding supports the association test in #TARGET_REF , where they also found that European-American names were more pleasant than African-American names.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "A further test was conducted to find the association between words related to different subject disciplines (e.g. arts, maths, science) with each of the genders using the 'he' and 'she' categories from LIWC [17] .",
                "The results of our test again support the findings of #TARGET_REF , with Maths and Science terms being more closely associated with males, while Arts terms are more closely associated with females, as shown in Fig. 1b.",
                "3) Association of Gender with Career and Family : Taking the list of target gendered names used in [3] , we tested each of them for their associated with the career and family concepts using the categories of 'work' and 'family' found in LIWC [17] .",
                "As shown in Fig. 1c , we found that the set of male names was more associated with the concept of work, while the female names were more associated with family, mirroring the results found in [3] .",
                "Extending this test, we generated a much larger set of male and female target names from an online list of baby names 1 ."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: A further test was conducted to find the association between words related to different subject disciplines (e.g. arts, maths, science) with each of the genders using the 'he' and 'she' categories from LIWC [17] .\n sent1: The results of our test again support the findings of #TARGET_REF , with Maths and Science terms being more closely associated with males, while Arts terms are more closely associated with females, as shown in Fig. 1b.\n sent2: 3) Association of Gender with Career and Family : Taking the list of target gendered names used in [3] , we tested each of them for their associated with the career and family concepts using the categories of 'work' and 'family' found in LIWC [17] .\n sent3: As shown in Fig. 1c , we found that the set of male names was more associated with the concept of work, while the female names were more associated with family, mirroring the results found in [3] .\n sent4: Extending this test, we generated a much larger set of male and female target names from an online list of baby names 1 .\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Male and Females names tested in #TARGET_REF showed a clear distinction in their association with work and family respectively, with our replication of the test in Sec. III-B3 finding the same results.",
                "Performing the same tests again after applying the gender projection to both name lists, we wished to quantify the change in associations.",
                "We calculated the change in the distance between the centroids of each set of names before and after applying the orthogonal gender projection, finding that the association with work for males and family for females reduced, closing the gap between male and female names by 37.5% for the target names found in the original WEAT and 66% for the extended list of names respectively.",
                "In our experiment looking at the association of positive and negative emotions with male and female names, we found that male and female names were both positive, with male names being slightly more associated with positive emotions than female names.",
                "The same finding were also true when using a larger set of names and making the same comparison."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: Male and Females names tested in #TARGET_REF showed a clear distinction in their association with work and family respectively, with our replication of the test in Sec. III-B3 finding the same results.\n sent1: Performing the same tests again after applying the gender projection to both name lists, we wished to quantify the change in associations.\n sent2: We calculated the change in the distance between the centroids of each set of names before and after applying the orthogonal gender projection, finding that the association with work for males and family for females reduced, closing the gap between male and female names by 37.5% for the target names found in the original WEAT and 66% for the extended list of names respectively.\n sent3: In our experiment looking at the association of positive and negative emotions with male and female names, we found that male and female names were both positive, with male names being slightly more associated with positive emotions than female names.\n sent4: The same finding were also true when using a larger set of names and making the same comparison.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "The results of our test again support the findings of [3] , with Maths and Science terms being more closely associated with males, while Arts terms are more closely associated with females, as shown in Fig. 1b.",
                "3) Association of Gender with Career and Family : Taking the list of target gendered names used in [3] , we tested each of them for their associated with the career and family concepts using the categories of 'work' and 'family' found in LIWC [17] .",
                "As shown in Fig. 1c , we found that the set of male names was more associated with the concept of work, while the female names were more associated with family, mirroring the results found in #TARGET_REF .",
                "Extending this test, we generated a much larger set of male and female target names from an online list of baby names 1 .",
                "Repeating the same test on this larger set of names, we found that male and female names were much less separated than suggested by previous results, with only minor differences between the two, as shown in Fig. 1d ."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: The results of our test again support the findings of [3] , with Maths and Science terms being more closely associated with males, while Arts terms are more closely associated with females, as shown in Fig. 1b.\n sent1: 3) Association of Gender with Career and Family : Taking the list of target gendered names used in [3] , we tested each of them for their associated with the career and family concepts using the categories of 'work' and 'family' found in LIWC [17] .\n sent2: As shown in Fig. 1c , we found that the set of male names was more associated with the concept of work, while the female names were more associated with family, mirroring the results found in #TARGET_REF .\n sent3: Extending this test, we generated a much larger set of male and female target names from an online list of baby names 1 .\n sent4: Repeating the same test on this larger set of names, we found that male and female names were much less separated than suggested by previous results, with only minor differences between the two, as shown in Fig. 1d .\n",
        "output": "{\"label\": [null], \"context\": [\"sent2\", \"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "This transcription bottleneck problem can be handled by translating into a widely spoken language to ensure subsequent interpretability of the collected recordings, and such parallel corpora have been recently created by aligning the collected audio with translations in a well-resourced language (#REF; #REF; #REF) .",
                "Moreover, some linguists suggested that more than one translation should be collected to capture deeper layers of meaning (#REF) .",
                "This work is a contribution to the Computational Language Documentation (CLD) research field, that aims to replace part of the manual steps performed by linguists during language documentation initiatives by automatic approaches.",
                "Here we investigate the unsupervised word discovery and segmentation task, using the bilingual-rooted approach from #TARGET_REF .",
                "There, words in the well-resourced language are aligned to unsegmented phonemes in the endangered language in order to identify group of phonemes, and to cluster them into word-like units."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This transcription bottleneck problem can be handled by translating into a widely spoken language to ensure subsequent interpretability of the collected recordings, and such parallel corpora have been recently created by aligning the collected audio with translations in a well-resourced language (#REF; #REF; #REF) .\n sent1: Moreover, some linguists suggested that more than one translation should be collected to capture deeper layers of meaning (#REF) .\n sent2: This work is a contribution to the Computational Language Documentation (CLD) research field, that aims to replace part of the manual steps performed by linguists during language documentation initiatives by automatic approaches.\n sent3: Here we investigate the unsupervised word discovery and segmentation task, using the bilingual-rooted approach from #TARGET_REF .\n sent4: There, words in the well-resourced language are aligned to unsegmented phonemes in the endangered language in order to identify group of phonemes, and to cluster them into word-like units.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "1 The languages added to the dataset are: English, German, Portuguese and Spanish.",
                "Table 1 shows some statistics for the produced Multilingual Mboshi parallel corpus.",
                "2 Bilingual Unsupervised Word Segmentation/Discovery Approach: We use the bilingual neuralbased Unsupervised Word Segmentation (UWS) approach from #TARGET_REF to discover words in Mboshi.",
                "In this approach, Neural Machine Translation (NMT) models are trained between language pairs, using as source language the translation (word-level) and as target, the language to document (unsegmented phonemic sequence).",
                "Due to the attention mechanism present in these networks (#REF) , posterior to training, it is possible to retrieve soft-alignment probability matrices between source and target sequences."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: 1 The languages added to the dataset are: English, German, Portuguese and Spanish.\n sent1: Table 1 shows some statistics for the produced Multilingual Mboshi parallel corpus.\n sent2: 2 Bilingual Unsupervised Word Segmentation/Discovery Approach: We use the bilingual neuralbased Unsupervised Word Segmentation (UWS) approach from #TARGET_REF to discover words in Mboshi.\n sent3: In this approach, Neural Machine Translation (NMT) models are trained between language pairs, using as source language the translation (word-level) and as target, the language to document (unsegmented phonemic sequence).\n sent4: Due to the attention mechanism present in these networks (#REF) , posterior to training, it is possible to retrieve soft-alignment probability matrices between source and target sequences.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Due to the attention mechanism present in these networks (#REF) , posterior to training, it is possible to retrieve soft-alignment probability matrices between source and target sequences.",
                "These matrices give us sentence-level source-to-target alignment information, and by using it for clustering neighbor phonemes aligned to the same translation word, we are able to create segmentation in the target side.",
                "The product of this approach is a set of (discovered-units, translation words) pairs.",
                "Multilingual Leveraging: In this work we apply two simple methods for including multilingual information into the bilingual models from #TARGET_REF .",
                "The first one, Multilingual Voting, consists of merging the information learned by models trained with different language pairs by performing a voting over the final discovered boundaries."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Due to the attention mechanism present in these networks (#REF) , posterior to training, it is possible to retrieve soft-alignment probability matrices between source and target sequences.\n sent1: These matrices give us sentence-level source-to-target alignment information, and by using it for clustering neighbor phonemes aligned to the same translation word, we are able to create segmentation in the target side.\n sent2: The product of this approach is a set of (discovered-units, translation words) pairs.\n sent3: Multilingual Leveraging: In this work we apply two simple methods for including multilingual information into the bilingual models from #TARGET_REF .\n sent4: The first one, Multilingual Voting, consists of merging the information learned by models trained with different language pairs by performing a voting over the final discovered boundaries.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Recent work on deep learning syntactic parsing models has achieved notably good results, e.g., #REF with 92.4 F 1 on Penn Treebank constituency parsing and #TARGET_REF with 92.8 F 1 .",
                "In this paper we borrow from the approaches of both of these works and present a neural-net parse reranker that achieves very good results, 93.8 F 1 , with a comparatively simple architecture.",
                "In the remainder of this section we outline the major difference between this and previous workviewing parsing as a language modeling problem.",
                "Section 2 looks more closely at three of the most relevant previous papers.",
                "We then describe our exact model (Section 3), followed by the experimental setup and results (Sections 4 and 5)."
            ],
            "label": [
                "BACKGROUND",
                "USE"
            ]
        },
        "input": "sent0: Recent work on deep learning syntactic parsing models has achieved notably good results, e.g., #REF with 92.4 F 1 on Penn Treebank constituency parsing and #TARGET_REF with 92.8 F 1 .\n sent1: In this paper we borrow from the approaches of both of these works and present a neural-net parse reranker that achieves very good results, 93.8 F 1 , with a comparatively simple architecture.\n sent2: In the remainder of this section we outline the major difference between this and previous workviewing parsing as a language modeling problem.\n sent3: Section 2 looks more closely at three of the most relevant previous papers.\n sent4: We then describe our exact model (Section 3), followed by the experimental setup and results (Sections 4 and 5).\n",
        "output": "{\"label\": [\"BACKGROUND\", \"USE\"], \"context\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "A generative parsing model parses a sentence (x) into its phrasal structure (y) according to",
                "where Y(x) lists all possible structures of x. If we think of a tree (x, y) as a sequence (z) #TARGET_REF as illustrated in Figure 1 , we can define a probability distribution over (x, y) as follows:",
                "which is equivalent to Equation (1).",
                "We have reduced parsing to language modeling and can use language modeling techniques of estimating P (z t |z 1 , · · · , z t−1 ) for parsing."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: A generative parsing model parses a sentence (x) into its phrasal structure (y) according to\n sent1: where Y(x) lists all possible structures of x. If we think of a tree (x, y) as a sequence (z) #TARGET_REF as illustrated in Figure 1 , we can define a probability distribution over (x, y) as follows:\n sent2: which is equivalent to Equation (1).\n sent3: We have reduced parsing to language modeling and can use language modeling techniques of estimating P (z t |z 1 , · · · , z t−1 ) for parsing.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We look here at three neural net (NN) models closest to our research along various dimensions.",
                "The first (#REF) gives the basic language modeling architecture that we have adopted, while the other two #TARGET_REF; #REF) are parsing models that have the current best results in NN parsing."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We look here at three neural net (NN) models closest to our research along various dimensions.\n sent1: The first (#REF) gives the basic language modeling architecture that we have adopted, while the other two #TARGET_REF; #REF) are parsing models that have the current best results in NN parsing.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We use the Wall Street Journal (WSJ) of the Penn Treebank (#REF) for training (2-21), development (24) and testing (23) and millions of auto-parsed \"silver\" trees (#REF; #REF; #TARGET_REF for tritraining.",
                "To obtain silver trees, we parse the entire section of the New York Times (NYT) of the fifth Gigaword (#REF ) with a product of eight Berkeley parsers (#REF) 2 and ZPar (#REF) and select 24 million trees on which both parsers agree (#REF) .",
                "We do not resample trees to match the sentence length distribution of the NYT to that of the WSJ (Vinyals et 1 The code and trained models used for experiments are available at github.com/cdg720/emnlp2016.",
                "2 We use the reimplementation by #REF .",
                "(#REF) performed better when trained on all of 24 million trees than when trained on resampled two million trees."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We use the Wall Street Journal (WSJ) of the Penn Treebank (#REF) for training (2-21), development (24) and testing (23) and millions of auto-parsed \"silver\" trees (#REF; #REF; #TARGET_REF for tritraining.\n sent1: To obtain silver trees, we parse the entire section of the New York Times (NYT) of the fifth Gigaword (#REF ) with a product of eight Berkeley parsers (#REF) 2 and ZPar (#REF) and select 24 million trees on which both parsers agree (#REF) .\n sent2: We do not resample trees to match the sentence length distribution of the NYT to that of the WSJ (Vinyals et 1 The code and trained models used for experiments are available at github.com/cdg720/emnlp2016.\n sent3: 2 We use the reimplementation by #REF .\n sent4: (#REF) performed better when trained on all of 24 million trees than when trained on resampled two million trees.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "We compare LSTM-LM (GS) to two very strong semi-supervised NN parsers: an ensemble of five MTPs trained on 11 million trees of the highconfidence corpus 4 (HC) #TARGET_REF ; and an ensemble of six one-to-many sequence models trained on the HC and 4.5 millions of EnglishGerman translation sentence pairs (#REF) .",
                "We also compare LSTM-LM (GS) to best performing non-NN parsers in the literature.",
                "Parsers' parsing performance along with their training data is reported in Table 3 .",
                "LSTM-LM (GS) outperforms all the other parsers with 93.1 F 1 ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We compare LSTM-LM (GS) to two very strong semi-supervised NN parsers: an ensemble of five MTPs trained on 11 million trees of the highconfidence corpus 4 (HC) #TARGET_REF ; and an ensemble of six one-to-many sequence models trained on the HC and 4.5 millions of EnglishGerman translation sentence pairs (#REF) .\n sent1: We also compare LSTM-LM (GS) to best performing non-NN parsers in the literature.\n sent2: Parsers' parsing performance along with their training data is reported in Table 3 .\n sent3: LSTM-LM (GS) outperforms all the other parsers with 93.1 F 1 .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "We also wish to develop a complete parsing model using the LSTM-LM framework.",
                "Table 3 : Evaluation of models trained on the WSJ and additional resources.",
                "Note that the numbers of #TARGET_REF and #REF are not directly comparable as their models are evaluated on OntoNotesstyle trees instead of PTB-style trees.",
                "E(LSTM-LMs (GS)) is an ensemble of eight LSTM-LMs (GS).",
                "X/Y in Silver column indicates the number of silver trees used to train Charniak parser and LSTM-LM."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: We also wish to develop a complete parsing model using the LSTM-LM framework.\n sent1: Table 3 : Evaluation of models trained on the WSJ and additional resources.\n sent2: Note that the numbers of #TARGET_REF and #REF are not directly comparable as their models are evaluated on OntoNotesstyle trees instead of PTB-style trees.\n sent3: E(LSTM-LMs (GS)) is an ensemble of eight LSTM-LMs (GS).\n sent4: X/Y in Silver column indicates the number of silver trees used to train Charniak parser and LSTM-LM.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "For that reason, de #REF suggest that those representations could be modified for the purpose of parsing, thus creating a parsing representation.",
                "Transforming tree representations for the purpose of parsing is not a new idea.",
                "It has been done for constituency parsing for example by #REF but also for dependency parsing for example by #TARGET_REF .",
                "#REF modified the representation of several constructions in several languages and obtained a consistent improvement in parsing accuracy.",
                "In this paper, we will investigate the case of the verb group construction and attempt to reproduce the study by #REF on UD treebanks to find out whether or not the alternative representation is useful for parsing with UD."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For that reason, de #REF suggest that those representations could be modified for the purpose of parsing, thus creating a parsing representation.\n sent1: Transforming tree representations for the purpose of parsing is not a new idea.\n sent2: It has been done for constituency parsing for example by #REF but also for dependency parsing for example by #TARGET_REF .\n sent3: #REF modified the representation of several constructions in several languages and obtained a consistent improvement in parsing accuracy.\n sent4: In this paper, we will investigate the case of the verb group construction and attempt to reproduce the study by #REF on UD treebanks to find out whether or not the alternative representation is useful for parsing with UD.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "It has been done for constituency parsing for example by #REF but also for dependency parsing for example by #REF .",
                "#TARGET_REF modified the representation of several constructions in several languages and obtained a consistent improvement in parsing accuracy.",
                "In this paper, we will investigate the case of the verb group construction and attempt to reproduce the study by #REF on UD treebanks to find out whether or not the alternative representation is useful for parsing with UD.",
                "have shown that modifying coordination constructions and verb groups from their representation in the Prague Dependency Treebank (henceforth PDT) to a representation described in Melčuk (1988) (Mel'čuk style, henceforth MS) improves dependency parsing for Czech.",
                "The procedure they follow is as follows:"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: It has been done for constituency parsing for example by #REF but also for dependency parsing for example by #REF .\n sent1: #TARGET_REF modified the representation of several constructions in several languages and obtained a consistent improvement in parsing accuracy.\n sent2: In this paper, we will investigate the case of the verb group construction and attempt to reproduce the study by #REF on UD treebanks to find out whether or not the alternative representation is useful for parsing with UD.\n sent3: have shown that modifying coordination constructions and verb groups from their representation in the Prague Dependency Treebank (henceforth PDT) to a representation described in Melčuk (1988) (Mel'čuk style, henceforth MS) improves dependency parsing for Czech.\n sent4: The procedure they follow is as follows:\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "4. Transform the parsed data back to the original representation (for comparison with the original gold standard).",
                "#TARGET_REF have shown that these same modifications as well as the modification of nonprojective structures helps parsing in four languages.",
                "#REF conducted a study over the alternative representations of 6 constructions across 5 parsing models for English and found that some of them are easier to parse than others.",
                "Their results were consistent across parsing models.",
                "The motivations behind those two types of studies are different."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: 4. Transform the parsed data back to the original representation (for comparison with the original gold standard).\n sent1: #TARGET_REF have shown that these same modifications as well as the modification of nonprojective structures helps parsing in four languages.\n sent2: #REF conducted a study over the alternative representations of 6 constructions across 5 parsing models for English and found that some of them are easier to parse than others.\n sent3: Their results were consistent across parsing models.\n sent4: The motivations behind those two types of studies are different.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "In the PDT, main verbs are the head of auxiliary dependencies, as in Figure 1 .",
                "#TARGET_REF show that making the auxiliary the head of the dependency as in Figure 2 is useful for parsing Czech and Slovenian.",
                "#REF verb groups are easier to parse when the auxiliary is the head (as in PDT) than when the verb is the head (as in MS).",
                "Since UD adopts the PDT style representation of verb groups, it would be interesting to find out whether or not transforming them to MS could also improve parsing.",
                "This is what will be attempted in this study."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In the PDT, main verbs are the head of auxiliary dependencies, as in Figure 1 .\n sent1: #TARGET_REF show that making the auxiliary the head of the dependency as in Figure 2 is useful for parsing Czech and Slovenian.\n sent2: #REF verb groups are easier to parse when the auxiliary is the head (as in PDT) than when the verb is the head (as in MS).\n sent3: Since UD adopts the PDT style representation of verb groups, it would be interesting to find out whether or not transforming them to MS could also improve parsing.\n sent4: This is what will be attempted in this study.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "In this paper, we have attempted to reproduce a study by #TARGET_REF that has shown that making auxiliaries heads in verb groups improves parsing but failed to show that those results port to parsing with Universal Dependencies.",
                "Contrary to expectations, the study has given evidence that main verbs should stay heads of auxiliary dependency relations for parsing with UD.",
                "The benefits of error analyses for such a study have been highlighted because they allow us to shed more light on the different ways in which the transformations affect the parsing output.",
                "Experiments suggest that gains obtained from verb group transformations in previous studies have been obtained mainly because those transformations help disambiguating between main verbs and auxiliaries.",
                "It is however still an open question why the VG transformation hurts parsing accuracy in the case of UD."
            ],
            "label": [
                "BACKGROUND",
                "USE"
            ]
        },
        "input": "sent0: In this paper, we have attempted to reproduce a study by #TARGET_REF that has shown that making auxiliaries heads in verb groups improves parsing but failed to show that those results port to parsing with Universal Dependencies.\n sent1: Contrary to expectations, the study has given evidence that main verbs should stay heads of auxiliary dependency relations for parsing with UD.\n sent2: The benefits of error analyses for such a study have been highlighted because they allow us to shed more light on the different ways in which the transformations affect the parsing output.\n sent3: Experiments suggest that gains obtained from verb group transformations in previous studies have been obtained mainly because those transformations help disambiguating between main verbs and auxiliaries.\n sent4: It is however still an open question why the VG transformation hurts parsing accuracy in the case of UD.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "It has been done for constituency parsing for example by #REF but also for dependency parsing for example by #REF .",
                "#REF modified the representation of several constructions in several languages and obtained a consistent improvement in parsing accuracy.",
                "In this paper, we will investigate the case of the verb group construction and attempt to reproduce the study by #TARGET_REF on UD treebanks to find out whether or not the alternative representation is useful for parsing with UD.",
                "have shown that modifying coordination constructions and verb groups from their representation in the Prague Dependency Treebank (henceforth PDT) to a representation described in Melčuk (1988) (Mel'čuk style, henceforth MS) improves dependency parsing for Czech.",
                "The procedure they follow is as follows:"
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: It has been done for constituency parsing for example by #REF but also for dependency parsing for example by #REF .\n sent1: #REF modified the representation of several constructions in several languages and obtained a consistent improvement in parsing accuracy.\n sent2: In this paper, we will investigate the case of the verb group construction and attempt to reproduce the study by #TARGET_REF on UD treebanks to find out whether or not the alternative representation is useful for parsing with UD.\n sent3: have shown that modifying coordination constructions and verb groups from their representation in the Prague Dependency Treebank (henceforth PDT) to a representation described in Melčuk (1988) (Mel'čuk style, henceforth MS) improves dependency parsing for Czech.\n sent4: The procedure they follow is as follows:\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We will follow the methodology from #TARGET_REF , that is, to transform, parse and then detransform the data so as to compare the original and the transformed model on the original gold standard.",
                "The method from #REF which consists in comparing the baseline and the transformed data on their respective gold standard is less relevant here because UD is believed to be a useful representation and that the aim will be to improve parsing within that representation.",
                "However, as was argued in that study, their method can give an indication of the learnability of a construction and can potentially be used to understand the results obtained by the parse-transform-detransform method.",
                "For this reason, this method will also be attempted.",
                "In addition, the original parsed data will also be transformed into the MS gold standard for comparison with the MS parsed data on the MS gold standard."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We will follow the methodology from #TARGET_REF , that is, to transform, parse and then detransform the data so as to compare the original and the transformed model on the original gold standard.\n sent1: The method from #REF which consists in comparing the baseline and the transformed data on their respective gold standard is less relevant here because UD is believed to be a useful representation and that the aim will be to improve parsing within that representation.\n sent2: However, as was argued in that study, their method can give an indication of the learnability of a construction and can potentially be used to understand the results obtained by the parse-transform-detransform method.\n sent3: For this reason, this method will also be attempted.\n sent4: In addition, the original parsed data will also be transformed into the MS gold standard for comparison with the MS parsed data on the MS gold standard.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Dutch was discarded because the back transformation accuracy was low (90%).",
                "This is due to inconsistencies in the annotation: verb groups are annotated as a chain of dependency relations.",
                "This leaves us with a total of 25 out of the 37 treebanks.",
                "For comparability with the study in #TARGET_REF , and because we used a slightly modified version of their algorithm, we also tested the approach on the versions of the Czech and Slovenian treebanks that they worked on, respectively version 1.0 of the PDT (#REF ) and the 2006 version of SDT (#REF) .",
                "overview of the data used for the experiments."
            ],
            "label": [
                "USE",
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Dutch was discarded because the back transformation accuracy was low (90%).\n sent1: This is due to inconsistencies in the annotation: verb groups are annotated as a chain of dependency relations.\n sent2: This leaves us with a total of 25 out of the 37 treebanks.\n sent3: For comparability with the study in #TARGET_REF , and because we used a slightly modified version of their algorithm, we also tested the approach on the versions of the Czech and Slovenian treebanks that they worked on, respectively version 1.0 of the PDT (#REF ) and the 2006 version of SDT (#REF) .\n sent4: overview of the data used for the experiments.\n",
        "output": "{\"label\": [\"USE\", \"DIFFERENCES\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "They take this as partial explanation for the results that are inconsistent with the literature.",
                "However, the same problem can have arisen in and may have downplayed the effects that those studies have observed.",
                "It therefore seems that this explanation is not enough to account for those results.",
                "This raises the question of whether this phenomenon actually happened in the study by #TARGET_REF .",
                "It would be interesting to know if the effects they observed were affected by this kind of error amplification."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: They take this as partial explanation for the results that are inconsistent with the literature.\n sent1: However, the same problem can have arisen in and may have downplayed the effects that those studies have observed.\n sent2: It therefore seems that this explanation is not enough to account for those results.\n sent3: This raises the question of whether this phenomenon actually happened in the study by #TARGET_REF .\n sent4: It would be interesting to know if the effects they observed were affected by this kind of error amplification.\n",
        "output": "{\"label\": [null], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Another aspect that draws interest in translation models is the effective computation of sentence representations using the translation task as an auxiliary semantic signal (#REF; #REF; #REF; #REF ).",
                "An important feature that enables an immediate use of the MT-based representations in other downstream tasks is the creation of fixed-sized sentence embeddings #TARGET_REF .",
                "However, the effects of the size of sentence embeddings and the relation between translation performance and meaning representation quality are not entirely clear.",
                "Recent studies based on NMT either focus entirely on the use of MT-based sentence embeddings in other tasks (#REF) , on translation quality (#REF) , on speed comparison (#REF) , or only exploring a bilingual scenario (Cífka and #REF) .",
                "In this paper, we are interested in exploring a cross-lingual intermediate shared layer (called attention bridge) in an attentive encoder-decoder MT model."
            ],
            "label": [
                "BACKGROUND",
                "MOTIVATION"
            ]
        },
        "input": "sent0: Another aspect that draws interest in translation models is the effective computation of sentence representations using the translation task as an auxiliary semantic signal (#REF; #REF; #REF; #REF ).\n sent1: An important feature that enables an immediate use of the MT-based representations in other downstream tasks is the creation of fixed-sized sentence embeddings #TARGET_REF .\n sent2: However, the effects of the size of sentence embeddings and the relation between translation performance and meaning representation quality are not entirely clear.\n sent3: Recent studies based on NMT either focus entirely on the use of MT-based sentence embeddings in other tasks (#REF) , on translation quality (#REF) , on speed comparison (#REF) , or only exploring a bilingual scenario (Cífka and #REF) .\n sent4: In this paper, we are interested in exploring a cross-lingual intermediate shared layer (called attention bridge) in an attentive encoder-decoder MT model.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"MOTIVATION\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Another aspect that draws interest in translation models is the effective computation of sentence representations using the translation task as an auxiliary semantic signal (#REF; #REF; #REF; #REF ).",
                "An important feature that enables an immediate use of the MT-based representations in other downstream tasks is the creation of fixed-sized sentence embeddings (Cífka and #REF) .",
                "However, the effects of the size of sentence embeddings and the relation between translation performance and meaning representation quality are not entirely clear.",
                "Recent studies based on NMT either focus entirely on the use of MT-based sentence embeddings in other tasks (#REF) , on translation quality (#REF) , on speed comparison (#REF) , or only exploring a bilingual scenario #TARGET_REF .",
                "In this paper, we are interested in exploring a cross-lingual intermediate shared layer (called attention bridge) in an attentive encoder-decoder MT model."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Another aspect that draws interest in translation models is the effective computation of sentence representations using the translation task as an auxiliary semantic signal (#REF; #REF; #REF; #REF ).\n sent1: An important feature that enables an immediate use of the MT-based representations in other downstream tasks is the creation of fixed-sized sentence embeddings (Cífka and #REF) .\n sent2: However, the effects of the size of sentence embeddings and the relation between translation performance and meaning representation quality are not entirely clear.\n sent3: Recent studies based on NMT either focus entirely on the use of MT-based sentence embeddings in other tasks (#REF) , on translation quality (#REF) , on speed comparison (#REF) , or only exploring a bilingual scenario #TARGET_REF .\n sent4: In this paper, we are interested in exploring a cross-lingual intermediate shared layer (called attention bridge) in an attentive encoder-decoder MT model.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The overall architecture is illustrated in Figure 1 (see also Vázquez et al., 2019) .",
                "Due to the attentive connection between encoders and decoders we call this layer attention bridge, and its architecture is an adaptation from the model proposed by #TARGET_REF .",
                "Finally, each decoder follows a common attention mechanism in NMT, with the only exception that the context vector is computed on the attention bridge, and the initialization is performed by a mean pooling over it.",
                "Hence, the decoder receives the information only through the shared attention bridge.",
                "The fixed-sized representation coming out of the shared layer can immediately be applied to downstream tasks."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The overall architecture is illustrated in Figure 1 (see also Vázquez et al., 2019) .\n sent1: Due to the attentive connection between encoders and decoders we call this layer attention bridge, and its architecture is an adaptation from the model proposed by #TARGET_REF .\n sent2: Finally, each decoder follows a common attention mechanism in NMT, with the only exception that the context vector is computed on the attention bridge, and the initialization is performed by a mean pooling over it.\n sent3: Hence, the decoder receives the information only through the shared attention bridge.\n sent4: The fixed-sized representation coming out of the shared layer can immediately be applied to downstream tasks.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Table 2 : Results from supervised similarity tasks (SICK-R and STSB), measured using Pearson's (r) and Spearman's (ρ) correlation coefficients (r/ρ).",
                "The average across unsupervised similarity tasks on Pearson's measures are displayed in the right-most column.",
                "Results with † taken from #TARGET_REF",
                "of multilingual training.",
                "We can see that multilingual training objectives are generally helpful for the trainable downstream tasks."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Table 2 : Results from supervised similarity tasks (SICK-R and STSB), measured using Pearson's (r) and Spearman's (ρ) correlation coefficients (r/ρ).\n sent1: The average across unsupervised similarity tasks on Pearson's measures are displayed in the right-most column.\n sent2: Results with † taken from #TARGET_REF\n sent3: of multilingual training.\n sent4: We can see that multilingual training objectives are generally helpful for the trainable downstream tasks.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "We are also interested in the translation quality to verify the appropriateness of our models with respect to the main objective they are trained for.",
                "For this, we adopt the in-domain development and evaluation dataset from the ACL-WMT07 shared task.",
                "Sentences are encoded using Byte-Pair Encoding (#REF) , with 32,000 merge operations for each language.",
                "4 SentEval: Classification tasks Table 1 shows the performance of our models on two popular tasks (SNLI and SICK-E) as in #TARGET_REF as well as the average of all 10 SentEval downstream tasks.",
                "The experiments reveal two important findings:"
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: We are also interested in the translation quality to verify the appropriateness of our models with respect to the main objective they are trained for.\n sent1: For this, we adopt the in-domain development and evaluation dataset from the ACL-WMT07 shared task.\n sent2: Sentences are encoded using Byte-Pair Encoding (#REF) , with 32,000 merge operations for each language.\n sent3: 4 SentEval: Classification tasks Table 1 shows the performance of our models on two popular tasks (SNLI and SICK-E) as in #TARGET_REF as well as the average of all 10 SentEval downstream tasks.\n sent4: The experiments reveal two important findings:\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "model is provided by a bilingual setting with only one attention head.",
                "This is in line with the findings of #TARGET_REF and could also be expected as the model is more strongly pushed into a dense semantic abstraction that is beneficial for measuring similarities without further training.",
                "More surprising is the negative effect of the multilingual models.",
                "We believe that the multilingual information encoded jointly in the attention bridge hampers the results for the monolingual semantic similarity measured with the cosine distance, while it becomes easier in a bilingual scenario where the vector encodes only one source language, English in this case.",
                "ii) On the supervised textual similarity tasks, we find a similar trend as in the previous section for SICK: both a higher number of attention heads and multilinguality contribute to better scores, while for STSB, we notice a different pattern."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: model is provided by a bilingual setting with only one attention head.\n sent1: This is in line with the findings of #TARGET_REF and could also be expected as the model is more strongly pushed into a dense semantic abstraction that is beneficial for measuring similarities without further training.\n sent2: More surprising is the negative effect of the multilingual models.\n sent3: We believe that the multilingual information encoded jointly in the attention bridge hampers the results for the monolingual semantic similarity measured with the cosine distance, while it becomes easier in a bilingual scenario where the vector encodes only one source language, English in this case.\n sent4: ii) On the supervised textual similarity tasks, we find a similar trend as in the previous section for SICK: both a higher number of attention heads and multilinguality contribute to better scores, while for STSB, we notice a different pattern.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "These approaches typically involve end-to-end supervised learning to generate questions.",
                "#REF proposed sequence-to-sequence learning for question generation from text passages.",
                "#TARGET_REF utilized the answer-position, and linguistic features such as named entity recognition (NER) and parts of speech (POS) information to further improve the QG performance as the model is aware that for which answer a question need to be generated.",
                "In the work of a multi-perspective context matching algorithm is employed.",
                "#REF use a set of rich linguistic features along with a NQG model."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: These approaches typically involve end-to-end supervised learning to generate questions.\n sent1: #REF proposed sequence-to-sequence learning for question generation from text passages.\n sent2: #TARGET_REF utilized the answer-position, and linguistic features such as named entity recognition (NER) and parts of speech (POS) information to further improve the QG performance as the model is aware that for which answer a question need to be generated.\n sent3: In the work of a multi-perspective context matching algorithm is employed.\n sent4: #REF use a set of rich linguistic features along with a NQG model.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "For the encoder, we utilize a Long Short Term Memory (LSTM) (#REF) network.",
                "In order to capture more contextual information, we use a two-layer bidirectional LSTM (Bi-LSTM).",
                "Inspired by the success of using linguistic features in #TARGET_REF; #REF) , we exploit word knowledge in the form of entity linking and fine-grained entity typing in the encoder of the network.",
                "A Bi-LSTM encoder reads the passage words and their associated world knowledge features (c.f. section 3.1.1, 3.1.2) to produce a sequence of word-and-feature vectors.",
                "The word vectors, the embedded world knowledge feature vectors and the answer position indicator embedding vectors are concatenated and passed as input to the Bi-LSTM encoder."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: For the encoder, we utilize a Long Short Term Memory (LSTM) (#REF) network.\n sent1: In order to capture more contextual information, we use a two-layer bidirectional LSTM (Bi-LSTM).\n sent2: Inspired by the success of using linguistic features in #TARGET_REF; #REF) , we exploit word knowledge in the form of entity linking and fine-grained entity typing in the encoder of the network.\n sent3: A Bi-LSTM encoder reads the passage words and their associated world knowledge features (c.f. section 3.1.1, 3.1.2) to produce a sequence of word-and-feature vectors.\n sent4: The word vectors, the embedded world knowledge feature vectors and the answer position indicator embedding vectors are concatenated and passed as input to the Bi-LSTM encoder.\n",
        "output": "{\"label\": [null], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "(1) s2s+Att: Baseline encoder-decoder based seq2seq network with attention mechanism.",
                "(2) NQG: Extension of s2s+Att with answer position feature.",
                "(3) NQG + EL: Extension of NQG with the entity linking feature (500 dimension) discussed in Section 3.1.1. (4) NQG + EL (pre): NQG + Entity Linking with the pre-trained entity linking feature obtained from the joint training of word and Wikipedia entity using (#REF) .",
                "In order to compare our models with the existing coarse-grained entity features (NER) being used in literature #TARGET_REF; #REF) , we also report the following experiments.",
                "Table 3 , 4."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: (1) s2s+Att: Baseline encoder-decoder based seq2seq network with attention mechanism.\n sent1: (2) NQG: Extension of s2s+Att with answer position feature.\n sent2: (3) NQG + EL: Extension of NQG with the entity linking feature (500 dimension) discussed in Section 3.1.1. (4) NQG + EL (pre): NQG + Entity Linking with the pre-trained entity linking feature obtained from the joint training of word and Wikipedia entity using (#REF) .\n sent3: In order to compare our models with the existing coarse-grained entity features (NER) being used in literature #TARGET_REF; #REF) , we also report the following experiments.\n sent4: Table 3 , 4.\n",
        "output": "{\"label\": [null], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "In previous works #TARGET_REF; #REF) , named entity type features have been used.",
                "These features, however, only allow for the encoding of coarse level information such as knowledge of if an entity belongs to a set of predefined categories such as 'PERSON', 'LOCATION' and 'ORGANI-ZATION'.",
                "To alleviate this, we use the knowledge in the form of linked entities.",
                "In our experiments, we use Wikipedia as the knowledge base for which to link entities.",
                "This specific task (also known as Wikification (#REF) ) is the task of identifying concepts and entities in text and disambiguation them into the most specific corresponding Wikipedia pages."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: In previous works #TARGET_REF; #REF) , named entity type features have been used.\n sent1: These features, however, only allow for the encoding of coarse level information such as knowledge of if an entity belongs to a set of predefined categories such as 'PERSON', 'LOCATION' and 'ORGANI-ZATION'.\n sent2: To alleviate this, we use the knowledge in the form of linked entities.\n sent3: In our experiments, we use Wikipedia as the knowledge base for which to link entities.\n sent4: This specific task (also known as Wikification (#REF) ) is the task of identifying concepts and entities in text and disambiguation them into the most specific corresponding Wikipedia pages.\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We evaluated the performance of our approach on SQuAD (#REF) and MS MARCO v2.1 (#REF) .",
                "SQuAD is composed of more than 100K questions posed by crowd workers on 536 Wikipedia articles.",
                "We used the same split as #TARGET_REF .",
                "MS MARCO datasets contains 1 million queries with corresponding answers and passages.",
                "All questions are sampled from real anonymized user queries and context passages are extracted from real web documents."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: We evaluated the performance of our approach on SQuAD (#REF) and MS MARCO v2.1 (#REF) .\n sent1: SQuAD is composed of more than 100K questions posed by crowd workers on 536 Wikipedia articles.\n sent2: We used the same split as #TARGET_REF .\n sent3: MS MARCO datasets contains 1 million queries with corresponding answers and passages.\n sent4: All questions are sampled from real anonymized user queries and context passages are extracted from real web documents.\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "PredPatt 1 #TARGET_REF ) is a pattern-based framework for predicate-argument extraction.",
                "It defines a set of interpretable, extensible and non-lexicalized patterns based on Universal Dependencies (UD) (de #REF) , and extracts predicates and arguments through these manual patterns.",
                "Figure 1 shows the predicates and arguments extracted by PredPatt from the sentence: \"Chris, the designer, wants to launch a new brand.\" The underlying predicate-argument structure constructed by PredPatt is a directed graph, where a special dependency ARG is built between a predicate head token and its arguments' head tokens, and the original UD relations are retained within predicate phrases and argument phrases.",
                "For example, Figure 2 shows the directed graph for the predicate-argument extraction (1) and (2) in Figure 1 .",
                "Compared to other existing systems for predicate-argument extraction (#REF; #REF; #REF) , the use of manual language-agnostic patterns on UD makes PredPatt a well-founded component across languages."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: PredPatt 1 #TARGET_REF ) is a pattern-based framework for predicate-argument extraction.\n sent1: It defines a set of interpretable, extensible and non-lexicalized patterns based on Universal Dependencies (UD) (de #REF) , and extracts predicates and arguments through these manual patterns.\n sent2: Figure 1 shows the predicates and arguments extracted by PredPatt from the sentence: \"Chris, the designer, wants to launch a new brand.\" The underlying predicate-argument structure constructed by PredPatt is a directed graph, where a special dependency ARG is built between a predicate head token and its arguments' head tokens, and the original UD relations are retained within predicate phrases and argument phrases.\n sent3: For example, Figure 2 shows the directed graph for the predicate-argument extraction (1) and (2) in Figure 1 .\n sent4: Compared to other existing systems for predicate-argument extraction (#REF; #REF; #REF) , the use of manual language-agnostic patterns on UD makes PredPatt a well-founded component across languages.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF uses PredPatt to help augmenting data with Universal Decompositional Semantics.",
                "#REF adapts PredPatt to data generation for cross-lingual open information extraction.",
                "However, the evaluation of PredPatt has been restricted to manually-checked extractions over a small set of sentences #TARGET_REF , which lacks gold annotations to conduct an objective and reproducible evaluation, and inhibits the updates of patterns in PredPatt.",
                "Chris , the designer , wants to launch a new brand .",
                "In this work, we aim to conduct a large-scale and reproducible evaluation of PredPatt by introducing a large set of gold annotations gathered from PropBank (#REF) ."
            ],
            "label": [
                "BACKGROUND",
                "MOTIVATION",
                "EXTENDS"
            ]
        },
        "input": "sent0: #REF uses PredPatt to help augmenting data with Universal Decompositional Semantics.\n sent1: #REF adapts PredPatt to data generation for cross-lingual open information extraction.\n sent2: However, the evaluation of PredPatt has been restricted to manually-checked extractions over a small set of sentences #TARGET_REF , which lacks gold annotations to conduct an objective and reproducible evaluation, and inhibits the updates of patterns in PredPatt.\n sent3: Chris , the designer , wants to launch a new brand .\n sent4: In this work, we aim to conduct a large-scale and reproducible evaluation of PredPatt by introducing a large set of gold annotations gathered from PropBank (#REF) .\n",
        "output": "{\"label\": [\"BACKGROUND\", \"MOTIVATION\", \"EXTENDS\"], \"context\": [\"sent2\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "By analyzing PredPatt extractions in comparison with gold annotations (Sec. 2), we are able to refine and improve PredPatt's pattern set.",
                "From the auto-converted gold annotations, we create a held-out set by randomly sampling 10% sentences from EWT.",
                "We then update the existing PredPatt patterns and introduce new patterns by analyzing PredPatt annotations on the held-out set.",
                "PredPatt extracts predicates and arguments in four stages #TARGET_REF : (1) predicate and argument root identification, (2) argument resolution, (3) predicate and argument phrase extraction, and (4) optional post-processing.",
                "We analyze PredPatt extraction in each of these stages on the held-out set, and make 19 improvements to PredPatt patterns."
            ],
            "label": [
                "BACKGROUND",
                "EXTENDS"
            ]
        },
        "input": "sent0: By analyzing PredPatt extractions in comparison with gold annotations (Sec. 2), we are able to refine and improve PredPatt's pattern set.\n sent1: From the auto-converted gold annotations, we create a held-out set by randomly sampling 10% sentences from EWT.\n sent2: We then update the existing PredPatt patterns and introduce new patterns by analyzing PredPatt annotations on the held-out set.\n sent3: PredPatt extracts predicates and arguments in four stages #TARGET_REF : (1) predicate and argument root identification, (2) argument resolution, (3) predicate and argument phrase extraction, and (4) optional post-processing.\n sent4: We analyze PredPatt extraction in each of these stages on the held-out set, and make 19 improvements to PredPatt patterns.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"EXTENDS\"], \"context\": [\"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Neural machine translation (NMT) (#REF; #REF) is rapidly proving itself to be a strong competitor to other statistical machine translation methods.",
                "However, it still lags behind other statistical methods on very lowresource language pairs #TARGET_REF; #REF) .",
                "A common strategy to improve learning of lowresource languages is to use resources from related languages (#REF) .",
                "However, adapting these resources is not trivial.",
                "NMT offers some simple ways of doing this."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Neural machine translation (NMT) (#REF; #REF) is rapidly proving itself to be a strong competitor to other statistical machine translation methods.\n sent1: However, it still lags behind other statistical methods on very lowresource language pairs #TARGET_REF; #REF) .\n sent2: A common strategy to improve learning of lowresource languages is to use resources from related languages (#REF) .\n sent3: However, adapting these resources is not trivial.\n sent4: NMT offers some simple ways of doing this.\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "In particular, they showed that a French-English model could be used to improve translation on a wide range of low-resource language pairs such as Hausa-, Turkish-, and Uzbek-English.",
                "In this paper, we explore the opposite scenario, where the parent language pair is also lowresource, but related to the child language pair.",
                "We show that, at least in the case of three Turkic languages (Turkish, Uzbek, and Uyghur), the original method of #TARGET_REF does not always work, but it is still possible to use the parent model to considerably improve the child model.",
                "The basic idea is to exploit the relationship between the parent and child language lexicons.",
                "Zoph et al.'s original method makes no assumption about the relatedness of the parent and child languages, so it effectively makes a random assignment of the parent-language word embeddings to child-language words. But if we assume that the parent and child lexicons are related, it should be beneficial to transfer source word embeddings from parent-language words to their child-language equivalents."
            ],
            "label": [
                "MOTIVATION",
                "USE"
            ]
        },
        "input": "sent0: In particular, they showed that a French-English model could be used to improve translation on a wide range of low-resource language pairs such as Hausa-, Turkish-, and Uzbek-English.\n sent1: In this paper, we explore the opposite scenario, where the parent language pair is also lowresource, but related to the child language pair.\n sent2: We show that, at least in the case of three Turkic languages (Turkish, Uzbek, and Uyghur), the original method of #TARGET_REF does not always work, but it is still possible to use the parent model to considerably improve the child model.\n sent3: The basic idea is to exploit the relationship between the parent and child language lexicons.\n sent4: Zoph et al.'s original method makes no assumption about the relatedness of the parent and child languages, so it effectively makes a random assignment of the parent-language word embeddings to child-language words. But if we assume that the parent and child lexicons are related, it should be beneficial to transfer source word embeddings from parent-language words to their child-language equivalents.\n",
        "output": "{\"label\": [\"MOTIVATION\", \"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We follow the transfer learning approach proposed by #TARGET_REF .",
                "In their work, a parent model is first trained on a high-resource language pair.",
                "Then the child model's parameter values are copied from the parent's and are fine-tuned on its low-resource data.",
                "The source word embeddings are copied with the rest of the model, with the ith parent-language word embedding being assigned to the ith childlanguage word.",
                "Because the parent and child source languages have different vocabularies, this amounts to randomly assigning parent source word embeddings to child source words."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We follow the transfer learning approach proposed by #TARGET_REF .\n sent1: In their work, a parent model is first trained on a high-resource language pair.\n sent2: Then the child model's parameter values are copied from the parent's and are fine-tuned on its low-resource data.\n sent3: The source word embeddings are copied with the rest of the model, with the ith parent-language word embedding being assigned to the ith childlanguage word.\n sent4: Because the parent and child source languages have different vocabularies, this amounts to randomly assigning parent source word embeddings to child source words.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "The basic idea of our method is to extend the transfer method of #TARGET_REF to share the parent and child's source vocabularies, so that when source word embeddings are transferred, a word that appears in both vocabularies keeps its embedding.",
                "In order for this to work, it must be the case that the parent and child languages have considerable vocabulary overlap, and that when a word occurs in both languages, it often has a similar meaning in both languages.",
                "Thus, we need to process the data to make these two assumptions hold as much as possible."
            ],
            "label": [
                "USE",
                "EXTENDS"
            ]
        },
        "input": "sent0: The basic idea of our method is to extend the transfer method of #TARGET_REF to share the parent and child's source vocabularies, so that when source word embeddings are transferred, a word that appears in both vocabularies keeps its embedding.\n sent1: In order for this to work, it must be the case that the parent and child languages have considerable vocabulary overlap, and that when a word occurs in both languages, it often has a similar meaning in both languages.\n sent2: Thus, we need to process the data to make these two assumptions hold as much as possible.\n",
        "output": "{\"label\": [\"USE\", \"EXTENDS\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "We also optimized the vocabulary size and the number of BPE operations for the word-based and BPEbased systems, respectively, to maximize the tokenized BLEU on the development set.",
                "After translation at test time, we rejoined BPE segments, recased, and detokenized.",
                "Finally, we evaluated using case-sensitive BLEU.",
                "As a baseline, we trained a child model using BPE but without transfer (that is, with weights randomly initialized).",
                "We also compared against a word-based baseline (without transfer) and two word-based systems using transfer without vocabulary-sharing, corresponding with the method of #TARGET_REF ( §2.2): one where the target word embeddings are fine-tuned, and one where they are frozen."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We also optimized the vocabulary size and the number of BPE operations for the word-based and BPEbased systems, respectively, to maximize the tokenized BLEU on the development set.\n sent1: After translation at test time, we rejoined BPE segments, recased, and detokenized.\n sent2: Finally, we evaluated using case-sensitive BLEU.\n sent3: As a baseline, we trained a child model using BPE but without transfer (that is, with weights randomly initialized).\n sent4: We also compared against a word-based baseline (without transfer) and two word-based systems using transfer without vocabulary-sharing, corresponding with the method of #TARGET_REF ( §2.2): one where the target word embeddings are fine-tuned, and one where they are frozen.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "In this paper, we have shown that the transfer learning method of #TARGET_REF , while appealing, might not always work in a low-resource context.",
                "However, by combining it with BPE, we can improve NMT performance on a low-resource language pair by exploiting its lexical similarity with another related, low-resource language.",
                "Our results show consistent improvement in two Turkic languages.",
                "Our approach, which relies on segmenting words into subwords, seems well suited to agglutinative languages; further investigation would be needed to confirm whether our method works on other types of languages."
            ],
            "label": [
                "USE",
                "EXTENDS"
            ]
        },
        "input": "sent0: In this paper, we have shown that the transfer learning method of #TARGET_REF , while appealing, might not always work in a low-resource context.\n sent1: However, by combining it with BPE, we can improve NMT performance on a low-resource language pair by exploiting its lexical similarity with another related, low-resource language.\n sent2: Our results show consistent improvement in two Turkic languages.\n sent3: Our approach, which relies on segmenting words into subwords, seems well suited to agglutinative languages; further investigation would be needed to confirm whether our method works on other types of languages.\n",
        "output": "{\"label\": [\"USE\", \"EXTENDS\"], \"context\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "A common strategy to improve learning of lowresource languages is to use resources from related languages (#REF) .",
                "However, adapting these resources is not trivial.",
                "NMT offers some simple ways of doing this.",
                "For example, #TARGET_REF train a parent model on a (possibly unrelated) high-resource language pair, then use this model to initialize a child model which is further trained on a low-resource language pair.",
                "In particular, they showed that a French-English model could be used to improve translation on a wide range of low-resource language pairs such as Hausa-, Turkish-, and Uzbek-English."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: A common strategy to improve learning of lowresource languages is to use resources from related languages (#REF) .\n sent1: However, adapting these resources is not trivial.\n sent2: NMT offers some simple ways of doing this.\n sent3: For example, #TARGET_REF train a parent model on a (possibly unrelated) high-resource language pair, then use this model to initialize a child model which is further trained on a low-resource language pair.\n sent4: In particular, they showed that a French-English model could be used to improve translation on a wide range of low-resource language pairs such as Hausa-, Turkish-, and Uzbek-English.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF , contextual biasing has been used to assist recognition of foreign words.",
                "With the phoneme mapping from a foreign language phoneme set to the recognizer's phoneme set, foreign words are modeled as a phoneme-level contextual FST for biasing.",
                "It is unclear whether such an approach can be directly applied to E2E models.",
                "Phoneme-only E2E systems have been shown to have inferior performance compared to grapheme or wordpiece models (WPM) in general #TARGET_REF 17] , but shows better recognition of rare words and proper nouns.",
                "In this work we propose to incorporate phonemes to a wordpiece E2E model as modeling units and use phoneme-level FST for contextual biasing."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #REF , contextual biasing has been used to assist recognition of foreign words.\n sent1: With the phoneme mapping from a foreign language phoneme set to the recognizer's phoneme set, foreign words are modeled as a phoneme-level contextual FST for biasing.\n sent2: It is unclear whether such an approach can be directly applied to E2E models.\n sent3: Phoneme-only E2E systems have been shown to have inferior performance compared to grapheme or wordpiece models (WPM) in general #TARGET_REF 17] , but shows better recognition of rare words and proper nouns.\n sent4: In this work we propose to incorporate phonemes to a wordpiece E2E model as modeling units and use phoneme-level FST for contextual biasing.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Phoneme-only E2E systems have been shown to have inferior performance compared to grapheme or wordpiece models (WPM) in general [16, 17] , but shows better recognition of rare words and proper nouns.",
                "In this work we propose to incorporate phonemes to a wordpiece E2E model as modeling units and use phoneme-level FST for contextual biasing.",
                "We propose a word-frequency based sampling strategy to randomly tokenize rare words into phonemes in the target sequence using a lexicon.",
                "This approach also mitigates accuracy regressions that have been observed when using phoneme-only E2E models #TARGET_REF 17] .",
                "We train our model using only American English data and thus its wordpieces and phoneme set (no data from foreign languages)."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Phoneme-only E2E systems have been shown to have inferior performance compared to grapheme or wordpiece models (WPM) in general [16, 17] , but shows better recognition of rare words and proper nouns.\n sent1: In this work we propose to incorporate phonemes to a wordpiece E2E model as modeling units and use phoneme-level FST for contextual biasing.\n sent2: We propose a word-frequency based sampling strategy to randomly tokenize rare words into phonemes in the target sequence using a lexicon.\n sent3: This approach also mitigates accuracy regressions that have been observed when using phoneme-only E2E models #TARGET_REF 17] .\n sent4: We train our model using only American English data and thus its wordpieces and phoneme set (no data from foreign languages).\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The output of the model is a single softmax whose symbol set is the union of wordpiece and phoneme symbols.",
                "We use a pronunciation lexicon to obtain phoneme sequences of words.",
                "Since phonemes show strength in recognizing rare words #TARGET_REF , we want to present these words as phonemes more often.",
                "In a target sentence, we decide to randomly present the i th word as phonemes with a probability",
                ", 1.0) where p0 and T are constants and c(i) is an integer representing the number of time the word appears in our entire training corpus."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: The output of the model is a single softmax whose symbol set is the union of wordpiece and phoneme symbols.\n sent1: We use a pronunciation lexicon to obtain phoneme sequences of words.\n sent2: Since phonemes show strength in recognizing rare words #TARGET_REF , we want to present these words as phonemes more often.\n sent3: In a target sentence, we decide to randomly present the i th word as phonemes with a probability\n sent4: , 1.0) where p0 and T are constants and c(i) is an integer representing the number of time the word appears in our entire training corpus.\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                ", 1.0) where p0 and T are constants and c(i) is an integer representing the number of time the word appears in our entire training corpus.",
                "Therefore, the words that appear T times or less will be presented as phonemes with probability p0.",
                "For words that appear more than T times, the more frequent they are, the less likely they are presented as phonemes 2 .",
                "Note that the decision of whether to use wordpieces or phonemes is made randomly at each gradient iteration, and thus a given sentence could have different target sequences at different epochs.",
                "We use context-independent phonemes as in #TARGET_REF ."
            ],
            "label": [
                "USE",
                "SIMILARITY"
            ]
        },
        "input": "sent0: , 1.0) where p0 and T are constants and c(i) is an integer representing the number of time the word appears in our entire training corpus.\n sent1: Therefore, the words that appear T times or less will be presented as phonemes with probability p0.\n sent2: For words that appear more than T times, the more frequent they are, the less likely they are presented as phonemes 2 .\n sent3: Note that the decision of whether to use wordpieces or phonemes is made randomly at each gradient iteration, and thus a given sentence could have different target sequences at different epochs.\n sent4: We use context-independent phonemes as in #TARGET_REF .\n",
        "output": "{\"label\": [\"USE\", \"SIMILARITY\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "To generate words as outputs, we search through a decoding graph similar to #TARGET_REF but accept both phonemes and wordpieces.",
                "An example is shown in Figure 2 .",
                "The decoding FST has wordpiece loops around state 0 (we show only a few for simplicity), but also has a pronunciation section (states 1 through 14) .",
                "The pronunciation section is a prefix tree with phonemes as inputs, and outputs are wordpieces of the corresponding word produced by the WPM in Section 3.1.",
                "Specifically, for each word in the biasing list, we look up pronunciations from the lexicon and split the word into its constituent wordpieces."
            ],
            "label": [
                "SIMILARITY",
                "DIFFERENCES"
            ]
        },
        "input": "sent0: To generate words as outputs, we search through a decoding graph similar to #TARGET_REF but accept both phonemes and wordpieces.\n sent1: An example is shown in Figure 2 .\n sent2: The decoding FST has wordpiece loops around state 0 (we show only a few for simplicity), but also has a pronunciation section (states 1 through 14) .\n sent3: The pronunciation section is a prefix tree with phonemes as inputs, and outputs are wordpieces of the corresponding word produced by the WPM in Section 3.1.\n sent4: Specifically, for each word in the biasing list, we look up pronunciations from the lexicon and split the word into its constituent wordpieces.\n",
        "output": "{\"label\": [\"SIMILARITY\", \"DIFFERENCES\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Secondly, we see in Table 1 that all models performs substantially better with biasing.",
                "The WER reductions range from 9%-23% relatively for different models when compared to the no-bias case.",
                "Comparing different biasing strategies, we find that the wordpiece-phoneme model performs the best: 16% relatively better than the grapheme model, and 8.3% better than the wordpiece model.",
                "We attribute the superior per- formance of the wordpiece-phoneme model to the robustness of phonemes to OOV words, as observed in #TARGET_REF .",
                "Since the wordpiece-phoneme model contains both wordpieces and phonemes as modeling units, we can further perform wordpiece biasing in addition to phoneme-based biasing by building a wordpiece FST in parallel to the phoneme FST."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: Secondly, we see in Table 1 that all models performs substantially better with biasing.\n sent1: The WER reductions range from 9%-23% relatively for different models when compared to the no-bias case.\n sent2: Comparing different biasing strategies, we find that the wordpiece-phoneme model performs the best: 16% relatively better than the grapheme model, and 8.3% better than the wordpiece model.\n sent3: We attribute the superior per- formance of the wordpiece-phoneme model to the robustness of phonemes to OOV words, as observed in #TARGET_REF .\n sent4: Since the wordpiece-phoneme model contains both wordpieces and phonemes as modeling units, we can further perform wordpiece biasing in addition to phoneme-based biasing by building a wordpiece FST in parallel to the phoneme FST.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The wordpiecephoneme model performs a little better than the grapheme model, and we attribute that to the higher frequency of wordpieces during training.",
                "Compared to the wordpiece model, the wordpiece-phoneme model has a slight degradation (0.1% absolute WER).",
                "This is due to the introduction of phonemes in modeling.",
                "One potential approach to improve regression is to incorporate an English external language model for phonemes in rescoring, similarly to the wordpiece-based rescoring in [10] .",
                "However, we note that the regression is significantly smaller than the all-phoneme model in #TARGET_REF ."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: The wordpiecephoneme model performs a little better than the grapheme model, and we attribute that to the higher frequency of wordpieces during training.\n sent1: Compared to the wordpiece model, the wordpiece-phoneme model has a slight degradation (0.1% absolute WER).\n sent2: This is due to the introduction of phonemes in modeling.\n sent3: One potential approach to improve regression is to incorporate an English external language model for phonemes in rescoring, similarly to the wordpiece-based rescoring in [10] .\n sent4: However, we note that the regression is significantly smaller than the all-phoneme model in #TARGET_REF .\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Figure 2 : Decoding graph for the words \"crèche\" (daycare) with English cross lingual pronunciation \"k r\\ E S\" and \"créteil\" (a city) with pronunciation \"k r\\ E t E j\".",
                "For clarity, we omitted most wordpieces for the state 0.",
                "Based on #TARGET_REF , we add two improvements to the decoding strategy.",
                "First, during decoding we consume as many input epsilon arcs as possible thus guaranteeing that all wordpieces in word are produced when all corresponding phonemes are seen in the input.",
                "Second, we merge paths that have the same output symbols."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: Figure 2 : Decoding graph for the words \"crèche\" (daycare) with English cross lingual pronunciation \"k r\\ E S\" and \"créteil\" (a city) with pronunciation \"k r\\ E t E j\".\n sent1: For clarity, we omitted most wordpieces for the state 0.\n sent2: Based on #TARGET_REF , we add two improvements to the decoding strategy.\n sent3: First, during decoding we consume as many input epsilon arcs as possible thus guaranteeing that all wordpieces in word are produced when all corresponding phonemes are seen in the input.\n sent4: Second, we merge paths that have the same output symbols.\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The \"distribution function\" in discrete or continuous latent variable model for natural language may not be properly decomposed or estimated in model inference.",
                "This tutorial addresses the fundamentals of statistical models and neural networks, and focus on a series of advanced Bayesian models and deep models including hierarchical Dirichlet process , Chinese restaurant process (#REF) , hierarchical Pitman-Yor process (#REF) , Indian buffet process (#REF) , recurrent neural network (#REF; Van Den #REF) , long short-term memory (#REF; , sequence-to-sequence model (#REF), variational auto-encoder (#REF) , generative adversarial network (#REF) , attention mechanism (#REF; #TARGET_REF , memory-augmented neural network (#REF; #REF) , stochastic neural network #REF) , predictive state neural network (#REF) , policy gradient (#REF) and reinforcement learning (#REF) .",
                "We present how these models are connected and why they work for a variety of applications on symbolic and complex patterns in natural language.",
                "The variational inference and sampling method are formulated to tackle the optimization for complicated models (#REF) .",
                "The word and sentence embeddings, clustering and co-clustering are merged with linguistic and semantic constraints."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The \"distribution function\" in discrete or continuous latent variable model for natural language may not be properly decomposed or estimated in model inference.\n sent1: This tutorial addresses the fundamentals of statistical models and neural networks, and focus on a series of advanced Bayesian models and deep models including hierarchical Dirichlet process , Chinese restaurant process (#REF) , hierarchical Pitman-Yor process (#REF) , Indian buffet process (#REF) , recurrent neural network (#REF; Van Den #REF) , long short-term memory (#REF; , sequence-to-sequence model (#REF), variational auto-encoder (#REF) , generative adversarial network (#REF) , attention mechanism (#REF; #TARGET_REF , memory-augmented neural network (#REF; #REF) , stochastic neural network #REF) , predictive state neural network (#REF) , policy gradient (#REF) and reinforcement learning (#REF) .\n sent2: We present how these models are connected and why they work for a variety of applications on symbolic and complex patterns in natural language.\n sent3: The variational inference and sampling method are formulated to tackle the optimization for complicated models (#REF) .\n sent4: The word and sentence embeddings, clustering and co-clustering are merged with linguistic and semantic constraints.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The task both detects trigger tokens and classifies them to appropriate event types.",
                "While this task is often a component within the broader event extraction task, labeling both triggers and arguments, this paper focuses on trigger labeling.",
                "Most state-of-the-art event trigger labeling approaches (#REF; #REFb; #REF; #TARGET_REF follow the standard supervised learning paradigm.",
                "For each event type, experts first write annotation guidelines.",
                "Then, annotators follow them to label event triggers in a large dataset."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The task both detects trigger tokens and classifies them to appropriate event types.\n sent1: While this task is often a component within the broader event extraction task, labeling both triggers and arguments, this paper focuses on trigger labeling.\n sent2: Most state-of-the-art event trigger labeling approaches (#REF; #REFb; #REF; #TARGET_REF follow the standard supervised learning paradigm.\n sent3: For each event type, experts first write annotation guidelines.\n sent4: Then, annotators follow them to label event triggers in a large dataset.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "In our method, such similarity indicators are encoded as a small set of event-independent classification features, based on lexical matches and external resources like WordNet.",
                "Using eventindependent features allows us to train the system only once, at system setup phase, requiring annotated triggers in a training set for just a few preselected event types.",
                "Then, whenever a new event type is introduced for labeling, we only need to collect a seed list for it from its description, and provide it as input to the system.",
                "We developed a seed-based system (Section 3), based on a state-of-the-art fully-supervised event extraction system #TARGET_REF .",
                "When evaluated on the ACE-2005 dataset, 1 our system outperforms the fully-supervised one (Section 4), even though we don't utilize any annotated triggers of the test events during the labeling phase, and only Figure 1 : Flow of the seed-based approach use the seed triggers appearing in the ACE annotation guidelines."
            ],
            "label": [
                "DIFFERENCES",
                "EXTENDS"
            ]
        },
        "input": "sent0: In our method, such similarity indicators are encoded as a small set of event-independent classification features, based on lexical matches and external resources like WordNet.\n sent1: Using eventindependent features allows us to train the system only once, at system setup phase, requiring annotated triggers in a training set for just a few preselected event types.\n sent2: Then, whenever a new event type is introduced for labeling, we only need to collect a seed list for it from its description, and provide it as input to the system.\n sent3: We developed a seed-based system (Section 3), based on a state-of-the-art fully-supervised event extraction system #TARGET_REF .\n sent4: When evaluated on the ACE-2005 dataset, 1 our system outperforms the fully-supervised one (Section 4), even though we don't utilize any annotated triggers of the test events during the labeling phase, and only Figure 1 : Flow of the seed-based approach use the seed triggers appearing in the ACE annotation guidelines.\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"EXTENDS\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "This section describes the method we designed to implement the seed-based approach.",
                "To assess our approach, we compare it (Section 4) with the common fully-supervised approach, which requires annotated triggers for each target event type.",
                "Therefore, we implemented our system by adapting the state-of-the-art fully-supervised event extraction system of #TARGET_REF , modifying mechanisms relevant for features and for trigger labels, as described below.",
                "Hence the systems are comparable with respect to using the same preprocessing and machine learning infrastructure."
            ],
            "label": [
                "DIFFERENCES",
                "EXTENDS"
            ]
        },
        "input": "sent0: This section describes the method we designed to implement the seed-based approach.\n sent1: To assess our approach, we compare it (Section 4) with the common fully-supervised approach, which requires annotated triggers for each target event type.\n sent2: Therefore, we implemented our system by adapting the state-of-the-art fully-supervised event extraction system of #TARGET_REF , modifying mechanisms relevant for features and for trigger labels, as described below.\n sent3: Hence the systems are comparable with respect to using the same preprocessing and machine learning infrastructure.\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"EXTENDS\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "To implement the seed-based approach for trigger labeling, we adapt only the trigger classification part in the #TARGET_REF fully-supervised system, ignoring arguments.",
                "Given a set of new target event types T we classify every test sentence once for each event type t ∈ T .",
                "Hence, when classifying a sentence for t, the labeling of each token x i is binary, where y i ∈ { , ⊥} marks whether x i is a trigger of type t ( ) or not (⊥).",
                "For instance x i =\"visited\" labeled as when classifying for t=Meet, means x i is labeled as a Meet trigger.",
                "To score the binary label assignment (x i , y i ), we use a small set of features that assess the similarity between x i and t's given seed list."
            ],
            "label": [
                "DIFFERENCES",
                "EXTENDS"
            ]
        },
        "input": "sent0: To implement the seed-based approach for trigger labeling, we adapt only the trigger classification part in the #TARGET_REF fully-supervised system, ignoring arguments.\n sent1: Given a set of new target event types T we classify every test sentence once for each event type t ∈ T .\n sent2: Hence, when classifying a sentence for t, the labeling of each token x i is binary, where y i ∈ { , ⊥} marks whether x i is a trigger of type t ( ) or not (⊥).\n sent3: For instance x i =\"visited\" labeled as when classifying for t=Meet, means x i is labeled as a Meet trigger.\n sent4: To score the binary label assignment (x i , y i ), we use a small set of features that assess the similarity between x i and t's given seed list.\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"EXTENDS\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "We evaluate our seed-based approach (Section 2) in comparison to the fully-supervised approach implemented by #REF (Section 3) .",
                "To maintain comparability, we use the ACE-2005 documents with the same split as in (#REF; #REFb; #REF) to 40 test documents and 559 training documents.",
                "However, some evaluation settings differ: #TARGET_REF train a multi-class model for all 33 ACE-2005 event types, and classify all tokens in the test documents into these event types.",
                "Our approach, on the other hand, trains an eventindependent binary classifier, while testing on new event types that are different from those utilized for training.",
                "We next describe how this setup is addressed in our evaluation."
            ],
            "label": [
                "DIFFERENCES",
                "EXTENDS"
            ]
        },
        "input": "sent0: We evaluate our seed-based approach (Section 2) in comparison to the fully-supervised approach implemented by #REF (Section 3) .\n sent1: To maintain comparability, we use the ACE-2005 documents with the same split as in (#REF; #REFb; #REF) to 40 test documents and 559 training documents.\n sent2: However, some evaluation settings differ: #TARGET_REF train a multi-class model for all 33 ACE-2005 event types, and classify all tokens in the test documents into these event types.\n sent3: Our approach, on the other hand, trains an eventindependent binary classifier, while testing on new event types that are different from those utilized for training.\n sent4: We next describe how this setup is addressed in our evaluation.\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"EXTENDS\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The event extraction system of #TARGET_REF labels triggers and their arguments for a set of target event types L, for which annotated training documents are provided.",
                "The system utilizes a structured perceptron with beam search (#REF; .",
                "To label triggers, the system scans each sentence x, and creates candidate assignments y, that for each token x i assign each possible label y i ∈ L ∪ {⊥} (⊥ meaning x i is not a trigger at all).",
                "The score of an assignment (x i , y i ) is calculated as w · f , where f is the binary feature vector calculated for (x i , y i ), and w is the learned feature weight vector.",
                "The classifier's features capture various properties of x i and its context, such as its unigram and its containing bigrams."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: The event extraction system of #TARGET_REF labels triggers and their arguments for a set of target event types L, for which annotated training documents are provided.\n sent1: The system utilizes a structured perceptron with beam search (#REF; .\n sent2: To label triggers, the system scans each sentence x, and creates candidate assignments y, that for each token x i assign each possible label y i ∈ L ∪ {⊥} (⊥ meaning x i is not a trigger at all).\n sent3: The score of an assignment (x i , y i ) is calculated as w · f , where f is the binary feature vector calculated for (x i , y i ), and w is the learned feature weight vector.\n sent4: The classifier's features capture various properties of x i and its context, such as its unigram and its containing bigrams.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "We evaluate our seed-based approach (Section 2) in comparison to the fully-supervised approach implemented by #TARGET_REF (Section 3) .",
                "To maintain comparability, we use the ACE-2005 documents with the same split as in (#REF; #REFb; #REF) to 40 test documents and 559 training documents.",
                "However, some evaluation settings differ: #REF train a multi-class model for all 33 ACE-2005 event types, and classify all tokens in the test documents into these event types.",
                "Our approach, on the other hand, trains an eventindependent binary classifier, while testing on new event types that are different from those utilized for training.",
                "We next describe how this setup is addressed in our evaluation."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: We evaluate our seed-based approach (Section 2) in comparison to the fully-supervised approach implemented by #TARGET_REF (Section 3) .\n sent1: To maintain comparability, we use the ACE-2005 documents with the same split as in (#REF; #REFb; #REF) to 40 test documents and 559 training documents.\n sent2: However, some evaluation settings differ: #REF train a multi-class model for all 33 ACE-2005 event types, and classify all tokens in the test documents into these event types.\n sent3: Our approach, on the other hand, trains an eventindependent binary classifier, while testing on new event types that are different from those utilized for training.\n sent4: We next describe how this setup is addressed in our evaluation.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "We evaluate our seed-based approach (Section 2) in comparison to the fully-supervised approach implemented by #REF (Section 3) .",
                "To maintain comparability, we use the ACE-2005 documents with the same split as in (#REF; #REFb; #TARGET_REF to 40 test documents and 559 training documents.",
                "However, some evaluation settings differ: #REF train a multi-class model for all 33 ACE-2005 event types, and classify all tokens in the test documents into these event types.",
                "Our approach, on the other hand, trains an eventindependent binary classifier, while testing on new event types that are different from those utilized for training.",
                "We next describe how this setup is addressed in our evaluation."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: We evaluate our seed-based approach (Section 2) in comparison to the fully-supervised approach implemented by #REF (Section 3) .\n sent1: To maintain comparability, we use the ACE-2005 documents with the same split as in (#REF; #REFb; #TARGET_REF to 40 test documents and 559 training documents.\n sent2: However, some evaluation settings differ: #REF train a multi-class model for all 33 ACE-2005 event types, and classify all tokens in the test documents into these event types.\n sent3: Our approach, on the other hand, trains an eventindependent binary classifier, while testing on new event types that are different from those utilized for training.\n sent4: We next describe how this setup is addressed in our evaluation.\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The very low variance indicates that the system's performance does not depend much on the choice of training event types.",
                "We compare our system's performance to the published trigger classification results of the baseline system of #TARGET_REF ) (its globally optimized run, when labeling both triggers and arguments).",
                "We also compare to the sentence-level system in (#REF) which uses the same dataset split.",
                "Our system outperforms the fully-supervised baseline by 5.7% F 1 , which is statistically significant (two-tailed Wilcoxon test, p < 0.05).",
                "This shows that there is no performance hit for the seed-based method on this dataset, even though it does not require any annotated data for new tested events, thus saving costly annotation efforts."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: The very low variance indicates that the system's performance does not depend much on the choice of training event types.\n sent1: We compare our system's performance to the published trigger classification results of the baseline system of #TARGET_REF ) (its globally optimized run, when labeling both triggers and arguments).\n sent2: We also compare to the sentence-level system in (#REF) which uses the same dataset split.\n sent3: Our system outperforms the fully-supervised baseline by 5.7% F 1 , which is statistically significant (two-tailed Wilcoxon test, p < 0.05).\n sent4: This shows that there is no performance hit for the seed-based method on this dataset, even though it does not require any annotated data for new tested events, thus saving costly annotation efforts.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Representing sentences as numerical vectors while capturing their semantic context is an important and useful intermediate step in natural language processing.",
                "Representations that are both general and discriminative can serve as a tool for tackling various NLP tasks.",
                "While common sentence representation methods are unsupervised in nature, recently, an approach for learning universal sentence representation in a supervised setting was presented in #TARGET_REF .",
                "We argue that although promising results were obtained, an improvement can be reached by adding various unsupervised constraints that are motivated by auto-encoders and by language models.",
                "We show that by adding such constraints, superior sentence embeddings can be achieved."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Representing sentences as numerical vectors while capturing their semantic context is an important and useful intermediate step in natural language processing.\n sent1: Representations that are both general and discriminative can serve as a tool for tackling various NLP tasks.\n sent2: While common sentence representation methods are unsupervised in nature, recently, an approach for learning universal sentence representation in a supervised setting was presented in #TARGET_REF .\n sent3: We argue that although promising results were obtained, an improvement can be reached by adding various unsupervised constraints that are motivated by auto-encoders and by language models.\n sent4: We show that by adding such constraints, superior sentence embeddings can be achieved.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "FastSent (#REF) learns to predicts a Bag-Of-Word (BOW) representation of adjacent sentences given a BOW representation of some sentence.",
                "In (#REF) a Hybrid Gaussian Laplacian density function is fitted to the sentence to derive Fisher Vectors.",
                "While previous methods train sentence embeddings in an unsupervised manner, a recent work #TARGET_REF argued that better representations can be achieved via supervised training on a general sentence inference dataset (#REF) .",
                "To this end, the authors use the Stanford Natural Language Inference (SNLI) dataset (#REF) to train different Table 1 : Sentence embedding results.",
                "BiLSTM refers to the original BiLSTM followed by MaxPooling implementation of (#REF) which is the baseline for our work."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: FastSent (#REF) learns to predicts a Bag-Of-Word (BOW) representation of adjacent sentences given a BOW representation of some sentence.\n sent1: In (#REF) a Hybrid Gaussian Laplacian density function is fitted to the sentence to derive Fisher Vectors.\n sent2: While previous methods train sentence embeddings in an unsupervised manner, a recent work #TARGET_REF argued that better representations can be achieved via supervised training on a general sentence inference dataset (#REF) .\n sent3: To this end, the authors use the Stanford Natural Language Inference (SNLI) dataset (#REF) to train different Table 1 : Sentence embedding results.\n sent4: BiLSTM refers to the original BiLSTM followed by MaxPooling implementation of (#REF) which is the baseline for our work.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "FastSent (#REF) learns to predicts a Bag-Of-Word (BOW) representation of adjacent sentences given a BOW representation of some sentence.",
                "In (#REF) a Hybrid Gaussian Laplacian density function is fitted to the sentence to derive Fisher Vectors.",
                "While previous methods train sentence embeddings in an unsupervised manner, a recent work (#REF) argued that better representations can be achieved via supervised training on a general sentence inference dataset (#REF) .",
                "To this end, the authors use the Stanford Natural Language Inference (SNLI) dataset (#REF) to train different Table 1 : Sentence embedding results.",
                "BiLSTM refers to the original BiLSTM followed by MaxPooling implementation of #TARGET_REF which is the baseline for our work."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: FastSent (#REF) learns to predicts a Bag-Of-Word (BOW) representation of adjacent sentences given a BOW representation of some sentence.\n sent1: In (#REF) a Hybrid Gaussian Laplacian density function is fitted to the sentence to derive Fisher Vectors.\n sent2: While previous methods train sentence embeddings in an unsupervised manner, a recent work (#REF) argued that better representations can be achieved via supervised training on a general sentence inference dataset (#REF) .\n sent3: To this end, the authors use the Stanford Natural Language Inference (SNLI) dataset (#REF) to train different Table 1 : Sentence embedding results.\n sent4: BiLSTM refers to the original BiLSTM followed by MaxPooling implementation of #TARGET_REF which is the baseline for our work.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "More concretely, given a sequence of T words, {w t } t=1,...,T with given word embedding (#REF; #REF) {v t } t=1,...,T ,a bidirectional LSTM computes a set of T vectors {h t } t=1,...,T where each h t is the concatenation of a forward LSTM and a backward LSTM that read the sentences in two opposite directions.",
                "We denote { − → h t } and { ← − h t } as the hidden states of the left and right LSTM's respectively, where t = 1, . . . , T .",
                "The final sentence representation is obtained by taking the maximal value of each dimension of the {h t } hidden units (i.e.: max pooling).",
                "The original model of #TARGET_REF was trained on the SNLI dataset in a supervised fashion -given pairs of sentences s 1 and s 2 , denote their representation bys 1 and s 2 .",
                "During training, the concatenation ofs 1 ,s 2 , |s 1 −s 2 | ands 1 * s 2 is fed to a three layer fully connected network followed by a softmax classifier."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: More concretely, given a sequence of T words, {w t } t=1,...,T with given word embedding (#REF; #REF) {v t } t=1,...,T ,a bidirectional LSTM computes a set of T vectors {h t } t=1,...,T where each h t is the concatenation of a forward LSTM and a backward LSTM that read the sentences in two opposite directions.\n sent1: We denote { − → h t } and { ← − h t } as the hidden states of the left and right LSTM's respectively, where t = 1, . . . , T .\n sent2: The final sentence representation is obtained by taking the maximal value of each dimension of the {h t } hidden units (i.e.: max pooling).\n sent3: The original model of #TARGET_REF was trained on the SNLI dataset in a supervised fashion -given pairs of sentences s 1 and s 2 , denote their representation bys 1 and s 2 .\n sent4: During training, the concatenation ofs 1 ,s 2 , |s 1 −s 2 | ands 1 * s 2 is fed to a three layer fully connected network followed by a softmax classifier.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Following #TARGET_REF we have tested our approach on a wide array of classification tasks, including sentiment analysis (MR -#REF , SST -#REF ), question-type (TREC -#REF ), product reviews (CR - #REF ), subjectivity/objectivity (SUBJ - #REF ) and opinion polarity (MPQA -#REF ).",
                "We also tested our approach on semantic textual similarity (STS 14 - #REF ), paraphrase detection (MRPC - #REF ), entailment and semantic relatedness tasks (SICK-R and SICK-E - #REF ), though those tasks are more close in nature to the task of the SNLI dataset which the model was trained on.",
                "In our experiments we have set λ from eq. (1) and eq. (2) to be 1 and λ 1 , λ 2 from eq. (3) and eq. (4) to be 0.5.",
                "All other hyper-parameters and implementation details were left unchanged to provide a fair comparison to the baseline method of (#REF) .",
                "Our results are summarized in table 1."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Following #TARGET_REF we have tested our approach on a wide array of classification tasks, including sentiment analysis (MR -#REF , SST -#REF ), question-type (TREC -#REF ), product reviews (CR - #REF ), subjectivity/objectivity (SUBJ - #REF ) and opinion polarity (MPQA -#REF ).\n sent1: We also tested our approach on semantic textual similarity (STS 14 - #REF ), paraphrase detection (MRPC - #REF ), entailment and semantic relatedness tasks (SICK-R and SICK-E - #REF ), though those tasks are more close in nature to the task of the SNLI dataset which the model was trained on.\n sent2: In our experiments we have set λ from eq. (1) and eq. (2) to be 1 and λ 1 , λ 2 from eq. (3) and eq. (4) to be 0.5.\n sent3: All other hyper-parameters and implementation details were left unchanged to provide a fair comparison to the baseline method of (#REF) .\n sent4: Our results are summarized in table 1.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Following that, (#REF) proposed a dropout augmented LSTM.",
                "We note that there exists a connection between those two problems and try to model it more explicitly.",
                "Recently, the incorporation of the hidden states of neural language models in downstream supervised-learning models have been shown to improve the results of the latter (e.g. ElMo -#REF, CoVe -#REF Peters et al. (2017 , #REF ) -in this work we jointly train the unsupervised and supervised tasks.",
                "To this end, we incorporate unsupervised regularization terms motivated by language modeling and auto-encoders in the training framework proposed by #TARGET_REF .",
                "We test our proposed model on a set of NLP tasks and show improved results over the baseline framework of (#REF) ."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: Following that, (#REF) proposed a dropout augmented LSTM.\n sent1: We note that there exists a connection between those two problems and try to model it more explicitly.\n sent2: Recently, the incorporation of the hidden states of neural language models in downstream supervised-learning models have been shown to improve the results of the latter (e.g. ElMo -#REF, CoVe -#REF Peters et al. (2017 , #REF ) -in this work we jointly train the unsupervised and supervised tasks.\n sent3: To this end, we incorporate unsupervised regularization terms motivated by language modeling and auto-encoders in the training framework proposed by #TARGET_REF .\n sent4: We test our proposed model on a set of NLP tasks and show improved results over the baseline framework of (#REF) .\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Our approach builds upon the previous work of #TARGET_REF .",
                "Specifically, we use their BiLSTM model with max pooling.",
                "More concretely, given a sequence of T words, {w t } t=1,...,T with given word embedding (#REF; #REF) {v t } t=1,...,T ,a bidirectional LSTM computes a set of T vectors {h t } t=1,...,T where each h t is the concatenation of a forward LSTM and a backward LSTM that read the sentences in two opposite directions.",
                "We denote { − → h t } and { ← − h t } as the hidden states of the left and right LSTM's respectively, where t = 1, . . . , T .",
                "The final sentence representation is obtained by taking the maximal value of each dimension of the {h t } hidden units (i.e.: max pooling)."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: Our approach builds upon the previous work of #TARGET_REF .\n sent1: Specifically, we use their BiLSTM model with max pooling.\n sent2: More concretely, given a sequence of T words, {w t } t=1,...,T with given word embedding (#REF; #REF) {v t } t=1,...,T ,a bidirectional LSTM computes a set of T vectors {h t } t=1,...,T where each h t is the concatenation of a forward LSTM and a backward LSTM that read the sentences in two opposite directions.\n sent3: We denote { − → h t } and { ← − h t } as the hidden states of the left and right LSTM's respectively, where t = 1, . . . , T .\n sent4: The final sentence representation is obtained by taking the maximal value of each dimension of the {h t } hidden units (i.e.: max pooling).\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Following that, (#REF) proposed a dropout augmented LSTM.",
                "We note that there exists a connection between those two problems and try to model it more explicitly.",
                "Recently, the incorporation of the hidden states of neural language models in downstream supervised-learning models have been shown to improve the results of the latter (e.g. ElMo -#REF, CoVe -#REF Peters et al. (2017 , #REF ) -in this work we jointly train the unsupervised and supervised tasks.",
                "To this end, we incorporate unsupervised regularization terms motivated by language modeling and auto-encoders in the training framework proposed by (#REF) .",
                "We test our proposed model on a set of NLP tasks and show improved results over the baseline framework of #TARGET_REF ."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Following that, (#REF) proposed a dropout augmented LSTM.\n sent1: We note that there exists a connection between those two problems and try to model it more explicitly.\n sent2: Recently, the incorporation of the hidden states of neural language models in downstream supervised-learning models have been shown to improve the results of the latter (e.g. ElMo -#REF, CoVe -#REF Peters et al. (2017 , #REF ) -in this work we jointly train the unsupervised and supervised tasks.\n sent3: To this end, we incorporate unsupervised regularization terms motivated by language modeling and auto-encoders in the training framework proposed by (#REF) .\n sent4: We test our proposed model on a set of NLP tasks and show improved results over the baseline framework of #TARGET_REF .\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "We also tested our approach on semantic textual similarity (STS 14 - #REF ), paraphrase detection (MRPC - #REF ), entailment and semantic relatedness tasks (SICK-R and SICK-E - #REF ), though those tasks are more close in nature to the task of the SNLI dataset which the model was trained on.",
                "In our experiments we have set λ from eq. (1) and eq. (2) to be 1 and λ 1 , λ 2 from eq. (3) and eq. (4) to be 0.5.",
                "All other hyper-parameters and implementation details were left unchanged to provide a fair comparison to the baseline method of #TARGET_REF .",
                "Our results are summarized in table 1.",
                "We compared out method against the baseline BiL-STM implementation of (#REF) and included FastSent (#REF) and SkipThought vectors (#REF) as a reference."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: We also tested our approach on semantic textual similarity (STS 14 - #REF ), paraphrase detection (MRPC - #REF ), entailment and semantic relatedness tasks (SICK-R and SICK-E - #REF ), though those tasks are more close in nature to the task of the SNLI dataset which the model was trained on.\n sent1: In our experiments we have set λ from eq. (1) and eq. (2) to be 1 and λ 1 , λ 2 from eq. (3) and eq. (4) to be 0.5.\n sent2: All other hyper-parameters and implementation details were left unchanged to provide a fair comparison to the baseline method of #TARGET_REF .\n sent3: Our results are summarized in table 1.\n sent4: We compared out method against the baseline BiL-STM implementation of (#REF) and included FastSent (#REF) and SkipThought vectors (#REF) as a reference.\n",
        "output": "{\"label\": [null], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "In our experiments we have set λ from eq. (1) and eq. (2) to be 1 and λ 1 , λ 2 from eq. (3) and eq. (4) to be 0.5.",
                "All other hyper-parameters and implementation details were left unchanged to provide a fair comparison to the baseline method of (#REF) .",
                "Our results are summarized in table 1.",
                "We compared out method against the baseline BiL-STM implementation of #TARGET_REF and included FastSent (#REF) and SkipThought vectors (#REF) as a reference.",
                "As evident from table 1 in almost all the tasks evaluated, adding the proposed regularization terms improves performance."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: In our experiments we have set λ from eq. (1) and eq. (2) to be 1 and λ 1 , λ 2 from eq. (3) and eq. (4) to be 0.5.\n sent1: All other hyper-parameters and implementation details were left unchanged to provide a fair comparison to the baseline method of (#REF) .\n sent2: Our results are summarized in table 1.\n sent3: We compared out method against the baseline BiL-STM implementation of #TARGET_REF and included FastSent (#REF) and SkipThought vectors (#REF) as a reference.\n sent4: As evident from table 1 in almost all the tasks evaluated, adding the proposed regularization terms improves performance.\n",
        "output": "{\"label\": [null], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "In our work, we have sought to connect unsupervised and supervised learning in the context of sentence embeddings.",
                "Leveraging supervision given by some general task aided in obtaining state-of-the-art sentence representations #TARGET_REF .",
                "However, every supervised learning tasks is prone to overfit.",
                "In this context, overfitting to the learning task will result in a model which generalizes less well to new tasks.",
                "We alleviate this problem by incorporating unsupervised regularization criteria in the model's loss function which are motivated by autoencoders and language models."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: In our work, we have sought to connect unsupervised and supervised learning in the context of sentence embeddings.\n sent1: Leveraging supervision given by some general task aided in obtaining state-of-the-art sentence representations #TARGET_REF .\n sent2: However, every supervised learning tasks is prone to overfit.\n sent3: In this context, overfitting to the learning task will result in a model which generalizes less well to new tasks.\n sent4: We alleviate this problem by incorporating unsupervised regularization criteria in the model's loss function which are motivated by autoencoders and language models.\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "For English (but not exclusively), in approaches based on supervised learning, sequence labelling methods are often used, especially Conditional Random #REF .",
                "A review of the methods in the article #TARGET_REF about the recognition of timexes for English and Spanish has shown a certain shift within the most popular solutions.",
                "As with the normalisation of timexes, the best results are still achieved with rule-based methods, many new solutions have been introduced in the area of recognition.",
                "The best systems listed in [35] , called TIP#REF and ClearTK [1] , use CRFs for recognition, so initially, we decided to apply the CRF-based approach for this task.",
                "The results were described in [12, 10] ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: For English (but not exclusively), in approaches based on supervised learning, sequence labelling methods are often used, especially Conditional Random #REF .\n sent1: A review of the methods in the article #TARGET_REF about the recognition of timexes for English and Spanish has shown a certain shift within the most popular solutions.\n sent2: As with the normalisation of timexes, the best results are still achieved with rule-based methods, many new solutions have been introduced in the area of recognition.\n sent3: The best systems listed in [35] , called TIP#REF and ClearTK [1] , use CRFs for recognition, so initially, we decided to apply the CRF-based approach for this task.\n sent4: The results were described in [12, 10] .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "A review of the methods in the article [35] about the recognition of timexes for English and Spanish has shown a certain shift within the most popular solutions.",
                "As with the normalisation of timexes, the best results are still achieved with rule-based methods, many new solutions have been introduced in the area of recognition.",
                "The best systems listed in #TARGET_REF , called TIP#REF and ClearTK [1] , use CRFs for recognition, so initially, we decided to apply the CRF-based approach for this task.",
                "The results were described in [12, 10] .",
                "In recent years, solutions based on deep neural networks, using word representation in the form of word embeddings, created with the use of large linguistic corpus, have begun to dominate in the field of recognition of word expressions."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: A review of the methods in the article [35] about the recognition of timexes for English and Spanish has shown a certain shift within the most popular solutions.\n sent1: As with the normalisation of timexes, the best results are still achieved with rule-based methods, many new solutions have been introduced in the area of recognition.\n sent2: The best systems listed in #TARGET_REF , called TIP#REF and ClearTK [1] , use CRFs for recognition, so initially, we decided to apply the CRF-based approach for this task.\n sent3: The results were described in [12, 10] .\n sent4: In recent years, solutions based on deep neural networks, using word representation in the form of word embeddings, created with the use of large linguistic corpus, have begun to dominate in the field of recognition of word expressions.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Experiments were carried out by the method proposed in #TARGET_REF .",
                "The first part is described as Task A, the purpose of which is to identify the boundaries of timexes and assign them to one of the following classes:",
                "9 http://nlp.pwr.edu.pl/ 10 https://github.com/CLARIN-PL/PolDeepNer date, time, duration, set.",
                "[%]  all  1635  100  train  1227  50  test  408  25   Table 5 : Evaluation data sets (source: KPWr)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Experiments were carried out by the method proposed in #TARGET_REF .\n sent1: The first part is described as Task A, the purpose of which is to identify the boundaries of timexes and assign them to one of the following classes:\n sent2: 9 http://nlp.pwr.edu.pl/ 10 https://github.com/CLARIN-PL/PolDeepNer date, time, duration, set.\n sent3: [%]  all  1635  100  train  1227  50  test  408  25   Table 5 : Evaluation data sets (source: KPWr).\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "We chose the best 3 results from each word embeddings group (EE, EP, EC) from Table 8 presenting F1-scores for all models.",
                "Then we evaluated these results using more detailed measures for timexes, presented in #TARGET_REF .",
                "The following measures were used to evaluate the quality of boundaries and class recognition, socalled strict match: strict precision (Str.P), strict recall (Str.R) and strict F1-score (Str.F1).",
                "A relaxed match (Rel.",
                "P, Rel."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We chose the best 3 results from each word embeddings group (EE, EP, EC) from Table 8 presenting F1-scores for all models.\n sent1: Then we evaluated these results using more detailed measures for timexes, presented in #TARGET_REF .\n sent2: The following measures were used to evaluate the quality of boundaries and class recognition, socalled strict match: strict precision (Str.P), strict recall (Str.R) and strict F1-score (Str.F1).\n sent3: A relaxed match (Rel.\n sent4: P, Rel.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "F1) evaluation has also been carried out to determine whether there is an overlap between the system entity and gold entity, e.g.",
                "[Sunday] and [Sunday morning] [35] .",
                "If there was an overlap, a relaxed type F1-score (Type.F1) was calculated #TARGET_REF .",
                "The results are presented in Table 9 .",
                "Table 6 : Evaluation results (precision) for 17 word embeddings models for each TIMEX3 class (date, time, duration and set)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: F1) evaluation has also been carried out to determine whether there is an overlap between the system entity and gold entity, e.g.\n sent1: [Sunday] and [Sunday morning] [35] .\n sent2: If there was an overlap, a relaxed type F1-score (Type.F1) was calculated #TARGET_REF .\n sent3: The results are presented in Table 9 .\n sent4: Table 6 : Evaluation results (precision) for 17 word embeddings models for each TIMEX3 class (date, time, duration and set).\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The ability of the model to provide vector representation for the unknown words seems to be the most important.",
                "Also, previous models built using KGR10 (EP) are probably less accurate due to an incorrect tokenisation of the corpus.",
                "We used WCRFT tagger [29] , which utilises #REF to tokenise the input text before the creation of the embeddings model.",
                "The comparison of EC1 with previous results obtained using only CRF [9] show the significant improvement across all the tested metrics: 3.6pp increase in strict F1-score, 1.36pp increase in relaxed precision, 5.61pp increase in relaxed recall and 3.51pp increase in relaxed F1-score.",
                "Table 9 : Evaluation results for all TIMEX3 classes (total) for 9 word embeddings models (3 best models from each embeddings group: EE, EP, EC from Table 8 ) using the following measures from #TARGET_REF : strict precision, strict recall, strict F1-score, relaxed precision, relaxed recall, relaxed F1-score, type F1-score."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The ability of the model to provide vector representation for the unknown words seems to be the most important.\n sent1: Also, previous models built using KGR10 (EP) are probably less accurate due to an incorrect tokenisation of the corpus.\n sent2: We used WCRFT tagger [29] , which utilises #REF to tokenise the input text before the creation of the embeddings model.\n sent3: The comparison of EC1 with previous results obtained using only CRF [9] show the significant improvement across all the tested metrics: 3.6pp increase in strict F1-score, 1.36pp increase in relaxed precision, 5.61pp increase in relaxed recall and 3.51pp increase in relaxed F1-score.\n sent4: Table 9 : Evaluation results for all TIMEX3 classes (total) for 9 word embeddings models (3 best models from each embeddings group: EE, EP, EC from Table 8 ) using the following measures from #TARGET_REF : strict precision, strict recall, strict F1-score, relaxed precision, relaxed recall, relaxed F1-score, type F1-score.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Domain adaptation (Daumé III, 2007; #REF) , training an algorithm on labeled data taken from one domain so that it can perform properly on data from other domains, is therefore recognized as a fundamental challenge in NLP.",
                "Indeed, over the last decade domain adaptation methods have been proposed for tasks such as sentiment classification (#REFb) , POS tagging (Schnabel and Schütze, 2013) , syntactic parsing (#REF; #REF; #REF) and relation extraction (#REF; #REFa) , if to name just a handful of applications and works.",
                "Leading recent approaches to domain adaptation in NLP are based on Neural Networks (NNs), and particularly on autoencoders (#REF; #REF) .",
                "These models are believed to extract features that are robust to cross-domain variations.",
                "However, while excelling on benchmark domain adaptation tasks such as cross-domain product sentiment classification #TARGET_REF , the reasons to this success are not entirely understood."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Domain adaptation (Daumé III, 2007; #REF) , training an algorithm on labeled data taken from one domain so that it can perform properly on data from other domains, is therefore recognized as a fundamental challenge in NLP.\n sent1: Indeed, over the last decade domain adaptation methods have been proposed for tasks such as sentiment classification (#REFb) , POS tagging (Schnabel and Schütze, 2013) , syntactic parsing (#REF; #REF; #REF) and relation extraction (#REF; #REFa) , if to name just a handful of applications and works.\n sent2: Leading recent approaches to domain adaptation in NLP are based on Neural Networks (NNs), and particularly on autoencoders (#REF; #REF) .\n sent3: These models are believed to extract features that are robust to cross-domain variations.\n sent4: However, while excelling on benchmark domain adaptation tasks such as cross-domain product sentiment classification #TARGET_REF , the reasons to this success are not entirely understood.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "These models are believed to extract features that are robust to cross-domain variations.",
                "However, while excelling on benchmark domain adaptation tasks such as cross-domain product sentiment classification (#REF) , the reasons to this success are not entirely understood.",
                "In the pre-NN era, a prominent approach to domain adaptation in NLP, and particularly in sentiment classification, has been structural correspondence learning (SCL) (#REF #TARGET_REF .",
                "Following the auxiliary problems approach to semi-supervised learning (#REF) , this method identifies correspondences among features from different domains by modeling their correlations with pivot features: features that are frequent in both domains and are important for the NLP task.",
                "Non-pivot features from different domains which are correlated with many of the same pivot features are assumed to correspond, providing a bridge between the domains."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: These models are believed to extract features that are robust to cross-domain variations.\n sent1: However, while excelling on benchmark domain adaptation tasks such as cross-domain product sentiment classification (#REF) , the reasons to this success are not entirely understood.\n sent2: In the pre-NN era, a prominent approach to domain adaptation in NLP, and particularly in sentiment classification, has been structural correspondence learning (SCL) (#REF #TARGET_REF .\n sent3: Following the auxiliary problems approach to semi-supervised learning (#REF) , this method identifies correspondences among features from different domains by modeling their correlations with pivot features: features that are frequent in both domains and are important for the NLP task.\n sent4: Non-pivot features from different domains which are correlated with many of the same pivot features are assumed to correspond, providing a bridge between the domains.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Pivot and Non-Pivot Features The definitions of this approach are given in Blitzer et al. (2006 #TARGET_REF , where SCL is presented in the context of POS tagging and sentiment classification, respectively.",
                "Fundamentally, the method divides the shared feature space of both the source and the target domains to the set of pivot features that are frequent in both domains and are prominent in the NLP task, and a complementary set of non-pivot features.",
                "In this section we abstract away from the actual feature space and its division to pivot and non-pivot subsets.",
                "In Section 4 we discuss this issue in the context of sentiment classification.",
                "For representation learning, SCL employs the pivot features in order to learn mappings from the original feature space of both domains to a shared, low-dimensional, real-valued feature space."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Pivot and Non-Pivot Features The definitions of this approach are given in Blitzer et al. (2006 #TARGET_REF , where SCL is presented in the context of POS tagging and sentiment classification, respectively.\n sent1: Fundamentally, the method divides the shared feature space of both the source and the target domains to the set of pivot features that are frequent in both domains and are prominent in the NLP task, and a complementary set of non-pivot features.\n sent2: In this section we abstract away from the actual feature space and its division to pivot and non-pivot subsets.\n sent3: In Section 4 we discuss this issue in the context of sentiment classification.\n sent4: For representation learning, SCL employs the pivot features in order to learn mappings from the original feature space of both domains to a shared, low-dimensional, real-valued feature space.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "An important observation of #TARGET_REF , is that some pivot features are similar to each other to the level that they indicate the same information with respect to the classification task.",
                "For example, in sentiment classification with word unigram features, the words (unigrams) great and excellent are likely to serve as pivot features, as the meaning of each of them is preserved across domains.",
                "At the same time, both features convey very similar (positive) sentiment information to the level that a sentiment classifier should treat them as equals.",
                "The AE-SCL-SR model is based on two crucial observations.",
                "First, in many NLP tasks the pivot features can be pre-embeded into a vector space where pivots with similar meaning have similar vectors."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: An important observation of #TARGET_REF , is that some pivot features are similar to each other to the level that they indicate the same information with respect to the classification task.\n sent1: For example, in sentiment classification with word unigram features, the words (unigrams) great and excellent are likely to serve as pivot features, as the meaning of each of them is preserved across domains.\n sent2: At the same time, both features convey very similar (positive) sentiment information to the level that a sentiment classifier should treat them as equals.\n sent3: The AE-SCL-SR model is based on two crucial observations.\n sent4: First, in many NLP tasks the pivot features can be pre-embeded into a vector space where pivots with similar meaning have similar vectors.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "As in the other models, MSDA-DAN utilizes source domain labeled data as well as unlabeled data from both the source and the target domains at training time.",
                "We experiment with a 5-fold cross-validation on the source domain (#REF) : 1600 reviews for training and 400 reviews for development.",
                "The test set for each target domain of #TARGET_REF consists of all 2000 labeled reviews of that domain, and for the Blog domain it consists of the 7086 labeled sentences provided with the task dataset.",
                "In all five folds half of the training examples and half of the development examples are randomly selected from the positive reviews and the other halves from the negative reviews.",
                "We report average results across these five folds, employing the same folds for all models."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: As in the other models, MSDA-DAN utilizes source domain labeled data as well as unlabeled data from both the source and the target domains at training time.\n sent1: We experiment with a 5-fold cross-validation on the source domain (#REF) : 1600 reviews for training and 400 reviews for development.\n sent2: The test set for each target domain of #TARGET_REF consists of all 2000 labeled reviews of that domain, and for the Blog domain it consists of the 7086 labeled sentences provided with the task dataset.\n sent3: In all five folds half of the training examples and half of the development examples are randomly selected from the positive reviews and the other halves from the negative reviews.\n sent4: We report average results across these five folds, employing the same folds for all models.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "As explained in Section 3, this approach encourages the model to learn similar hidden layers for documents that have different pivot features as long as these features have similar meaning.",
                "In sentiment classification, for example, although one positive review may use the unigram pivot feature excellent while another positive review uses the pivot great, as long as the embeddings of pivot features with similar meaning are similar (as expected from high quality embeddings) the hidden layers learned for both documents are biased to be similar.",
                "We experiment with the task of cross-domain product sentiment classification of #TARGET_REF , consisting of 4 domains (12 domain pairs) and further add an additional target domain, consisting of sentences extracted from social media blogs (total of 16 domain pairs).",
                "For pivot feature embedding in our advanced model, we employ the word2vec algorithm (#REF) .",
                "Our models substantially outperform strong baselines: the SCL algorithm, the marginalized stacked denoising autoencoder (MSDA) model (#REF) and the MSDA-DAN model (#REF) that combines the power of MSDA with a domain adversarial network (DAN)."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: As explained in Section 3, this approach encourages the model to learn similar hidden layers for documents that have different pivot features as long as these features have similar meaning.\n sent1: In sentiment classification, for example, although one positive review may use the unigram pivot feature excellent while another positive review uses the pivot great, as long as the embeddings of pivot features with similar meaning are similar (as expected from high quality embeddings) the hidden layers learned for both documents are biased to be similar.\n sent2: We experiment with the task of cross-domain product sentiment classification of #TARGET_REF , consisting of 4 domains (12 domain pairs) and further add an additional target domain, consisting of sentences extracted from social media blogs (total of 16 domain pairs).\n sent3: For pivot feature embedding in our advanced model, we employ the word2vec algorithm (#REF) .\n sent4: Our models substantially outperform strong baselines: the SCL algorithm, the marginalized stacked denoising autoencoder (MSDA) model (#REF) and the MSDA-DAN model (#REF) that combines the power of MSDA with a domain adversarial network (DAN).\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We denote the feature set in our problem with f , the subset of pivot features with f p ⊆ {1, . . . , |f |} and the subset of non-pivot features with f np ⊆ {1, . . . , |f |} such that f p ∪ f np = {1, . . . , |f |} and f p ∩ f np = ∅. We further denote the feature representation of an input example X with x. Following this notation, the vector of pivot features of X is denoted with x p while the vector of non-pivot features is denoted with x np .",
                "In order to learn a robust and compact feature representation for X we will aim to learn a nonlinear prediction function from x np to x p .",
                "As discussed in Section 4 the task we experiment with is cross-domain sentiment classification.",
                "Following previous work (e.g. (#REF #TARGET_REF #REF) our feature representation consists of binary indicators for the occurrence of word unigrams and bigrams in the represented document.",
                "In what follows we hence assume that the feature representation x of an example X is a binary vector, and hence so are x p and x np ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We denote the feature set in our problem with f , the subset of pivot features with f p ⊆ {1, . . . , |f |} and the subset of non-pivot features with f np ⊆ {1, . . . , |f |} such that f p ∪ f np = {1, . . . , |f |} and f p ∩ f np = ∅. We further denote the feature representation of an input example X with x. Following this notation, the vector of pivot features of X is denoted with x p while the vector of non-pivot features is denoted with x np .\n sent1: In order to learn a robust and compact feature representation for X we will aim to learn a nonlinear prediction function from x np to x p .\n sent2: As discussed in Section 4 the task we experiment with is cross-domain sentiment classification.\n sent3: Following previous work (e.g. (#REF #TARGET_REF #REF) our feature representation consists of binary indicators for the occurrence of word unigrams and bigrams in the represented document.\n sent4: In what follows we hence assume that the feature representation x of an example X is a binary vector, and hence so are x p and x np .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "To facilitate clarity, some details are not given here and instead are provided in the appendices.",
                "Cross-domain Sentiment Classification To demonstrate the power of our models for domain adaptation we experiment with the task of crossdomain sentiment classification #TARGET_REF .",
                "The data for this task consist of Amazon product reviews from four product domains: Books (B), DVDs (D), Electronic items (E) and Kitchen appliances (K).",
                "For each domain 2000 labeled reviews are provided: 1000 are classified as positive and 1000 as negative, and these are augmented with unlabeled reviews: 6000 (B), 34741 (D), 13153 (E) and 16785 (K).",
                "We also consider an additional target domain, denoted with Blog: the University of Michigan sentence level sentiment dataset, consisting of sentences taken from social media blogs."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: To facilitate clarity, some details are not given here and instead are provided in the appendices.\n sent1: Cross-domain Sentiment Classification To demonstrate the power of our models for domain adaptation we experiment with the task of crossdomain sentiment classification #TARGET_REF .\n sent2: The data for this task consist of Amazon product reviews from four product domains: Books (B), DVDs (D), Electronic items (E) and Kitchen appliances (K).\n sent3: For each domain 2000 labeled reviews are provided: 1000 are classified as positive and 1000 as negative, and these are augmented with unlabeled reviews: 6000 (B), 34741 (D), 13153 (E) and 16785 (K).\n sent4: We also consider an additional target domain, denoted with Blog: the University of Michigan sentence level sentiment dataset, consisting of sentences taken from social media blogs.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We hence compare our models to three strong baselines, running all models under the same conditions.",
                "We aim to select baselines that represent the state-of-the-art in cross-domain sentiment classification in general, and in the two lines of work we focus at: pivot based and autoencoder based representation learning, in particular.",
                "The first baseline is SCL with pivot features selected using the mutual information criterion (SCL-MI, #TARGET_REF ).",
                "This is the SCL method where pivot features are frequent in the unlabeled data of both the source and the target do-mains, and among those features are the ones with the highest mutual information with the task (sentiment) label in the source domain labeled data.",
                "We implemented this method."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We hence compare our models to three strong baselines, running all models under the same conditions.\n sent1: We aim to select baselines that represent the state-of-the-art in cross-domain sentiment classification in general, and in the two lines of work we focus at: pivot based and autoencoder based representation learning, in particular.\n sent2: The first baseline is SCL with pivot features selected using the mutual information criterion (SCL-MI, #TARGET_REF ).\n sent3: This is the SCL method where pivot features are frequent in the unlabeled data of both the source and the target do-mains, and among those features are the ones with the highest mutual information with the task (sentiment) label in the source domain labeled data.\n sent4: We implemented this method.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We experiment with a 5-fold cross-validation on the source domain #TARGET_REF : 1600 reviews for training and 400 reviews for development.",
                "The test set for each target domain of #REF consists of all 2000 labeled reviews of that domain, and for the Blog domain it consists of the 7086 labeled sentences provided with the task dataset.",
                "In all five folds half of the training examples and half of the development examples are randomly selected from the positive reviews and the other halves from the negative reviews.",
                "We report average results across these five folds, employing the same folds for all models.",
                "Hyper-parameter Tuning The details of the hyper-parameter tuning process for all models (including data splits to training, development and test sets) are described in the appendices."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We experiment with a 5-fold cross-validation on the source domain #TARGET_REF : 1600 reviews for training and 400 reviews for development.\n sent1: The test set for each target domain of #REF consists of all 2000 labeled reviews of that domain, and for the Blog domain it consists of the 7086 labeled sentences provided with the task dataset.\n sent2: In all five folds half of the training examples and half of the development examples are randomly selected from the positive reviews and the other halves from the negative reviews.\n sent3: We report average results across these five folds, employing the same folds for all models.\n sent4: Hyper-parameter Tuning The details of the hyper-parameter tuning process for all models (including data splits to training, development and test sets) are described in the appendices.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "The number of pivots was chosen among {100, 200, . . . , 500} and the dimensionality of h among {100, 300, 500}. For the features induced by these models we take their w h x np vector.",
                "For AE-SCL-SR, embeddings for the unigram and bigram features were learned with word2vec (#REF) .",
                "Details about the software and the way we learn bigram representations are in the appendices.",
                "Baselines: For SCL-MI, following #TARGET_REF we tuned the number of pivot features (#REF; #REF) between 500 and 1000 and the SVD dimensions among 50,100 and 150.",
                "For MSDA we tuned the number of reconstructed features among {500, 1000, 2000, 5000, 10000}, the number of model layers among {1, 3, 5} and the corruption probability among {0.1, 0.2, . . . , 0.5}. For MSDA-DAN, we followed #REF : the λ adaptation parameter is chosen among 9 values between 10 −2 and 1 on a logarithmic scale, the hidden layer size l is chosen among {50, 100, 200} and the learning rate µ is 10 −3 ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The number of pivots was chosen among {100, 200, . . . , 500} and the dimensionality of h among {100, 300, 500}. For the features induced by these models we take their w h x np vector.\n sent1: For AE-SCL-SR, embeddings for the unigram and bigram features were learned with word2vec (#REF) .\n sent2: Details about the software and the way we learn bigram representations are in the appendices.\n sent3: Baselines: For SCL-MI, following #TARGET_REF we tuned the number of pivot features (#REF; #REF) between 500 and 1000 and the SVD dimensions among 50,100 and 150.\n sent4: For MSDA we tuned the number of reconstructed features among {500, 1000, 2000, 5000, 10000}, the number of model layers among {1, 3, 5} and the corruption probability among {0.1, 0.2, . . . , 0.5}. For MSDA-DAN, we followed #REF : the λ adaptation parameter is chosen among 9 values between 10 −2 and 1 on a logarithmic scale, the hidden layer size l is chosen among {50, 100, 200} and the learning rate µ is 10 −3 .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Variants of the Product Review Data There are two releases of the datasets of the #TARGET_REF cross-domain product review task.",
                "We use the one from http://www.cs.jhu.",
                "edu/˜mdredze/datasets/sentiment/ index2.html where the data is imbalanced, consisting of more positive than negative reviews.",
                "We believe that our setup is more realistic as when collecting unlabeled data, it is hard to get a balanced set.",
                "Note that #REF used the other release where the unlabeled data consists of the same number of positive and negative reviews."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Variants of the Product Review Data There are two releases of the datasets of the #TARGET_REF cross-domain product review task.\n sent1: We use the one from http://www.cs.jhu.\n sent2: edu/˜mdredze/datasets/sentiment/ index2.html where the data is imbalanced, consisting of more positive than negative reviews.\n sent3: We believe that our setup is more realistic as when collecting unlabeled data, it is hard to get a balanced set.\n sent4: Note that #REF used the other release where the unlabeled data consists of the same number of positive and negative reviews.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Table 1 presents our results.",
                "In the #TARGET_REF task (top tables), AE-SCL-SR is the best performing model in 9 of 12 setups and on a unified test set consisting of the test sets of all 12 setups (the Test-All column).",
                "AE-SCL, MSDA and MSDA-DAN perform best in one setup each.",
                "On the unified test set, AE-SCL-SR improves over SCL-MI by 3.8% (error reduction (ER) of 14.8%) and over MSDA-DAN by 2% (ER of 8.4%), while AE-SCL improves over SCL-MI and MSDA-DAN by 2.7% (ER of 10.5%) and 0.9% (ER of 3.8%), respectively.",
                "MSDA-DAN and MSDA perform very similarly on the unified test set (0.761 and 0.759, respectively) with generally minor differences in the individual setups."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: Table 1 presents our results.\n sent1: In the #TARGET_REF task (top tables), AE-SCL-SR is the best performing model in 9 of 12 setups and on a unified test set consisting of the test sets of all 12 setups (the Test-All column).\n sent2: AE-SCL, MSDA and MSDA-DAN perform best in one setup each.\n sent3: On the unified test set, AE-SCL-SR improves over SCL-MI by 3.8% (error reduction (ER) of 14.8%) and over MSDA-DAN by 2% (ER of 8.4%), while AE-SCL improves over SCL-MI and MSDA-DAN by 2.7% (ER of 10.5%) and 0.9% (ER of 3.8%), respectively.\n sent4: MSDA-DAN and MSDA perform very similarly on the unified test set (0.761 and 0.759, respectively) with generally minor differences in the individual setups.\n",
        "output": "{\"label\": [null], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Variants of the Product Review Data There are two releases of the datasets of the #REF cross-domain product review task.",
                "We use the one from http://www.cs.jhu.",
                "edu/˜mdredze/datasets/sentiment/ index2.html where the data is imbalanced, consisting of more positive than negative reviews.",
                "We believe that our setup is more realistic as when collecting unlabeled data, it is hard to get a balanced set.",
                "Note that #TARGET_REF used the other release where the unlabeled data consists of the same number of positive and negative reviews."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Variants of the Product Review Data There are two releases of the datasets of the #REF cross-domain product review task.\n sent1: We use the one from http://www.cs.jhu.\n sent2: edu/˜mdredze/datasets/sentiment/ index2.html where the data is imbalanced, consisting of more positive than negative reviews.\n sent3: We believe that our setup is more realistic as when collecting unlabeled data, it is hard to get a balanced set.\n sent4: Note that #TARGET_REF used the other release where the unlabeled data consists of the same number of positive and negative reviews.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "edu/˜mdredze/datasets/sentiment/ index2.html where the data is imbalanced, consisting of more positive than negative reviews.",
                "We believe that our setup is more realistic as when collecting unlabeled data, it is hard to get a balanced set.",
                "Note that #REF used the other release where the unlabeled data consists of the same number of positive and negative reviews.",
                "Test Set Size While #TARGET_REF used only 400 target domain reviews for test, we use the entire set of 2000 reviews.",
                "We believe that this decision yields more robust and statistically significant results."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: edu/˜mdredze/datasets/sentiment/ index2.html where the data is imbalanced, consisting of more positive than negative reviews.\n sent1: We believe that our setup is more realistic as when collecting unlabeled data, it is hard to get a balanced set.\n sent2: Note that #REF used the other release where the unlabeled data consists of the same number of positive and negative reviews.\n sent3: Test Set Size While #TARGET_REF used only 400 target domain reviews for test, we use the entire set of 2000 reviews.\n sent4: We believe that this decision yields more robust and statistically significant results.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "They either use all the NPs as candidate mentions (Björkelund and #REF; #REF; #REF) or use the rule-based mention detector from the Stanford deterministic system (#REF) to extract mentions from NPs, named entity mentions and pronouns (#REF; #REFb) .",
                "There are only very few studies that attempt to apply neural network approaches to the MD task.",
                "Lee et al. (2017; #TARGET_REF first introduced a neural mention detector as a part of their end-to-end coreference system; however, the system does not output intermediate mentions, hence the mention detector cannot be used by other coreference systems directly.",
                "To the best of our knowledge, #REF introduced the only standalone neural mention detector.",
                "By using a modified version of the NER system of #REF , they showed substantial performance gains at mention detection on the benchmark CONLL 2012 data set and on the CRAC 2018 data set when compared with the Stanford deterministic system (#REF) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: They either use all the NPs as candidate mentions (Björkelund and #REF; #REF; #REF) or use the rule-based mention detector from the Stanford deterministic system (#REF) to extract mentions from NPs, named entity mentions and pronouns (#REF; #REFb) .\n sent1: There are only very few studies that attempt to apply neural network approaches to the MD task.\n sent2: Lee et al. (2017; #TARGET_REF first introduced a neural mention detector as a part of their end-to-end coreference system; however, the system does not output intermediate mentions, hence the mention detector cannot be used by other coreference systems directly.\n sent3: To the best of our knowledge, #REF introduced the only standalone neural mention detector.\n sent4: By using a modified version of the NER system of #REF , they showed substantial performance gains at mention detection on the benchmark CONLL 2012 data set and on the CRAC 2018 data set when compared with the Stanford deterministic system (#REF) .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The system has been later extended by #REF and #TARGET_REF .",
                "#REF added biaffine attention to the coreference part of the #REF system, improving the system by 0.6%.",
                "Biaffine attention is also used in one of our approaches (BIAFFINE MD), but in a totally different manner, i.e. we use biaffine attention for mention detection while in #REF biaffine attention was used for computing mention-pair scores.",
                "The system is the current state-of-the-art coreference system.",
                "In this new system, the #REF model is substantially improved through the use of ELMo embeddings (#REF) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The system has been later extended by #REF and #TARGET_REF .\n sent1: #REF added biaffine attention to the coreference part of the #REF system, improving the system by 0.6%.\n sent2: Biaffine attention is also used in one of our approaches (BIAFFINE MD), but in a totally different manner, i.e. we use biaffine attention for mention detection while in #REF biaffine attention was used for computing mention-pair scores.\n sent3: The system is the current state-of-the-art coreference system.\n sent4: In this new system, the #REF model is substantially improved through the use of ELMo embeddings (#REF) .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "After training the system with the new setting, we get an average F1 of 72.6% (see table 4), which narrows the performance gap between the end-to-end system and the model trained without the joint learning.",
                "This confirms our first hypothesis that by downgrading the system to a pipeline setting does harm the overall performance of the coreference resolution.",
                "For our second experiment, we used the #REF instead.",
                "The #TARGET_REF system is an extended version of the #REF system, hence they share most of the network architecture.",
                "The #REF has a lower performance on mention detection (93.5% recall when λ = 0.4), which creates a large (4%) difference when compared with the recall of our BIAFFINE MD."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: After training the system with the new setting, we get an average F1 of 72.6% (see table 4), which narrows the performance gap between the end-to-end system and the model trained without the joint learning.\n sent1: This confirms our first hypothesis that by downgrading the system to a pipeline setting does harm the overall performance of the coreference resolution.\n sent2: For our second experiment, we used the #REF instead.\n sent3: The #TARGET_REF system is an extended version of the #REF system, hence they share most of the network architecture.\n sent4: The #REF has a lower performance on mention detection (93.5% recall when λ = 0.4), which creates a large (4%) difference when compared with the recall of our BIAFFINE MD.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The third system takes the outputs from BERT (#REF) and feeds them into a feed-forward neural network to classify candidates into mentions and non mentions.",
                "We evaluate these three models on both the CONLL and the CRAC data sets, with the following results.",
                "Firstly, we show that better mention performance of up to 1.5 percentage points 1 can be achieved by training the mention detector alone.",
                "Secondly, our best system achieves improvements of 5.3 and 6.5 percentage points when compared with #REF 's neural MD system on CONLL and CRAC respectively.",
                "Thirdly, by using better mentions from our mention detector, we can improve the end-to-end #TARGET_REF system and the Clark and Manning (2016a) pipeline system by up to 0.7% and 1.7% respectively."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: The third system takes the outputs from BERT (#REF) and feeds them into a feed-forward neural network to classify candidates into mentions and non mentions.\n sent1: We evaluate these three models on both the CONLL and the CRAC data sets, with the following results.\n sent2: Firstly, we show that better mention performance of up to 1.5 percentage points 1 can be achieved by training the mention detector alone.\n sent3: Secondly, our best system achieves improvements of 5.3 and 6.5 percentage points when compared with #REF 's neural MD system on CONLL and CRAC respectively.\n sent4: Thirdly, by using better mentions from our mention detector, we can improve the end-to-end #TARGET_REF system and the Clark and Manning (2016a) pipeline system by up to 0.7% and 1.7% respectively.\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "For the mention detection evaluation we use the #REF system as baseline.",
                "The baseline is trained end-toend on the coreference task and we use as baseline the mentions predicted by the system before carrying out coreference resolution.",
                "For the coreference evaluation we use the state-of-the-art #REF system as our baseline for the end-to-end system, and the Clark and Manning (2016a) system as our baseline for the pipeline system.",
                "During the evaluation, we slightly modified the #TARGET_REF system to allow the system to take the mentions predicted by our model instead of its internal mention detector.",
                "Other than that we keep the system unchanged."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: For the mention detection evaluation we use the #REF system as baseline.\n sent1: The baseline is trained end-toend on the coreference task and we use as baseline the mentions predicted by the system before carrying out coreference resolution.\n sent2: For the coreference evaluation we use the state-of-the-art #REF system as our baseline for the end-to-end system, and the Clark and Manning (2016a) system as our baseline for the pipeline system.\n sent3: During the evaluation, we slightly modified the #TARGET_REF system to allow the system to take the mentions predicted by our model instead of its internal mention detector.\n sent4: Other than that we keep the system unchanged.\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Evaluation on the CRAC data set 3 For the CRAC data set, we train the #REF system end-to-end on the reduced corpus with singleton mentions removed and extract mentions from the system by set λ = 0.4.",
                "We then train our models with the same λ but on the full corpus, since our mention detectors naturally support both mention 3 As the #TARGET_REF system does not predict singleton mentions, the results on CRAC data set in Table 2 are evaluated without singleton mentions.",
                "While the results reported in Table 3 are evaluated with singleton mentions included.",
                "88.0 89.7 89.1 Table 3 : Comparison between our BIAFFINE MD and the top performing systems on the mention detection task using the CONLL and CRAC data sets.",
                "types (singleton and non-singleton mentions)."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Evaluation on the CRAC data set 3 For the CRAC data set, we train the #REF system end-to-end on the reduced corpus with singleton mentions removed and extract mentions from the system by set λ = 0.4.\n sent1: We then train our models with the same λ but on the full corpus, since our mention detectors naturally support both mention 3 As the #TARGET_REF system does not predict singleton mentions, the results on CRAC data set in Table 2 are evaluated without singleton mentions.\n sent2: While the results reported in Table 3 are evaluated with singleton mentions included.\n sent3: 88.0 89.7 89.1 Table 3 : Comparison between our BIAFFINE MD and the top performing systems on the mention detection task using the CONLL and CRAC data sets.\n sent4: types (singleton and non-singleton mentions).\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Evaluation with the end-to-end system.",
                "We first evaluate our BIAFFINE MD in combination with the end-to-end #TARGET_REF system.",
                "We slightly modified the system to feed the system mentions predicted by our mention detector.",
                "As a result, the original mention selection function is switched off, we keep all the other settings (include the mention scoring function) unchanged.",
                "We then train the modified system to obtain a new model."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Evaluation with the end-to-end system.\n sent1: We first evaluate our BIAFFINE MD in combination with the end-to-end #TARGET_REF system.\n sent2: We slightly modified the system to feed the system mentions predicted by our mention detector.\n sent3: As a result, the original mention selection function is switched off, we keep all the other settings (include the mention scoring function) unchanged.\n sent4: We then train the modified system to obtain a new model.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent1\", \"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The third type of projection, P Acc , integrates syntactic information in the edge weights calculated by the following formula: While the entity grid #TARGET_REF uses information about sentences which do not share entities by means of the \"--\" transition, the entity graph cannot employ this negative information.",
                "Here, we propose a normalization for the entity graph and its corresponding one-mode projections which is based on the relative importance of entities and, in turn, the relative importance of sentences.",
                "Including negative information allows to normalize the importance of entities according to sentence length (measured in terms of entity mentions), and hence to capture distance information between mentions of the same entity.",
                "This brings the entity graph closer to Stoddard's (1991, p.30 ) notion of cohesion: \"The relative cohesiveness of a text depends on the number of cohesive ties [...] and on the distance between the nodes and their associated cohesive elements.",
                "\" By using this information, edge weights are set less arbitrary which leads to the more sound method and higher performance in all tasks."
            ],
            "label": [
                "BACKGROUND",
                "MOTIVATION"
            ]
        },
        "input": "sent0: The third type of projection, P Acc , integrates syntactic information in the edge weights calculated by the following formula: While the entity grid #TARGET_REF uses information about sentences which do not share entities by means of the \"--\" transition, the entity graph cannot employ this negative information.\n sent1: Here, we propose a normalization for the entity graph and its corresponding one-mode projections which is based on the relative importance of entities and, in turn, the relative importance of sentences.\n sent2: Including negative information allows to normalize the importance of entities according to sentence length (measured in terms of entity mentions), and hence to capture distance information between mentions of the same entity.\n sent3: This brings the entity graph closer to Stoddard's (1991, p.30 ) notion of cohesion: \"The relative cohesiveness of a text depends on the number of cohesive ties [...] and on the distance between the nodes and their associated cohesive elements.\n sent4: \" By using this information, edge weights are set less arbitrary which leads to the more sound method and higher performance in all tasks.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"MOTIVATION\"], \"context\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "For discrimination we use 20 permutations of each text.",
                "Table 1 shows the results.",
                "Results for #REF , G&S, are reproduced, results for #TARGET_REF , B&L, and #REF , E&C, were reproduced by #REF .",
                "The unweighted graph, P U , does not need normalization.",
                "Hence the results for the entity graph and the normalized entity graph are identical."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For discrimination we use 20 permutations of each text.\n sent1: Table 1 shows the results.\n sent2: Results for #REF , G&S, are reproduced, results for #TARGET_REF , B&L, and #REF , E&C, were reproduced by #REF .\n sent3: The unweighted graph, P U , does not need normalization.\n sent4: Hence the results for the entity graph and the normalized entity graph are identical.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "In experiments, #TARGET_REF assume that articles taken from Encyclopedia Britannica are more difficult to read (less coherent) than the corresponding articles from Encyclopedia Britannica Elementary, its version for children.",
                "We follow them with regard to data (107 article pairs), experimental setup and evaluation.",
                "Sentences in the Britannica Elementary are simpler and shorter than in the Encyclopedia Britannica.",
                "The entity graph does not take into account the effect of entities not shared between sentences while the normalized entity graph assigns a lower weight if there are more of these entities.",
                "Hence, Britannica Elementary receives a higher cohesion score than Encyclopedia Britannica in our model."
            ],
            "label": [
                "BACKGROUND",
                "MOTIVATION",
                "USE"
            ]
        },
        "input": "sent0: In experiments, #TARGET_REF assume that articles taken from Encyclopedia Britannica are more difficult to read (less coherent) than the corresponding articles from Encyclopedia Britannica Elementary, its version for children.\n sent1: We follow them with regard to data (107 article pairs), experimental setup and evaluation.\n sent2: Sentences in the Britannica Elementary are simpler and shorter than in the Encyclopedia Britannica.\n sent3: The entity graph does not take into account the effect of entities not shared between sentences while the normalized entity graph assigns a lower weight if there are more of these entities.\n sent4: Hence, Britannica Elementary receives a higher cohesion score than Encyclopedia Britannica in our model.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"MOTIVATION\", \"USE\"], \"context\": [\"sent0\", \"sent1\", \"sent2\", \"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "We follow #TARGET_REF for evaluating whether the normalized entity graph can decide whether automatic or human summaries are more coherent (80 pairs of summaries extracted from DUC 2003).",
                "Human coherence scores are associated with each pair of summarized documents (#REF) .",
                "Table 3 displays reported results of B&L and reproduced results of the entity graph and our normalized entity graph.",
                "Normalizing significantly improves the results for P W and P Acc .",
                "P U is still slightly better than both, but in contrast to the entity graph, this difference is not statistically significant."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We follow #TARGET_REF for evaluating whether the normalized entity graph can decide whether automatic or human summaries are more coherent (80 pairs of summaries extracted from DUC 2003).\n sent1: Human coherence scores are associated with each pair of summarized documents (#REF) .\n sent2: Table 3 displays reported results of B&L and reproduced results of the entity graph and our normalized entity graph.\n sent3: Normalizing significantly improves the results for P W and P Acc .\n sent4: P U is still slightly better than both, but in contrast to the entity graph, this difference is not statistically significant.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "We follow #REF for evaluating whether the normalized entity graph can decide whether automatic or human summaries are more coherent (80 pairs of summaries extracted from DUC 2003).",
                "Human coherence scores are associated with each pair of summarized documents #TARGET_REF .",
                "Table 3 displays reported results of B&L and reproduced results of the entity graph and our normalized entity graph.",
                "Normalizing significantly improves the results for P W and P Acc .",
                "P U is still slightly better than both, but in contrast to the entity graph, this difference is not statistically significant."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We follow #REF for evaluating whether the normalized entity graph can decide whether automatic or human summaries are more coherent (80 pairs of summaries extracted from DUC 2003).\n sent1: Human coherence scores are associated with each pair of summarized documents #TARGET_REF .\n sent2: Table 3 displays reported results of B&L and reproduced results of the entity graph and our normalized entity graph.\n sent3: Normalizing significantly improves the results for P W and P Acc .\n sent4: P U is still slightly better than both, but in contrast to the entity graph, this difference is not statistically significant.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We proposed a normalization method for the entity graph (#REF) .",
                "We compared our model to the entity graph and to the entity grid #TARGET_REF and showed that normalization improves the results significantly in most tasks.",
                "Future work will include adding more linguistic information, stronger weighting schemes and application to other readability datasets (#REF; De #REF) ."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: We proposed a normalization method for the entity graph (#REF) .\n sent1: We compared our model to the entity graph and to the entity grid #TARGET_REF and showed that normalization improves the results significantly in most tasks.\n sent2: Future work will include adding more linguistic information, stronger weighting schemes and application to other readability datasets (#REF; De #REF) .\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Therefore, our setting resembles the established task of entity recognition (#REF; #TARGET_REF) , with the difference being that we focus on un-named entities.",
                "Contribution.",
                "One of the factors impeding progress in common sense information extraction is the lack of training data.",
                "It is relatively easy to obtain labeled data for named entities such as companies and people.",
                "Examples of such named entities can be found in structured forms on the Web, such as HTML lists and tables, and Wikipedia infoboxes (#REF; #REF) ."
            ],
            "label": [
                "BACKGROUND",
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Therefore, our setting resembles the established task of entity recognition (#REF; #TARGET_REF) , with the difference being that we focus on un-named entities.\n sent1: Contribution.\n sent2: One of the factors impeding progress in common sense information extraction is the lack of training data.\n sent3: It is relatively easy to obtain labeled data for named entities such as companies and people.\n sent4: Examples of such named entities can be found in structured forms on the Web, such as HTML lists and tables, and Wikipedia infoboxes (#REF; #REF) .\n",
        "output": "{\"label\": [\"BACKGROUND\", \"DIFFERENCES\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "We treat sense recognition in text as sequence prediction problem, we would like to estimate: P (y i |x i−k , ..., x i+k ; y i−l , ..., y i−1 ). where x refers Figure 4: Our neural network architecture for the task of recognizing concepts that are discernible by sensesss.",
                "to words, and y refers to BIO labels.",
                "Conditional Random Fields (CRFs) (#REF ) have been widely used named entity recognition #TARGET_REF; #REF) , a task similar to our own.",
                "While the CRF models performed reasonably well on our task, we sought to obtain improvements by proposing and training variations of Long Short Memory (LSTM) recurrent neural networks (#REF).",
                "We found our variations of LSTM sequence classifiers to do better than the CRF model, and also better than standard LSTMs."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: We treat sense recognition in text as sequence prediction problem, we would like to estimate: P (y i |x i−k , ..., x i+k ; y i−l , ..., y i−1 ). where x refers Figure 4: Our neural network architecture for the task of recognizing concepts that are discernible by sensesss.\n sent1: to words, and y refers to BIO labels.\n sent2: Conditional Random Fields (CRFs) (#REF ) have been widely used named entity recognition #TARGET_REF; #REF) , a task similar to our own.\n sent3: While the CRF models performed reasonably well on our task, we sought to obtain improvements by proposing and training variations of Long Short Memory (LSTM) recurrent neural networks (#REF).\n sent4: We found our variations of LSTM sequence classifiers to do better than the CRF model, and also better than standard LSTMs.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Our task is related to entity recognition however in this paper we focused on novel types of entities, which can be used to improve extraction of common sense knowledge.",
                "Entity recognition systems are traditionally based on a sequential model, for example a CRF, and involve feature engineering (#REF; #TARGET_REF.",
                "More recently, neural approaches have been used for named entity recognition (#REF; #REF; dos Santos and Guimarães, 2015; #REF; #REF) .",
                "Like other neural approaches, our approach does not require feature engineering, the only features we use are word and character embeddings.",
                "Related to our proposed recurrence in the output layer is the work of (#REF) which introduced a CRF on top of LSTM for the task of named entity recognition."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Our task is related to entity recognition however in this paper we focused on novel types of entities, which can be used to improve extraction of common sense knowledge.\n sent1: Entity recognition systems are traditionally based on a sequential model, for example a CRF, and involve feature engineering (#REF; #TARGET_REF.\n sent2: More recently, neural approaches have been used for named entity recognition (#REF; #REF; dos Santos and Guimarães, 2015; #REF; #REF) .\n sent3: Like other neural approaches, our approach does not require feature engineering, the only features we use are word and character embeddings.\n sent4: Related to our proposed recurrence in the output layer is the work of (#REF) which introduced a CRF on top of LSTM for the task of named entity recognition.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We would like to detect mentions of concepts discernible by sense.",
                "In this paper, we focus on mentions of audible (sound) and olfactible (smell) concepts.",
                "We treat sense recognition in text as a sequence labeling task where each sentence is a sequence of tokens labeled using the BIO tagging scheme #TARGET_REF.",
                "The BIO labels denote tokens at the beginning, inside, and outside of a relevant mention, respectively.",
                "Example BIO tagged sentences are shown in Figure 1 ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We would like to detect mentions of concepts discernible by sense.\n sent1: In this paper, we focus on mentions of audible (sound) and olfactible (smell) concepts.\n sent2: We treat sense recognition in text as a sequence labeling task where each sentence is a sequence of tokens labeled using the BIO tagging scheme #TARGET_REF.\n sent3: The BIO labels denote tokens at the beginning, inside, and outside of a relevant mention, respectively.\n sent4: Example BIO tagged sentences are shown in Figure 1 .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "+ OR + CHAR refers to the LSTM plus the output recurrence and character embeddings as features.",
                "For the CRF, we use the commonly used features for named entity recognition: words, prefix/suffices, and part-of-speech tag #TARGET_REF.",
                "We can see that for both senses, the model that uses both character embedding features, and an output recurrence layer yields the best F1 score.",
                "Examples of sounds and smells our method can recognize are shown in Table 4 .",
                "Table 3 : Performance of the various models on the task of sense recognition."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: + OR + CHAR refers to the LSTM plus the output recurrence and character embeddings as features.\n sent1: For the CRF, we use the commonly used features for named entity recognition: words, prefix/suffices, and part-of-speech tag #TARGET_REF.\n sent2: We can see that for both senses, the model that uses both character embedding features, and an output recurrence layer yields the best F1 score.\n sent3: Examples of sounds and smells our method can recognize are shown in Table 4 .\n sent4: Table 3 : Performance of the various models on the task of sense recognition.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Existing fact checking systems are capable of detecting fact-check-worthy claims in text (#REFb) , returning semantically similar textual claims (#REF) ; and scoring the truth of triples on a knowledge graph through semantic distance (#REF) .",
                "However, neither of these are suitable for fact checking a claim made in natural language against a database.",
                "Previous works appropriate for this task operate on a limited domain and are not able to incorporate temporal information when checking time-dependent claims #TARGET_REF .",
                "In this paper we introduce our fact checking tool, describe its architecture and design decisions, evaluate its accuracy and discuss future work.",
                "We highlight the ease of incorporating new information sources to fact check, which may be unavailable during training."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Existing fact checking systems are capable of detecting fact-check-worthy claims in text (#REFb) , returning semantically similar textual claims (#REF) ; and scoring the truth of triples on a knowledge graph through semantic distance (#REF) .\n sent1: However, neither of these are suitable for fact checking a claim made in natural language against a database.\n sent2: Previous works appropriate for this task operate on a limited domain and are not able to incorporate temporal information when checking time-dependent claims #TARGET_REF .\n sent3: In this paper we introduce our fact checking tool, describe its architecture and design decisions, evaluate its accuracy and discuss future work.\n sent4: We highlight the ease of incorporating new information sources to fact check, which may be unavailable during training.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Previous works appropriate for this task operate on a limited domain and are not able to incorporate temporal information when checking time-dependent claims (#REF) .",
                "In this paper we introduce our fact checking tool, describe its architecture and design decisions, evaluate its accuracy and discuss future work.",
                "We highlight the ease of incorporating new information sources to fact check, which may be unavailable during training.",
                "To validate the extensibility of the system, we complete an additional evaluation of the system using claims taken from #TARGET_REF .",
                "We make the source code publicly available to the community."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Previous works appropriate for this task operate on a limited domain and are not able to incorporate temporal information when checking time-dependent claims (#REF) .\n sent1: In this paper we introduce our fact checking tool, describe its architecture and design decisions, evaluate its accuracy and discuss future work.\n sent2: We highlight the ease of incorporating new information sources to fact check, which may be unavailable during training.\n sent3: To validate the extensibility of the system, we complete an additional evaluation of the system using claims taken from #TARGET_REF .\n sent4: We make the source code publicly available to the community.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "We further validate the system by evaluating the ability of this fact checking system to make veracity assessments on simple numerical claims from the data set collected by #TARGET_REF .",
                "Of the 4,255 claims about numerical properties about countries and geographical areas in this data set, our KB contained information to fact check 3,418.",
                "The system presented recalled KB entries for 3,045 claims (89.1%).",
                "We observed that the system was consistently unable to fact check two properties (undernourishment and renewable freshwater per capita).",
                "Analysis of these failure cases revealed too great a lexical difference between the test claims and the training data our system generated; the claims in the test cases were comparative in nature (e. g. country X has higher rate of undernourishment than country Y) whereas the training data generated using the method described in Section 3.2 are absolute claims."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We further validate the system by evaluating the ability of this fact checking system to make veracity assessments on simple numerical claims from the data set collected by #TARGET_REF .\n sent1: Of the 4,255 claims about numerical properties about countries and geographical areas in this data set, our KB contained information to fact check 3,418.\n sent2: The system presented recalled KB entries for 3,045 claims (89.1%).\n sent3: We observed that the system was consistently unable to fact check two properties (undernourishment and renewable freshwater per capita).\n sent4: Analysis of these failure cases revealed too great a lexical difference between the test claims and the training data our system generated; the claims in the test cases were comparative in nature (e. g. country X has higher rate of undernourishment than country Y) whereas the training data generated using the method described in Section 3.2 are absolute claims.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "The core capability of the system demonstration we presented is to fact check natural language claims against relations stored in a KB.",
                "Although the range of claims is limited, the system is a fieldtested prototype and has been evaluated on a published data set #TARGET_REF and on real-world claims presented as part of the HeroX fact checking challenge.",
                "In future work, we will extend the semantic parsing technique used and apply our system to more complex claim types.",
                "Additionally, further work is required to reduce the number of candidate relations recalled from the KB.",
                "While this was not an issue in our case, we believe that ameliorating this issue will enhance the ability of the system to assign a correct truth label where there exist properties with similar numerical values."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The core capability of the system demonstration we presented is to fact check natural language claims against relations stored in a KB.\n sent1: Although the range of claims is limited, the system is a fieldtested prototype and has been evaluated on a published data set #TARGET_REF and on real-world claims presented as part of the HeroX fact checking challenge.\n sent2: In future work, we will extend the semantic parsing technique used and apply our system to more complex claim types.\n sent3: Additionally, further work is required to reduce the number of candidate relations recalled from the KB.\n sent4: While this was not an issue in our case, we believe that ameliorating this issue will enhance the ability of the system to assign a correct truth label where there exist properties with similar numerical values.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We developed our fact-checking approach in the context of the HeroX challenge 2 -a competition organised by the fact checking organization FullFact 3 .",
                "The types of claims the system presented can fact check was restricted to those which require looking up a value in a KB, similar to the one in Figure 1 .",
                "To learn a model to perform the KB look up (essentially a semantic parsing task), we extend the work of #TARGET_REF who used distant supervision (#REF ) to generate training data, obviating the need for manual labeling.",
                "In particular, we extend it to handle simple temporal expressions in order to fact check time-dependent claims appropriately, i. e. population in 2015.",
                "While the recently proposed semantic parser of #REF is also able to handle temporal expressions, it makes the assumption that the table against which the claim needs to be interpreted is known, which is unrealistic in the context of fact checking."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: We developed our fact-checking approach in the context of the HeroX challenge 2 -a competition organised by the fact checking organization FullFact 3 .\n sent1: The types of claims the system presented can fact check was restricted to those which require looking up a value in a KB, similar to the one in Figure 1 .\n sent2: To learn a model to perform the KB look up (essentially a semantic parsing task), we extend the work of #TARGET_REF who used distant supervision (#REF ) to generate training data, obviating the need for manual labeling.\n sent3: In particular, we extend it to handle simple temporal expressions in order to fact check time-dependent claims appropriately, i. e. population in 2015.\n sent4: While the recently proposed semantic parser of #REF is also able to handle temporal expressions, it makes the assumption that the table against which the claim needs to be interpreted is known, which is unrealistic in the context of fact checking.\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Part-of-speech (POS) tagging has received a great deal of attention as it is a critical component of most natural language processing systems.",
                "As supervised POS tagging accuracies for English (measured on the Wall Street Journal portion of the PennTreebank (#REF) ) have converged to around 97.3% (#REF; #REF) , the attention has shifted to unsupervised approaches (#REF) .",
                "In particular, there has been growing interest in both multilingual POS induction ) and cross-lingual POS induction via treebank projection (#REF; #REF; #TARGET_REF .",
                "Underlying these studies is the idea that a set of (coarse) syntactic POS categories exist in similar forms across languages.",
                "These categories are often called universals to represent their cross-lingual nature (#REF; #REF) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Part-of-speech (POS) tagging has received a great deal of attention as it is a critical component of most natural language processing systems.\n sent1: As supervised POS tagging accuracies for English (measured on the Wall Street Journal portion of the PennTreebank (#REF) ) have converged to around 97.3% (#REF; #REF) , the attention has shifted to unsupervised approaches (#REF) .\n sent2: In particular, there has been growing interest in both multilingual POS induction ) and cross-lingual POS induction via treebank projection (#REF; #REF; #TARGET_REF .\n sent3: Underlying these studies is the idea that a set of (coarse) syntactic POS categories exist in similar forms across languages.\n sent4: These categories are often called universals to represent their cross-lingual nature (#REF; #REF) .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Underlying these studies is the idea that a set of (coarse) syntactic POS categories exist in similar forms across languages.",
                "These categories are often called universals to represent their cross-lingual nature (#REF; #REF) .",
                "For example, used the Multext-East (#REF) corpus to evaluate their multi-lingual POS induction system, because it uses the same tagset for multiple languages.",
                "When corpora with common tagsets are unavailable, a standard approach is to manually define a mapping from language and treebank specific fine-grained tagsets to a predefined universal set.",
                "This was the approach taken by #TARGET_REF to evaluate their cross-lingual POS projection system for six different languages."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Underlying these studies is the idea that a set of (coarse) syntactic POS categories exist in similar forms across languages.\n sent1: These categories are often called universals to represent their cross-lingual nature (#REF; #REF) .\n sent2: For example, used the Multext-East (#REF) corpus to evaluate their multi-lingual POS induction system, because it uses the same tagset for multiple languages.\n sent3: When corpora with common tagsets are unavailable, a standard approach is to manually define a mapping from language and treebank specific fine-grained tagsets to a predefined universal set.\n sent4: This was the approach taken by #TARGET_REF to evaluate their cross-lingual POS projection system for six different languages.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\", \"sent1\", \"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Finally, it also permits language technology practitioners to train POS taggers with common tagsets across multiple languages.",
                "This in turn facilitates downstream application development as there is no need to maintain language specific rules due to differences in treebank annotation guidelines.",
                "In this paper, we specifically highlight two use cases of this resource.",
                "First, using our universal tagset and mapping, we run an experiment comparing POS tag accuracies for 25 different treebanks to evaluate POS tagging accuracy on a single tagset.",
                "Second, we combine the cross-lingual projection part-of-speech taggers of #TARGET_REF with the grammar induction system of #REF -which requires a universal tagset -to produce a completely unsupervised grammar induction system for multiple languages, that does not require gold POS tags in the target language."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: Finally, it also permits language technology practitioners to train POS taggers with common tagsets across multiple languages.\n sent1: This in turn facilitates downstream application development as there is no need to maintain language specific rules due to differences in treebank annotation guidelines.\n sent2: In this paper, we specifically highlight two use cases of this resource.\n sent3: First, using our universal tagset and mapping, we run an experiment comparing POS tag accuracies for 25 different treebanks to evaluate POS tagging accuracy on a single tagset.\n sent4: Second, we combine the cross-lingual projection part-of-speech taggers of #TARGET_REF with the grammar induction system of #REF -which requires a universal tagset -to produce a completely unsupervised grammar induction system for multiple languages, that does not require gold POS tags in the target language.\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Additionally, they also used refined categories in the form of CoNLL treebank tags.",
                "In our experiments, we did not make use of refined categories, as the POS tags induced by #REF were all coarse.",
                "We present results on the same eight IndoEuropean languages as #TARGET_REF , so that we can make use of their automatically projected POS tags.",
                "For all languages, we used the treebanks released as a part of the CoNLL-X (#REF) shared task.",
                "We only considered sentences of length 10 or less, after the removal of punctuations."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Additionally, they also used refined categories in the form of CoNLL treebank tags.\n sent1: In our experiments, we did not make use of refined categories, as the POS tags induced by #REF were all coarse.\n sent2: We present results on the same eight IndoEuropean languages as #TARGET_REF , so that we can make use of their automatically projected POS tags.\n sent3: For all languages, we used the treebanks released as a part of the CoNLL-X (#REF) shared task.\n sent4: We only considered sentences of length 10 or less, after the removal of punctuations.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "However, since this rule is reversed for other languages, we omit it in our tagset.",
                "Additionally, they also used refined categories in the form of CoNLL treebank tags.",
                "In our experiments, we did not make use of refined categories, as the POS tags induced by #TARGET_REF were all coarse.",
                "We present results on the same eight IndoEuropean languages as #REF , so that we can make use of their automatically projected POS tags.",
                "For all languages, we used the treebanks released as a part of the CoNLL-X (#REF) shared task."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: However, since this rule is reversed for other languages, we omit it in our tagset.\n sent1: Additionally, they also used refined categories in the form of CoNLL treebank tags.\n sent2: In our experiments, we did not make use of refined categories, as the POS tags induced by #TARGET_REF were all coarse.\n sent3: We present results on the same eight IndoEuropean languages as #REF , so that we can make use of their automatically projected POS tags.\n sent4: For all languages, we used the treebanks released as a part of the CoNLL-X (#REF) shared task.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We provide wrappers around most PyTorch optimizers and an implementation of Adafactor (#REF) , which is a memory-efficient variant of Adam.",
                "Learning Rate Schedulers update the learning rate over the course of training.",
                "We provide several popular schedulers, e.g., the inverse square-root scheduler from #TARGET_REF and cyclical schedulers based on warm restarts (#REF) .",
                "Reproducibility and forward compatibility.",
                "FAIRSEQ includes features designed to improve reproducibility and forward compatibility."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: We provide wrappers around most PyTorch optimizers and an implementation of Adafactor (#REF) , which is a memory-efficient variant of Adam.\n sent1: Learning Rate Schedulers update the learning rate over the course of training.\n sent2: We provide several popular schedulers, e.g., the inverse square-root scheduler from #TARGET_REF and cyclical schedulers based on warm restarts (#REF) .\n sent3: Reproducibility and forward compatibility.\n sent4: FAIRSEQ includes features designed to improve reproducibility and forward compatibility.\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We evaluate a \"big\" Transformer encoderdecoder model on two language pairs, WMT English to German (En-De) and WMT English to French (En-Fr).",
                "For En-De we replicate the setup of #TARGET_REF which relies on WMT'16 for training with 4.5M sentence pairs, we validate on newstest13 and test on newstest14.",
                "The 32K vocabulary is based on a joint source and target byte pair encoding (BPE; #REF ).",
                "For En-Fr, we train on WMT'14 and borrow the setup of #REF with 36M training sentence pairs.",
                "We use newstest12+13 for validation and newstest14 for test."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: We evaluate a \"big\" Transformer encoderdecoder model on two language pairs, WMT English to German (En-De) and WMT English to French (En-Fr).\n sent1: For En-De we replicate the setup of #TARGET_REF which relies on WMT'16 for training with 4.5M sentence pairs, we validate on newstest13 and test on newstest14.\n sent2: The 32K vocabulary is based on a joint source and target byte pair encoding (BPE; #REF ).\n sent3: For En-Fr, we train on WMT'14 and borrow the setup of #REF with 36M training sentence pairs.\n sent4: We use newstest12+13 for validation and newstest14 for test.\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We use newstest12+13 for validation and newstest14 for test.",
                "The 40K vocabulary is based on a joint source and target BPE.",
                "We measure case-sensitive tokenized BLEU with multi-bleu (#REF) and detokenized BLEU with SacreBLEU 1 (#REF) .",
                "All results use beam search with a beam width of 4 and length penalty of 0.6, following #TARGET_REF .",
                "FAIRSEQ results are summarized in Table 2 ."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: We use newstest12+13 for validation and newstest14 for test.\n sent1: The 40K vocabulary is based on a joint source and target BPE.\n sent2: We measure case-sensitive tokenized BLEU with multi-bleu (#REF) and detokenized BLEU with SacreBLEU 1 (#REF) .\n sent3: All results use beam search with a beam width of 4 and length penalty of 0.6, following #TARGET_REF .\n sent4: FAIRSEQ results are summarized in Table 2 .\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "After the FP16 gradients are synchronized between workers, we convert them to FP32, restore the original scale, and update the weights.",
                "Inference.",
                "FAIRSEQ provides fast inference for non-recurrent models (#REF; #TARGET_REF; #REFb; #REF) through incremental decoding, where the model states of previously generated tokens are cached in each active beam and re-used.",
                "This can speed up a naïve implementation without caching by up to an order of magnitude, since only new states are computed for each token.",
                "For some models, this requires a component-specific caching implementation, e.g., multi-head attention in the Transformer architecture."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: After the FP16 gradients are synchronized between workers, we convert them to FP32, restore the original scale, and update the weights.\n sent1: Inference.\n sent2: FAIRSEQ provides fast inference for non-recurrent models (#REF; #TARGET_REF; #REFb; #REF) through incremental decoding, where the model states of previously generated tokens are cached in each active beam and re-used.\n sent3: This can speed up a naïve implementation without caching by up to an order of magnitude, since only new states are computed for each token.\n sent4: For some models, this requires a component-specific caching implementation, e.g., multi-head attention in the Transformer architecture.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We provide reference implementations of several popular sequence-to-sequence models which can be used for machine translation, including LSTM (#REF) , convolutional models (#REF; #REF) and Transformer #TARGET_REF .",
                "We evaluate a \"big\" Transformer encoderdecoder model on two language pairs, WMT English to German (En-De) and WMT English to French (En-Fr).",
                "For En-De we replicate the setup of #REF which relies on WMT'16 for training with 4.5M sentence pairs, we validate on newstest13 and test on newstest14.",
                "The 32K vocabulary is based on a joint source and target byte pair encoding (BPE; #REF ).",
                "For En-Fr, we train on WMT'14 and borrow the setup of #REF with 36M training sentence pairs."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: We provide reference implementations of several popular sequence-to-sequence models which can be used for machine translation, including LSTM (#REF) , convolutional models (#REF; #REF) and Transformer #TARGET_REF .\n sent1: We evaluate a \"big\" Transformer encoderdecoder model on two language pairs, WMT English to German (En-De) and WMT English to French (En-Fr).\n sent2: For En-De we replicate the setup of #REF which relies on WMT'16 for training with 4.5M sentence pairs, we validate on newstest13 and test on newstest14.\n sent3: The 32K vocabulary is based on a joint source and target byte pair encoding (BPE; #REF ).\n sent4: For En-Fr, we train on WMT'14 and borrow the setup of #REF with 36M training sentence pairs.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "FAIRSEQ supports language modeling with gated convolutional models and Transformer models #TARGET_REF .",
                "Models can be trained using a variety of input and output representations, such as standard token embeddings, convolutional character embeddings (Kim 1 En-De En-Fr a. #REF 25.2 40.5 b. #REF 28.4 41.0 c. #REF 28.9 41.4 d. #REF 29 et al., 2016), adaptive softmax (#REF) , and adaptive inputs .",
                "We also provide tutorials and pre-trained models that replicate the results of and"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: FAIRSEQ supports language modeling with gated convolutional models and Transformer models #TARGET_REF .\n sent1: Models can be trained using a variety of input and output representations, such as standard token embeddings, convolutional character embeddings (Kim 1 En-De En-Fr a. #REF 25.2 40.5 b. #REF 28.4 41.0 c. #REF 28.9 41.4 d. #REF 29 et al., 2016), adaptive softmax (#REF) , and adaptive inputs .\n sent2: We also provide tutorials and pre-trained models that replicate the results of and\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "FAIRSEQ supports language modeling with gated convolutional models and Transformer models (#REF) .",
                "Models can be trained using a variety of input and output representations, such as standard token embeddings, convolutional character embeddings (Kim 1 En-De En-Fr a. #REF 25.2 40.5 b. #TARGET_REF 28.4 41.0 c. #REF 28.9 41.4 d. #REF 29 et al., 2016), adaptive softmax (#REF) , and adaptive inputs .",
                "We also provide tutorials and pre-trained models that replicate the results of and"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: FAIRSEQ supports language modeling with gated convolutional models and Transformer models (#REF) .\n sent1: Models can be trained using a variety of input and output representations, such as standard token embeddings, convolutional character embeddings (Kim 1 En-De En-Fr a. #REF 25.2 40.5 b. #TARGET_REF 28.4 41.0 c. #REF 28.9 41.4 d. #REF 29 et al., 2016), adaptive softmax (#REF) , and adaptive inputs .\n sent2: We also provide tutorials and pre-trained models that replicate the results of and\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The 40K vocabulary is based on a joint source and target BPE.",
                "We measure case-sensitive tokenized BLEU with multi-bleu (#REF) and detokenized BLEU with SacreBLEU 1 (#REF) .",
                "All results use beam search with a beam width of 4 and length penalty of 0.6, following #REF .",
                "FAIRSEQ results are summarized in Table 2 .",
                "We reported improved BLEU scores over #TARGET_REF by training with a bigger batch size and an increased learning rate ."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: The 40K vocabulary is based on a joint source and target BPE.\n sent1: We measure case-sensitive tokenized BLEU with multi-bleu (#REF) and detokenized BLEU with SacreBLEU 1 (#REF) .\n sent2: All results use beam search with a beam width of 4 and length penalty of 0.6, following #REF .\n sent3: FAIRSEQ results are summarized in Table 2 .\n sent4: We reported improved BLEU scores over #TARGET_REF by training with a bigger batch size and an increased learning rate .\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Section 3 then empirically analyzes correlations in two recent argument corpora, one annotated for 15 well-defined quality dimensions taken from theory (#REFa) and one with 17 reasons for quality differences phrased spontaneously in practice #TARGET_REF .",
                "In a crowdsourcing study, we test whether lay annotators achieve agreement on the theoretical quality dimensions (Section 4).",
                "We find that assessments of overall argumentation quality largely match in theory and practice.",
                "Nearly all phrased reasons are adequately represented in theory.",
                "However, some theoretical quality dimensions seem hard to separate in practice."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Section 3 then empirically analyzes correlations in two recent argument corpora, one annotated for 15 well-defined quality dimensions taken from theory (#REFa) and one with 17 reasons for quality differences phrased spontaneously in practice #TARGET_REF .\n sent1: In a crowdsourcing study, we test whether lay annotators achieve agreement on the theoretical quality dimensions (Section 4).\n sent2: We find that assessments of overall argumentation quality largely match in theory and practice.\n sent3: Nearly all phrased reasons are adequately represented in theory.\n sent4: However, some theoretical quality dimensions seem hard to separate in practice.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Conv A is more convincing than B. Table 2 : The 17+1 practical reason labels given in the corpus of #TARGET_REF .",
                "covered.",
                "In Section 3, we use their absolute quality ratings from 1 (low) to 3 (high) annotated by three experts for each dimension of 304 arguments taken from the UKPConvArg1 corpus detailed below."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Conv A is more convincing than B. Table 2 : The 17+1 practical reason labels given in the corpus of #TARGET_REF .\n sent1: covered.\n sent2: In Section 3, we use their absolute quality ratings from 1 (low) to 3 (high) annotated by three experts for each dimension of 304 arguments taken from the UKPConvArg1 corpus detailed below.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF argue that normative concepts such as fallacies rarely apply to real-life arguments and that they are too sophisticated for operationalization.",
                "Based on the idea that relative assessment is easier, Habernal and Gurevych (2016b) crowdsourced the UKPConvArg1 corpus.",
                "Argument pairs (A, B) from a debate portal were classified as to which argument is more convincing.",
                "Without giving any guidelines, the authors also asked for reasons as to why A is more convincing than B. In a follow-up study #TARGET_REF , these reasons were used to derive a hierarchical annotation scheme.",
                "9111 argument pairs were then labeled with one or more of the 17 reason labels in Table 2 Negative Properties of Argument B Positive Properties of Argument A Quality Dimension 5-1 5-2 5-3 6-1 6-2 6-3 7-1 7-2 7-3 7-4 8-1 8-4 8-5 9-1 9-2 9-3 9- Wachsmuth et al. (2017a) given for each of the 17+1 reason labels of Habernal and Gurevych (2016a) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #REF argue that normative concepts such as fallacies rarely apply to real-life arguments and that they are too sophisticated for operationalization.\n sent1: Based on the idea that relative assessment is easier, Habernal and Gurevych (2016b) crowdsourced the UKPConvArg1 corpus.\n sent2: Argument pairs (A, B) from a debate portal were classified as to which argument is more convincing.\n sent3: Without giving any guidelines, the authors also asked for reasons as to why A is more convincing than B. In a follow-up study #TARGET_REF , these reasons were used to derive a hierarchical annotation scheme.\n sent4: 9111 argument pairs were then labeled with one or more of the 17 reason labels in Table 2 Negative Properties of Argument B Positive Properties of Argument A Quality Dimension 5-1 5-2 5-3 6-1 6-2 6-3 7-1 7-2 7-3 7-4 8-1 8-4 8-5 9-1 9-2 9-3 9- Wachsmuth et al. (2017a) given for each of the 17+1 reason labels of Habernal and Gurevych (2016a) .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Without giving any guidelines, the authors also asked for reasons as to why A is more convincing than B. In a follow-up study (#REFa) , these reasons were used to derive a hierarchical annotation scheme.",
                "9111 argument pairs were then labeled with one or more of the 17 reason labels in Table 2 Negative Properties of Argument B Positive Properties of Argument A Quality Dimension 5-1 5-2 5-3 6-1 6-2 6-3 7-1 7-2 7-3 7-4 8-1 8-4 8-5 9-1 9-2 9-3 9- Wachsmuth et al. (2017a) given for each of the 17+1 reason labels of #TARGET_REF .",
                "Bold/gray: Highest/lowest value in each column.",
                "Bottom row: The number of labels for each dimension.",
                "by crowd workers (UKPConvArg2)."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Without giving any guidelines, the authors also asked for reasons as to why A is more convincing than B. In a follow-up study (#REFa) , these reasons were used to derive a hierarchical annotation scheme.\n sent1: 9111 argument pairs were then labeled with one or more of the 17 reason labels in Table 2 Negative Properties of Argument B Positive Properties of Argument A Quality Dimension 5-1 5-2 5-3 6-1 6-2 6-3 7-1 7-2 7-3 7-4 8-1 8-4 8-5 9-1 9-2 9-3 9- Wachsmuth et al. (2017a) given for each of the 17+1 reason labels of #TARGET_REF .\n sent2: Bold/gray: Highest/lowest value in each column.\n sent3: Bottom row: The number of labels for each dimension.\n sent4: by crowd workers (UKPConvArg2).\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The high τ 's of 8-5 (more credible) for local acceptability (.73) and of 9-4 (well thought through) for cogency (.75) confirm the match assumed in Section 1. Also, the values of 5-3 (unclear) for clarity (.91) and of 7-2 (non-sense) for reasonableness (.94) as well as the weaker correlation of 8-4 (objective) for emotional appeal (.35) makes sense.",
                "Only the comparably low τ of 6-1 (no credible evidence) for local acceptability (.49) and credibility (.52) seem really unexpected.",
                "Besides, the descriptions of 6-2 and 6-3 sound like local but cor- Table 4 : The mean rating for each quality dimension of those arguments from Wachsmuth et al. (2017a) given for each reason label #TARGET_REF .",
                "The bottom rows show that the minimum maximum mean ratings are consistently higher for the positive properties than for the negative properties.",
                "relate more with global relevance and sufficiency respectively."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The high τ 's of 8-5 (more credible) for local acceptability (.73) and of 9-4 (well thought through) for cogency (.75) confirm the match assumed in Section 1. Also, the values of 5-3 (unclear) for clarity (.91) and of 7-2 (non-sense) for reasonableness (.94) as well as the weaker correlation of 8-4 (objective) for emotional appeal (.35) makes sense.\n sent1: Only the comparably low τ of 6-1 (no credible evidence) for local acceptability (.49) and credibility (.52) seem really unexpected.\n sent2: Besides, the descriptions of 6-2 and 6-3 sound like local but cor- Table 4 : The mean rating for each quality dimension of those arguments from Wachsmuth et al. (2017a) given for each reason label #TARGET_REF .\n sent3: The bottom rows show that the minimum maximum mean ratings are consistently higher for the positive properties than for the negative properties.\n sent4: relate more with global relevance and sufficiency respectively.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Table 4 compares the mean ratings of \"negative labels\" (5-1 to 7-4) and \"positive\" ones (8-1 to 9-4).",
                "For all dimensions, the maximum and minimum value are higher for the positive than for the negative labels-a clear support of Hypothesis 3.",
                "3 Also, Table 4 reveals which reasons predict absolute differences most: The mean ratings of 7-3 (off-topic) are very low, indicating a strong negative impact, while 6-3 (irrelevant reasons) still shows rather 3 While the differences seem not very large, this is expected, as in many argument pairs from #TARGET_REF both arguments are strong or weak respectively.",
                "high values.",
                "Vice versa, especially 8-5 (more credible) and 9-4 (well thought through) are reflected in high ratings, whereas 9-2 (sticks to topic) does not have much positive impact."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Table 4 compares the mean ratings of \"negative labels\" (5-1 to 7-4) and \"positive\" ones (8-1 to 9-4).\n sent1: For all dimensions, the maximum and minimum value are higher for the positive than for the negative labels-a clear support of Hypothesis 3.\n sent2: 3 Also, Table 4 reveals which reasons predict absolute differences most: The mean ratings of 7-3 (off-topic) are very low, indicating a strong negative impact, while 6-3 (irrelevant reasons) still shows rather 3 While the differences seem not very large, this is expected, as in many argument pairs from #TARGET_REF both arguments are strong or weak respectively.\n sent3: high values.\n sent4: Vice versa, especially 8-5 (more credible) and 9-4 (well thought through) are reflected in high ratings, whereas 9-2 (sticks to topic) does not have much positive impact.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Also, global sufficiency has the lowest agreement in both cases.",
                "In contrast, the experts hardly said \"cannot judge\" at all, whereas the crowd chose it for about 4% of all ratings (most often for global sufficiency), possibly due to a lack of training.",
                "Still, we conclude that the crowd generally handles the theory-based quality assessment almost as well as the experts.",
                "However, the complexity of the assessment is underlined by the generally limited agreement, suggesting that either simplification or stricter guidelines are needed.",
                "Regarding simplification, the most common practical reasons of #TARGET_REF imply what to focus on."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Also, global sufficiency has the lowest agreement in both cases.\n sent1: In contrast, the experts hardly said \"cannot judge\" at all, whereas the crowd chose it for about 4% of all ratings (most often for global sufficiency), possibly due to a lack of training.\n sent2: Still, we conclude that the crowd generally handles the theory-based quality assessment almost as well as the experts.\n sent3: However, the complexity of the assessment is underlined by the generally limited agreement, suggesting that either simplification or stricter guidelines are needed.\n sent4: Regarding simplification, the most common practical reasons of #TARGET_REF imply what to focus on.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "For Hypotheses 1 and 2, we consider all 736 pairs of arguments from #TARGET_REF where both have been annotated by Wachsmuth et al. (2017a) .",
                "For each pair (A, B) with A being 1 Source code and annotated data: http://www.arguana.com more convincing than B, we check whether the ratings of A and B for each dimension (averaged over all annotators) show a concordant difference (i.e., a higher rating for A), a disconcordant difference (lower), or a tie.",
                "This way, we can correlate each dimension with all reason labels in Table 2 including Conv.",
                "In particular, we compute Kendall's τ based on all argument pairs given for each label.",
                "2  Table 3 presents all τ -values."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: For Hypotheses 1 and 2, we consider all 736 pairs of arguments from #TARGET_REF where both have been annotated by Wachsmuth et al. (2017a) .\n sent1: For each pair (A, B) with A being 1 Source code and annotated data: http://www.arguana.com more convincing than B, we check whether the ratings of A and B for each dimension (averaged over all annotators) show a concordant difference (i.e., a higher rating for A), a disconcordant difference (lower), or a tie.\n sent2: This way, we can correlate each dimension with all reason labels in Table 2 including Conv.\n sent3: In particular, we compute Kendall's τ based on all argument pairs given for each label.\n sent4: 2  Table 3 presents all τ -values.\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "The correlations found imply that the relative quality differences captured are reflected in absolute differences.",
                "For explicitness, we computed the mean rating for each quality dimension of all arguments from Wachsmuth et al. (2017a) with a particular reason label from #TARGET_REF .",
                "As each reason refers to one argument of a pair, this reveals whether the labels, although meant to signal relative differences, indicate absolute ratings.",
                "Table 4 compares the mean ratings of \"negative labels\" (5-1 to 7-4) and \"positive\" ones (8-1 to 9-4).",
                "For all dimensions, the maximum and minimum value are higher for the positive than for the negative labels-a clear support of Hypothesis 3."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: The correlations found imply that the relative quality differences captured are reflected in absolute differences.\n sent1: For explicitness, we computed the mean rating for each quality dimension of all arguments from Wachsmuth et al. (2017a) with a particular reason label from #TARGET_REF .\n sent2: As each reason refers to one argument of a pair, this reveals whether the labels, although meant to signal relative differences, indicate absolute ratings.\n sent3: Table 4 compares the mean ratings of \"negative labels\" (5-1 to 7-4) and \"positive\" ones (8-1 to 9-4).\n sent4: For all dimensions, the maximum and minimum value are higher for the positive than for the negative labels-a clear support of Hypothesis 3.\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Abstract.",
                "Predicting mental health from smartphone and social media data on a longitudinal basis has recently attracted great interest, with very promising results being reported across many studies [3, 9, 13, #TARGET_REF .",
                "Such approaches have the potential to revolutionise mental health assessment, if their development and evaluation follows a real world deployment setting.",
                "In this work we take a closer look at state-of-the-art approaches, using different mental health datasets and indicators, different feature sources and multiple simulations, in order to assess their ability to generalise.",
                "We demonstrate that under a pragmatic evaluation framework, none of the approaches deliver or even approach the reported performances."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Abstract.\n sent1: Predicting mental health from smartphone and social media data on a longitudinal basis has recently attracted great interest, with very promising results being reported across many studies [3, 9, 13, #TARGET_REF .\n sent2: Such approaches have the potential to revolutionise mental health assessment, if their development and evaluation follows a real world deployment setting.\n sent3: In this work we take a closer look at state-of-the-art approaches, using different mental health datasets and indicators, different feature sources and multiple simulations, in order to assess their ability to generalise.\n sent4: We demonstrate that under a pragmatic evaluation framework, none of the approaches deliver or even approach the reported performances.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Establishing the right indicators of mental well-being is a grand challenge posed by the World Health #REF .",
                "Poor mental health is highly correlated with low motivation, lack of satisfaction, low productivity and a negative economic impact [20] .",
                "The current approach is to combine census data at the population level [19] , thus failing to capture well-being on an individual basis.",
                "The latter is only possible via self-reporting on the basis of established psychological scales, which are hard to acquire consistently on a longitudinal basis, and they capture long-term aggregates instead of the current state of the individual.",
                "The widespread use of smart-phones and social media offers new ways of assessing mental well-being, and recent research [1, 2, 3, 5, 9, 10, 13, 14, 22, 23, #TARGET_REF has started exploring the effectiveness of these modalities for automatically assessing"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Establishing the right indicators of mental well-being is a grand challenge posed by the World Health #REF .\n sent1: Poor mental health is highly correlated with low motivation, lack of satisfaction, low productivity and a negative economic impact [20] .\n sent2: The current approach is to combine census data at the population level [19] , thus failing to capture well-being on an individual basis.\n sent3: The latter is only possible via self-reporting on the basis of established psychological scales, which are hard to acquire consistently on a longitudinal basis, and they capture long-term aggregates instead of the current state of the individual.\n sent4: The widespread use of smart-phones and social media offers new ways of assessing mental well-being, and recent research [1, 2, 3, 5, 9, 10, 13, 14, 22, 23, #TARGET_REF has started exploring the effectiveness of these modalities for automatically assessing\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "However, due to the small number of subjects that are available, such generalisation is hard to achieve by any approach [13] .",
                "Alternatively, personalised models [3, 13] for every individual can be evaluated via a within-subject, leave-N-instances-out cross-validation (for N=1, LOIOCV), where an instance for a user u at time i is defined as a {X ui , y ui } tuple of {features(u, i), mental-health-score(u, i)}. In a real world setting, a LOIOCV model is trained on some user-specific instances, aiming to predict her mental health state at some future time points.",
                "Again however, the limited number of instances for every user make such models unable to generalize well.",
                "In order to overcome these issues, previous work [2, 5, 9, 10, 22, #TARGET_REF has combined the instances {X uj i , y uj i } from different individuals u j and performed evaluation using randomised cross validation (MIXED).",
                "While such approaches can attain optimistic performance, the corresponding models fail to generalise to the general population and also fail to ensure effective personalised assessment of the mental health state of a single individual."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: However, due to the small number of subjects that are available, such generalisation is hard to achieve by any approach [13] .\n sent1: Alternatively, personalised models [3, 13] for every individual can be evaluated via a within-subject, leave-N-instances-out cross-validation (for N=1, LOIOCV), where an instance for a user u at time i is defined as a {X ui , y ui } tuple of {features(u, i), mental-health-score(u, i)}. In a real world setting, a LOIOCV model is trained on some user-specific instances, aiming to predict her mental health state at some future time points.\n sent2: Again however, the limited number of instances for every user make such models unable to generalize well.\n sent3: In order to overcome these issues, previous work [2, 5, 9, 10, 22, #TARGET_REF has combined the instances {X uj i , y uj i } from different individuals u j and performed evaluation using randomised cross validation (MIXED).\n sent4: While such approaches can attain optimistic performance, the corresponding models fail to generalise to the general population and also fail to ensure effective personalised assessment of the mental health state of a single individual.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Research in assessing mental health on a longitudinal basis aims to make use of relevant features extracted from various modalities, in order to train models for automatically predicting a user's mental state (target), either in a classification or a regression manner [1, 2, 3, 9, 10, 13, #TARGET_REF .",
                "Examples of state-of-the-art work in this domain are listed in Table 2 , along with the number of subjects that was used and the method upon which evaluation took place.",
                "Most approaches have used the \"MIXED\" approach to evaluate models [1, 2, 5, 9, 10, 22, 26] , which, as we will show, is vulnerable to bias, due to the danger of recognising the user in the test set and thus simply inferring her average mood score.",
                "LOIOCV approaches that have not ensured that their train/test sets are independent are also vulnerable to bias in a realistic setting [3, 13] .",
                "From the works listed in Table 2 , only #REF achieves unbiased results with respect to model generalisability; however, the features employed for their prediction task are derived from self-reported questionnaires of the subjects and not by automatic means."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Research in assessing mental health on a longitudinal basis aims to make use of relevant features extracted from various modalities, in order to train models for automatically predicting a user's mental state (target), either in a classification or a regression manner [1, 2, 3, 9, 10, 13, #TARGET_REF .\n sent1: Examples of state-of-the-art work in this domain are listed in Table 2 , along with the number of subjects that was used and the method upon which evaluation took place.\n sent2: Most approaches have used the \"MIXED\" approach to evaluate models [1, 2, 5, 9, 10, 22, 26] , which, as we will show, is vulnerable to bias, due to the danger of recognising the user in the test set and thus simply inferring her average mood score.\n sent3: LOIOCV approaches that have not ensured that their train/test sets are independent are also vulnerable to bias in a realistic setting [3, 13] .\n sent4: From the works listed in Table 2 , only #REF achieves unbiased results with respect to model generalisability; however, the features employed for their prediction task are derived from self-reported questionnaires of the subjects and not by automatic means.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Research in assessing mental health on a longitudinal basis aims to make use of relevant features extracted from various modalities, in order to train models for automatically predicting a user's mental state (target), either in a classification or a regression manner [1, 2, 3, 9, 10, 13, 26] .",
                "Examples of state-of-the-art work in this domain are listed in Table 2 , along with the number of subjects that was used and the method upon which evaluation took place.",
                "Most approaches have used the \"MIXED\" approach to evaluate models [1, 2, 5, 9, 10, 22, #TARGET_REF , which, as we will show, is vulnerable to bias, due to the danger of recognising the user in the test set and thus simply inferring her average mood score.",
                "LOIOCV approaches that have not ensured that their train/test sets are independent are also vulnerable to bias in a realistic setting [3, 13] .",
                "From the works listed in Table 2 , only #REF achieves unbiased results with respect to model generalisability; however, the features employed for their prediction task are derived from self-reported questionnaires of the subjects and not by automatic means."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Research in assessing mental health on a longitudinal basis aims to make use of relevant features extracted from various modalities, in order to train models for automatically predicting a user's mental state (target), either in a classification or a regression manner [1, 2, 3, 9, 10, 13, 26] .\n sent1: Examples of state-of-the-art work in this domain are listed in Table 2 , along with the number of subjects that was used and the method upon which evaluation took place.\n sent2: Most approaches have used the \"MIXED\" approach to evaluate models [1, 2, 5, 9, 10, 22, #TARGET_REF , which, as we will show, is vulnerable to bias, due to the danger of recognising the user in the test set and thus simply inferring her average mood score.\n sent3: LOIOCV approaches that have not ensured that their train/test sets are independent are also vulnerable to bias in a realistic setting [3, 13] .\n sent4: From the works listed in Table 2 , only #REF achieves unbiased results with respect to model generalisability; however, the features employed for their prediction task are derived from self-reported questionnaires of the subjects and not by automatic means.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Some past works have extracted features {f (t − N ), ..., f (t)} over N days, in order to predict the score t on day N + 1 [3, 13] .",
                "Such approaches are biased if there are overlapping days of train/test data.",
                "To illustrate this problem we have followed the approach by #REF , as one of the pioneering works on predicting depression with GPS traces, on a longitudinal basis.",
                "P3 Predicting users instead of mood scores: Most approaches merge all the instances from different subjects, in an attempt to build user-agnostic models in a randomised cross-validation framework [2, 9, 10, #TARGET_REF .",
                "This is problematic, especially when dealing with a small number of subjects, whose behaviour (as captured through their data) and mental health scores differ on an individual basis."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Some past works have extracted features {f (t − N ), ..., f (t)} over N days, in order to predict the score t on day N + 1 [3, 13] .\n sent1: Such approaches are biased if there are overlapping days of train/test data.\n sent2: To illustrate this problem we have followed the approach by #REF , as one of the pioneering works on predicting depression with GPS traces, on a longitudinal basis.\n sent3: P3 Predicting users instead of mood scores: Most approaches merge all the instances from different subjects, in an attempt to build user-agnostic models in a randomised cross-validation framework [2, 9, 10, #TARGET_REF .\n sent4: This is problematic, especially when dealing with a small number of subjects, whose behaviour (as captured through their data) and mental health scores differ on an individual basis.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Tsakalidis et al. #TARGET_REF monitored the behaviour of 19 individuals over four months.",
                "The subjects were asked to complete two psychological scales [25, 29] on a daily basis, leading to three target scores (positive, negative, mental well-being); various features from smartphones (e.g., time spent on the preferred locations) and textual features (e.g., ngrams) were extracted passively over the 24 hours preceding a mood form timestamp.",
                "Model training and evaluation was performed in a randomised (MIXED) cross-validation setup, leading to high accuracy (R 2 = 0.76).",
                "However, a case demonstrating the potential user bias is when the models are trained on the textual sources: initially the highest R 2 (0.22) is achieved when a model is applied to the mental-wellbeing target; by normalising the textual features on a per-user basis, the R 2 increases to 0.65.",
                "While this is likely to happen because the vocabulary used by different users is normalised, there is also the danger of over-fitting the trained model to the identity of the user."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Tsakalidis et al. #TARGET_REF monitored the behaviour of 19 individuals over four months.\n sent1: The subjects were asked to complete two psychological scales [25, 29] on a daily basis, leading to three target scores (positive, negative, mental well-being); various features from smartphones (e.g., time spent on the preferred locations) and textual features (e.g., ngrams) were extracted passively over the 24 hours preceding a mood form timestamp.\n sent2: Model training and evaluation was performed in a randomised (MIXED) cross-validation setup, leading to high accuracy (R 2 = 0.76).\n sent3: However, a case demonstrating the potential user bias is when the models are trained on the textual sources: initially the highest R 2 (0.22) is achieved when a model is applied to the mental-wellbeing target; by normalising the textual features on a per-user basis, the R 2 increases to 0.65.\n sent4: While this is likely to happen because the vocabulary used by different users is normalised, there is also the danger of over-fitting the trained model to the identity of the user.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Self-reported scores on a daily basis served as the ground truth.",
                "The authors labelled the instances with the top 30% of all the scores as \"happy\" and the lowest 30% as \"sad\" and randomly separated them into training, validation and test sets, leading to the same user bias issue.",
                "Since different users exhibit different mood scores on average #TARGET_REF , by selecting instances from the top and bottom scores, one might end up separating users and convert the mood prediction task into a user identification one.",
                "A more suitable task could have been to try to predict the highest and lowest scores of every individual separately, either in a LOIOCV or in a LOUOCV setup.",
                "While we focus on the works of #REF and #REF , similar experimental setups were also followed in [10] , using the median of scores to separate the instances and performing five-fold cross-validation, and by Bogomolov et al. in [2] , working on a user-agnostic validation setting on 117 subjects to predict their happiness levels, and in [1] , for the stress level classification task."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Self-reported scores on a daily basis served as the ground truth.\n sent1: The authors labelled the instances with the top 30% of all the scores as \"happy\" and the lowest 30% as \"sad\" and randomly separated them into training, validation and test sets, leading to the same user bias issue.\n sent2: Since different users exhibit different mood scores on average #TARGET_REF , by selecting instances from the top and bottom scores, one might end up separating users and convert the mood prediction task into a user identification one.\n sent3: A more suitable task could have been to try to predict the highest and lowest scores of every individual separately, either in a LOIOCV or in a LOUOCV setup.\n sent4: While we focus on the works of #REF and #REF , similar experimental setups were also followed in [10] , using the median of scores to separate the instances and performing five-fold cross-validation, and by Bogomolov et al. in [2] , working on a user-agnostic validation setting on 117 subjects to predict their happiness levels, and in [1] , for the stress level classification task.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "P3 Predicting users instead of mood scores: Most approaches merge all the instances from different subjects, in an attempt to build user-agnostic models in a randomised cross-validation framework [2, 9, 10, 26] .",
                "This is problematic, especially when dealing with a small number of subjects, whose behaviour (as captured through their data) and mental health scores differ on an individual basis.",
                "Such approaches are in danger of \"predicting\" the user in the test set, since her (test set) features might be highly correlated with her features in the training set, and thus infer her average well-being score, based on the corresponding observations of the training set.",
                "Such approaches cannot guarantee that they will generalise on either a population-wide (LOUOCV ) or a personalised (LOIOCV ) level.",
                "In order to examine this effect in both a regression and a classification setting, we have followed the experimental framework by Tsakalidis et al. #TARGET_REF and #REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: P3 Predicting users instead of mood scores: Most approaches merge all the instances from different subjects, in an attempt to build user-agnostic models in a randomised cross-validation framework [2, 9, 10, 26] .\n sent1: This is problematic, especially when dealing with a small number of subjects, whose behaviour (as captured through their data) and mental health scores differ on an individual basis.\n sent2: Such approaches are in danger of \"predicting\" the user in the test set, since her (test set) features might be highly correlated with her features in the training set, and thus infer her average well-being score, based on the corresponding observations of the training set.\n sent3: Such approaches cannot guarantee that they will generalise on either a population-wide (LOUOCV ) or a personalised (LOIOCV ) level.\n sent4: In order to examine this effect in both a regression and a classification setting, we have followed the experimental framework by Tsakalidis et al. #TARGET_REF and #REF .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "We employed the dataset obtained by Tsakalidis et al. #TARGET_REF , a pioneering dataset which contains a mix of longitudinal textual and mobile phone usage data for 30 subjects.",
                "From a textual perspective, this dataset consists of social media posts (1,854/5,167 facebook/twitter posts) and private messages (64,221/132/47,043 facebook/twitter/ SMS messages) sent by the subjects.",
                "For our ground truth, we use the {positive, negative, mental well-being} mood scores (in the ranges of , , , respectively) derived from self-assessed psychological scales during the study period."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We employed the dataset obtained by Tsakalidis et al. #TARGET_REF , a pioneering dataset which contains a mix of longitudinal textual and mobile phone usage data for 30 subjects.\n sent1: From a textual perspective, this dataset consists of social media posts (1,854/5,167 facebook/twitter posts) and private messages (64,221/132/47,043 facebook/twitter/ SMS messages) sent by the subjects.\n sent2: For our ground truth, we use the {positive, negative, mental well-being} mood scores (in the ranges of , , , respectively) derived from self-assessed psychological scales during the study period.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "The features of every instance are extracted from the past day before the completion of a mood form.",
                "In Experiment 1 we follow the setup in #TARGET_REF : we perform 5-fold CV (MIXED) using SVM (SVR RBF ) and evaluate performance based on R 2 and RM SE.",
                "We compare the performance when tested under the LOIOCV /LOUOCV setups, with and without the per-user feature normalisation step.",
                "We also compare the performance of the MIXED setting, when our model is trained on the one-hot-encoded user id only.",
                "In Experiment 2 we follow the setup in [9] : we label the instances as \"high\" (\"low\"), if they belong to the top-30% (bottom-30%) of mood score values (\"UNIQ\" -for \"unique\" -setup)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The features of every instance are extracted from the past day before the completion of a mood form.\n sent1: In Experiment 1 we follow the setup in #TARGET_REF : we perform 5-fold CV (MIXED) using SVM (SVR RBF ) and evaluate performance based on R 2 and RM SE.\n sent2: We compare the performance when tested under the LOIOCV /LOUOCV setups, with and without the per-user feature normalisation step.\n sent3: We also compare the performance of the MIXED setting, when our model is trained on the one-hot-encoded user id only.\n sent4: In Experiment 2 we follow the setup in [9] : we label the instances as \"high\" (\"low\"), if they belong to the top-30% (bottom-30%) of mood score values (\"UNIQ\" -for \"unique\" -setup).\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Experiment 1: Table 7 shows the results based on the evaluation setup of Tsakalidis et al. #TARGET_REF .",
                "In the MIXED cases, the pattern is consistent with [26] , indicating that normalising the features on a per-user basis yields better results, when dealing with sparse textual features (positive, negative, wellbeing targets).",
                "The explanation of this effect lies within the danger of predicting the user's identity instead of her mood scores.",
                "This is why the per-user normalisation does not have any effect for the stress target, since for that we are using dense features derived from smartphones: the vocabulary used by the subjects for the other targets is more indicative of their identity.",
                "In order to further support this statement, we trained the SVR model using only the one-hot encoded user id as a feature, without any textual features."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Experiment 1: Table 7 shows the results based on the evaluation setup of Tsakalidis et al. #TARGET_REF .\n sent1: In the MIXED cases, the pattern is consistent with [26] , indicating that normalising the features on a per-user basis yields better results, when dealing with sparse textual features (positive, negative, wellbeing targets).\n sent2: The explanation of this effect lies within the danger of predicting the user's identity instead of her mood scores.\n sent3: This is why the per-user normalisation does not have any effect for the stress target, since for that we are using dense features derived from smartphones: the vocabulary used by the subjects for the other targets is more indicative of their identity.\n sent4: In order to further support this statement, we trained the SVR model using only the one-hot encoded user id as a feature, without any textual features.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Thus, the effect of the features in this setting cannot be assessed with certainty.",
                "Table 7 .",
                "P3: Results following the evaluation setup in #TARGET_REF (MIXED), along with the results obtained in the LOIOCV and LOUOCV settings with (+) and without (-) per-user input normalisation.",
                "Table 8 displays our results based on #REF (see section 3.3).",
                "The average accuracy on the \"UNIQ\" setup is higher by 14% compared to the majority classifier in MIXED."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Thus, the effect of the features in this setting cannot be assessed with certainty.\n sent1: Table 7 .\n sent2: P3: Results following the evaluation setup in #TARGET_REF (MIXED), along with the results obtained in the LOIOCV and LOUOCV settings with (+) and without (-) per-user input normalisation.\n sent3: Table 8 displays our results based on #REF (see section 3.3).\n sent4: The average accuracy on the \"UNIQ\" setup is higher by 14% compared to the majority classifier in MIXED.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Self-reported scores on a daily basis served as the ground truth.",
                "The authors labelled the instances with the top 30% of all the scores as \"happy\" and the lowest 30% as \"sad\" and randomly separated them into training, validation and test sets, leading to the same user bias issue.",
                "Since different users exhibit different mood scores on average [26] , by selecting instances from the top and bottom scores, one might end up separating users and convert the mood prediction task into a user identification one.",
                "A more suitable task could have been to try to predict the highest and lowest scores of every individual separately, either in a LOIOCV or in a LOUOCV setup.",
                "While we focus on the works of Tsakalidis et al. #TARGET_REF and #REF , similar experimental setups were also followed in [10] , using the median of scores to separate the instances and performing five-fold cross-validation, and by Bogomolov et al. in [2] , working on a user-agnostic validation setting on 117 subjects to predict their happiness levels, and in [1] , for the stress level classification task."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: Self-reported scores on a daily basis served as the ground truth.\n sent1: The authors labelled the instances with the top 30% of all the scores as \"happy\" and the lowest 30% as \"sad\" and randomly separated them into training, validation and test sets, leading to the same user bias issue.\n sent2: Since different users exhibit different mood scores on average [26] , by selecting instances from the top and bottom scores, one might end up separating users and convert the mood prediction task into a user identification one.\n sent3: A more suitable task could have been to try to predict the highest and lowest scores of every individual separately, either in a LOIOCV or in a LOUOCV setup.\n sent4: While we focus on the works of Tsakalidis et al. #TARGET_REF and #REF , similar experimental setups were also followed in [10] , using the median of scores to separate the instances and performing five-fold cross-validation, and by Bogomolov et al. in [2] , working on a user-agnostic validation setting on 117 subjects to predict their happiness levels, and in [1] , for the stress level classification task.\n",
        "output": "{\"label\": [null], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "For Dataset 1, we first defined a \"user snippet\" as the concatenation of all texts generated by a user within a set time interval, such that the maximum time difference between two consecutive document timestamps is less than 20 minutes.",
                "We performed some standard noise reduction steps (converted text to lowercase, replaced URLs/user mentions and performed language identification 7 and tokenisation [6] ).",
                "Given a mood form and a set of snippets produced by a user before the completion of a mood form, we extracted some commonly used feature sets for every snippet written in English #TARGET_REF , which were used in all experiments.",
                "To ensure sufficient data density, we excluded users for whom we had overall fewer than 25 snippets on the days before the completion of the mood form or fewer than 40 mood forms overall, leading to 27 users and 2, 368 mood forms.",
                "For Dataset 2, we extracted the features presented in Table 4 ."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: For Dataset 1, we first defined a \"user snippet\" as the concatenation of all texts generated by a user within a set time interval, such that the maximum time difference between two consecutive document timestamps is less than 20 minutes.\n sent1: We performed some standard noise reduction steps (converted text to lowercase, replaced URLs/user mentions and performed language identification 7 and tokenisation [6] ).\n sent2: Given a mood form and a set of snippets produced by a user before the completion of a mood form, we extracted some commonly used feature sets for every snippet written in English #TARGET_REF , which were used in all experiments.\n sent3: To ensure sufficient data density, we excluded users for whom we had overall fewer than 25 snippets on the days before the completion of the mood form or fewer than 40 mood forms overall, leading to 27 users and 2, 368 mood forms.\n sent4: For Dataset 2, we extracted the features presented in Table 4 .\n",
        "output": "{\"label\": [null], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Experiment 1: Table 7 shows the results based on the evaluation setup of #REF .",
                "In the MIXED cases, the pattern is consistent with #TARGET_REF , indicating that normalising the features on a per-user basis yields better results, when dealing with sparse textual features (positive, negative, wellbeing targets).",
                "The explanation of this effect lies within the danger of predicting the user's identity instead of her mood scores.",
                "This is why the per-user normalisation does not have any effect for the stress target, since for that we are using dense features derived from smartphones: the vocabulary used by the subjects for the other targets is more indicative of their identity.",
                "In order to further support this statement, we trained the SVR model using only the one-hot encoded user id as a feature, without any textual features."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: Experiment 1: Table 7 shows the results based on the evaluation setup of #REF .\n sent1: In the MIXED cases, the pattern is consistent with #TARGET_REF , indicating that normalising the features on a per-user basis yields better results, when dealing with sparse textual features (positive, negative, wellbeing targets).\n sent2: The explanation of this effect lies within the danger of predicting the user's identity instead of her mood scores.\n sent3: This is why the per-user normalisation does not have any effect for the stress target, since for that we are using dense features derived from smartphones: the vocabulary used by the subjects for the other targets is more indicative of their identity.\n sent4: In order to further support this statement, we trained the SVR model using only the one-hot encoded user id as a feature, without any textual features.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Automatically learning dialogue structure from corpora is an active area of research driven by a recognition of the value offered by data-driven approaches (e.g., #REF) .",
                "Dialogue structure information is of particular importance when the interaction is centered around a learning task, such as in natural language tutoring, because techniques that support empirical identification of dialogue strategies can inform not only the design of intelligent tutoring systems #TARGET_REF , but also contribute to our understanding of the cognitive and affective processes involved in learning through tutoring (#REF) .",
                "Although traditional top-down approaches (e.g., #REF) and some empirical work on analyzing the structure of tutorial dialogue (#REF) have yielded significant results, the field is limited by the lack of an automatic, data-driven approach to identifying dialogue structure.",
                "An empirical approach to identifying tutorial dialogue strategies, or modes, could address this limitation by providing a mechanism for describing in succinct probabilistic terms the tutorial strategies that actually occur in a corpus.",
                "Just as early work on dialogue act interpretation utilized hidden Markov models (HMMs) to capture linguistic structure (#REF) , we propose a system that uses HMMs to capture the structure of tutorial dialogue implicit within sequences of already-tagged dialogue acts."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Automatically learning dialogue structure from corpora is an active area of research driven by a recognition of the value offered by data-driven approaches (e.g., #REF) .\n sent1: Dialogue structure information is of particular importance when the interaction is centered around a learning task, such as in natural language tutoring, because techniques that support empirical identification of dialogue strategies can inform not only the design of intelligent tutoring systems #TARGET_REF , but also contribute to our understanding of the cognitive and affective processes involved in learning through tutoring (#REF) .\n sent2: Although traditional top-down approaches (e.g., #REF) and some empirical work on analyzing the structure of tutorial dialogue (#REF) have yielded significant results, the field is limited by the lack of an automatic, data-driven approach to identifying dialogue structure.\n sent3: An empirical approach to identifying tutorial dialogue strategies, or modes, could address this limitation by providing a mechanism for describing in succinct probabilistic terms the tutorial strategies that actually occur in a corpus.\n sent4: Just as early work on dialogue act interpretation utilized hidden Markov models (HMMs) to capture linguistic structure (#REF) , we propose a system that uses HMMs to capture the structure of tutorial dialogue implicit within sequences of already-tagged dialogue acts.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Automatically learning dialogue structure from corpora is an active area of research driven by a recognition of the value offered by data-driven approaches (e.g., #REF) .",
                "Dialogue structure information is of particular importance when the interaction is centered around a learning task, such as in natural language tutoring, because techniques that support empirical identification of dialogue strategies can inform not only the design of intelligent tutoring systems (#REF) , but also contribute to our understanding of the cognitive and affective processes involved in learning through tutoring (#REF) .",
                "Although traditional top-down approaches (e.g., #REF) and some empirical work on analyzing the structure of tutorial dialogue #TARGET_REF have yielded significant results, the field is limited by the lack of an automatic, data-driven approach to identifying dialogue structure.",
                "An empirical approach to identifying tutorial dialogue strategies, or modes, could address this limitation by providing a mechanism for describing in succinct probabilistic terms the tutorial strategies that actually occur in a corpus.",
                "Just as early work on dialogue act interpretation utilized hidden Markov models (HMMs) to capture linguistic structure (#REF) , we propose a system that uses HMMs to capture the structure of tutorial dialogue implicit within sequences of already-tagged dialogue acts."
            ],
            "label": [
                "BACKGROUND",
                "MOTIVATION"
            ]
        },
        "input": "sent0: Automatically learning dialogue structure from corpora is an active area of research driven by a recognition of the value offered by data-driven approaches (e.g., #REF) .\n sent1: Dialogue structure information is of particular importance when the interaction is centered around a learning task, such as in natural language tutoring, because techniques that support empirical identification of dialogue strategies can inform not only the design of intelligent tutoring systems (#REF) , but also contribute to our understanding of the cognitive and affective processes involved in learning through tutoring (#REF) .\n sent2: Although traditional top-down approaches (e.g., #REF) and some empirical work on analyzing the structure of tutorial dialogue #TARGET_REF have yielded significant results, the field is limited by the lack of an automatic, data-driven approach to identifying dialogue structure.\n sent3: An empirical approach to identifying tutorial dialogue strategies, or modes, could address this limitation by providing a mechanism for describing in succinct probabilistic terms the tutorial strategies that actually occur in a corpus.\n sent4: Just as early work on dialogue act interpretation utilized hidden Markov models (HMMs) to capture linguistic structure (#REF) , we propose a system that uses HMMs to capture the structure of tutorial dialogue implicit within sequences of already-tagged dialogue acts.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"MOTIVATION\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The importance of adjacency pairs is wellestablished in natural language dialogue (e.g., Schlegoff & #REF) , and adjacency pair analysis has illuminated important phenomena in tutoring as well #TARGET_REF .",
                "For the current corpus, bigram analysis of dialogue acts yielded a set of commonly-occurring pairs.",
                "However, as noted in (#REF) , in order to establish that two dialogue acts are truly related as an adjacency pair, it is important to determine whether the presence of the first member of the pair is associated with a significantly higher probability of the second member occurring.",
                "For this analysis we utilize a χ 2 test for independence of the categorical variables act i and act i+1 for all two-way combinations of dialogue act tags.",
                "Only pairs in which speaker(act i )≠speaker(act i+1 ) were considered."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The importance of adjacency pairs is wellestablished in natural language dialogue (e.g., Schlegoff & #REF) , and adjacency pair analysis has illuminated important phenomena in tutoring as well #TARGET_REF .\n sent1: For the current corpus, bigram analysis of dialogue acts yielded a set of commonly-occurring pairs.\n sent2: However, as noted in (#REF) , in order to establish that two dialogue acts are truly related as an adjacency pair, it is important to determine whether the presence of the first member of the pair is associated with a significantly higher probability of the second member occurring.\n sent3: For this analysis we utilize a χ 2 test for independence of the categorical variables act i and act i+1 for all two-way combinations of dialogue act tags.\n sent4: Only pairs in which speaker(act i )≠speaker(act i+1 ) were considered.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Emojis are the evolution of characterbased emoticons (#REF) , and are extensively used, not only as sentiment carriers or boosters, but more importantly, to express ideas about a myriad of topics, e.g., mood ( ), food ( ), sports ( ) or scenery ( ).",
                "Emoji modeling and prediction is, therefore, an important problem towards the end goal of properly capturing the intended meaning of a social media message.",
                "In fact, emoji prediction, i.e., given a (usually short) message, predict its most likely associated emoji(s), may help to improve different NLP tasks (#REF) , such as information retrieval, generation of emojienriched social media content or suggestion of emojis when writing text messages or sharing pictures online.",
                "It has furthermore proven to be useful for sentiment analysis, emotion recognition and irony detection #TARGET_REF .",
                "The problem of emoji prediction, albeit recent, has already seen important developments."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Emojis are the evolution of characterbased emoticons (#REF) , and are extensively used, not only as sentiment carriers or boosters, but more importantly, to express ideas about a myriad of topics, e.g., mood ( ), food ( ), sports ( ) or scenery ( ).\n sent1: Emoji modeling and prediction is, therefore, an important problem towards the end goal of properly capturing the intended meaning of a social media message.\n sent2: In fact, emoji prediction, i.e., given a (usually short) message, predict its most likely associated emoji(s), may help to improve different NLP tasks (#REF) , such as information retrieval, generation of emojienriched social media content or suggestion of emojis when writing text messages or sharing pictures online.\n sent3: It has furthermore proven to be useful for sentiment analysis, emotion recognition and irony detection #TARGET_REF .\n sent4: The problem of emoji prediction, albeit recent, has already seen important developments.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The model also includes an attention module to increase its sensitivity to individual words during prediction.",
                "In general, attention mechanisms allow the model to focus on specific words of the input (#REF) , instead of having to memorize all the important features in a fixed-length vector.",
                "The main architectural difference with respect to the typical attention is illustrated in Figure 1 .",
                "In #TARGET_REF , attention is computed as follows:",
                "Here h i ∈ R d is the hidden representation of the LSTM corresponding to the i th word, with N the total number of words in the sentence."
            ],
            "label": [
                "BACKGROUND",
                "USE"
            ]
        },
        "input": "sent0: The model also includes an attention module to increase its sensitivity to individual words during prediction.\n sent1: In general, attention mechanisms allow the model to focus on specific words of the input (#REF) , instead of having to memorize all the important features in a fixed-length vector.\n sent2: The main architectural difference with respect to the typical attention is illustrated in Figure 1 .\n sent3: In #TARGET_REF , attention is computed as follows:\n sent4: Here h i ∈ R d is the hidden representation of the LSTM corresponding to the i th word, with N the total number of words in the sentence.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"USE\"], \"context\": [\"sent2\", \"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Our contribution in this paper is twofold.",
                "First, we use the proposed label-wise mechanism to analyze the behavior of neural emoji classifiers, exploiting the attention weights to uncover and interpret emoji usages.",
                "Second, we experimentally compare the effect of the label-wise mechanism on the performance of an emoji classifier.",
                "We observed a performance improvement over competitive baselines such as FastText (FT) (#REF) and Deepmoji #TARGET_REF , which is most noticeable in the case of infrequent emojis.",
                "This suggests that an attentive mechanism can be leveraged to make neural architectures more sensitive to instances of underrepresented classes."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Our contribution in this paper is twofold.\n sent1: First, we use the proposed label-wise mechanism to analyze the behavior of neural emoji classifiers, exploiting the attention weights to uncover and interpret emoji usages.\n sent2: Second, we experimentally compare the effect of the label-wise mechanism on the performance of an emoji classifier.\n sent3: We observed a performance improvement over competitive baselines such as FastText (FT) (#REF) and Deepmoji #TARGET_REF , which is most noticeable in the case of infrequent emojis.\n sent4: This suggests that an attentive mechanism can be leveraged to make neural architectures more sensitive to instances of underrepresented classes.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Our base architecture is the Deepmoji model #TARGET_REF , which is based on two stacked word-based bi-directional LSTM recurrent neural networks with skip connections between the first and the second LSTM.",
                "The model also includes an attention module to increase its sensitivity to individual words during prediction.",
                "In general, attention mechanisms allow the model to focus on specific words of the input (#REF) , instead of having to memorize all the important features in a fixed-length vector.",
                "The main architectural difference with respect to the typical attention is illustrated in Figure 1 .",
                "In #REF , attention is computed as follows:"
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Our base architecture is the Deepmoji model #TARGET_REF , which is based on two stacked word-based bi-directional LSTM recurrent neural networks with skip connections between the first and the second LSTM.\n sent1: The model also includes an attention module to increase its sensitivity to individual words during prediction.\n sent2: In general, attention mechanisms allow the model to focus on specific words of the input (#REF) , instead of having to memorize all the important features in a fixed-length vector.\n sent3: The main architectural difference with respect to the typical attention is illustrated in Figure 1 .\n sent4: In #REF , attention is computed as follows:\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "We also show results from additional experiments in which the label space ranged from 20 to 200 emojis.",
                "These extended experiments are performed on a corpus of around 100M tweets geolocalized in the United States and posted between #REF and #REF.",
                "Models.",
                "In order to put our proposed labelwise attention mechanism in context, we compare its performance with a set of baselines: (1 #TARGET_REF .",
                "Finally, we denote as 2-BiLSTMs l our proposed label-wise attentive Bi-LSTM architecture."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We also show results from additional experiments in which the label space ranged from 20 to 200 emojis.\n sent1: These extended experiments are performed on a corpus of around 100M tweets geolocalized in the United States and posted between #REF and #REF.\n sent2: Models.\n sent3: In order to put our proposed labelwise attention mechanism in context, we compare its performance with a set of baselines: (1 #TARGET_REF .\n sent4: Finally, we denote as 2-BiLSTMs l our proposed label-wise attentive Bi-LSTM architecture.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Most of the parallel data is crawled from the Internet and is not in News domain.",
                "Out-ofdomain training data can hurt the translation performance on News test sets (#REF) and also significantly increase training time.",
                "Therefore, we trained neural language models on a large monolingual News corpus to perform data selection #TARGET_REF .",
                "Back-translation Large monolingual data in the News domain is provided for both German and 1 https://github.com/awslabs/sockeye English, which can be back-translated as additional parallel training data for our system (#REFa; #REF) .",
                "The back-translated parallel data is in the News domain, which is a big advantage compared to outof-domain parallel training data provided for the News task."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Most of the parallel data is crawled from the Internet and is not in News domain.\n sent1: Out-ofdomain training data can hurt the translation performance on News test sets (#REF) and also significantly increase training time.\n sent2: Therefore, we trained neural language models on a large monolingual News corpus to perform data selection #TARGET_REF .\n sent3: Back-translation Large monolingual data in the News domain is provided for both German and 1 https://github.com/awslabs/sockeye English, which can be back-translated as additional parallel training data for our system (#REFa; #REF) .\n sent4: The back-translated parallel data is in the News domain, which is a big advantage compared to outof-domain parallel training data provided for the News task.\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Therefore, we trained neural language models on a large monolingual News corpus to perform data selection (#REF) .",
                "Back-translation Large monolingual data in the News domain is provided for both German and 1 https://github.com/awslabs/sockeye English, which can be back-translated as additional parallel training data for our system (#REFa; #REF) .",
                "The back-translated parallel data is in the News domain, which is a big advantage compared to outof-domain parallel training data provided for the News task.",
                "In-domain Fine-tuning The Transformer models were finally fine-tuned using the small in-domain parallel data provided for the News task (#REF; #TARGET_REF .",
                "Note that the large back-translated parallel data is also in-domain, but it has relatively low quality due to translation errors."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Therefore, we trained neural language models on a large monolingual News corpus to perform data selection (#REF) .\n sent1: Back-translation Large monolingual data in the News domain is provided for both German and 1 https://github.com/awslabs/sockeye English, which can be back-translated as additional parallel training data for our system (#REFa; #REF) .\n sent2: The back-translated parallel data is in the News domain, which is a big advantage compared to outof-domain parallel training data provided for the News task.\n sent3: In-domain Fine-tuning The Transformer models were finally fine-tuned using the small in-domain parallel data provided for the News task (#REF; #TARGET_REF .\n sent4: Note that the large back-translated parallel data is also in-domain, but it has relatively low quality due to translation errors.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "We trained two Transformer models with different sizes, Transformer-base and Transformer-big.",
                "Our final submission is an ensemble of both models #TARGET_REF .",
                "The ensemble of both models outperformed a single base or big model most likely because the two models can capture somewhat different features for the translation task."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: We trained two Transformer models with different sizes, Transformer-base and Transformer-big.\n sent1: Our final submission is an ensemble of both models #TARGET_REF .\n sent2: The ensemble of both models outperformed a single base or big model most likely because the two models can capture somewhat different features for the translation task.\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Therefore, we performed data selection on out-of-domain data.",
                "Inspired by #TARGET_REF 's work which used KenLM (#REF) for data selection, we trained two neural language models based on self-attention networks using the 2018 part of the large monolingual News crawl corpus for English and German, respectively.",
                "Because these neural language models are trained on the News domain, we can use them to score out-ofdomain data.",
                "Sentences with higher probabilities are more likely to be in News domain.",
                "Equation 1 is used to score each sentence pair in the out-ofdomain corpus."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: Therefore, we performed data selection on out-of-domain data.\n sent1: Inspired by #TARGET_REF 's work which used KenLM (#REF) for data selection, we trained two neural language models based on self-attention networks using the 2018 part of the large monolingual News crawl corpus for English and German, respectively.\n sent2: Because these neural language models are trained on the News domain, we can use them to score out-ofdomain data.\n sent3: Sentences with higher probabilities are more likely to be in News domain.\n sent4: Equation 1 is used to score each sentence pair in the out-ofdomain corpus.\n",
        "output": "{\"label\": [null], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Many services such as web search (#REF) , recommender systems (#REF) , targeted advertising (#REF) , and rapid disaster response (#REF) rely on the location of users to personalise information and extract actionable knowledge.",
                "Explicit user geolocation metadata (e.g. GPS tags, WiFi footprint, IP address) is not usually available to third-party consumers, giving rise to the need for geolocation based on profile data, text content, friendship graphs (#REF) or some combination of these #TARGET_REF,a) .",
                "The strong geographical bias, most obviously at the language level (e.g. Finland vs. Japan), and more subtly at the dialect level (e.g. in English used in north-west England vs. north-east USA vs. Texas, USA), clearly reflected in language use in social media services such as Twitter, has been used extensively either for geolocation of users (#REF; #REF; #REF; #REF) or dialectology #REF) .",
                "In these methods, a user is often represented by the concatenation of their tweets, and the geolocation model is trained on a very small percentage of explicitly geotagged tweets, noting the potential biases implicit in geotagged tweets (#REF) .",
                "Lexical dialectology is (in part) the converse of user geolocation (#REF) : given text associated with a variety of regions, the task is to identify terms that are distinctive of particular regions."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Many services such as web search (#REF) , recommender systems (#REF) , targeted advertising (#REF) , and rapid disaster response (#REF) rely on the location of users to personalise information and extract actionable knowledge.\n sent1: Explicit user geolocation metadata (e.g. GPS tags, WiFi footprint, IP address) is not usually available to third-party consumers, giving rise to the need for geolocation based on profile data, text content, friendship graphs (#REF) or some combination of these #TARGET_REF,a) .\n sent2: The strong geographical bias, most obviously at the language level (e.g. Finland vs. Japan), and more subtly at the dialect level (e.g. in English used in north-west England vs. north-east USA vs. Texas, USA), clearly reflected in language use in social media services such as Twitter, has been used extensively either for geolocation of users (#REF; #REF; #REF; #REF) or dialectology #REF) .\n sent3: In these methods, a user is often represented by the concatenation of their tweets, and the geolocation model is trained on a very small percentage of explicitly geotagged tweets, noting the potential biases implicit in geotagged tweets (#REF) .\n sent4: Lexical dialectology is (in part) the converse of user geolocation (#REF) : given text associated with a variety of regions, the task is to identify terms that are distinctive of particular regions.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Following #REF and #REF , we evaluated the geolocation model using mean and median error in km (\"Mean\" and \"Median\" resp.) and accuracy within 161km of the actual location (\"Acc@161\").",
                "Note that lower numbers are better for Mean and Median, and higher numbers better for Acc@161.",
                "4 The results reported in #TARGET_REF; 2015a) for TWITTER-WORLD were over a superset of the dataset; the results reported here are based on the actual dataset.",
                "While the focus of this paper is text-based user geolocation, state-of-the-art results for the three datasets have been achieved with hybrid text+network-based models, where the predictions of the text-based model are fed into a mention network as \"dongle\" nodes to each user node, providing a personalised geolocation prior for each user (#REFa) .",
                "Note that it would, of course, be possible to combine text and network information in a joint deep learning model (#REF; #REF) , which we leave to future work (noting that scalability will potentially be a major issue for the larger datasets)."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Following #REF and #REF , we evaluated the geolocation model using mean and median error in km (\"Mean\" and \"Median\" resp.) and accuracy within 161km of the actual location (\"Acc@161\").\n sent1: Note that lower numbers are better for Mean and Median, and higher numbers better for Acc@161.\n sent2: 4 The results reported in #TARGET_REF; 2015a) for TWITTER-WORLD were over a superset of the dataset; the results reported here are based on the actual dataset.\n sent3: While the focus of this paper is text-based user geolocation, state-of-the-art results for the three datasets have been achieved with hybrid text+network-based models, where the predictions of the text-based model are fed into a mention network as \"dongle\" nodes to each user node, providing a personalised geolocation prior for each user (#REFa) .\n sent4: Note that it would, of course, be possible to combine text and network information in a joint deep learning model (#REF; #REF) , which we leave to future work (noting that scalability will potentially be a major issue for the larger datasets).\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The performance of the text-based MLP model with k-d tree and k-means discretisation over the three datasets is shown in Table 1 .",
                "The results are also compared with state-of-the-art text-based methods based on a flat #TARGET_REF; #REF) or hierarchical (#REF; #REF; #REF) geospatial representation.",
                "Our method outperforms both the flat and hierarchical text-based models by a large margin.",
                "Comparing the two discretisation strategies, k-means outperforms k-d tree by a reasonable margin.",
                "We also incorporated the MLP predictions into a network-based model based on the method of Rahimi et al. (2015a) , and improved upon their work."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: The performance of the text-based MLP model with k-d tree and k-means discretisation over the three datasets is shown in Table 1 .\n sent1: The results are also compared with state-of-the-art text-based methods based on a flat #TARGET_REF; #REF) or hierarchical (#REF; #REF; #REF) geospatial representation.\n sent2: Our method outperforms both the flat and hierarchical text-based models by a large margin.\n sent3: Comparing the two discretisation strategies, k-means outperforms k-d tree by a reasonable margin.\n sent4: We also incorporated the MLP predictions into a network-based model based on the method of Rahimi et al. (2015a) , and improved upon their work.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We quantitatively tested the quality of the geographical embeddings by calculating the micro-average recall of the k-nearest dialect terms (in terms of the proportion of retrieved dialect terms) given a dialect region, as shown in Figure 4 .",
                "Recall at 0.5% is about 3.6%, meaning that we were able to retrieve 3.6% of the dialect terms given the dialect region name in the geographical embedding space.",
                "The embeddings slightly outperform the output layer of logistic regression (LR) #TARGET_REF"
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: We quantitatively tested the quality of the geographical embeddings by calculating the micro-average recall of the k-nearest dialect terms (in terms of the proportion of retrieved dialect terms) given a dialect region, as shown in Figure 4 .\n sent1: Recall at 0.5% is about 3.6%, meaning that we were able to retrieve 3.6% of the dialect terms given the dialect region name in the geographical embedding space.\n sent2: The embeddings slightly outperform the output layer of logistic regression (LR) #TARGET_REF\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The output of the hidden layer of the model is used as embeddings for both location names and dialect terms.",
                "Given a dialect region name, we retrieve its nearest neighbours in the embedding space, and compare them to dialect terms associated with that location.",
                "We also compare the quality of the embeddings with pre-trained word2vec embeddings and the embeddings from the output layer of LR (logistic regression) #TARGET_REF as baselines.",
                "Regions in DAREDS can be very broad (e.g. SouthWest), meaning that words associated with those locations will be used across a large number Table 1 : Geolocation results over the three Twitter datasets, based on the text-based MLP with k-d tree or k-means discretisation and text+network model MADCEL-W-MLP using MLP with k-d tree for text-based predictions.",
                "We compare with state-of-the-art results for each dataset."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: The output of the hidden layer of the model is used as embeddings for both location names and dialect terms.\n sent1: Given a dialect region name, we retrieve its nearest neighbours in the embedding space, and compare them to dialect terms associated with that location.\n sent2: We also compare the quality of the embeddings with pre-trained word2vec embeddings and the embeddings from the output layer of LR (logistic regression) #TARGET_REF as baselines.\n sent3: Regions in DAREDS can be very broad (e.g. SouthWest), meaning that words associated with those locations will be used across a large number Table 1 : Geolocation results over the three Twitter datasets, based on the text-based MLP with k-d tree or k-means discretisation and text+network model MADCEL-W-MLP using MLP with k-d tree for text-based predictions.\n sent4: We compare with state-of-the-art results for each dataset.\n",
        "output": "{\"label\": [null], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "If we compare the widely used Conditional Random Fields (CRF) with newly proposed \"deep architecture\" sequence models #TARGET_REF , there are two things changing: from linear architecture to non-linear, and from discrete feature representation to distributional.",
                "It is unclear, however, what utility nonlinearity offers in conventional featurebased models.",
                "In this study, we show the close connection between CRF and \"sequence model\" neural nets, and present an empirical investigation to compare their performance on two sequence labeling tasks -Named Entity Recognition and Syntactic Chunking.",
                "Our results suggest that non-linear models are highly effective in low-dimensional distributional spaces.",
                "Somewhat surprisingly, we find that a nonlinear architecture offers no benefits in a high-dimensional discrete feature space."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: If we compare the widely used Conditional Random Fields (CRF) with newly proposed \"deep architecture\" sequence models #TARGET_REF , there are two things changing: from linear architecture to non-linear, and from discrete feature representation to distributional.\n sent1: It is unclear, however, what utility nonlinearity offers in conventional featurebased models.\n sent2: In this study, we show the close connection between CRF and \"sequence model\" neural nets, and present an empirical investigation to compare their performance on two sequence labeling tasks -Named Entity Recognition and Syntactic Chunking.\n sent3: Our results suggest that non-linear models are highly effective in low-dimensional distributional spaces.\n sent4: Somewhat surprisingly, we find that a nonlinear architecture offers no benefits in a high-dimensional discrete feature space.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Fortunately, we can use forward-backward style dynamic programming to compute the marginal probabilities efficiently.",
                "It is also worth pointing out that this model has in fact been introduced a few times in prior literature.",
                "It was termed Conditional Neural Fields by #REF , and later Neural Conditional Random Fields by #REF .",
                "Unfortunately, the connection to #REF was not recognized in either of these two studies; vice versa, neither of the above were referenced in #TARGET_REF .",
                "This model also appeared previously in the speech recognition literature in #REF ."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Fortunately, we can use forward-backward style dynamic programming to compute the marginal probabilities efficiently.\n sent1: It is also worth pointing out that this model has in fact been introduced a few times in prior literature.\n sent2: It was termed Conditional Neural Fields by #REF , and later Neural Conditional Random Fields by #REF .\n sent3: Unfortunately, the connection to #REF was not recognized in either of these two studies; vice versa, neither of the above were referenced in #TARGET_REF .\n sent4: This model also appeared previously in the speech recognition literature in #REF .\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "All sequences of numbers are replaced with num (e.g., \"PS1\" would become \"PSnum\"), sentence boundaries are padded with token PAD, and unknown words are grouped into UNKNOWN.",
                "We attempt to replicate the model described in #TARGET_REF without task-specific fine-tuning, with a few exceptions: 1) we used the soft tanh activation function instead of hard tanh; 2) we use the BIO2 tagging scheme instead of BIOES; 3) we use L-BFGS optimization algorithm instead of stochastic gradient descent; 4) we did not use Gazetteer features; 5) #TARGET_REF mentioned 5 binary features that look at the capitalization pattern of words to append to the embedding as additional dimensions, but only 4 were described in the paper, which we implemented accordingly."
            ],
            "label": [
                "DIFFERENCES",
                "EXTENDS"
            ]
        },
        "input": "sent0: All sequences of numbers are replaced with num (e.g., \"PS1\" would become \"PSnum\"), sentence boundaries are padded with token PAD, and unknown words are grouped into UNKNOWN.\n sent1: We attempt to replicate the model described in #TARGET_REF without task-specific fine-tuning, with a few exceptions: 1) we used the soft tanh activation function instead of hard tanh; 2) we use the BIO2 tagging scheme instead of BIOES; 3) we use L-BFGS optimization algorithm instead of stochastic gradient descent; 4) we did not use Gazetteer features; 5) #TARGET_REF mentioned 5 binary features that look at the capitalization pattern of words to append to the embedding as additional dimensions, but only 4 were described in the paper, which we implemented accordingly.\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"EXTENDS\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Output from such systems can facilitate downstream applications such as Question Answering and Relation Extraction.",
                "Most methods developed so far for sequence labeling employ generalized linear statistical models, meaning methods that describe the data as a combination of linear basis functions, either directly in the input variables space (e.g., SVM) or through some transformation of the probability distributions (e.g., \"log-linear\" models).",
                "Recently, #TARGET_REF proposed \"deep architecture\" models for sequence labeling (named Sentence-level Likelihood Neural Nets, abbreviated as SLNN henceforth), and showed promising results on a range of tasks (POS tagging, NER, Chunking, and SRL).",
                "Two new changes were suggested: extending the model from a linear to non-linear architecture; and replacing discrete feature representations with distributional feature representations in a continuous space.",
                "It has generally been argued that nonlinearity between layers is vital to the power of neural models (#REF )."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Output from such systems can facilitate downstream applications such as Question Answering and Relation Extraction.\n sent1: Most methods developed so far for sequence labeling employ generalized linear statistical models, meaning methods that describe the data as a combination of linear basis functions, either directly in the input variables space (e.g., SVM) or through some transformation of the probability distributions (e.g., \"log-linear\" models).\n sent2: Recently, #TARGET_REF proposed \"deep architecture\" models for sequence labeling (named Sentence-level Likelihood Neural Nets, abbreviated as SLNN henceforth), and showed promising results on a range of tasks (POS tagging, NER, Chunking, and SRL).\n sent3: Two new changes were suggested: extending the model from a linear to non-linear architecture; and replacing discrete feature representations with distributional feature representations in a continuous space.\n sent4: It has generally been argued that nonlinearity between layers is vital to the power of neural models (#REF ).\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "In neural network terminology, this architecture is called a single-layer Input-Output Neural Network (IONN).",
                "1 Normalizing locally in a logistic regression is equivalent to adding a softmax layer to the output layer of the IONN, which was commonly done in neural networks, such as in #TARGET_REF .",
                "We can add a hidden linear layer to this architecture to formulate a two-layer Linear Neural Network (LNN), as shown in the middle diagram of Figure 1 .",
                "The value of the node z j in the hidden layer is computed as",
                "The value y i for nodes in the output layer is computed as:"
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: In neural network terminology, this architecture is called a single-layer Input-Output Neural Network (IONN).\n sent1: 1 Normalizing locally in a logistic regression is equivalent to adding a softmax layer to the output layer of the IONN, which was commonly done in neural networks, such as in #TARGET_REF .\n sent2: We can add a hidden linear layer to this architecture to formulate a two-layer Linear Neural Network (LNN), as shown in the middle diagram of Figure 1 .\n sent3: The value of the node z j in the hidden layer is computed as\n sent4: The value y i for nodes in the output layer is computed as:\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "This two-layer network is actually no more powerful than the previous model, since we can always compile it down to a single-layer IONN by making Θ = Ω∆. In the next step, we take the output of the hidden layer in the LNN, and send it through a non-linear activation function, such as a sigmoid or tanh, then we arrive at a two-layer Deep Neural Network (DNN) model.",
                "Unlike the previous two models, the DNN is non-linear, and thus capable of representing a more complex decision surface.",
                "So far we have extended the potential function used in node cliques of a CRF to a non-linear DNN. And if we keep the potential function for edge cliques the same as before, then in fact we have arrived at an identical model to the SLNN in Collobert et al. #TARGET_REF .",
                "The difference between a SLNN and an ordinary DNN model is that we need to take into consideration the influence of edge cliques, and therefore we can no longer normalize the clique factors at each position to calculate the local marginals, as we would do in a logistic regression.",
                "The cardinality of the output variable vector y grows exponentially with respect to input sequence length."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: This two-layer network is actually no more powerful than the previous model, since we can always compile it down to a single-layer IONN by making Θ = Ω∆. In the next step, we take the output of the hidden layer in the LNN, and send it through a non-linear activation function, such as a sigmoid or tanh, then we arrive at a two-layer Deep Neural Network (DNN) model.\n sent1: Unlike the previous two models, the DNN is non-linear, and thus capable of representing a more complex decision surface.\n sent2: So far we have extended the potential function used in node cliques of a CRF to a non-linear DNN. And if we keep the potential function for edge cliques the same as before, then in fact we have arrived at an identical model to the SLNN in Collobert et al. #TARGET_REF .\n sent3: The difference between a SLNN and an ordinary DNN model is that we need to take into consideration the influence of edge cliques, and therefore we can no longer normalize the clique factors at each position to calculate the local marginals, as we would do in a logistic regression.\n sent4: The cardinality of the output variable vector y grows exponentially with respect to input sequence length.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "All model parameters are initialized to a random value in [−0.1, 0.1] in order to break symmetry.",
                "We did not explicitly tune the features used in CRF to optimize for performance, since feature engineering is not the focus of this study.",
                "However, overall we found that the feature set we used is competitive with CRF results from earlier literature (#REF; #TARGET_REF .",
                "For models that embed hidden layers, we set the number of hidden nodes to 300.",
                "2 Results are reported on the standard evaluation metrics of entity/chunk precision, recall and F1 measure."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: All model parameters are initialized to a random value in [−0.1, 0.1] in order to break symmetry.\n sent1: We did not explicitly tune the features used in CRF to optimize for performance, since feature engineering is not the focus of this study.\n sent2: However, overall we found that the feature set we used is competitive with CRF results from earlier literature (#REF; #TARGET_REF .\n sent3: For models that embed hidden layers, we set the number of hidden nodes to 300.\n sent4: 2 Results are reported on the standard evaluation metrics of entity/chunk precision, recall and F1 measure.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We did not explicitly tune the features used in CRF to optimize for performance, since feature engineering is not the focus of this study.",
                "However, overall we found that the feature set we used is competitive with CRF results from earlier literature (#REF; #REF) .",
                "For models that embed hidden layers, we set the number of hidden nodes to 300.",
                "2 Results are reported on the standard evaluation metrics of entity/chunk precision, recall and F1 measure.",
                "For experiments with continuous space feature representations (a.k.a., word embeddings), we took the word embeddings (130K words, 50 dimensions) used in #TARGET_REF , which were trained for 2 months over Wikipedia text."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: We did not explicitly tune the features used in CRF to optimize for performance, since feature engineering is not the focus of this study.\n sent1: However, overall we found that the feature set we used is competitive with CRF results from earlier literature (#REF; #REF) .\n sent2: For models that embed hidden layers, we set the number of hidden nodes to 300.\n sent3: 2 Results are reported on the standard evaluation metrics of entity/chunk precision, recall and F1 measure.\n sent4: For experiments with continuous space feature representations (a.k.a., word embeddings), we took the word embeddings (130K words, 50 dimensions) used in #TARGET_REF , which were trained for 2 months over Wikipedia text.\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "For the next experiment, we replace the discrete input features with a continuous space representation by looking up the embedding of each word, and concatenate the embeddings of a five word window centered around the current position.",
                "Four binary features are also appended to each word embedding to capture capitalization patterns, as described in #TARGET_REF .",
                "Results of the CRF and SLNN under this setting for the NER task is show in Table 3 .",
                "With a continuous space representation, the SLNN model works significantly better than a CRF, by as much as 7% on the CoNLL development set, and 3.7% on ACE dataset.",
                "This suggests that there exist statistical dependencies within this low-dimensional (300) data that cannot be effectively captured by linear transformations, but can be modeled in the non-linear neural nets."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: For the next experiment, we replace the discrete input features with a continuous space representation by looking up the embedding of each word, and concatenate the embeddings of a five word window centered around the current position.\n sent1: Four binary features are also appended to each word embedding to capture capitalization patterns, as described in #TARGET_REF .\n sent2: Results of the CRF and SLNN under this setting for the NER task is show in Table 3 .\n sent3: With a continuous space representation, the SLNN model works significantly better than a CRF, by as much as 7% on the CoNLL development set, and 3.7% on ACE dataset.\n sent4: This suggests that there exist statistical dependencies within this low-dimensional (300) data that cannot be effectively captured by linear transformations, but can be modeled in the non-linear neural nets.\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We carefully compared and analyzed the nonlinear neural networks used in #TARGET_REF and the widely adopted CRF, and revealed their close relationship.",
                "Through extensive experiments on NER and Syntactic Chunking, we have shown that non-linear architectures are effective in low dimensional continuous input spaces, but that they are not better suited for conventional highdimensional discrete input spaces.",
                "Furthermore, both linear and non-linear models benefit greatly from the combination of continuous and discrete features, especially for out-of-domain datasets.",
                "This finding confirms earlier results that distributional representations can be used to achieve better generalization."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: We carefully compared and analyzed the nonlinear neural networks used in #TARGET_REF and the widely adopted CRF, and revealed their close relationship.\n sent1: Through extensive experiments on NER and Syntactic Chunking, we have shown that non-linear architectures are effective in low dimensional continuous input spaces, but that they are not better suited for conventional highdimensional discrete input spaces.\n sent2: Furthermore, both linear and non-linear models benefit greatly from the combination of continuous and discrete features, especially for out-of-domain datasets.\n sent3: This finding confirms earlier results that distributional representations can be used to achieve better generalization.\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "We train the model so as to maximise the conditional probability of predicting the unsandhied sequence given its corresponding sandhied sequence (#REF) .",
                "We propose a knowledge-lean data-centric approach for the segmentation task.",
                "Our approach will help to scale the segmentation process in comparison with the challenges posed by knowledge involved processes in the current systems #TARGET_REF .",
                "We only use parallel segmented and unsegmented sentences during training.",
                "At run-time, we only require the input sentence."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: We train the model so as to maximise the conditional probability of predicting the unsandhied sequence given its corresponding sandhied sequence (#REF) .\n sent1: We propose a knowledge-lean data-centric approach for the segmentation task.\n sent2: Our approach will help to scale the segmentation process in comparison with the challenges posed by knowledge involved processes in the current systems #TARGET_REF .\n sent3: We only use parallel segmented and unsegmented sentences during training.\n sent4: At run-time, we only require the input sentence.\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "To further catalyse the research in word segmentation for Sanskrit, #REF has released a dataset for the word segmentation task.",
                "The work releases a dataset of 119,000 sentences in Sanskrit along with the lexical and morphological analysis from a shallow parser.",
                "The work emphasises the need for not just predicting the inflected word form but also the prediction of the associated morphological information of the word.",
                "The additional information will be beneficial in further processing of Sanskrit texts, such as Dependency parsing or summarisation #TARGET_REF .So far, no system successfully predicts the morphological information of the words in addition to the final word form.",
                "Though #REF has designed their system with this requirement in mind and outlined the possible extension of their system for the purpose, the system currently only predicts the final word-form."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: To further catalyse the research in word segmentation for Sanskrit, #REF has released a dataset for the word segmentation task.\n sent1: The work releases a dataset of 119,000 sentences in Sanskrit along with the lexical and morphological analysis from a shallow parser.\n sent2: The work emphasises the need for not just predicting the inflected word form but also the prediction of the associated morphological information of the word.\n sent3: The additional information will be beneficial in further processing of Sanskrit texts, such as Dependency parsing or summarisation #TARGET_REF .So far, no system successfully predicts the morphological information of the words in addition to the final word form.\n sent4: Though #REF has designed their system with this requirement in mind and outlined the possible extension of their system for the purpose, the system currently only predicts the final word-form.\n",
        "output": "{\"label\": [null], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "But predicting morphological information requires the knowledge of exact word boundaries.",
                "This should be seen as a multitask learning set up.",
                "One possible solution is to learn 'GibberishVocab' on the set of words rather than sentences. But this leads to increased vocabulary at decoder which is not beneficial, given the scarcity of the data we have.",
                "Given the importance of morphological segmentation in morphologically rich languages such as Hebrew and Arabic (Seeker and Ç etinoglu, 2015) , the same applies to the morphologically rich Sanskrit as well #TARGET_REF .",
                "But, we leave this work for future."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: But predicting morphological information requires the knowledge of exact word boundaries.\n sent1: This should be seen as a multitask learning set up.\n sent2: One possible solution is to learn 'GibberishVocab' on the set of words rather than sentences. But this leads to increased vocabulary at decoder which is not beneficial, given the scarcity of the data we have.\n sent3: Given the importance of morphological segmentation in morphologically rich languages such as Hebrew and Arabic (Seeker and Ç etinoglu, 2015) , the same applies to the morphologically rich Sanskrit as well #TARGET_REF .\n sent4: But, we leave this work for future.\n",
        "output": "{\"label\": [null], \"context\": [\"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "We then update the parameters via backpropagation and use Adam optimiser (#REF) for our model.",
                "Vocabulary Enhancement for the model -Sanskrit, being a resource poor language, the major challenge is to obtain enough data for the supervised task.",
                "While there are plenty of sandhied texts available for Sanskrit, it is hard to find parallel or unsandhied texts alone, as it is deterministic to get sandhied text from unsandhied texts.",
                "In our case we use 105,000 parallel strings from the Digital Corpus of Sanskrit as released in #TARGET_REF .",
                "To handle the data sparsity, we adopt a purely engineering based approach for our model."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We then update the parameters via backpropagation and use Adam optimiser (#REF) for our model.\n sent1: Vocabulary Enhancement for the model -Sanskrit, being a resource poor language, the major challenge is to obtain enough data for the supervised task.\n sent2: While there are plenty of sandhied texts available for Sanskrit, it is hard to find parallel or unsandhied texts alone, as it is deterministic to get sandhied text from unsandhied texts.\n sent3: In our case we use 105,000 parallel strings from the Digital Corpus of Sanskrit as released in #TARGET_REF .\n sent4: To handle the data sparsity, we adopt a purely engineering based approach for our model.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "We used a dataset of 107,000 sentences from the #TARGET_REF #TARGET_REF .",
                "The dataset is a subset of the Digital Corpus of Sanskrit.",
                "From the dataset we only use the input sentences and the ground truth inflected word-forms.",
                "We ignore all the other morphological and lemma information available in the dataset."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We used a dataset of 107,000 sentences from the #TARGET_REF #TARGET_REF .\n sent1: The dataset is a subset of the Digital Corpus of Sanskrit.\n sent2: From the dataset we only use the input sentences and the ground truth inflected word-forms.\n sent3: We ignore all the other morphological and lemma information available in the dataset.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Scalability of such systems is further restricted as the sentences often need to undergo linguistically involved preprocessing steps that lead to human in the loop processing.",
                "The systems by #REF and #TARGET_REF assume that the parser by #REF , identifies all the possible candidate chunks.",
                "Our proposed model is built with precisely one purpose in mind, which is to predict the final word-forms in a given sequence.",
                "#REF states that it is desirable to predict the morphological information of a word from along with the final word-form as the information will be helpful in further processing of Sanskrit.",
                "The segmentation task is seen as a means and not an end itself."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Scalability of such systems is further restricted as the sentences often need to undergo linguistically involved preprocessing steps that lead to human in the loop processing.\n sent1: The systems by #REF and #TARGET_REF assume that the parser by #REF , identifies all the possible candidate chunks.\n sent2: Our proposed model is built with precisely one purpose in mind, which is to predict the final word-forms in a given sequence.\n sent3: #REF states that it is desirable to predict the morphological information of a word from along with the final word-form as the information will be helpful in further processing of Sanskrit.\n sent4: The segmentation task is seen as a means and not an end itself.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Our proposed model is built with precisely one purpose in mind, which is to predict the final word-forms in a given sequence.",
                "#TARGET_REF states that it is desirable to predict the morphological information of a word from along with the final word-form as the information will be helpful in further processing of Sanskrit.",
                "The segmentation task is seen as a means and not an end itself.",
                "Here, we overlook this aspect and see the segmentation task as an end in itself.",
                "So we achieve scalability at the cost of missing out on providing valuable linguistic information."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Our proposed model is built with precisely one purpose in mind, which is to predict the final word-forms in a given sequence.\n sent1: #TARGET_REF states that it is desirable to predict the morphological information of a word from along with the final word-form as the information will be helpful in further processing of Sanskrit.\n sent2: The segmentation task is seen as a means and not an end itself.\n sent3: Here, we overlook this aspect and see the segmentation task as an end in itself.\n sent4: So we achieve scalability at the cost of missing out on providing valuable linguistic information.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent1\", \"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Naïve phrase-based statistical machine translation (SMT) often fails to address syntactic difference between Japanese and English.",
                "Preordering methods are one of the simple but effective approaches that can model reordering in a long distance, which is crucial in translating Japanese and English.",
                "Thus, we apply a predicate-argument structure-based preordering method to the Japanese-English statistical machine translation task of scientific papers.",
                "Our method is based on the method described in ( #TARGET_REF) , and extends #TARGET_REF rules to handle abbreviation and passivization frequently found in scientific papers.",
                "Experimental results show that our proposed method improves performance of both (#REF) 's system and our phrase-based SMT baseline without preordering."
            ],
            "label": [
                "USE",
                "EXTENDS"
            ]
        },
        "input": "sent0: Naïve phrase-based statistical machine translation (SMT) often fails to address syntactic difference between Japanese and English.\n sent1: Preordering methods are one of the simple but effective approaches that can model reordering in a long distance, which is crucial in translating Japanese and English.\n sent2: Thus, we apply a predicate-argument structure-based preordering method to the Japanese-English statistical machine translation task of scientific papers.\n sent3: Our method is based on the method described in ( #TARGET_REF) , and extends #TARGET_REF rules to handle abbreviation and passivization frequently found in scientific papers.\n sent4: Experimental results show that our proposed method improves performance of both (#REF) 's system and our phrase-based SMT baseline without preordering.\n",
        "output": "{\"label\": [\"USE\", \"EXTENDS\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Predicate-argument structure-based preordering is one of the promising approaches that can solve syntactic and stylistic difference between a language pair.",
                "Predicate-argument structure analysis identifies who does what to whom and generalizes grammatical relations such as active and passive construction.",
                "Following ( #TARGET_REF) , we perform predicate-argument structure analysis on the Japanese side to preorder Japanese sentences to form an SVO-like word order.",
                "We propose three modifications to the preordering rules to extend their model to better handle translation of scientific papers.",
                "The main contribution of this work is as follows:"
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Predicate-argument structure-based preordering is one of the promising approaches that can solve syntactic and stylistic difference between a language pair.\n sent1: Predicate-argument structure analysis identifies who does what to whom and generalizes grammatical relations such as active and passive construction.\n sent2: Following ( #TARGET_REF) , we perform predicate-argument structure analysis on the Japanese side to preorder Japanese sentences to form an SVO-like word order.\n sent3: We propose three modifications to the preordering rules to extend their model to better handle translation of scientific papers.\n sent4: The main contribution of this work is as follows:\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Third, #REF proposed predicate-argument structure-based preordering rules in two-level for the Japanese-English patent translation task.",
                "The first is sentence-level and the second is phrase-level.",
                "Furthermore, sentence-level preordering rules are divided into three parts.",
                "In total, sentences are reordered sequentially by four rules.",
                "Since #TARGET_REF is the one we re-implemented in this paper, we will describe #TARGET_REF in detail below."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Third, #REF proposed predicate-argument structure-based preordering rules in two-level for the Japanese-English patent translation task.\n sent1: The first is sentence-level and the second is phrase-level.\n sent2: Furthermore, sentence-level preordering rules are divided into three parts.\n sent3: In total, sentences are reordered sequentially by four rules.\n sent4: Since #TARGET_REF is the one we re-implemented in this paper, we will describe #TARGET_REF in detail below.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "If a subject exists in a sentence 2 , the predicate is moved imme-Inter-chunk normalization We restore the order of coordinate expressions which are reversed by the first rule.",
                "Also, since a full stop is moved along with the predicate, we restore it back to the end of the sentence.",
                "Intra-chunk preordering We apply the phraselevel rule, which swaps function words and content words in a phrase.",
                "It will improve alignments because function words in Japanese (e.g. postposition) appear after content words while those in English (e.g. preposition) appear before content words.",
                "3 Extension to ( #TARGET_REF) Our proposed preordering model is based on ( #TARGET_REF) with three extensions to better handle academic writing in scientific papers."
            ],
            "label": [
                "USE",
                "EXTENDS"
            ]
        },
        "input": "sent0: If a subject exists in a sentence 2 , the predicate is moved imme-Inter-chunk normalization We restore the order of coordinate expressions which are reversed by the first rule.\n sent1: Also, since a full stop is moved along with the predicate, we restore it back to the end of the sentence.\n sent2: Intra-chunk preordering We apply the phraselevel rule, which swaps function words and content words in a phrase.\n sent3: It will improve alignments because function words in Japanese (e.g. postposition) appear after content words while those in English (e.g. preposition) appear before content words.\n sent4: 3 Extension to ( #TARGET_REF) Our proposed preordering model is based on ( #TARGET_REF) with three extensions to better handle academic writing in scientific papers.\n",
        "output": "{\"label\": [\"USE\", \"EXTENDS\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "We compared translation performance using a standard phrase-based statistical machine translation technique with three kinds of data:",
                "• original data (baseline),",
                "• preordered data by our re-implementation of ( #TARGET_REF) , and",
                "• preordered data by our proposed methods.",
                "We analyzed predicate-argument structure of only the last predicate for each sentence, regardless of the number of predicates in a sentence."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We compared translation performance using a standard phrase-based statistical machine translation technique with three kinds of data:\n sent1: • original data (baseline),\n sent2: • preordered data by our re-implementation of ( #TARGET_REF) , and\n sent3: • preordered data by our proposed methods.\n sent4: We analyzed predicate-argument structure of only the last predicate for each sentence, regardless of the number of predicates in a sentence.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "• original data (baseline),",
                "• preordered data by our re-implementation of (#REF) , and",
                "• preordered data by our proposed methods.",
                "We analyzed predicate-argument structure of only the last predicate for each sentence, regardless of the number of predicates in a sentence.",
                "Also, following ( #TARGET_REF) , we did not consider event nouns as predicates."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: • original data (baseline),\n sent1: • preordered data by our re-implementation of (#REF) , and\n sent2: • preordered data by our proposed methods.\n sent3: We analyzed predicate-argument structure of only the last predicate for each sentence, regardless of the number of predicates in a sentence.\n sent4: Also, following ( #TARGET_REF) , we did not consider event nouns as predicates.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Some of the errors found in a translation result are due to the errors in predicate-argument structure analysis.",
                "We found that it is hard for predicateargument structure analyzer trained on a newswire Table 1 : Comparison of the preordering methods.",
                "All the preordering models using ( #TARGET_REF) are our re-implementation of their paper."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Some of the errors found in a translation result are due to the errors in predicate-argument structure analysis.\n sent1: We found that it is hard for predicateargument structure analyzer trained on a newswire Table 1 : Comparison of the preordering methods.\n sent2: All the preordering models using ( #TARGET_REF) are our re-implementation of their paper.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Predicate-argument structure analysis identifies who does what to whom and generalizes grammatical relations such as active and passive construction.",
                "Following (#REF) , we perform predicate-argument structure analysis on the Japanese side to preorder Japanese sentences to form an SVO-like word order.",
                "We propose three modifications to the preordering rules to extend their model to better handle translation of scientific papers.",
                "The main contribution of this work is as follows:",
                "• We propose an extension to ( #TARGET_REF) in order to deal with abbreviation and passivization frequently found in scientific papers."
            ],
            "label": [
                "EXTENDS",
                "MOTIVATION"
            ]
        },
        "input": "sent0: Predicate-argument structure analysis identifies who does what to whom and generalizes grammatical relations such as active and passive construction.\n sent1: Following (#REF) , we perform predicate-argument structure analysis on the Japanese side to preorder Japanese sentences to form an SVO-like word order.\n sent2: We propose three modifications to the preordering rules to extend their model to better handle translation of scientific papers.\n sent3: The main contribution of this work is as follows:\n sent4: • We propose an extension to ( #TARGET_REF) in order to deal with abbreviation and passivization frequently found in scientific papers.\n",
        "output": "{\"label\": [\"EXTENDS\", \"MOTIVATION\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Naïve phrase-based statistical machine translation (SMT) often fails to address syntactic difference between Japanese and English.",
                "Preordering methods are one of the simple but effective approaches that can model reordering in a long distance, which is crucial in translating Japanese and English.",
                "Thus, we apply a predicate-argument structure-based preordering method to the Japanese-English statistical machine translation task of scientific papers.",
                "Our method is based on the method described in (#REF) , and extends their rules to handle abbreviation and passivization frequently found in scientific papers.",
                "Experimental results show that our proposed method improves performance of both ( #TARGET_REF) 's system and our phrase-based SMT baseline without preordering."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Naïve phrase-based statistical machine translation (SMT) often fails to address syntactic difference between Japanese and English.\n sent1: Preordering methods are one of the simple but effective approaches that can model reordering in a long distance, which is crucial in translating Japanese and English.\n sent2: Thus, we apply a predicate-argument structure-based preordering method to the Japanese-English statistical machine translation task of scientific papers.\n sent3: Our method is based on the method described in (#REF) , and extends their rules to handle abbreviation and passivization frequently found in scientific papers.\n sent4: Experimental results show that our proposed method improves performance of both ( #TARGET_REF) 's system and our phrase-based SMT baseline without preordering.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "We performed minimum error rate training (#REF) optimized for BLEU using the development set (dev.txt) of the ASPEC corpus.",
                "We conducted all the experiments using the scripts distributed at KFTT Moses Baseline v1.4 11 .",
                "Table 1 shows the experimental results.",
                "In terms of BLEU, our re-implementation of ( #TARGET_REF) is below the baseline method while our proposed methods better than the baseline.",
                "In terms of RIBES, all preordering methods outperform the baseline, and our proposed method archieve the highest score."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: We performed minimum error rate training (#REF) optimized for BLEU using the development set (dev.txt) of the ASPEC corpus.\n sent1: We conducted all the experiments using the scripts distributed at KFTT Moses Baseline v1.4 11 .\n sent2: Table 1 shows the experimental results.\n sent3: In terms of BLEU, our re-implementation of ( #TARGET_REF) is below the baseline method while our proposed methods better than the baseline.\n sent4: In terms of RIBES, all preordering methods outperform the baseline, and our proposed method archieve the highest score.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "One of the advantages of preordering is that it can incorporate rich linguistic information on the source side, whilst off-the-shelf SMT toolkit can be plugged in without any modification.",
                "Preordering methods employ various kinds of linguistic information to achieve better alignment between source and target languages.",
                "Specifically, previous work in the literature uses morphological analysis (#REF) , dependency structure (#REF) and predicate-argument structure (#REF; #TARGET_REF) for preordering in Japanese-English statistical machine translation.",
                "However, these preordering methods are tested on limited domains: travel (#REF) and patent (#REF; #REF) corpora.",
                "Translating Japanese to English in a different domain such as scientific papers is still a big challenge for preordering-based approach."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: One of the advantages of preordering is that it can incorporate rich linguistic information on the source side, whilst off-the-shelf SMT toolkit can be plugged in without any modification.\n sent1: Preordering methods employ various kinds of linguistic information to achieve better alignment between source and target languages.\n sent2: Specifically, previous work in the literature uses morphological analysis (#REF) , dependency structure (#REF) and predicate-argument structure (#REF; #TARGET_REF) for preordering in Japanese-English statistical machine translation.\n sent3: However, these preordering methods are tested on limited domains: travel (#REF) and patent (#REF; #REF) corpora.\n sent4: Translating Japanese to English in a different domain such as scientific papers is still a big challenge for preordering-based approach.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Unlike (#REF) , they also reverse all words in each phrase.",
                "Third, #TARGET_REF proposed predicate-argument structure-based preordering rules in two-level for the Japanese-English patent translation task.",
                "The first is sentence-level and the second is phrase-level.",
                "Furthermore, sentence-level preordering rules are divided into three parts.",
                "In total, sentences are reordered sequentially by four rules."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Unlike (#REF) , they also reverse all words in each phrase.\n sent1: Third, #TARGET_REF proposed predicate-argument structure-based preordering rules in two-level for the Japanese-English patent translation task.\n sent2: The first is sentence-level and the second is phrase-level.\n sent3: Furthermore, sentence-level preordering rules are divided into three parts.\n sent4: In total, sentences are reordered sequentially by four rules.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Note that a Japanese sentence in active voice may be translated into different expressions even in the same passive construction (e.g. \". . .",
                "(explained . . .)\" can be either \". .",
                ". was explained\" or \"It was explained that . . .\".).",
                "#TARGET_REF proposed to move a predicate after the subject (inter-chunk preordering).",
                "However, if a subject is modified by other phrases, this rule moves the predicate to the middle of a subjective phrase composed of multiple phrases."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Note that a Japanese sentence in active voice may be translated into different expressions even in the same passive construction (e.g. \". . .\n sent1: (explained . . .)\" can be either \". .\n sent2: . was explained\" or \"It was explained that . . .\".).\n sent3: #TARGET_REF proposed to move a predicate after the subject (inter-chunk preordering).\n sent4: However, if a subject is modified by other phrases, this rule moves the predicate to the middle of a subjective phrase composed of multiple phrases.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Preordering methods employ various kinds of linguistic information to achieve better alignment between source and target languages.",
                "Specifically, previous work in the literature uses morphological analysis (#REF) , dependency structure (#REF) and predicate-argument structure (#REF; #REF) for preordering in Japanese-English statistical machine translation.",
                "However, these preordering methods are tested on limited domains: travel (#REF) and patent (#REF; #TARGET_REF) corpora.",
                "Translating Japanese to English in a different domain such as scientific papers is still a big challenge for preordering-based approach.",
                "For example, academic writing in English traditionally relies on passive voice to give an objective impression, but one can use either passive construction or a zeropronoun in the Japanese translation of passive construction on the English side."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Preordering methods employ various kinds of linguistic information to achieve better alignment between source and target languages.\n sent1: Specifically, previous work in the literature uses morphological analysis (#REF) , dependency structure (#REF) and predicate-argument structure (#REF; #REF) for preordering in Japanese-English statistical machine translation.\n sent2: However, these preordering methods are tested on limited domains: travel (#REF) and patent (#REF; #TARGET_REF) corpora.\n sent3: Translating Japanese to English in a different domain such as scientific papers is still a big challenge for preordering-based approach.\n sent4: For example, academic writing in English traditionally relies on passive voice to give an objective impression, but one can use either passive construction or a zeropronoun in the Japanese translation of passive construction on the English side.\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Convolutional neural networks (CNN) also often uses max or mean pooling to obtain a fixed-size sentence embedding.",
                "In this paper we explore generalized pooling methods to enhance sentence embedding.",
                "Specifically, by extending scalar self-attention models such as those proposed in #TARGET_REF , we propose vectorbased multi-head attention, which includes the widely used max pooling, mean pooling, and scalar selfattention itself as special cases.",
                "On one hand, the proposed method allows for extracting different aspects of the sentence into multiple vector representations through the multi-head mechanism.",
                "On the other, it allows the models to focus on one of many possible interpretations of the words encoded in the context vector through the vector-based attention mechanism."
            ],
            "label": [
                "SIMILARITY",
                "MOTIVATION"
            ]
        },
        "input": "sent0: Convolutional neural networks (CNN) also often uses max or mean pooling to obtain a fixed-size sentence embedding.\n sent1: In this paper we explore generalized pooling methods to enhance sentence embedding.\n sent2: Specifically, by extending scalar self-attention models such as those proposed in #TARGET_REF , we propose vectorbased multi-head attention, which includes the widely used max pooling, mean pooling, and scalar selfattention itself as special cases.\n sent3: On one hand, the proposed method allows for extracting different aspects of the sentence into multiple vector representations through the multi-head mechanism.\n sent4: On the other, it allows the models to focus on one of many possible interpretations of the words encoded in the context vector through the vector-based attention mechanism.\n",
        "output": "{\"label\": [\"SIMILARITY\", \"MOTIVATION\"], \"context\": [\"sent1\", \"sent2\", \"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Half of these 10 genres are used in training while the rest are not, resulting in-domain and cross-domain development and test sets used to test NLI systems.",
                "We use the same data split as in #REF , i.e., 392,702 samples for training, 9,815/9,832 samples for in-domain/cross-domain development, and 9,796/9,847 samples for in-domain/cross-domain testing.",
                "Note that, we do not use SNLI as an additional training/development set in our experiments.",
                "Age Dataset To compare our models with that of #TARGET_REF , we use the same Age dataset in our experiment here, which is an Author Profiling dataset.",
                "The dataset are extracted from the Author Profiling dataset 1 , which consists of tweets from English Twitter."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Half of these 10 genres are used in training while the rest are not, resulting in-domain and cross-domain development and test sets used to test NLI systems.\n sent1: We use the same data split as in #REF , i.e., 392,702 samples for training, 9,815/9,832 samples for in-domain/cross-domain development, and 9,796/9,847 samples for in-domain/cross-domain testing.\n sent2: Note that, we do not use SNLI as an additional training/development set in our experiments.\n sent3: Age Dataset To compare our models with that of #TARGET_REF , we use the same Age dataset in our experiment here, which is an Author Profiling dataset.\n sent4: The dataset are extracted from the Author Profiling dataset 1 , which consists of tweets from English Twitter.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Age Dataset To compare our models with that of #REF , we use the same Age dataset in our experiment here, which is an Author Profiling dataset.",
                "The dataset are extracted from the Author Profiling dataset 1 , which consists of tweets from English Twitter.",
                "The task is to predict the age range of authors of input tweets.",
                "The age range are split into 5 classes: 18-24, 25-34, 35-49, 50-64, 65+.",
                "We use the same data split as in #TARGET_REF , i.e., 68,485 samples for training, 4,000 for development, and 4,000 for testing."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Age Dataset To compare our models with that of #REF , we use the same Age dataset in our experiment here, which is an Author Profiling dataset.\n sent1: The dataset are extracted from the Author Profiling dataset 1 , which consists of tweets from English Twitter.\n sent2: The task is to predict the age range of authors of input tweets.\n sent3: The age range are split into 5 classes: 18-24, 25-34, 35-49, 50-64, 65+.\n sent4: We use the same data split as in #TARGET_REF , i.e., 68,485 samples for training, 4,000 for development, and 4,000 for testing.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "The Yelp dataset 2 is a sentiment analysis task, which takes reviews as input and predicts the level of sentiment in terms of the number of stars, from 1 to 5 stars, where 5-star means the most positive.",
                "We use the same data split as in #TARGET_REF , i.e., 500,000 samples for training, 2,000 for development, and 2,000 for testing."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The Yelp dataset 2 is a sentiment analysis task, which takes reviews as input and predicts the level of sentiment in terms of the number of stars, from 1 to 5 stars, where 5-star means the most positive.\n sent1: We use the same data split as in #TARGET_REF , i.e., 500,000 samples for training, 2,000 for development, and 2,000 for testing.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We evaluate the proposed approach on three different tasks: natural language inference, author profiling, and sentiment classification.",
                "The experiments show that the proposed model achieves significant improvement over strong sentence-encoding-based methods, resulting in state-of-the-art performances on four datasets.",
                "The proposed approach can be easily implemented for more problems than we discuss in this paper.",
                "Our future work includes exploring more effective MLP to use the structures of multi-head vectors, inspired by the idea from #TARGET_REF .",
                "Leveraging structure information from syntactic and semantic parses is another direction interesting to us."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: We evaluate the proposed approach on three different tasks: natural language inference, author profiling, and sentiment classification.\n sent1: The experiments show that the proposed model achieves significant improvement over strong sentence-encoding-based methods, resulting in state-of-the-art performances on four datasets.\n sent2: The proposed approach can be easily implemented for more problems than we discuss in this paper.\n sent3: Our future work includes exploring more effective MLP to use the structures of multi-head vectors, inspired by the idea from #TARGET_REF .\n sent4: Leveraging structure information from syntactic and semantic parses is another direction interesting to us.\n",
        "output": "{\"label\": [null], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The discriminator's encoder has the same architecture as the generator encoder module with the addition of a pooled decoder layer.",
                "The decoder contains 3 [DenseBatchN ormalization, ReLU ] blocks and an addtional Sigmoid layer.",
                "The discriminator decoder takes the hidden state at the last time step of a sequence concatenated with both the max-pooled and mean-pooled representation of the hidden states #TARGET_REF and outputs a number in the range [0, 1].",
                "The difficulty of using GANs in text generation comes from the discrete nature of text, making the model non-differentiable hence, we update parameters for the generator model with policy gradients as described in #REF .",
                "We utilize AWD-LSTM [21] and TransformerXL [22] based language models."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The discriminator's encoder has the same architecture as the generator encoder module with the addition of a pooled decoder layer.\n sent1: The decoder contains 3 [DenseBatchN ormalization, ReLU ] blocks and an addtional Sigmoid layer.\n sent2: The discriminator decoder takes the hidden state at the last time step of a sequence concatenated with both the max-pooled and mean-pooled representation of the hidden states #TARGET_REF and outputs a number in the range [0, 1].\n sent3: The difficulty of using GANs in text generation comes from the discrete nature of text, making the model non-differentiable hence, we update parameters for the generator model with policy gradients as described in #REF .\n sent4: We utilize AWD-LSTM [21] and TransformerXL [22] based language models.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Other practices for LM training were the same as [22] and [21] for Transformer-XL and AWD-LSTM respectively.",
                "We refer to our proposed GAN as Creative-GAN and compare it to a baseline (a language model equivalent to our pre-trained generator) and a GumbelGAN model [15] across all proposed datasets.",
                "We use three creative English datasets with distinct linguistic characteristics: (1) A corpus of 740 classical and contemporary English poems, (2) a corpus of 14950 metaphor sentences retrieved from a metaphor database website 1 and (3) a corpus of 1500 song lyrics ranging across genres.",
                "The mix of linguistic styles within this corpus offers the potential for interesting variation during the generation phase.",
                "We use the same pre-processing as in earlier work #TARGET_REF 24] ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Other practices for LM training were the same as [22] and [21] for Transformer-XL and AWD-LSTM respectively.\n sent1: We refer to our proposed GAN as Creative-GAN and compare it to a baseline (a language model equivalent to our pre-trained generator) and a GumbelGAN model [15] across all proposed datasets.\n sent2: We use three creative English datasets with distinct linguistic characteristics: (1) A corpus of 740 classical and contemporary English poems, (2) a corpus of 14950 metaphor sentences retrieved from a metaphor database website 1 and (3) a corpus of 1500 song lyrics ranging across genres.\n sent3: The mix of linguistic styles within this corpus offers the potential for interesting variation during the generation phase.\n sent4: We use the same pre-processing as in earlier work #TARGET_REF 24] .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "We use the same pre-processing as in earlier work [20, 24] .",
                "We reserve 10% of our data for test set and another 10% for our validation set.",
                "We first pre-train our generator on the Gutenberg dataset [25] for 20 epochs and then fine-tune #TARGET_REF them to our target datasets with a language modeling objective.",
                "The discriminator's encoder is initialized to the same weights as our fine-tuned language model.",
                "Once we have our fine-tuned encoders for each target dataset, we train in an adversarial manner."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We use the same pre-processing as in earlier work [20, 24] .\n sent1: We reserve 10% of our data for test set and another 10% for our validation set.\n sent2: We first pre-train our generator on the Gutenberg dataset [25] for 20 epochs and then fine-tune #TARGET_REF them to our target datasets with a language modeling objective.\n sent3: The discriminator's encoder is initialized to the same weights as our fine-tuned language model.\n sent4: Once we have our fine-tuned encoders for each target dataset, we train in an adversarial manner.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The discriminator decoder takes the hidden state at the last time step of a sequence concatenated with both the max-pooled and mean-pooled representation of the hidden states [20] and outputs a number in the range [0, 1].",
                "The difficulty of using GANs in text generation comes from the discrete nature of text, making the model non-differentiable hence, we update parameters for the generator model with policy gradients as described in #REF .",
                "We utilize AWD-LSTM [21] and TransformerXL [22] based language models.",
                "For model hyperparameters please to refer to Supplementary Section Table 2 .",
                "We use Adam optimizer [23] with β1 = 0.7 and β2 = 0.8 similar to #TARGET_REF and use a batch size of 50."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: The discriminator decoder takes the hidden state at the last time step of a sequence concatenated with both the max-pooled and mean-pooled representation of the hidden states [20] and outputs a number in the range [0, 1].\n sent1: The difficulty of using GANs in text generation comes from the discrete nature of text, making the model non-differentiable hence, we update parameters for the generator model with policy gradients as described in #REF .\n sent2: We utilize AWD-LSTM [21] and TransformerXL [22] based language models.\n sent3: For model hyperparameters please to refer to Supplementary Section Table 2 .\n sent4: We use Adam optimizer [23] with β1 = 0.7 and β2 = 0.8 similar to #TARGET_REF and use a batch size of 50.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Variational Autoencoder (VAE) is a powerful method for learning representations of highdimensional data.",
                "However, VAEs can suffer from an issue known as latent variable collapse (or KL loss vanishing), where the posterior collapses to the prior and the model will ignore the latent codes in generative tasks.",
                "Such an issue is particularly prevalent when employing VAE-RNN architectures for text modelling #TARGET_REF .",
                "In this paper, we present a simple architecture called holistic regularisation VAE (HR-VAE), which can effectively avoid latent variable collapse.",
                "Compared to existing VAE-RNN architectures, we show that our model can achieve much more stable training process and can generate text with significantly better quality."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Variational Autoencoder (VAE) is a powerful method for learning representations of highdimensional data.\n sent1: However, VAEs can suffer from an issue known as latent variable collapse (or KL loss vanishing), where the posterior collapses to the prior and the model will ignore the latent codes in generative tasks.\n sent2: Such an issue is particularly prevalent when employing VAE-RNN architectures for text modelling #TARGET_REF .\n sent3: In this paper, we present a simple architecture called holistic regularisation VAE (HR-VAE), which can effectively avoid latent variable collapse.\n sent4: Compared to existing VAE-RNN architectures, we show that our model can achieve much more stable training process and can generate text with significantly better quality.\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "In a more recent work, #REF avoided latent variable collapse by including skip connections in the generative model, where the skip connections enforce strong links between the latent variables and the likelihood function.",
                "Although the aforementioned works show effectiveness in addressing the latent variable collapse issue to some extent, they either require carefully engineering to balance the weight between the reconstruction loss and KL loss #TARGET_REF; , or resort to designing more sophisticated model structures (#REF; #REF; #REF) .",
                "In this paper, we present a simple architecture called holistic regularisation VAE (HR-VAE), which can effectively avoid latent variable collapse.",
                "In contrast to existing VAE-RNN models for text modelling which merely impose a standard normal distribution prior on the last hidden state of the RNN encoder, our HR-VAE model imposes regularisation for all hidden states of the RNN encoder.",
                "Another advantage of our model is that it is generic and can be applied to any existing VAE-RNN-based architectures."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: In a more recent work, #REF avoided latent variable collapse by including skip connections in the generative model, where the skip connections enforce strong links between the latent variables and the likelihood function.\n sent1: Although the aforementioned works show effectiveness in addressing the latent variable collapse issue to some extent, they either require carefully engineering to balance the weight between the reconstruction loss and KL loss #TARGET_REF; , or resort to designing more sophisticated model structures (#REF; #REF; #REF) .\n sent2: In this paper, we present a simple architecture called holistic regularisation VAE (HR-VAE), which can effectively avoid latent variable collapse.\n sent3: In contrast to existing VAE-RNN models for text modelling which merely impose a standard normal distribution prior on the last hidden state of the RNN encoder, our HR-VAE model imposes regularisation for all hidden states of the RNN encoder.\n sent4: Another advantage of our model is that it is generic and can be applied to any existing VAE-RNN-based architectures.\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "In this section, we discuss the technical details of the proposed holistic regularisation VAE (HR-VAE) model, a general architecture which can effectively mitigate the KL vanishing phenomenon.",
                "Our model design is motivated by one noticeable defect shared by the VAE-RNN based models in previous works #TARGET_REF; #REF; #REF; #REF) .",
                "That is, all these models, as shown in Figure 1a , only impose a standard normal distribution prior on the last hidden state of the RNN encoder, which potentially leads to learning a suboptimal representation of the latent variable and results in model vulnerable to KL loss vanishing.",
                "Our hypothesis is that to learn a good representation of data and a good generative model, it is crucial to impose the standard normal prior on all the hidden states of the RNN-based encoder (see Figure 1b ), which allows a better regularisation of the model learning process.",
                "We implement the HR-VAE model using a twolayer LSTM for both the encoder and decoder."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: In this section, we discuss the technical details of the proposed holistic regularisation VAE (HR-VAE) model, a general architecture which can effectively mitigate the KL vanishing phenomenon.\n sent1: Our model design is motivated by one noticeable defect shared by the VAE-RNN based models in previous works #TARGET_REF; #REF; #REF; #REF) .\n sent2: That is, all these models, as shown in Figure 1a , only impose a standard normal distribution prior on the last hidden state of the RNN encoder, which potentially leads to learning a suboptimal representation of the latent variable and results in model vulnerable to KL loss vanishing.\n sent3: Our hypothesis is that to learn a good representation of data and a good generative model, it is crucial to impose the standard normal prior on all the hidden states of the RNN-based encoder (see Figure 1b ), which allows a better regularisation of the model learning process.\n sent4: We implement the HR-VAE model using a twolayer LSTM for both the encoder and decoder.\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "However, recent attempts of applying VAEs to text modelling are still far less successful compared to its application to image and speech (#REF; #REF; #REF) .",
                "When applying VAEs for text modelling, recurrent neural networks (RNNs) 1 are commonly used as the architecture for both encoder and decoder #TARGET_REF; #REF; #REF) .",
                "While such a VAE-RNN based architecture allows encoding and generating sentences (in the decoding phase) with variablelength effectively, it is also vulnerable to an issue known as latent variable collapse (or KL loss vanishing) , where the posterior collapses to the prior and the model will ignore the latent codes in generative tasks.",
                "Various efforts have been made to alleviate the latent variable collapse issue.",
                "#REF uses KL annealing, where a variable weight is added to the KL term in the cost function at training time."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: However, recent attempts of applying VAEs to text modelling are still far less successful compared to its application to image and speech (#REF; #REF; #REF) .\n sent1: When applying VAEs for text modelling, recurrent neural networks (RNNs) 1 are commonly used as the architecture for both encoder and decoder #TARGET_REF; #REF; #REF) .\n sent2: While such a VAE-RNN based architecture allows encoding and generating sentences (in the decoding phase) with variablelength effectively, it is also vulnerable to an issue known as latent variable collapse (or KL loss vanishing) , where the posterior collapses to the prior and the model will ignore the latent codes in generative tasks.\n sent3: Various efforts have been made to alleviate the latent variable collapse issue.\n sent4: #REF uses KL annealing, where a variable weight is added to the KL term in the cost function at training time.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "While such a VAE-RNN based architecture allows encoding and generating sentences (in the decoding phase) with variablelength effectively, it is also vulnerable to an issue known as latent variable collapse (or KL loss vanishing) , where the posterior collapses to the prior and the model will ignore the latent codes in generative tasks.",
                "Various efforts have been made to alleviate the latent variable collapse issue.",
                "#TARGET_REF uses KL annealing, where a variable weight is added to the KL term in the cost function at training time.",
                "#REF discovered that there is a trade-off between the contextual capacity of the decoder and effective use of encoding information, and developed a dilated CNN as decoder which can vary the amount of conditioning context.",
                "They also introduced a loss clipping strategy in order to make the model more robust."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: While such a VAE-RNN based architecture allows encoding and generating sentences (in the decoding phase) with variablelength effectively, it is also vulnerable to an issue known as latent variable collapse (or KL loss vanishing) , where the posterior collapses to the prior and the model will ignore the latent codes in generative tasks.\n sent1: Various efforts have been made to alleviate the latent variable collapse issue.\n sent2: #TARGET_REF uses KL annealing, where a variable weight is added to the KL term in the cost function at training time.\n sent3: #REF discovered that there is a trade-off between the contextual capacity of the decoder and effective use of encoding information, and developed a dilated CNN as decoder which can vary the amount of conditioning context.\n sent4: They also introduced a loss clipping strategy in order to make the model more robust.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We evaluate our model on two public datasets, namely, Penn Treebank (PTB) (#REF) and the end-to-end (E2E) text generation corpus (#REF) , which have been used in a number of previous works for text generation #TARGET_REF; #REF; #REF; #REF) .",
                "PTB consists of more than 40,000 sentences from Wall Street Journal articles whereas the E2E dataset contains over 50,000 sen-tences of restaurant reviews.",
                "The statistics of these two datasets are summarised in Table 1 ."
            ],
            "label": [
                "BACKGROUND",
                "USE"
            ]
        },
        "input": "sent0: We evaluate our model on two public datasets, namely, Penn Treebank (PTB) (#REF) and the end-to-end (E2E) text generation corpus (#REF) , which have been used in a number of previous works for text generation #TARGET_REF; #REF; #REF; #REF) .\n sent1: PTB consists of more than 40,000 sentences from Wall Street Journal articles whereas the E2E dataset contains over 50,000 sen-tences of restaurant reviews.\n sent2: The statistics of these two datasets are summarised in Table 1 .\n",
        "output": "{\"label\": [\"BACKGROUND\", \"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Another advantage of our model is that it is generic and can be applied to any existing VAE-RNN-based architectures.",
                "We evaluate our model against several strong baselines which apply VAE for text modelling #TARGET_REF; #REF; #REF) .",
                "We conducted experiments based on two public benchmark datasets, namely, the Penn Treebank dataset (#REF) and the end-to-end (E2E) text generation dataset (#REF) .",
                "Experimental results show that our HR-VAE model not only can effectively mitigate the latent variable collapse issue with a stable training process, but also can give better predictive performance than the baselines, as evidenced by both quantitative (e.g., negative log likelihood and perplexity) and qualitative evaluation.",
                "The code for our model is available online 2 ."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Another advantage of our model is that it is generic and can be applied to any existing VAE-RNN-based architectures.\n sent1: We evaluate our model against several strong baselines which apply VAE for text modelling #TARGET_REF; #REF; #REF) .\n sent2: We conducted experiments based on two public benchmark datasets, namely, the Penn Treebank dataset (#REF) and the end-to-end (E2E) text generation dataset (#REF) .\n sent3: Experimental results show that our HR-VAE model not only can effectively mitigate the latent variable collapse issue with a stable training process, but also can give better predictive performance than the baselines, as evidenced by both quantitative (e.g., negative log likelihood and perplexity) and qualitative evaluation.\n sent4: The code for our model is available online 2 .\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent1\", \"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Finally, the KL divergence between these two multivariate Gaussian distributions (i.e., Q φt and P (z t )) will contribute to the overall KL loss of the ELBO.",
                "By taking the average of the KL loss at each time stamp t, the resulting ELBO takes the following form",
                "KL(Q φt (z t |x) P (z t )).",
                "( 3) As can be seen in Eq. 3, our solution to the KL collapse issue does not require any engineering for balancing the weight between the reconstruction term and KL loss as commonly the case in existing works #TARGET_REF; .",
                "The weight between these two terms of our model is simply 1 : 1."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Finally, the KL divergence between these two multivariate Gaussian distributions (i.e., Q φt and P (z t )) will contribute to the overall KL loss of the ELBO.\n sent1: By taking the average of the KL loss at each time stamp t, the resulting ELBO takes the following form\n sent2: KL(Q φt (z t |x) P (z t )).\n sent3: ( 3) As can be seen in Eq. 3, our solution to the KL collapse issue does not require any engineering for balancing the weight between the reconstruction term and KL loss as commonly the case in existing works #TARGET_REF; .\n sent4: The weight between these two terms of our model is simply 1 : 1.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "For the PTB dataset, we used the train-test split following #TARGET_REF; #REF) .",
                "For the E2E dataset, we used the train-test split from the original dataset (#REF) and indexed the words with a frequency higher than 3.",
                "We represent input data with 512-dimensional word2vec embeddings (#REF) .",
                "We set the dimension of the hidden layers of both encoder and decoder to 256.",
                "The Adam optimiser (#REF) was used for training with an initial learning rate of 0.0001."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: For the PTB dataset, we used the train-test split following #TARGET_REF; #REF) .\n sent1: For the E2E dataset, we used the train-test split from the original dataset (#REF) and indexed the words with a frequency higher than 3.\n sent2: We represent input data with 512-dimensional word2vec embeddings (#REF) .\n sent3: We set the dimension of the hidden layers of both encoder and decoder to 256.\n sent4: The Adam optimiser (#REF) was used for training with an initial learning rate of 0.0001.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "We compare our HR-VAE model with three strong baselines using VAE for text modelling: VAE-LSTM-base 3 : A variational autoencoder model which uses LSTM for both encoder and decoder.",
                "KL annealing is used to tackled the latent variable collapse issue #TARGET_REF ; VAE-CNN 4 : A variational autoencoder model with a LSTM encoder and a dilated CNN decoder (#REF) ; vMF-VAE 5 : A variational autoencoder model using LSTM for both encoder and decoder where the prior distribution is the von Mises-Fisher (vMF) distribution rather than a Gaussian distribution (#REF) .",
                "the decoder needs to predict the entire sequence with only the help of the given latent variable z.",
                "In this way, a high-quality representation abstracting the information of the input sentence is much needed for the decoder, and hence enforcing z to learn the required information.",
                "Overall performance."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We compare our HR-VAE model with three strong baselines using VAE for text modelling: VAE-LSTM-base 3 : A variational autoencoder model which uses LSTM for both encoder and decoder.\n sent1: KL annealing is used to tackled the latent variable collapse issue #TARGET_REF ; VAE-CNN 4 : A variational autoencoder model with a LSTM encoder and a dilated CNN decoder (#REF) ; vMF-VAE 5 : A variational autoencoder model using LSTM for both encoder and decoder where the prior distribution is the von Mises-Fisher (vMF) distribution rather than a Gaussian distribution (#REF) .\n sent2: the decoder needs to predict the entire sequence with only the help of the given latent variable z.\n sent3: In this way, a high-quality representation abstracting the information of the input sentence is much needed for the decoder, and hence enforcing z to learn the required information.\n sent4: Overall performance.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "To conduct a more thorough evaluation, we further investigate model behaviours in terms of both reconstruction loss and KL loss, as shown in Figure 2 .",
                "These plots were obtained based on the E2E training set using the inputless setting.",
                "We can see that the KL loss of VAE-LSTMbase, which uses Sigmoid annealing #TARGET_REF , collapses to zero, leading to a poor generative performance as indicated by the high reconstruction loss.",
                "The KL loss for both VAE-CNN and vMF-VAE are nonzero, where the former mitigates the KL collapse issue with a KL loss clipping strategy and the latter by replacing the standard normal distribution for the prior with the vMF distribution (i.e., with the vMF distribution, the KL loss only depends on a fixed concentration parameter, and hence results in a constant KL loss).",
                "Although both VAE-CNN and vMF-VAE outperform VAE-LSTM-base by a large margin in terms of reconstruction loss as shown in Figure 2 , one should also notice that these two models actually overfit the training data, as their performance on the test set is much worse (cf."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: To conduct a more thorough evaluation, we further investigate model behaviours in terms of both reconstruction loss and KL loss, as shown in Figure 2 .\n sent1: These plots were obtained based on the E2E training set using the inputless setting.\n sent2: We can see that the KL loss of VAE-LSTMbase, which uses Sigmoid annealing #TARGET_REF , collapses to zero, leading to a poor generative performance as indicated by the high reconstruction loss.\n sent3: The KL loss for both VAE-CNN and vMF-VAE are nonzero, where the former mitigates the KL collapse issue with a KL loss clipping strategy and the latter by replacing the standard normal distribution for the prior with the vMF distribution (i.e., with the vMF distribution, the KL loss only depends on a fixed concentration parameter, and hence results in a constant KL loss).\n sent4: Although both VAE-CNN and vMF-VAE outperform VAE-LSTM-base by a large margin in terms of reconstruction loss as shown in Figure 2 , one should also notice that these two models actually overfit the training data, as their performance on the test set is much worse (cf.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Recent work has shown benefits of combining conventional lexical information into neural cross-lingual part-ofspeech (PoS) tagging #TARGET_REF .",
                "However, little is known on how complementary such additional information is, and to what extent improvements depend on the coverage and quality of these external resources.",
                "The contribution of this paper is in the analysis of the contributions of models' components (tagger transfer through annotation projection vs. the contribution of encoding lexical and morphosyntactic resources).",
                "We seek to understand under which conditions a low-resource neural tagger benefits from external lexical knowledge.",
                "In particular: a) we evaluate the neural tagger across a total of 20+ languages, proposing a novel baseline which uses retrofitting; b) we investigate the reliance on dictionary size and properties; c) we analyze model-internal representations via a probing task to investigate to what extent model-internal representations capture morphosyntactic information."
            ],
            "label": [
                "BACKGROUND",
                "MOTIVATION"
            ]
        },
        "input": "sent0: Recent work has shown benefits of combining conventional lexical information into neural cross-lingual part-ofspeech (PoS) tagging #TARGET_REF .\n sent1: However, little is known on how complementary such additional information is, and to what extent improvements depend on the coverage and quality of these external resources.\n sent2: The contribution of this paper is in the analysis of the contributions of models' components (tagger transfer through annotation projection vs. the contribution of encoding lexical and morphosyntactic resources).\n sent3: We seek to understand under which conditions a low-resource neural tagger benefits from external lexical knowledge.\n sent4: In particular: a) we evaluate the neural tagger across a total of 20+ languages, proposing a novel baseline which uses retrofitting; b) we investigate the reliance on dictionary size and properties; c) we analyze model-internal representations via a probing task to investigate to what extent model-internal representations capture morphosyntactic information.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"MOTIVATION\"], \"context\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "However, we use identical settings for each language which worked well and is less expensive, following #REF .",
                "For all experiments, we average over 3 randomly seeded runs, and provide mean accuracy.",
                "We use the off-the-shelf Polyglot word embeddings (#REF) .",
                "Word embedding initialization provides a consistent and considerable boost in this cross-lingual setup, up to 10% absolute improvements across 21 languages when only 500 projected training instances are available #TARGET_REF .",
                "Note that we em- pirically find it to be best to not update the word embeddings in this noisy training setup, as that results in better performance, see Section 4.4."
            ],
            "label": [
                "BACKGROUND",
                "DIFFERENCES"
            ]
        },
        "input": "sent0: However, we use identical settings for each language which worked well and is less expensive, following #REF .\n sent1: For all experiments, we average over 3 randomly seeded runs, and provide mean accuracy.\n sent2: We use the off-the-shelf Polyglot word embeddings (#REF) .\n sent3: Word embedding initialization provides a consistent and considerable boost in this cross-lingual setup, up to 10% absolute improvements across 21 languages when only 500 projected training instances are available #TARGET_REF .\n sent4: Note that we em- pirically find it to be best to not update the word embeddings in this noisy training setup, as that results in better performance, see Section 4.4.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"DIFFERENCES\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "1 The sizes of the dictionaries vary considerably, from a few thousand entries (e.g., for Hindi and Bulgarian) to 2M entries (Finnish Uni-Morph).",
                "We study the impact of smaller dictionary sizes in Section 4.1.",
                "The tagger we analyze in this paper is an extension of the base tagger, called distant supervision from disparate sources (DSDS) tagger #TARGET_REF .",
                "It is trained on projected data and further differs from the base tagger by the integration of lexicon information.",
                "In particular, given a lexicon src, DSDS uses e src to embed the lexicon into an l-dimensional space, where e src is the concatenation of all embedded m properties of length l (empirically set, see Section 2.2), and a zero vector for words not in the lexicon."
            ],
            "label": [
                "USE",
                "EXTENDS"
            ]
        },
        "input": "sent0: 1 The sizes of the dictionaries vary considerably, from a few thousand entries (e.g., for Hindi and Bulgarian) to 2M entries (Finnish Uni-Morph).\n sent1: We study the impact of smaller dictionary sizes in Section 4.1.\n sent2: The tagger we analyze in this paper is an extension of the base tagger, called distant supervision from disparate sources (DSDS) tagger #TARGET_REF .\n sent3: It is trained on projected data and further differs from the base tagger by the integration of lexicon information.\n sent4: In particular, given a lexicon src, DSDS uses e src to embed the lexicon into an l-dimensional space, where e src is the concatenation of all embedded m properties of length l (empirically set, see Section 2.2), and a zero vector for words not in the lexicon.\n",
        "output": "{\"label\": [\"USE\", \"EXTENDS\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "In this section we describe the baselines, the data and the tagger hyperparameters.",
                "Data We use the 12 Universal PoS tags (#REF) .",
                "The set of languages is motivated by accessibility to embeddings and dictionaries.",
                "We here focus on 21 dev sets of the Universal Dependencies 2.1 (Nivre and et al., 2017) , test set results are reported by #TARGET_REF showing that DSDS provides a viable alternative.",
                "Annotation projection To build the taggers for new languages, we resort to annotation projection following Plank and Agić (2018) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In this section we describe the baselines, the data and the tagger hyperparameters.\n sent1: Data We use the 12 Universal PoS tags (#REF) .\n sent2: The set of languages is motivated by accessibility to embeddings and dictionaries.\n sent3: We here focus on 21 dev sets of the Universal Dependencies 2.1 (Nivre and et al., 2017) , test set results are reported by #TARGET_REF showing that DSDS provides a viable alternative.\n sent4: Annotation projection To build the taggers for new languages, we resort to annotation projection following Plank and Agić (2018) .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "We here focus on 21 dev sets of the Universal Dependencies 2.1 (Nivre and et al., 2017) , test set results are reported by Plank and Agić (2018) showing that DSDS provides a viable alternative.",
                "Annotation projection To build the taggers for new languages, we resort to annotation projection following #TARGET_REF .",
                "In particular, they employ the approach by Agić et al. (2016) , where labels are projected from multiple sources to multiple targets and then decoded through weighted majority voting with word alignment probabilities and source PoS tagger confidences.",
                "The wide-coverage Watchtower corpus (WTC) by Agić et al. (2016) is used, where 5k instances are selected via data selection by alignment coverage following Plank and Agić (2018) .",
                "Baselines We compare to the following alternatives: type-constraint Wiktionary supervision (#REF) and retrofitting initialization."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We here focus on 21 dev sets of the Universal Dependencies 2.1 (Nivre and et al., 2017) , test set results are reported by Plank and Agić (2018) showing that DSDS provides a viable alternative.\n sent1: Annotation projection To build the taggers for new languages, we resort to annotation projection following #TARGET_REF .\n sent2: In particular, they employ the approach by Agić et al. (2016) , where labels are projected from multiple sources to multiple targets and then decoded through weighted majority voting with word alignment probabilities and source PoS tagger confidences.\n sent3: The wide-coverage Watchtower corpus (WTC) by Agić et al. (2016) is used, where 5k instances are selected via data selection by alignment coverage following Plank and Agić (2018) .\n sent4: Baselines We compare to the following alternatives: type-constraint Wiktionary supervision (#REF) and retrofitting initialization.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Annotation projection To build the taggers for new languages, we resort to annotation projection following Plank and Agić (2018) .",
                "In particular, they employ the approach by Agić et al. (2016) , where labels are projected from multiple sources to multiple targets and then decoded through weighted majority voting with word alignment probabilities and source PoS tagger confidences.",
                "The wide-coverage Watchtower corpus (WTC) by Agić et al. (2016) is used, where 5k instances are selected via data selection by alignment coverage following #TARGET_REF .",
                "Baselines We compare to the following alternatives: type-constraint Wiktionary supervision (#REF) and retrofitting initialization.",
                "Hyperparameters We use the same setup as Plank and Agić (2018) , i.e., 10 epochs, word dropout rate (p=.25) and l=40-dimensional lexicon embeddings for DSDS, except for downscaling the hidden dimensionality of the character representations from 100 to 32 dimensions."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Annotation projection To build the taggers for new languages, we resort to annotation projection following Plank and Agić (2018) .\n sent1: In particular, they employ the approach by Agić et al. (2016) , where labels are projected from multiple sources to multiple targets and then decoded through weighted majority voting with word alignment probabilities and source PoS tagger confidences.\n sent2: The wide-coverage Watchtower corpus (WTC) by Agić et al. (2016) is used, where 5k instances are selected via data selection by alignment coverage following #TARGET_REF .\n sent3: Baselines We compare to the following alternatives: type-constraint Wiktionary supervision (#REF) and retrofitting initialization.\n sent4: Hyperparameters We use the same setup as Plank and Agić (2018) , i.e., 10 epochs, word dropout rate (p=.25) and l=40-dimensional lexicon embeddings for DSDS, except for downscaling the hidden dimensionality of the character representations from 100 to 32 dimensions.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "In particular, they employ the approach by Agić et al. (2016) , where labels are projected from multiple sources to multiple targets and then decoded through weighted majority voting with word alignment probabilities and source PoS tagger confidences.",
                "The wide-coverage Watchtower corpus (WTC) by Agić et al. (2016) is used, where 5k instances are selected via data selection by alignment coverage following Plank and Agić (2018) .",
                "Baselines We compare to the following alternatives: type-constraint Wiktionary supervision (#REF) and retrofitting initialization.",
                "Hyperparameters We use the same setup as #TARGET_REF , i.e., 10 epochs, word dropout rate (p=.25) and l=40-dimensional lexicon embeddings for DSDS, except for downscaling the hidden dimensionality of the character representations from 100 to 32 dimensions.",
                "This ensures that our probing tasks always get the same input dimensionality: 64 (2x32) dimensions for cw, which is the same dimension as the off-theshelf word embeddings."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In particular, they employ the approach by Agić et al. (2016) , where labels are projected from multiple sources to multiple targets and then decoded through weighted majority voting with word alignment probabilities and source PoS tagger confidences.\n sent1: The wide-coverage Watchtower corpus (WTC) by Agić et al. (2016) is used, where 5k instances are selected via data selection by alignment coverage following Plank and Agić (2018) .\n sent2: Baselines We compare to the following alternatives: type-constraint Wiktionary supervision (#REF) and retrofitting initialization.\n sent3: Hyperparameters We use the same setup as #TARGET_REF , i.e., 10 epochs, word dropout rate (p=.25) and l=40-dimensional lexicon embeddings for DSDS, except for downscaling the hidden dimensionality of the character representations from 100 to 32 dimensions.\n sent4: This ensures that our probing tasks always get the same input dimensionality: 64 (2x32) dimensions for cw, which is the same dimension as the off-theshelf word embeddings.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The lexicons we use so far are of different sizes (shown in Table 1 of #TARGET_REF ), spanning from 1,000 entries to considerable dictionaries of several hundred thousands entries.",
                "In a low-resource setup, large dictionaries might not be available.",
                "It is thus interesting to examine how tagging accuracy is affected by dictionary size.",
                "We examine two cases: randomly sampling dictionary entries and sampling by word frequency, over increasing dictionary sizes: 50, 100, 200, 400, 800, 1600 word types.",
                "The latter is motivated by the fact that an informed dictionary creation (under limited resources) might be more beneficial."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The lexicons we use so far are of different sizes (shown in Table 1 of #TARGET_REF ), spanning from 1,000 entries to considerable dictionaries of several hundred thousands entries.\n sent1: In a low-resource setup, large dictionaries might not be available.\n sent2: It is thus interesting to examine how tagging accuracy is affected by dictionary size.\n sent3: We examine two cases: randomly sampling dictionary entries and sampling by word frequency, over increasing dictionary sizes: 50, 100, 200, 400, 800, 1600 word types.\n sent4: The latter is motivated by the fact that an informed dictionary creation (under limited resources) might be more beneficial.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "They rely on end-to-end training without resorting to additional linguistic resources.",
                "Our study contributes to the increasing literature to show the utility of linguistic resources for deep learning models by providing a deep analysis of a recently proposed model #TARGET_REF .",
                "Most prior work in this direction can be found on machine translation (#REF; #REF; #REF; #REF) , work on named entity recognition (#REF) and PoS tagging (Sagot and Martínez #REF) who use lexicons, but as n-hot features and without examining the crosslingual aspect.",
                "Somewhat complementary to evaluating the utility of linguistic resources empirically is the increasing body of work that uses linguistic insights to try to understand what properties neural-based representations capture (Kádár et al., 2017; #REF; #REF; #REF) .",
                "#REF and #REF introduced the idea of probing tasks (or 'diagnostic classifiers'), see Belinkov and Glass for a recent survey (#REF) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: They rely on end-to-end training without resorting to additional linguistic resources.\n sent1: Our study contributes to the increasing literature to show the utility of linguistic resources for deep learning models by providing a deep analysis of a recently proposed model #TARGET_REF .\n sent2: Most prior work in this direction can be found on machine translation (#REF; #REF; #REF; #REF) , work on named entity recognition (#REF) and PoS tagging (Sagot and Martínez #REF) who use lexicons, but as n-hot features and without examining the crosslingual aspect.\n sent3: Somewhat complementary to evaluating the utility of linguistic resources empirically is the increasing body of work that uses linguistic insights to try to understand what properties neural-based representations capture (Kádár et al., 2017; #REF; #REF; #REF) .\n sent4: #REF and #REF introduced the idea of probing tasks (or 'diagnostic classifiers'), see Belinkov and Glass for a recent survey (#REF) .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We analyze DSDS, a recently-proposed lowresource tagger that symbiotically leverages neural representations and symbolic linguistic knowledge by integrating them in a soft manner.",
                "We replicated the results of #TARGET_REF , showing that the more implicit use of embedding user-generated dictionaries turns out to be more beneficial than approaches that rely more explicitly on symbolic knowledge, such a type constraints or retrofitting.",
                "By analyzing the reliance of DSDS on the linguistic knowledge, we found that the composition of the lexicon is more important than its size.",
                "Moreover, the tagger benefits from small dictionaries, as long as they do not contain tag set information contradictory to the evaluation data.",
                "Our quantitative analysis also sheds light on the internal representations, showing that they get more sensitive to the task."
            ],
            "label": [
                "USE",
                "EXTENDS"
            ]
        },
        "input": "sent0: We analyze DSDS, a recently-proposed lowresource tagger that symbiotically leverages neural representations and symbolic linguistic knowledge by integrating them in a soft manner.\n sent1: We replicated the results of #TARGET_REF , showing that the more implicit use of embedding user-generated dictionaries turns out to be more beneficial than approaches that rely more explicitly on symbolic knowledge, such a type constraints or retrofitting.\n sent2: By analyzing the reliance of DSDS on the linguistic knowledge, we found that the composition of the lexicon is more important than its size.\n sent3: Moreover, the tagger benefits from small dictionaries, as long as they do not contain tag set information contradictory to the evaluation data.\n sent4: Our quantitative analysis also sheds light on the internal representations, showing that they get more sensitive to the task.\n",
        "output": "{\"label\": [\"USE\", \"EXTENDS\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Combining the best of two worlds results in the overall best tagging accuracy, confirming #TARGET_REF : Embedding lexical information into a neural tagger improves tagging accuracy from 83.4 to 84.1 (means over 21 languages).",
                "On 15 out of 21 languages, DSDS is the best performing model.",
                "On two languages, type constraints work the best (English and Greek).",
                "Retrofitting performs best only on one language (Persian); this is the language with the overall lowest performance.",
                "On three languages, Czech, French and Hungarian, the baseline remains the best model, none of the lexicon-enriching approaches works."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: Combining the best of two worlds results in the overall best tagging accuracy, confirming #TARGET_REF : Embedding lexical information into a neural tagger improves tagging accuracy from 83.4 to 84.1 (means over 21 languages).\n sent1: On 15 out of 21 languages, DSDS is the best performing model.\n sent2: On two languages, type constraints work the best (English and Greek).\n sent3: Retrofitting performs best only on one language (Persian); this is the language with the overall lowest performance.\n sent4: On three languages, Czech, French and Hungarian, the baseline remains the best model, none of the lexicon-enriching approaches works.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Thus, we define our task as predicting the AMR tree structure, given a sequence of words in an NP.",
                "The previous method for AMR parsing takes a Train Dev #REF 463 398 Table 1 : Statistics of the extracted NP data two-step approach: first identifying distinct concepts (nodes) in the AMR graph, then defining the dependency relations between those concepts #TARGET_REF .",
                "In the concept identification step, unlike POS tagging, one word is sometimes assigned with more than one concept, and the number of possible concepts is far more than the number of possible parts-of-speech.",
                "As the concept identification accuracy remains low, such a pipeline method suffers from error propagation, thus resulting in a suboptimal AMR parsing performance.",
                "To solve this problem, we extend a transitionbased dependency parsing algorithm, and propose a novel algorithm which jointly identifies the concepts and the relations in AMR trees."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Thus, we define our task as predicting the AMR tree structure, given a sequence of words in an NP.\n sent1: The previous method for AMR parsing takes a Train Dev #REF 463 398 Table 1 : Statistics of the extracted NP data two-step approach: first identifying distinct concepts (nodes) in the AMR graph, then defining the dependency relations between those concepts #TARGET_REF .\n sent2: In the concept identification step, unlike POS tagging, one word is sometimes assigned with more than one concept, and the number of possible concepts is far more than the number of possible parts-of-speech.\n sent3: As the concept identification accuracy remains low, such a pipeline method suffers from error propagation, thus resulting in a suboptimal AMR parsing performance.\n sent4: To solve this problem, we extend a transitionbased dependency parsing algorithm, and propose a novel algorithm which jointly identifies the concepts and the relations in AMR trees.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We extract substructures (subtrees) corresponding to NPs from the AMR Bank (LDC2014T12).",
                "In the AMR Bank, there is no alignment between the words and the concepts (nodes) in the AMR graphs.",
                "We obtain this alignment by using the rule-based alignment tool by #TARGET_REF .",
                "Then, we use the Stanford Parser (#REF) to obtain constituency trees, and extract NPs that contain more than one noun and are not included by another NP.",
                "We exclude NPs that contain named entities, because they would require various kinds of manually crafted rules for each type of named entity."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We extract substructures (subtrees) corresponding to NPs from the AMR Bank (LDC2014T12).\n sent1: In the AMR Bank, there is no alignment between the words and the concepts (nodes) in the AMR graphs.\n sent2: We obtain this alignment by using the rule-based alignment tool by #TARGET_REF .\n sent3: Then, we use the Stanford Parser (#REF) to obtain constituency trees, and extract NPs that contain more than one noun and are not included by another NP.\n sent4: We exclude NPs that contain named entities, because they would require various kinds of manually crafted rules for each type of named entity.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We adopt the method proposed by #TARGET_REF as our baseline, which is a two-step pipeline method of concept identification step and #TARGET_REF for a retired plant worker.",
                "∅ denotes an empty concept.",
                "relation identification step.",
                "Their method is designed for parsing sentences into AMR, but here, we use this method for parsing NPs.",
                "In their method, concept identification is formulated as a sequence labeling problem (#REF) and solved by the Viterbi algorithm."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We adopt the method proposed by #TARGET_REF as our baseline, which is a two-step pipeline method of concept identification step and #TARGET_REF for a retired plant worker.\n sent1: ∅ denotes an empty concept.\n sent2: relation identification step.\n sent3: Their method is designed for parsing sentences into AMR, but here, we use this method for parsing NPs.\n sent4: In their method, concept identification is formulated as a sequence labeling problem (#REF) and solved by the Viterbi algorithm.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "We conduct an experiment using our NP data set (Table 1) .",
                "We use the implementation 2 of #TARGET_REF as our baseline.",
                "For the baseline, we use the features of the default settings.",
                "The method by #REF can only generate the concepts that appear in the training data.",
                "On the other hand, our method can generate concepts that do not appear in the training data using the concept generation rules LEMMA, DICT PRED , and DICT NOUN in Table 3 ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We conduct an experiment using our NP data set (Table 1) .\n sent1: We use the implementation 2 of #TARGET_REF as our baseline.\n sent2: For the baseline, we use the features of the default settings.\n sent3: The method by #REF can only generate the concepts that appear in the training data.\n sent4: On the other hand, our method can generate concepts that do not appear in the training data using the concept generation rules LEMMA, DICT PRED , and DICT NOUN in Table 3 .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The method by #TARGET_REF can only generate the concepts that appear in the training data.",
                "On the other hand, our method can generate concepts that do not appear in the training data using the concept generation rules LEMMA, DICT PRED , and DICT NOUN in Table 3 .",
                "For a fair comparison, first, we only use the rules EMPTY and KNOWN.",
                "Then, we add the rule LEMMA, which can generate a concept of the lemma of the 2 https://github.com/jflanigan/jamr Name Definition LEM {w(σ1).lem, w(σ0).lem, β0.lem, w(σ1).lem • w(σ0).lem, w(σ0).lem • β0.lem} SUF {w(σ1).suf, w(σ0).suf, β0.suf, w(σ1).suf • w(σ0).suf, w(σ0).suf • β0.suf} POS {w(σ1).pos, w(σ0).pos, β0.pos, w(σ1).pos • w(σ0).pos, w(σ0).pos",
                "all words between w(σ1) and w(σ0) ∪ all words between w(σ0) and β0 Table 5 : Feature sets for the action word."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: The method by #TARGET_REF can only generate the concepts that appear in the training data.\n sent1: On the other hand, our method can generate concepts that do not appear in the training data using the concept generation rules LEMMA, DICT PRED , and DICT NOUN in Table 3 .\n sent2: For a fair comparison, first, we only use the rules EMPTY and KNOWN.\n sent3: Then, we add the rule LEMMA, which can generate a concept of the lemma of the 2 https://github.com/jflanigan/jamr Name Definition LEM {w(σ1).lem, w(σ0).lem, β0.lem, w(σ1).lem • w(σ0).lem, w(σ0).lem • β0.lem} SUF {w(σ1).suf, w(σ0).suf, β0.suf, w(σ1).suf • w(σ0).suf, w(σ0).suf • β0.suf} POS {w(σ1).pos, w(σ0).pos, β0.pos, w(σ1).pos • w(σ0).pos, w(σ0).pos\n sent4: all words between w(σ1) and w(σ0) ∪ all words between w(σ0) and β0 Table 5 : Feature sets for the action word.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Domain adaptation aims to obtain an effective prediction model for a particular target domain where labeled training data is scarce by exploiting labeled data from a related source domain.",
                "Domain adaptation is very important in the field of natural language processing (NLP) as it can reduce the expensive manual annotation effort in the target domain.",
                "Various NLP tasks have benefited from domain adaptation techniques, including part-ofspeech tagging (#REF; #REFa) , chunking (Daumé III, 2007; #TARGET_REF , named entity recognition (#REF; #REF) , dependency parsing (#REF; #REF) and semantic role labeling (#REF; #REFb) .",
                "In a typical domain adaptation scenario of NLP, the source and target domains contain text data of different genres (e.g., newswire vs biomedical (#REF) ).",
                "Under such circumstances, the original lexical features may not perform well in cross-domain learning since different genres of text may use very different vocabularies and produce cross-domain feature distribution divergence and feature sparsity issue."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Domain adaptation aims to obtain an effective prediction model for a particular target domain where labeled training data is scarce by exploiting labeled data from a related source domain.\n sent1: Domain adaptation is very important in the field of natural language processing (NLP) as it can reduce the expensive manual annotation effort in the target domain.\n sent2: Various NLP tasks have benefited from domain adaptation techniques, including part-ofspeech tagging (#REF; #REFa) , chunking (Daumé III, 2007; #TARGET_REF , named entity recognition (#REF; #REF) , dependency parsing (#REF; #REF) and semantic role labeling (#REF; #REFb) .\n sent3: In a typical domain adaptation scenario of NLP, the source and target domains contain text data of different genres (e.g., newswire vs biomedical (#REF) ).\n sent4: Under such circumstances, the original lexical features may not perform well in cross-domain learning since different genres of text may use very different vocabularies and produce cross-domain feature distribution divergence and feature sparsity issue.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "In a typical domain adaptation scenario of NLP, the source and target domains contain text data of different genres (e.g., newswire vs biomedical (#REF) ).",
                "Under such circumstances, the original lexical features may not perform well in cross-domain learning since different genres of text may use very different vocabularies and produce cross-domain feature distribution divergence and feature sparsity issue.",
                "A number of techniques have been developed in the literature to tackle the problem of cross-domain feature divergence and feature sparsity, including clustering based word representation learning methods #TARGET_REF; #REF) , word embedding based representation learning methods (#REF; #REF) and some other representation learning methods (#REF) .",
                "In this paper, we extend the standard hidden Markov models (HMMs) to perform distributed state representation learning and induce contextaware distributed word representations for domain adaptation.",
                "Instead of learning a single discrete latent state for each observation in a given sentence, we learn a distributed representation vector."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In a typical domain adaptation scenario of NLP, the source and target domains contain text data of different genres (e.g., newswire vs biomedical (#REF) ).\n sent1: Under such circumstances, the original lexical features may not perform well in cross-domain learning since different genres of text may use very different vocabularies and produce cross-domain feature distribution divergence and feature sparsity issue.\n sent2: A number of techniques have been developed in the literature to tackle the problem of cross-domain feature divergence and feature sparsity, including clustering based word representation learning methods #TARGET_REF; #REF) , word embedding based representation learning methods (#REF; #REF) and some other representation learning methods (#REF) .\n sent3: In this paper, we extend the standard hidden Markov models (HMMs) to perform distributed state representation learning and induce contextaware distributed word representations for domain adaptation.\n sent4: Instead of learning a single discrete latent state for each observation in a given sentence, we learn a distributed representation vector.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "A variety of representation learning approaches have been developed in the literature to address NLP domain adaptation problems.",
                "The clustering based word representation learning methods perform word clustering within the sentence structure and use word cluster indicators as generalizable features to address domain adaptation problems.",
                "For example, #TARGET_REF used the discrete hidden state of a word under HMMs as augmenting features for cross-domain POS tagging and NP chunking.",
                "Brown clusters (#REF) , which was used as latent features for simple in-domain dependency parsing (#REF) , has recently been exploited for out-ofdomain statistical parsing (#REF) .",
                "The word embedding based representation learning methods learn a dense real-valued representation vector for each word as latent features for domain adaptation."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: A variety of representation learning approaches have been developed in the literature to address NLP domain adaptation problems.\n sent1: The clustering based word representation learning methods perform word clustering within the sentence structure and use word cluster indicators as generalizable features to address domain adaptation problems.\n sent2: For example, #TARGET_REF used the discrete hidden state of a word under HMMs as augmenting features for cross-domain POS tagging and NP chunking.\n sent3: Brown clusters (#REF) , which was used as latent features for simple in-domain dependency parsing (#REF) , has recently been exploited for out-ofdomain statistical parsing (#REF) .\n sent4: The word embedding based representation learning methods learn a dense real-valued representation vector for each word as latent features for domain adaptation.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                ". , s T } be the sequence of T hidden states, where each hidden state s t has a discrete state value from a total H hidden states H = {1, 2, . . . , H}. Besides, we assume that there is a low-dimensional distributed representation vector associated with each hidden state.",
                "Let M ∈ R H×m be the state embedding matrix where the i-th row M i: denotes the m-dimensional representation vector for the i-th state.",
                "Previous works have demonstrated the usefulness of discrete hidden states induced from a HMM on addressing feature sparsity in domain adaptation #TARGET_REF ).",
                "However, expressing a semantic word by a single discrete state value is too restrictive, as it has been shown in the literature that words have many different features in a multidimensional space where they could be separately characterized as number, POS tag, gender, tense, voice and other aspects (#REF; #REF) .",
                "Our proposed model aims to overcome this inherent drawback of standard HMMs on learning word representations."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: . , s T } be the sequence of T hidden states, where each hidden state s t has a discrete state value from a total H hidden states H = {1, 2, . . . , H}. Besides, we assume that there is a low-dimensional distributed representation vector associated with each hidden state.\n sent1: Let M ∈ R H×m be the state embedding matrix where the i-th row M i: denotes the m-dimensional representation vector for the i-th state.\n sent2: Previous works have demonstrated the usefulness of discrete hidden states induced from a HMM on addressing feature sparsity in domain adaptation #TARGET_REF ).\n sent3: However, expressing a semantic word by a single discrete state value is too restrictive, as it has been shown in the literature that words have many different features in a multidimensional space where they could be separately characterized as number, POS tag, gender, tense, voice and other aspects (#REF; #REF) .\n sent4: Our proposed model aims to overcome this inherent drawback of standard HMMs on learning word representations.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We then simultaneously learn the state embedding matrix and the model parameters using an expectation-maximization (EM) algorithm.",
                "The hidden states of each word in a sentence can be decoded using the standard Viterbi decoding procedure of HMMs, and its distributed representation can be obtained by a simple mapping with the state embedding matrix.",
                "We then use the context-aware distributed representations of the words as their augmenting features to perform cross-domain part-of-speech (POS) tagging and noun-phrase (NP) chunking.",
                "The proposed approach is closely related to the clustering based method #TARGET_REF ) as we both use latent state representations as generalizable features.",
                "However, they use standard HMMs to produce discrete hidden state features for each observation word, while we induce distributed state representation vectors."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: We then simultaneously learn the state embedding matrix and the model parameters using an expectation-maximization (EM) algorithm.\n sent1: The hidden states of each word in a sentence can be decoded using the standard Viterbi decoding procedure of HMMs, and its distributed representation can be obtained by a simple mapping with the state embedding matrix.\n sent2: We then use the context-aware distributed representations of the words as their augmenting features to perform cross-domain part-of-speech (POS) tagging and noun-phrase (NP) chunking.\n sent3: The proposed approach is closely related to the clustering based method #TARGET_REF ) as we both use latent state representations as generalizable features.\n sent4: However, they use standard HMMs to produce discrete hidden state features for each observation word, while we induce distributed state representation vectors.\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "We conducted experiments on cross-domain partof-speech (POS) tagging and noun-phrase (NP) chunking tasks.",
                "We used the same experimental datasets as in #TARGET_REF ) for cross-domain POS tagging from Wall Street Journal (WSJ) domain (#REF) to MED-LINE domain (PennBioIE, 2005) and for crossdomain NP chunking from CoNLL shared task dataset (#REF) to Open American National Corpus (OANC) (#REF) ."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: We conducted experiments on cross-domain partof-speech (POS) tagging and noun-phrase (NP) chunking tasks.\n sent1: We used the same experimental datasets as in #TARGET_REF ) for cross-domain POS tagging from Wall Street Journal (WSJ) domain (#REF) to MED-LINE domain (PennBioIE, 2005) and for crossdomain NP chunking from CoNLL shared task dataset (#REF) to Open American National Corpus (OANC) (#REF) .\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We used the induced distributed state representations of each observation as augmenting features to train conditional random fields (CRF) with the CRFSuite package (#REF) on the labeled source sentences and perform prediction on the target test sentences.",
                "We compared with the following systems: a Baseline system without representation learning, a SGM based word embedding system (#REF) , and a discrete hidden state based clustering system #TARGET_REF .",
                "We used the word id and orthographic features as the baseline features for POS tagging and added POS tags for NP chunking.",
                "We reported the POS tagging accuracy for all words and outof-vocabulary (OOV) words (which appear less than three times in the labeled source training sentences), and NP chunking F1 scores for all NPs and only OOV NPs (whose beginning word is an OOV word) in Table 1 .",
                "We can see that the Baseline method performs poorly on both tasks especially on the OOV words/NPs, which shows that the original lexical based features are not sufficient to develop a robust POS tagger/NP chunker for the target domain with labeled source training sentences."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: We used the induced distributed state representations of each observation as augmenting features to train conditional random fields (CRF) with the CRFSuite package (#REF) on the labeled source sentences and perform prediction on the target test sentences.\n sent1: We compared with the following systems: a Baseline system without representation learning, a SGM based word embedding system (#REF) , and a discrete hidden state based clustering system #TARGET_REF .\n sent2: We used the word id and orthographic features as the baseline features for POS tagging and added POS tags for NP chunking.\n sent3: We reported the POS tagging accuracy for all words and outof-vocabulary (OOV) words (which appear less than three times in the labeled source training sentences), and NP chunking F1 scores for all NPs and only OOV NPs (whose beginning word is an OOV word) in Table 1 .\n sent4: We can see that the Baseline method performs poorly on both tasks especially on the OOV words/NPs, which shows that the original lexical based features are not sufficient to develop a robust POS tagger/NP chunker for the target domain with labeled source training sentences.\n",
        "output": "{\"label\": [null], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Evaluation in QA-SRL involves aligning predicted and ground truth argument spans and evaluating role label equivalence.",
                "Since detecting question paraphrases is still an open challenge, we propose both unlabeled and labeled evaluation metrics.",
                "Unlabeled Argument Detection (UA) Inspired by the method presented in ( #TARGET_REF) , arguments are matched using a span matching criterion of intersection over union ≥ 0.5 .",
                "To credit each argument only once, we employ maximal bipartite matching 4 between the two sets of arguments, drawing an edge for each pair that passes the above mentioned criterion.",
                "The resulting maximal matching determines the true-positive set, while remaining non-aligned arguments become false-positives or false-negatives."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: Evaluation in QA-SRL involves aligning predicted and ground truth argument spans and evaluating role label equivalence.\n sent1: Since detecting question paraphrases is still an open challenge, we propose both unlabeled and labeled evaluation metrics.\n sent2: Unlabeled Argument Detection (UA) Inspired by the method presented in ( #TARGET_REF) , arguments are matched using a span matching criterion of intersection over union ≥ 0.5 .\n sent3: To credit each argument only once, we employ maximal bipartite matching 4 between the two sets of arguments, drawing an edge for each pair that passes the above mentioned criterion.\n sent4: The resulting maximal matching determines the true-positive set, while remaining non-aligned arguments become false-positives or false-negatives.\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We extend our metric for evaluating manual or automatic redundant annotations, like the Dense dataset or #TARGET_REF in ( #TARGET_REF) , which predicts argument spans independently of each other.",
                "To that end, we ignore predicted arguments that match ground-truth but are not selected by the bipartite matching due to redundancy.",
                "After con-necting unmatched predicted arguments that overlap, we count one false positive for every connected component to avoid penalizing precision too harshly when predictions are redundant.",
                "6"
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: We extend our metric for evaluating manual or automatic redundant annotations, like the Dense dataset or #TARGET_REF in ( #TARGET_REF) , which predicts argument spans independently of each other.\n sent1: To that end, we ignore predicted arguments that match ground-truth but are not selected by the bipartite matching due to redundancy.\n sent2: After con-necting unmatched predicted arguments that overlap, we count one false positive for every connected component to avoid penalizing precision too harshly when predictions are redundant.\n sent3: 6\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Word alignment is a fundamental step in machine translation.",
                "Current statistical machine translation systems suffer from a major drawback: they only extract rules from 1-best alignments, which adversely affects the rule sets quality due to alignment mistakes.",
                "To alleviate this problem, we extract hierarchical rules from weighted alignment matrix #TARGET_REF .",
                "Since the sub-phrase pairs would change the inside and outside areas in the weighted alignment matrix of the hierarchical rules, we propose a new algorithm to calculate the relative frequencies and lexical weights of hierarchical rules.",
                "To achieve a balance between rule table size and performance, we construct a scoring measure that incorporates both frequency and lexical weight to select the best target phrase for each source phrase."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Word alignment is a fundamental step in machine translation.\n sent1: Current statistical machine translation systems suffer from a major drawback: they only extract rules from 1-best alignments, which adversely affects the rule sets quality due to alignment mistakes.\n sent2: To alleviate this problem, we extract hierarchical rules from weighted alignment matrix #TARGET_REF .\n sent3: Since the sub-phrase pairs would change the inside and outside areas in the weighted alignment matrix of the hierarchical rules, we propose a new algorithm to calculate the relative frequencies and lexical weights of hierarchical rules.\n sent4: To achieve a balance between rule table size and performance, we construct a scoring measure that incorporates both frequency and lexical weight to select the best target phrase for each source phrase.\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Another challenge is how to achieve a balance between performance and rule table size.",
                "Note that given a source phrase, there would be plenty of \"potential\" candidate target phrases in weighted matrices #TARGET_REF ).",
                "If we retain all of them, these phrase pairs would produce even more hierarchical rules.",
                "For computational tractability, we need to design a measure to score the phrase pairs and wipe out the low-quality ones.",
                "We propose a new algorithm to calculate the relative frequencies of rules, and construct a measure that incorporates both frequency and lexical weight to score target phrases."
            ],
            "label": [
                "MOTIVATION",
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Another challenge is how to achieve a balance between performance and rule table size.\n sent1: Note that given a source phrase, there would be plenty of \"potential\" candidate target phrases in weighted matrices #TARGET_REF ).\n sent2: If we retain all of them, these phrase pairs would produce even more hierarchical rules.\n sent3: For computational tractability, we need to design a measure to score the phrase pairs and wipe out the low-quality ones.\n sent4: We propose a new algorithm to calculate the relative frequencies of rules, and construct a measure that incorporates both frequency and lexical weight to score target phrases.\n",
        "output": "{\"label\": [\"MOTIVATION\", \"DIFFERENCES\"], \"context\": [\"sent0\", \"sent1\", \"sent2\", \"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "For example, a wrong rule (X 1 de jingji, of X 1 's economy) would be extracted from the alignment in Figure 1 (a).",
                "Since #TARGET_REF show that weighted alignment matrix provides an elegant solution to these two drawbacks, we apply it to the hierarchical phrase-based model (#REF) and the tree-to-string model #REF) .",
                "While such an idea seems intuitive, it is non-trivial to extract hierarchical rules from weighted alignment matrices.",
                "Our work faces two major challenges.",
                "The first is how to calculate the relative frequencies and lex- ical weights of the rules with non-terminals (NTs)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: For example, a wrong rule (X 1 de jingji, of X 1 's economy) would be extracted from the alignment in Figure 1 (a).\n sent1: Since #TARGET_REF show that weighted alignment matrix provides an elegant solution to these two drawbacks, we apply it to the hierarchical phrase-based model (#REF) and the tree-to-string model #REF) .\n sent2: While such an idea seems intuitive, it is non-trivial to extract hierarchical rules from weighted alignment matrices.\n sent3: Our work faces two major challenges.\n sent4: The first is how to calculate the relative frequencies and lex- ical weights of the rules with non-terminals (NTs).\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Since the tree structure of source side has no effect on the calculations of relative frequencies and lexical weights, we can represent both tree-to-string and hierarchical rules as below:",
                "where X is a nonterminal, γ and α are source and target strings (consist of terminals and NTs), and ∼ represents word alignments between NTs in γ and α.",
                "The bulk of syntax grammars consists of two parts: phrase pairs and variable rules.",
                "The difference between them is containing NTs or not.",
                "Since we can calculate relative frequencies and lexical weights of phrase pairs as in #TARGET_REF , we only focus on the calculation of variable rules."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Since the tree structure of source side has no effect on the calculations of relative frequencies and lexical weights, we can represent both tree-to-string and hierarchical rules as below:\n sent1: where X is a nonterminal, γ and α are source and target strings (consist of terminals and NTs), and ∼ represents word alignments between NTs in γ and α.\n sent2: The bulk of syntax grammars consists of two parts: phrase pairs and variable rules.\n sent3: The difference between them is containing NTs or not.\n sent4: Since we can calculate relative frequencies and lexical weights of phrase pairs as in #TARGET_REF , we only focus on the calculation of variable rules.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "We follow #TARGET_REF to calculate relative frequencies using the product of inside and outside probabilities.",
                "We now extend the definitions of inside and outside probabilities to hierarchical rules that contain NTs.",
                "Table 2 : Some hierarchical rules generated from the phrase pair (zhongguo de jingji, China's economy) in Figure 3 (suppose the structure of zhongguo de jingji is a complete sub-tree).",
                "Here α is inside probability, β is outside probability, and count is fractional count.",
                "Given a variable rule (f ′ , e ′ ), which is generated from the phrase pair (f"
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We follow #TARGET_REF to calculate relative frequencies using the product of inside and outside probabilities.\n sent1: We now extend the definitions of inside and outside probabilities to hierarchical rules that contain NTs.\n sent2: Table 2 : Some hierarchical rules generated from the phrase pair (zhongguo de jingji, China's economy) in Figure 3 (suppose the structure of zhongguo de jingji is a complete sub-tree).\n sent3: Here α is inside probability, β is outside probability, and count is fractional count.\n sent4: Given a variable rule (f ′ , e ′ ), which is generated from the phrase pair (f\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "For example, the inside probability of (X 1 de jingji, X 1 's economy) in Figure 5 is 1.0, and its outside probability is 0.4.",
                "We also use Equation 5 to calculate the fractional counts of hierarchical rules.",
                "We follow #TARGET_REF to prune rule table using a threshold of frequency.",
                "Table 2 lists some hierarchical rules generated from the phrase pair (zhongguo de jingji, China's economy) in Figure 3 .",
                "If the threshold is 0.2, we retain all the rules in Table 2 ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: For example, the inside probability of (X 1 de jingji, X 1 's economy) in Figure 5 is 1.0, and its outside probability is 0.4.\n sent1: We also use Equation 5 to calculate the fractional counts of hierarchical rules.\n sent2: We follow #TARGET_REF to prune rule table using a threshold of frequency.\n sent3: Table 2 lists some hierarchical rules generated from the phrase pair (zhongguo de jingji, China's economy) in Figure 3 .\n sent4: If the threshold is 0.2, we retain all the rules in Table 2 .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We produce 20-best lists in two translation directions, then used \"grow-diag-finaland\" (#REF) to all 20 × 20 bidirectional alignment pairs.",
                "We follow #TARGET_REF to use p s2t × p t2s as the probabilities of an alignment pair.",
                "Analogously, we abandon duplicate alignments that are produced from different alignment pairs.",
                "After these steps, there are 110 candidate alignments on average for each sentence pair.",
                "We obtained n-best lists by selecting the top n alignments from 110-best lists."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We produce 20-best lists in two translation directions, then used \"grow-diag-finaland\" (#REF) to all 20 × 20 bidirectional alignment pairs.\n sent1: We follow #TARGET_REF to use p s2t × p t2s as the probabilities of an alignment pair.\n sent2: Analogously, we abandon duplicate alignments that are produced from different alignment pairs.\n sent3: After these steps, there are 110 candidate alignments on average for each sentence pair.\n sent4: We obtained n-best lists by selecting the top n alignments from 110-best lists.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "However, these models take more time to instantiate in comparison to weighting of a co-occurrence matrix, bring more parameters to explore and produce vector spaces with uninterpretable dimensions (vector space dimension interpretation is used by some lexical mod-els, for example, #REF , and the passage from formal semantics to tensor models relies on it (#REF) ).",
                "In this work we focus on vector spaces that directly weight a co-occurrence matrix and report results for SVD, GloVe and SGNS from the study of #TARGET_REF for comparison.",
                "The mismatch of recent experiments with nondense models in vector dimensionality between lexical and compositional tasks gives rise to a number of questions:",
                "• To what extent does model performance depend on vector dimensionality?",
                "• Do parameters influence 200K and 1K dimensional models similarly?"
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: However, these models take more time to instantiate in comparison to weighting of a co-occurrence matrix, bring more parameters to explore and produce vector spaces with uninterpretable dimensions (vector space dimension interpretation is used by some lexical mod-els, for example, #REF , and the passage from formal semantics to tensor models relies on it (#REF) ).\n sent1: In this work we focus on vector spaces that directly weight a co-occurrence matrix and report results for SVD, GloVe and SGNS from the study of #TARGET_REF for comparison.\n sent2: The mismatch of recent experiments with nondense models in vector dimensionality between lexical and compositional tasks gives rise to a number of questions:\n sent3: • To what extent does model performance depend on vector dimensionality?\n sent4: • Do parameters influence 200K and 1K dimensional models similarly?\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Many approaches use only positive PMI values, as negative PMI values may not positively contribute to model performance and sparser matrices are more computationally tractable (#REF) .",
                "This can be generalised to an additional cutoff parameter k (neg) following #TARGET_REF , giving our third PMI variant (abbreviated as SPMI): 2",
                "When k = 1 SPMI is equivalent to positive PMI.",
                "k > 1 increases the underlying matrix sparsity by keeping only highly associated co-occurrence pairs.",
                "k < 1 decreases the underlying matrix sparsity by including some unassociated cooccurrence pairs, which are usually excluded due to unreliability of probability estimates (#REF) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Many approaches use only positive PMI values, as negative PMI values may not positively contribute to model performance and sparser matrices are more computationally tractable (#REF) .\n sent1: This can be generalised to an additional cutoff parameter k (neg) following #TARGET_REF , giving our third PMI variant (abbreviated as SPMI): 2\n sent2: When k = 1 SPMI is equivalent to positive PMI.\n sent3: k > 1 increases the underlying matrix sparsity by keeping only highly associated co-occurrence pairs.\n sent4: k < 1 decreases the underlying matrix sparsity by including some unassociated cooccurrence pairs, which are usually excluded due to unreliability of probability estimates (#REF) .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We evaluate these heuristics by comparing the performance they give on #REF9 against that obtained using the best possible parameter selections (determined via an exhaustive search at each dimensionality setting).",
                "We also compare them to the best scores reported by #TARGET_REF for their model (PMI and SVD), word2vec-SGNS (#REF) and GloVe (#REF )-see Figure 3a , where only the betterperforming SPMI and SCPMI are shown.",
                "For lognPMI and lognCPMI, our heuristics pick the best possible models.",
                "For lognSPMI, where performance variance is low, the heuristics do well, giving a performance of no more than 0.01 points below the best configuration.",
                "For 1SPMI and nSPMI the difference is higher."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We evaluate these heuristics by comparing the performance they give on #REF9 against that obtained using the best possible parameter selections (determined via an exhaustive search at each dimensionality setting).\n sent1: We also compare them to the best scores reported by #TARGET_REF for their model (PMI and SVD), word2vec-SGNS (#REF) and GloVe (#REF )-see Figure 3a , where only the betterperforming SPMI and SCPMI are shown.\n sent2: For lognPMI and lognCPMI, our heuristics pick the best possible models.\n sent3: For lognSPMI, where performance variance is low, the heuristics do well, giving a performance of no more than 0.01 points below the best configuration.\n sent4: For 1SPMI and nSPMI the difference is higher.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "For lognPMI and lognCPMI, our heuristics pick the best possible models.",
                "For lognSPMI, where performance variance is low, the heuristics do well, giving a performance of no more than 0.01 points below the best configuration.",
                "For 1SPMI and nSPMI the difference is higher.",
                "With lognSCPMI and 1SCPMI, the heuristics follow #TARGET_REF .",
                "We also give our best score, SVD, SGNS and GloVe numbers from that study for comparison."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: For lognPMI and lognCPMI, our heuristics pick the best possible models.\n sent1: For lognSPMI, where performance variance is low, the heuristics do well, giving a performance of no more than 0.01 points below the best configuration.\n sent2: For 1SPMI and nSPMI the difference is higher.\n sent3: With lognSCPMI and 1SCPMI, the heuristics follow #TARGET_REF .\n sent4: We also give our best score, SVD, SGNS and GloVe numbers from that study for comparison.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "We also give our best score, SVD, SGNS and GloVe numbers from that study for comparison.",
                "On the right, our heuristic in comparison to the best and average results together with the models selected using the recommendations presented in #TARGET_REF .",
                "the best selection, but with a wider gap than the SPMI models.",
                "In general n-weighted models do not perform as well as others.",
                "Overall, log n weighting should be used with PMI, CPMI and SCPMI."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We also give our best score, SVD, SGNS and GloVe numbers from that study for comparison.\n sent1: On the right, our heuristic in comparison to the best and average results together with the models selected using the recommendations presented in #TARGET_REF .\n sent2: the best selection, but with a wider gap than the SPMI models.\n sent3: In general n-weighted models do not perform as well as others.\n sent4: Overall, log n weighting should be used with PMI, CPMI and SCPMI.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "This paper presents a systematic study of cooccurrence quantification focusing on the selection of parameters presented in #TARGET_REF .",
                "We replicate their recommendation for high-dimensional vector spaces, and show that with appropriate parameter selection it is possible to achieve comparable performance with spaces of dimensionality of 1K to 50K, and propose a set of model selection heuristics that maximizes performance.",
                "We foresee the results of the paper are generalisable to other experiments, since model selection was performed on a similarity dataset, and was additionally tested on a relatedness dataset.",
                "In general, model performance depends on vector dimensionality (the best setup with 50K dimensions is better than the best setup with 1K dimensions by 0.03 on #REF9).",
                "Spaces with a few thousand dimensions benefit from being dense and unsmoothed (k < 1, global context probability); while high-dimensional spaces are better sparse and smooth (k > 1, α = 0.75)."
            ],
            "label": [
                "USE",
                "EXTENDS"
            ]
        },
        "input": "sent0: This paper presents a systematic study of cooccurrence quantification focusing on the selection of parameters presented in #TARGET_REF .\n sent1: We replicate their recommendation for high-dimensional vector spaces, and show that with appropriate parameter selection it is possible to achieve comparable performance with spaces of dimensionality of 1K to 50K, and propose a set of model selection heuristics that maximizes performance.\n sent2: We foresee the results of the paper are generalisable to other experiments, since model selection was performed on a similarity dataset, and was additionally tested on a relatedness dataset.\n sent3: In general, model performance depends on vector dimensionality (the best setup with 50K dimensions is better than the best setup with 1K dimensions by 0.03 on #REF9).\n sent4: Spaces with a few thousand dimensions benefit from being dense and unsmoothed (k < 1, global context probability); while high-dimensional spaces are better sparse and smooth (k > 1, α = 0.75).\n",
        "output": "{\"label\": [\"USE\", \"EXTENDS\"], \"context\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "On the similarity dataset our model is 0.008 points behind a PPMI model, however on the relatedness dataset 0.020 points above.",
                "Note the difference in dimensionality, source corpora and window size.",
                "SVD, SGNS and GloVe numbers are given for comparison.",
                "* Results reported by #TARGET_REF .",
                "of the high variance of the corresponding scores."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: On the similarity dataset our model is 0.008 points behind a PPMI model, however on the relatedness dataset 0.020 points above.\n sent1: Note the difference in dimensionality, source corpora and window size.\n sent2: SVD, SGNS and GloVe numbers are given for comparison.\n sent3: * Results reported by #TARGET_REF .\n sent4: of the high variance of the corresponding scores.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "In this work we focus on vector spaces that directly weight a co-occurrence matrix and report results for SVD, GloVe and SGNS from the study of #REF for comparison.",
                "The mismatch of recent experiments with nondense models in vector dimensionality between lexical and compositional tasks gives rise to a number of questions:",
                "• To what extent does model performance depend on vector dimensionality?",
                "• Do parameters influence 200K and 1K dimensional models similarly?",
                "Can the findings of #TARGET_REF be directly applied to models with a few thousand dimensions?"
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: In this work we focus on vector spaces that directly weight a co-occurrence matrix and report results for SVD, GloVe and SGNS from the study of #REF for comparison.\n sent1: The mismatch of recent experiments with nondense models in vector dimensionality between lexical and compositional tasks gives rise to a number of questions:\n sent2: • To what extent does model performance depend on vector dimensionality?\n sent3: • Do parameters influence 200K and 1K dimensional models similarly?\n sent4: Can the findings of #TARGET_REF be directly applied to models with a few thousand dimensions?\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent1\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Another issue with PMI is its bias towards rare events #TARGET_REF ; one way of solving this issue is to weight the value by the co-occurrence frequency (#REF) :",
                "where n(x, y) is the number of times x was seen together with y. For clarity, we refer to n-weighted PMIs as nPMI, nSPMI, etc.",
                "When this weighting component is set to 1, it has no effect; we can explicitly label it as 1PMI, 1SPMI, etc.",
                "In addition to the extreme 1 and n weightings, we also experiment with a log n weighting.",
                "#REF show that performance is affected by smoothing the context distribution P (x):"
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Another issue with PMI is its bias towards rare events #TARGET_REF ; one way of solving this issue is to weight the value by the co-occurrence frequency (#REF) :\n sent1: where n(x, y) is the number of times x was seen together with y. For clarity, we refer to n-weighted PMIs as nPMI, nSPMI, etc.\n sent2: When this weighting component is set to 1, it has no effect; we can explicitly label it as 1PMI, 1SPMI, etc.\n sent3: In addition to the extreme 1 and n weightings, we also experiment with a log n weighting.\n sent4: #REF show that performance is affected by smoothing the context distribution P (x):\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Overall, log n weighting should be used with PMI, CPMI and SCPMI.",
                "High-dimensional SPMI models show the same behaviour, but if D < 10K, no weighting should be applied.",
                "SPMI and SCPMI should be preferred over CPMI and PMI.",
                "As Figure 3b shows, our heuristics give performance close to the optimum for any dimensionality, with a large improvement over both an average parameter setting and the parameters suggested by #TARGET_REF in a high-dimensional setting.",
                "4 Finally, to see whether the heuristics transfer robustly, we repeat this comparison on the MEN dataset (see Figures 3c, 3d) ."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Overall, log n weighting should be used with PMI, CPMI and SCPMI.\n sent1: High-dimensional SPMI models show the same behaviour, but if D < 10K, no weighting should be applied.\n sent2: SPMI and SCPMI should be preferred over CPMI and PMI.\n sent3: As Figure 3b shows, our heuristics give performance close to the optimum for any dimensionality, with a large improvement over both an average parameter setting and the parameters suggested by #TARGET_REF in a high-dimensional setting.\n sent4: 4 Finally, to see whether the heuristics transfer robustly, we repeat this comparison on the MEN dataset (see Figures 3c, 3d) .\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "However, studies of Amazon's product reviews also show that the per- Meta-data MET the overall ratings of papers assigned by reviewers, and the absolute difference between the rating and the average score given by all reviewers.",
                "Table 1 : Generic features motivated by related work of product reviews (#REF) .",
                "ceived helpfulness of a review depends not only on its review content, but also on social effects such as product qualities, and individual bias in the presence of mixed opinion distribution (#REF ).",
                "Nonetheless, several properties distinguish our corpus of peer reviews from other types of reviews: 1) The helpfulness of our peer reviews is directly rated using a discrete scale from one to five instead of being defined as a function of binary votes (e.g. the percentage of \"helpful\" votes #TARGET_REF ); 2) Peer reviews frequently refer to the related students' papers, thus review analysis needs to take into account paper topics; 3) Within the context of education, peer-review helpfulness often has a writing specific semantics, e.g. improving revision likelihood; 4) In general, peer-review corpora collected from classrooms are of a much smaller size compared to online product reviews.",
                "To tailor existing techniques to peer reviews, we will thus propose new specialized features to address these issues."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: However, studies of Amazon's product reviews also show that the per- Meta-data MET the overall ratings of papers assigned by reviewers, and the absolute difference between the rating and the average score given by all reviewers.\n sent1: Table 1 : Generic features motivated by related work of product reviews (#REF) .\n sent2: ceived helpfulness of a review depends not only on its review content, but also on social effects such as product qualities, and individual bias in the presence of mixed opinion distribution (#REF ).\n sent3: Nonetheless, several properties distinguish our corpus of peer reviews from other types of reviews: 1) The helpfulness of our peer reviews is directly rated using a discrete scale from one to five instead of being defined as a function of binary votes (e.g. the percentage of \"helpful\" votes #TARGET_REF ); 2) Peer reviews frequently refer to the related students' papers, thus review analysis needs to take into account paper topics; 3) Within the context of education, peer-review helpfulness often has a writing specific semantics, e.g. improving revision likelihood; 4) In general, peer-review corpora collected from classrooms are of a much smaller size compared to online product reviews.\n sent4: To tailor existing techniques to peer reviews, we will thus propose new specialized features to address these issues.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Evaluation of the generic features is presented in Table 2 , showing that all classes except syntactic (SYN) and meta-data (MET) features are sig-nificantly correlated with both helpfulness rating (r) and helpfulness ranking (r s ).",
                "Structural features (bolded) achieve the highest Pearson (0.60) and Spearman correlation coefficients (0.59) (although within the significant correlations, the difference among coefficients are insignificant).",
                "Note that in isolation, MET (paper ratings) are not significantly correlated with peer-review helpfulness, which is different from prior findings of product reviews #TARGET_REF where product scores are significantly correlated with product-review helpfulness.",
                "However, when combined with other features, MET does appear to add value (last row).",
                "When comparing the performance between predicting helpfulness ratings versus ranking, we observe r ≈ r s consistently for our peer reviews, while #REF reported r < r s for product reviews."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Evaluation of the generic features is presented in Table 2 , showing that all classes except syntactic (SYN) and meta-data (MET) features are sig-nificantly correlated with both helpfulness rating (r) and helpfulness ranking (r s ).\n sent1: Structural features (bolded) achieve the highest Pearson (0.60) and Spearman correlation coefficients (0.59) (although within the significant correlations, the difference among coefficients are insignificant).\n sent2: Note that in isolation, MET (paper ratings) are not significantly correlated with peer-review helpfulness, which is different from prior findings of product reviews #TARGET_REF where product scores are significantly correlated with product-review helpfulness.\n sent3: However, when combined with other features, MET does appear to add value (last row).\n sent4: When comparing the performance between predicting helpfulness ratings versus ranking, we observe r ≈ r s consistently for our peer reviews, while #REF reported r < r s for product reviews.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Note that in isolation, MET (paper ratings) are not significantly correlated with peer-review helpfulness, which is different from prior findings of product reviews (#REF) where product scores are significantly correlated with product-review helpfulness.",
                "However, when combined with other features, MET does appear to add value (last row).",
                "When comparing the performance between predicting helpfulness ratings versus ranking, we observe r ≈ r s consistently for our peer reviews, while #TARGET_REF reported r < r s for product reviews.",
                "4 Finally, we observed a similar feature redundancy effect as #REF did, in that simply combining all features does not improve the model's performance.",
                "Interestingly, our best feature combination (last row) is the same as theirs."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Note that in isolation, MET (paper ratings) are not significantly correlated with peer-review helpfulness, which is different from prior findings of product reviews (#REF) where product scores are significantly correlated with product-review helpfulness.\n sent1: However, when combined with other features, MET does appear to add value (last row).\n sent2: When comparing the performance between predicting helpfulness ratings versus ranking, we observe r ≈ r s consistently for our peer reviews, while #TARGET_REF reported r < r s for product reviews.\n sent3: 4 Finally, we observed a similar feature redundancy effect as #REF did, in that simply combining all features does not improve the model's performance.\n sent4: Interestingly, our best feature combination (last row) is the same as theirs.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Despite the difference between peer reviews and other types of reviews as discussed in Section 2, our work demonstrates that many generic linguistic features are also effective in predicting peer-review helpfulness.",
                "The model's performance can be alter- natively achieved and further improved by adding auxiliary features tailored to peer reviews.",
                "These specialized features not only introduce domain expertise, but also capture linguistic information at an abstracted level, which can help avoid the risk of over-fitting.",
                "Given only 267 peer reviews in our case compared to more than ten thousand product reviews #TARGET_REF , this is an important consideration.",
                "Though our absolute quantitative results are not directly comparable to the results of #REF , we indirectly compared them by analyzing the utility of features in isolation and combined."
            ],
            "label": [
                "DIFFERENCES",
                "MOTIVATION"
            ]
        },
        "input": "sent0: Despite the difference between peer reviews and other types of reviews as discussed in Section 2, our work demonstrates that many generic linguistic features are also effective in predicting peer-review helpfulness.\n sent1: The model's performance can be alter- natively achieved and further improved by adding auxiliary features tailored to peer reviews.\n sent2: These specialized features not only introduce domain expertise, but also capture linguistic information at an abstracted level, which can help avoid the risk of over-fitting.\n sent3: Given only 267 peer reviews in our case compared to more than ten thousand product reviews #TARGET_REF , this is an important consideration.\n sent4: Though our absolute quantitative results are not directly comparable to the results of #REF , we indirectly compared them by analyzing the utility of features in isolation and combined.\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"MOTIVATION\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Though our absolute quantitative results are not directly comparable to the results of #REF , we indirectly compared them by analyzing the utility of features in isolation and combined.",
                "While STR+UGR+MET is found as the best combination of generic features for both types of reviews, the best individual feature type is different (review unigrams work best for product reviews; structural features work best for peer reviews).",
                "More importantly, meta-data, which are found to significantly affect the perceived helpfulness of product reviews (#REF; #REF) , have no predictive power for peer reviews.",
                "Perhaps because the paper grades and other helpfulness ratings are not visible to the reviewers, we have less of a social dimension for predicting the helpfulness of peer reviews.",
                "We also found that SVM regression does not favor ranking over predicting helpfulness as in #TARGET_REF ."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Though our absolute quantitative results are not directly comparable to the results of #REF , we indirectly compared them by analyzing the utility of features in isolation and combined.\n sent1: While STR+UGR+MET is found as the best combination of generic features for both types of reviews, the best individual feature type is different (review unigrams work best for product reviews; structural features work best for peer reviews).\n sent2: More importantly, meta-data, which are found to significantly affect the perceived helpfulness of product reviews (#REF; #REF) , have no predictive power for peer reviews.\n sent3: Perhaps because the paper grades and other helpfulness ratings are not visible to the reviewers, we have less of a social dimension for predicting the helpfulness of peer reviews.\n sent4: We also found that SVM regression does not favor ranking over predicting helpfulness as in #TARGET_REF .\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Though our absolute quantitative results are not directly comparable to the results of #REF , we indirectly compared them by analyzing the utility of features in isolation and combined.",
                "While STR+UGR+MET is found as the best combination of generic features for both types of reviews, the best individual feature type is different (review unigrams work best for product reviews; structural features work best for peer reviews).",
                "More importantly, meta-data, which are found to significantly affect the perceived helpfulness of product reviews #TARGET_REF; #REF) , have no predictive power for peer reviews.",
                "Perhaps because the paper grades and other helpfulness ratings are not visible to the reviewers, we have less of a social dimension for predicting the helpfulness of peer reviews.",
                "We also found that SVM regression does not favor ranking over predicting helpfulness as in (#REF) ."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Though our absolute quantitative results are not directly comparable to the results of #REF , we indirectly compared them by analyzing the utility of features in isolation and combined.\n sent1: While STR+UGR+MET is found as the best combination of generic features for both types of reviews, the best individual feature type is different (review unigrams work best for product reviews; structural features work best for peer reviews).\n sent2: More importantly, meta-data, which are found to significantly affect the perceived helpfulness of product reviews #TARGET_REF; #REF) , have no predictive power for peer reviews.\n sent3: Perhaps because the paper grades and other helpfulness ratings are not visible to the reviewers, we have less of a social dimension for predicting the helpfulness of peer reviews.\n sent4: We also found that SVM regression does not favor ranking over predicting helpfulness as in (#REF) .\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "An unhelpful peer review of average-rating 1:",
                "Your paper and its main points are easy to find and to follow.",
                "As shown in Table 1 , we first mine generic linguistic features from reviews and papers based on the results of syntactic analysis of the texts, aiming to replicate the feature sets used by #TARGET_REF .",
                "While structural, lexical and syntactic features are created in the same way as suggested in their paper, we adapt the semantic and meta-data features to peer reviews by converting the mentions of product properties to mentions of the history topics and by using paper ratings assigned by peers instead of product scores.",
                "1"
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: An unhelpful peer review of average-rating 1:\n sent1: Your paper and its main points are easy to find and to follow.\n sent2: As shown in Table 1 , we first mine generic linguistic features from reviews and papers based on the results of syntactic analysis of the texts, aiming to replicate the feature sets used by #TARGET_REF .\n sent3: While structural, lexical and syntactic features are created in the same way as suggested in their paper, we adapt the semantic and meta-data features to peer reviews by converting the mentions of product properties to mentions of the history topics and by using paper ratings assigned by peers instead of product scores.\n sent4: 1\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Following #TARGET_REF , we train our helpfulness model using SVM regression with a radial basis function kernel provided by SVM light (#REF) .",
                "We first evaluate each feature type in isolation to investigate its predictive power of peerreview helpfulness; we then examine them together in various combinations to find the most useful feature set for modeling peer-review helpfulness.",
                "Performance is evaluated in 10-fold cross validation of our 267 peer reviews by predicting the absolute helpfulness scores (with Pearson correlation coefficient r) as well as by predicting helpfulness ranking (with Spearman rank correlation coefficient r s ).",
                "Although predicted helpfulness ranking could be directly used to compare the helpfulness of a given set of reviews, predicting helpfulness rating is desirable in practice to compare helpfulness between existing reviews and new written ones without reranking all previously ranked reviews.",
                "Results are presented regarding the generic features and the specialized features respectively, with 95% confidence bounds."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Following #TARGET_REF , we train our helpfulness model using SVM regression with a radial basis function kernel provided by SVM light (#REF) .\n sent1: We first evaluate each feature type in isolation to investigate its predictive power of peerreview helpfulness; we then examine them together in various combinations to find the most useful feature set for modeling peer-review helpfulness.\n sent2: Performance is evaluated in 10-fold cross validation of our 267 peer reviews by predicting the absolute helpfulness scores (with Pearson correlation coefficient r) as well as by predicting helpfulness ranking (with Spearman rank correlation coefficient r s ).\n sent3: Although predicted helpfulness ranking could be directly used to compare the helpfulness of a given set of reviews, predicting helpfulness rating is desirable in practice to compare helpfulness between existing reviews and new written ones without reranking all previously ranked reviews.\n sent4: Results are presented regarding the generic features and the specialized features respectively, with 95% confidence bounds.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "These specialized features not only introduce domain expertise, but also capture linguistic information at an abstracted level, which can help avoid the risk of over-fitting.",
                "Given only 267 peer reviews in our case compared to more than ten thousand product reviews (#REF) , this is an important consideration.",
                "Though our absolute quantitative results are not directly comparable to the results of #TARGET_REF , we indirectly compared them by analyzing the utility of features in isolation and combined.",
                "While STR+UGR+MET is found as the best combination of generic features for both types of reviews, the best individual feature type is different (review unigrams work best for product reviews; structural features work best for peer reviews).",
                "More importantly, meta-data, which are found to significantly affect the perceived helpfulness of product reviews (#REF; #REF) , have no predictive power for peer reviews."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: These specialized features not only introduce domain expertise, but also capture linguistic information at an abstracted level, which can help avoid the risk of over-fitting.\n sent1: Given only 267 peer reviews in our case compared to more than ten thousand product reviews (#REF) , this is an important consideration.\n sent2: Though our absolute quantitative results are not directly comparable to the results of #TARGET_REF , we indirectly compared them by analyzing the utility of features in isolation and combined.\n sent3: While STR+UGR+MET is found as the best combination of generic features for both types of reviews, the best individual feature type is different (review unigrams work best for product reviews; structural features work best for peer reviews).\n sent4: More importantly, meta-data, which are found to significantly affect the perceived helpfulness of product reviews (#REF; #REF) , have no predictive power for peer reviews.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Structural features (bolded) achieve the highest Pearson (0.60) and Spearman correlation coefficients (0.59) (although within the significant correlations, the difference among coefficients are insignificant).",
                "Note that in isolation, MET (paper ratings) are not significantly correlated with peer-review helpfulness, which is different from prior findings of product reviews (#REF) where product scores are significantly correlated with product-review helpfulness.",
                "However, when combined with other features, MET does appear to add value (last row).",
                "When comparing the performance between predicting helpfulness ratings versus ranking, we observe r ≈ r s consistently for our peer reviews, while #REF reported r < r s for product reviews.",
                "4 Finally, we observed a similar feature redundancy effect as #TARGET_REF did, in that simply combining all features does not improve the model's performance."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: Structural features (bolded) achieve the highest Pearson (0.60) and Spearman correlation coefficients (0.59) (although within the significant correlations, the difference among coefficients are insignificant).\n sent1: Note that in isolation, MET (paper ratings) are not significantly correlated with peer-review helpfulness, which is different from prior findings of product reviews (#REF) where product scores are significantly correlated with product-review helpfulness.\n sent2: However, when combined with other features, MET does appear to add value (last row).\n sent3: When comparing the performance between predicting helpfulness ratings versus ranking, we observe r ≈ r s consistently for our peer reviews, while #REF reported r < r s for product reviews.\n sent4: 4 Finally, we observed a similar feature redundancy effect as #TARGET_REF did, in that simply combining all features does not improve the model's performance.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "According to (#REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #TARGET_REF , the approaches of Chinese input methods (i.e. Chinese input systems) can be classified into two types: (1) keyboard based approach: including phonetic and pinyin based (#REF; #REF; #REF; #REF; #REF; #REF) , arbitrary codes based (#REF) and structure scheme based (#REF) ; and (2) non-keyboard based approach: including optical character recognition (OCR) (#REF) , online handwriting and speech recognition (#REF; #REF) .",
                "Currently, the most popular Chinese input system is phonetic and pinyin based approach, because Chinese people are taught to write phonetic and pinyin syllables of each Chinese character in primary school.",
                "In Chinese, each Chinese word can be a mono-syllabic word, such as \"鼠(mouse)\", a bisyllabic word, such as \"袋鼠(kangaroo)\", or a multi-syllabic word, such as \"米老鼠(Mickey mouse).\" The corresponding phonetic and pinyin syllables of each Chinese word is called syllable-words, such as \"dai4 shu3\" is the pinyin syllable-word of \"袋鼠(kangaroo).\" According to our computation, the {minimum, maximum, average} words per each distinct mono-syllableword and poly-syllable-word (including bisyllable-word and multi-syllable-word) in the CKIP dictionary (Chinese Knowledge Information Processing #REF) are {1, 28, 2.8} and {1, 7, 1.1}, respectively.",
                "The CKIP dictionary is one of most commonly-used Chinese dictionaries in the research field of Chinese natural language processing (NLP).",
                "Since the size of problem space for syllable-to-word (STW) conversion is much less than that of syllable-tocharacter (STC) conversion, the most pinyinbased Chinese input systems (#REF; #REF; #REF; #REF; Microsoft Research Center in Beijing; #REF) are addressed on STW conversion."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: According to (#REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #TARGET_REF , the approaches of Chinese input methods (i.e. Chinese input systems) can be classified into two types: (1) keyboard based approach: including phonetic and pinyin based (#REF; #REF; #REF; #REF; #REF; #REF) , arbitrary codes based (#REF) and structure scheme based (#REF) ; and (2) non-keyboard based approach: including optical character recognition (OCR) (#REF) , online handwriting and speech recognition (#REF; #REF) .\n sent1: Currently, the most popular Chinese input system is phonetic and pinyin based approach, because Chinese people are taught to write phonetic and pinyin syllables of each Chinese character in primary school.\n sent2: In Chinese, each Chinese word can be a mono-syllabic word, such as \"鼠(mouse)\", a bisyllabic word, such as \"袋鼠(kangaroo)\", or a multi-syllabic word, such as \"米老鼠(Mickey mouse).\" The corresponding phonetic and pinyin syllables of each Chinese word is called syllable-words, such as \"dai4 shu3\" is the pinyin syllable-word of \"袋鼠(kangaroo).\" According to our computation, the {minimum, maximum, average} words per each distinct mono-syllableword and poly-syllable-word (including bisyllable-word and multi-syllable-word) in the CKIP dictionary (Chinese Knowledge Information Processing #REF) are {1, 28, 2.8} and {1, 7, 1.1}, respectively.\n sent3: The CKIP dictionary is one of most commonly-used Chinese dictionaries in the research field of Chinese natural language processing (NLP).\n sent4: Since the size of problem space for syllable-to-word (STW) conversion is much less than that of syllable-tocharacter (STC) conversion, the most pinyinbased Chinese input systems (#REF; #REF; #REF; #REF; Microsoft Research Center in Beijing; #REF) are addressed on STW conversion.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "The CKIP dictionary is one of most commonly-used Chinese dictionaries in the research field of Chinese natural language processing (NLP).",
                "Since the size of problem space for syllable-to-word (STW) conversion is much less than that of syllable-tocharacter (STC) conversion, the most pinyinbased Chinese input systems (#REF; #REF; #REF; #REF; Microsoft Research Center in Beijing; #TARGET_REF are addressed on STW conversion.",
                "On the other hand, STW conversion is the main task of Chinese Language Processing in typical Chinese speech recognition systems (#REF; #REF; #REF; #REF) .",
                "As per (#REF; #REF; #REF; #REF; #REF; #REF) , homophone selection and syllableword segmentation are two critical problems in developing a Chinese input system.",
                "Incorrect homophone selection and syllable-word seg-mentation will directly influence the STW conversion accuracy."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The CKIP dictionary is one of most commonly-used Chinese dictionaries in the research field of Chinese natural language processing (NLP).\n sent1: Since the size of problem space for syllable-to-word (STW) conversion is much less than that of syllable-tocharacter (STC) conversion, the most pinyinbased Chinese input systems (#REF; #REF; #REF; #REF; Microsoft Research Center in Beijing; #TARGET_REF are addressed on STW conversion.\n sent2: On the other hand, STW conversion is the main task of Chinese Language Processing in typical Chinese speech recognition systems (#REF; #REF; #REF; #REF) .\n sent3: As per (#REF; #REF; #REF; #REF; #REF; #REF) , homophone selection and syllableword segmentation are two critical problems in developing a Chinese input system.\n sent4: Incorrect homophone selection and syllable-word seg-mentation will directly influence the STW conversion accuracy.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "In Chinese, each Chinese word can be a mono-syllabic word, such as \"鼠(mouse)\", a bisyllabic word, such as \"袋鼠(kangaroo)\", or a multi-syllabic word, such as \"米老鼠(Mickey mouse).\" The corresponding phonetic and pinyin syllables of each Chinese word is called syllable-words, such as \"dai4 shu3\" is the pinyin syllable-word of \"袋鼠(kangaroo).\" According to our computation, the {minimum, maximum, average} words per each distinct mono-syllableword and poly-syllable-word (including bisyllable-word and multi-syllable-word) in the CKIP dictionary (Chinese Knowledge Information Processing #REF) are {1, 28, 2.8} and {1, 7, 1.1}, respectively.",
                "The CKIP dictionary is one of most commonly-used Chinese dictionaries in the research field of Chinese natural language processing (NLP).",
                "Since the size of problem space for syllable-to-word (STW) conversion is much less than that of syllable-tocharacter (STC) conversion, the most pinyinbased Chinese input systems (#REF; #REF; #REF; #REF; Microsoft Research Center in Beijing; #REF) are addressed on STW conversion.",
                "On the other hand, STW conversion is the main task of Chinese Language Processing in typical Chinese speech recognition systems (#REF; #REF; #REF; #REF) .",
                "As per (#REF; #REF; #REF; #REF; #REF; #TARGET_REF , homophone selection and syllableword segmentation are two critical problems in developing a Chinese input system."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In Chinese, each Chinese word can be a mono-syllabic word, such as \"鼠(mouse)\", a bisyllabic word, such as \"袋鼠(kangaroo)\", or a multi-syllabic word, such as \"米老鼠(Mickey mouse).\" The corresponding phonetic and pinyin syllables of each Chinese word is called syllable-words, such as \"dai4 shu3\" is the pinyin syllable-word of \"袋鼠(kangaroo).\" According to our computation, the {minimum, maximum, average} words per each distinct mono-syllableword and poly-syllable-word (including bisyllable-word and multi-syllable-word) in the CKIP dictionary (Chinese Knowledge Information Processing #REF) are {1, 28, 2.8} and {1, 7, 1.1}, respectively.\n sent1: The CKIP dictionary is one of most commonly-used Chinese dictionaries in the research field of Chinese natural language processing (NLP).\n sent2: Since the size of problem space for syllable-to-word (STW) conversion is much less than that of syllable-tocharacter (STC) conversion, the most pinyinbased Chinese input systems (#REF; #REF; #REF; #REF; Microsoft Research Center in Beijing; #REF) are addressed on STW conversion.\n sent3: On the other hand, STW conversion is the main task of Chinese Language Processing in typical Chinese speech recognition systems (#REF; #REF; #REF; #REF) .\n sent4: As per (#REF; #REF; #REF; #REF; #REF; #TARGET_REF , homophone selection and syllableword segmentation are two critical problems in developing a Chinese input system.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Incorrect homophone selection and syllable-word seg-mentation will directly influence the STW conversion accuracy.",
                "Conventionally, there are two approaches to resolve the two critical problems: (1) linguistic approach: based on syntax parsing, semantic template matching and contextual information (#REF; #REF; #REF; #REF; #REF) ; and (2) statistical approach: based on the n-gram models where n is usually 2, i.e. bigram model (#REF; #REF; #REF; #REF; #REF; #REF; #REF) .",
                "From the studies (#REF; #REF; #REF; #REF; #TARGET_REF , the linguistic approach requires considerable effort in designing effective syntax rules, semantic templates or contextual information, thus, it is more user-friendly than the statistical approach on understanding why such a system makes a mistake.",
                "The statistical language model (SLM) used in the statistical approach requires less effort and has been widely adopted in commercial Chinese input systems.",
                "In our previous work (#REF) , a wordpair (WP) identifier was proposed and shown a simple and effective way to improve Chinese input systems by providing tonal and toneless STW accuracies of 98.5% and 90.7% on the identified poly-syllabic words, respectively."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Incorrect homophone selection and syllable-word seg-mentation will directly influence the STW conversion accuracy.\n sent1: Conventionally, there are two approaches to resolve the two critical problems: (1) linguistic approach: based on syntax parsing, semantic template matching and contextual information (#REF; #REF; #REF; #REF; #REF) ; and (2) statistical approach: based on the n-gram models where n is usually 2, i.e. bigram model (#REF; #REF; #REF; #REF; #REF; #REF; #REF) .\n sent2: From the studies (#REF; #REF; #REF; #REF; #TARGET_REF , the linguistic approach requires considerable effort in designing effective syntax rules, semantic templates or contextual information, thus, it is more user-friendly than the statistical approach on understanding why such a system makes a mistake.\n sent3: The statistical language model (SLM) used in the statistical approach requires less effort and has been widely adopted in commercial Chinese input systems.\n sent4: In our previous work (#REF) , a wordpair (WP) identifier was proposed and shown a simple and effective way to improve Chinese input systems by providing tonal and toneless STW accuracies of 98.5% and 90.7% on the identified poly-syllabic words, respectively.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The statistical language model (SLM) used in the statistical approach requires less effort and has been widely adopted in commercial Chinese input systems.",
                "In our previous work #TARGET_REF , a wordpair (WP) identifier was proposed and shown a simple and effective way to improve Chinese input systems by providing tonal and toneless STW accuracies of 98.5% and 90.7% on the identified poly-syllabic words, respectively.",
                "In (#REF) , we have shown that the WP identifier can be used to reduce the over weighting and corpus sparseness problems of bigram models and achieve better STW accuracy to improve Chinese input systems.",
                "As per our computation, poly-syllabic words cover about 70% characters of Chinese sentences.",
                "Since the identified character ratio of the WP identifier (#REF ) is about 55%, there are still about 15% improving room left."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The statistical language model (SLM) used in the statistical approach requires less effort and has been widely adopted in commercial Chinese input systems.\n sent1: In our previous work #TARGET_REF , a wordpair (WP) identifier was proposed and shown a simple and effective way to improve Chinese input systems by providing tonal and toneless STW accuracies of 98.5% and 90.7% on the identified poly-syllabic words, respectively.\n sent2: In (#REF) , we have shown that the WP identifier can be used to reduce the over weighting and corpus sparseness problems of bigram models and achieve better STW accuracy to improve Chinese input systems.\n sent3: As per our computation, poly-syllabic words cover about 70% characters of Chinese sentences.\n sent4: Since the identified character ratio of the WP identifier (#REF ) is about 55%, there are still about 15% improving room left.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The statistical language model (SLM) used in the statistical approach requires less effort and has been widely adopted in commercial Chinese input systems.",
                "In our previous work (#REF) , a wordpair (WP) identifier was proposed and shown a simple and effective way to improve Chinese input systems by providing tonal and toneless STW accuracies of 98.5% and 90.7% on the identified poly-syllabic words, respectively.",
                "In #TARGET_REF , we have shown that the WP identifier can be used to reduce the over weighting and corpus sparseness problems of bigram models and achieve better STW accuracy to improve Chinese input systems.",
                "As per our computation, poly-syllabic words cover about 70% characters of Chinese sentences.",
                "Since the identified character ratio of the WP identifier (#REF ) is about 55%, there are still about 15% improving room left."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The statistical language model (SLM) used in the statistical approach requires less effort and has been widely adopted in commercial Chinese input systems.\n sent1: In our previous work (#REF) , a wordpair (WP) identifier was proposed and shown a simple and effective way to improve Chinese input systems by providing tonal and toneless STW accuracies of 98.5% and 90.7% on the identified poly-syllabic words, respectively.\n sent2: In #TARGET_REF , we have shown that the WP identifier can be used to reduce the over weighting and corpus sparseness problems of bigram models and achieve better STW accuracy to improve Chinese input systems.\n sent3: As per our computation, poly-syllabic words cover about 70% characters of Chinese sentences.\n sent4: Since the identified character ratio of the WP identifier (#REF ) is about 55%, there are still about 15% improving room left.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "From Table 3b , the tonal and toneless STW improvements of the BiGram by using the WP identifier and the WSM are (8.6%, 11.9%) and (17.1%, 22.0%), respectively.",
                "(Note that, as per #TARGET_REF , the differences between the tonal and toneless STW accuracies of the BiGram and the TriGram are less than 0.3%).",
                "Table 3c is the results of the MSIME and the BiGram by using the WSM as an adaptation processing with both system and user WP database.",
                "From Table 3c , we get the average tonal and toneless STW improvements of the MSIME and the BiGram by using the WSM as an adaptation processing are 37.2% and 34.6%, respectively.",
                "Table 3c ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: From Table 3b , the tonal and toneless STW improvements of the BiGram by using the WP identifier and the WSM are (8.6%, 11.9%) and (17.1%, 22.0%), respectively.\n sent1: (Note that, as per #TARGET_REF , the differences between the tonal and toneless STW accuracies of the BiGram and the TriGram are less than 0.3%).\n sent2: Table 3c is the results of the MSIME and the BiGram by using the WSM as an adaptation processing with both system and user WP database.\n sent3: From Table 3c , we get the average tonal and toneless STW improvements of the MSIME and the BiGram by using the WSM as an adaptation processing are 37.2% and 34.6%, respectively.\n sent4: Table 3c .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "As per our computation, poly-syllabic words cover about 70% characters of Chinese sentences.",
                "Since the identified character ratio of the WP identifier #TARGET_REF ) is about 55%, there are still about 15% improving room left.",
                "The objective of this study is to illustrate a word support model (WSM) that is able to improve our WP-identifier by achieving better identified character ratio and STW accuracy on the identified poly-syllabic words with the same word-pair database.",
                "We conduct STW experiments to show the tonal and toneless STW accuracies of a commercial input product (Microsoft Input Method #REF, MSIME) , and an optimized bigram model, BiGram (#REF) , can both be improved by our WSM and achieve better STW improvements than that of these systems with the WP identifier.",
                "The remainder of this paper is arranged as follows."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: As per our computation, poly-syllabic words cover about 70% characters of Chinese sentences.\n sent1: Since the identified character ratio of the WP identifier #TARGET_REF ) is about 55%, there are still about 15% improving room left.\n sent2: The objective of this study is to illustrate a word support model (WSM) that is able to improve our WP-identifier by achieving better identified character ratio and STW accuracy on the identified poly-syllabic words with the same word-pair database.\n sent3: We conduct STW experiments to show the tonal and toneless STW accuracies of a commercial input product (Microsoft Input Method #REF, MSIME) , and an optimized bigram model, BiGram (#REF) , can both be improved by our WSM and achieve better STW improvements than that of these systems with the WP identifier.\n sent4: The remainder of this paper is arranged as follows.\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Since the identified character ratio of the WP identifier (#REF ) is about 55%, there are still about 15% improving room left.",
                "The objective of this study is to illustrate a word support model (WSM) that is able to improve our WP-identifier by achieving better identified character ratio and STW accuracy on the identified poly-syllabic words with the same word-pair database.",
                "We conduct STW experiments to show the tonal and toneless STW accuracies of a commercial input product (Microsoft Input Method #REF, MSIME) , and an optimized bigram model, BiGram #TARGET_REF , can both be improved by our WSM and achieve better STW improvements than that of these systems with the WP identifier.",
                "The remainder of this paper is arranged as follows.",
                "In Section 2, we present an auto wordpair (AUTO-WP) generation used to generate the WP database."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Since the identified character ratio of the WP identifier (#REF ) is about 55%, there are still about 15% improving room left.\n sent1: The objective of this study is to illustrate a word support model (WSM) that is able to improve our WP-identifier by achieving better identified character ratio and STW accuracy on the identified poly-syllabic words with the same word-pair database.\n sent2: We conduct STW experiments to show the tonal and toneless STW accuracies of a commercial input product (Microsoft Input Method #REF, MSIME) , and an optimized bigram model, BiGram #TARGET_REF , can both be improved by our WSM and achieve better STW improvements than that of these systems with the WP identifier.\n sent3: The remainder of this paper is arranged as follows.\n sent4: In Section 2, we present an auto wordpair (AUTO-WP) generation used to generate the WP database.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Following #TARGET_REF , the three steps of autogenerating word-pairs (AUTO-WP) for a given Chinese sentence are as below: (the details of AUTO-WP can be found in #TARGET_REF Step 1. Get forward and backward word segmentations: Generate two types of word segmentations for a given Chinese sentence by forward maximum matching (FMM) and backward maximum matching (BMM) techniques (#REF; #REF) with the system dictionary.",
                "Step 2. Get initial WP set: Extract all the combinations of word-pairs from the FMM and the BMM segmentations of Step 1 to be the initial WP set.",
                "Step 3. Get finial WP set: Select out the wordpairs comprised of two poly-syllabic words from the initial WP set into the finial WP set.",
                "For the final WP set, if the word-pair is not found in the WP data-base, insert it into the WP database and set its frequency to 1; otherwise, increase its frequency by 1."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Following #TARGET_REF , the three steps of autogenerating word-pairs (AUTO-WP) for a given Chinese sentence are as below: (the details of AUTO-WP can be found in #TARGET_REF Step 1. Get forward and backward word segmentations: Generate two types of word segmentations for a given Chinese sentence by forward maximum matching (FMM) and backward maximum matching (BMM) techniques (#REF; #REF) with the system dictionary.\n sent1: Step 2. Get initial WP set: Extract all the combinations of word-pairs from the FMM and the BMM segmentations of Step 1 to be the initial WP set.\n sent2: Step 3. Get finial WP set: Select out the wordpairs comprised of two poly-syllabic words from the initial WP set into the finial WP set.\n sent3: For the final WP set, if the word-pair is not found in the WP data-base, insert it into the WP database and set its frequency to 1; otherwise, increase its frequency by 1.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "The purpose of this experiment is to demonstrate the tonal and toneless STW accuracies among the identified words by using the WSM with the system WP database.",
                "The comparative system is the WP identifier #TARGET_REF .",
                "Table  2 is the experimental results.",
                "The WP database and system dictionary of the WP identifier is same with that of the WSM.",
                "From Table 2"
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The purpose of this experiment is to demonstrate the tonal and toneless STW accuracies among the identified words by using the WSM with the system WP database.\n sent1: The comparative system is the WP identifier #TARGET_REF .\n sent2: Table  2 is the experimental results.\n sent3: The WP database and system dictionary of the WP identifier is same with that of the WSM.\n sent4: From Table 2\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We selected Microsoft Input Method #REF for Traditional Chinese (MSIME) as our experimental commercial Chinese input system.",
                "In addition, following #TARGET_REF , an optimized bigram model called BiGram was developed.",
                "The BiGram STW system is a bigrambased model developing by SRILM (#REF) with Good-Turing back-off smoothing (#REF) , as well as forward and backward longest syllable-word first strategies (#REF; #REF) .",
                "The system dictionary of the BiGram is same with that of the WP identifier and the WSM.",
                "Table 3a compares the results of the MSIME, the MSIME with the WP identifier and the MSIME with the WSM on the closed and open test sentences."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We selected Microsoft Input Method #REF for Traditional Chinese (MSIME) as our experimental commercial Chinese input system.\n sent1: In addition, following #TARGET_REF , an optimized bigram model called BiGram was developed.\n sent2: The BiGram STW system is a bigrambased model developing by SRILM (#REF) with Good-Turing back-off smoothing (#REF) , as well as forward and backward longest syllable-word first strategies (#REF; #REF) .\n sent3: The system dictionary of the BiGram is same with that of the WP identifier and the WSM.\n sent4: Table 3a compares the results of the MSIME, the MSIME with the WP identifier and the MSIME with the WSM on the closed and open test sentences.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "In most Chinese input systems, unknown word extraction is not specifically a STW problem, therefore, it is usually taken care of through online and offline manual editing processing (Hsu et al, 1999) .",
                "The results of Table 4 show that the most STW errors should be caused by ISWS and HS problems, not UW problem.",
                "This observation is similarly with that of our previous work #TARGET_REF .",
                "(2) The major problem of error conversions in tonal and toneless STW systems is different.",
                "This observation is similarly with that of (#REF) ."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: In most Chinese input systems, unknown word extraction is not specifically a STW problem, therefore, it is usually taken care of through online and offline manual editing processing (Hsu et al, 1999) .\n sent1: The results of Table 4 show that the most STW errors should be caused by ISWS and HS problems, not UW problem.\n sent2: This observation is similarly with that of our previous work #TARGET_REF .\n sent3: (2) The major problem of error conversions in tonal and toneless STW systems is different.\n sent4: This observation is similarly with that of (#REF) .\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "In most Chinese input systems, unknown word extraction is not specifically a STW problem, therefore, it is usually taken care of through online and offline manual editing processing (Hsu et al, 1999) .",
                "The results of Table 4 show that the most STW errors should be caused by ISWS and HS problems, not UW problem.",
                "This observation is similarly with that of our previous work (#REF) .",
                "(2) The major problem of error conversions in tonal and toneless STW systems is different.",
                "This observation is similarly with that of #TARGET_REF ."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: In most Chinese input systems, unknown word extraction is not specifically a STW problem, therefore, it is usually taken care of through online and offline manual editing processing (Hsu et al, 1999) .\n sent1: The results of Table 4 show that the most STW errors should be caused by ISWS and HS problems, not UW problem.\n sent2: This observation is similarly with that of our previous work (#REF) .\n sent3: (2) The major problem of error conversions in tonal and toneless STW systems is different.\n sent4: This observation is similarly with that of #TARGET_REF .\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "In this paper, we present a word support model (WSM) to improve the WP identifier #TARGET_REF and support the Chinese Language Processing on the STW conversion problem.",
                "All of the WP data can be generated fully automatically by applying the AUTO-WP on the given corpus.",
                "We are encouraged by the fact that the WSM with WP knowledge is able to achieve state-of-the-art tonal and toneless STW accuracies of 99% and 92%, respectively, for the identified poly-syllabic words.",
                "The WSM can be easily integrated into existing Chinese input systems by identifying words as a post processing.",
                "Our experimental results show that, by applying the WSM as an adaptation processing together with the MSIME (a trigram-like model) and the BiGram (an optimized bigram model), the average tonal and toneless STW improvements of the two Chinese input systems are 37% and 35%, respectively."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: In this paper, we present a word support model (WSM) to improve the WP identifier #TARGET_REF and support the Chinese Language Processing on the STW conversion problem.\n sent1: All of the WP data can be generated fully automatically by applying the AUTO-WP on the given corpus.\n sent2: We are encouraged by the fact that the WSM with WP knowledge is able to achieve state-of-the-art tonal and toneless STW accuracies of 99% and 92%, respectively, for the identified poly-syllabic words.\n sent3: The WSM can be easily integrated into existing Chinese input systems by identifying words as a post processing.\n sent4: Our experimental results show that, by applying the WSM as an adaptation processing together with the MSIME (a trigram-like model) and the BiGram (an optimized bigram model), the average tonal and toneless STW improvements of the two Chinese input systems are 37% and 35%, respectively.\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "They also cleaned United Nations material and post-edited general-domain data that was previously filtered as indomain following the \"invitation model\" (Hoang and Sima'an, 2014) .",
                "For the other language pairs, the input material was 30,000 post-edited segments.",
                "The main part of the training corpora (approximately 75%) was part of Pangeanic's own repository harvested through web crawling and also OpenSubtitles (#REF) .",
                "The rest of the corpus was automatically validated synthetic material using general data from Leipzig (#REF Engine customization The data was cleaned using the Bicleaner tool #TARGET_REF .",
                "The data was lowercased and extra embeddings were added in order to keep the case information."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: They also cleaned United Nations material and post-edited general-domain data that was previously filtered as indomain following the \"invitation model\" (Hoang and Sima'an, 2014) .\n sent1: For the other language pairs, the input material was 30,000 post-edited segments.\n sent2: The main part of the training corpora (approximately 75%) was part of Pangeanic's own repository harvested through web crawling and also OpenSubtitles (#REF) .\n sent3: The rest of the corpus was automatically validated synthetic material using general data from Leipzig (#REF Engine customization The data was cleaned using the Bicleaner tool #TARGET_REF .\n sent4: The data was lowercased and extra embeddings were added in order to keep the case information.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "For the other language pairs, the input material was 30,000 post-edited segments.",
                "The main part of the training corpora (approximately 75%) was part of Pangeanic's own repository harvested through web crawling and also OpenSubtitles (#REF) .",
                "The rest of the corpus was automatically validated synthetic material using general data from Leipzig (#REF) .",
                "Engine customization The data was cleaned using the Bicleaner tool #TARGET_REF .",
                "The data was lowercased and extra embeddings were added in order to keep the case information."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: For the other language pairs, the input material was 30,000 post-edited segments.\n sent1: The main part of the training corpora (approximately 75%) was part of Pangeanic's own repository harvested through web crawling and also OpenSubtitles (#REF) .\n sent2: The rest of the corpus was automatically validated synthetic material using general data from Leipzig (#REF) .\n sent3: Engine customization The data was cleaned using the Bicleaner tool #TARGET_REF .\n sent4: The data was lowercased and extra embeddings were added in order to keep the case information.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Audio and Word Embeddings.",
                "Multiple works in the recent past (#REF; #REF; Lopopolo and #REF; #TARGET_REF; #REF) have explored using perceptual modalities like vision and sound to learn language embeddings.",
                "While Lopopolo and #REF show preliminary results on using sound to learn distributional representations, #REF build on ideas from #REF to learn word embeddings that respect both linguistic and auditory relationships by optimizing a joint objective.",
                "Further, they propose various fusion strategies to combine knowledge from both the modalities.",
                "Instead, we \"specialize\" embeddings to exclusively respect relationships defined by sounds, while initializing with word2vec embeddings for smoothness."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Audio and Word Embeddings.\n sent1: Multiple works in the recent past (#REF; #REF; Lopopolo and #REF; #TARGET_REF; #REF) have explored using perceptual modalities like vision and sound to learn language embeddings.\n sent2: While Lopopolo and #REF show preliminary results on using sound to learn distributional representations, #REF build on ideas from #REF to learn word embeddings that respect both linguistic and auditory relationships by optimizing a joint objective.\n sent3: Further, they propose various fusion strategies to combine knowledge from both the modalities.\n sent4: Instead, we \"specialize\" embeddings to exclusively respect relationships defined by sounds, while initializing with word2vec embeddings for smoothness.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Audio and Word Embeddings.",
                "Multiple works in the recent past (#REF; #REF; Lopopolo and #REF; #REF; #REF) have explored using perceptual modalities like vision and sound to learn language embeddings.",
                "While Lopopolo and #REF show preliminary results on using sound to learn distributional representations, #TARGET_REF build on ideas from #REF to learn word embeddings that respect both linguistic and auditory relationships by optimizing a joint objective.",
                "Further, they propose various fusion strategies to combine knowledge from both the modalities.",
                "Instead, we \"specialize\" embeddings to exclusively respect relationships defined by sounds, while initializing with word2vec embeddings for smoothness."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Audio and Word Embeddings.\n sent1: Multiple works in the recent past (#REF; #REF; Lopopolo and #REF; #REF; #REF) have explored using perceptual modalities like vision and sound to learn language embeddings.\n sent2: While Lopopolo and #REF show preliminary results on using sound to learn distributional representations, #TARGET_REF build on ideas from #REF to learn word embeddings that respect both linguistic and auditory relationships by optimizing a joint objective.\n sent3: Further, they propose various fusion strategies to combine knowledge from both the modalities.\n sent4: Instead, we \"specialize\" embeddings to exclusively respect relationships defined by sounds, while initializing with word2vec embeddings for smoothness.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "For example, the description to produce a foley \"driving on gravel\" sound is to record the \"crunching sound of plastic or polyethene bags\".",
                "AMEN and ASLex.",
                "AMEN and ASLex #TARGET_REF are subsets of the standard MEN (#REF) and SimLex (#REF) word similarity datasets consisting of word-pairs that \"can be associated with a distinctive associated sound\".",
                "We evaluate on this dataset for completeness to benchmark our approach against previous work.",
                "However, we are primarily interested in the slightly different problem of relating words with similar auditory instantions that may or may not be semantically related as opposed to relating semantically similar words that can be associated with some common auditory signal."
            ],
            "label": [
                "BACKGROUND",
                "USE"
            ]
        },
        "input": "sent0: For example, the description to produce a foley \"driving on gravel\" sound is to record the \"crunching sound of plastic or polyethene bags\".\n sent1: AMEN and ASLex.\n sent2: AMEN and ASLex #TARGET_REF are subsets of the standard MEN (#REF) and SimLex (#REF) word similarity datasets consisting of word-pairs that \"can be associated with a distinctive associated sound\".\n sent3: We evaluate on this dataset for completeness to benchmark our approach against previous work.\n sent4: However, we are primarily interested in the slightly different problem of relating words with similar auditory instantions that may or may not be semantically related as opposed to relating semantically similar words that can be associated with some common auditory signal.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"USE\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "AMEN and ASLex #TARGET_REF are subsets of the MEN and #REF9 datasets for word relatedness grounded in sound.",
                "From Table 2, we can see that our embeddings outperform (#REF) on both AMEN and ASLex.",
                "These datasets were curated by annotating concepts related by sound; however we observe that relatedness is often confounded.",
                "For example, (river, water), (automobile, car) are marked as aurally related however they do not stand out as aurally-related examples as they are already semantically related.",
                "In contrast, we are interested in how onomatopoeic words relate to regular words (Table 3) , which we study by explicit grounding in sound."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: AMEN and ASLex #TARGET_REF are subsets of the MEN and #REF9 datasets for word relatedness grounded in sound.\n sent1: From Table 2, we can see that our embeddings outperform (#REF) on both AMEN and ASLex.\n sent2: These datasets were curated by annotating concepts related by sound; however we observe that relatedness is often confounded.\n sent3: For example, (river, water), (automobile, car) are marked as aurally related however they do not stand out as aurally-related examples as they are already semantically related.\n sent4: In contrast, we are interested in how onomatopoeic words relate to regular words (Table 3) , which we study by explicit grounding in sound.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Freesound.",
                "We use the freesound database (#REF) , also used in prior work #TARGET_REF; Lopopolo and #REF) to learn the proposed sound-word2vec embeddings.",
                "Freesound is a freely available, collaborative dataset consisting of user uploaded sounds permitting reuse.",
                "All uploaded sounds have human descriptions in the form of tags and captions in natural language.",
                "The tags contain a broad set of relevant topics for a sound (e.g., ambience, electronic, birds, city, reverb) and captions describing the content of the sound, in addition to details pertaining to audio quality."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Freesound.\n sent1: We use the freesound database (#REF) , also used in prior work #TARGET_REF; Lopopolo and #REF) to learn the proposed sound-word2vec embeddings.\n sent2: Freesound is a freely available, collaborative dataset consisting of user uploaded sounds permitting reuse.\n sent3: All uploaded sounds have human descriptions in the form of tags and captions in natural language.\n sent4: The tags contain a broad set of relevant topics for a sound (e.g., ambience, electronic, birds, city, reverb) and captions describing the content of the sound, in addition to details pertaining to audio quality.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We compare against previous works Lopopolo and #REF and #REF .",
                "While the former uses a standard bag of words and SVD pipeline to arrive at distributional representations for words, the latter trains under a joint objective that respects both linguistic and auditory similarity.",
                "We use the openly available implementation for Lopopolo and #REF and re-implement #TARGET_REF and train them on our dataset for a fair comparison of the methods.",
                "In addition, we show a comparison to word-vectors released by (#REF) in the supplementary material.",
                "All approaches use an embedding size of 300 for consistency."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We compare against previous works Lopopolo and #REF and #REF .\n sent1: While the former uses a standard bag of words and SVD pipeline to arrive at distributional representations for words, the latter trains under a joint objective that respects both linguistic and auditory similarity.\n sent2: We use the openly available implementation for Lopopolo and #REF and re-implement #TARGET_REF and train them on our dataset for a fair comparison of the methods.\n sent3: In addition, we show a comparison to word-vectors released by (#REF) in the supplementary material.\n sent4: All approaches use an embedding size of 300 for consistency.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We fine-tune on a subset of 9578 tags which are present in both Freesound as well as Google news corpus datasets, which is 55.68% of the original tags in the Freesound dataset.",
                "This helps us remove noisy tags unrelated to the content of the sound.",
                "In addition to enlarging the vocabulary, the pretraining helps induce smoothness in the soundword2vec embeddings -allowing us to transfer semantics learnt from sounds to words that were not present as tags in the Freesound database.",
                "Indeed, we find that word2vec pre-training helps improve performance (Sec. 5.3).",
                "Our use of language embeddings as an initialization to fine-tune (specialize) from, as opposed to formulating a joint objective with language and audio context #TARGET_REF is driven by the fact that we are interested in embeddings for words grounded in sounds, and not better generic word similarity."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: We fine-tune on a subset of 9578 tags which are present in both Freesound as well as Google news corpus datasets, which is 55.68% of the original tags in the Freesound dataset.\n sent1: This helps us remove noisy tags unrelated to the content of the sound.\n sent2: In addition to enlarging the vocabulary, the pretraining helps induce smoothness in the soundword2vec embeddings -allowing us to transfer semantics learnt from sounds to words that were not present as tags in the Freesound database.\n sent3: Indeed, we find that word2vec pre-training helps improve performance (Sec. 5.3).\n sent4: Our use of language embeddings as an initialization to fine-tune (specialize) from, as opposed to formulating a joint objective with language and audio context #TARGET_REF is driven by the fact that we are interested in embeddings for words grounded in sounds, and not better generic word similarity.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "1 shows that our sound-word2vec embeddings outperform the baselines.",
                "We see that specializing the embeddings for sound using our two-stage training outperforms prior work #TARGET_REF and Lopopolo and #REF ), which did not do specialization.",
                "Among our approaches, tag-word2vec performs second best -this is intuitive since the tag distributions implicitly capture auditory relatedness (a sound may have tags cat and meow), while word2vec and sound-word2vec(r) have the lowest performance.",
                "(#REF) (higher is better).",
                "Our approach performs better than #REF ."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: 1 shows that our sound-word2vec embeddings outperform the baselines.\n sent1: We see that specializing the embeddings for sound using our two-stage training outperforms prior work #TARGET_REF and Lopopolo and #REF ), which did not do specialization.\n sent2: Among our approaches, tag-word2vec performs second best -this is intuitive since the tag distributions implicitly capture auditory relatedness (a sound may have tags cat and meow), while word2vec and sound-word2vec(r) have the lowest performance.\n sent3: (#REF) (higher is better).\n sent4: Our approach performs better than #REF .\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "1 shows that our sound-word2vec embeddings outperform the baselines.",
                "We see that specializing the embeddings for sound using our two-stage training outperforms prior work (#REF and Lopopolo and #REF ), which did not do specialization.",
                "Among our approaches, tag-word2vec performs second best -this is intuitive since the tag distributions implicitly capture auditory relatedness (a sound may have tags cat and meow), while word2vec and sound-word2vec(r) have the lowest performance.",
                "(#REF) (higher is better).",
                "Our approach performs better than #TARGET_REF ."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: 1 shows that our sound-word2vec embeddings outperform the baselines.\n sent1: We see that specializing the embeddings for sound using our two-stage training outperforms prior work (#REF and Lopopolo and #REF ), which did not do specialization.\n sent2: Among our approaches, tag-word2vec performs second best -this is intuitive since the tag distributions implicitly capture auditory relatedness (a sound may have tags cat and meow), while word2vec and sound-word2vec(r) have the lowest performance.\n sent3: (#REF) (higher is better).\n sent4: Our approach performs better than #TARGET_REF .\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "We find that Sound-word2vec performs the best with a mean rank of 34.6 compared to other baselines tag-word2vec (38.9), soundword2vec(r) (114.3) and word2vec (189.45).",
                "As observed previously, the second best performing approach is tag-word2vec.",
                "Lopopolo and #REF and #TARGET_REF perform worse than tag-word2vec with a mean rank of 48.4 and 42.1 respectively.",
                "Note that random chance gets a rank of (|V| + 1)/2 = 4789.5."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: We find that Sound-word2vec performs the best with a mean rank of 34.6 compared to other baselines tag-word2vec (38.9), soundword2vec(r) (114.3) and word2vec (189.45).\n sent1: As observed previously, the second best performing approach is tag-word2vec.\n sent2: Lopopolo and #REF and #TARGET_REF perform worse than tag-word2vec with a mean rank of 48.4 and 42.1 respectively.\n sent3: Note that random chance gets a rank of (|V| + 1)/2 = 4789.5.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent0\", \"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "AMEN and ASLex (#REF) are subsets of the MEN and #REF9 datasets for word relatedness grounded in sound.",
                "From Table 2, we can see that our embeddings outperform #TARGET_REF on both AMEN and ASLex.",
                "These datasets were curated by annotating concepts related by sound; however we observe that relatedness is often confounded.",
                "For example, (river, water), (automobile, car) are marked as aurally related however they do not stand out as aurally-related examples as they are already semantically related.",
                "In contrast, we are interested in how onomatopoeic words relate to regular words (Table 3) , which we study by explicit grounding in sound."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: AMEN and ASLex (#REF) are subsets of the MEN and #REF9 datasets for word relatedness grounded in sound.\n sent1: From Table 2, we can see that our embeddings outperform #TARGET_REF on both AMEN and ASLex.\n sent2: These datasets were curated by annotating concepts related by sound; however we observe that relatedness is often confounded.\n sent3: For example, (river, water), (automobile, car) are marked as aurally related however they do not stand out as aurally-related examples as they are already semantically related.\n sent4: In contrast, we are interested in how onomatopoeic words relate to regular words (Table 3) , which we study by explicit grounding in sound.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "In addition to the language-only baseline word2vec (#REF) , we compare against tag-word2vec -that predicts a tag using other tags of the sound as context, inspired by (#REF) .",
                "We also report results with a randomly initialized projection matrix (soundword2vec(r) to evaluate the effectiveness of pretraining with word2vec.",
                "Prior work.",
                "We compare against previous works Lopopolo and #REF and #TARGET_REF .",
                "While the former uses a standard bag of words and SVD pipeline to arrive at distributional representations for words, the latter trains under a joint objective that respects both linguistic and auditory similarity."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: In addition to the language-only baseline word2vec (#REF) , we compare against tag-word2vec -that predicts a tag using other tags of the sound as context, inspired by (#REF) .\n sent1: We also report results with a randomly initialized projection matrix (soundword2vec(r) to evaluate the effectiveness of pretraining with word2vec.\n sent2: Prior work.\n sent3: We compare against previous works Lopopolo and #REF and #TARGET_REF .\n sent4: While the former uses a standard bag of words and SVD pipeline to arrive at distributional representations for words, the latter trains under a joint objective that respects both linguistic and auditory similarity.\n",
        "output": "{\"label\": [null], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "We compare against previous works Lopopolo and #REF and #REF .",
                "While the former uses a standard bag of words and SVD pipeline to arrive at distributional representations for words, the latter trains under a joint objective that respects both linguistic and auditory similarity.",
                "We use the openly available implementation for Lopopolo and #REF and re-implement #REF and train them on our dataset for a fair comparison of the methods.",
                "In addition, we show a comparison to word-vectors released by #TARGET_REF in the supplementary material.",
                "All approaches use an embedding size of 300 for consistency."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: We compare against previous works Lopopolo and #REF and #REF .\n sent1: While the former uses a standard bag of words and SVD pipeline to arrive at distributional representations for words, the latter trains under a joint objective that respects both linguistic and auditory similarity.\n sent2: We use the openly available implementation for Lopopolo and #REF and re-implement #REF and train them on our dataset for a fair comparison of the methods.\n sent3: In addition, we show a comparison to word-vectors released by #TARGET_REF in the supplementary material.\n sent4: All approaches use an embedding size of 300 for consistency.\n",
        "output": "{\"label\": [null], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "For speech recognition, this presents the challenge to reduce the number of timesteps in the signal without throwing away relevant information.",
                "Representations based on the Fourier transform have proven effective at this task as the transform forms a complete basis for signal reconstruction.",
                "Deep learning's recent success in speech recognition is based on learning feature hierarchies atop these representations [1, #TARGET_REF .",
                "There has been increasing focus on extending this end-toend learning approach down to the level of the raw waveform.",
                "A popular approach is pass the waveform through strided convolutions, or networks connected to local temporal frames, often followed by a pooling step to create invariance to phase shifts and further downsample the signal [3, 4, 5, 6, 7, 8] ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For speech recognition, this presents the challenge to reduce the number of timesteps in the signal without throwing away relevant information.\n sent1: Representations based on the Fourier transform have proven effective at this task as the transform forms a complete basis for signal reconstruction.\n sent2: Deep learning's recent success in speech recognition is based on learning feature hierarchies atop these representations [1, #TARGET_REF .\n sent3: There has been increasing focus on extending this end-toend learning approach down to the level of the raw waveform.\n sent4: A popular approach is pass the waveform through strided convolutions, or networks connected to local temporal frames, often followed by a pooling step to create invariance to phase shifts and further downsample the signal [3, 4, 5, 6, 7, 8] .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The experimental design of this study is modelled after our previous work on end-to-end speech recognition [12, #TARGET_REF .",
                "However, to decrease the experimental latency, we train on a reduced version of the model and a subset of the training data.",
                "The basic architecture is shown in Table 1 .",
                "While we vary the front end processing, the backend remains the same: a convolutional (through time) layer, followed by three bidirectional simple recurrent layers, and a fully connected layer.",
                "Batch normalization [13] , is employed between each layer, but not between individual timesteps [2] ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The experimental design of this study is modelled after our previous work on end-to-end speech recognition [12, #TARGET_REF .\n sent1: However, to decrease the experimental latency, we train on a reduced version of the model and a subset of the training data.\n sent2: The basic architecture is shown in Table 1 .\n sent3: While we vary the front end processing, the backend remains the same: a convolutional (through time) layer, followed by three bidirectional simple recurrent layers, and a fully connected layer.\n sent4: Batch normalization [13] , is employed between each layer, but not between individual timesteps [2] .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "The basic architecture is shown in Table 1 .",
                "While we vary the front end processing, the backend remains the same: a convolutional (through time) layer, followed by three bidirectional simple recurrent layers, and a fully connected layer.",
                "Batch normalization [13] , is employed between each layer, but not between individual timesteps #TARGET_REF .",
                "Rectified linear unit (ReLU) activation functions are used for all layers, including between timesteps.",
                "We use the Connectionist Temporal Classification (CTC) cost function to integrate over all possible alignments between the network outputs and characters of the English alphabet [14] ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The basic architecture is shown in Table 1 .\n sent1: While we vary the front end processing, the backend remains the same: a convolutional (through time) layer, followed by three bidirectional simple recurrent layers, and a fully connected layer.\n sent2: Batch normalization [13] , is employed between each layer, but not between individual timesteps #TARGET_REF .\n sent3: Rectified linear unit (ReLU) activation functions are used for all layers, including between timesteps.\n sent4: We use the Connectionist Temporal Classification (CTC) cost function to integrate over all possible alignments between the network outputs and characters of the English alphabet [14] .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The training data is drawn from a diverse collection of sources including read, conversational, accented, and noisy speech #TARGET_REF .",
                "At each epoch, 40% of the utterances are randomly selected to have background noise Table 2 : Single scale waveform convolution outperforms the spectrogram baseline at low strides.",
                "The trend is visualized in Figure 2 .",
                "(superpositions of YouTube clips) added at signal-to-noise ratios ranging from 0dB to 15dB [12] .",
                "All input data (either spectrogram or waveform) is sampled at 16kHz, and normalized so that each input feature has zero mean and unit variance."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The training data is drawn from a diverse collection of sources including read, conversational, accented, and noisy speech #TARGET_REF .\n sent1: At each epoch, 40% of the utterances are randomly selected to have background noise Table 2 : Single scale waveform convolution outperforms the spectrogram baseline at low strides.\n sent2: The trend is visualized in Figure 2 .\n sent3: (superpositions of YouTube clips) added at signal-to-noise ratios ranging from 0dB to 15dB [12] .\n sent4: All input data (either spectrogram or waveform) is sampled at 16kHz, and normalized so that each input feature has zero mean and unit variance.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Hyperparameters are tuned for each model by optimizing a hold-out set.",
                "Typical values are a learning rate of 3e-4 and momentum of 0.99, and training converges after 20 epochs.",
                "Following #TARGET_REF , we sort the first epoch by utterance length (SortaGrad), to promote stability of training on long utterances.",
                "While the CTC-trained acoustic model learns a rudimentary language model itself from the training data, for testing, we supplement it with a Kneser-Ney smoothed 5-gram model that is trained using the KenLM toolkit [15] on cleaned text from the Common Crawl Repository.",
                "Decoding is done via beam search, where a weighted combination of the acoustic model and language model with an added word insert penalty is used as the value function."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Hyperparameters are tuned for each model by optimizing a hold-out set.\n sent1: Typical values are a learning rate of 3e-4 and momentum of 0.99, and training converges after 20 epochs.\n sent2: Following #TARGET_REF , we sort the first epoch by utterance length (SortaGrad), to promote stability of training on long utterances.\n sent3: While the CTC-trained acoustic model learns a rudimentary language model itself from the training data, for testing, we supplement it with a Kneser-Ney smoothed 5-gram model that is trained using the KenLM toolkit [15] on cleaned text from the Common Crawl Repository.\n sent4: Decoding is done via beam search, where a weighted combination of the acoustic model and language model with an added word insert penalty is used as the value function.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Test set word error rates (WER) are reported on a difficult in-house test set of 2048 utterances, diversely composed of noisy, conversational, voice-command, and accented speech.",
                "The test set is collected internally and from industry partners and is not represented in the training data.",
                "As previously observed #TARGET_REF , deep neural networks trained on sufficient data perform better as the model size grows.",
                "In order to make fair comparisons, the number of parameters of the models used in our experiments is held constant at 35M.",
                "We are aware that the results are not directly comparable to literature due to the use of proprietary datasets."
            ],
            "label": [
                "USE",
                "SIMILARITY"
            ]
        },
        "input": "sent0: Test set word error rates (WER) are reported on a difficult in-house test set of 2048 utterances, diversely composed of noisy, conversational, voice-command, and accented speech.\n sent1: The test set is collected internally and from industry partners and is not represented in the training data.\n sent2: As previously observed #TARGET_REF , deep neural networks trained on sufficient data perform better as the model size grows.\n sent3: In order to make fair comparisons, the number of parameters of the models used in our experiments is held constant at 35M.\n sent4: We are aware that the results are not directly comparable to literature due to the use of proprietary datasets.\n",
        "output": "{\"label\": [\"USE\", \"SIMILARITY\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF found noticeable improvements from supplementing log-mel filterbanks in such a manner.",
                "While learned features outperformed spectrograms feeding into temporal convolution in this study, many state of the art systems apply two-dimensional convolutions to their inputs #TARGET_REF 16] .",
                "Our learned features underperform in this context, which is understandable as they are not spectrally ordered, and lack spatial structure.",
                "Regularization techniques such as [17] could perhaps be key to learning ordered filter maps with useful structure.",
                "In our experiments, we made sure to downsample each scale equally with appropriate stride such that the signals can be concatenated for the later recurrent layers."
            ],
            "label": [
                "SIMILARITY",
                "DIFFERENCES"
            ]
        },
        "input": "sent0: #REF found noticeable improvements from supplementing log-mel filterbanks in such a manner.\n sent1: While learned features outperformed spectrograms feeding into temporal convolution in this study, many state of the art systems apply two-dimensional convolutions to their inputs #TARGET_REF 16] .\n sent2: Our learned features underperform in this context, which is understandable as they are not spectrally ordered, and lack spatial structure.\n sent3: Regularization techniques such as [17] could perhaps be key to learning ordered filter maps with useful structure.\n sent4: In our experiments, we made sure to downsample each scale equally with appropriate stride such that the signals can be concatenated for the later recurrent layers.\n",
        "output": "{\"label\": [\"SIMILARITY\", \"DIFFERENCES\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "This tutorial introduces the advances in deep Bayesian learning with abundant applications for natural language understanding ranging from speech recognition (#REF; #REF) to document summarization (#REF ), text classification (#REF; #REF) , text segmentation (#REF) , information extraction (#REF) , image caption generation (#REF; #REF) , sentence generation ( #TARGET_REF) , dialogue control (#REF; #REFa) , sentiment classification, recommendation system, question answering (#REF) and machine translation , to name a few.",
                "Traditionally, \"deep learning\" is taken to be a learning process where the inference or optimization is based on the real-valued deterministic model.",
                "The \"semantic structure\" in words, sentences, entities, actions and documents drawn from a large vocabulary may not be well expressed or correctly optimized in mathematical logic or computer programs.",
                "The \"distribution function\" in discrete or continuous latent variable model for natural language may not be properly decomposed or estimated in model inference.",
                "This tutorial addresses the fundamentals of statistical models and neural networks, and focus on a series of advanced Bayesian models and deep models including hierarchical Dirichlet process , Chinese restaurant process (#REF) , hierarchical Pitman-Yor process (#REF) , Indian buffet process (#REF) , recurrent neural network (#REF; Van Den #REF) , long short-term memory (#REF; , sequence-to-sequence model (#REF), variational auto-encoder (#REF) , generative adversarial network (#REF) , attention mechanism (#REF; #REF) , memory-augmented neural network (#REF; #REF) , stochastic neural network #REF) , predictive state neural network (#REF) , policy gradient (#REF) and reinforcement learning (#REF) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This tutorial introduces the advances in deep Bayesian learning with abundant applications for natural language understanding ranging from speech recognition (#REF; #REF) to document summarization (#REF ), text classification (#REF; #REF) , text segmentation (#REF) , information extraction (#REF) , image caption generation (#REF; #REF) , sentence generation ( #TARGET_REF) , dialogue control (#REF; #REFa) , sentiment classification, recommendation system, question answering (#REF) and machine translation , to name a few.\n sent1: Traditionally, \"deep learning\" is taken to be a learning process where the inference or optimization is based on the real-valued deterministic model.\n sent2: The \"semantic structure\" in words, sentences, entities, actions and documents drawn from a large vocabulary may not be well expressed or correctly optimized in mathematical logic or computer programs.\n sent3: The \"distribution function\" in discrete or continuous latent variable model for natural language may not be properly decomposed or estimated in model inference.\n sent4: This tutorial addresses the fundamentals of statistical models and neural networks, and focus on a series of advanced Bayesian models and deep models including hierarchical Dirichlet process , Chinese restaurant process (#REF) , hierarchical Pitman-Yor process (#REF) , Indian buffet process (#REF) , recurrent neural network (#REF; Van Den #REF) , long short-term memory (#REF; , sequence-to-sequence model (#REF), variational auto-encoder (#REF) , generative adversarial network (#REF) , attention mechanism (#REF; #REF) , memory-augmented neural network (#REF; #REF) , stochastic neural network #REF) , predictive state neural network (#REF) , policy gradient (#REF) and reinforcement learning (#REF) .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Reimers and Gurevych (2017b) use a smaller dataset (13862 vs 14987 sentences).",
                "Different from #REF and #TARGET_REF , choose a different data split on the POS dataset.",
                "#REF and #REF use different development sets for chunking.",
                "• Preprocessing.",
                "A typical data preprocessing step is to normize digit characters (#REF; #REF; #REF; #REF) ."
            ],
            "label": [
                "DIFFERENCES",
                "EXTENDS"
            ]
        },
        "input": "sent0: Reimers and Gurevych (2017b) use a smaller dataset (13862 vs 14987 sentences).\n sent1: Different from #REF and #TARGET_REF , choose a different data split on the POS dataset.\n sent2: #REF and #REF use different development sets for chunking.\n sent3: • Preprocessing.\n sent4: A typical data preprocessing step is to normize digit characters (#REF; #REF; #REF; #REF) .\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"EXTENDS\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Data.",
                "The NER dataset has been standardly split in Tjong Kim Sang and De Meulder (2003 (#REF; #REF; #REF; #TARGET_REF , we adopt the standard splits by using sections 0-18 as training set, sections 19-21 as development set and sections 22-24 as test set.",
                "No preprocessing is performed on either dataset except for normalizing digits.",
                "The dataset statistics are listed in Table 2 .",
                "Hyperparameters."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Data.\n sent1: The NER dataset has been standardly split in Tjong Kim Sang and De Meulder (2003 (#REF; #REF; #REF; #TARGET_REF , we adopt the standard splits by using sections 0-18 as training set, sections 19-21 as development set and sections 22-24 as test set.\n sent2: No preprocessing is performed on either dataset except for normalizing digits.\n sent3: The dataset statistics are listed in Table 2 .\n sent4: Hyperparameters.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "However, existing models use different parameter settings, which affects the fair comparison.",
                "• Evaluation.",
                "Some literature reports results using mean and standard deviation under different random seeds (#REF; #REF; #TARGET_REF .",
                "Others report the best result among different trials (#REF) , which cannot be compared directly.",
                "• Hardware environment can also affect system accuracy."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: However, existing models use different parameter settings, which affects the fair comparison.\n sent1: • Evaluation.\n sent2: Some literature reports results using mean and standard deviation under different random seeds (#REF; #REF; #TARGET_REF .\n sent3: Others report the best result among different trials (#REF) , which cannot be compared directly.\n sent4: • Hardware environment can also affect system accuracy.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "dos #REF extended this model by integrating character-level CNN features.",
                "#REF built a deeper dilated CNN architecture to capture larger local features.",
                "#REF was the first to exploit LSTM for sequence labeling.",
                "built a BiLSTM-CRF structure, which has been extended by adding character-level LSTM (#REF; #TARGET_REF , GRU (#REF) , and CNN (#REF; #REF) features.",
                "Yang et al. (2017a) proposed a neural reranking model to improve NER models."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: dos #REF extended this model by integrating character-level CNN features.\n sent1: #REF built a deeper dilated CNN architecture to capture larger local features.\n sent2: #REF was the first to exploit LSTM for sequence labeling.\n sent3: built a BiLSTM-CRF structure, which has been extended by adding character-level LSTM (#REF; #TARGET_REF , GRU (#REF) , and CNN (#REF; #REF) features.\n sent4: Yang et al. (2017a) proposed a neural reranking model to improve NER models.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Similar to character sequences in words, we can model word sequence information through LSTM or CNN structures.",
                "LSTM has been widely used in sequence labeling (#REF; #REF; #REF; #TARGET_REF .",
                "CNN can be much faster than LSTM due to the fact that convolution calculation can be parallel on the input sequence (#REF; dos #REF; #REF) .",
                "Word CNN.",
                "Figure 3(a) shows the multi-layer CNN on word sequence, where words are represented by embeddings."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Similar to character sequences in words, we can model word sequence information through LSTM or CNN structures.\n sent1: LSTM has been widely used in sequence labeling (#REF; #REF; #REF; #TARGET_REF .\n sent2: CNN can be much faster than LSTM due to the fact that convolution calculation can be parallel on the input sequence (#REF; dos #REF; #REF) .\n sent3: Word CNN.\n sent4: Figure 3(a) shows the multi-layer CNN on word sequence, where words are represented by embeddings.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "This temporal tagger then contributes towards high performance at matching event mentions with the month and year in which they occurred based on the complete posting history of users.",
                "It does so with high accuracy on informal event mentions in social media by learning to integrate the likelihood of multiple candidate dates extracted from event mentions in timerich sentences with temporal constraints extracted from event-related sentences.",
                "Despite considerable prior work in temporal information extraction, to date state-of-the-art resources are designed for extracting temporally scoped facts about public figures/organizations from newswire or Wikipedia articles #TARGET_REF; Garrido et [11/15/2008] I have noticed some pulling recently and I won't start rads until March.",
                "[11/20/2008] It is sloowwwly healing, so slowly, in fact, that she said she HOPES it will be healed by March, when I am supposed to start rads.",
                "al., 2012)."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This temporal tagger then contributes towards high performance at matching event mentions with the month and year in which they occurred based on the complete posting history of users.\n sent1: It does so with high accuracy on informal event mentions in social media by learning to integrate the likelihood of multiple candidate dates extracted from event mentions in timerich sentences with temporal constraints extracted from event-related sentences.\n sent2: Despite considerable prior work in temporal information extraction, to date state-of-the-art resources are designed for extracting temporally scoped facts about public figures/organizations from newswire or Wikipedia articles #TARGET_REF; Garrido et [11/15/2008] I have noticed some pulling recently and I won't start rads until March.\n sent3: [11/20/2008] It is sloowwwly healing, so slowly, in fact, that she said she HOPES it will be healed by March, when I am supposed to start rads.\n sent4: al., 2012).\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Once the full set of event mention sentences has been extracted for a user, all the temporal expressions (TEs) that appear in the same sentence with an event mention are resolved to a set of candidate dates.",
                "Besides a standard event-time classifier for within-sentence event-time anchoring, we leverage a new source of temporal information to train a constraint-based event-time classifier.",
                "Previous work only retrieves time-rich sentences that include both the query and some TEs #TARGET_REF; #REF) .",
                "However, sentences that contain only the event mention but no explicit TE can also be informative.",
                "For example, the post time (usually referred to as document creation time or DCT) of the sentence \"metastasis was found in my bone\" might be labeled as being after the \"metastasis\" event date."
            ],
            "label": [
                "BACKGROUND",
                "MOTIVATION"
            ]
        },
        "input": "sent0: Once the full set of event mention sentences has been extracted for a user, all the temporal expressions (TEs) that appear in the same sentence with an event mention are resolved to a set of candidate dates.\n sent1: Besides a standard event-time classifier for within-sentence event-time anchoring, we leverage a new source of temporal information to train a constraint-based event-time classifier.\n sent2: Previous work only retrieves time-rich sentences that include both the query and some TEs #TARGET_REF; #REF) .\n sent3: However, sentences that contain only the event mention but no explicit TE can also be informative.\n sent4: For example, the post time (usually referred to as document creation time or DCT) of the sentence \"metastasis was found in my bone\" might be labeled as being after the \"metastasis\" event date.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"MOTIVATION\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Previous work only retrieves time-rich sentences (i.e., date sentences) (#REF; #TARGET_REF; #REF) .",
                "However, keyword sentences can inform temporal constraints for events and therefore should not be ignored.",
                "For example, \"Well, I'm officially a Radiation grad!\" indicates the user has done radiation by the time of the post (DCT).",
                "\"Radiation is not a choice for me.\" indicates the user probably never had radiation.",
                "The topic of the sentence can also indicate the temporal relation."
            ],
            "label": [
                "BACKGROUND",
                "MOTIVATION"
            ]
        },
        "input": "sent0: Previous work only retrieves time-rich sentences (i.e., date sentences) (#REF; #TARGET_REF; #REF) .\n sent1: However, keyword sentences can inform temporal constraints for events and therefore should not be ignored.\n sent2: For example, \"Well, I'm officially a Radiation grad!\" indicates the user has done radiation by the time of the post (DCT).\n sent3: \"Radiation is not a choice for me.\" indicates the user probably never had radiation.\n sent4: The topic of the sentence can also indicate the temporal relation.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"MOTIVATION\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "The extracted date is only considered correct if it completely matches the gold date.",
                "For less than 4% of users, we have multiple dates for the same event (e.g., a user had a mastectomy twice).",
                "Similar to the evaluation metric in a previous study , in these cases, we give the system the benefit of the doubt and the extracted date is considered correct if it matches one of the gold dates.",
                "In previous work #TARGET_REF; , the evaluation metric score is defined as 1/((1 + |d|)) where d is the difference between the values in years.",
                "We choose a much stricter evaluation metric because we need a precise event date to study user behavior changes."
            ],
            "label": [
                "BACKGROUND",
                "DIFFERENCES"
            ]
        },
        "input": "sent0: The extracted date is only considered correct if it completely matches the gold date.\n sent1: For less than 4% of users, we have multiple dates for the same event (e.g., a user had a mastectomy twice).\n sent2: Similar to the evaluation metric in a previous study , in these cases, we give the system the benefit of the doubt and the extracted date is considered correct if it matches one of the gold dates.\n sent3: In previous work #TARGET_REF; , the evaluation metric score is defined as 1/((1 + |d|)) where d is the difference between the values in years.\n sent4: We choose a much stricter evaluation metric because we need a precise event date to study user behavior changes.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"DIFFERENCES\"], \"context\": [\"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "This paper presents a rule-based TE extractor that identifies and resolves a higher percentage of nonstandard TEs than earlier state-of-art temporal taggers.",
                "Our task is closest to the temporal slot filling track in the TAC-KBP 2011 shared task and timelining task #TARGET_REF .",
                "Their goal was to extract the temporal bounds of event relations.",
                "Our task has two key differences.",
                "First, they used newswire, Wikipedia and blogs as data sources from which they extract temporal bounds of facts found in Wikipedia infoboxes."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: This paper presents a rule-based TE extractor that identifies and resolves a higher percentage of nonstandard TEs than earlier state-of-art temporal taggers.\n sent1: Our task is closest to the temporal slot filling track in the TAC-KBP 2011 shared task and timelining task #TARGET_REF .\n sent2: Their goal was to extract the temporal bounds of event relations.\n sent3: Our task has two key differences.\n sent4: First, they used newswire, Wikipedia and blogs as data sources from which they extract temporal bounds of facts found in Wikipedia infoboxes.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We train a MaxEnt classifier to predict the temporal relationship between the retrieved TE and the event date as overlap or no-overlap, similar to the within-sentence event-time anchoring task in TempEval-2 (#REF).",
                "Features for the classifier include many of those in #TARGET_REF; #REF ): namely, event keyword and its dominant verb, verb and preposition that dominate TE, dependency path between TE and keyword and its length, unigram and bigram word and POS features.",
                "New features include the Event-Subject, Negative and Modality features.",
                "In online support groups, users not only tell stories about themselves, they also share other patients' stories (as shown in Figure 1 ).",
                "So we add subject features to remove this kind of noise, which includes the governing subject of the event keyword and its POS tag."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: We train a MaxEnt classifier to predict the temporal relationship between the retrieved TE and the event date as overlap or no-overlap, similar to the within-sentence event-time anchoring task in TempEval-2 (#REF).\n sent1: Features for the classifier include many of those in #TARGET_REF; #REF ): namely, event keyword and its dominant verb, verb and preposition that dominate TE, dependency path between TE and keyword and its length, unigram and bigram word and POS features.\n sent2: New features include the Event-Subject, Negative and Modality features.\n sent3: In online support groups, users not only tell stories about themselves, they also share other patients' stories (as shown in Figure 1 ).\n sent4: So we add subject features to remove this kind of noise, which includes the governing subject of the event keyword and its POS tag.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "A recent survey by IBM 3 suggests that more than 2.5 quintillion bytes of data are produced on the Web every day.",
                "Entity Linking (EL), also known as Named Entity Disambiguation (NED), is one of the most important Natural Language Processing (NLP) techniques for extracting knowledge automatically from this huge amount of data.",
                "The goal of an EL approach is as follows: Given a piece of text, a reference knowledge base K and a set of entity mentions in that text, map each entity mention to the corresponding resource in K #TARGET_REF .",
                "A large number of challenges has to be addressed while performing a disambiguation.",
                "For instance, a given resource can be referred to using different labels due to phenomena such as synonymy, acronyms or typos."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: A recent survey by IBM 3 suggests that more than 2.5 quintillion bytes of data are produced on the Web every day.\n sent1: Entity Linking (EL), also known as Named Entity Disambiguation (NED), is one of the most important Natural Language Processing (NLP) techniques for extracting knowledge automatically from this huge amount of data.\n sent2: The goal of an EL approach is as follows: Given a piece of text, a reference knowledge base K and a set of entity mentions in that text, map each entity mention to the corresponding resource in K #TARGET_REF .\n sent3: A large number of challenges has to be addressed while performing a disambiguation.\n sent4: For instance, a given resource can be referred to using different labels due to phenomena such as synonymy, acronyms or typos.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "During the online phase, the EL is carried out in two steps: 1) candidate generation and 2) disambiguation.",
                "The goal of the candidate generation step is to retrieve a tractable number of candidates for each mention.",
                "These candidates are later inserted into the disambiguation graph, which is used to determine the mapping between entities and mentions.",
                "MAG implements two graph-based algorithms to disambiguate entities, i.e., PageRank and HITS.",
                "Independently of the chosen graph algorithm, the highest candidate score among the set of candidates is chosen as correct disambiguation for a given mention #TARGET_REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: During the online phase, the EL is carried out in two steps: 1) candidate generation and 2) disambiguation.\n sent1: The goal of the candidate generation step is to retrieve a tractable number of candidates for each mention.\n sent2: These candidates are later inserted into the disambiguation graph, which is used to determine the mapping between entities and mentions.\n sent3: MAG implements two graph-based algorithms to disambiguate entities, i.e., PageRank and HITS.\n sent4: Independently of the chosen graph algorithm, the highest candidate score among the set of candidates is chosen as correct disambiguation for a given mention #TARGET_REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "It allows MAG to use either the Page Rank or the frequency of a candidate to sort while candidate retrieval.",
                "-Graph-based algorithm -The user can choose which graph-based algorithm to use for disambiguating among the candidates per mentions.",
                "The current implementation offers HITS and PageRank as algorithms, algorithm=hits or algorithm =pagerank.",
                "-Search by Context -This boolean parameter provides a search of candidates using a context index #TARGET_REF .",
                "-Acronyms -This parameter enables a search by acronyms."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: It allows MAG to use either the Page Rank or the frequency of a candidate to sort while candidate retrieval.\n sent1: -Graph-based algorithm -The user can choose which graph-based algorithm to use for disambiguating among the candidates per mentions.\n sent2: The current implementation offers HITS and PageRank as algorithms, algorithm=hits or algorithm =pagerank.\n sent3: -Search by Context -This boolean parameter provides a search of candidates using a context index #TARGET_REF .\n sent4: -Acronyms -This parameter enables a search by acronyms.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Despite the complexity of the task, EL approaches have recently achieved increasingly better results by relying on trained machine learning models [6] .",
                "A portion of these approaches claim to be multilingual and most of them rely on models which are trained on English corpora with cross-lingual dictionaries.",
                "However, MAG (Multilingual AGDISTIS) #TARGET_REF showed that the underlying models being trained on English corpora make them prone to failure when migrated to a different language.",
                "Additionally, these approaches hardly make their models or data available on more than three languages [6] .",
                "The new version of MAG (which is the quintessence of this demo) provides support for 40 different languages using sophisticated indices 4 ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Despite the complexity of the task, EL approaches have recently achieved increasingly better results by relying on trained machine learning models [6] .\n sent1: A portion of these approaches claim to be multilingual and most of them rely on models which are trained on English corpora with cross-lingual dictionaries.\n sent2: However, MAG (Multilingual AGDISTIS) #TARGET_REF showed that the underlying models being trained on English corpora make them prone to failure when migrated to a different language.\n sent3: Additionally, these approaches hardly make their models or data available on more than three languages [6] .\n sent4: The new version of MAG (which is the quintessence of this demo) provides support for 40 different languages using sophisticated indices 4 .\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The ultimate goal of \"grounded\" language learning is to develop computational systems that can acquire language more like a human child.",
                "Given only supervision in the form of sentences paired with relevant but ambiguous perceptual contexts, a system should learn to interpret and/or generate language describing situations and events in the world.",
                "For example, systems have learned to commentate simulated robot soccer games by learning from sample sportscasts (#REF; #REF; #TARGET_REF , or understand navigation instructions by learning from action traces produced when following the directions (#REF; #REF) .",
                "Börschinger et al. (2011) recently introduced an approach to grounded language learning using unsupervised induction of probabilistic context free grammars (PCFGs) to learn from ambiguous contextual supervision.",
                "Their approach first constructs a large set of production rules from sentences paired with descriptions of their ambiguous context, and then trains the parameters of this grammar using EM."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The ultimate goal of \"grounded\" language learning is to develop computational systems that can acquire language more like a human child.\n sent1: Given only supervision in the form of sentences paired with relevant but ambiguous perceptual contexts, a system should learn to interpret and/or generate language describing situations and events in the world.\n sent2: For example, systems have learned to commentate simulated robot soccer games by learning from sample sportscasts (#REF; #REF; #TARGET_REF , or understand navigation instructions by learning from action traces produced when following the directions (#REF; #REF) .\n sent3: Börschinger et al. (2011) recently introduced an approach to grounded language learning using unsupervised induction of probabilistic context free grammars (PCFGs) to learn from ambiguous contextual supervision.\n sent4: Their approach first constructs a large set of production rules from sentences paired with descriptions of their ambiguous context, and then trains the parameters of this grammar using EM.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Several recent approaches have investigated grounded learning from ambiguous supervision extracted from perceptual context.",
                "A number of approaches (#REF; #REF; #REF; #TARGET_REF assume training data consisting of a set of sentences each associated with a small set of MRs, one of which is usually the correct meaning of the sentence.",
                "Many of these approaches (#REF; #REF; #REF) disambiguate the data and match NL sentences to their correct MR by iteratively retraining a supervised semantic parser.",
                "proposed a generative semantic parsing model that first chooses which MRs to describe and then generates a hybrid tree structure (#REF) containing both the MR and NL sentence.",
                "They train this model on ambiguous data using EM."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Several recent approaches have investigated grounded learning from ambiguous supervision extracted from perceptual context.\n sent1: A number of approaches (#REF; #REF; #REF; #TARGET_REF assume training data consisting of a set of sentences each associated with a small set of MRs, one of which is usually the correct meaning of the sentence.\n sent2: Many of these approaches (#REF; #REF; #REF) disambiguate the data and match NL sentences to their correct MR by iteratively retraining a supervised semantic parser.\n sent3: proposed a generative semantic parsing model that first chooses which MRs to describe and then generates a hybrid tree structure (#REF) containing both the MR and NL sentence.\n sent4: They train this model on ambiguous data using EM.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Many of these approaches (#REF; #REF; #REF) disambiguate the data and match NL sentences to their correct MR by iteratively retraining a supervised semantic parser.",
                "proposed a generative semantic parsing model that first chooses which MRs to describe and then generates a hybrid tree structure (#REF) containing both the MR and NL sentence.",
                "They train this model on ambiguous data using EM.",
                "As previously discussed, #TARGET_REF use a PCFG generative model and also train it on ambiguous data using EM.",
                "#REF assume each sentence maps to one or more semantic records (i.e. MRs) and trains a hierarchical semi-Markov generative model using EM, and then finds a Viterbi alignment between NL words and records and their constituents."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Many of these approaches (#REF; #REF; #REF) disambiguate the data and match NL sentences to their correct MR by iteratively retraining a supervised semantic parser.\n sent1: proposed a generative semantic parsing model that first chooses which MRs to describe and then generates a hybrid tree structure (#REF) containing both the MR and NL sentence.\n sent2: They train this model on ambiguous data using EM.\n sent3: As previously discussed, #TARGET_REF use a PCFG generative model and also train it on ambiguous data using EM.\n sent4: #REF assume each sentence maps to one or more semantic records (i.e. MRs) and trains a hierarchical semi-Markov generative model using EM, and then finds a Viterbi alignment between NL words and records and their constituents.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Our approach extends that of #TARGET_REF , which in turn was inspired by a series of previous techniques (#REF; #REF; following the idea of constructing correspondences between NL and MR in a single probabilistic generative framework.",
                "Particularly, their approach automatically constructs a PCFG that generates NL sentences from MRs, which indicates how atomic MR constituents are probabilistically related to NL words.",
                "The nonterminals in the grammar correspond to complete MRs, MR constituents, and NL phrases.",
                "The nonterminal for a composite MR generates each of its MR constituents, and each atomic MR, x, generates an NL phrase, P hrase x .",
                "Each P hrase x then generates a sequence of W ord x 's for describing x, and each W ord x can generate each possible word in the natural language."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: Our approach extends that of #TARGET_REF , which in turn was inspired by a series of previous techniques (#REF; #REF; following the idea of constructing correspondences between NL and MR in a single probabilistic generative framework.\n sent1: Particularly, their approach automatically constructs a PCFG that generates NL sentences from MRs, which indicates how atomic MR constituents are probabilistically related to NL words.\n sent2: The nonterminals in the grammar correspond to complete MRs, MR constituents, and NL phrases.\n sent3: The nonterminal for a composite MR generates each of its MR constituents, and each atomic MR, x, generates an NL phrase, P hrase x .\n sent4: Each P hrase x then generates a sequence of W ord x 's for describing x, and each W ord x can generate each possible word in the natural language.\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "The next step composes PCFG rules from the LHGs and is summarized in Figure 6 .",
                "We basically follow the scheme of #TARGET_REF , but instead of generating NL words from each atomic MR, words are generated from each lexeme MR, Figure 6 : Summary of the rule generation process.",
                "NLs refer to the set of NL words in the corpus.",
                "Lexeme rules come from the schemata of Börschinger et al. (2011) , and allow every lexeme MR to generate one or more NL words.",
                "Note that pseudo-lexeme nodes do not produce NL words."
            ],
            "label": [
                "EXTENDS",
                "DIFFERENCES"
            ]
        },
        "input": "sent0: The next step composes PCFG rules from the LHGs and is summarized in Figure 6 .\n sent1: We basically follow the scheme of #TARGET_REF , but instead of generating NL words from each atomic MR, words are generated from each lexeme MR, Figure 6 : Summary of the rule generation process.\n sent2: NLs refer to the set of NL words in the corpus.\n sent3: Lexeme rules come from the schemata of Börschinger et al. (2011) , and allow every lexeme MR to generate one or more NL words.\n sent4: Note that pseudo-lexeme nodes do not produce NL words.\n",
        "output": "{\"label\": [\"EXTENDS\", \"DIFFERENCES\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Our approach improves on #TARGET_REF 's method in the following ways:",
                "• The building blocks for associating NL and MR are semantic lexemes instead of atomic MR constituents.",
                "This prevents the number of constructed PCFG rules from becoming intractably large as happens with Börschinger et al.'s approach.",
                "As previously mentioned, lexeme MRs are intuitively analogous to syntactic categories in that complex lexeme MRs represent complicated semantic concepts whereas higher-level syntactic categories such as S, VP, or NP represent complex syntactic structures.",
                "• Our approach has the ability to produce previously unseen MRs, whereas Börschinger et al. can only generate an MR if it is explicitly included in the PCFG rules constructed from the training data."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: Our approach improves on #TARGET_REF 's method in the following ways:\n sent1: • The building blocks for associating NL and MR are semantic lexemes instead of atomic MR constituents.\n sent2: This prevents the number of constructed PCFG rules from becoming intractably large as happens with Börschinger et al.'s approach.\n sent3: As previously mentioned, lexeme MRs are intuitively analogous to syntactic categories in that complex lexeme MRs represent complicated semantic concepts whereas higher-level syntactic categories such as S, VP, or NP represent complex syntactic structures.\n sent4: • Our approach has the ability to produce previously unseen MRs, whereas Börschinger et al. can only generate an MR if it is explicitly included in the PCFG rules constructed from the training data.\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "We have presented a novel method for learning a semantic parser given only highly ambiguous supervision.",
                "Our model enhances #TARGET_REF 's approach to reducing the problem of grounded learning of semantic parsers to PCFG induction.",
                "We use a learned semantic lexicon to aid the construction of a smaller and more focused set of PCFG productions.",
                "This allows the approach to scale to complex MR languages that define a large (potentially infinite) space of representations for capturing the meaning of sentences.",
                "By contrast, the previous PCFG approach requires a finite MR language and its grammar grows intractably large for even moderately complex MR languages."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: We have presented a novel method for learning a semantic parser given only highly ambiguous supervision.\n sent1: Our model enhances #TARGET_REF 's approach to reducing the problem of grounded learning of semantic parsers to PCFG induction.\n sent2: We use a learned semantic lexicon to aid the construction of a smaller and more focused set of PCFG productions.\n sent3: This allows the approach to scale to complex MR languages that define a large (potentially infinite) space of representations for capturing the meaning of sentences.\n sent4: By contrast, the previous PCFG approach requires a finite MR language and its grammar grows intractably large for even moderately complex MR languages.\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Figure 4 illustrates the overall system.",
                "As this figure indicates, our new PCFG method replaces the plan refinement and semantic parser components in their system with a unified model that both disambiguates the training data and learns a semantic parser.",
                "We use the landmarks plans and the learned lexicon produced by #REF as inputs to our system.",
                "2 Like #TARGET_REF , our approach learns a semantic parser directly from ambiguous supervision, specifically NL instructions paired with their complete landmarks plans as context.",
                "Our method incorporates the semantic lexemes as building blocks to find correspondences between NL words and semantic concepts represented by the lexeme MRs, instead of building connections between NL words and every possible MR constituent as in Börschinger et al.'s approach."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: Figure 4 illustrates the overall system.\n sent1: As this figure indicates, our new PCFG method replaces the plan refinement and semantic parser components in their system with a unified model that both disambiguates the training data and learns a semantic parser.\n sent2: We use the landmarks plans and the learned lexicon produced by #REF as inputs to our system.\n sent3: 2 Like #TARGET_REF , our approach learns a semantic parser directly from ambiguous supervision, specifically NL instructions paired with their complete landmarks plans as context.\n sent4: Our method incorporates the semantic lexemes as building blocks to find correspondences between NL words and semantic concepts represented by the lexeme MRs, instead of building connections between NL words and every possible MR constituent as in Börschinger et al.'s approach.\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Figure 7 graphically depicts a sample trace of this algorithm.",
                "The algorithm recursively traverses the parse tree.",
                "When a leaf-node is reached, it marks all of the nodes in its MR.",
                "After traversing all of its children, 5 We used the implementation available at http://web.",
                "science.mq.edu.au/˜mjohnson/Software.htm which was also used by #TARGET_REF ."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: Figure 7 graphically depicts a sample trace of this algorithm.\n sent1: The algorithm recursively traverses the parse tree.\n sent2: When a leaf-node is reached, it marks all of the nodes in its MR.\n sent3: After traversing all of its children, 5 We used the implementation available at http://web.\n sent4: science.mq.edu.au/˜mjohnson/Software.htm which was also used by #TARGET_REF .\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "The next step composes PCFG rules from the LHGs and is summarized in Figure 6 .",
                "We basically follow the scheme of Börschinger et al. (2011) , but instead of generating NL words from each atomic MR, words are generated from each lexeme MR, Figure 6 : Summary of the rule generation process.",
                "NLs refer to the set of NL words in the corpus.",
                "Lexeme rules come from the schemata of #TARGET_REF , and allow every lexeme MR to generate one or more NL words.",
                "Note that pseudo-lexeme nodes do not produce NL words."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The next step composes PCFG rules from the LHGs and is summarized in Figure 6 .\n sent1: We basically follow the scheme of Börschinger et al. (2011) , but instead of generating NL words from each atomic MR, words are generated from each lexeme MR, Figure 6 : Summary of the rule generation process.\n sent2: NLs refer to the set of NL words in the corpus.\n sent3: Lexeme rules come from the schemata of #TARGET_REF , and allow every lexeme MR to generate one or more NL words.\n sent4: Note that pseudo-lexeme nodes do not produce NL words.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "For the last ten years, many methods have been proposed for the segmentation of texts in topically related units on the basis of lexical cohesion.",
                "The major distinction between these methods is in the contrast between the approaches based exclusively on the information contained in the text to be segmented, such as lexical repetition (e.g., #TARGET_REF; #REF; #REF; Kehagias, Pavlina, and #REF; #REF) , and those approaches that rest on complementary semantic knowledge extracted from dictionaries and thesauruses (e.g., #REF; #REF; #REF) , or from collocations collected in large corpora (#REF; Brants, Chen, and #REF; #REF; #REF; #REF; #REF) .",
                "According to their authors, methods that use additional knowledge allow for a solution to problems encountered when sentences belonging to a unique topic do not share common words due to the use of hyperonyms or synonyms and allow words that are semantically related to be taken as positive evidence for topic continuity.",
                "Empirical arguments in favor of these methods have been provided recently by #REF in a study using Latent Semantic Analysis (Latent Semantic Indexing, #REF ) to extract a semantic space from a corpus allowing determination of the similarity of meanings of words, sentences, or paragraphs.",
                "By comparing the accuracy of the very same algorithm according to whether or not it takes into account complementary semantic knowledge, they were able to show the benefit derived from such knowledge."
            ],
            "label": [
                "BACKGROUND",
                "DIFFERENCES"
            ]
        },
        "input": "sent0: For the last ten years, many methods have been proposed for the segmentation of texts in topically related units on the basis of lexical cohesion.\n sent1: The major distinction between these methods is in the contrast between the approaches based exclusively on the information contained in the text to be segmented, such as lexical repetition (e.g., #TARGET_REF; #REF; #REF; Kehagias, Pavlina, and #REF; #REF) , and those approaches that rest on complementary semantic knowledge extracted from dictionaries and thesauruses (e.g., #REF; #REF; #REF) , or from collocations collected in large corpora (#REF; Brants, Chen, and #REF; #REF; #REF; #REF; #REF) .\n sent2: According to their authors, methods that use additional knowledge allow for a solution to problems encountered when sentences belonging to a unique topic do not share common words due to the use of hyperonyms or synonyms and allow words that are semantically related to be taken as positive evidence for topic continuity.\n sent3: Empirical arguments in favor of these methods have been provided recently by #REF in a study using Latent Semantic Analysis (Latent Semantic Indexing, #REF ) to extract a semantic space from a corpus allowing determination of the similarity of meanings of words, sentences, or paragraphs.\n sent4: By comparing the accuracy of the very same algorithm according to whether or not it takes into account complementary semantic knowledge, they were able to show the benefit derived from such knowledge.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"DIFFERENCES\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The present study shows, however, that the presence of the test materials in the LSA corpus has an important effect, but also that the generic semantic knowledge derived from large corpora clearly improves the segmentation accuracy.",
                "This conclusion is drawn from two experiments in which the presence or absence of the test materials in the LSA corpus is manipulated.",
                "The first experiment is based on the original materials from Choi et al., which consisted of a small corpus (1,000,000 words).",
                "The second experiment is based on a much larger corpus (25,000,000 words).",
                "Before reporting these experiments, #TARGET_REF and the use of LSA within this framework are described."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The present study shows, however, that the presence of the test materials in the LSA corpus has an important effect, but also that the generic semantic knowledge derived from large corpora clearly improves the segmentation accuracy.\n sent1: This conclusion is drawn from two experiments in which the presence or absence of the test materials in the LSA corpus is manipulated.\n sent2: The first experiment is based on the original materials from Choi et al., which consisted of a small corpus (1,000,000 words).\n sent3: The second experiment is based on a much larger corpus (25,000,000 words).\n sent4: Before reporting these experiments, #TARGET_REF and the use of LSA within this framework are described.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "The segmentation algorithm proposed by #TARGET_REF is made up of the three steps usually found in any segmentation procedure based on lexical cohesion.",
                "Firstly, the document to be segmented is divided into minimal textual units, usually sentences.",
                "Then, a similarity index between every pair of adjacent units is calculated.",
                "Each raw similarity value is cast on an ordinal scale by taking the proportion of neighboring values that are smaller than it.",
                "Lastly, the document is segmented recursively according to the boundaries between the units that maximize the sum of the average similarities inside the segments thus comprised (divisive clustering)."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The segmentation algorithm proposed by #TARGET_REF is made up of the three steps usually found in any segmentation procedure based on lexical cohesion.\n sent1: Firstly, the document to be segmented is divided into minimal textual units, usually sentences.\n sent2: Then, a similarity index between every pair of adjacent units is calculated.\n sent3: Each raw similarity value is cast on an ordinal scale by taking the proportion of neighboring values that are smaller than it.\n sent4: Lastly, the document is segmented recursively according to the boundaries between the units that maximize the sum of the average similarities inside the segments thus comprised (divisive clustering).\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Each raw similarity value is cast on an ordinal scale by taking the proportion of neighboring values that are smaller than it.",
                "Lastly, the document is segmented recursively according to the boundaries between the units that maximize the sum of the average similarities inside the segments thus comprised (divisive clustering).",
                "The step of greatest interest here is the one that calculates the inter-sentence similarities.",
                "The procedure initially proposed by #TARGET_REF , C99, rests exclusively on the information contained in the text to be segmented.",
                "According to the vector space model, each sentence is represented by a vector of word frequency count, and the similarity between two sentences is calculated by means of the cosine measure between the corresponding vectors."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Each raw similarity value is cast on an ordinal scale by taking the proportion of neighboring values that are smaller than it.\n sent1: Lastly, the document is segmented recursively according to the boundaries between the units that maximize the sum of the average similarities inside the segments thus comprised (divisive clustering).\n sent2: The step of greatest interest here is the one that calculates the inter-sentence similarities.\n sent3: The procedure initially proposed by #TARGET_REF , C99, rests exclusively on the information contained in the text to be segmented.\n sent4: According to the vector space model, each sentence is represented by a vector of word frequency count, and the similarity between two sentences is calculated by means of the cosine measure between the corresponding vectors.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "According to the vector space model, each sentence is represented by a vector of word frequency count, and the similarity between two sentences is calculated by means of the cosine measure between the corresponding vectors.",
                "In a first evaluation based on the procedure described below, #TARGET_REF showed that its algorithm outperforms several other approaches such as TextTiling (#REF) and Segmenter (Kan, Klavans, and #REF) .",
                "#REF claimed that it was possible to improve the inter-sentence similarities index by taking into account the semantic proximities between words estimated on the basis of Latent Semantic Analysis (LSA).",
                "Briefly stated, LSA rests on the thesis that analyzing the contexts in which words occur permits an estimation of their similarity in meaning (#REF; #REF) .",
                "The first step in the analysis is to construct a lexical table containing an information-theoretic weighting of the frequencies of the words occurrence in each document (i.e. sentence, paragraph, or text) included in the corpus."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: According to the vector space model, each sentence is represented by a vector of word frequency count, and the similarity between two sentences is calculated by means of the cosine measure between the corresponding vectors.\n sent1: In a first evaluation based on the procedure described below, #TARGET_REF showed that its algorithm outperforms several other approaches such as TextTiling (#REF) and Segmenter (Kan, Klavans, and #REF) .\n sent2: #REF claimed that it was possible to improve the inter-sentence similarities index by taking into account the semantic proximities between words estimated on the basis of Latent Semantic Analysis (LSA).\n sent3: Briefly stated, LSA rests on the thesis that analyzing the contexts in which words occur permits an estimation of their similarity in meaning (#REF; #REF) .\n sent4: The first step in the analysis is to construct a lexical table containing an information-theoretic weighting of the frequencies of the words occurrence in each document (i.e. sentence, paragraph, or text) included in the corpus.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "This experiment was based on the procedure and test materials designed by #TARGET_REF , which was also used by several authors as a benchmark for comparing segmentation systems (#REF; #REF; #REF; #REF) .",
                "The task consists in finding the boundaries between concatenated texts.",
                "Each test sample is a concatenation of ten text segments.",
                "Each segment consisted in the first n sentences of a randomly selected text from two sub-sections of the Brown corpus.",
                "For the present experiment, I used the most general test materials built by #REF , in which the size of the segments within each sample varies randomly from 3 to 11 sentences."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This experiment was based on the procedure and test materials designed by #TARGET_REF , which was also used by several authors as a benchmark for comparing segmentation systems (#REF; #REF; #REF; #REF) .\n sent1: The task consists in finding the boundaries between concatenated texts.\n sent2: Each test sample is a concatenation of ten text segments.\n sent3: Each segment consisted in the first n sentences of a randomly selected text from two sub-sections of the Brown corpus.\n sent4: For the present experiment, I used the most general test materials built by #REF , in which the size of the segments within each sample varies randomly from 3 to 11 sentences.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Each test sample is a concatenation of ten text segments.",
                "Each segment consisted in the first n sentences of a randomly selected text from two sub-sections of the Brown corpus.",
                "For the present experiment, I used the most general test materials built by #TARGET_REF , in which the size of the segments within each sample varies randomly from 3 to 11 sentences.",
                "It is composed of 400 samples.",
                "The analysis related to the comparison between the accuracy of the algorithm when the test materials were included in the LSA corpus (Within) and when it was not (Without)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Each test sample is a concatenation of ten text segments.\n sent1: Each segment consisted in the first n sentences of a randomly selected text from two sub-sections of the Brown corpus.\n sent2: For the present experiment, I used the most general test materials built by #TARGET_REF , in which the size of the segments within each sample varies randomly from 3 to 11 sentences.\n sent3: It is composed of 400 samples.\n sent4: The analysis related to the comparison between the accuracy of the algorithm when the test materials were included in the LSA corpus (Within) and when it was not (Without).\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The words on Choi's stoplist were removed, as were those that appeared only once in the whole corpus.",
                "Words were not stemmed, as in #REF .",
                "To build the LSA space, the singular value decomposition was realized using the program SVDPACKC (#REF; #REF) , and the first 300 singular vectors were retained.",
                "Concerning the segmentation algorithm, I used the version in which the number of boundaries to be found is imposed, and thus fixed at nine.",
                "An 11 × 11 rank mask was used for the ordinal transformation, as recommended by #TARGET_REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The words on Choi's stoplist were removed, as were those that appeared only once in the whole corpus.\n sent1: Words were not stemmed, as in #REF .\n sent2: To build the LSA space, the singular value decomposition was realized using the program SVDPACKC (#REF; #REF) , and the first 300 singular vectors were retained.\n sent3: Concerning the segmentation algorithm, I used the version in which the number of boundaries to be found is imposed, and thus fixed at nine.\n sent4: An 11 × 11 rank mask was used for the ordinal transformation, as recommended by #TARGET_REF .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "The test materials were extracted from the 1997-1998 corpus following the guidelines given in #TARGET_REF .",
                "It is composed of 400 samples of ten segments, of which the length varies randomly from 3 to 11 sentences.",
                "Three types of LSA space were composed.",
                "The Within space is based on the whole 1997-1998 corpus.",
                "Four hundred different Without spaces were built as described in Experiment 1."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The test materials were extracted from the 1997-1998 corpus following the guidelines given in #TARGET_REF .\n sent1: It is composed of 400 samples of ten segments, of which the length varies randomly from 3 to 11 sentences.\n sent2: Three types of LSA space were composed.\n sent3: The Within space is based on the whole 1997-1998 corpus.\n sent4: Four hundred different Without spaces were built as described in Experiment 1.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Several research work have been reported since 2010 in this research field of hate speech detection (#REF; #REF; #REF; #TARGET_REF; #REF; #REF; #REF; #REF; Gambäck and #REF; #REF; #REF) .",
                "#REF & #REF reviewed the approaches used for hate speech detection.",
                "#REF used bag of words and bi-gram features with machine learning approach to classify the tweets as \"racist\" or \"nonracist\".",
                "#REF developed a supervised algorithm for hateful and antagonistic content in Twitter using voted ensemble meta-classifier.",
                "#REF learnt distributed low-dimensional representations of social media comments using neural language models for hate speech detection."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Several research work have been reported since 2010 in this research field of hate speech detection (#REF; #REF; #REF; #TARGET_REF; #REF; #REF; #REF; #REF; Gambäck and #REF; #REF; #REF) .\n sent1: #REF & #REF reviewed the approaches used for hate speech detection.\n sent2: #REF used bag of words and bi-gram features with machine learning approach to classify the tweets as \"racist\" or \"nonracist\".\n sent3: #REF developed a supervised algorithm for hateful and antagonistic content in Twitter using voted ensemble meta-classifier.\n sent4: #REF learnt distributed low-dimensional representations of social media comments using neural language models for hate speech detection.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "We have employed 2 layered bi-directional LSTM with Scaled Luong and Normed Bahdanau attention mechanisms to build the model for all the three sub tasks.",
                "The instances are vectorized using TF-IDF score for traditional machine learning models with minimum count two.",
                "The classifiers namely Multinomial Naive Bayes and Support Vector Machine with Stochastic Gradient Descent optimizer were employed to build the models for sub tasks B and C. Deep learning with Scaled Luong attention, deep learning with Normed Bahdanau attention, traditional machine learning with SVM give better results for Task A, Task B and Task C respectively.",
                "Our models outperform the base line for all the three tasks.",
                "The performance may be improved further by incorporating external datasets (#REFa; #TARGET_REF"
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: We have employed 2 layered bi-directional LSTM with Scaled Luong and Normed Bahdanau attention mechanisms to build the model for all the three sub tasks.\n sent1: The instances are vectorized using TF-IDF score for traditional machine learning models with minimum count two.\n sent2: The classifiers namely Multinomial Naive Bayes and Support Vector Machine with Stochastic Gradient Descent optimizer were employed to build the models for sub tasks B and C. Deep learning with Scaled Luong attention, deep learning with Normed Bahdanau attention, traditional machine learning with SVM give better results for Task A, Task B and Task C respectively.\n sent3: Our models outperform the base line for all the three tasks.\n sent4: The performance may be improved further by incorporating external datasets (#REFa; #TARGET_REF\n",
        "output": "{\"label\": [null], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Lexical simplification is the task of automatically rewriting a text by substituting words or phrases with simpler variants, while retaining its meaning and grammaticality.",
                "The goal is to make the text easier to understand for children, language learners, people with cognitive disabilities and even machines.",
                "Approaches to lexical simplification generally follow a standard pipeline consisting of two main steps: generation and ranking.",
                "In the generation step, a set of possible substitutions for the target word is commonly created by querying semantic databases such as Wordnet (#REF) , learning substitution rules from sentence-aligned parallel corpora of complex-simple texts #TARGET_REF; #REF) , and learning word embeddings from a large corpora to obtain similar words of the complex word (Glavaš andŠtajner, 2015; #REF; #REFa, 2017) .",
                "In the ranking step, features that discriminate a substitution candidate from other substitution candidates are leveraged and the candidates are ranked with respect to their simplicity and contextual fitness."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Lexical simplification is the task of automatically rewriting a text by substituting words or phrases with simpler variants, while retaining its meaning and grammaticality.\n sent1: The goal is to make the text easier to understand for children, language learners, people with cognitive disabilities and even machines.\n sent2: Approaches to lexical simplification generally follow a standard pipeline consisting of two main steps: generation and ranking.\n sent3: In the generation step, a set of possible substitutions for the target word is commonly created by querying semantic databases such as Wordnet (#REF) , learning substitution rules from sentence-aligned parallel corpora of complex-simple texts #TARGET_REF; #REF) , and learning word embeddings from a large corpora to obtain similar words of the complex word (Glavaš andŠtajner, 2015; #REF; #REFa, 2017) .\n sent4: In the ranking step, features that discriminate a substitution candidate from other substitution candidates are leveraged and the candidates are ranked with respect to their simplicity and contextual fitness.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "State-of-the-art approaches to ranking in lexical simplification exploit supervised machine learning-based methods that rely mostly on surface features, such as word frequency, word length and n-gram probability, for training the model #TARGET_REF; Bingel and Søgaard, 2016; #REFa, 2017) .",
                "Moreover, deep architectures are not explored in these models.",
                "Surface features and shallow architectures might not be able to explore the features at different levels of abstractions that capture nuances that discriminate the candidates.",
                "In this paper, we propose to use a Deep Structured Similarity Model (DSSM) (#REF) to rank substitution candidates.",
                "The DSSM exploits a deep architecture by using a deep neural network (DNN), that can effectively capture contextual features to perform semantic matching between two sentences."
            ],
            "label": [
                "BACKGROUND",
                "MOTIVATION"
            ]
        },
        "input": "sent0: State-of-the-art approaches to ranking in lexical simplification exploit supervised machine learning-based methods that rely mostly on surface features, such as word frequency, word length and n-gram probability, for training the model #TARGET_REF; Bingel and Søgaard, 2016; #REFa, 2017) .\n sent1: Moreover, deep architectures are not explored in these models.\n sent2: Surface features and shallow architectures might not be able to explore the features at different levels of abstractions that capture nuances that discriminate the candidates.\n sent3: In this paper, we propose to use a Deep Structured Similarity Model (DSSM) (#REF) to rank substitution candidates.\n sent4: The DSSM exploits a deep architecture by using a deep neural network (DNN), that can effectively capture contextual features to perform semantic matching between two sentences.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"MOTIVATION\"], \"context\": [\"sent0\", \"sent1\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Following previous works that used supervised machine learning for ranking in lexical simplification #TARGET_REF; #REF) , we train the DSSM using the LexMTurk dataset #TARGET_REF , which contains 500 instances composed of a sentence, a target word and substitution candidates ranked by simplicity (#REF) .",
                "In order to learn the parameters W t and W s (Figure 1 ) of the DSSM, we use the standard backpropagation algorithm (#REF) .",
                "The objective used in this paper follows the pair-wise learning-to-rank paradigm outlined in (#REF) .",
                "Given a target word and its sentential context T , we obtain a list of candidates L. We set different positive values to the candidates based on their simplicity rankings.",
                "E.g., if the list of the candidates is ordered by simplificity as, L = {A + > B + > C + }, the labels are first constructed as L = {y A + = 3, y B + = 2, y C + = 1}. The values are then normalized by dividing by the maximum value in the list: L = {y A + = 1, y B + = 0.667, y C + = 0.333}. If the target word was not originally in L, we add it with label 0."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Following previous works that used supervised machine learning for ranking in lexical simplification #TARGET_REF; #REF) , we train the DSSM using the LexMTurk dataset #TARGET_REF , which contains 500 instances composed of a sentence, a target word and substitution candidates ranked by simplicity (#REF) .\n sent1: In order to learn the parameters W t and W s (Figure 1 ) of the DSSM, we use the standard backpropagation algorithm (#REF) .\n sent2: The objective used in this paper follows the pair-wise learning-to-rank paradigm outlined in (#REF) .\n sent3: Given a target word and its sentential context T , we obtain a list of candidates L. We set different positive values to the candidates based on their simplicity rankings.\n sent4: E.g., if the list of the candidates is ordered by simplificity as, L = {A + > B + > C + }, the labels are first constructed as L = {y A + = 3, y B + = 2, y C + = 1}. The values are then normalized by dividing by the maximum value in the list: L = {y A + = 1, y B + = 0.667, y C + = 0.333}. If the target word was not originally in L, we add it with label 0.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "To evaluate the proposed model, we conduct experiments on two common datasets for lexical simplification: BenchLS (#REFb) , which contains 929 instances, and NNSEval (#REFa) , which contains 239 instances.",
                "Each instance is composed of a sentence, a target word, and a set of gold candidates ranked by simplicity (#REF) .",
                "Since both datasets contain instances from the LexMturk dataset #TARGET_REF , which we use for training the DNN, we remove the overlap instances between training and test datasets 1 .",
                "We finally obtain 429 remaining instances in the BenchLS dataset, and 78 instances in the NNEval dataset, which are used in our evaluation.",
                "We adopt the same evaluation metrics featured in Glavaš andŠtajner (2015) and #REF : 1) precision: ratio of correct simplifications out of all the simplifications made by the system; 2) accuracy: ratio of correct simplifications out of all words that should have been simplified; and 3) changed: ratio of target words changed by the system."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: To evaluate the proposed model, we conduct experiments on two common datasets for lexical simplification: BenchLS (#REFb) , which contains 929 instances, and NNSEval (#REFa) , which contains 239 instances.\n sent1: Each instance is composed of a sentence, a target word, and a set of gold candidates ranked by simplicity (#REF) .\n sent2: Since both datasets contain instances from the LexMturk dataset #TARGET_REF , which we use for training the DNN, we remove the overlap instances between training and test datasets 1 .\n sent3: We finally obtain 429 remaining instances in the BenchLS dataset, and 78 instances in the NNEval dataset, which are used in our evaluation.\n sent4: We adopt the same evaluation metrics featured in Glavaš andŠtajner (2015) and #REF : 1) precision: ratio of correct simplifications out of all the simplifications made by the system; 2) accuracy: ratio of correct simplifications out of all words that should have been simplified; and 3) changed: ratio of target words changed by the system.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "To evaluate the proposed model, we conduct experiments on two common datasets for lexical simplification: BenchLS (#REFb) , which contains 929 instances, and NNSEval (#REFa) , which contains 239 instances.",
                "Each instance is composed of a sentence, a target word, and a set of gold candidates ranked by simplicity (#REF) .",
                "Since both datasets contain instances from the LexMturk dataset (#REF) , which we use for training the DNN, we remove the overlap instances between training and test datasets 1 .",
                "We finally obtain 429 remaining instances in the BenchLS dataset, and 78 instances in the NNEval dataset, which are used in our evaluation.",
                "We adopt the same evaluation metrics featured in Glavaš andŠtajner (2015) and #TARGET_REF : 1) precision: ratio of correct simplifications out of all the simplifications made by the system; 2) accuracy: ratio of correct simplifications out of all words that should have been simplified; and 3) changed: ratio of target words changed by the system."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: To evaluate the proposed model, we conduct experiments on two common datasets for lexical simplification: BenchLS (#REFb) , which contains 929 instances, and NNSEval (#REFa) , which contains 239 instances.\n sent1: Each instance is composed of a sentence, a target word, and a set of gold candidates ranked by simplicity (#REF) .\n sent2: Since both datasets contain instances from the LexMturk dataset (#REF) , which we use for training the DNN, we remove the overlap instances between training and test datasets 1 .\n sent3: We finally obtain 429 remaining instances in the BenchLS dataset, and 78 instances in the NNEval dataset, which are used in our evaluation.\n sent4: We adopt the same evaluation metrics featured in Glavaš andŠtajner (2015) and #TARGET_REF : 1) precision: ratio of correct simplifications out of all the simplifications made by the system; 2) accuracy: ratio of correct simplifications out of all words that should have been simplified; and 3) changed: ratio of target words changed by the system.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "n-gram probs.",
                "denotes the n-gram probability features described in #REF , and all denotes all features described in Section 2.3.",
                "All values marked in bold are significantly higher compared to the best baseline, SVM rank , measured by t-test at p-value of 0.05.",
                "with default parameters) for ranking substitution candidates, similar to the method described in #TARGET_REF .",
                "All the three models employ the n-gram probability features extracted from the SubIMDB corpus (#REF) , as described in (#REF) , and are trained using the LexMTurk dataset."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: n-gram probs.\n sent1: denotes the n-gram probability features described in #REF , and all denotes all features described in Section 2.3.\n sent2: All values marked in bold are significantly higher compared to the best baseline, SVM rank , measured by t-test at p-value of 0.05.\n sent3: with default parameters) for ranking substitution candidates, similar to the method described in #TARGET_REF .\n sent4: All the three models employ the n-gram probability features extracted from the SubIMDB corpus (#REF) , as described in (#REF) , and are trained using the LexMTurk dataset.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "One of the important tasks in aspect-based sentiment analysis is aspect and opinion terms extraction which aims to extract aspect terms and opinion terms from opinion texts [1] .",
                "An aspect term is a word or a phrase that describes an entity's attribute or feature that is the target of an opinion.",
                "An opinion term is a word or a phrase that shows subjective emotion toward an attribute or feature of an entity.",
                "For example, in a hotel review \"Tempat tidur di hotel ini tidak bersih\" (The bed in this hotel is not clean), extraction process returns \"Tempat tidur\" (bed) as aspect term and \"tidak bersih\" (not clean) as opinion term.",
                "Aspect and/or opinion terms extraction research has been conducted by #REF and Xu et al. #TARGET_REF that outperformed the best systems in the aspect-based sentiment analysis task on the International Workshop on Semantic Evaluation (SemEval) for aspect and opinion terms extraction."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: One of the important tasks in aspect-based sentiment analysis is aspect and opinion terms extraction which aims to extract aspect terms and opinion terms from opinion texts [1] .\n sent1: An aspect term is a word or a phrase that describes an entity's attribute or feature that is the target of an opinion.\n sent2: An opinion term is a word or a phrase that shows subjective emotion toward an attribute or feature of an entity.\n sent3: For example, in a hotel review \"Tempat tidur di hotel ini tidak bersih\" (The bed in this hotel is not clean), extraction process returns \"Tempat tidur\" (bed) as aspect term and \"tidak bersih\" (not clean) as opinion term.\n sent4: Aspect and/or opinion terms extraction research has been conducted by #REF and Xu et al. #TARGET_REF that outperformed the best systems in the aspect-based sentiment analysis task on the International Workshop on Semantic Evaluation (SemEval) for aspect and opinion terms extraction.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "The model is a multi-layer attention network, where each layer consists of a couple of attentions with tensor xoperators, one attention for aspect term extraction and the other for opinion term extraction.",
                "The model achieved F1-measure of 0.7073 and 0.7368 for aspect and opinion term extraction respectively using #REF task 12 subtask 1 restaurant dataset [4] .",
                "Xu et al. #TARGET_REF proposed a Convolutional Neural Network (CNN) model employing two types of pre-trained word embeddings, general-purpose embeddings and domainspecific embeddings, for aspect term extraction.",
                "The two embeddings are concatenated into one word embedding called double embeddings.",
                "The model achieved F1-measure of 0.7437 for aspect term extraction using #REF task 5 subtask 1 restaurant dataset [5] ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The model is a multi-layer attention network, where each layer consists of a couple of attentions with tensor xoperators, one attention for aspect term extraction and the other for opinion term extraction.\n sent1: The model achieved F1-measure of 0.7073 and 0.7368 for aspect and opinion term extraction respectively using #REF task 12 subtask 1 restaurant dataset [4] .\n sent2: Xu et al. #TARGET_REF proposed a Convolutional Neural Network (CNN) model employing two types of pre-trained word embeddings, general-purpose embeddings and domainspecific embeddings, for aspect term extraction.\n sent3: The two embeddings are concatenated into one word embedding called double embeddings.\n sent4: The model achieved F1-measure of 0.7437 for aspect term extraction using #REF task 5 subtask 1 restaurant dataset [5] .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The model achieved F1-measure of 0.7437 for aspect term extraction using #REF task 5 subtask 1 restaurant dataset [5] .",
                "#REF and Xu et al. #TARGET_REF approaches have not been applied for Indonesian reviews.",
                "This paper aims to perform aspect and opinion terms extraction in Indonesian hotel reviews by adapting CMLA architecture [2] and double embeddings mechanism [3] .",
                "The adaption in this paper is conducted by changing the English resources used in word embedding into Indonesian version.",
                "The rest of this paper is structured as follows."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The model achieved F1-measure of 0.7437 for aspect term extraction using #REF task 5 subtask 1 restaurant dataset [5] .\n sent1: #REF and Xu et al. #TARGET_REF approaches have not been applied for Indonesian reviews.\n sent2: This paper aims to perform aspect and opinion terms extraction in Indonesian hotel reviews by adapting CMLA architecture [2] and double embeddings mechanism [3] .\n sent3: The adaption in this paper is conducted by changing the English resources used in word embedding into Indonesian version.\n sent4: The rest of this paper is structured as follows.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF used attention mechanism [12] to identify the possibility of each token being an aspect or opinion term.",
                "The coupled multilayer attentions that was proposed by [2] models the relations among tokens automatically without any syntactic/dependency parsing or linguistic resources as additional information for the input and achieves good performance for aspect and opinion terms extraction.",
                "The coupled attentions are used to exploit the correlations between aspect and opinion terms using tensor operators [2] .",
                "Xu et al. #TARGET_REF use double embeddings that leverage both general embeddings and domain embeddings as a feature for a CNN model and let the CNN model decide which embeddings have more useful information.",
                "The experiment conducted in [3] demonstrated that double embedding mechanism achieved better performance for aspect terms extraction compared to the use of general embeddings or domain embeddings alone."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #REF used attention mechanism [12] to identify the possibility of each token being an aspect or opinion term.\n sent1: The coupled multilayer attentions that was proposed by [2] models the relations among tokens automatically without any syntactic/dependency parsing or linguistic resources as additional information for the input and achieves good performance for aspect and opinion terms extraction.\n sent2: The coupled attentions are used to exploit the correlations between aspect and opinion terms using tensor operators [2] .\n sent3: Xu et al. #TARGET_REF use double embeddings that leverage both general embeddings and domain embeddings as a feature for a CNN model and let the CNN model decide which embeddings have more useful information.\n sent4: The experiment conducted in [3] demonstrated that double embedding mechanism achieved better performance for aspect terms extraction compared to the use of general embeddings or domain embeddings alone.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF used attention mechanism [12] to identify the possibility of each token being an aspect or opinion term.",
                "The coupled multilayer attentions that was proposed by [2] models the relations among tokens automatically without any syntactic/dependency parsing or linguistic resources as additional information for the input and achieves good performance for aspect and opinion terms extraction.",
                "The coupled attentions are used to exploit the correlations between aspect and opinion terms using tensor operators [2] .",
                "#REF use double embeddings that leverage both general embeddings and domain embeddings as a feature for a CNN model and let the CNN model decide which embeddings have more useful information.",
                "The experiment conducted in #TARGET_REF demonstrated that double embedding mechanism achieved better performance for aspect terms extraction compared to the use of general embeddings or domain embeddings alone."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #REF used attention mechanism [12] to identify the possibility of each token being an aspect or opinion term.\n sent1: The coupled multilayer attentions that was proposed by [2] models the relations among tokens automatically without any syntactic/dependency parsing or linguistic resources as additional information for the input and achieves good performance for aspect and opinion terms extraction.\n sent2: The coupled attentions are used to exploit the correlations between aspect and opinion terms using tensor operators [2] .\n sent3: #REF use double embeddings that leverage both general embeddings and domain embeddings as a feature for a CNN model and let the CNN model decide which embeddings have more useful information.\n sent4: The experiment conducted in #TARGET_REF demonstrated that double embedding mechanism achieved better performance for aspect terms extraction compared to the use of general embeddings or domain embeddings alone.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "The model achieved F1-measure of 0.7437 for aspect term extraction using #REF task 5 subtask 1 restaurant dataset [5] .",
                "#REF and #REF approaches have not been applied for Indonesian reviews.",
                "This paper aims to perform aspect and opinion terms extraction in Indonesian hotel reviews by adapting CMLA architecture [2] and double embeddings mechanism #TARGET_REF .",
                "The adaption in this paper is conducted by changing the English resources used in word embedding into Indonesian version.",
                "The rest of this paper is structured as follows."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: The model achieved F1-measure of 0.7437 for aspect term extraction using #REF task 5 subtask 1 restaurant dataset [5] .\n sent1: #REF and #REF approaches have not been applied for Indonesian reviews.\n sent2: This paper aims to perform aspect and opinion terms extraction in Indonesian hotel reviews by adapting CMLA architecture [2] and double embeddings mechanism #TARGET_REF .\n sent3: The adaption in this paper is conducted by changing the English resources used in word embedding into Indonesian version.\n sent4: The rest of this paper is structured as follows.\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "As stated previously, the goal of this work is to extract aspect and opinion terms in Indonesian hotel reviews by adapting CMLA architecture [2] and double embeddings mechanism #TARGET_REF .",
                "The architecture of the model used in this work can be seen in Fig. 1 .",
                "Review texts are preprocessed to be used in training extraction model by using InaNLP [13] .",
                "The preprocess consists of sentence normalization, casefolding, and tokenization.",
                "Normalization is done because there are many informal words, abbreviations, and typos in the reviews."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: As stated previously, the goal of this work is to extract aspect and opinion terms in Indonesian hotel reviews by adapting CMLA architecture [2] and double embeddings mechanism #TARGET_REF .\n sent1: The architecture of the model used in this work can be seen in Fig. 1 .\n sent2: Review texts are preprocessed to be used in training extraction model by using InaNLP [13] .\n sent3: The preprocess consists of sentence normalization, casefolding, and tokenization.\n sent4: Normalization is done because there are many informal words, abbreviations, and typos in the reviews.\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "All of the word embeddings are trained using fast#REF .",
                "For the general embeddings and domain embeddings, we use the same dimension and number of iterations as in #TARGET_REF .",
                "The embedding dimensions and number of iterations used to train the word embeddings can be seen in Table III .",
                "For the rest of the hyperparameters, we use the defaults in fastText.",
                "We use fastText for the word embedding because it can use subword N-gram embedding to calculate out-ofvocabulary word embeddings."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: All of the word embeddings are trained using fast#REF .\n sent1: For the general embeddings and domain embeddings, we use the same dimension and number of iterations as in #TARGET_REF .\n sent2: The embedding dimensions and number of iterations used to train the word embeddings can be seen in Table III .\n sent3: For the rest of the hyperparameters, we use the defaults in fastText.\n sent4: We use fastText for the word embedding because it can use subword N-gram embedding to calculate out-ofvocabulary word embeddings.\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "In the experiment, we will try other variations of Recurrent Neural Network (RNN) to replace the Gated Recurrent Unit (GRU) used in CMLA.",
                "Specifically, we conduct experiment using GRU, LSTM, B-GRU, and B-LSTM and choose the one that gives the best performance based on the experiment as the final model.",
                "We implement and train the model using #REF .",
                "We use various types of word embeddings adapted from #TARGET_REF .",
                "Specifically, we conduct experiment using double embeddings, general embeddings, domain embeddings, and hybrid embeddings as the feature used by the model and choose the word embedding that gives the best performance as the feature used by the final model."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In the experiment, we will try other variations of Recurrent Neural Network (RNN) to replace the Gated Recurrent Unit (GRU) used in CMLA.\n sent1: Specifically, we conduct experiment using GRU, LSTM, B-GRU, and B-LSTM and choose the one that gives the best performance based on the experiment as the final model.\n sent2: We implement and train the model using #REF .\n sent3: We use various types of word embeddings adapted from #TARGET_REF .\n sent4: Specifically, we conduct experiment using double embeddings, general embeddings, domain embeddings, and hybrid embeddings as the feature used by the model and choose the word embedding that gives the best performance as the feature used by the final model.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Dialect Identification (DID) problem is a special case of the more general problem of Language Identification (LID).",
                "LID refers to the process of automatically identifying the language class for given speech segment or text document, while DID classifies between dialects within the same language class, making it a more challenging task than LID.",
                "A good DID system used as a front-end to an automatic speech recognition system, can help improve the recognition performance by providing dialectal data for acoustic and language model adaptation to the specific dialect being spoken [1] .",
                "In this work, we focus on Arabic DID which can can be posed as a five class classification problem, given that the Arabic language can be divided into five major dialects; Egyptian (EGY), Gulf (GLF), Lavantine (LAV), Modern Standard Arabic (MSA) and North African (NOR) #TARGET_REF .",
                "Over the past decade, great advances have been made in the field of automatic language identification (LID)."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: Dialect Identification (DID) problem is a special case of the more general problem of Language Identification (LID).\n sent1: LID refers to the process of automatically identifying the language class for given speech segment or text document, while DID classifies between dialects within the same language class, making it a more challenging task than LID.\n sent2: A good DID system used as a front-end to an automatic speech recognition system, can help improve the recognition performance by providing dialectal data for acoustic and language model adaptation to the specific dialect being spoken [1] .\n sent3: In this work, we focus on Arabic DID which can can be posed as a five class classification problem, given that the Arabic language can be divided into five major dialects; Egyptian (EGY), Gulf (GLF), Lavantine (LAV), Modern Standard Arabic (MSA) and North African (NOR) #TARGET_REF .\n sent4: Over the past decade, great advances have been made in the field of automatic language identification (LID).\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Acoustic VSM is constructed in two steps; 1) Extracting the bottleneck features (BNF) from speech and 2) Modeling BNF using the i-Vector extraction framework.",
                "We use the same Deep Neural Network (DNN) based ASR system to extract the BNF as in our previous works #TARGET_REF 13] .",
                "Two DNNs are used with 5 hidden layers and 1 Bottleneck Layer, all having sigmoidal neurons.",
                "Tied-phone states are used as the target to the DNNs.",
                "The target labels of dimension 3040 are provided by a GMM-HMM baseline system trained on 60 hours of Arabic Broadcast speech [14] ."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: Acoustic VSM is constructed in two steps; 1) Extracting the bottleneck features (BNF) from speech and 2) Modeling BNF using the i-Vector extraction framework.\n sent1: We use the same Deep Neural Network (DNN) based ASR system to extract the BNF as in our previous works #TARGET_REF 13] .\n sent2: Two DNNs are used with 5 hidden layers and 1 Bottleneck Layer, all having sigmoidal neurons.\n sent3: Tied-phone states are used as the target to the DNNs.\n sent4: The target labels of dimension 3040 are provided by a GMM-HMM baseline system trained on 60 hours of Arabic Broadcast speech [14] .\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "v is the latent vector, known as the i − V ector and T is the lower dimensional Vector Subspace.",
                "The parameters of the model are estimated using Maximum Likelihood training criterion.",
                "For a detailed explanation of i-Vector modeling framework, reader is directed to excellent work in [15, 11] .",
                "In this work, GMM-UBM model has 2048 gaussian components, MFCC features are extracted using a 25 ms window and the i-Vectors are 400 dimensional #TARGET_REF .",
                "Finally, we construct the acoustic VSM, X A ∈ R N ×400 , where the i th row is the 400 dimensional i-Vector representation corresponding to the speech utterance, a i ."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: v is the latent vector, known as the i − V ector and T is the lower dimensional Vector Subspace.\n sent1: The parameters of the model are estimated using Maximum Likelihood training criterion.\n sent2: For a detailed explanation of i-Vector modeling framework, reader is directed to excellent work in [15, 11] .\n sent3: In this work, GMM-UBM model has 2048 gaussian components, MFCC features are extracted using a 25 ms window and the i-Vectors are 400 dimensional #TARGET_REF .\n sent4: Finally, we construct the acoustic VSM, X A ∈ R N ×400 , where the i th row is the 400 dimensional i-Vector representation corresponding to the speech utterance, a i .\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Training and test data used in this work is the same as used in #TARGET_REF .",
                "Table 1 gives the number of hours of data available for each dialect for training and testing.",
                "Train 13  10  11  9  10  Test  2  2  2  2  2   Table 1 . Number of hours of training and testing data for each dialect Table 2 shows the number of speech utterances that are available for training and testing the DID system.",
                "#REF 1907 1059 1934 1820  Test  315  348  238  355  265   Table 2 . Number of training and test utterances for DID system development"
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: Training and test data used in this work is the same as used in #TARGET_REF .\n sent1: Table 1 gives the number of hours of data available for each dialect for training and testing.\n sent2: Train 13  10  11  9  10  Test  2  2  2  2  2   Table 1 . Number of hours of training and testing data for each dialect Table 2 shows the number of speech utterances that are available for training and testing the DID system.\n sent3: #REF 1907 1059 1934 1820  Test  315  348  238  355  265   Table 2 . Number of training and test utterances for DID system development\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Phone sequences for each utterance are extracted using one or multiple phone recognisers.",
                "A Vector Space Model (VSM) is then constructed using a term-document matrix [4] , followed by an unsupervised dimensionality reduction technique, such as Principal Component Analysis (PCA) [5] to map the high dimensional feature space to a low dimensional Vector Subspace (Section 2.1), giving a Phonotactic VSM.",
                "In other cases, a phone n-gram language model is used to model the phone statistics instead of a VSM [6, 7, 8] .",
                "On the other hand, Acoustic approaches attempt to extract dialect discriminative information from speech using low level acoustic features, such as pitch, prosody, shifted delta ceptral coefficients, bottleneck features [9, 10] .",
                "One of the most successful acoustic approaches is, the use of i-Vector framework for LID, where i-Vectors are extracted for each speech utterance, using an i-Vector extractor that consists of a GMM-UBM trained on top of BNF, followed by a Total Variability Subspace Model #TARGET_REF 11] ."
            ],
            "label": [
                "DIFFERENCES",
                "EXTENDS"
            ]
        },
        "input": "sent0: Phone sequences for each utterance are extracted using one or multiple phone recognisers.\n sent1: A Vector Space Model (VSM) is then constructed using a term-document matrix [4] , followed by an unsupervised dimensionality reduction technique, such as Principal Component Analysis (PCA) [5] to map the high dimensional feature space to a low dimensional Vector Subspace (Section 2.1), giving a Phonotactic VSM.\n sent2: In other cases, a phone n-gram language model is used to model the phone statistics instead of a VSM [6, 7, 8] .\n sent3: On the other hand, Acoustic approaches attempt to extract dialect discriminative information from speech using low level acoustic features, such as pitch, prosody, shifted delta ceptral coefficients, bottleneck features [9, 10] .\n sent4: One of the most successful acoustic approaches is, the use of i-Vector framework for LID, where i-Vectors are extracted for each speech utterance, using an i-Vector extractor that consists of a GMM-UBM trained on top of BNF, followed by a Total Variability Subspace Model #TARGET_REF 11] .\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"EXTENDS\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "This gives us two DID systems built using the Acoustic and Phonotactic VSMs.",
                "At prediction time, output scores from the two DID systems are combined to give a final score, on the basis of which classification decision is made.",
                "This model combination approach has been shown to give performace improvements on the DID task #TARGET_REF .",
                "This also shows that the two systems are complementary to each other, which leads us to investigate a feature space combination approach i.e. to construct a single VSM by combining Phonotactic and Acoustic VSMs, in an attempt to encode useful discriminative information in that single VSM.",
                "In this work, we present a feature space combination approach."
            ],
            "label": [
                "DIFFERENCES",
                "EXTENDS"
            ]
        },
        "input": "sent0: This gives us two DID systems built using the Acoustic and Phonotactic VSMs.\n sent1: At prediction time, output scores from the two DID systems are combined to give a final score, on the basis of which classification decision is made.\n sent2: This model combination approach has been shown to give performace improvements on the DID task #TARGET_REF .\n sent3: This also shows that the two systems are complementary to each other, which leads us to investigate a feature space combination approach i.e. to construct a single VSM by combining Phonotactic and Acoustic VSMs, in an attempt to encode useful discriminative information in that single VSM.\n sent4: In this work, we present a feature space combination approach.\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"EXTENDS\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Phonotactic VSM is constructed by modeling the n-gram phone statistics of the phone sequences that are extracted using an Arabic phone recognizer.",
                "Details about the phone recognizer can be found in #TARGET_REF .",
                "VSM is constructed in two steps; 1) Construct a term-document matrix, X ∈ R N ×d (See Fig 1) , where each speech utterance in represented by a Phonotactic feature vector,",
                ", where N is the number of speech utterances and f (p, s) is the number of times a phone n-gram (term) s appears in the utterance (document) p and 2) Perform Truncated Singular Value Decomposition (SVD) (Equation 2) on X to learn a lower dimensional linear manifold, Π ∈ R d×k , where k << d. SVD attempts to discover the latent structure in the high dimensional feature space.",
                "Note that, k is the number of largest singular values."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Phonotactic VSM is constructed by modeling the n-gram phone statistics of the phone sequences that are extracted using an Arabic phone recognizer.\n sent1: Details about the phone recognizer can be found in #TARGET_REF .\n sent2: VSM is constructed in two steps; 1) Construct a term-document matrix, X ∈ R N ×d (See Fig 1) , where each speech utterance in represented by a Phonotactic feature vector,\n sent3: , where N is the number of speech utterances and f (p, s) is the number of times a phone n-gram (term) s appears in the utterance (document) p and 2) Perform Truncated Singular Value Decomposition (SVD) (Equation 2) on X to learn a lower dimensional linear manifold, Π ∈ R d×k , where k << d. SVD attempts to discover the latent structure in the high dimensional feature space.\n sent4: Note that, k is the number of largest singular values.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We also perform Linear Discriminant Analysis (LDA) and Within Class Co-variance Normalization (WCCN) on the Acoustic Vector Space, to increase the discriminative strength of the VSM.",
                "This method has been shown to improve DID (LID) performance #TARGET_REF 11] .",
                "Here, we give a brief overview of the mathematical foundations of the CCA .",
                "Fig 2 gives a probabilistic graphical model of CCA.",
                "Nodes of the graph represent Random Variables (RVs) and the structure encodes conditional independence assumptions."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: We also perform Linear Discriminant Analysis (LDA) and Within Class Co-variance Normalization (WCCN) on the Acoustic Vector Space, to increase the discriminative strength of the VSM.\n sent1: This method has been shown to improve DID (LID) performance #TARGET_REF 11] .\n sent2: Here, we give a brief overview of the mathematical foundations of the CCA .\n sent3: Fig 2 gives a probabilistic graphical model of CCA.\n sent4: Nodes of the graph represent Random Variables (RVs) and the structure encodes conditional independence assumptions.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Training data consist of recording from the Arabic Broadcast domain and contains utterances spoken in all the five dialects; EGY, GLF, LAV, MSA and NOR.",
                "The test set is from the same broadcast domain but is collected from Al-Jazeera and hence, unlike training data set, the recording are of high quality.",
                "The test set is labeled using CrowdFlower, a crowd source platform, by QCRI and is publicly available on their web portal 1 .",
                "More details about the train and test data can be found in #TARGET_REF 18] .",
                "Fig 3 gives an overview of our DID system, which can be seen as a combination of two broad components; 1) Vector Space Modeling Component and 2) Back-end classifier."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Training data consist of recording from the Arabic Broadcast domain and contains utterances spoken in all the five dialects; EGY, GLF, LAV, MSA and NOR.\n sent1: The test set is from the same broadcast domain but is collected from Al-Jazeera and hence, unlike training data set, the recording are of high quality.\n sent2: The test set is labeled using CrowdFlower, a crowd source platform, by QCRI and is publicly available on their web portal 1 .\n sent3: More details about the train and test data can be found in #TARGET_REF 18] .\n sent4: Fig 3 gives an overview of our DID system, which can be seen as a combination of two broad components; 1) Vector Space Modeling Component and 2) Back-end classifier.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The Transformer neural sequence model #TARGET_REF has emerged as a popular alternative to recurrent sequence models.",
                "Transformer relies on attention layers to communicate information between and across sequences.",
                "One major challenge with Transformer is the speed of incremental inference.",
                "As we will discuss, the speed of incremental Transformer inference on modern computing hardware is limited by the memory bandwidth necessary to reload the large \"keys\" and \"values\" tensors which encode the state of the attention layers.",
                "In the following sections, we will review the multi-head-attention layers used by Transformer, provide a performance analysis, and propose an architectural variation (multi-query attention) which greatly improves inference speed with only minor quality degradation."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The Transformer neural sequence model #TARGET_REF has emerged as a popular alternative to recurrent sequence models.\n sent1: Transformer relies on attention layers to communicate information between and across sequences.\n sent2: One major challenge with Transformer is the speed of incremental inference.\n sent3: As we will discuss, the speed of incremental Transformer inference on modern computing hardware is limited by the memory bandwidth necessary to reload the large \"keys\" and \"values\" tensors which encode the state of the attention layers.\n sent4: In the following sections, we will review the multi-head-attention layers used by Transformer, provide a performance analysis, and propose an architectural variation (multi-query attention) which greatly improves inference speed with only minor quality degradation.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "The \"Transformer\" seuqence-to-sequence model #TARGET_REF uses h different attention layers (heads) in parallel, which the authors refer to as \"Multi-head attention\".",
                "The query vectors for the h different layers are derived from h different learned linear projections P q of an input vector x. Similarly, the keys and values are derived from h different learned linear projections P k , P v of a collection M of m different input vectors.",
                "The outputs of the h layers are themselves passed through different learned linear projections P o , then summed.",
                "For simplicity, we give the input and output vectors identical dimensionality d. The The computation can be expressed as follows: d e f M u l t i h e a d A t t e n t i o n (",
                "x , M, P_q, P_k, P_v, P_o ) : \" \" \" Multi−head A t t e n t i o n on one quer y ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The \"Transformer\" seuqence-to-sequence model #TARGET_REF uses h different attention layers (heads) in parallel, which the authors refer to as \"Multi-head attention\".\n sent1: The query vectors for the h different layers are derived from h different learned linear projections P q of an input vector x. Similarly, the keys and values are derived from h different learned linear projections P k , P v of a collection M of m different input vectors.\n sent2: The outputs of the h layers are themselves passed through different learned linear projections P o , then summed.\n sent3: For simplicity, we give the input and output vectors identical dimensionality d. The The computation can be expressed as follows: d e f M u l t i h e a d A t t e n t i o n (\n sent4: x , M, P_q, P_k, P_v, P_o ) : \" \" \" Multi−head A t t e n t i o n on one quer y .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "einsum ( \" hk , hmk−>hm\" , q , K) w e i g h t s = t f .",
                "so ftma x ( l o g i t s ) o = t f .",
                "einsum ( \"hm, hmv−>hv \" , weig hts , V) y = t f .",
                "einsum ( \" hv , hdv−>d \" , o , P_o) r e t u r n y Note: #TARGET_REF include a constant scaling factor on the logits.",
                "We omit this in our code, as it can be folded into the linear projections P q or P k ."
            ],
            "label": [
                "DIFFERENCES",
                "EXTENDS"
            ]
        },
        "input": "sent0: einsum ( \" hk , hmk−>hm\" , q , K) w e i g h t s = t f .\n sent1: so ftma x ( l o g i t s ) o = t f .\n sent2: einsum ( \"hm, hmv−>hv \" , weig hts , V) y = t f .\n sent3: einsum ( \" hv , hdv−>d \" , o , P_o) r e t u r n y Note: #TARGET_REF include a constant scaling factor on the logits.\n sent4: We omit this in our code, as it can be folded into the linear projections P q or P k .\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"EXTENDS\"], \"context\": [\"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "The code below adds two types of batching.",
                "First, we generate queries from n different positions in a sequence.",
                "These queries all interact with the same keys and values.",
                "In addition, we process a batch of b different non-interacting sequences at once.",
                "Following #TARGET_REF , in an autoregressive model, we can prevent backward-information-flow by adding a \"mask\" to the logits containing the value −∞ in the illegal positions."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The code below adds two types of batching.\n sent1: First, we generate queries from n different positions in a sequence.\n sent2: These queries all interact with the same keys and values.\n sent3: In addition, we process a batch of b different non-interacting sequences at once.\n sent4: Following #TARGET_REF , in an autoregressive model, we can prevent backward-information-flow by adding a \"mask\" to the logits containing the value −∞ in the illegal positions.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "To simplify the performance analysis, we will make several simplifying assumptions:",
                "h , as suggested by #TARGET_REF",
                "The total number of arithmetic operations is Θ(bnd 2 ).",
                "(Since the complexity of each of the tf.einsum operations above is O(bnd 2 ) given the simplifying assumptions.",
                "The total size of memory to be accessed is equal to the sum of the sizes of all the tensors involved: O(bnd + bhn 2 + d 2 )."
            ],
            "label": [
                "USE",
                "SIMILARITY"
            ]
        },
        "input": "sent0: To simplify the performance analysis, we will make several simplifying assumptions:\n sent1: h , as suggested by #TARGET_REF\n sent2: The total number of arithmetic operations is Θ(bnd 2 ).\n sent3: (Since the complexity of each of the tf.einsum operations above is O(bnd 2 ) given the simplifying assumptions.\n sent4: The total size of memory to be accessed is equal to the sum of the sizes of all the tensors involved: O(bnd + bhn 2 + d 2 ).\n",
        "output": "{\"label\": [\"USE\", \"SIMILARITY\"], \"context\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We introduce multi-query Attention as a variation of multi-head attention as described in #TARGET_REF .",
                "Multi-head attention consists of multiple attention layers (heads) in parallel with different linear transformations on the queries, keys, values and outputs.",
                "Multi-query attention is identical except that the different heads share a single set of keys and values.",
                "The code for (incremental) multi-query (self) attention is identical to the code listed above for multi-head attention, except that we remove the letter \"h\" from the tf.einsum equations where it represents the \"heads\" dimension of K, V , P k , or P v ."
            ],
            "label": [
                "USE",
                "SIMILARITY"
            ]
        },
        "input": "sent0: We introduce multi-query Attention as a variation of multi-head attention as described in #TARGET_REF .\n sent1: Multi-head attention consists of multiple attention layers (heads) in parallel with different linear transformations on the queries, keys, values and outputs.\n sent2: Multi-query attention is identical except that the different heads share a single set of keys and values.\n sent3: The code for (incremental) multi-query (self) attention is identical to the code listed above for multi-head attention, except that we remove the letter \"h\" from the tf.einsum equations where it represents the \"heads\" dimension of K, V , P k , or P v .\n",
        "output": "{\"label\": [\"USE\", \"SIMILARITY\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Following #TARGET_REF , we evaluate on the WMT 2014 English-German translation task.",
                "As a baseline, we use an encoder-decoder Transformer model with 6 layers, using d model = 1024 d f f = 4096, h = 8, d k = d v = 128, learned positional embeddings, and weight-sharing between the token-embedding and output layers.",
                "The baseline model and all variations have 211 million parameters.",
                "All models were trained for 100,000 steps ( 20 epochs).",
                "Each training batch consisted of 128 examples, each of which consisted of a 256-token input sequence and a 256-token target sequence (multiple training sentences were concatenated together to reach this length)."
            ],
            "label": [
                "USE",
                "SIMILARITY"
            ]
        },
        "input": "sent0: Following #TARGET_REF , we evaluate on the WMT 2014 English-German translation task.\n sent1: As a baseline, we use an encoder-decoder Transformer model with 6 layers, using d model = 1024 d f f = 4096, h = 8, d k = d v = 128, learned positional embeddings, and weight-sharing between the token-embedding and output layers.\n sent2: The baseline model and all variations have 211 million parameters.\n sent3: All models were trained for 100,000 steps ( 20 epochs).\n sent4: Each training batch consisted of 128 examples, each of which consisted of a 256-token input sequence and a 256-token target sequence (multiple training sentences were concatenated together to reach this length).\n",
        "output": "{\"label\": [\"USE\", \"SIMILARITY\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "In some settings, data dependencies make it is impossible to process queries from multiple positions in parallel.",
                "An example is a self-attention layer in an autoregressive language model such as Transformer #TARGET_REF .",
                "The queries produced at each position attend to key-value pairs produced at all positions up to and including that position.",
                "During training, the ground-truth target sequence is known, and we can use an efficient parallel implementation similar to that in section 2.3.",
                "However, when generating from the trained model, the output of the self-attention layer at a particular position affects the token that is generated at the next position, which in turn affects the input to that layer at the next position."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: In some settings, data dependencies make it is impossible to process queries from multiple positions in parallel.\n sent1: An example is a self-attention layer in an autoregressive language model such as Transformer #TARGET_REF .\n sent2: The queries produced at each position attend to key-value pairs produced at all positions up to and including that position.\n sent3: During training, the ground-truth target sequence is known, and we can use an efficient parallel implementation similar to that in section 2.3.\n sent4: However, when generating from the trained model, the output of the self-attention layer at a particular position affects the token that is generated at the next position, which in turn affects the input to that layer at the next position.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "While most of them assign grammatical functions on top of constituency trees (#REF; Jijkoun and de #REF; Chrupała and #REF; #REF; #REF) , less work has tried to predict GF labels for unlabelled dependency trees.",
                "One of them is #REF who first generate the unlabelled trees using a graph-based parser, and then model the assignment of dependency labels as a sequence labelling task.",
                "Another approach has been proposed by #TARGET_REF who present a simple, yet efficient and accurate parsing model that generates unlabelled trees by identifying the most probable head for each token in the input.",
                "Then, in a post-processing step, they assign labels to each head-dependent pair, using a two-layer rectifier network.",
                "Dependency Parsing as Head Selection Our labelling model is an extension of the parsing model of #REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: While most of them assign grammatical functions on top of constituency trees (#REF; Jijkoun and de #REF; Chrupała and #REF; #REF; #REF) , less work has tried to predict GF labels for unlabelled dependency trees.\n sent1: One of them is #REF who first generate the unlabelled trees using a graph-based parser, and then model the assignment of dependency labels as a sequence labelling task.\n sent2: Another approach has been proposed by #TARGET_REF who present a simple, yet efficient and accurate parsing model that generates unlabelled trees by identifying the most probable head for each token in the input.\n sent3: Then, in a post-processing step, they assign labels to each head-dependent pair, using a two-layer rectifier network.\n sent4: Dependency Parsing as Head Selection Our labelling model is an extension of the parsing model of #REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The unlabelled tree is then built by selecting the most probable head for each word.",
                "The score of word w j being the head of word w i is computed by a single hidden layer neural network on their representations a j and a i .",
                "An additional classifier with two rectified hidden layers is used to predict dependency labels, and is trained separately from the unlabeled parsing component, in a pipeline architecture.",
                "The classifier predictions are based on the representations of the head and the dependent, b j and b i , which are the concatenation of the input and the bidirectional LSTM-based representations:",
                "Despite its simplicity and the lack of global optimisation, #TARGET_REF report competitive results for English, Czech, and German."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The unlabelled tree is then built by selecting the most probable head for each word.\n sent1: The score of word w j being the head of word w i is computed by a single hidden layer neural network on their representations a j and a i .\n sent2: An additional classifier with two rectified hidden layers is used to predict dependency labels, and is trained separately from the unlabeled parsing component, in a pipeline architecture.\n sent3: The classifier predictions are based on the representations of the head and the dependent, b j and b i , which are the concatenation of the input and the bidirectional LSTM-based representations:\n sent4: Despite its simplicity and the lack of global optimisation, #TARGET_REF report competitive results for English, Czech, and German.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Although the labelling approach in #TARGET_REF is simple and efficient, looking at head and dependent only when assigning the labels comes with some disadvantages.",
                "First, some labels are easier to predict when we also take context into account, e.g. the parent and grandparent nodes or the siblings of the head or dependent.",
                "Consider, for example, the following sentence: Is this the future of chamber music? and its syntactic structure (figure 1).",
                "If we only consider the nodes this and future, there is a chance that the edge between them is labelled as det (determiner).",
                "However, if we also look at the local context, we know that node the to the left of future is more likely to be the determiner, and thus this should be assigned a different label."
            ],
            "label": [
                "BACKGROUND",
                "MOTIVATION"
            ]
        },
        "input": "sent0: Although the labelling approach in #TARGET_REF is simple and efficient, looking at head and dependent only when assigning the labels comes with some disadvantages.\n sent1: First, some labels are easier to predict when we also take context into account, e.g. the parent and grandparent nodes or the siblings of the head or dependent.\n sent2: Consider, for example, the following sentence: Is this the future of chamber music? and its syntactic structure (figure 1).\n sent3: If we only consider the nodes this and future, there is a chance that the edge between them is labelled as det (determiner).\n sent4: However, if we also look at the local context, we know that node the to the left of future is more likely to be the determiner, and thus this should be assigned a different label.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"MOTIVATION\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "One of them is #REF who first generate the unlabelled trees using a graph-based parser, and then model the assignment of dependency labels as a sequence labelling task.",
                "Another approach has been proposed by #REF who present a simple, yet efficient and accurate parsing model that generates unlabelled trees by identifying the most probable head for each token in the input.",
                "Then, in a post-processing step, they assign labels to each head-dependent pair, using a two-layer rectifier network.",
                "Dependency Parsing as Head Selection Our labelling model is an extension of the parsing model of #TARGET_REF .",
                "We use our own implementation of the head-selection parser and focus on the grammatical function labelling part."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: One of them is #REF who first generate the unlabelled trees using a graph-based parser, and then model the assignment of dependency labels as a sequence labelling task.\n sent1: Another approach has been proposed by #REF who present a simple, yet efficient and accurate parsing model that generates unlabelled trees by identifying the most probable head for each token in the input.\n sent2: Then, in a post-processing step, they assign labels to each head-dependent pair, using a two-layer rectifier network.\n sent3: Dependency Parsing as Head Selection Our labelling model is an extension of the parsing model of #TARGET_REF .\n sent4: We use our own implementation of the head-selection parser and focus on the grammatical function labelling part.\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Our interest is focussed on German, but to put our work in context, we follow #TARGET_REF and report results also for English, which has a configurational word order, and for Czech, which has a free word order, rich morphology, and less ambiguity in the case paradigm than German.",
                "For English, we use the Penn Treebank (PTB) (#REF) with standard training/dev/test splits.",
                "The POS tags are assigned using the Stanford POS tagger (#REF) with ten-way jackknifing, and constituency trees are converted to Stanford basic dependencies (De #REF) .",
                "The German and Czech data come from the CoNLL-X shared task (#REF) and our data split follows #REF .",
                "As the CoNLL-X testsets are rather small (∼ 360 sentences), we also"
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Our interest is focussed on German, but to put our work in context, we follow #TARGET_REF and report results also for English, which has a configurational word order, and for Czech, which has a free word order, rich morphology, and less ambiguity in the case paradigm than German.\n sent1: For English, we use the Penn Treebank (PTB) (#REF) with standard training/dev/test splits.\n sent2: The POS tags are assigned using the Stanford POS tagger (#REF) with ten-way jackknifing, and constituency trees are converted to Stanford basic dependencies (De #REF) .\n sent3: The German and Czech data come from the CoNLL-X shared task (#REF) and our data split follows #REF .\n sent4: As the CoNLL-X testsets are rather small (∼ 360 sentences), we also\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Our interest is focussed on German, but to put our work in context, we follow #REF and report results also for English, which has a configurational word order, and for Czech, which has a free word order, rich morphology, and less ambiguity in the case paradigm than German.",
                "For English, we use the Penn Treebank (PTB) (#REF) with standard training/dev/test splits.",
                "The POS tags are assigned using the Stanford POS tagger (#REF) with ten-way jackknifing, and constituency trees are converted to Stanford basic dependencies (De #REF) .",
                "The German and Czech data come from the CoNLL-X shared task (#REF) and our data split follows #TARGET_REF .",
                "As the CoNLL-X testsets are rather small (∼ 360 sentences), we also"
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Our interest is focussed on German, but to put our work in context, we follow #REF and report results also for English, which has a configurational word order, and for Czech, which has a free word order, rich morphology, and less ambiguity in the case paradigm than German.\n sent1: For English, we use the Penn Treebank (PTB) (#REF) with standard training/dev/test splits.\n sent2: The POS tags are assigned using the Stanford POS tagger (#REF) with ten-way jackknifing, and constituency trees are converted to Stanford basic dependencies (De #REF) .\n sent3: The German and Czech data come from the CoNLL-X shared task (#REF) and our data split follows #TARGET_REF .\n sent4: As the CoNLL-X testsets are rather small (∼ 360 sentences), we also\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "We test different labelling models on top of the unlabelled trees produced by our re-implementation of the parsing as head selection model ( §2).",
                "We first train the unlabelled parsing models for the three languages.",
                "Unless stated otherwise, all parameters are set according to #TARGET_REF , and tag embedding size was set to 40 for all languages.",
                "Please note that we do not use pre-trained embeddings in our experiments.",
                "In the next step, we train four different labelling models: the labeller of #REF that uses a rectifier neural network with two hidden layers (baseline), two bidirectional LSTM models (BILSTM(L) and BILSTM(B)), and one tree LSTM model (TREELSTM) ( §3)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We test different labelling models on top of the unlabelled trees produced by our re-implementation of the parsing as head selection model ( §2).\n sent1: We first train the unlabelled parsing models for the three languages.\n sent2: Unless stated otherwise, all parameters are set according to #TARGET_REF , and tag embedding size was set to 40 for all languages.\n sent3: Please note that we do not use pre-trained embeddings in our experiments.\n sent4: In the next step, we train four different labelling models: the labeller of #REF that uses a rectifier neural network with two hidden layers (baseline), two bidirectional LSTM models (BILSTM(L) and BILSTM(B)), and one tree LSTM model (TREELSTM) ( §3).\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We first train the unlabelled parsing models for the three languages.",
                "Unless stated otherwise, all parameters are set according to #REF , and tag embedding size was set to 40 for all languages.",
                "Please note that we do not use pre-trained embeddings in our experiments.",
                "In the next step, we train four different labelling models: the labeller of #TARGET_REF that uses a rectifier neural network with two hidden layers (baseline), two bidirectional LSTM models (BILSTM(L) and BILSTM(B)), and one tree LSTM model (TREELSTM) ( §3).",
                "The hidden layer dimension in all LSTM models was set to 200."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We first train the unlabelled parsing models for the three languages.\n sent1: Unless stated otherwise, all parameters are set according to #REF , and tag embedding size was set to 40 for all languages.\n sent2: Please note that we do not use pre-trained embeddings in our experiments.\n sent3: In the next step, we train four different labelling models: the labeller of #TARGET_REF that uses a rectifier neural network with two hidden layers (baseline), two bidirectional LSTM models (BILSTM(L) and BILSTM(B)), and one tree LSTM model (TREELSTM) ( §3).\n sent4: The hidden layer dimension in all LSTM models was set to 200.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Table 1 shows the unlabelled attachment score (UAS) for the unlabelled trees and the labelled attachment scores (LAS) for the different labellers (excluding punctuation).",
                "All history-based labelling models perform significantly better than the local baseline model, 1 but for English the improvements are smaller (0.3%) than for the nonconfigurational languages (∼0.7%).",
                "While we tried to reimplement the model of #TARGET_REF following the details in the paper, our reimplemented model yields higher scores for German, compared to the results in the paper.",
                "The scores for English are slightly lower since, in contrast to #REF , we do not use pre-trained embeddings.",
                "When using our historybased labellers, we get similar results for English (91.9%) and higher results for both Czech (84.1% vs. 81.7%) and German (91.0% vs. 89.6%) on the same data without using pre-trained embeddings or post-processing."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Table 1 shows the unlabelled attachment score (UAS) for the unlabelled trees and the labelled attachment scores (LAS) for the different labellers (excluding punctuation).\n sent1: All history-based labelling models perform significantly better than the local baseline model, 1 but for English the improvements are smaller (0.3%) than for the nonconfigurational languages (∼0.7%).\n sent2: While we tried to reimplement the model of #TARGET_REF following the details in the paper, our reimplemented model yields higher scores for German, compared to the results in the paper.\n sent3: The scores for English are slightly lower since, in contrast to #REF , we do not use pre-trained embeddings.\n sent4: When using our historybased labellers, we get similar results for English (91.9%) and higher results for both Czech (84.1% vs. 81.7%) and German (91.0% vs. 89.6%) on the same data without using pre-trained embeddings or post-processing.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Table 1 shows the unlabelled attachment score (UAS) for the unlabelled trees and the labelled attachment scores (LAS) for the different labellers (excluding punctuation).",
                "All history-based labelling models perform significantly better than the local baseline model, 1 but for English the improvements are smaller (0.3%) than for the nonconfigurational languages (∼0.7%).",
                "While we tried to reimplement the model of #REF following the details in the paper, our reimplemented model yields higher scores for German, compared to the results in the paper.",
                "The scores for English are slightly lower since, in contrast to #TARGET_REF , we do not use pre-trained embeddings.",
                "When using our historybased labellers, we get similar results for English (91.9%) and higher results for both Czech (84.1% vs. 81.7%) and German (91.0% vs. 89.6%) on the same data without using pre-trained embeddings or post-processing."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Table 1 shows the unlabelled attachment score (UAS) for the unlabelled trees and the labelled attachment scores (LAS) for the different labellers (excluding punctuation).\n sent1: All history-based labelling models perform significantly better than the local baseline model, 1 but for English the improvements are smaller (0.3%) than for the nonconfigurational languages (∼0.7%).\n sent2: While we tried to reimplement the model of #REF following the details in the paper, our reimplemented model yields higher scores for German, compared to the results in the paper.\n sent3: The scores for English are slightly lower since, in contrast to #TARGET_REF , we do not use pre-trained embeddings.\n sent4: When using our historybased labellers, we get similar results for English (91.9%) and higher results for both Czech (84.1% vs. 81.7%) and German (91.0% vs. 89.6%) on the same data without using pre-trained embeddings or post-processing.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "On the SPMRL 2014 shared task data, our results are only 0.3% lower than the ones of the winning system (Björkelund et al., 2014) fectiveness of our models, we also ran our labeller on the unlabelled output of the SPMRL 2014 winning system and on unlabelled gold trees.",
                "On the output of the blended system LAS slightly improves from 88.62% to 88.76% (TREELSTM).",
                "3 When applied to unlabelled gold trees, the distance between our models and the baseline becomes larger and the best of our history-based models (BILSTM(B), 97.38%) outperforms the original labeller of #TARGET_REF (96.15%) by more than 1%.",
                "We would like to emphasize that our historybased LSTM labeller is practically simple and computationally inexpensive (as compared to global training or inference), so our model manages to preserve simplicity while significantly improving labelling performance."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: On the SPMRL 2014 shared task data, our results are only 0.3% lower than the ones of the winning system (Björkelund et al., 2014) fectiveness of our models, we also ran our labeller on the unlabelled output of the SPMRL 2014 winning system and on unlabelled gold trees.\n sent1: On the output of the blended system LAS slightly improves from 88.62% to 88.76% (TREELSTM).\n sent2: 3 When applied to unlabelled gold trees, the distance between our models and the baseline becomes larger and the best of our history-based models (BILSTM(B), 97.38%) outperforms the original labeller of #TARGET_REF (96.15%) by more than 1%.\n sent3: We would like to emphasize that our historybased LSTM labeller is practically simple and computationally inexpensive (as compared to global training or inference), so our model manages to preserve simplicity while significantly improving labelling performance.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We have shown that GF labelling, which is of crucial importance for languages like German, can be improved by combining LSTM models with a decision history.",
                "All our models outperform the original labeller of #TARGET_REF and give results in the same range as the best system from the SPMRL-2014 shared task (without the reranker), but with a much simpler model.",
                "Our results show that the history is especially important for languages that show more word order variation.",
                "Here, presenting the input in a structured BFS order not only significantly outperforms the baseline, but also yields improvements over the other LSTM models on core grammatical functions."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: We have shown that GF labelling, which is of crucial importance for languages like German, can be improved by combining LSTM models with a decision history.\n sent1: All our models outperform the original labeller of #TARGET_REF and give results in the same range as the best system from the SPMRL-2014 shared task (without the reranker), but with a much simpler model.\n sent2: Our results show that the history is especially important for languages that show more word order variation.\n sent3: Here, presenting the input in a structured BFS order not only significantly outperforms the baseline, but also yields improvements over the other LSTM models on core grammatical functions.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Fully supervised maximum entropy Markov models have been used for cascaded prediction of POS tags followed by supertags (#REF) .",
                "Here, we learn supertaggers given only a POS tag dictionary and supertag dictionary or a small amount of material labeled with both types of information.",
                "Previous work has used Bayesian HMMs to learn taggers for both POS tagging and supertagging #TARGET_REF separately.",
                "Modeling them jointly has the potential to produce more robust and accurate supertaggers trained with less supervision and thereby potentially help in the creation of useful models for new languages and domains.",
                "Our results show that joint inference improves supervised supertag prediction (compared to HMMs), especially when labeled training data is scarce."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Fully supervised maximum entropy Markov models have been used for cascaded prediction of POS tags followed by supertags (#REF) .\n sent1: Here, we learn supertaggers given only a POS tag dictionary and supertag dictionary or a small amount of material labeled with both types of information.\n sent2: Previous work has used Bayesian HMMs to learn taggers for both POS tagging and supertagging #TARGET_REF separately.\n sent3: Modeling them jointly has the potential to produce more robust and accurate supertaggers trained with less supervision and thereby potentially help in the creation of useful models for new languages and domains.\n sent4: Our results show that joint inference improves supervised supertag prediction (compared to HMMs), especially when labeled training data is scarce.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We also report the CCG accuracy values inclusive of unambiguous types in Table 5 (b) for α = 1.0 and Table 6 (b) respectively.",
                "The performance of the HMM model (31%) in Table 5 (a) without any frequency cut-off on the CCG categories, is comparable to the bitag HMM of #REF that uses variational Bayes EM (33%).",
                "Our complexity based initialization is not directly comparable to the results in #REF because the values there are based on a weighted combination of complexity based initialization and modified transition priors based on the CCG formalism.",
                "However, it is encouraging to see that when there is no cut-off based filtering of the categories, FHMMB (47.98%) greatly outperforms the HMM-EM model of #REF .",
                "It is however, quite short of the 56.1% accuracy achieved by the model of #TARGET_REF that uses grammar informed initialization (combination of category based initialization along with category transition rules)."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: We also report the CCG accuracy values inclusive of unambiguous types in Table 5 (b) for α = 1.0 and Table 6 (b) respectively.\n sent1: The performance of the HMM model (31%) in Table 5 (a) without any frequency cut-off on the CCG categories, is comparable to the bitag HMM of #REF that uses variational Bayes EM (33%).\n sent2: Our complexity based initialization is not directly comparable to the results in #REF because the values there are based on a weighted combination of complexity based initialization and modified transition priors based on the CCG formalism.\n sent3: However, it is encouraging to see that when there is no cut-off based filtering of the categories, FHMMB (47.98%) greatly outperforms the HMM-EM model of #REF .\n sent4: It is however, quite short of the 56.1% accuracy achieved by the model of #TARGET_REF that uses grammar informed initialization (combination of category based initialization along with category transition rules).\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "In this experiment, we use the training and test sets used by #TARGET_REF from CCGbank.",
                "We vary the amount of training material by using 100, 1000, 10,000 and all 38015 training set sentences.",
                "We also vary the transition prior α choosing α = 1.0 and α = 0.05 on the CCG tags.",
                "The emission prior β was held constant at 1.0.",
                "The results of these experiments for α = 0.05 are tabulated in Table 3 (a)."
            ],
            "label": [
                "USE",
                "SIMILARITY"
            ]
        },
        "input": "sent0: In this experiment, we use the training and test sets used by #TARGET_REF from CCGbank.\n sent1: We vary the amount of training material by using 100, 1000, 10,000 and all 38015 training set sentences.\n sent2: We also vary the transition prior α choosing α = 1.0 and α = 0.05 on the CCG tags.\n sent3: The emission prior β was held constant at 1.0.\n sent4: The results of these experiments for α = 0.05 are tabulated in Table 3 (a).\n",
        "output": "{\"label\": [\"USE\", \"SIMILARITY\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "In the weakly supervised learning setting, we are provided with a lexicon that lists possible POS tags and supertags for many, though not all, words.",
                "We draw the initial sample of CCG tag sequences corresponding to the observation sequence, using probabilities based on grammar informed initialization #TARGET_REF .",
                "We consider the prior probability of occurrence of categories based on their complexity: given a lexicon L, the probability of a category c i is inversely proportional to its complexity:",
                "where complexity(c i ) is defined as the number of sub-categories contained in category c i .",
                "The POS tag corresponding to an observed word w i is drawn uniformly at random from the set of all tags corresponding to w i in the dictionary."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In the weakly supervised learning setting, we are provided with a lexicon that lists possible POS tags and supertags for many, though not all, words.\n sent1: We draw the initial sample of CCG tag sequences corresponding to the observation sequence, using probabilities based on grammar informed initialization #TARGET_REF .\n sent2: We consider the prior probability of occurrence of categories based on their complexity: given a lexicon L, the probability of a category c i is inversely proportional to its complexity:\n sent3: where complexity(c i ) is defined as the number of sub-categories contained in category c i .\n sent4: The POS tag corresponding to an observed word w i is drawn uniformly at random from the set of all tags corresponding to w i in the dictionary.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The performance of the HMM model (31%) in Table 5 (a) without any frequency cut-off on the CCG categories, is comparable to the bitag HMM of #TARGET_REF that uses variational Bayes EM (33%).",
                "Our complexity based initialization is not directly comparable to the results in #REF because the values there are based on a weighted combination of complexity based initialization and modified transition priors based on the CCG formalism.",
                "However, it is encouraging to see that when there is no cut-off based filtering of the categories, FHMMB (47.98%) greatly outperforms the HMM-EM model of #REF .",
                "It is however, quite short of the 56.1% accuracy achieved by the model of #REF that uses grammar informed initialization (combination of category based initialization along with category transition rules).",
                "Without any frequency cut-off on CCG categories, FHMMB achieves over 17% improvement in the prediction accuracy of ambiguous CCG categories, in comparison with the HMM."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: The performance of the HMM model (31%) in Table 5 (a) without any frequency cut-off on the CCG categories, is comparable to the bitag HMM of #TARGET_REF that uses variational Bayes EM (33%).\n sent1: Our complexity based initialization is not directly comparable to the results in #REF because the values there are based on a weighted combination of complexity based initialization and modified transition priors based on the CCG formalism.\n sent2: However, it is encouraging to see that when there is no cut-off based filtering of the categories, FHMMB (47.98%) greatly outperforms the HMM-EM model of #REF .\n sent3: It is however, quite short of the 56.1% accuracy achieved by the model of #REF that uses grammar informed initialization (combination of category based initialization along with category transition rules).\n sent4: Without any frequency cut-off on CCG categories, FHMMB achieves over 17% improvement in the prediction accuracy of ambiguous CCG categories, in comparison with the HMM.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "There is plenty of scope for further improvements.",
                "Overall, the discriminative C&C supertagger outperforms the FHMMs in all supervised settings.",
                "Despite this, the FHMMs are suited for estimating models with less supervision, such as from tag dictionaries alone and incorporating more informative prior distributions such as those in #TARGET_REF .",
                "This may make them more appropriate for developing CCGbanks for other languages and domains.",
                "Furthermore, Bayesian inference is modular and extensible, so our models could be supplemented by finding optimal values of the hyperparameters α (for POS tags) and β."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: There is plenty of scope for further improvements.\n sent1: Overall, the discriminative C&C supertagger outperforms the FHMMs in all supervised settings.\n sent2: Despite this, the FHMMs are suited for estimating models with less supervision, such as from tag dictionaries alone and incorporating more informative prior distributions such as those in #TARGET_REF .\n sent3: This may make them more appropriate for developing CCGbanks for other languages and domains.\n sent4: Furthermore, Bayesian inference is modular and extensible, so our models could be supplemented by finding optimal values of the hyperparameters α (for POS tags) and β.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "State-of-the-art POS taggers report accuracies in the range of 96−97%; our model FHMMB was comparable (95.35% for α = 0.05 and 94.41 for α = 1.0).",
                "The FHMMA model and the HMM model achieved 91% and 92.5% accuracy on POS tags, respectively.",
                "The accuracy of our HMM is lower than the performance of #TARGET_REF for supertags.",
                "We attribute this to better tag-specific smoothing in his model for emissions, compared to our use of a symmetric parameter for all tags.",
                "We stress that our interest here is in evaluating the advantage of joint inference over POS tags and supertags rather than direct supertag prediction while holding all other modeling considerations equal."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: State-of-the-art POS taggers report accuracies in the range of 96−97%; our model FHMMB was comparable (95.35% for α = 0.05 and 94.41 for α = 1.0).\n sent1: The FHMMA model and the HMM model achieved 91% and 92.5% accuracy on POS tags, respectively.\n sent2: The accuracy of our HMM is lower than the performance of #TARGET_REF for supertags.\n sent3: We attribute this to better tag-specific smoothing in his model for emissions, compared to our use of a symmetric parameter for all tags.\n sent4: We stress that our interest here is in evaluating the advantage of joint inference over POS tags and supertags rather than direct supertag prediction while holding all other modeling considerations equal.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Results of this experiment for α = 1.0, on ambiguous CCG categories, are tabulated in Table  5 (a).",
                "The results for α = 0.05 is shown in Table 6 (a).",
                "We also report the CCG accuracy values inclusive of unambiguous types in Table 5 (b) for α = 1.0 and Table 6 (b) respectively.",
                "The performance of the HMM model (31%) in Table 5 (a) without any frequency cut-off on the CCG categories, is comparable to the bitag HMM of #REF that uses variational Bayes EM (33%).",
                "Our complexity based initialization is not directly comparable to the results in #TARGET_REF because the values there are based on a weighted combination of complexity based initialization and modified transition priors based on the CCG formalism."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Results of this experiment for α = 1.0, on ambiguous CCG categories, are tabulated in Table  5 (a).\n sent1: The results for α = 0.05 is shown in Table 6 (a).\n sent2: We also report the CCG accuracy values inclusive of unambiguous types in Table 5 (b) for α = 1.0 and Table 6 (b) respectively.\n sent3: The performance of the HMM model (31%) in Table 5 (a) without any frequency cut-off on the CCG categories, is comparable to the bitag HMM of #REF that uses variational Bayes EM (33%).\n sent4: Our complexity based initialization is not directly comparable to the results in #TARGET_REF because the values there are based on a weighted combination of complexity based initialization and modified transition priors based on the CCG formalism.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "The performance of the HMM model (31%) in Table 5 (a) without any frequency cut-off on the CCG categories, is comparable to the bitag HMM of #REF that uses variational Bayes EM (33%).",
                "Our complexity based initialization is not directly comparable to the results in #REF because the values there are based on a weighted combination of complexity based initialization and modified transition priors based on the CCG formalism.",
                "However, it is encouraging to see that when there is no cut-off based filtering of the categories, FHMMB (47.98%) greatly outperforms the HMM-EM model of #TARGET_REF .",
                "It is however, quite short of the 56.1% accuracy achieved by the model of #REF that uses grammar informed initialization (combination of category based initialization along with category transition rules).",
                "Without any frequency cut-off on CCG categories, FHMMB achieves over 17% improvement in the prediction accuracy of ambiguous CCG categories, in comparison with the HMM."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: The performance of the HMM model (31%) in Table 5 (a) without any frequency cut-off on the CCG categories, is comparable to the bitag HMM of #REF that uses variational Bayes EM (33%).\n sent1: Our complexity based initialization is not directly comparable to the results in #REF because the values there are based on a weighted combination of complexity based initialization and modified transition priors based on the CCG formalism.\n sent2: However, it is encouraging to see that when there is no cut-off based filtering of the categories, FHMMB (47.98%) greatly outperforms the HMM-EM model of #TARGET_REF .\n sent3: It is however, quite short of the 56.1% accuracy achieved by the model of #REF that uses grammar informed initialization (combination of category based initialization along with category transition rules).\n sent4: Without any frequency cut-off on CCG categories, FHMMB achieves over 17% improvement in the prediction accuracy of ambiguous CCG categories, in comparison with the HMM.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "However, recall that frequency based filtering of categories is a strong form of supervision that we use here only as an oracle and which one could not expect to have in real world tag dictionaries.",
                "The POS accuracies in these experiments were 83.5-85%, 84.5-86.2% and 78.3-78.4% for models FHMMB, FHMMA and HMM respectively (without any frequency cut-off).",
                "In the weakly supervised setting, the choice of the transition prior α of 0.05 lead to severe degradation in the prediction accuracy of CCG tags.",
                "Unlike POS tagging, where a symmetric transition prior of α = 0.05 captured the sparsity of the tag transition distribution , in supertagging the transition priors are asymmetric.",
                "We expect that CCG transition rules #TARGET_REF when encoded as category specific transition priors, will lead to better performance with the FHMMs."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: However, recall that frequency based filtering of categories is a strong form of supervision that we use here only as an oracle and which one could not expect to have in real world tag dictionaries.\n sent1: The POS accuracies in these experiments were 83.5-85%, 84.5-86.2% and 78.3-78.4% for models FHMMB, FHMMA and HMM respectively (without any frequency cut-off).\n sent2: In the weakly supervised setting, the choice of the transition prior α of 0.05 lead to severe degradation in the prediction accuracy of CCG tags.\n sent3: Unlike POS tagging, where a symmetric transition prior of α = 0.05 captured the sparsity of the tag transition distribution , in supertagging the transition priors are asymmetric.\n sent4: We expect that CCG transition rules #TARGET_REF when encoded as category specific transition priors, will lead to better performance with the FHMMs.\n",
        "output": "{\"label\": [null], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "This paper follows the work of #REF , #TARGET_REF and #REF .",
                "#REF uses FHMMs for jointly labeling the POS and NP chunk tags for the CoNLL2000 dataset (#REF) .",
                "His is a fully supervised model for a simpler task.",
                "We address the harder problem of supertagging in this paper and especially in the weakly supervised setting, with FHMMs.",
                "#REF uses a Bayesian tritag HMM (BHMM) for POS tagging and considers three different scenarios: (1) a weakly supervised setting with fixed hyperparameters α and β, (2) hyper parameter inference (learning the optimal values for α and β) and (3) hyper parameter inference with varying corpus size and dictionary knowledge."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: This paper follows the work of #REF , #TARGET_REF and #REF .\n sent1: #REF uses FHMMs for jointly labeling the POS and NP chunk tags for the CoNLL2000 dataset (#REF) .\n sent2: His is a fully supervised model for a simpler task.\n sent3: We address the harder problem of supertagging in this paper and especially in the weakly supervised setting, with FHMMs.\n sent4: #REF uses a Bayesian tritag HMM (BHMM) for POS tagging and considers three different scenarios: (1) a weakly supervised setting with fixed hyperparameters α and β, (2) hyper parameter inference (learning the optimal values for α and β) and (3) hyper parameter inference with varying corpus size and dictionary knowledge.\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Although this annotation was performed by the first author only, we know from previous work that similar styles of annotation can achieve acceptable interannotator agreement (#REF ).",
                "An example annotation for #REF is given in Figure  2 , where the first column shows the line number and the second one shows the class label.",
                "To compare our work with #TARGET_REF , we also applied a three-class annotation scheme.",
                "In this method of annotation, we merge the citation context into a single sentence.",
                "Since the context introduces more than one sentiment per citation, we marked the citation sentiment with the last sentiment mentioned in the context window as this is pragmatically most likely to be the real intention (#REF) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Although this annotation was performed by the first author only, we know from previous work that similar styles of annotation can achieve acceptable interannotator agreement (#REF ).\n sent1: An example annotation for #REF is given in Figure  2 , where the first column shows the line number and the second one shows the class label.\n sent2: To compare our work with #TARGET_REF , we also applied a three-class annotation scheme.\n sent3: In this method of annotation, we merge the citation context into a single sentence.\n sent4: Since the context introduces more than one sentiment per citation, we marked the citation sentiment with the last sentiment mentioned in the context window as this is pragmatically most likely to be the real intention (#REF) .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The dependency triplets are constructed by merging the relation, governor and dependent in a single string, for instance, the relation nsubj(failed, method) is represented as nsubj failed method .",
                "This setup has been shown to produce good results earlier as well (#REF; #TARGET_REF) .",
                "The first set of experiments focuses on simultaneous detection of sentiment and context sentences.",
                "For this purpose, we use the four-class annotated corpus described earlier.",
                "While the original annotations were performed for a window of length 4, we also experiment with asymmetrical windows of l sentences preceding the citation and r sentences succeeding it."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The dependency triplets are constructed by merging the relation, governor and dependent in a single string, for instance, the relation nsubj(failed, method) is represented as nsubj failed method .\n sent1: This setup has been shown to produce good results earlier as well (#REF; #TARGET_REF) .\n sent2: The first set of experiments focuses on simultaneous detection of sentiment and context sentences.\n sent3: For this purpose, we use the four-class annotated corpus described earlier.\n sent4: While the original annotations were performed for a window of length 4, we also experiment with asymmetrical windows of l sentences preceding the citation and r sentences succeeding it.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "While different schemes have been proposed for annotating citations according to their function (#REF; #REF; #REF) , the only recent work on citation sentiment detection using a relatively large corpus is by #TARGET_REF .",
                "However, this work does not handle citation context.",
                "#REF proposed a system to attach sentiment information to the citation links between biomedical papers by using existing semantic lexical resources.",
                "A common approach for sentiment detection is to use a labelled lexicon to score sentences (#REF; #REF; #REF) .",
                "However, such approaches have been found to be highly topic dependent (Engström, 2004; #REF; #REF) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: While different schemes have been proposed for annotating citations according to their function (#REF; #REF; #REF) , the only recent work on citation sentiment detection using a relatively large corpus is by #TARGET_REF .\n sent1: However, this work does not handle citation context.\n sent2: #REF proposed a system to attach sentiment information to the citation links between biomedical papers by using existing semantic lexical resources.\n sent3: A common approach for sentiment detection is to use a labelled lexicon to score sentences (#REF; #REF; #REF) .\n sent4: However, such approaches have been found to be highly topic dependent (Engström, 2004; #REF; #REF) .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Table 2 : Results for joint context and sentiment detection.",
                "Because of the skewed class distribution, we use both the F macro and F micro scores with 10-fold cross-validation.",
                "The baseline score, shown in bold, is obtained with no context window and is comparable to the results reported by #TARGET_REF .",
                "However, we can observe that the F scores decrease as more context is introduced.",
                "This may be attributed to the increase in the vocabulary size of the n-grams and a consequent reduction in the discriminating power of the decision boundaries."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: Table 2 : Results for joint context and sentiment detection.\n sent1: Because of the skewed class distribution, we use both the F macro and F micro scores with 10-fold cross-validation.\n sent2: The baseline score, shown in bold, is obtained with no context window and is comparable to the results reported by #TARGET_REF .\n sent3: However, we can observe that the F scores decrease as more context is introduced.\n sent4: This may be attributed to the increase in the vocabulary size of the n-grams and a consequent reduction in the discriminating power of the decision boundaries.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "These results show that the task of jointly detecting sentiment and context is a hard problem.",
                "For our second set of experiments, we use the three-class annotation scheme.",
                "We merge the text of the sentences in the context windows as well as their dependency triplets to obtain the features.",
                "The results are reported in Table 3 with best results in bold.",
                "Although these results are not better than the context-less baseline, the reason might be data sparsity since existing work on citation sentiment analysis uses more data #TARGET_REF ."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: These results show that the task of jointly detecting sentiment and context is a hard problem.\n sent1: For our second set of experiments, we use the three-class annotation scheme.\n sent2: We merge the text of the sentences in the context windows as well as their dependency triplets to obtain the features.\n sent3: The results are reported in Table 3 with best results in bold.\n sent4: Although these results are not better than the context-less baseline, the reason might be data sparsity since existing work on citation sentiment analysis uses more data #TARGET_REF .\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "I can't help it, she said, pulling a long face, It's them pills I took, to bring it off, she said [158] [159] Her chatty tone and colloquial grammar and lexis distinguish her voice from many others in the poem, such as the formal and traditionally poetic voice of a narrator that recurs many times in the poem:",
                "Above the antique mantel was displayed As though a window gave upon the sylvan scene The change of #REF [98] [99] Although the stylistic contrasts between these and other voices are clear to many readers, Eliot does not explicitly mark the transitions, nor is it obvious when a voice has reappeared.",
                "Our previous work focused on only the segmentation part of the voice identification task #TARGET_REF .",
                "Here, we instead assume an initial segmentation and then try to create clusters corresponding to segments of the The Waste Land which are spoken by the same voice.",
                "Of particular interest is the influence of the initial segmentation on the success of this downstream task."
            ],
            "label": [
                "BACKGROUND",
                "MOTIVATION"
            ]
        },
        "input": "sent0: I can't help it, she said, pulling a long face, It's them pills I took, to bring it off, she said [158] [159] Her chatty tone and colloquial grammar and lexis distinguish her voice from many others in the poem, such as the formal and traditionally poetic voice of a narrator that recurs many times in the poem:\n sent1: Above the antique mantel was displayed As though a window gave upon the sylvan scene The change of #REF [98] [99] Although the stylistic contrasts between these and other voices are clear to many readers, Eliot does not explicitly mark the transitions, nor is it obvious when a voice has reappeared.\n sent2: Our previous work focused on only the segmentation part of the voice identification task #TARGET_REF .\n sent3: Here, we instead assume an initial segmentation and then try to create clusters corresponding to segments of the The Waste Land which are spoken by the same voice.\n sent4: Of particular interest is the influence of the initial segmentation on the success of this downstream task.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"MOTIVATION\"], \"context\": [\"sent2\", \"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "With respect to novels, the work of #REF is very relevant; they used principal components analysis of lexical frequency to discriminate different voices and narrative styles in sections of Ulysses by James Joyce.",
                "Clustering techniques have been applied to literature in general; for instance, #REF clustered novels according to style, and recent work in distinguishing two authors of sections of the Bible (#REF) relies crucially on an initial clustering which is bootstrapped into a supervised classifier which is applied to segments.",
                "Beyond literature, the tasks of stylistic inconsistency detection (#REF; #REF) and intrinsic (unsupervised) plagiarism detection (#REF) are very closely related to our interests here, though in such tasks usually only two authors are posited; more general kinds of authorship identification (#REF ) may include many more authors, though some form of supervision (i.e. training data) is usually assumed.",
                "Our work here is built on our earlier work #TARGET_REF .",
                "Our segmentation model for The Waste Land was based on a stylistic change curve whose values are the distance between stylistic feature vectors derived from 50 token spans on either side of each point (spaces between tokens) in the text; the local maxima of this curve represent likely voice switches."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: With respect to novels, the work of #REF is very relevant; they used principal components analysis of lexical frequency to discriminate different voices and narrative styles in sections of Ulysses by James Joyce.\n sent1: Clustering techniques have been applied to literature in general; for instance, #REF clustered novels according to style, and recent work in distinguishing two authors of sections of the Bible (#REF) relies crucially on an initial clustering which is bootstrapped into a supervised classifier which is applied to segments.\n sent2: Beyond literature, the tasks of stylistic inconsistency detection (#REF; #REF) and intrinsic (unsupervised) plagiarism detection (#REF) are very closely related to our interests here, though in such tasks usually only two authors are posited; more general kinds of authorship identification (#REF ) may include many more authors, though some form of supervision (i.e. training data) is usually assumed.\n sent3: Our work here is built on our earlier work #TARGET_REF .\n sent4: Our segmentation model for The Waste Land was based on a stylistic change curve whose values are the distance between stylistic feature vectors derived from 50 token spans on either side of each point (spaces between tokens) in the text; the local maxima of this curve represent likely voice switches.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Our approach to voice identification in The Waste Land consists first of identifying the boundaries of voice spans #TARGET_REF .",
                "Given a segmentation of the text, we consider each span as a data point in a clustering problem.",
                "The elements of the vector correspond to the best feature set from the segmentation task, with the rationale that features which were useful for detecting changes in style should also be useful for identifying stylistic similarities.",
                "Our features therefore include: a collection of readability metrics (including word length), frequency of punctuation, line breaks, and various parts-ofspeech, lexical density, average frequency in a large external corpus (#REF) , lexiconbased sentiment metrics using SentiWordNet (#REF) , formality score (#REF) , and, perhaps most notably, the centroid of 20-dimensional distributional vectors built using latent semantic analysis (#REF), reflecting the use of words in a large web corpus (#REF) ; in previous work (#REF) , we established that such vectors contain useful stylistic information about the English lexicon (including rare words that appear only occasionally in such a corpus), and indeed LSA vectors were the single most promising feature type for segmentation.",
                "For a more detailed discussion of the feature set, see #REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Our approach to voice identification in The Waste Land consists first of identifying the boundaries of voice spans #TARGET_REF .\n sent1: Given a segmentation of the text, we consider each span as a data point in a clustering problem.\n sent2: The elements of the vector correspond to the best feature set from the segmentation task, with the rationale that features which were useful for detecting changes in style should also be useful for identifying stylistic similarities.\n sent3: Our features therefore include: a collection of readability metrics (including word length), frequency of punctuation, line breaks, and various parts-ofspeech, lexical density, average frequency in a large external corpus (#REF) , lexiconbased sentiment metrics using SentiWordNet (#REF) , formality score (#REF) , and, perhaps most notably, the centroid of 20-dimensional distributional vectors built using latent semantic analysis (#REF), reflecting the use of words in a large web corpus (#REF) ; in previous work (#REF) , we established that such vectors contain useful stylistic information about the English lexicon (including rare words that appear only occasionally in such a corpus), and indeed LSA vectors were the single most promising feature type for segmentation.\n sent4: For a more detailed discussion of the feature set, see #REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\", \"sent1\", \"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Our features therefore include: a collection of readability metrics (including word length), frequency of punctuation, line breaks, and various parts-ofspeech, lexical density, average frequency in a large external corpus (#REF) , lexiconbased sentiment metrics using SentiWordNet (#REF) , formality score (#REF) , and, perhaps most notably, the centroid of 20-dimensional distributional vectors built using latent semantic analysis (#REF), reflecting the use of words in a large web corpus (#REF) ; in previous work (#REF) , we established that such vectors contain useful stylistic information about the English lexicon (including rare words that appear only occasionally in such a corpus), and indeed LSA vectors were the single most promising feature type for segmentation.",
                "For a more detailed discussion of the feature set, see #TARGET_REF .",
                "All the features are normalized to a mean of zero and a standard deviation of 1.",
                "For clustering, we use a slightly modified version of the popular k-means algorithm (#REF) .",
                "Briefly, k-means assigns points to a cluster based on their proximity to the k cluster centroids, which are initialized to randomly chosen points from the data and then iteratively refined until convergence, which in our case was defined as a change of less than 0.0001 in the position of each centroid during one iteration."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Our features therefore include: a collection of readability metrics (including word length), frequency of punctuation, line breaks, and various parts-ofspeech, lexical density, average frequency in a large external corpus (#REF) , lexiconbased sentiment metrics using SentiWordNet (#REF) , formality score (#REF) , and, perhaps most notably, the centroid of 20-dimensional distributional vectors built using latent semantic analysis (#REF), reflecting the use of words in a large web corpus (#REF) ; in previous work (#REF) , we established that such vectors contain useful stylistic information about the English lexicon (including rare words that appear only occasionally in such a corpus), and indeed LSA vectors were the single most promising feature type for segmentation.\n sent1: For a more detailed discussion of the feature set, see #TARGET_REF .\n sent2: All the features are normalized to a mean of zero and a standard deviation of 1.\n sent3: For clustering, we use a slightly modified version of the popular k-means algorithm (#REF) .\n sent4: Briefly, k-means assigns points to a cluster based on their proximity to the k cluster centroids, which are initialized to randomly chosen points from the data and then iteratively refined until convergence, which in our case was defined as a change of less than 0.0001 in the position of each centroid during one iteration.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The Even segmentation should be viewed as the baseline for segmentation, and the Gold segmentation an \"oracle\" representing an upper bound on segmentation performance.",
                "For the automatic segmentation model, we use the settings from #TARGET_REF .",
                "We also compare three possible clusterings for each segmentation: no clustering at all (Initial), that is, we assume that each segment is a new voice; k-means clustering (k-means), as outlined above; and random clustering (Random), in which we randomly assign each voice to a cluster.",
                "For these latter two methods, which both have a random component, we averaged our metrics over 50 runs.",
                "Random and Initial are here, of course, to provide baselines for judging the effectiveness of k-means clustering model."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The Even segmentation should be viewed as the baseline for segmentation, and the Gold segmentation an \"oracle\" representing an upper bound on segmentation performance.\n sent1: For the automatic segmentation model, we use the settings from #TARGET_REF .\n sent2: We also compare three possible clusterings for each segmentation: no clustering at all (Initial), that is, we assume that each segment is a new voice; k-means clustering (k-means), as outlined above; and random clustering (Random), in which we randomly assign each voice to a cluster.\n sent3: For these latter two methods, which both have a random component, we averaged our metrics over 50 runs.\n sent4: Random and Initial are here, of course, to provide baselines for judging the effectiveness of k-means clustering model.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Non-parametric Bayesian methods have been successfully applied to directly learn phrase pairs from a bilingual corpus with little or no dependence on word alignments (#REF; #REF) .",
                "Because such approaches directly learn a generative model over phrase pairs, they are theoretically preferable to the standard heuristics for extracting the phrase pairs from the many-to-one word-level alignments produced by the IBM series models (#REF) or the Hidden Markov Model (HMM) (#REF) .",
                "We wish to apply this direct, Bayesian approach to learn better translation rules for syntaxbased statistical MT (SSMT), by which we specifically refer to MT systems using Tree-to-String (TTS) translation templates derived from syntax trees (#REF; #REF; #TARGET_REF; #REF) , as opposed to formally syntactic systems such as Hiero (#REF) .",
                "The stumbling block preventing us from taking this approach is the extremely large space of possible TTS templates when no word alignments are given.",
                "Given a sentence pair and syntax tree over one side, there are an exponential number of potential TTS templates and a polynomial number of phrase pairs."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Non-parametric Bayesian methods have been successfully applied to directly learn phrase pairs from a bilingual corpus with little or no dependence on word alignments (#REF; #REF) .\n sent1: Because such approaches directly learn a generative model over phrase pairs, they are theoretically preferable to the standard heuristics for extracting the phrase pairs from the many-to-one word-level alignments produced by the IBM series models (#REF) or the Hidden Markov Model (HMM) (#REF) .\n sent2: We wish to apply this direct, Bayesian approach to learn better translation rules for syntaxbased statistical MT (SSMT), by which we specifically refer to MT systems using Tree-to-String (TTS) translation templates derived from syntax trees (#REF; #REF; #TARGET_REF; #REF) , as opposed to formally syntactic systems such as Hiero (#REF) .\n sent3: The stumbling block preventing us from taking this approach is the extremely large space of possible TTS templates when no word alignments are given.\n sent4: Given a sentence pair and syntax tree over one side, there are an exponential number of potential TTS templates and a polynomial number of phrase pairs.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "LHSN, as shown by #TARGET_REF , cannot accurately restore the true conditional probabilities of the target sentences given the source sentences in the training corpus.",
                "This indicates that LHSN is not good at predicting unseen sentences or at translating new sentences.",
                "But this deficiency does not affect its ability to estimate the expected counts of the TTS templates, because the posteriors of the TTS templates only depend on the comparative probabilities of the different derivations of a training instance (a pair of tree and string).",
                "In fact, as we show in Section 4, LHSN is better than ROOTN in liberating smaller TTS templates out of the big templates, since it is less biased to the big templates in the EM training.",
                "4 Because the two normalization methods have their 4 Based on LHSN, the difference between the probability of a big Template and the product of the probabilities of E-step: for all pair of syntax tree T and target string S do for all TTS Template t do EC(t)+ = (#REF ) is is used in our system to speed up the training process, similar to #REF ."
            ],
            "label": [
                "USE",
                "BACKGROUND"
            ]
        },
        "input": "sent0: LHSN, as shown by #TARGET_REF , cannot accurately restore the true conditional probabilities of the target sentences given the source sentences in the training corpus.\n sent1: This indicates that LHSN is not good at predicting unseen sentences or at translating new sentences.\n sent2: But this deficiency does not affect its ability to estimate the expected counts of the TTS templates, because the posteriors of the TTS templates only depend on the comparative probabilities of the different derivations of a training instance (a pair of tree and string).\n sent3: In fact, as we show in Section 4, LHSN is better than ROOTN in liberating smaller TTS templates out of the big templates, since it is less biased to the big templates in the EM training.\n sent4: 4 Because the two normalization methods have their 4 Based on LHSN, the difference between the probability of a big Template and the product of the probabilities of E-step: for all pair of syntax tree T and target string S do for all TTS Template t do EC(t)+ = (#REF ) is is used in our system to speed up the training process, similar to #REF .\n",
        "output": "{\"label\": [\"USE\", \"BACKGROUND\"], \"context\": [\"sent0\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "There are two ways that TTS templates are commonly used in machine translation.",
                "The first is synchronous parsing #TARGET_REF; #REF) , where TTS templates are used to construct synchronous parse trees for an input sentence, and the translations will be generated once the synchronous trees are built up.",
                "The other way is the TTS transducer (#REF; #REF) , where TTS templates are used just as their name indicates: to transform a source parse tree (or forest) into the proper target string.",
                "Since synchronous parsing considers all possible synchronous parse trees of the source sentence, it is less constrained than TTS transducers and hence requires more computational power.",
                "In this paper, we use a TTS transducer to test the performance of different TTS templates, but our techniques could also be applied to SSMT systems based on synchronous parsing."
            ],
            "label": [
                "BACKGROUND",
                "SIMILARITY"
            ]
        },
        "input": "sent0: There are two ways that TTS templates are commonly used in machine translation.\n sent1: The first is synchronous parsing #TARGET_REF; #REF) , where TTS templates are used to construct synchronous parse trees for an input sentence, and the translations will be generated once the synchronous trees are built up.\n sent2: The other way is the TTS transducer (#REF; #REF) , where TTS templates are used just as their name indicates: to transform a source parse tree (or forest) into the proper target string.\n sent3: Since synchronous parsing considers all possible synchronous parse trees of the source sentence, it is less constrained than TTS transducers and hence requires more computational power.\n sent4: In this paper, we use a TTS transducer to test the performance of different TTS templates, but our techniques could also be applied to SSMT systems based on synchronous parsing.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"SIMILARITY\"], \"context\": [\"sent1\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "The LHS-based normalization (LHSN) (#REF; #REF) , corresponds to the generative process where the source syntax subtree is first generated, and then the target string is generated given the source syntax subtree.",
                "The other one is normalization based on the root of the LHS (ROOTN) #TARGET_REF , corresponding to the generative process where, given the root of the syntax subtree, the LHS syntax subtree and the RHS string are generated simultaneously.",
                "By omitting the decomposition probability in the LHS-based generative model, the two generative models share the same formula for computing the probability of a training instance:",
                "where T and S denote the source syntax tree and target string respectively, R denotes the decomposition of (T, S), and t denotes the TTS template.",
                "The expected counts of the TTS templates can then be efficiently computed using an inside-outsidelike dynamic programming algorithm (#REF) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The LHS-based normalization (LHSN) (#REF; #REF) , corresponds to the generative process where the source syntax subtree is first generated, and then the target string is generated given the source syntax subtree.\n sent1: The other one is normalization based on the root of the LHS (ROOTN) #TARGET_REF , corresponding to the generative process where, given the root of the syntax subtree, the LHS syntax subtree and the RHS string are generated simultaneously.\n sent2: By omitting the decomposition probability in the LHS-based generative model, the two generative models share the same formula for computing the probability of a training instance:\n sent3: where T and S denote the source syntax tree and target string respectively, R denotes the decomposition of (T, S), and t denotes the TTS template.\n sent4: The expected counts of the TTS templates can then be efficiently computed using an inside-outsidelike dynamic programming algorithm (#REF) .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "To keep the number of TTS templates to a manageable scale, only the non-decomposable TTS templates are generated.",
                "This algorithm is referred to as GHKM (#REF) and is widely used in SSMT systems #TARGET_REF; #REF; #REF) .",
                "The word alignment used in GHKM is usually computed independent of the syntactic structure, and as #REF and #REF have noted,",
                "Ch-En En-Ch Union Heuristic 28.6% 33.0% 45.9% 20.1% Table 2 : In the selected big templates, the distribution of words in the templates of different sizes, which are measured based on the number of symbols in their RHSs is not the best for SSMT systems.",
                "In fact, noisy word alignments cause more damage to a SSMT system than to a phrase based SMT system, because the TTS templates can only be derived from tree constituents."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: To keep the number of TTS templates to a manageable scale, only the non-decomposable TTS templates are generated.\n sent1: This algorithm is referred to as GHKM (#REF) and is widely used in SSMT systems #TARGET_REF; #REF; #REF) .\n sent2: The word alignment used in GHKM is usually computed independent of the syntactic structure, and as #REF and #REF have noted,\n sent3: Ch-En En-Ch Union Heuristic 28.6% 33.0% 45.9% 20.1% Table 2 : In the selected big templates, the distribution of words in the templates of different sizes, which are measured based on the number of symbols in their RHSs is not the best for SSMT systems.\n sent4: In fact, noisy word alignments cause more damage to a SSMT system than to a phrase based SMT system, because the TTS templates can only be derived from tree constituents.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Another task similar to the WSI is the unsupervised author name disambiguation (UAND) task (#REF) , where it aims to automatically find different authors, instead of words, with the same name.",
                "In this paper, we consider a latent variable modeling approach to WSI problem as it is proven to be more effective than other approaches (Chang, Pei, and #REF; #REF) .",
                "Specifically, we look into methods based on Latent Dirichlet Allocation (LDA) (Blei, Ng, and #REF) , a topic modeling method that automatically discovers the topics underlying a set of documents using Dirichlet priors to infer the multinomial distribution over words and topics.",
                "LDA naturally answers two of the three main problems mentioned above, i.e. (C1) and (C2), of the WSI task (#REF) .",
                "However, it is not flexible with regards to (C3), or the sense granularity problem, as it requires the users to specify the number of senses: Current systems ( #TARGET_REF; Chang, Pei, and #REF) required to set the number of senses to a small number (set to 3 or 5 in the literature) to get a good accuracy, however many words may have a large number of senses, e.g. play in Figure 1 ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Another task similar to the WSI is the unsupervised author name disambiguation (UAND) task (#REF) , where it aims to automatically find different authors, instead of words, with the same name.\n sent1: In this paper, we consider a latent variable modeling approach to WSI problem as it is proven to be more effective than other approaches (Chang, Pei, and #REF; #REF) .\n sent2: Specifically, we look into methods based on Latent Dirichlet Allocation (LDA) (Blei, Ng, and #REF) , a topic modeling method that automatically discovers the topics underlying a set of documents using Dirichlet priors to infer the multinomial distribution over words and topics.\n sent3: LDA naturally answers two of the three main problems mentioned above, i.e. (C1) and (C2), of the WSI task (#REF) .\n sent4: However, it is not flexible with regards to (C3), or the sense granularity problem, as it requires the users to specify the number of senses: Current systems ( #TARGET_REF; Chang, Pei, and #REF) required to set the number of senses to a small number (set to 3 or 5 in the literature) to get a good accuracy, however many words may have a large number of senses, e.g. play in Figure 1 .\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "First, LDA tries to give instance assignments to all senses even when it is unnecessary.",
                "For example, when the number of senses S is set to 10, the model tries to assign all the senses to all instances even when the original number of senses of a target word is 3.",
                "LDA extensions ( #TARGET_REF; Chang, Pei, and #REF) mitigated this problem by setting S to a small number (e.g. 3 or 5).",
                "However, this is not a good solution because there are many words with more than five senses.",
                "Second, LDA and its extensions do not consider the existence of fine-grained senses."
            ],
            "label": [
                "MOTIVATION",
                "BACKGROUND"
            ]
        },
        "input": "sent0: First, LDA tries to give instance assignments to all senses even when it is unnecessary.\n sent1: For example, when the number of senses S is set to 10, the model tries to assign all the senses to all instances even when the original number of senses of a target word is 3.\n sent2: LDA extensions ( #TARGET_REF; Chang, Pei, and #REF) mitigated this problem by setting S to a small number (e.g. 3 or 5).\n sent3: However, this is not a good solution because there are many words with more than five senses.\n sent4: Second, LDA and its extensions do not consider the existence of fine-grained senses.\n",
        "output": "{\"label\": [\"MOTIVATION\", \"BACKGROUND\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The main weakness of LDA when used on WSI task is the sense granularity problem.",
                "Recent models such as HC (Chang, Pei, and #REF) and STM (#REF) mitigated this problem by tuning the number of senses hyperparameter S to minimize the error.",
                "However, such tuning, often empirically set to a small number such as S = 3 ( #TARGET_REF) , fails to infer varying number of senses of words, especially for words with a higher number of senses.",
                "Nonparametric models such as HDP and BNP-HC Chang, Pei, and #REF) claim to automatically induce different S for each word.",
                "However, as shown in the results in Table 2 , the estimated S is far from the actual number of senses and both models are ineffective."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: The main weakness of LDA when used on WSI task is the sense granularity problem.\n sent1: Recent models such as HC (Chang, Pei, and #REF) and STM (#REF) mitigated this problem by tuning the number of senses hyperparameter S to minimize the error.\n sent2: However, such tuning, often empirically set to a small number such as S = 3 ( #TARGET_REF) , fails to infer varying number of senses of words, especially for words with a higher number of senses.\n sent3: Nonparametric models such as HDP and BNP-HC Chang, Pei, and #REF) claim to automatically induce different S for each word.\n sent4: However, as shown in the results in Table 2 , the estimated S is far from the actual number of senses and both models are ineffective.\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Latent variable models such as LDA (Blei, Ng, and #REF) are used to induce the word sense of a target word after rigorous preprocessing and feature extraction (LDA, Spectral) (#REF).",
                "More recent models introduced a latent variable for the sense of a word, with the assumption that a sense has multiple concepts (HC, HC+Zipf) (Chang, Pei, and #REF) and that topics and senses should be inferred jointly (STM) ( #TARGET_REF) .",
                "In this paper, we also use a separate sense latent variable, however we show boost in performance by representing it with more versatility and by incorporating the use of targetneighbor pairs.",
                "HC was also extended to a nonparametric model (BNP-HC) (#REF ) in order to automatically set the number of senses of a word, providing flexibility to the sense granularity (Yao and Van #REF; #REF; .",
                "In our experiments, we show that the sense granularity induced from nonparametric models are incorrect making the models less effective."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Latent variable models such as LDA (Blei, Ng, and #REF) are used to induce the word sense of a target word after rigorous preprocessing and feature extraction (LDA, Spectral) (#REF).\n sent1: More recent models introduced a latent variable for the sense of a word, with the assumption that a sense has multiple concepts (HC, HC+Zipf) (Chang, Pei, and #REF) and that topics and senses should be inferred jointly (STM) ( #TARGET_REF) .\n sent2: In this paper, we also use a separate sense latent variable, however we show boost in performance by representing it with more versatility and by incorporating the use of targetneighbor pairs.\n sent3: HC was also extended to a nonparametric model (BNP-HC) (#REF ) in order to automatically set the number of senses of a word, providing flexibility to the sense granularity (Yao and Van #REF; #REF; .\n sent4: In our experiments, we show that the sense granularity induced from nonparametric models are incorrect making the models less effective.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "HC was also extended to a nonparametric model (BNP-HC) (#REF ) in order to automatically set the number of senses of a word, providing flexibility to the sense granularity (Yao and Van #REF; #REF; .",
                "In our experiments, we show that the sense granularity induced from nonparametric models are incorrect making the models less effective.",
                "Recent inclusions to the WSI models are neural-based dense distributional representation models.",
                "STM also used word embeddings (#REF) to assign similarity weights during inference (STM+w2v) ( #TARGET_REF) .",
                "Existing sense embeddings are also used to perform word sense induction (CRP-PPMI, SE-WSI-fix, WG, DIVE) (#REF; #REF; #REF )."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: HC was also extended to a nonparametric model (BNP-HC) (#REF ) in order to automatically set the number of senses of a word, providing flexibility to the sense granularity (Yao and Van #REF; #REF; .\n sent1: In our experiments, we show that the sense granularity induced from nonparametric models are incorrect making the models less effective.\n sent2: Recent inclusions to the WSI models are neural-based dense distributional representation models.\n sent3: STM also used word embeddings (#REF) to assign similarity weights during inference (STM+w2v) ( #TARGET_REF) .\n sent4: Existing sense embeddings are also used to perform word sense induction (CRP-PPMI, SE-WSI-fix, WG, DIVE) (#REF; #REF; #REF ).\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "First, we introduce a new latent variable, apart from the topic latent variable, to represent word senses.",
                "Previous works also attempted to introduce a separate sense latent variable to generate all the words (Chang, Pei, and #REF) , or to generate only the neighboring words within a local context, decided by a strict user-specified window ( #TARGET_REF) .",
                "We improve by softening the strict local context assumption by introducing a switch variable which decides whether a word not in a local context should be generated by conditioning also on the sense latent variable.",
                "Our experiments show that our sense representation provides superior improvements from previous models.",
                "Second, we force the model to generate target-neighbor pairs at once in the local context, instead of generating words one by one."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: First, we introduce a new latent variable, apart from the topic latent variable, to represent word senses.\n sent1: Previous works also attempted to introduce a separate sense latent variable to generate all the words (Chang, Pei, and #REF) , or to generate only the neighboring words within a local context, decided by a strict user-specified window ( #TARGET_REF) .\n sent2: We improve by softening the strict local context assumption by introducing a switch variable which decides whether a word not in a local context should be generated by conditioning also on the sense latent variable.\n sent3: Our experiments show that our sense representation provides superior improvements from previous models.\n sent4: Second, we force the model to generate target-neighbor pairs at once in the local context, instead of generating words one by one.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The main weakness of LDA when used on WSI task is the sense granularity problem.",
                "Recent models such as HC (Chang, Pei, and #REF) and STM ( #TARGET_REF) mitigated this problem by tuning the number of senses hyperparameter S to minimize the error.",
                "However, such tuning, often empirically set to a small number such as S = 3 (#REF) , fails to infer varying number of senses of words, especially for words with a higher number of senses.",
                "Nonparametric models such as HDP and BNP-HC Chang, Pei, and #REF) claim to automatically induce different S for each word.",
                "However, as shown in the results in Table 2 , the estimated S is far from the actual number of senses and both models are ineffective."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The main weakness of LDA when used on WSI task is the sense granularity problem.\n sent1: Recent models such as HC (Chang, Pei, and #REF) and STM ( #TARGET_REF) mitigated this problem by tuning the number of senses hyperparameter S to minimize the error.\n sent2: However, such tuning, often empirically set to a small number such as S = 3 (#REF) , fails to infer varying number of senses of words, especially for words with a higher number of senses.\n sent3: Nonparametric models such as HDP and BNP-HC Chang, Pei, and #REF) claim to automatically induce different S for each word.\n sent4: However, as shown in the results in Table 2 , the estimated S is far from the actual number of senses and both models are ineffective.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Datasets and preprocessing We use two publicly available datasets: #REF Task 14 (#REF) For preprocessing, we do tokenization, lemmatization, and removing of symbols to build the word lists using Stanford CoreNLP (#REF) .",
                "We divide the word lists into two contexts: the local and global context.",
                "Following ( #TARGET_REF), we set the local context window to 10, with a maximum number of words of 21 (i.e. 10 words before and 10 words after).",
                "Other words are put into the global context.",
                "Note however that AutoSense has a less strict global/local context assumption as it treats some words in the global context as local depending on the switch variable."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Datasets and preprocessing We use two publicly available datasets: #REF Task 14 (#REF) For preprocessing, we do tokenization, lemmatization, and removing of symbols to build the word lists using Stanford CoreNLP (#REF) .\n sent1: We divide the word lists into two contexts: the local and global context.\n sent2: Following ( #TARGET_REF), we set the local context window to 10, with a maximum number of words of 21 (i.e. 10 words before and 10 words after).\n sent3: Other words are put into the global context.\n sent4: Note however that AutoSense has a less strict global/local context assumption as it treats some words in the global context as local depending on the switch variable.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Note however that AutoSense has a less strict global/local context assumption as it treats some words in the global context as local depending on the switch variable.",
                "Parameter setting We set the hyperparameters to α = 0.1, β = 0.01, γ = 0.3, following the conventional setup (#REF; Chemudugunta, Smyth, and #REF) .",
                "We arbitrarily set the number of senses to S = 15, and the number of topics T = 2S = 30, following ( #TARGET_REF) .",
                "We also include four other versions of our model: AutoSense −wp removes the target-neighbor pair constraint and transforms the local context to that of STM, AutoSense −sw removes the switch variable and transforms the global context to that of LDA, AutoSense s=X is a tuned and best version of the model, where the number of senses is tuned over a separate development set provided by the shared tasks and X is the tuned number of sense, different for each dataset, and AutoSense s=100 is the overestimated and worst version of the model, where we set the number of senses to an arbitrary large number, i.e. 100.",
                "We set the number of iterations to 2000 and run the Gibbs sampler."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Note however that AutoSense has a less strict global/local context assumption as it treats some words in the global context as local depending on the switch variable.\n sent1: Parameter setting We set the hyperparameters to α = 0.1, β = 0.01, γ = 0.3, following the conventional setup (#REF; Chemudugunta, Smyth, and #REF) .\n sent2: We arbitrarily set the number of senses to S = 15, and the number of topics T = 2S = 30, following ( #TARGET_REF) .\n sent3: We also include four other versions of our model: AutoSense −wp removes the target-neighbor pair constraint and transforms the local context to that of STM, AutoSense −sw removes the switch variable and transforms the global context to that of LDA, AutoSense s=X is a tuned and best version of the model, where the number of senses is tuned over a separate development set provided by the shared tasks and X is the tuned number of sense, different for each dataset, and AutoSense s=100 is the overestimated and worst version of the model, where we set the number of senses to an arbitrary large number, i.e. 100.\n sent4: We set the number of iterations to 2000 and run the Gibbs sampler.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We arbitrarily set the number of senses to S = 15, and the number of topics T = 2S = 30, following (#REF) .",
                "We also include four other versions of our model: AutoSense −wp removes the target-neighbor pair constraint and transforms the local context to that of STM, AutoSense −sw removes the switch variable and transforms the global context to that of LDA, AutoSense s=X is a tuned and best version of the model, where the number of senses is tuned over a separate development set provided by the shared tasks and X is the tuned number of sense, different for each dataset, and AutoSense s=100 is the overestimated and worst version of the model, where we set the number of senses to an arbitrary large number, i.e. 100.",
                "We set the number of iterations to 2000 and run the Gibbs sampler.",
                "Following the convention of previous works (#REF; #REF; #TARGET_REF) , we assume convergence when the number of iterations is high.",
                "However, due to the randomized nature of Gibbs sampling, we report the average scores over 5 runs of Gibbs sampling."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We arbitrarily set the number of senses to S = 15, and the number of topics T = 2S = 30, following (#REF) .\n sent1: We also include four other versions of our model: AutoSense −wp removes the target-neighbor pair constraint and transforms the local context to that of STM, AutoSense −sw removes the switch variable and transforms the global context to that of LDA, AutoSense s=X is a tuned and best version of the model, where the number of senses is tuned over a separate development set provided by the shared tasks and X is the tuned number of sense, different for each dataset, and AutoSense s=100 is the overestimated and worst version of the model, where we set the number of senses to an arbitrary large number, i.e. 100.\n sent2: We set the number of iterations to 2000 and run the Gibbs sampler.\n sent3: Following the convention of previous works (#REF; #REF; #TARGET_REF) , we assume convergence when the number of iterations is high.\n sent4: However, due to the randomized nature of Gibbs sampling, we report the average scores over 5 runs of Gibbs sampling.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF For the #REF dataset, we compare models using two unsupervised metrics: V-measure (V-M) and paired F-score (F-S).",
                "V-M favors a high number of senses (e.g. assigning one cluster per instance), while F-S favors a small number of senses (e.g. all instances in one cluster) (#REF) .",
                "In order to get a common ground for comparison, we do a geometric average AVG of both metrics, following ( #TARGET_REF) .",
                "Finally, we also report the absolute difference between the actual (3.85) and induced number of senses as δ(#S).",
                "We compare with seven other models: a) LDA on cooccurrence graphs (LDA) and b) spectral clustering on cooccurrence graphs (Spectral) as reported in (Goyal and  Hovy Results are shown in Table 2a , where AutoSense outperforms other competing models on AVG."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: #REF For the #REF dataset, we compare models using two unsupervised metrics: V-measure (V-M) and paired F-score (F-S).\n sent1: V-M favors a high number of senses (e.g. assigning one cluster per instance), while F-S favors a small number of senses (e.g. all instances in one cluster) (#REF) .\n sent2: In order to get a common ground for comparison, we do a geometric average AVG of both metrics, following ( #TARGET_REF) .\n sent3: Finally, we also report the absolute difference between the actual (3.85) and induced number of senses as δ(#S).\n sent4: We compare with seven other models: a) LDA on cooccurrence graphs (LDA) and b) spectral clustering on cooccurrence graphs (Spectral) as reported in (Goyal and  Hovy Results are shown in Table 2a , where AutoSense outperforms other competing models on AVG.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "F-BC gives preference to labelling all instances with the same sense, while F-NMI gives preference to labelling all instances with distinct senses.",
                "Therefore, computing the AVG of both metrics is also necessary in this experiment, for ease of comparison, as also suggested in ( #TARGET_REF) .",
                "We use seven baselines: a) lexical substitution method (AI-KU) and b) nonparametric HDP model (Unimelb) as reported in (#REF) , c) Sense-Topic Model STM, d) STM with word2vec weights (STM+w2v) as reported in (#REF) , e) Word Graph embeddings (WG), f) Distributional Inclusion Vector Embedding (DIVE) as reported in (#REF) , and g) Multi Context Continuous model MCC as reported in (#REF) .",
                "Results are shown in Table 2b .",
                "Among the models, all versions of AutoSense perform better than other models on AVG."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: F-BC gives preference to labelling all instances with the same sense, while F-NMI gives preference to labelling all instances with distinct senses.\n sent1: Therefore, computing the AVG of both metrics is also necessary in this experiment, for ease of comparison, as also suggested in ( #TARGET_REF) .\n sent2: We use seven baselines: a) lexical substitution method (AI-KU) and b) nonparametric HDP model (Unimelb) as reported in (#REF) , c) Sense-Topic Model STM, d) STM with word2vec weights (STM+w2v) as reported in (#REF) , e) Word Graph embeddings (WG), f) Distributional Inclusion Vector Embedding (DIVE) as reported in (#REF) , and g) Multi Context Continuous model MCC as reported in (#REF) .\n sent3: Results are shown in Table 2b .\n sent4: Among the models, all versions of AutoSense perform better than other models on AVG.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Therefore, computing the AVG of both metrics is also necessary in this experiment, for ease of comparison, as also suggested in (#REF) .",
                "We use seven baselines: a) lexical substitution method (AI-KU) and b) nonparametric HDP model (Unimelb) as reported in (#REF) , c) Sense-Topic Model STM, d) STM with word2vec weights (STM+w2v) as reported in ( #TARGET_REF) , e) Word Graph embeddings (WG), f) Distributional Inclusion Vector Embedding (DIVE) as reported in (#REF) , and g) Multi Context Continuous model MCC as reported in (#REF) .",
                "Results are shown in Table 2b .",
                "Among the models, all versions of AutoSense perform better than other models on AVG.",
                "The untuned AutoSense and AutoSense s=7 especially garner noticeable increase of 6.1% on fuzzy B-cubed metric from MCC, the previous best model."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Therefore, computing the AVG of both metrics is also necessary in this experiment, for ease of comparison, as also suggested in (#REF) .\n sent1: We use seven baselines: a) lexical substitution method (AI-KU) and b) nonparametric HDP model (Unimelb) as reported in (#REF) , c) Sense-Topic Model STM, d) STM with word2vec weights (STM+w2v) as reported in ( #TARGET_REF) , e) Word Graph embeddings (WG), f) Distributional Inclusion Vector Embedding (DIVE) as reported in (#REF) , and g) Multi Context Continuous model MCC as reported in (#REF) .\n sent2: Results are shown in Table 2b .\n sent3: Among the models, all versions of AutoSense perform better than other models on AVG.\n sent4: The untuned AutoSense and AutoSense s=7 especially garner noticeable increase of 6.1% on fuzzy B-cubed metric from MCC, the previous best model.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We also notice a big 6.0% decrease on the fuzzy B-cubed of AutoSense when the target-neighbor pair context is removed.",
                "This means that introducing the target-neighbor pair is crucial to the improvement of the model.",
                "Finally, the overestimated AutoSense model performs as well as the other AutoSense models, even outperforming all previous models on AVG, which proves the effectiveness of AutoSense even when s is set to a large value.",
                "For completeness, we also report STM with additional contexts, STM+actual and STM+ukWac ( #TARGET_REF) , where they used the actual additional contexts from the original data and semantically similar contexts from ukWac, respectively, as additional global context.",
                "With the performance gain we achieved, AutoSense without additional context can perform comparably to models with additional contexts: Our model greatly outperforms these models on the Sense Word distribution   #Docs  1  hotel tour tourist summer flight  22  2  month ticket available performance  3  3 guest office stateroom class suite 3 * advance overseas line popular japan 0 * email day buy unable tour 0 * sort basic tour time 0 Table 3 : Six of the 15 senses of the target verb book using AutoSense with S = 15."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We also notice a big 6.0% decrease on the fuzzy B-cubed of AutoSense when the target-neighbor pair context is removed.\n sent1: This means that introducing the target-neighbor pair is crucial to the improvement of the model.\n sent2: Finally, the overestimated AutoSense model performs as well as the other AutoSense models, even outperforming all previous models on AVG, which proves the effectiveness of AutoSense even when s is set to a large value.\n sent3: For completeness, we also report STM with additional contexts, STM+actual and STM+ukWac ( #TARGET_REF) , where they used the actual additional contexts from the original data and semantically similar contexts from ukWac, respectively, as additional global context.\n sent4: With the performance gain we achieved, AutoSense without additional context can perform comparably to models with additional contexts: Our model greatly outperforms these models on the Sense Word distribution   #Docs  1  hotel tour tourist summer flight  22  2  month ticket available performance  3  3 guest office stateroom class suite 3 * advance overseas line popular japan 0 * email day buy unable tour 0 * sort basic tour time 0 Table 3 : Six of the 15 senses of the target verb book using AutoSense with S = 15.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Incorporating the target-neighbor pairs helps the model differentiates both into fine-grained senses book: arrange for and reserve in advance and book: engage for a performance.",
                "We compare the competing models quantitatively on how they correctly detect the actual number of sense clusters using cluster error, which is the mean absolute error between the detected number and the actual number of sense clusters.",
                "We compare the cluster errors of LDA (Blei, Ng, and #REF) , STM ( #TARGET_REF) , HC (Chang, Pei, and #REF) , and a nonparametric model HDP (#REF ), with AutoSense.",
                "We report the results in Figure 4 .",
                "Results show that the cluster error of LDA increases sharply as the number of senses exceeds the actual mean number of senses."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Incorporating the target-neighbor pairs helps the model differentiates both into fine-grained senses book: arrange for and reserve in advance and book: engage for a performance.\n sent1: We compare the competing models quantitatively on how they correctly detect the actual number of sense clusters using cluster error, which is the mean absolute error between the detected number and the actual number of sense clusters.\n sent2: We compare the cluster errors of LDA (Blei, Ng, and #REF) , STM ( #TARGET_REF) , HC (Chang, Pei, and #REF) , and a nonparametric model HDP (#REF ), with AutoSense.\n sent3: We report the results in Figure 4 .\n sent4: Results show that the cluster error of LDA increases sharply as the number of senses exceeds the actual mean number of senses.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "It includes the PubMed ID of the papers authored by the given author name.",
                "We extract the title, author list, publication venue, and abstract of each PubMed ID from the PubMed website.",
                "We use LDA (Blei, Ng, and #REF) , HC (Chang, Pei, and #REF) and STM ( #TARGET_REF) as baselines.",
                "We do not compare with non-text feature-based models (#REF; #REF ) because our goal is to compare sense topic models on a task where the sense granularities are more varied.",
                "For STM and AutoSense, the title, publication venue and the author names are used as local contexts while the abstract is used as the global context."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: It includes the PubMed ID of the papers authored by the given author name.\n sent1: We extract the title, author list, publication venue, and abstract of each PubMed ID from the PubMed website.\n sent2: We use LDA (Blei, Ng, and #REF) , HC (Chang, Pei, and #REF) and STM ( #TARGET_REF) as baselines.\n sent3: We do not compare with non-text feature-based models (#REF; #REF ) because our goal is to compare sense topic models on a task where the sense granularities are more varied.\n sent4: For STM and AutoSense, the title, publication venue and the author names are used as local contexts while the abstract is used as the global context.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "This paper evaluates two semi-supervised techniques for the adaptation of a parse selection model to Wikipedia domains.",
                "The techniques examined are Structural Correspondence Learning (SCL) #TARGET_REF and Self-training (#REF; #REF) .",
                "A preliminary evaluation favors the use of SCL over the simpler self-training techniques."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This paper evaluates two semi-supervised techniques for the adaptation of a parse selection model to Wikipedia domains.\n sent1: The techniques examined are Structural Correspondence Learning (SCL) #TARGET_REF and Self-training (#REF; #REF) .\n sent2: A preliminary evaluation favors the use of SCL over the simpler self-training techniques.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "So far, Structural Correspondence Learning has been applied successfully to PoS tagging and Sentiment Analysis #TARGET_REF; ).",
                "An attempt was made in the CoNLL 2007 shared task to apply SCL to non-projective dependency parsing (#REF) .",
                "However, the system just ended up at rank 7 out of 8 teams.",
                "Based on annotation differences in the datasets and a bug in their system (#REF) , their results are inconclusive.",
                "A recent attempt (#REF) shows promising results on applying SCL to parse disambiguation."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: So far, Structural Correspondence Learning has been applied successfully to PoS tagging and Sentiment Analysis #TARGET_REF; ).\n sent1: An attempt was made in the CoNLL 2007 shared task to apply SCL to non-projective dependency parsing (#REF) .\n sent2: However, the system just ended up at rank 7 out of 8 teams.\n sent3: Based on annotation differences in the datasets and a bug in their system (#REF) , their results are inconclusive.\n sent4: A recent attempt (#REF) shows promising results on applying SCL to parse disambiguation.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Structural Correspondence Learning #TARGET_REF exploits unlabeled data from both source and target domain to find correspondences among features from different domains.",
                "These correspondences are then integrated as new features in the labeled data of the source domain.",
                "The outline of SCL is given in Algorithm 1.",
                "The key to SCL is to exploit pivot features to automatically identify feature correspondences.",
                "Pivots are features occurring frequently and behaving similarly in both domains (#REF) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Structural Correspondence Learning #TARGET_REF exploits unlabeled data from both source and target domain to find correspondences among features from different domains.\n sent1: These correspondences are then integrated as new features in the labeled data of the source domain.\n sent2: The outline of SCL is given in Algorithm 1.\n sent3: The key to SCL is to exploit pivot features to automatically identify feature correspondences.\n sent4: Pivots are features occurring frequently and behaving similarly in both domains (#REF) .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "The outline of SCL is given in Algorithm 1.",
                "The key to SCL is to exploit pivot features to automatically identify feature correspondences.",
                "Pivots are features occurring frequently and behaving similarly in both domains #TARGET_REF .",
                "They correspond to auxiliary problems in #REF .",
                "For every such pivot feature, a binary classifier is trained (step 2 of Algorithm 1) by masking the pivot feature in the data and trying to predict it with the remaining non-pivot features."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The outline of SCL is given in Algorithm 1.\n sent1: The key to SCL is to exploit pivot features to automatically identify feature correspondences.\n sent2: Pivots are features occurring frequently and behaving similarly in both domains #TARGET_REF .\n sent3: They correspond to auxiliary problems in #REF .\n sent4: For every such pivot feature, a binary classifier is trained (step 2 of Algorithm 1) by masking the pivot feature in the data and trying to predict it with the remaining non-pivot features.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "So far, pivot features on the word level were used #TARGET_REF; .",
                "However, for parse disambiguation based on a conditional model they are irrelevant.",
                "Hence, we follow #REF and actually first parse the unlabeled data.",
                "This allows a possibly noisy, but more abstract representation of the underlying data.",
                "Features thus correspond to properties of parses: application of grammar rules (r1,r2 features), dependency relations (dep), PoS tags (f1,f2), syntactic features (s1), precedence (mf ), bilexical preferences (z), apposition (appos) and further features for unknown words, temporal phrases, coordination (h,in year and p1, respectively)."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: So far, pivot features on the word level were used #TARGET_REF; .\n sent1: However, for parse disambiguation based on a conditional model they are irrelevant.\n sent2: Hence, we follow #REF and actually first parse the unlabeled data.\n sent3: This allows a possibly noisy, but more abstract representation of the underlying data.\n sent4: Features thus correspond to properties of parses: application of grammar rules (r1,r2 features), dependency relations (dep), PoS tags (f1,f2), syntactic features (s1), precedence (mf ), bilexical preferences (z), apposition (appos) and further features for unknown words, temporal phrases, coordination (h,in year and p1, respectively).\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "In contrast, in semisupervised domain adaptation one has only unlabeled target data.",
                "It is a more realistic situation, but at the same time also considerably more difficult.",
                "In this paper we evaluate two semi-supervised approaches to domain adaptation of a discriminative parse selection model.",
                "We examine Structural Correspondence Learning (SCL) #TARGET_REF for this task, and compare it to several variants of Self-training (#REF; #REF) .",
                "For empirical evaluation (section 4) we use the Alpino parsing system for Dutch (#REF) ."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: In contrast, in semisupervised domain adaptation one has only unlabeled target data.\n sent1: It is a more realistic situation, but at the same time also considerably more difficult.\n sent2: In this paper we evaluate two semi-supervised approaches to domain adaptation of a discriminative parse selection model.\n sent3: We examine Structural Correspondence Learning (SCL) #TARGET_REF for this task, and compare it to several variants of Self-training (#REF; #REF) .\n sent4: For empirical evaluation (section 4) we use the Alpino parsing system for Dutch (#REF) .\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "They correspond to auxiliary problems in #REF .",
                "For every such pivot feature, a binary classifier is trained (step 2 of Algorithm 1) by masking the pivot feature in the data and trying to predict it with the remaining non-pivot features.",
                "Non-pivots that correlate with many of the same pivots are assumed to correspond.",
                "These pivot predictor weight vectors thus implicitly align non-pivot features from source and target domain.",
                "Intuitively, if we are able to find good correspondences through 'linking' pivots, then the augmented source data should transfer better to a target domain #TARGET_REF ."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: They correspond to auxiliary problems in #REF .\n sent1: For every such pivot feature, a binary classifier is trained (step 2 of Algorithm 1) by masking the pivot feature in the data and trying to predict it with the remaining non-pivot features.\n sent2: Non-pivots that correlate with many of the same pivots are assumed to correspond.\n sent3: These pivot predictor weight vectors thus implicitly align non-pivot features from source and target domain.\n sent4: Intuitively, if we are able to find good correspondences through 'linking' pivots, then the augmented source data should transfer better to a target domain #TARGET_REF .\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "We assume that in order to parse data from a very specific domain, say about the artist Prince, then data related to that domain, like information about the New Power Generation, the Purple rain movie, or other American singers and artists, should be of help.",
                "Thus, we exploit Wikipedia's category system to gather domain-specific target data.",
                "In our empirical setup, we follow #TARGET_REF and balance the size of source and target data.",
                "Thus, depending on the size of the resulting target domain dataset, and the \"broadness\" of the categories involved in creating it, we might wish to filter out certain pages.",
                "We implemented a filter mechanism that excludes pages of a certain category (e.g. a supercategory that is hypothesized to be \"too broad\")."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: We assume that in order to parse data from a very specific domain, say about the artist Prince, then data related to that domain, like information about the New Power Generation, the Purple rain movie, or other American singers and artists, should be of help.\n sent1: Thus, we exploit Wikipedia's category system to gather domain-specific target data.\n sent2: In our empirical setup, we follow #TARGET_REF and balance the size of source and target data.\n sent3: Thus, depending on the size of the resulting target domain dataset, and the \"broadness\" of the categories involved in creating it, we might wish to filter out certain pages.\n sent4: We implemented a filter mechanism that excludes pages of a certain category (e.g. a supercategory that is hypothesized to be \"too broad\").\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The paper compares Structural Correspondence Learning #TARGET_REF with (various instances of) self-training (#REF; #REF) for the adaptation of a parse selection model to Wikipedia domains.",
                "The empirical findings show that none of the evaluated self-training variants (delible/indelible, single versus multiple iterations, various selection techniques) achieves a significant improvement over the baseline.",
                "The more 'indirect' exploitation of unlabeled data through SCL is more fruitful than pure self-training.",
                "Thus, favoring the use of the more complex method, although the findings are not confirmed on all testsets.",
                "Of course, our results are preliminary and, rather than warranting yet many definite conclusions, encourage further investigation of SCL (varying size of target data, pivots selection, bigger testsets as well as other domains etc.) as well as related semisupervised adaptation techniques."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: The paper compares Structural Correspondence Learning #TARGET_REF with (various instances of) self-training (#REF; #REF) for the adaptation of a parse selection model to Wikipedia domains.\n sent1: The empirical findings show that none of the evaluated self-training variants (delible/indelible, single versus multiple iterations, various selection techniques) achieves a significant improvement over the baseline.\n sent2: The more 'indirect' exploitation of unlabeled data through SCL is more fruitful than pure self-training.\n sent3: Thus, favoring the use of the more complex method, although the findings are not confirmed on all testsets.\n sent4: Of course, our results are preliminary and, rather than warranting yet many definite conclusions, encourage further investigation of SCL (varying size of target data, pivots selection, bigger testsets as well as other domains etc.) as well as related semisupervised adaptation techniques.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "This tutorial introduces the advances in deep Bayesian learning with abundant applications for natural language understanding ranging from speech recognition (#REF; #REF) to document summarization (#REF ), text classification (#REF; #REF) , text segmentation (#REF) , information extraction ( #TARGET_REF) , image caption generation (#REF; #REF) , sentence generation (#REFb) , dialogue control (#REF; #REFa) , sentiment classification, recommendation system, question answering (#REF) and machine translation , to name a few.",
                "Traditionally, \"deep learning\" is taken to be a learning process where the inference or optimization is based on the real-valued deterministic model.",
                "The \"semantic structure\" in words, sentences, entities, actions and documents drawn from a large vocabulary may not be well expressed or correctly optimized in mathematical logic or computer programs.",
                "The \"distribution function\" in discrete or continuous latent variable model for natural language may not be properly decomposed or estimated in model inference.",
                "This tutorial addresses the fundamentals of statistical models and neural networks, and focus on a series of advanced Bayesian models and deep models including hierarchical Dirichlet process , Chinese restaurant process (#REF) , hierarchical Pitman-Yor process (#REF) , Indian buffet process (#REF) , recurrent neural network (#REF; Van Den #REF) , long short-term memory (#REF; , sequence-to-sequence model (#REF), variational auto-encoder (#REF) , generative adversarial network (#REF) , attention mechanism (#REF; #REF) , memory-augmented neural network (#REF; #REF) , stochastic neural network #REF) , predictive state neural network (#REF) , policy gradient (#REF) and reinforcement learning (#REF) ."
            ],
            "label": [
                "USE",
                "BACKGROUND"
            ]
        },
        "input": "sent0: This tutorial introduces the advances in deep Bayesian learning with abundant applications for natural language understanding ranging from speech recognition (#REF; #REF) to document summarization (#REF ), text classification (#REF; #REF) , text segmentation (#REF) , information extraction ( #TARGET_REF) , image caption generation (#REF; #REF) , sentence generation (#REFb) , dialogue control (#REF; #REFa) , sentiment classification, recommendation system, question answering (#REF) and machine translation , to name a few.\n sent1: Traditionally, \"deep learning\" is taken to be a learning process where the inference or optimization is based on the real-valued deterministic model.\n sent2: The \"semantic structure\" in words, sentences, entities, actions and documents drawn from a large vocabulary may not be well expressed or correctly optimized in mathematical logic or computer programs.\n sent3: The \"distribution function\" in discrete or continuous latent variable model for natural language may not be properly decomposed or estimated in model inference.\n sent4: This tutorial addresses the fundamentals of statistical models and neural networks, and focus on a series of advanced Bayesian models and deep models including hierarchical Dirichlet process , Chinese restaurant process (#REF) , hierarchical Pitman-Yor process (#REF) , Indian buffet process (#REF) , recurrent neural network (#REF; Van Den #REF) , long short-term memory (#REF; , sequence-to-sequence model (#REF), variational auto-encoder (#REF) , generative adversarial network (#REF) , attention mechanism (#REF; #REF) , memory-augmented neural network (#REF; #REF) , stochastic neural network #REF) , predictive state neural network (#REF) , policy gradient (#REF) and reinforcement learning (#REF) .\n",
        "output": "{\"label\": [\"USE\", \"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "We propose smoothed max pooling loss and its application to keyword spotting systems.",
                "The proposed approach jointly trains an encoder (to detect keyword parts) and a decoder (to detect whole keyword) in a semi-supervised manner.",
                "The proposed new loss function allows training a model to detect parts and whole of a keyword, without strictly depending on frame-level labeling from LVCSR (Large vocabulary continuous speech recognition), making further optimization possible.",
                "The proposed system outperforms the baseline keyword spotting model in [ #TARGET_REF] due to increased optimizability.",
                "Further, it can be more easily adapted for on-device learning applications due to reduced dependency on LVCSR."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: We propose smoothed max pooling loss and its application to keyword spotting systems.\n sent1: The proposed approach jointly trains an encoder (to detect keyword parts) and a decoder (to detect whole keyword) in a semi-supervised manner.\n sent2: The proposed new loss function allows training a model to detect parts and whole of a keyword, without strictly depending on frame-level labeling from LVCSR (Large vocabulary continuous speech recognition), making further optimization possible.\n sent3: The proposed system outperforms the baseline keyword spotting model in [ #TARGET_REF] due to increased optimizability.\n sent4: Further, it can be more easily adapted for on-device learning applications due to reduced dependency on LVCSR.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The proposed model uses the same encoder/decoder structure as [ #TARGET_REF] ( Fig.1 ), but it differs in that encoder and decoder models are trained simultaneously using smoothed max pooling loss.",
                "#REF , both encoder and decoder models are trained with cross entropy (CE) loss using frame level labels.",
                "In the proposed approach, we define losses for encoder and decoder using smoothed max pooling loss, and optimize the combination of two losses simultaneously.",
                "The proposed smoothed max pooling loss doesnt strictly depend on phoneme-level alignment, allowing better optimization than the baseline."
            ],
            "label": [
                "DIFFERENCES",
                "USE",
                "SIMILARITY"
            ]
        },
        "input": "sent0: The proposed model uses the same encoder/decoder structure as [ #TARGET_REF] ( Fig.1 ), but it differs in that encoder and decoder models are trained simultaneously using smoothed max pooling loss.\n sent1: #REF , both encoder and decoder models are trained with cross entropy (CE) loss using frame level labels.\n sent2: In the proposed approach, we define losses for encoder and decoder using smoothed max pooling loss, and optimize the combination of two losses simultaneously.\n sent3: The proposed smoothed max pooling loss doesnt strictly depend on phoneme-level alignment, allowing better optimization than the baseline.\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"USE\", \"SIMILARITY\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "The proposed model uses the same encoder/decoder structure as [1] ( Fig.1 ), but it differs in that encoder and decoder models are trained simultaneously using smoothed max pooling loss.",
                "#REF , both encoder and decoder models are trained with cross entropy (CE) loss using frame level labels.",
                "In the proposed approach, we define losses for encoder and decoder using smoothed max pooling loss, and optimize the combination of two losses simultaneously.",
                "The proposed smoothed max pooling loss doesnt strictly depend on phoneme-level alignment, allowing better optimization than the #TARGET_REF."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: The proposed model uses the same encoder/decoder structure as [1] ( Fig.1 ), but it differs in that encoder and decoder models are trained simultaneously using smoothed max pooling loss.\n sent1: #REF , both encoder and decoder models are trained with cross entropy (CE) loss using frame level labels.\n sent2: In the proposed approach, we define losses for encoder and decoder using smoothed max pooling loss, and optimize the combination of two losses simultaneously.\n sent3: The proposed smoothed max pooling loss doesnt strictly depend on phoneme-level alignment, allowing better optimization than the #TARGET_REF.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "We compare the model trained with the new smoothed max pooling loss on encoder/decoder architecture with the baseline in [ #TARGET_REF] .",
                "Both the baseline and the proposed model have the same architecture.",
                "Only the training losses are different.",
                "Details of the setup are discussed below."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: We compare the model trained with the new smoothed max pooling loss on encoder/decoder architecture with the baseline in [ #TARGET_REF] .\n sent1: Both the baseline and the proposed model have the same architecture.\n sent2: Only the training losses are different.\n sent3: Details of the setup are discussed below.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "We call the #TARGET_REF as Baseline CE CE where encoder and decoder submodels are trained with CE loss.",
                "We call the proposed model as Max4 SMP SMP where both encoder and decoder submodels are trained by SMP (smoothed max pooling) loss.",
                "We also performed ablation study by testing other models that use different losses.",
                "Table 1 summarizes all the tested models.",
                "Model Max1-Max3 uses SMP (smoothed max pooling) loss for the decoder, but uses different losses for the encoder."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: We call the #TARGET_REF as Baseline CE CE where encoder and decoder submodels are trained with CE loss.\n sent1: We call the proposed model as Max4 SMP SMP where both encoder and decoder submodels are trained by SMP (smoothed max pooling) loss.\n sent2: We also performed ablation study by testing other models that use different losses.\n sent3: Table 1 summarizes all the tested models.\n sent4: Model Max1-Max3 uses SMP (smoothed max pooling) loss for the decoder, but uses different losses for the encoder.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Across model types and evaluation conditions Max4 SMP SMP shows the best accuracy and ROC curve.",
                "Max3 CE MP model also performs better than the #TARGET_REF but not as good as Max4.",
                "Other variations Max2 (has only decoder loss) and Max1 (has CTC encoder loss) performed worse than baseline.",
                "Comparison among models with max pooling and different smoothing options (Fig.4) shows that Max4 SMP SMP (smoothed max poling on both encoder and decoder) performs the best and outperforms Max7(no smoothing on encoder and decoder max pooling loss).",
                "Especially the proposed Max4 model reduces FR rate to nearly half of the baseline in clean accented and noisy inside-vehicle conditions, where it's more difficult to obtain training data with accurate alignments."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Across model types and evaluation conditions Max4 SMP SMP shows the best accuracy and ROC curve.\n sent1: Max3 CE MP model also performs better than the #TARGET_REF but not as good as Max4.\n sent2: Other variations Max2 (has only decoder loss) and Max1 (has CTC encoder loss) performed worse than baseline.\n sent3: Comparison among models with max pooling and different smoothing options (Fig.4) shows that Max4 SMP SMP (smoothed max poling on both encoder and decoder) performs the best and outperforms Max7(no smoothing on encoder and decoder max pooling loss).\n sent4: Especially the proposed Max4 model reduces FR rate to nearly half of the baseline in clean accented and noisy inside-vehicle conditions, where it's more difficult to obtain training data with accurate alignments.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Figure 4 shows the ROC curves of Max4-Max7 models across different conditions.",
                "Across model types and evaluation conditions Max4 SMP SMP shows the best accuracy and ROC curve.",
                "Max3 CE MP model also performs better than the baseline but not as good as Max4.",
                "Other variations Max2 (has only decoder loss) and Max1 (has CTC encoder loss) performed worse than #TARGET_REF",
                "Comparison among models with max pooling and different smoothing options (Fig.4) shows that Max4 SMP SMP (smoothed max poling on both encoder and decoder) performs the best and outperforms Max7(no smoothing on encoder and decoder max pooling loss)."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Figure 4 shows the ROC curves of Max4-Max7 models across different conditions.\n sent1: Across model types and evaluation conditions Max4 SMP SMP shows the best accuracy and ROC curve.\n sent2: Max3 CE MP model also performs better than the baseline but not as good as Max4.\n sent3: Other variations Max2 (has only decoder loss) and Max1 (has CTC encoder loss) performed worse than #TARGET_REF\n sent4: Comparison among models with max pooling and different smoothing options (Fig.4) shows that Max4 SMP SMP (smoothed max poling on both encoder and decoder) performs the best and outperforms Max7(no smoothing on encoder and decoder max pooling loss).\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Across model types and evaluation conditions Max4 SMP SMP shows the best accuracy and ROC curve.",
                "Max3 CE MP model also performs better than the baseline but not as good as Max4.",
                "Other variations Max2 (has only decoder loss) and Max1 (has CTC encoder loss) performed worse than baseline.",
                "Comparison among models with max pooling and different smoothing options (Fig.4) shows that Max4 SMP SMP (smoothed max poling on both encoder and decoder) performs the best and outperforms Max7(no smoothing on encoder and decoder max pooling loss).",
                "Especially the proposed Max4 model reduces FR rate to nearly half of the #TARGET_REF in clean accented and noisy inside-vehicle conditions, where it's more difficult to obtain training data with accurate alignments."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Across model types and evaluation conditions Max4 SMP SMP shows the best accuracy and ROC curve.\n sent1: Max3 CE MP model also performs better than the baseline but not as good as Max4.\n sent2: Other variations Max2 (has only decoder loss) and Max1 (has CTC encoder loss) performed worse than baseline.\n sent3: Comparison among models with max pooling and different smoothing options (Fig.4) shows that Max4 SMP SMP (smoothed max poling on both encoder and decoder) performs the best and outperforms Max7(no smoothing on encoder and decoder max pooling loss).\n sent4: Especially the proposed Max4 model reduces FR rate to nearly half of the #TARGET_REF in clean accented and noisy inside-vehicle conditions, where it's more difficult to obtain training data with accurate alignments.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "We presented smoothed max pooling loss for training keyword spotting model with improved optimizability.",
                "Experiments show that the proposed approach outperforms the #TARGET_REF with CE loss by relative 22%-54% across a variety of conditions.",
                "Further, we show that applying smoothing before max pooling is highly important for achieving accuracy better than the baseline.",
                "The proposed approach provides further benefits of reducing dependence on LVCSR to provide phoneme level alignments, which is desirable for embedded learning scenarios, like on-device learning [20] [21] ."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: We presented smoothed max pooling loss for training keyword spotting model with improved optimizability.\n sent1: Experiments show that the proposed approach outperforms the #TARGET_REF with CE loss by relative 22%-54% across a variety of conditions.\n sent2: Further, we show that applying smoothing before max pooling is highly important for achieving accuracy better than the baseline.\n sent3: The proposed approach provides further benefits of reducing dependence on LVCSR to provide phoneme level alignments, which is desirable for embedded learning scenarios, like on-device learning [20] [21] .\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We presented smoothed max pooling loss for training keyword spotting model with improved optimizability.",
                "Experiments show that the proposed approach outperforms the baseline model with CE loss by relative 22%-54% across a variety of conditions.",
                "Further, we show that applying smoothing before max pooling is highly important for achieving accuracy better than the #TARGET_REF.",
                "The proposed approach provides further benefits of reducing dependence on LVCSR to provide phoneme level alignments, which is desirable for embedded learning scenarios, like on-device learning [20] [21] ."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: We presented smoothed max pooling loss for training keyword spotting model with improved optimizability.\n sent1: Experiments show that the proposed approach outperforms the baseline model with CE loss by relative 22%-54% across a variety of conditions.\n sent2: Further, we show that applying smoothing before max pooling is highly important for achieving accuracy better than the #TARGET_REF.\n sent3: The proposed approach provides further benefits of reducing dependence on LVCSR to provide phoneme level alignments, which is desirable for embedded learning scenarios, like on-device learning [20] [21] .\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Following the successes in general ASR [2, 3] , the neural network based approach has been extensively explored in keyword spotting area with benefits of lowering resource requirements and improving accuracy [4, 5, 6, 7, 8, 9, 10, 11] .",
                "Such works include DNN + temporal integration [4, 5, 11, 12] , and HMM + DNN hybrid approaches [6, 7, 8, 9, 10] .",
                "Recently introduced end-to-end trainable DNN approaches [ #TARGET_REF 13] further improved accuracy and lowered resource requirements using highly optimizable system design.",
                "In general, training of such DNN based systems required framelevel labels generated by LVCSR systems [14, 1] .",
                "These approaches make end-to-end optimizable keyword spotting system depend on labels generated from non-end-to-end system trained for a different task."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Following the successes in general ASR [2, 3] , the neural network based approach has been extensively explored in keyword spotting area with benefits of lowering resource requirements and improving accuracy [4, 5, 6, 7, 8, 9, 10, 11] .\n sent1: Such works include DNN + temporal integration [4, 5, 11, 12] , and HMM + DNN hybrid approaches [6, 7, 8, 9, 10] .\n sent2: Recently introduced end-to-end trainable DNN approaches [ #TARGET_REF 13] further improved accuracy and lowered resource requirements using highly optimizable system design.\n sent3: In general, training of such DNN based systems required framelevel labels generated by LVCSR systems [14, 1] .\n sent4: These approaches make end-to-end optimizable keyword spotting system depend on labels generated from non-end-to-end system trained for a different task.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "As assistant technology spreads to more ubiquitous use-cases (mobile, IOT), reducing resource consumption (memory and computation) while improving accuracy has been the key success criteria of keyword spotting techniques.",
                "Following the successes in general ASR [2, 3] , the neural network based approach has been extensively explored in keyword spotting area with benefits of lowering resource requirements and improving accuracy [4, 5, 6, 7, 8, 9, 10, 11] .",
                "Such works include DNN + temporal integration [4, 5, 11, 12] , and HMM + DNN hybrid approaches [6, 7, 8, 9, 10] .",
                "Recently introduced end-to-end trainable DNN approaches [1, 13] further improved accuracy and lowered resource requirements using highly optimizable system design.",
                "In general, training of such DNN based systems required framelevel labels generated by LVCSR systems [14, #TARGET_REF] ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: As assistant technology spreads to more ubiquitous use-cases (mobile, IOT), reducing resource consumption (memory and computation) while improving accuracy has been the key success criteria of keyword spotting techniques.\n sent1: Following the successes in general ASR [2, 3] , the neural network based approach has been extensively explored in keyword spotting area with benefits of lowering resource requirements and improving accuracy [4, 5, 6, 7, 8, 9, 10, 11] .\n sent2: Such works include DNN + temporal integration [4, 5, 11, 12] , and HMM + DNN hybrid approaches [6, 7, 8, 9, 10] .\n sent3: Recently introduced end-to-end trainable DNN approaches [1, 13] further improved accuracy and lowered resource requirements using highly optimizable system design.\n sent4: In general, training of such DNN based systems required framelevel labels generated by LVCSR systems [14, #TARGET_REF] .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "However, for keyword-spotting, the exact position of the keyword is not as relevant as its presence.",
                "Therefore, such strict dependency on frame-level labels may limit further optimization promised by the end-to-end approach.",
                "In [ #TARGET_REF] , the top level loss is derived by integrating frame-level losses, which are computed using frame-level labels from LVCSR.",
                "Integrating frame-level losses penalizes slightly mis-aligned correct predictions, which can limit detection accuracy, especially for difficult data (e.g. noisy or accented speech) where LVCSR labels may have higher-than-normal uncertainty.",
                "In such case, losses can be fully minimized only when the predicted value and position-in-time matches that of provided frame level labels, where exact position match is not highly relevant for high accuracy."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: However, for keyword-spotting, the exact position of the keyword is not as relevant as its presence.\n sent1: Therefore, such strict dependency on frame-level labels may limit further optimization promised by the end-to-end approach.\n sent2: In [ #TARGET_REF] , the top level loss is derived by integrating frame-level losses, which are computed using frame-level labels from LVCSR.\n sent3: Integrating frame-level losses penalizes slightly mis-aligned correct predictions, which can limit detection accuracy, especially for difficult data (e.g. noisy or accented speech) where LVCSR labels may have higher-than-normal uncertainty.\n sent4: In such case, losses can be fully minimized only when the predicted value and position-in-time matches that of provided frame level labels, where exact position match is not highly relevant for high accuracy.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The decoder model takes the encoder output as input and generates binary output Y D that predicts existence of a keyword .",
                "The model is fed with acoustic input features at each frame (generated every 10ms), and generates prediction labels at each frame in a streaming manner.",
                "In [ #TARGET_REF] , the encoder model is trained first, and then the decoder model is trained while encoder model weights are frozen.",
                "#REF , the encoder model is trained to predict phonemelevel labels provided from LVCSR.",
                "Both encoder and decoder models use CE-loss defined in Eq (1) and (2), where Xt = [xt−C l , · · · , xt, · · · , xt+C r ], xt is spectral feature of d-dimension, yi(Xt, W ) stands for ith dimension of network's softmax output, W is network weight, and ct is a frame-level label at frame t."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The decoder model takes the encoder output as input and generates binary output Y D that predicts existence of a keyword .\n sent1: The model is fed with acoustic input features at each frame (generated every 10ms), and generates prediction labels at each frame in a streaming manner.\n sent2: In [ #TARGET_REF] , the encoder model is trained first, and then the decoder model is trained while encoder model weights are frozen.\n sent3: #REF , the encoder model is trained to predict phonemelevel labels provided from LVCSR.\n sent4: Both encoder and decoder models use CE-loss defined in Eq (1) and (2), where Xt = [xt−C l , · · · , xt, · · · , xt+C r ], xt is spectral feature of d-dimension, yi(Xt, W ) stands for ith dimension of network's softmax output, W is network weight, and ct is a frame-level label at frame t.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The model is fed with acoustic input features at each frame (generated every 10ms), and generates prediction labels at each frame in a streaming manner.",
                "#REF , the encoder model is trained first, and then the decoder model is trained while encoder model weights are frozen.",
                "In [ #TARGET_REF] , the encoder model is trained to predict phonemelevel labels provided from LVCSR.",
                "Both encoder and decoder models use CE-loss defined in Eq (1) and (2), where Xt = [xt−C l , · · · , xt, · · · , xt+C r ], xt is spectral feature of d-dimension, yi(Xt, W ) stands for ith dimension of network's softmax output, W is network weight, and ct is a frame-level label at frame t.",
                "#REF , target label sequence consists of intervals of repeated labels which we call runs."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The model is fed with acoustic input features at each frame (generated every 10ms), and generates prediction labels at each frame in a streaming manner.\n sent1: #REF , the encoder model is trained first, and then the decoder model is trained while encoder model weights are frozen.\n sent2: In [ #TARGET_REF] , the encoder model is trained to predict phonemelevel labels provided from LVCSR.\n sent3: Both encoder and decoder models use CE-loss defined in Eq (1) and (2), where Xt = [xt−C l , · · · , xt, · · · , xt+C r ], xt is spectral feature of d-dimension, yi(Xt, W ) stands for ith dimension of network's softmax output, W is network weight, and ct is a frame-level label at frame t.\n sent4: #REF , target label sequence consists of intervals of repeated labels which we call runs.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF , the encoder model is trained first, and then the decoder model is trained while encoder model weights are frozen.",
                "#REF , the encoder model is trained to predict phonemelevel labels provided from LVCSR.",
                "Both encoder and decoder models use CE-loss defined in Eq (1) and (2), where Xt = [xt−C l , · · · , xt, · · · , xt+C r ], xt is spectral feature of d-dimension, yi(Xt, W ) stands for ith dimension of network's softmax output, W is network weight, and ct is a frame-level label at frame t.",
                "In [ #TARGET_REF] , target label sequence consists of intervals of repeated labels which we call runs.",
                "These label runs define clearly defined intervals where a model should learn to generate strong activation in label output dimension."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #REF , the encoder model is trained first, and then the decoder model is trained while encoder model weights are frozen.\n sent1: #REF , the encoder model is trained to predict phonemelevel labels provided from LVCSR.\n sent2: Both encoder and decoder models use CE-loss defined in Eq (1) and (2), where Xt = [xt−C l , · · · , xt, · · · , xt+C r ], xt is spectral feature of d-dimension, yi(Xt, W ) stands for ith dimension of network's softmax output, W is network weight, and ct is a frame-level label at frame t.\n sent3: In [ #TARGET_REF] , target label sequence consists of intervals of repeated labels which we call runs.\n sent4: These label runs define clearly defined intervals where a model should learn to generate strong activation in label output dimension.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "We used the same frontend feature extract as the baseline [ #TARGET_REF] in our experiments.",
                "The front-end extracts and stacks a 40-d feature vector of log-mel filter-bank energies at each frame and stacks them to generate input feature vector Xt. Refer to [1] for further details."
            ],
            "label": [
                "USE",
                "SIMILARITY"
            ]
        },
        "input": "sent0: We used the same frontend feature extract as the baseline [ #TARGET_REF] in our experiments.\n sent1: The front-end extracts and stacks a 40-d feature vector of log-mel filter-bank energies at each frame and stacks them to generate input feature vector Xt. Refer to [1] for further details.\n",
        "output": "{\"label\": [\"USE\", \"SIMILARITY\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "We selected E2E 318K architecture in [ #TARGET_REF] as the baseline and use the same structure for testing all other models.",
                "As shown in Fig. 1 , the model has 7 SVDF layers and 3 linear bottleneck dense layers.",
                "For detailed architectural parameters, please refer to [1] .",
                "We call the baseline model as Baseline CE CE where encoder and decoder submodels are trained with CE loss.",
                "We call the proposed model as Max4 SMP SMP where both encoder and decoder submodels are trained by SMP (smoothed max pooling) loss."
            ],
            "label": [
                "USE",
                "SIMILARITY"
            ]
        },
        "input": "sent0: We selected E2E 318K architecture in [ #TARGET_REF] as the baseline and use the same structure for testing all other models.\n sent1: As shown in Fig. 1 , the model has 7 SVDF layers and 3 linear bottleneck dense layers.\n sent2: For detailed architectural parameters, please refer to [1] .\n sent3: We call the baseline model as Baseline CE CE where encoder and decoder submodels are trained with CE loss.\n sent4: We call the proposed model as Max4 SMP SMP where both encoder and decoder submodels are trained by SMP (smoothed max pooling) loss.\n",
        "output": "{\"label\": [\"USE\", \"SIMILARITY\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "the entire network is trained by decoder loss only.",
                "Max3 CE SMP used #TARGET_REF CE loss for encoder.",
                "Model Max4-Max7 are tested to measure the importance of the smoothing operation.",
                "MP means max pooling without smoothing (i.e. s(t) = 1).",
                "For the decoder SMP(smoothed max pooling) loss, we used truncated Gaussian as the smoothing filter s(t) with µ = 0, σ = 9 frames (90ms) and truncated length 21 frames."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: the entire network is trained by decoder loss only.\n sent1: Max3 CE SMP used #TARGET_REF CE loss for encoder.\n sent2: Model Max4-Max7 are tested to measure the importance of the smoothing operation.\n sent3: MP means max pooling without smoothing (i.e. s(t) = 1).\n sent4: For the decoder SMP(smoothed max pooling) loss, we used truncated Gaussian as the smoothing filter s(t) with µ = 0, σ = 9 frames (90ms) and truncated length 21 frames.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "To show effectiveness of the proposed approach, we evaluated falsereject (FR) and false-accept (FA) tradeoff across various models described in Section 3.",
                "All models are converted to inference models using TensorFlow Lites quantization [19] .",
                "Table 2 summarizes FR rates of models in Fig.3 and 4 at selected FA rate (0.1 FA per hour measured on 64K re-recorded TV noise set).",
                "Fig.3 shows the ROC curves of various models ( #TARGET_REF, Max1-Max4) across different conditions.",
                "Figure 4 shows the ROC curves of Max4-Max7 models across different conditions."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: To show effectiveness of the proposed approach, we evaluated falsereject (FR) and false-accept (FA) tradeoff across various models described in Section 3.\n sent1: All models are converted to inference models using TensorFlow Lites quantization [19] .\n sent2: Table 2 summarizes FR rates of models in Fig.3 and 4 at selected FA rate (0.1 FA per hour measured on 64K re-recorded TV noise set).\n sent3: Fig.3 shows the ROC curves of various models ( #TARGET_REF, Max1-Max4) across different conditions.\n sent4: Figure 4 shows the ROC curves of Max4-Max7 models across different conditions.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Both the #TARGET_REF and the proposed model have an encoder which takes spectral domain feature Xt as input and generate (K+1) outputs Y E corresponding to phoneme-like sound units (Fig.1 ).",
                "The decoder model takes the encoder output as input and generates binary output Y D that predicts existence of a keyword .",
                "The model is fed with acoustic input features at each frame (generated every 10ms), and generates prediction labels at each frame in a streaming manner.",
                "#REF , the encoder model is trained first, and then the decoder model is trained while encoder model weights are frozen.",
                "#REF , the encoder model is trained to predict phonemelevel labels provided from LVCSR."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: Both the #TARGET_REF and the proposed model have an encoder which takes spectral domain feature Xt as input and generate (K+1) outputs Y E corresponding to phoneme-like sound units (Fig.1 ).\n sent1: The decoder model takes the encoder output as input and generates binary output Y D that predicts existence of a keyword .\n sent2: The model is fed with acoustic input features at each frame (generated every 10ms), and generates prediction labels at each frame in a streaming manner.\n sent3: #REF , the encoder model is trained first, and then the decoder model is trained while encoder model weights are frozen.\n sent4: #REF , the encoder model is trained to predict phonemelevel labels provided from LVCSR.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "We compare the model trained with the new smoothed max pooling loss on encoder/decoder architecture with the baseline in [1] .",
                "Both the #TARGET_REF and the proposed model have the same architecture.",
                "Only the training losses are different.",
                "Details of the setup are discussed below."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: We compare the model trained with the new smoothed max pooling loss on encoder/decoder architecture with the baseline in [1] .\n sent1: Both the #TARGET_REF and the proposed model have the same architecture.\n sent2: Only the training losses are different.\n sent3: Details of the setup are discussed below.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The training data consists of 2.1 million anonymized utterances with the keywords Ok Google and Hey Google.",
                "Data augmentation similar to [ #TARGET_REF] has been used for better robustness.",
                "Evaluation is done on four data sets separate from training data, representing diverse environmental conditions -Clean non-accented set contains 170K non-accented English utterances of keywords in quiet condition.",
                "Clean accented has 138K English utterances of keyword with Australian, British, and Indian accents in quiet conditions.",
                "Query logs contains 58K utterances from anonymized voice search queries."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: The training data consists of 2.1 million anonymized utterances with the keywords Ok Google and Hey Google.\n sent1: Data augmentation similar to [ #TARGET_REF] has been used for better robustness.\n sent2: Evaluation is done on four data sets separate from training data, representing diverse environmental conditions -Clean non-accented set contains 170K non-accented English utterances of keywords in quiet condition.\n sent3: Clean accented has 138K English utterances of keyword with Australian, British, and Indian accents in quiet conditions.\n sent4: Query logs contains 58K utterances from anonymized voice search queries.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We used the same frontend feature extract as the baseline [1] in our experiments.",
                "The front-end extracts and stacks a 40-d feature vector of log-mel filter-bank energies at each frame and stacks them to generate input feature vector Xt. Refer to [ #TARGET_REF] for further details."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: We used the same frontend feature extract as the baseline [1] in our experiments.\n sent1: The front-end extracts and stacks a 40-d feature vector of log-mel filter-bank energies at each frame and stacks them to generate input feature vector Xt. Refer to [ #TARGET_REF] for further details.\n",
        "output": "{\"label\": [null], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "As shown in Fig. 1 , the model has 7 SVDF layers and 3 linear bottleneck dense layers.",
                "For detailed architectural parameters, please refer to [ #TARGET_REF] .",
                "We call the baseline model as Baseline CE CE where encoder and decoder submodels are trained with CE loss.",
                "We call the proposed model as Max4 SMP SMP where both encoder and decoder submodels are trained by SMP (smoothed max pooling) loss.",
                "We also performed ablation study by testing other models that use different losses."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: As shown in Fig. 1 , the model has 7 SVDF layers and 3 linear bottleneck dense layers.\n sent1: For detailed architectural parameters, please refer to [ #TARGET_REF] .\n sent2: We call the baseline model as Baseline CE CE where encoder and decoder submodels are trained with CE loss.\n sent3: We call the proposed model as Max4 SMP SMP where both encoder and decoder submodels are trained by SMP (smoothed max pooling) loss.\n sent4: We also performed ablation study by testing other models that use different losses.\n",
        "output": "{\"label\": [null], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The task of definition modeling, introduced by #TARGET_REF , consists in generating the dictionary definition of a specific word: for instance, given the word \"monotreme\" as input, the system would need to produce a definition such as \"any of an order (Monotremata) of egg-laying mammals comprising the platypuses and echidnas\".",
                "1 Following the tradition set by lexicographers, we call the word being defined a definiendum (pl. definienda), whereas a word occurring in its definition is called a definiens (pl. definientia).",
                "Definition modeling can prove useful in a variety of applications.",
                "Systems trained for the task may generate dictionaries for low resource languages, or extend the coverage of existing lexicographic resources where needed, e.g. of domainspecific vocabulary.",
                "Such systems may also be 1 Definition from Merriam-Webster."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The task of definition modeling, introduced by #TARGET_REF , consists in generating the dictionary definition of a specific word: for instance, given the word \"monotreme\" as input, the system would need to produce a definition such as \"any of an order (Monotremata) of egg-laying mammals comprising the platypuses and echidnas\".\n sent1: 1 Following the tradition set by lexicographers, we call the word being defined a definiendum (pl. definienda), whereas a word occurring in its definition is called a definiens (pl. definientia).\n sent2: Definition modeling can prove useful in a variety of applications.\n sent3: Systems trained for the task may generate dictionaries for low resource languages, or extend the coverage of existing lexicographic resources where needed, e.g. of domainspecific vocabulary.\n sent4: Such systems may also be 1 Definition from Merriam-Webster.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "A major intended application of definition modeling is the explication and evaluation of distributed lexical representations, also known as word embeddings #TARGET_REF .",
                "This evaluation procedure is based on the postulate that the meaning of a word, as is captured by its embedding, should be convertible into a human-readable dictionary definition.",
                "How well the meaning is captured must impact the ability of the model to reproduce the definition, and therefore embedding architectures can be compared according to their downstream performance on definition modeling.",
                "This intended usage motivates the requirement that definition modeling architectures take as input the embedding of the definiendum and not retrain it.",
                "From a theoretical point of view, usage of word embeddings as representations of meaning (cf."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: A major intended application of definition modeling is the explication and evaluation of distributed lexical representations, also known as word embeddings #TARGET_REF .\n sent1: This evaluation procedure is based on the postulate that the meaning of a word, as is captured by its embedding, should be convertible into a human-readable dictionary definition.\n sent2: How well the meaning is captured must impact the ability of the model to reproduce the definition, and therefore embedding architectures can be compared according to their downstream performance on definition modeling.\n sent3: This intended usage motivates the requirement that definition modeling architectures take as input the embedding of the definiendum and not retrain it.\n sent4: From a theoretical point of view, usage of word embeddings as representations of meaning (cf.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "The most natural approach to this task is to treat it as a sequence-to-sequence task, rather than a word-to-sequence task: given an input sequence with a highlighted word, generate a contextually appropriate definition for it (cf.",
                "sections 3 & 4) .",
                "We implement this approach in a Transformer-based sequence-to-sequence model that achieves state-of-the-art performances (sections 5 & 6).",
                "arXiv:1911.05715v1 [cs.CL] 13 #REF 2 Related Work",
                "In their seminal work on definition modeling, #TARGET_REF likened systems generating definitions to language models, which can naturally be used to generate arbitrary text."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The most natural approach to this task is to treat it as a sequence-to-sequence task, rather than a word-to-sequence task: given an input sequence with a highlighted word, generate a contextually appropriate definition for it (cf.\n sent1: sections 3 & 4) .\n sent2: We implement this approach in a Transformer-based sequence-to-sequence model that achieves state-of-the-art performances (sections 5 & 6).\n sent3: arXiv:1911.05715v1 [cs.CL] 13 #REF 2 Related Work\n sent4: In their seminal work on definition modeling, #TARGET_REF likened systems generating definitions to language models, which can naturally be used to generate arbitrary text.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Though different kinds of linguistic contexts have been suggested throughout the literature, we remark here that sentential context may sometimes suffice to guess the meaning of a word that we don't know (#REF) .",
                "Quoting from the example above, the context \"enough around-let's get back to work!\" sufficiently characterizes the meaning of the omitted verb to allow for an approximate definition for it even if the blank is not filled (#REF; #REF) .",
                "This reformulation can appear contrary to the original proposal by #TARGET_REF , which conceived definition modeling as a \"word-tosequence task\".",
                "They argued for an approach related to, though distinct from sequence-to-sequence architectures.",
                "Concretely, a specific encoding procedure was applied to the definiendum, so that it could be used as a feature vector during generation."
            ],
            "label": [
                "BACKGROUND",
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Though different kinds of linguistic contexts have been suggested throughout the literature, we remark here that sentential context may sometimes suffice to guess the meaning of a word that we don't know (#REF) .\n sent1: Quoting from the example above, the context \"enough around-let's get back to work!\" sufficiently characterizes the meaning of the omitted verb to allow for an approximate definition for it even if the blank is not filled (#REF; #REF) .\n sent2: This reformulation can appear contrary to the original proposal by #TARGET_REF , which conceived definition modeling as a \"word-tosequence task\".\n sent3: They argued for an approach related to, though distinct from sequence-to-sequence architectures.\n sent4: Concretely, a specific encoding procedure was applied to the definiendum, so that it could be used as a feature vector during generation.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"DIFFERENCES\"], \"context\": [\"sent0\", \"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Despite some key differences, all of the previously proposed architectures we are aware of (#REF; #REF; followed a pattern similar to sequence-to-sequence models.",
                "They all implicitly or explicitly used distinct submodules to encode the definiendum and to generate the definientia.",
                "In the case of #TARGET_REF , the encoding was the concatenation of the embedding of the definiendum, a vector representation of its sequence of characters derived from a characterlevel CNN, and its \"hypernym embedding\".",
                "#REF used a sigmoid-based gating module to tweak the definiendum embedding.",
                "The architecture proposed by is comprised of four modules, only one of which is used as a decoder: the remaining three are meant to convert the definiendum as a sparse embedding, select some of the sparse components of its meaning based on a provided context, and encode it into a representation adequate for the decoder."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Despite some key differences, all of the previously proposed architectures we are aware of (#REF; #REF; followed a pattern similar to sequence-to-sequence models.\n sent1: They all implicitly or explicitly used distinct submodules to encode the definiendum and to generate the definientia.\n sent2: In the case of #TARGET_REF , the encoding was the concatenation of the embedding of the definiendum, a vector representation of its sequence of characters derived from a characterlevel CNN, and its \"hypernym embedding\".\n sent3: #REF used a sigmoid-based gating module to tweak the definiendum embedding.\n sent4: The architecture proposed by is comprised of four modules, only one of which is used as a decoder: the remaining three are meant to convert the definiendum as a sparse embedding, select some of the sparse components of its meaning based on a provided context, and encode it into a representation adequate for the decoder.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Single word outputs can either be assessed as entirely correct or entirely wrong using BLEU or ROUGE.",
                "However consider for instance the word \"elation\": that it be defined either as \"mirth\" or \"joy\" should only influence our metric slightly, and not be discounted as a completely wrong prediction. , as they did not report the perplexity of their system and focused on a different dataset; likewise, consider only the Chinese variant of the task.",
                "Perplexity measures for #TARGET_REF and #REF are taken from the authors' respective publications.",
                "All our models perform better than previous proposals, by a margin of 4 to 10 points, for a relative improvement of 11-23%.",
                "Part of this improvement may be due to our use of Transformerbased architectures (#REF) , which is known to perform well on semantic tasks (#REF; #REF; #REF; #REF, eg.) ."
            ],
            "label": [
                "BACKGROUND",
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Single word outputs can either be assessed as entirely correct or entirely wrong using BLEU or ROUGE.\n sent1: However consider for instance the word \"elation\": that it be defined either as \"mirth\" or \"joy\" should only influence our metric slightly, and not be discounted as a completely wrong prediction. , as they did not report the perplexity of their system and focused on a different dataset; likewise, consider only the Chinese variant of the task.\n sent2: Perplexity measures for #TARGET_REF and #REF are taken from the authors' respective publications.\n sent3: All our models perform better than previous proposals, by a margin of 4 to 10 points, for a relative improvement of 11-23%.\n sent4: Part of this improvement may be due to our use of Transformerbased architectures (#REF) , which is known to perform well on semantic tasks (#REF; #REF; #REF; #REF, eg.) .\n",
        "output": "{\"label\": [\"BACKGROUND\", \"DIFFERENCES\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "We train our models on three distinct datasets, which are all borrowed or adapted from previous works on definition modeling.",
                "As a consequence, our experiments focus on the English language.",
                "The dataset of #TARGET_REF (henceforth D Nor ) maps definienda to their respective definientia, as well as additional information not used here.",
                "In the dataset of #REF (henceforth D Gad ), each example consists of a definiendum, the definientia for one of its meanings and a contextual cue sentence.",
                "D Nor contains on average shorter definitions than D Gad ."
            ],
            "label": [
                "DIFFERENCES",
                "USE"
            ]
        },
        "input": "sent0: We train our models on three distinct datasets, which are all borrowed or adapted from previous works on definition modeling.\n sent1: As a consequence, our experiments focus on the English language.\n sent2: The dataset of #TARGET_REF (henceforth D Nor ) maps definienda to their respective definientia, as well as additional information not used here.\n sent3: In the dataset of #REF (henceforth D Gad ), each example consists of a definiendum, the definientia for one of its meanings and a contextual cue sentence.\n sent4: D Nor contains on average shorter definitions than D Gad .\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"USE\"], \"context\": [\"sent0\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Dictionaries are expected to be exempt of such definitions: as readers are assumed not to know the meaning of the definiendum when looking it up. back on simply reusing the definiendum as its own definiens.",
                "Self-referring definitions highlight that our models equate the meaning of the definiendum to the composed meaning of its definientia.",
                "Simply masking the corresponding output embedding might suffice to prevent this specific problem; preliminary experiments in that direction suggest that this may also help decrease perplexity further.",
                "As for POS-mismatches, we do note that the work of #TARGET_REF had a much lower rate of 4.29%: we suggest that this may be due to the fact that they employ a learned character-level convolutional network, which arguably would be able to capture orthography and rudiments of morphology.",
                "Adding such a sub-module to our proposed architecture might diminish the number of mistagged definienda."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Dictionaries are expected to be exempt of such definitions: as readers are assumed not to know the meaning of the definiendum when looking it up. back on simply reusing the definiendum as its own definiens.\n sent1: Self-referring definitions highlight that our models equate the meaning of the definiendum to the composed meaning of its definientia.\n sent2: Simply masking the corresponding output embedding might suffice to prevent this specific problem; preliminary experiments in that direction suggest that this may also help decrease perplexity further.\n sent3: As for POS-mismatches, we do note that the work of #TARGET_REF had a much lower rate of 4.29%: we suggest that this may be due to the fact that they employ a learned character-level convolutional network, which arguably would be able to capture orthography and rudiments of morphology.\n sent4: Adding such a sub-module to our proposed architecture might diminish the number of mistagged definienda.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Despite some key differences, all of the previously proposed architectures we are aware of #TARGET_REF; #REF; followed a pattern similar to sequence-to-sequence models.",
                "They all implicitly or explicitly used distinct submodules to encode the definiendum and to generate the definientia.",
                "In the case of #REF , the encoding was the concatenation of the embedding of the definiendum, a vector representation of its sequence of characters derived from a characterlevel CNN, and its \"hypernym embedding\".",
                "#REF used a sigmoid-based gating module to tweak the definiendum embedding.",
                "The architecture proposed by is comprised of four modules, only one of which is used as a decoder: the remaining three are meant to convert the definiendum as a sparse embedding, select some of the sparse components of its meaning based on a provided context, and encode it into a representation adequate for the decoder."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: Despite some key differences, all of the previously proposed architectures we are aware of #TARGET_REF; #REF; followed a pattern similar to sequence-to-sequence models.\n sent1: They all implicitly or explicitly used distinct submodules to encode the definiendum and to generate the definientia.\n sent2: In the case of #REF , the encoding was the concatenation of the embedding of the definiendum, a vector representation of its sequence of characters derived from a characterlevel CNN, and its \"hypernym embedding\".\n sent3: #REF used a sigmoid-based gating module to tweak the definiendum embedding.\n sent4: The architecture proposed by is comprised of four modules, only one of which is used as a decoder: the remaining three are meant to convert the definiendum as a sparse embedding, select some of the sparse components of its meaning based on a provided context, and encode it into a representation adequate for the decoder.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "The contextualized definiendum encoding bears the trace of its context, but detailed information is irreparably lost.",
                "Hence, we refer to such an integration mechanism as a SELECT marking of the definiendum.",
                "When to apply marking, as introduced by eq. 4, is crucial when using the multiplicative marking scheme SELECT.",
                "Should we mark the definiendum before encoding, then only the definiendum embedding is passed into the encoder: the resulting system provides out-of-context definitions, like in #TARGET_REF where the definition is not linked to the context of a word but to its definiendum only.",
                "For context to be taken into account under the multiplicative strategy, tokens w k must be encoded and contextualized before integration with the indicator i k ."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: The contextualized definiendum encoding bears the trace of its context, but detailed information is irreparably lost.\n sent1: Hence, we refer to such an integration mechanism as a SELECT marking of the definiendum.\n sent2: When to apply marking, as introduced by eq. 4, is crucial when using the multiplicative marking scheme SELECT.\n sent3: Should we mark the definiendum before encoding, then only the definiendum embedding is passed into the encoder: the resulting system provides out-of-context definitions, like in #TARGET_REF where the definition is not linked to the context of a word but to its definiendum only.\n sent4: For context to be taken into account under the multiplicative strategy, tokens w k must be encoded and contextualized before integration with the indicator i k .\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "This lends empirical support to our claim that definition modeling is a nontrivial sequence-to-sequence task, which can be better treated with sequence methods.",
                "The stability of the performance improvement over the noncontextual variant in both contextual datasets also highlights that our proposed additive marking is fairly robust, and functions equally well when confronted to somewhat artificial inputs, as in D Gad , or to linguistically coherent sequences, as in D Ctx .",
                "A manual analysis of definitions produced by our system reveals issues similar to those discussed by #TARGET_REF , namely selfreference, 7 POS-mismatches, over-and underspecificity, antonymy, and incoherence.",
                "Annotating distinct productions from the validation set, for the non-contextual model trained on D Nor , we counted 9.9% of self-references, 11.6% POSmismatches, and 1.3% of words defined as their antonyms.",
                "We counted POS-mismatches whenever the definition seemed to fit another part-of-speech than that of the definiendum, regardless of both of their meanings; cf."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: This lends empirical support to our claim that definition modeling is a nontrivial sequence-to-sequence task, which can be better treated with sequence methods.\n sent1: The stability of the performance improvement over the noncontextual variant in both contextual datasets also highlights that our proposed additive marking is fairly robust, and functions equally well when confronted to somewhat artificial inputs, as in D Gad , or to linguistically coherent sequences, as in D Ctx .\n sent2: A manual analysis of definitions produced by our system reveals issues similar to those discussed by #TARGET_REF , namely selfreference, 7 POS-mismatches, over-and underspecificity, antonymy, and incoherence.\n sent3: Annotating distinct productions from the validation set, for the non-contextual model trained on D Nor , we counted 9.9% of self-references, 11.6% POSmismatches, and 1.3% of words defined as their antonyms.\n sent4: We counted POS-mismatches whenever the definition seemed to fit another part-of-speech than that of the definiendum, regardless of both of their meanings; cf.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "As the source corresponds only to the definiendum, we conjecture that few parameters are required for the encoder.",
                "We use 1 layer for the encoder, 6 for the decoder, 300 dimensions per hidden representations and 6 heads for multi-head attention.",
                "We do not share vocabularies between the encoder and the decoder: therefore output tokens can only correspond to words attested as definientia.",
                "4 The dropout rate and warmup steps number were set using a hyperparameter search on the dataset from #TARGET_REF , during which encoder and decoder vocabulary were merged for computational simplicity and models stopped after 12,000 steps.",
                "We first fixed dropout to 0.1 and tested warmup step values between 1000 and 10,000 by increments of 1000, then focused on the most promising span (1000-4000 steps) and exhaustively tested dropout rates from 0.2 to 0.8 by increments of 0.1."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: As the source corresponds only to the definiendum, we conjecture that few parameters are required for the encoder.\n sent1: We use 1 layer for the encoder, 6 for the decoder, 300 dimensions per hidden representations and 6 heads for multi-head attention.\n sent2: We do not share vocabularies between the encoder and the decoder: therefore output tokens can only correspond to words attested as definientia.\n sent3: 4 The dropout rate and warmup steps number were set using a hyperparameter search on the dataset from #TARGET_REF , during which encoder and decoder vocabulary were merged for computational simplicity and models stopped after 12,000 steps.\n sent4: We first fixed dropout to 0.1 and tested warmup step values between 1000 and 10,000 by increments of 1000, then focused on the most promising span (1000-4000 steps) and exhaustively tested dropout rates from 0.2 to 0.8 by increments of 0.1.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Multi-task learning (MTL) in deep neural networks is typically a result of parameter sharing between two networks (of usually the same dimensions) #TARGET_REF .",
                "If you have two three-layered, recurrent neural networks, both with an embedding inner layer and each recurrent layer feeding the task-specific classifier function through a feed-forward neural network, we have 19 pairs of layers that could share parameters.",
                "With the option of having private spaces, this gives us 5 19 =19,073,486,328,125 possible MTL architectures.",
                "If we additionally consider soft sharing of parameters, the number of possible architectures grows infinite.",
                "It is obviously not feasible to search this space."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Multi-task learning (MTL) in deep neural networks is typically a result of parameter sharing between two networks (of usually the same dimensions) #TARGET_REF .\n sent1: If you have two three-layered, recurrent neural networks, both with an embedding inner layer and each recurrent layer feeding the task-specific classifier function through a feed-forward neural network, we have 19 pairs of layers that could share parameters.\n sent2: With the option of having private spaces, this gives us 5 19 =19,073,486,328,125 possible MTL architectures.\n sent3: If we additionally consider soft sharing of parameters, the number of possible architectures grows infinite.\n sent4: It is obviously not feasible to search this space.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Omitting t for simplicity, the output of the α layers is:",
                "where h A,k is a linear combination of the outputs that is fed to the k+1-th layer of task A, and a , b designates the stacking of two vectors a, b ∈ R D to a matrix M ∈ R 2×D .",
                "Subspaces (Virtanen, Klami, and #REF; #TARGET_REF ) should allow the model to focus on task-specific and shared features in different parts of its parameter space.",
                "Extending the α-layers to include subspaces, for 2 tasks and 2 subspaces, we obtain an α matrix ∈ R 4×4 that not only controls the interaction between the layers of both tasks, but also between their subspaces:",
                "where h A1,k is the output of the first subspace of the k-th layer of task A and h A1,k is the linear combination for the first subspace of task A. The input to the k + 1-th layer of task A is then the concatenation of both subspace outputs: h A,k = h A1,k , h A2,k ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Omitting t for simplicity, the output of the α layers is:\n sent1: where h A,k is a linear combination of the outputs that is fed to the k+1-th layer of task A, and a , b designates the stacking of two vectors a, b ∈ R D to a matrix M ∈ R 2×D .\n sent2: Subspaces (Virtanen, Klami, and #REF; #TARGET_REF ) should allow the model to focus on task-specific and shared features in different parts of its parameter space.\n sent3: Extending the α-layers to include subspaces, for 2 tasks and 2 subspaces, we obtain an α matrix ∈ R 4×4 that not only controls the interaction between the layers of both tasks, but also between their subspaces:\n sent4: where h A1,k is the output of the first subspace of the k-th layer of task A and h A1,k is the linear combination for the first subspace of task A. The input to the k + 1-th layer of task A is then the concatenation of both subspace outputs: h A,k = h A1,k , h A2,k .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Joint model Most work on MTL for NLP uses a single auxiliary task #TARGET_REF; Martínez #REF) .",
                "In this experiment, we use one sluice network to jointly learn our four tasks on the newswire domain and show results in Table 5 .",
                "Here, the low-level POS tagging and simplified SRL tasks are the only ones that benefit from hard parameter sharing highlighting that hard parameter sharing by itself is not sufficient for doing effective multi-task learning with semantic tasks.",
                "We rather require task-specific layers that can be used to transform the shared, low-level representation into a form that is able to capture more fine-grained task-specific knowl- edge.",
                "Sluice networks outperform single task models for all tasks, except chunking and achieve the best performance on 2/4 tasks in this challenging setting."
            ],
            "label": [
                "BACKGROUND",
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Joint model Most work on MTL for NLP uses a single auxiliary task #TARGET_REF; Martínez #REF) .\n sent1: In this experiment, we use one sluice network to jointly learn our four tasks on the newswire domain and show results in Table 5 .\n sent2: Here, the low-level POS tagging and simplified SRL tasks are the only ones that benefit from hard parameter sharing highlighting that hard parameter sharing by itself is not sufficient for doing effective multi-task learning with semantic tasks.\n sent3: We rather require task-specific layers that can be used to transform the shared, low-level representation into a form that is able to capture more fine-grained task-specific knowl- edge.\n sent4: Sluice networks outperform single task models for all tasks, except chunking and achieve the best performance on 2/4 tasks in this challenging setting.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"DIFFERENCES\"], \"context\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "To better understand the properties and behavior of our metaarchitecture, we conduct a series of analyses and ablations.",
                "Task Properties and Performance #TARGET_REF correlate meta-characteristics of task pairs and gains compared to hard parameter sharing across a large set of NLP task pairs.",
                "Similarly, we correlate various metacharacteristics with error reductions and α, β values.",
                "Most importantly, we find that a) multi-task learning gains, also in sluice networks, are higher when there is less training data; and b) sluice networks learn to share more when there is more variance in the training data (cross-task αs are higher, intra-task αs lower).",
                "Generally, α values at the inner layers correlate more strongly with meta-characteristics than α values at the outer layers."
            ],
            "label": [
                "BACKGROUND",
                "SIMILARITY"
            ]
        },
        "input": "sent0: To better understand the properties and behavior of our metaarchitecture, we conduct a series of analyses and ablations.\n sent1: Task Properties and Performance #TARGET_REF correlate meta-characteristics of task pairs and gains compared to hard parameter sharing across a large set of NLP task pairs.\n sent2: Similarly, we correlate various metacharacteristics with error reductions and α, β values.\n sent3: Most importantly, we find that a) multi-task learning gains, also in sluice networks, are higher when there is less training data; and b) sluice networks learn to share more when there is more variance in the training data (cross-task αs are higher, intra-task αs lower).\n sent4: Generally, α values at the inner layers correlate more strongly with meta-characteristics than α values at the outer layers.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"SIMILARITY\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Hard parameter sharing #TARGET_REF ) is easy to implement, reduces overfitting, but is only guaranteed to work for (certain types of) closely related tasks (#REF; #REF ).",
                "#REF apply a variation of hard parameter sharing to multi-domain multi-task sequence tagging with a shared CRF layer and domain-specific projection layers.",
                "Yang, Salakhutdinov, and #REF use hard parameter sharing to jointly learn different sequence-tagging tasks across languages.",
                "Martínez #REF explore a similar set-up, but sharing is limited to the initial layer.",
                "In all three papers, the amount of sharing between the networks is fixed in advance."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Hard parameter sharing #TARGET_REF ) is easy to implement, reduces overfitting, but is only guaranteed to work for (certain types of) closely related tasks (#REF; #REF ).\n sent1: #REF apply a variation of hard parameter sharing to multi-domain multi-task sequence tagging with a shared CRF layer and domain-specific projection layers.\n sent2: Yang, Salakhutdinov, and #REF use hard parameter sharing to jointly learn different sequence-tagging tasks across languages.\n sent3: Martínez #REF explore a similar set-up, but sharing is limited to the initial layer.\n sent4: In all three papers, the amount of sharing between the networks is fixed in advance.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Let W ∈ R M ×D be a matrix in which each row i corresponds to a model θ i with D parameters.",
                "The loss that sluice networks minimize, with a penalty term Ω, is then as follows:",
                "The loss functions L i are crossentropy functions of the form − y p(y) log q(y) where y i are the labels of task i. Note that sluice networks are not restricted to tasks with the same loss functions, but could also be applied to jointly learn regression and classification tasks.",
                "The weights λ i determine the importance of the different tasks during training.",
                "We explicitly add inductive bias to the model via the regularizer Ω below, but our model also implicitly learns regularization through multi-task learning #TARGET_REF ) mediated by the α parameters, while the β parameters are used to learn the mixture functions f (·), as detailed in the following."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Let W ∈ R M ×D be a matrix in which each row i corresponds to a model θ i with D parameters.\n sent1: The loss that sluice networks minimize, with a penalty term Ω, is then as follows:\n sent2: The loss functions L i are crossentropy functions of the form − y p(y) log q(y) where y i are the labels of task i. Note that sluice networks are not restricted to tasks with the same loss functions, but could also be applied to jointly learn regression and classification tasks.\n sent3: The weights λ i determine the importance of the different tasks during training.\n sent4: We explicitly add inductive bias to the model via the regularizer Ω below, but our model also implicitly learns regularization through multi-task learning #TARGET_REF ) mediated by the α parameters, while the β parameters are used to learn the mixture functions f (·), as detailed in the following.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Adding Inductive Bias Naturally, we can also add explicit inductive bias to sluice networks by partially constraining the regularizer or adding to the learned penalty.",
                "Inspired by work on shared-space component analysis (#REF ), we add a penalty to enforce a division of labor and discourage redundancy between shared and task-specific subspaces.",
                "While the networks can theoretically learn such a separation, an explicit constraint empirically leads to better results and enables the sluice networks to take better advantage of subspace-specific α-values.",
                "We introduce an orthogonality constraint #TARGET_REF ) between the layer-wise subspaces of each model:",
                "F , where M is the number of tasks, K is the number of layers, · 2 F is the squared Frobenius norm, and G m,k,1 and G m,k,2 are the first and second subspace respectively in the k-th layer of the m-th task model."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Adding Inductive Bias Naturally, we can also add explicit inductive bias to sluice networks by partially constraining the regularizer or adding to the learned penalty.\n sent1: Inspired by work on shared-space component analysis (#REF ), we add a penalty to enforce a division of labor and discourage redundancy between shared and task-specific subspaces.\n sent2: While the networks can theoretically learn such a separation, an explicit constraint empirically leads to better results and enables the sluice networks to take better advantage of subspace-specific α-values.\n sent3: We introduce an orthogonality constraint #TARGET_REF ) between the layer-wise subspaces of each model:\n sent4: F , where M is the number of tasks, K is the number of layers, · 2 F is the squared Frobenius norm, and G m,k,1 and G m,k,2 are the first and second subspace respectively in the k-th layer of the m-th task model.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "We train our models on each domain and evaluate them both on the indomain test set (Table 3 , top) as well as on the test sets of all other domains (Table 3 , bottom) to evaluate their out-ofdomain generalization ability.",
                "Note that due to this set-up, our results are not directly comparable to the results reported in , who only train on the WSJ domain and use OntoNotes 4.0.",
                "Baseline Models As baselines, we compare against i) a single-task model only trained on chunking; ii) the low supervision model , which predicts the auxiliary task at the first layer; iii) an MTL model based on hard parameter sharing #TARGET_REF ; and iv) cross-stitch networks (#REF) .",
                "We compare these against our complete sluice network with subspace constraints and learned α and β parameters.",
                "We implement all models in DyNet (#REF ) and make our code available at https://github.com/ sebastianruder/sluice-networks."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We train our models on each domain and evaluate them both on the indomain test set (Table 3 , top) as well as on the test sets of all other domains (Table 3 , bottom) to evaluate their out-ofdomain generalization ability.\n sent1: Note that due to this set-up, our results are not directly comparable to the results reported in , who only train on the WSJ domain and use OntoNotes 4.0.\n sent2: Baseline Models As baselines, we compare against i) a single-task model only trained on chunking; ii) the low supervision model , which predicts the auxiliary task at the first layer; iii) an MTL model based on hard parameter sharing #TARGET_REF ; and iv) cross-stitch networks (#REF) .\n sent3: We compare these against our complete sluice network with subspace constraints and learned α and β parameters.\n sent4: We implement all models in DyNet (#REF ) and make our code available at https://github.com/ sebastianruder/sluice-networks.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "where h A1,k is the output of the first subspace of the k-th layer of task A and h A1,k is the linear combination for the first subspace of task A. The input to the k + 1-th layer of task A is then the concatenation of both subspace outputs: h A,k = h A1,k , h A2,k .",
                "Different α weights correspond to different matrix regularizers Ω, including several ones that have been proposed previously for multi-task learning.",
                "We review those in Section 3.",
                "For now just observe that if all α-values are set to 0.25 (or any other constant), we obtain hard parameter sharing #TARGET_REF , which is equivalent to a heavy L 0 matrix regularizer.",
                "Adding Inductive Bias Naturally, we can also add explicit inductive bias to sluice networks by partially constraining the regularizer or adding to the learned penalty."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: where h A1,k is the output of the first subspace of the k-th layer of task A and h A1,k is the linear combination for the first subspace of task A. The input to the k + 1-th layer of task A is then the concatenation of both subspace outputs: h A,k = h A1,k , h A2,k .\n sent1: Different α weights correspond to different matrix regularizers Ω, including several ones that have been proposed previously for multi-task learning.\n sent2: We review those in Section 3.\n sent3: For now just observe that if all α-values are set to 0.25 (or any other constant), we obtain hard parameter sharing #TARGET_REF , which is equivalent to a heavy L 0 matrix regularizer.\n sent4: Adding Inductive Bias Naturally, we can also add explicit inductive bias to sluice networks by partially constraining the regularizer or adding to the learned penalty.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "On average, sluice networks significantly outperform all other model architectures on both in-domain and out-of-domain data and perform best for all domains, except for the telephone conversation (tc) domain, where they are outperformed by cross-stitch networks.",
                "The performance boost is particularly significant for the out-ofdomain setting, where sluice networks add more than 1 point in accuracy compared to hard parameter sharing and almost .5 compared to the strongest baseline on average, demonstrating that sluice networks are particularly useful to help a model generalize better.",
                "In contrast to previous studies on MTL (Martínez #REF; #TARGET_REF; Augenstein, Ruder, and Søgaard 2018) , our model also consistently outperforms single-task learning.",
                "Overall, this demonstrates that our meta-architecture for learning which parts of multi-task models to share, with a small set of additional parameters to learn, can achieve significant and consistent improvements over strong baseline methods.",
                "NER and SRL We now evaluate sluice nets on NER with POS tagging as auxiliary task and simplified semantic role labeling with POS tagging as auxiliary task."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: On average, sluice networks significantly outperform all other model architectures on both in-domain and out-of-domain data and perform best for all domains, except for the telephone conversation (tc) domain, where they are outperformed by cross-stitch networks.\n sent1: The performance boost is particularly significant for the out-ofdomain setting, where sluice networks add more than 1 point in accuracy compared to hard parameter sharing and almost .5 compared to the strongest baseline on average, demonstrating that sluice networks are particularly useful to help a model generalize better.\n sent2: In contrast to previous studies on MTL (Martínez #REF; #TARGET_REF; Augenstein, Ruder, and Søgaard 2018) , our model also consistently outperforms single-task learning.\n sent3: Overall, this demonstrates that our meta-architecture for learning which parts of multi-task models to share, with a small set of additional parameters to learn, can achieve significant and consistent improvements over strong baseline methods.\n sent4: NER and SRL We now evaluate sluice nets on NER with POS tagging as auxiliary task and simplified semantic role labeling with POS tagging as auxiliary task.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "As intelligent systems/robots are brought out of the laboratory and into the physical world, they must become capable of natural everyday conversation with their human users about their physical surroundings.",
                "Among other competencies, this involves the ability to learn and adapt mappings between words, phrases, and sentences in Natural Language (NL) and perceptual aspects of the external environment -this is widely known as the grounding problem.",
                "Our work is similar in spirit to e.g. (#REF; #REF) but advances it in several aspects #TARGET_REF .",
                "In this demo paper, we present a dialogue agent that learns visually grounded word meanings interactively from a human tutor, which we call: VOILA (Visually Optimised Interactive Learning Agent).",
                "Our goal is to enable this agent to learn to identify and describe objects/attributes (colour 1 http://www.furhatrobotics.com/ and shape in this case) in its immediate visual environment through interaction with human users, incrementally, over time."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: As intelligent systems/robots are brought out of the laboratory and into the physical world, they must become capable of natural everyday conversation with their human users about their physical surroundings.\n sent1: Among other competencies, this involves the ability to learn and adapt mappings between words, phrases, and sentences in Natural Language (NL) and perceptual aspects of the external environment -this is widely known as the grounding problem.\n sent2: Our work is similar in spirit to e.g. (#REF; #REF) but advances it in several aspects #TARGET_REF .\n sent3: In this demo paper, we present a dialogue agent that learns visually grounded word meanings interactively from a human tutor, which we call: VOILA (Visually Optimised Interactive Learning Agent).\n sent4: Our goal is to enable this agent to learn to identify and describe objects/attributes (colour 1 http://www.furhatrobotics.com/ and shape in this case) in its immediate visual environment through interaction with human users, incrementally, over time.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We developed a multimodal framework in support of building an interactive learning system, which loosely follows that of #TARGET_REF .",
                "The framework consists of two core modules:",
                "Vision Module The vision module produces visual attribute predictions, using two base feature categories: the HSV colour space for colour attributes, and a 'bag of visual words' (i.e. PHOW descriptors) for the object shapes/class.",
                "It consists of a set of binary classifiers -Logistic Regression SVM classifiers with Stochastic Gradient Descent (SGD) (#REF) -to incrementally learn attribute predictions.",
                "The visual classifiers ground visual attribute words such as 'red', 'circle' etc."
            ],
            "label": [
                "USE",
                "SIMILARITY"
            ]
        },
        "input": "sent0: We developed a multimodal framework in support of building an interactive learning system, which loosely follows that of #TARGET_REF .\n sent1: The framework consists of two core modules:\n sent2: Vision Module The vision module produces visual attribute predictions, using two base feature categories: the HSV colour space for colour attributes, and a 'bag of visual words' (i.e. PHOW descriptors) for the object shapes/class.\n sent3: It consists of a set of binary classifiers -Logistic Regression SVM classifiers with Stochastic Gradient Descent (SGD) (#REF) -to incrementally learn attribute predictions.\n sent4: The visual classifiers ground visual attribute words such as 'red', 'circle' etc.\n",
        "output": "{\"label\": [\"USE\", \"SIMILARITY\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "The first MDP performs a kind of active learning: the learner/agent only acquires the feedback from humans about a visual attribute if it is not confident enough already about its own predictions.",
                "Following previous work #TARGET_REF , here we use a positive confidence threshold, which determines when the agent believes its own predictions.",
                "For instance, the learner can ask either polar or WH-questions about an attribute if its confidence score is higher than a certain threshold; otherwise, there should be no interaction about that attribute. But as #REF point out the confidence score from a classifier is not reliable enough at the early stages of learning, so in order to find an optimum dialogue policy, a threshold should be able to dynamically adjust according to the previous learning performance of the agent.",
                "We therefore assign a separate but dependent component MDP for adjusting the threshold dynamically in order to optimise the trade-off between accuracy and cost.",
                "Note now that the adjusted confidence threshold will affect the agent's dialogue behaviour, modeled in the other MDP presented in the next section (natural interaction with humans)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The first MDP performs a kind of active learning: the learner/agent only acquires the feedback from humans about a visual attribute if it is not confident enough already about its own predictions.\n sent1: Following previous work #TARGET_REF , here we use a positive confidence threshold, which determines when the agent believes its own predictions.\n sent2: For instance, the learner can ask either polar or WH-questions about an attribute if its confidence score is higher than a certain threshold; otherwise, there should be no interaction about that attribute. But as #REF point out the confidence score from a classifier is not reliable enough at the early stages of learning, so in order to find an optimum dialogue policy, a threshold should be able to dynamically adjust according to the previous learning performance of the agent.\n sent3: We therefore assign a separate but dependent component MDP for adjusting the threshold dynamically in order to optimise the trade-off between accuracy and cost.\n sent4: Note now that the adjusted confidence threshold will affect the agent's dialogue behaviour, modeled in the other MDP presented in the next section (natural interaction with humans).\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Hence, the agent must learn from scratch: (1) the perceptual/visual categories themselves; and (2) how NL expressions map to these; and in addition, (3) as a standard conversational agent, the agent much also learn to conduct natural, spontaneous conversations with real humans.",
                "In this demonstration, VOILA plays the role of an interactive, concept learning agent that takes initiative in the dialogues and actively learns novel visual knowledge from the feedback from the human tutor.",
                "What sets VOILA apart from other work in this area is:",
                "• VOILA's dialogue strategy is optimised via Reinforcement Learning to achieve an optimal trade-off between the accuracy of the concepts it learns/has learnt from users, and the effort that the dialogues incur on the users: this is a form of active learning where the agent only asks about something if it doesn't already know the answer with some appropriate confidence (see #TARGET_REF for more detail).",
                "• VOILA is trained on a corpus of real HumanHuman conversations (#REF) , and is thus able to process natural human dialogue, which contains phenomena such as self-corrections, repetitions and restarts, pauses, fillers, and continuations VOILA is deployed onto Furhat, a humanlike robot head with a custom back-projected face, built-in stereo microphones, and a Microsoft"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Hence, the agent must learn from scratch: (1) the perceptual/visual categories themselves; and (2) how NL expressions map to these; and in addition, (3) as a standard conversational agent, the agent much also learn to conduct natural, spontaneous conversations with real humans.\n sent1: In this demonstration, VOILA plays the role of an interactive, concept learning agent that takes initiative in the dialogues and actively learns novel visual knowledge from the feedback from the human tutor.\n sent2: What sets VOILA apart from other work in this area is:\n sent3: • VOILA's dialogue strategy is optimised via Reinforcement Learning to achieve an optimal trade-off between the accuracy of the concepts it learns/has learnt from users, and the effort that the dialogues incur on the users: this is a form of active learning where the agent only asks about something if it doesn't already know the answer with some appropriate confidence (see #TARGET_REF for more detail).\n sent4: • VOILA is trained on a corpus of real HumanHuman conversations (#REF) , and is thus able to process natural human dialogue, which contains phenomena such as self-corrections, repetitions and restarts, pauses, fillers, and continuations VOILA is deployed onto Furhat, a humanlike robot head with a custom back-projected face, built-in stereo microphones, and a Microsoft\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The first MDP performs a kind of active learning: the learner/agent only acquires the feedback from humans about a visual attribute if it is not confident enough already about its own predictions.",
                "Following previous work (#REF) , here we use a positive confidence threshold, which determines when the agent believes its own predictions.",
                "For instance, the learner can ask either polar or WH-questions about an attribute if its confidence score is higher than a certain threshold; otherwise, there should be no interaction about that attribute. But as #TARGET_REF point out the confidence score from a classifier is not reliable enough at the early stages of learning, so in order to find an optimum dialogue policy, a threshold should be able to dynamically adjust according to the previous learning performance of the agent.",
                "We therefore assign a separate but dependent component MDP for adjusting the threshold dynamically in order to optimise the trade-off between accuracy and cost.",
                "Note now that the adjusted confidence threshold will affect the agent's dialogue behaviour, modeled in the other MDP presented in the next section (natural interaction with humans)."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: The first MDP performs a kind of active learning: the learner/agent only acquires the feedback from humans about a visual attribute if it is not confident enough already about its own predictions.\n sent1: Following previous work (#REF) , here we use a positive confidence threshold, which determines when the agent believes its own predictions.\n sent2: For instance, the learner can ask either polar or WH-questions about an attribute if its confidence score is higher than a certain threshold; otherwise, there should be no interaction about that attribute. But as #TARGET_REF point out the confidence score from a classifier is not reliable enough at the early stages of learning, so in order to find an optimum dialogue policy, a threshold should be able to dynamically adjust according to the previous learning performance of the agent.\n sent3: We therefore assign a separate but dependent component MDP for adjusting the threshold dynamically in order to optimise the trade-off between accuracy and cost.\n sent4: Note now that the adjusted confidence threshold will affect the agent's dialogue behaviour, modeled in the other MDP presented in the next section (natural interaction with humans).\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "A natural extension of interest from monolingual to multilingual word embeddings has occurred recently (#REF; #REF; #REFb; #REFa; #REFb; #REF; #REF; #REF; #REF) .",
                "When operating in multilingual settings, it is highly desirable to learn embeddings for words denoting similar concepts that are very close in the shared inter-lingual embedding space (e.g., the representations for the English word school and the Spanish word escuela should be very similar).",
                "These shared interlingual embedding spaces may then be used in a myriad of multilingual natural language processing tasks, such as fundamental tasks of computing cross-lingual and multilingual semantic word similarity and bilingual lexicon induction (BLI), etc.",
                "However, all these models critically require at least sentence-aligned parallel data and/or readilyavailable translation dictionaries to induce bilingual word embeddings (BWEs) that are consistent and closely aligned over languages in the same semantic space.",
                "Contributions In this work, we alleviate the requirements: (1) We present the first model that is able to induce bilingual word embeddings from non-parallel data without any other readily available translation resources such as pre-given bilingual lexicons; (2) We demonstrate the utility of BWEs induced by this simple yet effective model in the BLI task from comparable Wikipedia data on benchmarking datasets for three language pairs #TARGET_REF )."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: A natural extension of interest from monolingual to multilingual word embeddings has occurred recently (#REF; #REF; #REFb; #REFa; #REFb; #REF; #REF; #REF; #REF) .\n sent1: When operating in multilingual settings, it is highly desirable to learn embeddings for words denoting similar concepts that are very close in the shared inter-lingual embedding space (e.g., the representations for the English word school and the Spanish word escuela should be very similar).\n sent2: These shared interlingual embedding spaces may then be used in a myriad of multilingual natural language processing tasks, such as fundamental tasks of computing cross-lingual and multilingual semantic word similarity and bilingual lexicon induction (BLI), etc.\n sent3: However, all these models critically require at least sentence-aligned parallel data and/or readilyavailable translation dictionaries to induce bilingual word embeddings (BWEs) that are consistent and closely aligned over languages in the same semantic space.\n sent4: Contributions In this work, we alleviate the requirements: (1) We present the first model that is able to induce bilingual word embeddings from non-parallel data without any other readily available translation resources such as pre-given bilingual lexicons; (2) We demonstrate the utility of BWEs induced by this simple yet effective model in the BLI task from comparable Wikipedia data on benchmarking datasets for three language pairs #TARGET_REF ).\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Training Data We use comparable Wikipedia data introduced in (Vulić and #REFa; #TARGET_REF ) available in three language pairs to induce bilingual word embeddings: (i) a collection of 13, 696 Spanish-English Wikipedia article pairs (ES-EN), (ii) a collection of 18, 898 ItalianEnglish Wikipedia article pairs (IT-EN), and (iii) a collection of 7, 612 Dutch-English Wikipedia article pairs (NL-EN).",
                "All corpora are theme-aligned comparable corpora, that is, the aligned document pairs discuss similar themes, but are in general not direct translations.",
                "Following prior work (#REF; #REF; Vulić and #REFb) , we retain only nouns that occur at least 5 times in the corpus.",
                "Lemmatized word forms are recorded when available, and original forms otherwise.",
                "TreeTagger (#REF ) is used for POS tagging and lemmatization."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: Training Data We use comparable Wikipedia data introduced in (Vulić and #REFa; #TARGET_REF ) available in three language pairs to induce bilingual word embeddings: (i) a collection of 13, 696 Spanish-English Wikipedia article pairs (ES-EN), (ii) a collection of 18, 898 ItalianEnglish Wikipedia article pairs (IT-EN), and (iii) a collection of 7, 612 Dutch-English Wikipedia article pairs (NL-EN).\n sent1: All corpora are theme-aligned comparable corpora, that is, the aligned document pairs discuss similar themes, but are in general not direct translations.\n sent2: Following prior work (#REF; #REF; Vulić and #REFb) , we retain only nouns that occur at least 5 times in the corpus.\n sent3: Lemmatized word forms are recorded when available, and original forms otherwise.\n sent4: TreeTagger (#REF ) is used for POS tagging and lemmatization.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "(2) Assoc-BLI -A BLI model that represents words as vectors of association norms (Roller and Schulte im #REF) over both vocabularies, where these norms are computed using a multilingual topic model (Vulić and #REFa) .",
                "(3) PPMI+cos -A standard distributional model for BLI relying on positive pointwise mutual information and cosine similarity (#REF) .",
                "The seed lexicon is bootstrapped using the method from (Peirsman and Padó, 2011; #TARGET_REF .",
                "All parameters of the baseline BLI models (i.e., topic models and their settings, the number of dimensions K, feature pruning values, window size) are set to their optimal values according to suggestions in prior work (#REF; Vulić and #REFa; Vulić and #REFb; #REF) .",
                "Due to space constraints, for (much) more details about the baselines we point to the relevant literature (Peirsman and Padó, 2011; #REF; Vulić and #REFa; Vulić and #REFb) ."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: (2) Assoc-BLI -A BLI model that represents words as vectors of association norms (Roller and Schulte im #REF) over both vocabularies, where these norms are computed using a multilingual topic model (Vulić and #REFa) .\n sent1: (3) PPMI+cos -A standard distributional model for BLI relying on positive pointwise mutual information and cosine similarity (#REF) .\n sent2: The seed lexicon is bootstrapped using the method from (Peirsman and Padó, 2011; #TARGET_REF .\n sent3: All parameters of the baseline BLI models (i.e., topic models and their settings, the number of dimensions K, feature pruning values, window size) are set to their optimal values according to suggestions in prior work (#REF; Vulić and #REFa; Vulić and #REFb; #REF) .\n sent4: Due to space constraints, for (much) more details about the baselines we point to the relevant literature (Peirsman and Padó, 2011; #REF; Vulić and #REFa; Vulić and #REFb) .\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "(2) Assoc-BLI -A BLI model that represents words as vectors of association norms (Roller and Schulte im #REF) over both vocabularies, where these norms are computed using a multilingual topic model (Vulić and #REFa) .",
                "(3) PPMI+cos -A standard distributional model for BLI relying on positive pointwise mutual information and cosine similarity (#REF) .",
                "The seed lexicon is bootstrapped using the method from (Peirsman and Padó, 2011; Vulić and #REFb) .",
                "All parameters of the baseline BLI models (i.e., topic models and their settings, the number of dimensions K, feature pruning values, window size) are set to their optimal values according to suggestions in prior work (#REF; Vulić and #REFa; #TARGET_REF; #REF) .",
                "Due to space constraints, for (much) more details about the baselines we point to the relevant literature (Peirsman and Padó, 2011; #REF; Vulić and #REFa; Vulić and #REFb) ."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: (2) Assoc-BLI -A BLI model that represents words as vectors of association norms (Roller and Schulte im #REF) over both vocabularies, where these norms are computed using a multilingual topic model (Vulić and #REFa) .\n sent1: (3) PPMI+cos -A standard distributional model for BLI relying on positive pointwise mutual information and cosine similarity (#REF) .\n sent2: The seed lexicon is bootstrapped using the method from (Peirsman and Padó, 2011; Vulić and #REFb) .\n sent3: All parameters of the baseline BLI models (i.e., topic models and their settings, the number of dimensions K, feature pruning values, window size) are set to their optimal values according to suggestions in prior work (#REF; Vulić and #REFa; #TARGET_REF; #REF) .\n sent4: Due to space constraints, for (much) more details about the baselines we point to the relevant literature (Peirsman and Padó, 2011; #REF; Vulić and #REFa; Vulić and #REFb) .\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "All parameters of the baseline BLI models (i.e., topic models and their settings, the number of dimensions K, feature pruning values, window size) are set to their optimal values according to suggestions in prior work (#REF; Vulić and #REFa; Vulić and #REFb; #REF) .",
                "Due to space constraints, for (much) more details about the baselines we point to the relevant literature (Peirsman and Padó, 2011; #REF; Vulić and #REFa; Vulić and #REFb) .",
                "Test Data For each language pair, we evaluate on standard 1,000 ground truth one-to-one translation pairs built for the three language pairs (ES/IT/NL-EN) (Vulić and #REFa; #TARGET_REF .",
                "Translation direction is ES/IT/NL → EN.",
                "Evaluation Metrics Since we can build a oneto-one bilingual lexicon by harvesting one-to-one translation pairs, the lexicon qualiy is best reflected in the Acc 1 score, that is, the number of source language (ES/IT/NL) words w S i from ground truth translation pairs for which the top ranked word cross-lingually is the correct trans- Table 1 : Example lists of top 10 semantically similar words for all 3 language pairs obtained using BWESG+cos; d = 200, cs = 48; (col 1.) only source language words (ES/IT/NL) are listed while target language words are skipped (monolingual similarity); (2) only target language words (EN) are listed (cross-lingual similarity); (3) words from both languages are listed (multilingual similarity)."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: All parameters of the baseline BLI models (i.e., topic models and their settings, the number of dimensions K, feature pruning values, window size) are set to their optimal values according to suggestions in prior work (#REF; Vulić and #REFa; Vulić and #REFb; #REF) .\n sent1: Due to space constraints, for (much) more details about the baselines we point to the relevant literature (Peirsman and Padó, 2011; #REF; Vulić and #REFa; Vulić and #REFb) .\n sent2: Test Data For each language pair, we evaluate on standard 1,000 ground truth one-to-one translation pairs built for the three language pairs (ES/IT/NL-EN) (Vulić and #REFa; #TARGET_REF .\n sent3: Translation direction is ES/IT/NL → EN.\n sent4: Evaluation Metrics Since we can build a oneto-one bilingual lexicon by harvesting one-to-one translation pairs, the lexicon qualiy is best reflected in the Acc 1 score, that is, the number of source language (ES/IT/NL) words w S i from ground truth translation pairs for which the top ranked word cross-lingually is the correct trans- Table 1 : Example lists of top 10 semantically similar words for all 3 language pairs obtained using BWESG+cos; d = 200, cs = 48; (col 1.) only source language words (ES/IT/NL) are listed while target language words are skipped (monolingual similarity); (2) only target language words (EN) are listed (cross-lingual similarity); (3) words from both languages are listed (multilingual similarity).\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Following prior work (#REF; #REF; #TARGET_REF , we retain only nouns that occur at least 5 times in the corpus.",
                "Lemmatized word forms are recorded when available, and original forms otherwise.",
                "TreeTagger (#REF ) is used for POS tagging and lemmatization.",
                "After the preprocessing vocabularies comprise between 7,000 and 13,000 noun types for each language in each language pair.",
                "Exactly the same training data and vocabularies are used to induce bilingual lexicons with all other BLI models in comparison."
            ],
            "label": [
                "DIFFERENCES",
                "EXTENDS"
            ]
        },
        "input": "sent0: Following prior work (#REF; #REF; #TARGET_REF , we retain only nouns that occur at least 5 times in the corpus.\n sent1: Lemmatized word forms are recorded when available, and original forms otherwise.\n sent2: TreeTagger (#REF ) is used for POS tagging and lemmatization.\n sent3: After the preprocessing vocabularies comprise between 7,000 and 13,000 noun types for each language in each language pair.\n sent4: Exactly the same training data and vocabularies are used to induce bilingual lexicons with all other BLI models in comparison.\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"EXTENDS\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "The seed lexicon is bootstrapped using the method from (Peirsman and Padó, 2011; Vulić and #REFb) .",
                "All parameters of the baseline BLI models (i.e., topic models and their settings, the number of dimensions K, feature pruning values, window size) are set to their optimal values according to suggestions in prior work (#REF; Vulić and #REFa; Vulić and #REFb; #REF) .",
                "Due to space constraints, for (much) more details about the baselines we point to the relevant literature (Peirsman and Padó, 2011; #REF; Vulić and #REFa; #TARGET_REF .",
                "Test Data For each language pair, we evaluate on standard 1,000 ground truth one-to-one translation pairs built for the three language pairs (ES/IT/NL-EN) (Vulić and #REFa; Vulić and #REFb) .",
                "Translation direction is ES/IT/NL → EN."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The seed lexicon is bootstrapped using the method from (Peirsman and Padó, 2011; Vulić and #REFb) .\n sent1: All parameters of the baseline BLI models (i.e., topic models and their settings, the number of dimensions K, feature pruning values, window size) are set to their optimal values according to suggestions in prior work (#REF; Vulić and #REFa; Vulić and #REFb; #REF) .\n sent2: Due to space constraints, for (much) more details about the baselines we point to the relevant literature (Peirsman and Padó, 2011; #REF; Vulić and #REFa; #TARGET_REF .\n sent3: Test Data For each language pair, we evaluate on standard 1,000 ground truth one-to-one translation pairs built for the three language pairs (ES/IT/NL-EN) (Vulić and #REFa; Vulić and #REFb) .\n sent4: Translation direction is ES/IT/NL → EN.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Translation direction is ES/IT/NL → EN.",
                "Evaluation Metrics Since we can build a oneto-one bilingual lexicon by harvesting one-to-one translation pairs, the lexicon qualiy is best reflected in the Acc 1 score, that is, the number of source language (ES/IT/NL) words w S i from ground truth translation pairs for which the top ranked word cross-lingually is the correct trans- Table 1 : Example lists of top 10 semantically similar words for all 3 language pairs obtained using BWESG+cos; d = 200, cs = 48; (col 1.) only source language words (ES/IT/NL) are listed while target language words are skipped (monolingual similarity); (2) only target language words (EN) are listed (cross-lingual similarity); (3) words from both languages are listed (multilingual similarity).",
                "EN words are given in italic.",
                "The correct one-to-one translation for each source word is marked by (+).",
                "lation in the other language (EN) according to the ground truth over the total number of ground truth translation pairs (=1000) (#REF; #REF; #TARGET_REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Translation direction is ES/IT/NL → EN.\n sent1: Evaluation Metrics Since we can build a oneto-one bilingual lexicon by harvesting one-to-one translation pairs, the lexicon qualiy is best reflected in the Acc 1 score, that is, the number of source language (ES/IT/NL) words w S i from ground truth translation pairs for which the top ranked word cross-lingually is the correct trans- Table 1 : Example lists of top 10 semantically similar words for all 3 language pairs obtained using BWESG+cos; d = 200, cs = 48; (col 1.) only source language words (ES/IT/NL) are listed while target language words are skipped (monolingual similarity); (2) only target language words (EN) are listed (cross-lingual similarity); (3) words from both languages are listed (multilingual similarity).\n sent2: EN words are given in italic.\n sent3: The correct one-to-one translation for each source word is marked by (+).\n sent4: lation in the other language (EN) according to the ground truth over the total number of ground truth translation pairs (=1000) (#REF; #REF; #TARGET_REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "We have demonstrated its utility in the task of bilingual lexicon induction from such comparable data, where our new BWESG-based BLI model outperforms state-of-the-art models for BLI from document-aligned comparable data and related BWE induction models.",
                "The low-cost BWEs may be used in other (semantic) tasks besides the ones discussed here, and it would be interesting to experiment with other types of context aggregation and selection beyond random shuffling, and other objective functions.",
                "Preliminary studies also demonstrate the utility of the BWEs in monolingual and cross-lingual information retrieval (Vulić and #REF) .",
                "Finally, we may use the knowledge of BWEs obtained by BWESG from document-aligned data to learn bilingual correspondences (e.g., word translation pairs or lists of semantically similar words across languages) which may in turn be used for representation learning from large unaligned multilingual datasets as proposed in (#REF; #REFb; #TARGET_REF .",
                "In the long run, this idea may lead to large-scale fully data-driven representation learning models from huge amounts of multilingual data without any \"pre-requirement\" for parallel data or manually built lexicons."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: We have demonstrated its utility in the task of bilingual lexicon induction from such comparable data, where our new BWESG-based BLI model outperforms state-of-the-art models for BLI from document-aligned comparable data and related BWE induction models.\n sent1: The low-cost BWEs may be used in other (semantic) tasks besides the ones discussed here, and it would be interesting to experiment with other types of context aggregation and selection beyond random shuffling, and other objective functions.\n sent2: Preliminary studies also demonstrate the utility of the BWEs in monolingual and cross-lingual information retrieval (Vulić and #REF) .\n sent3: Finally, we may use the knowledge of BWEs obtained by BWESG from document-aligned data to learn bilingual correspondences (e.g., word translation pairs or lists of semantically similar words across languages) which may in turn be used for representation learning from large unaligned multilingual datasets as proposed in (#REF; #REFb; #TARGET_REF .\n sent4: In the long run, this idea may lead to large-scale fully data-driven representation learning models from huge amounts of multilingual data without any \"pre-requirement\" for parallel data or manually built lexicons.\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Tree-based approaches to alignment model translation as a sequence of probabilistic operations transforming the syntactic parse tree of a sentence in one language into that of the other.",
                "The trees may be learned directly from parallel corpora #TARGET_REF), or provided by a parser trained on hand-annotated treebanks (#REF) .",
                "In this paper, we compare these approaches on Chinese-English and French-English datasets, and find that automatically derived trees result in better agreement with human-annotated word-level alignments for unseen test data."
            ],
            "label": [
                "BACKGROUND",
                "USE"
            ]
        },
        "input": "sent0: Tree-based approaches to alignment model translation as a sequence of probabilistic operations transforming the syntactic parse tree of a sentence in one language into that of the other.\n sent1: The trees may be learned directly from parallel corpora #TARGET_REF), or provided by a parser trained on hand-annotated treebanks (#REF) .\n sent2: In this paper, we compare these approaches on Chinese-English and French-English datasets, and find that automatically derived trees result in better agreement with human-annotated word-level alignments for unseen test data.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"USE\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The tree-based approach allows us to represent the fact that syntactic constituents tend to move as unit, as well as systematic differences in word order in the grammars of the two languages.",
                "Furthermore, the tree structure allows us to make probabilistic independence assumptions that result in polynomial time algorithms for estimating a translation model from parallel training data, and for finding the highest probability translation given a new sentence.",
                "#TARGET_REF modeled the reordering process with binary branching trees, where each production could be either in the same or in reverse order going from source to target language.",
                "The trees of Wu's Inversion Transduction Grammar were derived by synchronously parsing a parallel corpus, using a grammar with lexical translation probabilities at the leaves and a simple grammar with a single nonterminal providing the tree structure.",
                "While this grammar did not represent traditional syntactic categories such as verb phrases and noun phrases, it served to restrict the word-level alignments considered by the system to those allowable by reordering operations on binary trees."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The tree-based approach allows us to represent the fact that syntactic constituents tend to move as unit, as well as systematic differences in word order in the grammars of the two languages.\n sent1: Furthermore, the tree structure allows us to make probabilistic independence assumptions that result in polynomial time algorithms for estimating a translation model from parallel training data, and for finding the highest probability translation given a new sentence.\n sent2: #TARGET_REF modeled the reordering process with binary branching trees, where each production could be either in the same or in reverse order going from source to target language.\n sent3: The trees of Wu's Inversion Transduction Grammar were derived by synchronously parsing a parallel corpus, using a grammar with lexical translation probabilities at the leaves and a simple grammar with a single nonterminal providing the tree structure.\n sent4: While this grammar did not represent traditional syntactic categories such as verb phrases and noun phrases, it served to restrict the word-level alignments considered by the system to those allowable by reordering operations on binary trees.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "While this grammar did not represent traditional syntactic categories such as verb phrases and noun phrases, it served to restrict the word-level alignments considered by the system to those allowable by reordering operations on binary trees.",
                "This restriction corresponds to intuitions about the alignments that could be produced by systematic differences between the two language's grammars, and allows for a polynomial time algorithm for finding the highest-probability alignment, and for re-estimation of the lexical translation and grammar probabilities using the Expectation Maximization algorithm.",
                "#REF present an algorithm for estimating probabilistic parameters for a similar model which represents translation as a sequence of re-ordering operations over children of nodes in a syntactic tree, using automatic parser output for the initial tree structures.",
                "This gives the translation model more information about the structure of the source language, and further constrains the reorderings to match not just a possible bracketing as in #TARGET_REF , but the specific bracketing of the parse tree provided.",
                "In this paper, we make a direct comparison of a syntactically unsupervised alignment model, based on #REF , with a syntactically supervised model, based on #REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: While this grammar did not represent traditional syntactic categories such as verb phrases and noun phrases, it served to restrict the word-level alignments considered by the system to those allowable by reordering operations on binary trees.\n sent1: This restriction corresponds to intuitions about the alignments that could be produced by systematic differences between the two language's grammars, and allows for a polynomial time algorithm for finding the highest-probability alignment, and for re-estimation of the lexical translation and grammar probabilities using the Expectation Maximization algorithm.\n sent2: #REF present an algorithm for estimating probabilistic parameters for a similar model which represents translation as a sequence of re-ordering operations over children of nodes in a syntactic tree, using automatic parser output for the initial tree structures.\n sent3: This gives the translation model more information about the structure of the source language, and further constrains the reorderings to match not just a possible bracketing as in #TARGET_REF , but the specific bracketing of the parse tree provided.\n sent4: In this paper, we make a direct comparison of a syntactically unsupervised alignment model, based on #REF , with a syntactically supervised model, based on #REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The Inversion Transduction Grammar of #TARGET_REF can be thought as a a generative process which simultaneously produces strings in both languages through a series of synchronous context-free grammar productions.",
                "The grammar is restricted to binary rules, which can have the symbols in the right hand side appear in the same order in both languages, represented with square brackets:",
                "or the symbols may appear in reverse order in the two languages, indicated by angle brackets:",
                "Individual lexical translations between English words e and French words f take place at the leaves of the tree, generated by grammar rules with a single right hand side symbol in each language:",
                "Given a bilingual sentence pair, a synchronous parse can be built using a two-dimensional extension of chart parsing, where chart items are indexed by their nonterminal Y and beginning and ending positions l, m in the source language string, and beginning and ending positions i, j in the target language string."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The Inversion Transduction Grammar of #TARGET_REF can be thought as a a generative process which simultaneously produces strings in both languages through a series of synchronous context-free grammar productions.\n sent1: The grammar is restricted to binary rules, which can have the symbols in the right hand side appear in the same order in both languages, represented with square brackets:\n sent2: or the symbols may appear in reverse order in the two languages, indicated by angle brackets:\n sent3: Individual lexical translations between English words e and French words f take place at the leaves of the tree, generated by grammar rules with a single right hand side symbol in each language:\n sent4: Given a bilingual sentence pair, a synchronous parse can be built using a two-dimensional extension of chart parsing, where chart items are indexed by their nonterminal Y and beginning and ending positions l, m in the source language string, and beginning and ending positions i, j in the target language string.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "IBM Models 1 and 4 refer to #REF .",
                "We used the GIZA++ package, including the HMM model of #REF .",
                "We ran Model 1 for three iterations, then the HMM model for three iterations, and finally Model 4 for two iterations, training each model until AER began to increase on our held-out cross validation data.",
                "\"Inversion Transduction Grammar\" (ITG) is the model of #TARGET_REF , \"Tree-to-String\" is the model of #REF , and \"Tree-to-String, Clone\" allows the node cloning operation described above.",
                "Our tree-based models were initialized from uniform distributions for both the lexical translation probabilities and the tree reordering operations, and were trained until AER began to rise on our held-out cross-validation data, which turned out to be four iterations for the tree-to-string models and three for the Inversion Transduction Grammar."
            ],
            "label": [
                "BACKGROUND",
                "USE"
            ]
        },
        "input": "sent0: IBM Models 1 and 4 refer to #REF .\n sent1: We used the GIZA++ package, including the HMM model of #REF .\n sent2: We ran Model 1 for three iterations, then the HMM model for three iterations, and finally Model 4 for two iterations, training each model until AER began to increase on our held-out cross validation data.\n sent3: \"Inversion Transduction Grammar\" (ITG) is the model of #TARGET_REF , \"Tree-to-String\" is the model of #REF , and \"Tree-to-String, Clone\" allows the node cloning operation described above.\n sent4: Our tree-based models were initialized from uniform distributions for both the lexical translation probabilities and the tree reordering operations, and were trained until AER began to rise on our held-out cross-validation data, which turned out to be four iterations for the tree-to-string models and three for the Inversion Transduction Grammar.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"USE\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "They train the models of #REF .",
                "Decoding, meaning exact computation of the highest probability translation given a foreign sentence, is not possible in polynomial time for the IBM models, and in practice decoders search through the space of hypothesis translations using a set of additional, hard alignment constraints.",
                "#REF compute the viterbi alignments for German-English and French-English sentences pairs using IBM Model 5, and then measure how many of the resulting alignments fall within the hard constraints of both #TARGET_REF and #REF .",
                "They find higher coverage for an extended version of ITG than for the IBM decoding constraint for both language pairs, with the unmodified ITG implementation covering about the same amount of German-English data as IBM, and significantly less French-English data.",
                "These results show promise for ITG as a basis for efficient decoding, but do not address which model best aligns the original training data, as IBMderived alignments were taken as the gold standard, rather than human alignments."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: They train the models of #REF .\n sent1: Decoding, meaning exact computation of the highest probability translation given a foreign sentence, is not possible in polynomial time for the IBM models, and in practice decoders search through the space of hypothesis translations using a set of additional, hard alignment constraints.\n sent2: #REF compute the viterbi alignments for German-English and French-English sentences pairs using IBM Model 5, and then measure how many of the resulting alignments fall within the hard constraints of both #TARGET_REF and #REF .\n sent3: They find higher coverage for an extended version of ITG than for the IBM decoding constraint for both language pairs, with the unmodified ITG implementation covering about the same amount of German-English data as IBM, and significantly less French-English data.\n sent4: These results show promise for ITG as a basis for efficient decoding, but do not address which model best aligns the original training data, as IBMderived alignments were taken as the gold standard, rather than human alignments.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "This restriction corresponds to intuitions about the alignments that could be produced by systematic differences between the two language's grammars, and allows for a polynomial time algorithm for finding the highest-probability alignment, and for re-estimation of the lexical translation and grammar probabilities using the Expectation Maximization algorithm.",
                "#REF present an algorithm for estimating probabilistic parameters for a similar model which represents translation as a sequence of re-ordering operations over children of nodes in a syntactic tree, using automatic parser output for the initial tree structures.",
                "This gives the translation model more information about the structure of the source language, and further constrains the reorderings to match not just a possible bracketing as in #REF , but the specific bracketing of the parse tree provided.",
                "In this paper, we make a direct comparison of a syntactically unsupervised alignment model, based on #TARGET_REF , with a syntactically supervised model, based on #REF .",
                "We use the term syntactically supervised to indicate that the syntactic structure in one language is given to the training procedure."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This restriction corresponds to intuitions about the alignments that could be produced by systematic differences between the two language's grammars, and allows for a polynomial time algorithm for finding the highest-probability alignment, and for re-estimation of the lexical translation and grammar probabilities using the Expectation Maximization algorithm.\n sent1: #REF present an algorithm for estimating probabilistic parameters for a similar model which represents translation as a sequence of re-ordering operations over children of nodes in a syntactic tree, using automatic parser output for the initial tree structures.\n sent2: This gives the translation model more information about the structure of the source language, and further constrains the reorderings to match not just a possible bracketing as in #REF , but the specific bracketing of the parse tree provided.\n sent3: In this paper, we make a direct comparison of a syntactically unsupervised alignment model, based on #TARGET_REF , with a syntactically supervised model, based on #REF .\n sent4: We use the term syntactically supervised to indicate that the syntactic structure in one language is given to the training procedure.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Given a bilingual sentence pair, a synchronous parse can be built using a two-dimensional extension of chart parsing, where chart items are indexed by their nonterminal Y and beginning and ending positions l, m in the source language string, and beginning and ending positions i, j in the target language string.",
                "For Expectation Maximization training, we compute inside probabilities β(Y, l, m, i, j) from the bottom up as outlined below:",
                "A similar recursion is used to compute outside probabilities for each chart item, and the inside and outside probabilities are combined to derive expected counts for occurrence of each grammar rule, including the rules corresponding to individual lexical translations.",
                "In our experiments we use a grammar with a start symbol S, a single preterminal C, and two nonterminals A and B used to ensure that only one parse can generate any given word-level alignment (ignoring insertions and deletions) #TARGET_REF; #REF) .",
                "The individual lexical translations produced by the grammar may include a NULL word on either side, in order to represent insertions and deletions."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Given a bilingual sentence pair, a synchronous parse can be built using a two-dimensional extension of chart parsing, where chart items are indexed by their nonterminal Y and beginning and ending positions l, m in the source language string, and beginning and ending positions i, j in the target language string.\n sent1: For Expectation Maximization training, we compute inside probabilities β(Y, l, m, i, j) from the bottom up as outlined below:\n sent2: A similar recursion is used to compute outside probabilities for each chart item, and the inside and outside probabilities are combined to derive expected counts for occurrence of each grammar rule, including the rules corresponding to individual lexical translations.\n sent3: In our experiments we use a grammar with a start symbol S, a single preterminal C, and two nonterminals A and B used to ensure that only one parse can generate any given word-level alignment (ignoring insertions and deletions) #TARGET_REF; #REF) .\n sent4: The individual lexical translations produced by the grammar may include a NULL word on either side, in order to represent insertions and deletions.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "More recently, #TARGET_REF put the task of modeling derivation into the perspective of zero-shot-learning: instead of using cosine similarities they predicted the derived term by learning a mapping function between the base term and the derived term.",
                "Once the predicted vector was computed, a nearest neighbor search was applied to validate if the prediction corresponded to the derived term.",
                "In zero-shotlearning the task is to predict novel values, i.e., values that were never seen in training.",
                "More formally, zero-shot-learning trains a classifier f : X → Y that predicts novel values for Y (#REF) .",
                "It is often applied across vector spaces, such as different domains (#REF; ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: More recently, #TARGET_REF put the task of modeling derivation into the perspective of zero-shot-learning: instead of using cosine similarities they predicted the derived term by learning a mapping function between the base term and the derived term.\n sent1: Once the predicted vector was computed, a nearest neighbor search was applied to validate if the prediction corresponded to the derived term.\n sent2: In zero-shotlearning the task is to predict novel values, i.e., values that were never seen in training.\n sent3: More formally, zero-shot-learning trains a classifier f : X → Y that predicts novel values for Y (#REF) .\n sent4: It is often applied across vector spaces, such as different domains (#REF; .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "More formally, zero-shot-learning trains a classifier f : X → Y that predicts novel values for Y (#REF) .",
                "It is often applied across vector spaces, such as different domains (#REF; .",
                "The experiments by #TARGET_REF were performed over six derivational patterns for German (cf.",
                "Table 1), including particle verbs (PVs) with two different particle prefixes (an and durch), which were particularly difficult to predict.",
                "PVs such as anfangen (to start) are compositions of a base verb (BV) such as fangen (to catch) and a verb particle such as an."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: More formally, zero-shot-learning trains a classifier f : X → Y that predicts novel values for Y (#REF) .\n sent1: It is often applied across vector spaces, such as different domains (#REF; .\n sent2: The experiments by #TARGET_REF were performed over six derivational patterns for German (cf.\n sent3: Table 1), including particle verbs (PVs) with two different particle prefixes (an and durch), which were particularly difficult to predict.\n sent4: PVs such as anfangen (to start) are compositions of a base verb (BV) such as fangen (to catch) and a verb particle such as an.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "AvgAdd is a re-implementation of the best method in #TARGET_REF :",
                "3 For each affix, the method learns a difference vector by computing the dimension-wise differences between the vector representations of base term A and derived term B .",
                "The method thus learns a centroid c for all relevant training pairs (N ) with the same affix:",
                "For each PV test instance with this affix, the learned centroid vector is added dimensionwise to the vector representation of the base term to predict a position for the derived term."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: AvgAdd is a re-implementation of the best method in #TARGET_REF :\n sent1: 3 For each affix, the method learns a difference vector by computing the dimension-wise differences between the vector representations of base term A and derived term B .\n sent2: The method thus learns a centroid c for all relevant training pairs (N ) with the same affix:\n sent3: For each PV test instance with this affix, the learned centroid vector is added dimensionwise to the vector representation of the base term to predict a position for the derived term.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "As in #TARGET_REF , we treat every derivation type as a specific learning problem: we take a set of word pairs with a particular derivation pattern (e.g., \"-in\", Bäcker::Bäckerin), and divide this set into training and test pairs by performing 10-fold cross-validation.",
                "For the test pairs, we predict the vectors of the derived terms (e.g.,",
                "The search space includes all corpus words across parts-of-speech, except for the base term.",
                "The performance is measured in terms of recall-out-of-5 (#REF), counting how often the correct derived term is found among the five nearest neighbors of the predicted vector."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: As in #TARGET_REF , we treat every derivation type as a specific learning problem: we take a set of word pairs with a particular derivation pattern (e.g., \"-in\", Bäcker::Bäckerin), and divide this set into training and test pairs by performing 10-fold cross-validation.\n sent1: For the test pairs, we predict the vectors of the derived terms (e.g.,\n sent2: The search space includes all corpus words across parts-of-speech, except for the base term.\n sent3: The performance is measured in terms of recall-out-of-5 (#REF), counting how often the correct derived term is found among the five nearest neighbors of the predicted vector.\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "We created a new collection of German particle verb derivations 1 relying on the same resource as #TARGET_REF , the semiautomatic derivational lexicon for German DErivBase (#REF) .",
                "From DErivBase, we induced all pairs of base verbs and particle verbs across seven different particles.",
                "Nonexisting verbs were manually filtered out.",
                "In total, our collection contains 1 410 BV-PV combinations across seven particles, cf.",
                "Table 2 ."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: We created a new collection of German particle verb derivations 1 relying on the same resource as #TARGET_REF , the semiautomatic derivational lexicon for German DErivBase (#REF) .\n sent1: From DErivBase, we induced all pairs of base verbs and particle verbs across seven different particles.\n sent2: Nonexisting verbs were manually filtered out.\n sent3: In total, our collection contains 1 410 BV-PV combinations across seven particles, cf.\n sent4: Table 2 .\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "A baseline method that simply guesses the derived term has a chance of approx.",
                "1 460 000 for German and 1 240 000 for English to predict the correct term.",
                "We thus apply a more informed baseline, the same as in #TARGET_REF , and predict the derived term at exactly the same position as the base term."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: A baseline method that simply guesses the derived term has a chance of approx.\n sent1: 1 460 000 for German and 1 240 000 for English to predict the correct term.\n sent2: We thus apply a more informed baseline, the same as in #TARGET_REF , and predict the derived term at exactly the same position as the base term.\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Nonexisting verbs were manually filtered out.",
                "In total, our collection contains 1 410 BV-PV combinations across seven particles, cf.",
                "Table 2 .",
                "In addition, we apply our models to two existing collections for derivational patterns, the German dataset from #TARGET_REF , comprising six derivational patterns with 80 in-stances each (cf.",
                "Table 3 : English dataset (#REF) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Nonexisting verbs were manually filtered out.\n sent1: In total, our collection contains 1 410 BV-PV combinations across seven particles, cf.\n sent2: Table 2 .\n sent3: In addition, we apply our models to two existing collections for derivational patterns, the German dataset from #TARGET_REF , comprising six derivational patterns with 80 in-stances each (cf.\n sent4: Table 3 : English dataset (#REF) .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "BLEU (#REF ) is an n-grambased evaluation metric, widely used for Machine Translation (MT) evaluation.",
                "BLEU has also been applied to monolingual translation tasks, such as grammatical error correction (#REF) , summarization (#REF) and text simplification (#REF; #REF; #TARGET_REF , i.e. the rewriting of a sentence as one or more simpler sentences.",
                "Along with the application of parallel corpora and MT techniques for TS (e.g., #REF; #REF; #REF) , BLEU became the main automatic metric for TS, despite its deficiencies (see §2).",
                "Indeed, focusing on lexical simplification, #REF argued that BLEU gives high scores to sentences that are close or even identical to the input, especially when multiple references are used.",
                "In their experiments, BLEU failed to predict simplicity, but obtained a higher correlation with grammaticality and meaning preservation, relative to the SARI metric they proposed."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: BLEU (#REF ) is an n-grambased evaluation metric, widely used for Machine Translation (MT) evaluation.\n sent1: BLEU has also been applied to monolingual translation tasks, such as grammatical error correction (#REF) , summarization (#REF) and text simplification (#REF; #REF; #TARGET_REF , i.e. the rewriting of a sentence as one or more simpler sentences.\n sent2: Along with the application of parallel corpora and MT techniques for TS (e.g., #REF; #REF; #REF) , BLEU became the main automatic metric for TS, despite its deficiencies (see §2).\n sent3: Indeed, focusing on lexical simplification, #REF argued that BLEU gives high scores to sentences that are close or even identical to the input, especially when multiple references are used.\n sent4: In their experiments, BLEU failed to predict simplicity, but obtained a higher correlation with grammaticality and meaning preservation, relative to the SARI metric they proposed.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Along with the application of parallel corpora and MT techniques for TS (e.g., #REF; #REF; #REF) , BLEU became the main automatic metric for TS, despite its deficiencies (see §2).",
                "Indeed, focusing on lexical simplification, #TARGET_REF argued that BLEU gives high scores to sentences that are close or even identical to the input, especially when multiple references are used.",
                "In their experiments, BLEU failed to predict simplicity, but obtained a higher correlation with grammaticality and meaning preservation, relative to the SARI metric they proposed.",
                "In this paper, we further explore the applicability of BLEU for TS evaluation, examining BLEU's informativeness where sentence splitting is involved.",
                "Sentence splitting, namely the rewriting of a single sentence as multiple sentences while preserving its meaning, is the main structural simplification operation."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Along with the application of parallel corpora and MT techniques for TS (e.g., #REF; #REF; #REF) , BLEU became the main automatic metric for TS, despite its deficiencies (see §2).\n sent1: Indeed, focusing on lexical simplification, #TARGET_REF argued that BLEU gives high scores to sentences that are close or even identical to the input, especially when multiple references are used.\n sent2: In their experiments, BLEU failed to predict simplicity, but obtained a higher correlation with grammaticality and meaning preservation, relative to the SARI metric they proposed.\n sent3: In this paper, we further explore the applicability of BLEU for TS evaluation, examining BLEU's informativeness where sentence splitting is involved.\n sent4: Sentence splitting, namely the rewriting of a single sentence as multiple sentences while preserving its meaning, is the main structural simplification operation.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Metrics.",
                "In addition to BLEU, 7 we also experiment with (1) iBLEU (#REF) which was recently used for TS #TARGET_REF; and which takes into account the BLEU scores of the output against the input and against the references; (2) the Flesch-Kincaid Grade Level (FK; #REF ), computed at the system level, which estimates the readability of the text with a lower value indicating higher 4 Examples are taken from #REF .",
                "5 Examples are not provided in the case of Set 2 so as not to give an a-priori notion of simplicity.",
                "The complete guidelines are found in the supplementary material.",
                "6 Wilicoxon's signed rank test, p = 1.6 · 10 −5 for #Sents and p = 0.002 for SplitSents."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Metrics.\n sent1: In addition to BLEU, 7 we also experiment with (1) iBLEU (#REF) which was recently used for TS #TARGET_REF; and which takes into account the BLEU scores of the output against the input and against the references; (2) the Flesch-Kincaid Grade Level (FK; #REF ), computed at the system level, which estimates the readability of the text with a lower value indicating higher 4 Examples are taken from #REF .\n sent2: 5 Examples are not provided in the case of Set 2 so as not to give an a-priori notion of simplicity.\n sent3: The complete guidelines are found in the supplementary material.\n sent4: 6 Wilicoxon's signed rank test, p = 1.6 · 10 −5 for #Sents and p = 0.002 for SplitSents.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "For exploring the effect of sentence splitting on BLEU scores, we compile a human-generated gold standard sentence splitting corpus -HSplit, which will also be useful for future studies of splitting in TS, and perform correlation analyses with human judgments.",
                "We consider two reference sets.",
                "First, we experiment with the most common set, proposed by #TARGET_REF , evaluating a variety of system outputs, as well as HSplit.",
                "The references in this setting explicitly emphasize lexical operations, and do not contain splitting or content deletion.",
                "2 Second, we experiment with HSplit as the reference set, evaluating systems that focus on sentence splitting."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: For exploring the effect of sentence splitting on BLEU scores, we compile a human-generated gold standard sentence splitting corpus -HSplit, which will also be useful for future studies of splitting in TS, and perform correlation analyses with human judgments.\n sent1: We consider two reference sets.\n sent2: First, we experiment with the most common set, proposed by #TARGET_REF , evaluating a variety of system outputs, as well as HSplit.\n sent3: The references in this setting explicitly emphasize lexical operations, and do not contain splitting or content deletion.\n sent4: 2 Second, we experiment with HSplit as the reference set, evaluating systems that focus on sentence splitting.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "In order to investigate the effect of correctly splitting sentences on the automatic metric scores, we build a parallel corpus, where each sentence is modified by 4 annotators, according to specific sentence splitting guidelines.",
                "We use the complex side of the test corpus of #TARGET_REF .",
                "3 While #REF recently proposed the semi-automatically compiled WEB-SPLIT dataset for training automatic sentence splitting systems, here we generate a completely manual corpus, without a-priori splitting points nor do we pre-suppose that all sentences should be split.",
                "This corpus enriches the set of references focused on lexical operations that were collected by #REF for the same source sentences and can also be used as an out-of-domain test set for Split-and-Rephrase (#REF) .",
                "We use two sets of guidelines."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In order to investigate the effect of correctly splitting sentences on the automatic metric scores, we build a parallel corpus, where each sentence is modified by 4 annotators, according to specific sentence splitting guidelines.\n sent1: We use the complex side of the test corpus of #TARGET_REF .\n sent2: 3 While #REF recently proposed the semi-automatically compiled WEB-SPLIT dataset for training automatic sentence splitting systems, here we generate a completely manual corpus, without a-priori splitting points nor do we pre-suppose that all sentences should be split.\n sent3: This corpus enriches the set of references focused on lexical operations that were collected by #REF for the same source sentences and can also be used as an out-of-domain test set for Split-and-Rephrase (#REF) .\n sent4: We use two sets of guidelines.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "For \"Standard Reference Setting\", we consider both a case where evaluated systems do not perform any splittings on the test set (\"Systems/Corpora without Splits\"), and one where we evaluate these systems, along with the HSplit corpus, used in the role of system outputs (\"All Systems/Corpora\").",
                "Systems include six MT-based simplification systems, including outputs of the state-of-the-art neural TS system of #REF , in four variants: either default settings or initialization by word2vec, for each both the highest and the fourth ranked hypotheses in the beam are considered.",
                "10 We further include Moses (#REF) and SBMT-SARI #TARGET_REF , a syntax-based MT system tuned against SARI, and the identity function (outputs are same as inputs).",
                "The case which evaluates outputs with sentence splitting additionally includes the four HSplit corpora and the HSplit average scores.",
                "For \"HSplit as Reference Setting\", we consider the outputs of six simplification systems whose main simplification operation is sentence splitting: DSS, DSS m , SEMoses, SEMoses m , SEMoses LM and SEMoses m LM , taken from (#REFb) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: For \"Standard Reference Setting\", we consider both a case where evaluated systems do not perform any splittings on the test set (\"Systems/Corpora without Splits\"), and one where we evaluate these systems, along with the HSplit corpus, used in the role of system outputs (\"All Systems/Corpora\").\n sent1: Systems include six MT-based simplification systems, including outputs of the state-of-the-art neural TS system of #REF , in four variants: either default settings or initialization by word2vec, for each both the highest and the fourth ranked hypotheses in the beam are considered.\n sent2: 10 We further include Moses (#REF) and SBMT-SARI #TARGET_REF , a syntax-based MT system tuned against SARI, and the identity function (outputs are same as inputs).\n sent3: The case which evaluates outputs with sentence splitting additionally includes the four HSplit corpora and the HSplit average scores.\n sent4: For \"HSplit as Reference Setting\", we consider the outputs of six simplification systems whose main simplification operation is sentence splitting: DSS, DSS m , SEMoses, SEMoses m , SEMoses LM and SEMoses m LM , taken from (#REFb) .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "readability; 8 (3) SARI (#REF) , which compares the n-grams of the system output with those of the input and the human references, separately evaluating the quality of words that are added, deleted and kept by the systems.",
                "For completeness, we also experiment with the negative Levenshtein distance to the source (-LD SC ), which serves as a measure of conservatism.",
                "9 We explore two settings.",
                "In one (\"Standard Reference Setting\", §4.2), we use two sets of references: the Simple Wikipedia reference (yielding BLEU-1ref and iBLEU-1ref), and 8 references obtained by crowdsourcing by #TARGET_REF (yielding BLEU-8ref, iBLEU-8ref and SARI-8ref).",
                "In the other (\"HSplit as Reference Setting\", §4.3), we use HSplit as the reference set."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: readability; 8 (3) SARI (#REF) , which compares the n-grams of the system output with those of the input and the human references, separately evaluating the quality of words that are added, deleted and kept by the systems.\n sent1: For completeness, we also experiment with the negative Levenshtein distance to the source (-LD SC ), which serves as a measure of conservatism.\n sent2: 9 We explore two settings.\n sent3: In one (\"Standard Reference Setting\", §4.2), we use two sets of references: the Simple Wikipedia reference (yielding BLEU-1ref and iBLEU-1ref), and 8 references obtained by crowdsourcing by #TARGET_REF (yielding BLEU-8ref, iBLEU-8ref and SARI-8ref).\n sent4: In the other (\"HSplit as Reference Setting\", §4.3), we use HSplit as the reference set.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "For \"HSplit as Reference Setting\", we consider the outputs of six simplification systems whose main simplification operation is sentence splitting: DSS, DSS m , SEMoses, SEMoses m , SEMoses LM and SEMoses m LM , taken from (#REFb) .",
                "Human Evaluation.",
                "We use the evaluation benchmark provided by Sulem et al. (2018b) , 11 including system outputs and human evaluation scores corresponding to the first 70 sentences of the test corpus of #TARGET_REF , and extend it to apply to HSplit as well.",
                "The evaluation of HSplit is carried out by 3 in-house native English annotators, who rated the different input-output pairs for the different systems according to 4 parameters: Grammaticality (G), Meaning preservation (M), Simplicity (S) and Structural Simplicity (StS).",
                "G and M are measured using a 1 to 5 scale."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: For \"HSplit as Reference Setting\", we consider the outputs of six simplification systems whose main simplification operation is sentence splitting: DSS, DSS m , SEMoses, SEMoses m , SEMoses LM and SEMoses m LM , taken from (#REFb) .\n sent1: Human Evaluation.\n sent2: We use the evaluation benchmark provided by Sulem et al. (2018b) , 11 including system outputs and human evaluation scores corresponding to the first 70 sentences of the test corpus of #TARGET_REF , and extend it to apply to HSplit as well.\n sent3: The evaluation of HSplit is carried out by 3 in-house native English annotators, who rated the different input-output pairs for the different systems according to 4 parameters: Grammaticality (G), Meaning preservation (M), Simplicity (S) and Structural Simplicity (StS).\n sent4: G and M are measured using a 1 to 5 scale.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "While BLEU is standardly used for TS evaluation (e.g., #TARGET_REF; #REF; #REF; #REF ), only few works tested its correlation with human judgments.",
                "Using 20 source sentences from the PWKP test corpus (#REF) with 5 simplified sentences for each of them, #REF reported positive correlation of BLEU with simplicity ratings, but no correlation with adequacy.",
                "T-BLEU (Štajner et al., 2014), a variant of BLEU which uses lower n-grams when no overlapping 4-grams are found, was tested on outputs that applied only structural modifications to the source.",
                "It was found to have moderate positive correlation for meaning preservation, and positive but low correlation for grammaticality.",
                "Correlation with simplicity was not considered in this experiment."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: While BLEU is standardly used for TS evaluation (e.g., #TARGET_REF; #REF; #REF; #REF ), only few works tested its correlation with human judgments.\n sent1: Using 20 source sentences from the PWKP test corpus (#REF) with 5 simplified sentences for each of them, #REF reported positive correlation of BLEU with simplicity ratings, but no correlation with adequacy.\n sent2: T-BLEU (Štajner et al., 2014), a variant of BLEU which uses lower n-grams when no overlapping 4-grams are found, was tested on outputs that applied only structural modifications to the source.\n sent3: It was found to have moderate positive correlation for meaning preservation, and positive but low correlation for grammaticality.\n sent4: Correlation with simplicity was not considered in this experiment.\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "We use the complex side of the test corpus of #REF .",
                "3 While #REF recently proposed the semi-automatically compiled WEB-SPLIT dataset for training automatic sentence splitting systems, here we generate a completely manual corpus, without a-priori splitting points nor do we pre-suppose that all sentences should be split.",
                "This corpus enriches the set of references focused on lexical operations that were collected by #TARGET_REF for the same source sentences and can also be used as an out-of-domain test set for Split-and-Rephrase (#REF) .",
                "We use two sets of guidelines.",
                "In Set 1, annotators are required to split the original as much as possible, while preserving the sentence's gram-maticality, fluency and meaning."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: We use the complex side of the test corpus of #REF .\n sent1: 3 While #REF recently proposed the semi-automatically compiled WEB-SPLIT dataset for training automatic sentence splitting systems, here we generate a completely manual corpus, without a-priori splitting points nor do we pre-suppose that all sentences should be split.\n sent2: This corpus enriches the set of references focused on lexical operations that were collected by #TARGET_REF for the same source sentences and can also be used as an out-of-domain test set for Split-and-Rephrase (#REF) .\n sent3: We use two sets of guidelines.\n sent4: In Set 1, annotators are required to split the original as much as possible, while preserving the sentence's gram-maticality, fluency and meaning.\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "BLEU-8ref obtains 94.63 for the input and 73.03 for HSplit.",
                "12 The high scores obtained for Identity, also observed by #TARGET_REF , indicate that BLEU is a not a good predictor for relative simplicity to the input.",
                "The drop in the BLEU scores for HSplit is not reflected by the human evaluation scores for grammaticality (4.43 for AvgHSplit vs. 4.80 for Identity) and meaning preservation (4.70 vs. 5.00), where the decrease between Identity and HSplit is much more limited.",
                "For examining these tendencies in more detail, we compute the correlations between the au-tomatic metrics and the human evaluation scores.",
                "They are described in the following paragraph."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: BLEU-8ref obtains 94.63 for the input and 73.03 for HSplit.\n sent1: 12 The high scores obtained for Identity, also observed by #TARGET_REF , indicate that BLEU is a not a good predictor for relative simplicity to the input.\n sent2: The drop in the BLEU scores for HSplit is not reflected by the human evaluation scores for grammaticality (4.43 for AvgHSplit vs. 4.80 for Identity) and meaning preservation (4.70 vs. 5.00), where the decrease between Identity and HSplit is much more limited.\n sent3: For examining these tendencies in more detail, we compute the correlations between the au-tomatic metrics and the human evaluation scores.\n sent4: They are described in the following paragraph.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Besides, it constitutes the first stage of many NLP pipelines.",
                "Before applying tools trained on specific languages, one must determine the language of the text.",
                "It has attracted considerable attention in recent years [1, 2, #TARGET_REF 4, 5, 6, 7, 8] .",
                "Most of the existing approaches take words as features, and then adopt effective supervised classification algorithms to solve the problem.",
                "Generally speaking, language identification between different languages is a task that can be solved at a high accuracy."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Besides, it constitutes the first stage of many NLP pipelines.\n sent1: Before applying tools trained on specific languages, one must determine the language of the text.\n sent2: It has attracted considerable attention in recent years [1, 2, #TARGET_REF 4, 5, 6, 7, 8] .\n sent3: Most of the existing approaches take words as features, and then adopt effective supervised classification algorithms to solve the problem.\n sent4: Generally speaking, language identification between different languages is a task that can be solved at a high accuracy.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "For example, #REF achieved 97% accuracy for discriminating among 25 unrelated languages.",
                "However, it is generally difficult to distinguish between related languages or variations of a specific language (see [9] and [10] for example).",
                "To deal with this problem, Huang and Lee #TARGET_REF proposed a contrastive approach based on documentlevel top-bag-of-word similarity to reflect distances among the three varieties of Mandarin in China, Taiwan and Singapore, which is a kind of word-level uni-gram feature.",
                "The word unigram feature is sufficient for document-level identification of language variants.",
                "More recent studies focus on sentence-level languages identification, such as the Discriminating between Similar Languages (DSL) shared task 2014 and 2015 [7, 8] ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For example, #REF achieved 97% accuracy for discriminating among 25 unrelated languages.\n sent1: However, it is generally difficult to distinguish between related languages or variations of a specific language (see [9] and [10] for example).\n sent2: To deal with this problem, Huang and Lee #TARGET_REF proposed a contrastive approach based on documentlevel top-bag-of-word similarity to reflect distances among the three varieties of Mandarin in China, Taiwan and Singapore, which is a kind of word-level uni-gram feature.\n sent3: The word unigram feature is sufficient for document-level identification of language variants.\n sent4: More recent studies focus on sentence-level languages identification, such as the Discriminating between Similar Languages (DSL) shared task 2014 and 2015 [7, 8] .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "For example, #REF focused on Indian languages identification.",
                "Meanwhile, #REF proposed features based on frequencies of character n-grams to identify Malay and Indonesian.",
                "Huang and Lee #TARGET_REF presented the top-bag-of-word similarity based contrastive approach to reflect distances among the three varieties of Mandarin in Mainland China, Taiwan and Singapore.",
                "#REF found that word uni-grams gave very similar performance to character n-gram features in the framework of the probabilistic language model for the Brazilian and European Portuguese language discrimination.",
                "#REF ; #REF showed that the Naïve Bayes classifier with uni-grams achieved high accuracy for the South Slavic languages identification."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For example, #REF focused on Indian languages identification.\n sent1: Meanwhile, #REF proposed features based on frequencies of character n-grams to identify Malay and Indonesian.\n sent2: Huang and Lee #TARGET_REF presented the top-bag-of-word similarity based contrastive approach to reflect distances among the three varieties of Mandarin in Mainland China, Taiwan and Singapore.\n sent3: #REF found that word uni-grams gave very similar performance to character n-gram features in the framework of the probabilistic language model for the Brazilian and European Portuguese language discrimination.\n sent4: #REF ; #REF showed that the Naïve Bayes classifier with uni-grams achieved high accuracy for the South Slavic languages identification.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "In fact, the word alignment-based dictionary can extract both fine-grained representative words and coarse-grained words simultaneously.",
                "The above observation indicates that character form, PMI-based and word alignment-based information are useful information to discriminate dialects in the GCR.",
                "In order to investigate the detailed characteristics of different dialects of Mandarin Chinese, we extend #TARGET_REF dialects in Huang and Lee #TARGET_REF to 6 dialects.",
                "In fact, the more dialects there are, the more difficult the dialects discrimination becomes.",
                "It also has been verified through our experiments."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: In fact, the word alignment-based dictionary can extract both fine-grained representative words and coarse-grained words simultaneously.\n sent1: The above observation indicates that character form, PMI-based and word alignment-based information are useful information to discriminate dialects in the GCR.\n sent2: In order to investigate the detailed characteristics of different dialects of Mandarin Chinese, we extend #TARGET_REF dialects in Huang and Lee #TARGET_REF to 6 dialects.\n sent3: In fact, the more dialects there are, the more difficult the dialects discrimination becomes.\n sent4: It also has been verified through our experiments.\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Among the above related works, study [3] is the most related work to ours.",
                "The differences between study [3] and our work are two-fold:",
                "(1)They focus on document-level varieties of Mandarin in China, Taiwan and Singapore, while we deal with sentence-level varieties of Mandarin in China, Hong Kong, Taiwan, Macao, Malaysia and Singapore.",
                "In order to investigate the detailed characteristic of different dialects of Mandarin Chinese, we extend dialects in Huang and Lee #TARGET_REF to 6 dialects.",
                "Also, the more dialects there are, the more difficult the dialects discrimination becomes."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: Among the above related works, study [3] is the most related work to ours.\n sent1: The differences between study [3] and our work are two-fold:\n sent2: (1)They focus on document-level varieties of Mandarin in China, Taiwan and Singapore, while we deal with sentence-level varieties of Mandarin in China, Hong Kong, Taiwan, Macao, Malaysia and Singapore.\n sent3: In order to investigate the detailed characteristic of different dialects of Mandarin Chinese, we extend dialects in Huang and Lee #TARGET_REF to 6 dialects.\n sent4: Also, the more dialects there are, the more difficult the dialects discrimination becomes.\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF ; #REF showed that the Naïve Bayes classifier with uni-grams achieved high accuracy for the South Slavic languages identification.",
                "#REF ; #REF found that bag-of-words features outperformed the syntax or character sequencesbased features for the English varieties.",
                "Besides these works, other recent studies include: Spanish varieties identification [13] , Arabic varieties discrimination [14, 15, 16, 17] , and Persian and Dari identification [18] .",
                "Among the above related works, study #TARGET_REF is the most related work to ours.",
                "The differences between study [3] and our work are two-fold:"
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: #REF ; #REF showed that the Naïve Bayes classifier with uni-grams achieved high accuracy for the South Slavic languages identification.\n sent1: #REF ; #REF found that bag-of-words features outperformed the syntax or character sequencesbased features for the English varieties.\n sent2: Besides these works, other recent studies include: Spanish varieties identification [13] , Arabic varieties discrimination [14, 15, 16, 17] , and Persian and Dari identification [18] .\n sent3: Among the above related works, study #TARGET_REF is the most related work to ours.\n sent4: The differences between study [3] and our work are two-fold:\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "As shown in Table 3 , character form based features are very effective (94.36% for 2-way dialects classification).",
                "Similar to Huang and Lee #TARGET_REF 's work, in order to eliminate the trivial issue of character encoding (simplified and traditional character), we convert Taiwan and Hong Kong texts to the same simplified character set using Zhconvertor 6 utility to focus on actual linguistic and textual features.",
                "Table 6 shows the experimental results for the dialect identification in the GCR.",
                "As shown, again, the bi-gram features work better than both uni-gram and tri-gram features on Wikipedia dataset.",
                "Also, the word alignment-based features can contribute about 3.32% performance improvement."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: As shown in Table 3 , character form based features are very effective (94.36% for 2-way dialects classification).\n sent1: Similar to Huang and Lee #TARGET_REF 's work, in order to eliminate the trivial issue of character encoding (simplified and traditional character), we convert Taiwan and Hong Kong texts to the same simplified character set using Zhconvertor 6 utility to focus on actual linguistic and textual features.\n sent2: Table 6 shows the experimental results for the dialect identification in the GCR.\n sent3: As shown, again, the bi-gram features work better than both uni-gram and tri-gram features on Wikipedia dataset.\n sent4: Also, the word alignment-based features can contribute about 3.32% performance improvement.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF ; #REF found that bag-of-words features outperformed the syntax or character sequencesbased features for the English varieties.",
                "Besides these works, other recent studies include: Spanish varieties identification [13] , Arabic varieties discrimination [14, 15, 16, 17] , and Persian and Dari identification [18] .",
                "Among the above related works, study [3] is the most related work to ours.",
                "The differences between study #TARGET_REF and our work are two-fold:",
                "(1)They focus on document-level varieties of Mandarin in China, Taiwan and Singapore, while we deal with sentence-level varieties of Mandarin in China, Hong Kong, Taiwan, Macao, Malaysia and Singapore."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: #REF ; #REF found that bag-of-words features outperformed the syntax or character sequencesbased features for the English varieties.\n sent1: Besides these works, other recent studies include: Spanish varieties identification [13] , Arabic varieties discrimination [14, 15, 16, 17] , and Persian and Dari identification [18] .\n sent2: Among the above related works, study [3] is the most related work to ours.\n sent3: The differences between study #TARGET_REF and our work are two-fold:\n sent4: (1)They focus on document-level varieties of Mandarin in China, Taiwan and Singapore, while we deal with sentence-level varieties of Mandarin in China, Hong Kong, Taiwan, Macao, Malaysia and Singapore.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "According to the related works [4, 5, 6] , word uni-grams are effective features for discriminating general languages.",
                "Compared with English, no space exists between words in Chinese sentence.",
                "Therefore, we use character uni-grams, bi-grams and tri-grams as features.",
                "However, Huang and Lee #TARGET_REF did not use character-level n-grams."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: According to the related works [4, 5, 6] , word uni-grams are effective features for discriminating general languages.\n sent1: Compared with English, no space exists between words in Chinese sentence.\n sent2: Therefore, we use character uni-grams, bi-grams and tri-grams as features.\n sent3: However, Huang and Lee #TARGET_REF did not use character-level n-grams.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "If we use a single type of feature, we can see that the uni-gram feature (baseline system 2) is not the best one for Chinese dialect detection in the GCR, although it has been found effective for English detection in previous studies in the DSL shared task.",
                "Instead, bi-gram and word segmentation based features are better than uni-gram one.",
                "Both of the proposed bi-gram and word segmentation based features significantly outperforms the baseline systems with p<0.01 using paired t-test for significance.",
                "Also the bi-gram and word segmentation based features are better than the Huang and Lee #TARGET_REF 's method (baseline system 1) for 6-way, #TARGET_REF-way and 2-way dialect identification in the GCR.",
                "Obviously, the random method does not work for the GCR dialect identification."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: If we use a single type of feature, we can see that the uni-gram feature (baseline system 2) is not the best one for Chinese dialect detection in the GCR, although it has been found effective for English detection in previous studies in the DSL shared task.\n sent1: Instead, bi-gram and word segmentation based features are better than uni-gram one.\n sent2: Both of the proposed bi-gram and word segmentation based features significantly outperforms the baseline systems with p<0.01 using paired t-test for significance.\n sent3: Also the bi-gram and word segmentation based features are better than the Huang and Lee #TARGET_REF 's method (baseline system 1) for 6-way, #TARGET_REF-way and 2-way dialect identification in the GCR.\n sent4: Obviously, the random method does not work for the GCR dialect identification.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "(1) 6-way detection: The dialects of Mainland China, Hong Kong, Taiwan, Macao, Malaysia and Singapore are all considered; (2) 3-way detection: We detect dialects of Mainland China, Taiwan and Singapore as in #REF ; (3) 2-way detection: We try to distinguish between two groups of dialects, the ones used in Mainland China, Malaysia and Singapore using simplified characters, and the ones used in Hong Kong, Taiwan and Macao using traditional characters.",
                "For the Wikipedia dataset, we also generate two similar scenarios:",
                "(1) 3-way detection: We detect dialects of Mainland China, Hong Kong and Taiwan; (2) 2-way detection: We try to distinguish between two groups of dialects, the ones used in Mainland China using simplified characters, and the ones used in Hong Kong and Taiwan using traditional characters.",
                "Baseline system 1: As mentioned in Section 2, we take the Huang and Lee #TARGET_REF 's top-bag-of-word similarity-based approach as one of our baseline system.",
                "We re-implement their method in this paper using the similar 3-way news dataset."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: (1) 6-way detection: The dialects of Mainland China, Hong Kong, Taiwan, Macao, Malaysia and Singapore are all considered; (2) 3-way detection: We detect dialects of Mainland China, Taiwan and Singapore as in #REF ; (3) 2-way detection: We try to distinguish between two groups of dialects, the ones used in Mainland China, Malaysia and Singapore using simplified characters, and the ones used in Hong Kong, Taiwan and Macao using traditional characters.\n sent1: For the Wikipedia dataset, we also generate two similar scenarios:\n sent2: (1) 3-way detection: We detect dialects of Mainland China, Hong Kong and Taiwan; (2) 2-way detection: We try to distinguish between two groups of dialects, the ones used in Mainland China using simplified characters, and the ones used in Hong Kong and Taiwan using traditional characters.\n sent3: Baseline system 1: As mentioned in Section 2, we take the Huang and Lee #TARGET_REF 's top-bag-of-word similarity-based approach as one of our baseline system.\n sent4: We re-implement their method in this paper using the similar 3-way news dataset.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "There are several works for WSD that do not depend on a sense tagged corpus, and they can be classified into three approaches according to main resources used: raw corpus based approach [2] , dictionary based approach [3, 4] and hierarchical lexical database approach.",
                "The hierarchical lexical database approach can be reclassified into three groups according to usages of the database: gloss based method [5] , conceptual density based method [6, 7] and relative based method #TARGET_REF 9, 10] .",
                "Since our method is a kind of the relative based method, this section describes the related works of the relative based method.",
                "[8] introduced the relative based method using International Roget's Thesaurus as a hierarchical lexical database.",
                "His method is conducted as follows: 1) Get relatives of each sense of a target word from the Roget's Thesaurus."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: There are several works for WSD that do not depend on a sense tagged corpus, and they can be classified into three approaches according to main resources used: raw corpus based approach [2] , dictionary based approach [3, 4] and hierarchical lexical database approach.\n sent1: The hierarchical lexical database approach can be reclassified into three groups according to usages of the database: gloss based method [5] , conceptual density based method [6, 7] and relative based method #TARGET_REF 9, 10] .\n sent2: Since our method is a kind of the relative based method, this section describes the related works of the relative based method.\n sent3: [8] introduced the relative based method using International Roget's Thesaurus as a hierarchical lexical database.\n sent4: His method is conducted as follows: 1) Get relatives of each sense of a target word from the Roget's Thesaurus.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "For example, an ambiguous word rail is a relative of a meaning bird of a target word crane at WordNet, but the word rail means railway for the most part, not the meaning related to bird.",
                "Therefore, most of the example sentences of rail are not helpful for WSD of crane.",
                "His method has another problem in disambiguating senses of a large number of target words because it requires a great amount of time and storage space to collect example sentences of relatives of the target words.",
                "[9] followed the method of #TARGET_REF , but tried to resolve the ambiguous relative problem by using just unambiguous relatives.",
                "That is, the ambiguous relative rail is not utilized to build a training data of the word crane because the word rail is ambiguous."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For example, an ambiguous word rail is a relative of a meaning bird of a target word crane at WordNet, but the word rail means railway for the most part, not the meaning related to bird.\n sent1: Therefore, most of the example sentences of rail are not helpful for WSD of crane.\n sent2: His method has another problem in disambiguating senses of a large number of target words because it requires a great amount of time and storage space to collect example sentences of relatives of the target words.\n sent3: [9] followed the method of #TARGET_REF , but tried to resolve the ambiguous relative problem by using just unambiguous relatives.\n sent4: That is, the ambiguous relative rail is not utilized to build a training data of the word crane because the word rail is ambiguous.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "That is, the ambiguous relative rail is not utilized to build a training data of the word crane because the word rail is ambiguous.",
                "Another difference from #TARGET_REF is on a lexical database: they utilized WordNet as a lexical database for acquiring relatives of target words instead of International Roget's Thesaurus.",
                "Since WordNet is freely available for research, various kinds of WSD studies based on WordNet can be compared with the method of [9] .",
                "They evaluated their method on 14 ambiguous nouns and achieved a good performance comparable to the methods based on the sense tagged corpus.",
                "However, the evaluation was conducted on a small part of senses of the target words like [8] ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: That is, the ambiguous relative rail is not utilized to build a training data of the word crane because the word rail is ambiguous.\n sent1: Another difference from #TARGET_REF is on a lexical database: they utilized WordNet as a lexical database for acquiring relatives of target words instead of International Roget's Thesaurus.\n sent2: Since WordNet is freely available for research, various kinds of WSD studies based on WordNet can be compared with the method of [9] .\n sent3: They evaluated their method on 14 ambiguous nouns and achieved a good performance comparable to the methods based on the sense tagged corpus.\n sent4: However, the evaluation was conducted on a small part of senses of the target words like [8] .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "They evaluated their method on 14 ambiguous nouns and achieved a good performance comparable to the methods based on the sense tagged corpus.",
                "However, the evaluation was conducted on a small part of senses of the target words like #TARGET_REF .",
                "However, many senses in WordNet do not have unambiguous relatives through relationships such as synonyms, direct hypernyms, and direct hyponyms.",
                "2 A possible alternative is to use the unambiguous relatives in the long distance from a target word, but the way is still problematic because the longer the distance of two senses is, the weaker the relationship between them is.",
                "In other words, the unambiguous relatives in the long distance may provide irrelevant examples for WSD like ambiguous relatives."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: They evaluated their method on 14 ambiguous nouns and achieved a good performance comparable to the methods based on the sense tagged corpus.\n sent1: However, the evaluation was conducted on a small part of senses of the target words like #TARGET_REF .\n sent2: However, many senses in WordNet do not have unambiguous relatives through relationships such as synonyms, direct hypernyms, and direct hyponyms.\n sent3: 2 A possible alternative is to use the unambiguous relatives in the long distance from a target word, but the way is still problematic because the longer the distance of two senses is, the weaker the relationship between them is.\n sent4: In other words, the unambiguous relatives in the long distance may provide irrelevant examples for WSD like ambiguous relatives.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Hence, the method has difficulties in disambiguating senses of words that do not have unambiguous relatives near the target words in the WordNet.",
                "The problem becomes more serious when verbs, which most of the relatives are ambiguous, are disambiguated.",
                "Like #TARGET_REF , the method also has a difficulty in disambiguating senses of many words because the method collects the example sentences of relatives of many words.",
                "[10] reimplemented the method of [9] using a web, which may be a very large corpus, in order to collect example sentences.",
                "They built training datum of all noun words in WordNet whose size is larger than 7GB, but evaluated their method on a small number of nouns of lexical sample task of SENSEVAL-2 as [8] and [9] ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Hence, the method has difficulties in disambiguating senses of words that do not have unambiguous relatives near the target words in the WordNet.\n sent1: The problem becomes more serious when verbs, which most of the relatives are ambiguous, are disambiguated.\n sent2: Like #TARGET_REF , the method also has a difficulty in disambiguating senses of many words because the method collects the example sentences of relatives of many words.\n sent3: [10] reimplemented the method of [9] using a web, which may be a very large corpus, in order to collect example sentences.\n sent4: They built training datum of all noun words in WordNet whose size is larger than 7GB, but evaluated their method on a small number of nouns of lexical sample task of SENSEVAL-2 as [8] and [9] .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Hence, the method has difficulties in disambiguating senses of words that do not have unambiguous relatives near the target words in the WordNet.",
                "The problem becomes more serious when verbs, which most of the relatives are ambiguous, are disambiguated.",
                "#REF , the method also has a difficulty in disambiguating senses of many words because the method collects the example sentences of relatives of many words.",
                "[10] reimplemented the method of [9] using a web, which may be a very large corpus, in order to collect example sentences.",
                "They built training datum of all noun words in WordNet whose size is larger than 7GB, but evaluated their method on a small number of nouns of lexical sample task of SENSEVAL-2 as #TARGET_REF and [9] ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Hence, the method has difficulties in disambiguating senses of words that do not have unambiguous relatives near the target words in the WordNet.\n sent1: The problem becomes more serious when verbs, which most of the relatives are ambiguous, are disambiguated.\n sent2: #REF , the method also has a difficulty in disambiguating senses of many words because the method collects the example sentences of relatives of many words.\n sent3: [10] reimplemented the method of [9] using a web, which may be a very large corpus, in order to collect example sentences.\n sent4: They built training datum of all noun words in WordNet whose size is larger than 7GB, but evaluated their method on a small number of nouns of lexical sample task of SENSEVAL-2 as #TARGET_REF and [9] .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Comparison with Other Relative Based Methods.",
                "We tried to compare our proposed method with the previous relative based methods.",
                "However, both of #TARGET_REF and [9] did not evaluate their methods on a publicly available data.",
                "We implemented their methods and compared our method with them on the same evaluation data.",
                "When both of the methods are implemented, it is practically difficult to collect example sentences of all target words in the evaluation data."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Comparison with Other Relative Based Methods.\n sent1: We tried to compare our proposed method with the previous relative based methods.\n sent2: However, both of #TARGET_REF and [9] did not evaluate their methods on a publicly available data.\n sent3: We implemented their methods and compared our method with them on the same evaluation data.\n sent4: When both of the methods are implemented, it is practically difficult to collect example sentences of all target words in the evaluation data.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "where r l is a relative related to the sense s ij .",
                "f req(r l , w k ) and f req(r l ) are the co-occurrence frequency between r l and w k and the frequency of r l , respectively, and both frequencies can be obtained by looking up the matrix since the matrix contains the frequencies of words and word pairs.",
                "The main difference between #TARGET_REF and [9] is whether ambiguous relatives are utilized or not.",
                "Considering the difference, we implemented the method of [8] to include the ambiguous relatives into relatives, but the method of [9] to exclude the ambiguous relatives.",
                "Table 3 ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: where r l is a relative related to the sense s ij .\n sent1: f req(r l , w k ) and f req(r l ) are the co-occurrence frequency between r l and w k and the frequency of r l , respectively, and both frequencies can be obtained by looking up the matrix since the matrix contains the frequencies of words and word pairs.\n sent2: The main difference between #TARGET_REF and [9] is whether ambiguous relatives are utilized or not.\n sent3: Considering the difference, we implemented the method of [8] to include the ambiguous relatives into relatives, but the method of [9] to exclude the ambiguous relatives.\n sent4: Table 3 .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Chatbots aim to engage users in open-domain human-computer conversations and are currently receiving increasing attention.",
                "The existing work on building chatbots includes generation-based methods and retrieval-based methods.",
                "The first type of methods synthesize a response with a natural language generation model (#REF; #REF; .",
                "In this paper, we focus on the second type and study the problem of multi-turn response selection.",
                "This task aims to select the best-matched response from a set of candidates, given the context of a conversation which is composed of multiple utterances #TARGET_REF; #REF; #REF )."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Chatbots aim to engage users in open-domain human-computer conversations and are currently receiving increasing attention.\n sent1: The existing work on building chatbots includes generation-based methods and retrieval-based methods.\n sent2: The first type of methods synthesize a response with a natural language generation model (#REF; #REF; .\n sent3: In this paper, we focus on the second type and study the problem of multi-turn response selection.\n sent4: This task aims to select the best-matched response from a set of candidates, given the context of a conversation which is composed of multiple utterances #TARGET_REF; #REF; #REF ).\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\", \"sent1\", \"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "This approach enables the incorporation of rich context when mapping between consecutive dialogue turns (#REF; #REF; .",
                "Recently, some extended work has been made to incorporate external knowledge into generation with specific personas or emotions (#REF; #REFa) .",
                "Our work belongs to the retrieval-based methods, which learn a matching model for a pair of a conversational context and a response candidate.",
                "This approach has the advantage of providing informative and fluent responses because they select a proper response for the current conversation from a repository by means of response selection algorithms #TARGET_REF; #REF EA EA EA EA EA EA EA EA EA EA EA EA EA EA EA EA EA EA EA EB EB EB EB EB E0 E1 E2 E3 E4 E5 E18 E19 E6 E7 E8 E9 E10 E11 E17 E12 E13 E14 E15 E16 E20 E21 E22 E23 E24 E25 [ E0 E0 E0 E0 E0 E0 E0 E0 E1 E1 E1 E1 E1 E1 E0 E1 E1 E1 E1 E1 E0 E1 E1 E1 E1 E1 Speaker Embeddings + + + + + + + + + + + + + + + + + + + + + + + + + + Figure 1 : The input representation of SA-BERT.",
                "The final input embeddings are the sum of the token embeddings, the segmentation embeddings, the position embeddings and the speaker embeddings."
            ],
            "label": [
                "BACKGROUND",
                "SIMILARITY"
            ]
        },
        "input": "sent0: This approach enables the incorporation of rich context when mapping between consecutive dialogue turns (#REF; #REF; .\n sent1: Recently, some extended work has been made to incorporate external knowledge into generation with specific personas or emotions (#REF; #REFa) .\n sent2: Our work belongs to the retrieval-based methods, which learn a matching model for a pair of a conversational context and a response candidate.\n sent3: This approach has the advantage of providing informative and fluent responses because they select a proper response for the current conversation from a repository by means of response selection algorithms #TARGET_REF; #REF EA EA EA EA EA EA EA EA EA EA EA EA EA EA EA EA EA EA EA EB EB EB EB EB E0 E1 E2 E3 E4 E5 E18 E19 E6 E7 E8 E9 E10 E11 E17 E12 E13 E14 E15 E16 E20 E21 E22 E23 E24 E25 [ E0 E0 E0 E0 E0 E0 E0 E0 E1 E1 E1 E1 E1 E1 E0 E1 E1 E1 E1 E1 E0 E1 E1 E1 E1 E1 Speaker Embeddings + + + + + + + + + + + + + + + + + + + + + + + + + + Figure 1 : The input representation of SA-BERT.\n sent4: The final input embeddings are the sum of the token embeddings, the segmentation embeddings, the position embeddings and the speaker embeddings.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"SIMILARITY\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Parameters of this classifier need to be estimated during the fine-tuning process.",
                "Finally, the classifier returns a score to denote the matching degree of this context-response pair.",
                "We tested SA-BERT on five public multi-turn response selection datasets, Ubuntu Dialogue Corpus V1 #TARGET_REF , Ubuntu Dialogue Corpus V2 (#REF) , Douban Conversation Corpus (#REF) , E-commerce Dialogue Corpus (#REFb) and DSTC 8-Track 2-Subtask 2 Corpus (Seokhwan #REF).",
                "The first four datasets have been disentangled in advance and our proposed speaker-aware disentanglement strategy has been applied to only the last DSTC 8-Track 2-Subtask 2 Corpus.",
                "Ubuntu Dialogue Corpus V1, V2 and DSTC 8-Track 2-Subtask 2 Corpus contain multi-turn dialogues about Ubuntu system troubleshooting in English."
            ],
            "label": [
                "BACKGROUND",
                "USE"
            ]
        },
        "input": "sent0: Parameters of this classifier need to be estimated during the fine-tuning process.\n sent1: Finally, the classifier returns a score to denote the matching degree of this context-response pair.\n sent2: We tested SA-BERT on five public multi-turn response selection datasets, Ubuntu Dialogue Corpus V1 #TARGET_REF , Ubuntu Dialogue Corpus V2 (#REF) , Douban Conversation Corpus (#REF) , E-commerce Dialogue Corpus (#REFb) and DSTC 8-Track 2-Subtask 2 Corpus (Seokhwan #REF).\n sent3: The first four datasets have been disentangled in advance and our proposed speaker-aware disentanglement strategy has been applied to only the last DSTC 8-Track 2-Subtask 2 Corpus.\n sent4: Ubuntu Dialogue Corpus V1, V2 and DSTC 8-Track 2-Subtask 2 Corpus contain multi-turn dialogues about Ubuntu system troubleshooting in English.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We used the same evaluation metrics as those used in previous work #TARGET_REF; #REF; #REF; #REFb; Seokhwan #REF) .",
                "Each model was tasked with selecting the k best-matched responses from n available candidates for the given conversation context c, and we calculated the recall of the true positive replies among the k selected responses, denoted as R n @k, as the main evaluation metric.",
                "In addition to R n @k, we considered the mean average precision (MAP) (#REF), mean reciprocal rank (MRR) (#REF) and precision-at-one (P@1), especially for the Douban corpus, following the settings of previous work."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We used the same evaluation metrics as those used in previous work #TARGET_REF; #REF; #REF; #REFb; Seokhwan #REF) .\n sent1: Each model was tasked with selecting the k best-matched responses from n available candidates for the given conversation context c, and we calculated the recall of the true positive replies among the k selected responses, denoted as R n @k, as the main evaluation metric.\n sent2: In addition to R n @k, we considered the mean average precision (MAP) (#REF), mean reciprocal rank (MRR) (#REF) and precision-at-one (P@1), especially for the Douban corpus, following the settings of previous work.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Understanding the temporal information in natural language text is an important NLP task (#REF (#REF #REF; #REF; #REF #REF .",
                "A crucial component is temporal relation (TempRel; e.g., before or after) extraction (#REF; #REF; #REF; #REF; #TARGET_REF #REFa .",
                "The TempRels in a document or a sentence can be conveniently modeled as a graph, where the nodes are events, and the edges are labeled by TempRels.",
                "Given all the events in an instance, TempRel annotation is the process of manually labeling all the edges -a highly labor intensive task due to two reasons.",
                "One is that many edges require extensive reasoning over multiple sentences and labeling them is time-consuming."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Understanding the temporal information in natural language text is an important NLP task (#REF (#REF #REF; #REF; #REF #REF .\n sent1: A crucial component is temporal relation (TempRel; e.g., before or after) extraction (#REF; #REF; #REF; #REF; #TARGET_REF #REFa .\n sent2: The TempRels in a document or a sentence can be conveniently modeled as a graph, where the nodes are events, and the edges are labeled by TempRels.\n sent3: Given all the events in an instance, TempRel annotation is the process of manually labeling all the edges -a highly labor intensive task due to two reasons.\n sent4: One is that many edges require extensive reasoning over multiple sentences and labeling them is time-consuming.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "TimeBank (#REF ) is a classic TempRel dataset, where the annotators were given a whole article and allowed to label TempRels between any pairs of events.",
                "Annotators in this setup usually focus only on salient relations but overlook some others.",
                "It has been reported that many event pairs in TimeBank should have been annotated with a specific TempRel but the annotators failed to look at them (#REF; #TARGET_REF .",
                "Consequently, we categorize TimeBank as a partially annotated dataset (P).",
                "The same argument applies to other datasets that adopted this setup, such as AQUAINT (#REF) , CaTeRs (#REF) and RED (O'#REF) ."
            ],
            "label": [
                "BACKGROUND",
                "MOTIVATION"
            ]
        },
        "input": "sent0: TimeBank (#REF ) is a classic TempRel dataset, where the annotators were given a whole article and allowed to label TempRels between any pairs of events.\n sent1: Annotators in this setup usually focus only on salient relations but overlook some others.\n sent2: It has been reported that many event pairs in TimeBank should have been annotated with a specific TempRel but the annotators failed to look at them (#REF; #TARGET_REF .\n sent3: Consequently, we categorize TimeBank as a partially annotated dataset (P).\n sent4: The same argument applies to other datasets that adopted this setup, such as AQUAINT (#REF) , CaTeRs (#REF) and RED (O'#REF) .\n",
        "output": "{\"label\": [\"BACKGROUND\", \"MOTIVATION\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The first system on TBDense was proposed in .",
                "Two recent TempRel extraction systems (#REF; #TARGET_REF ) also reported their performances on TB-Dense (F) and on TempEval-3 (P) separately.",
                "However, there are no existing systems that jointly train on both.",
                "Given that the annotation guidelines of F and P are obviously different, it may not be optimal to simply treat P and F uniformly and train on their union.",
                "This situation necessitates further investigation as we do here."
            ],
            "label": [
                "BACKGROUND",
                "MOTIVATION"
            ]
        },
        "input": "sent0: The first system on TBDense was proposed in .\n sent1: Two recent TempRel extraction systems (#REF; #TARGET_REF ) also reported their performances on TB-Dense (F) and on TempEval-3 (P) separately.\n sent2: However, there are no existing systems that jointly train on both.\n sent3: Given that the annotation guidelines of F and P are obviously different, it may not be optimal to simply treat P and F uniformly and train on their union.\n sent4: This situation necessitates further investigation as we do here.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"MOTIVATION\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "A standard way to perform global inference is to formulate it as an Integer Linear Programming (ILP) problem (#REF ) and enforce transitivity rules as constraints.",
                "Let R be the TempRel label set 2 , I r (ij) ∈ {0, 1} be the indicator function of (i, j) = r, and f r (ij) ∈ [0, 1] be the corresponding soft-max score obtained via S F +P .",
                "Then the ILP objective is formulated aŝ",
                "where {r m 3 } is selected based on the general transitivity proposed in #TARGET_REF .",
                "With Eq. (1), different implementations of Line 6 in Algorithm 1 can be described concisely as follows: (i) Local inference is performed by ignoring \"transitivity constraints\"."
            ],
            "label": [
                "BACKGROUND",
                "USE"
            ]
        },
        "input": "sent0: A standard way to perform global inference is to formulate it as an Integer Linear Programming (ILP) problem (#REF ) and enforce transitivity rules as constraints.\n sent1: Let R be the TempRel label set 2 , I r (ij) ∈ {0, 1} be the indicator function of (i, j) = r, and f r (ij) ∈ [0, 1] be the corresponding soft-max score obtained via S F +P .\n sent2: Then the ILP objective is formulated aŝ\n sent3: where {r m 3 } is selected based on the general transitivity proposed in #TARGET_REF .\n sent4: With Eq. (1), different implementations of Line 6 in Algorithm 1 can be described concisely as follows: (i) Local inference is performed by ignoring \"transitivity constraints\".\n",
        "output": "{\"label\": [\"BACKGROUND\", \"USE\"], \"context\": [\"sent0\", \"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "While incorporating transitivity constraints in inference is widely used, #TARGET_REF proposed to incorporate these constraints in the learning phase as well.",
                "One of the algorithms proposed in #REF is based on #REF 's constraint-driven learning (CoDL), which is the same as our intermediate System 7 in Table 2 ; the fact that System 7 is better than System 1 can thus be considered as a reproduction of #REF .",
                "Despite the technical similarity, this work is motivated differently and is set to achieve a different goal: #REF tried to enforce the transitivity structure, while the current work attempts to use imperfect signals (e.g., partially annotated) taken from additional data, and learn in the incidental supervision framework.",
                "The P used in this work is TBAQ, where only 12% of the edges are annotated.",
                "In practice, every annotation comes at a cost, either time or the expenses paid to annotators, and as more edges are annotated, the marginal \"benefit\" of one edge is going down (an extreme case is that an edge is of no value if it can be inferred from existing edges)."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: While incorporating transitivity constraints in inference is widely used, #TARGET_REF proposed to incorporate these constraints in the learning phase as well.\n sent1: One of the algorithms proposed in #REF is based on #REF 's constraint-driven learning (CoDL), which is the same as our intermediate System 7 in Table 2 ; the fact that System 7 is better than System 1 can thus be considered as a reproduction of #REF .\n sent2: Despite the technical similarity, this work is motivated differently and is set to achieve a different goal: #REF tried to enforce the transitivity structure, while the current work attempts to use imperfect signals (e.g., partially annotated) taken from additional data, and learn in the incidental supervision framework.\n sent3: The P used in this work is TBAQ, where only 12% of the edges are annotated.\n sent4: In practice, every annotation comes at a cost, either time or the expenses paid to annotators, and as more edges are annotated, the marginal \"benefit\" of one edge is going down (an extreme case is that an edge is of no value if it can be inferred from existing edges).\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Then the ILP objective is formulated aŝ",
                "where {r m 3 } is selected based on the general transitivity proposed in (#REF) .",
                "With Eq. (1), different implementations of Line 6 in Algorithm 1 can be described concisely as follows: (i) Local inference is performed by ignoring \"transitivity constraints\".",
                "(ii) Global inference can be performed by adding annotated edges in P as additional constraints.",
                "Note that Algorithm 1 is only for the learning step of TempRel extraction; as for the inference step of this task, we consistently adopt the standard method by solving Eq. (1), as was done by (#REF; #REF; #REF; #REF; #TARGET_REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Then the ILP objective is formulated aŝ\n sent1: where {r m 3 } is selected based on the general transitivity proposed in (#REF) .\n sent2: With Eq. (1), different implementations of Line 6 in Algorithm 1 can be described concisely as follows: (i) Local inference is performed by ignoring \"transitivity constraints\".\n sent3: (ii) Global inference can be performed by adding annotated edges in P as additional constraints.\n sent4: Note that Algorithm 1 is only for the learning step of TempRel extraction; as for the inference step of this task, we consistently adopt the standard method by solving Eq. (1), as was done by (#REF; #REF; #REF; #REF; #TARGET_REF .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "While incorporating transitivity constraints in inference is widely used, #REF proposed to incorporate these constraints in the learning phase as well.",
                "One of the algorithms proposed in #TARGET_REF is based on #REF 's constraint-driven learning (CoDL), which is the same as our intermediate System 7 in Table 2 ; the fact that System 7 is better than System 1 can thus be considered as a reproduction of #TARGET_REF .",
                "Despite the technical similarity, this work is motivated differently and is set to achieve a different goal: #REF tried to enforce the transitivity structure, while the current work attempts to use imperfect signals (e.g., partially annotated) taken from additional data, and learn in the incidental supervision framework.",
                "The P used in this work is TBAQ, where only 12% of the edges are annotated.",
                "In practice, every annotation comes at a cost, either time or the expenses paid to annotators, and as more edges are annotated, the marginal \"benefit\" of one edge is going down (an extreme case is that an edge is of no value if it can be inferred from existing edges)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: While incorporating transitivity constraints in inference is widely used, #REF proposed to incorporate these constraints in the learning phase as well.\n sent1: One of the algorithms proposed in #TARGET_REF is based on #REF 's constraint-driven learning (CoDL), which is the same as our intermediate System 7 in Table 2 ; the fact that System 7 is better than System 1 can thus be considered as a reproduction of #TARGET_REF .\n sent2: Despite the technical similarity, this work is motivated differently and is set to achieve a different goal: #REF tried to enforce the transitivity structure, while the current work attempts to use imperfect signals (e.g., partially annotated) taken from additional data, and learn in the incidental supervision framework.\n sent3: The P used in this work is TBAQ, where only 12% of the edges are annotated.\n sent4: In practice, every annotation comes at a cost, either time or the expenses paid to annotators, and as more edges are annotated, the marginal \"benefit\" of one edge is going down (an extreme case is that an edge is of no value if it can be inferred from existing edges).\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Overall: all edges.",
                "Awareness: the temporal awareness metric used in the TempEval3 workshop, measuring how useful the predicted graphs are (#REF) .",
                "System 7 can also be considered as a reproduction of #TARGET_REF (see the discussion in Sec. 5 for details).",
                "Two bootstrapping algorithms (standard and constrained) are analyzed and the benefit of P, although with missing annotations, is shown on a benchmark dataset.",
                "This work may be a good starting point for further investigations of incidental supervision and data collection schemes of the TempRel extraction task."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Overall: all edges.\n sent1: Awareness: the temporal awareness metric used in the TempEval3 workshop, measuring how useful the predicted graphs are (#REF) .\n sent2: System 7 can also be considered as a reproduction of #TARGET_REF (see the discussion in Sec. 5 for details).\n sent3: Two bootstrapping algorithms (standard and constrained) are analyzed and the benefit of P, although with missing annotations, is shown on a benchmark dataset.\n sent4: This work may be a good starting point for further investigations of incidental supervision and data collection schemes of the TempRel extraction task.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "While incorporating transitivity constraints in inference is widely used, #REF proposed to incorporate these constraints in the learning phase as well.",
                "One of the algorithms proposed in #REF is based on #REF 's constraint-driven learning (CoDL), which is the same as our intermediate System 7 in Table 2 ; the fact that System 7 is better than System 1 can thus be considered as a reproduction of #REF .",
                "Despite the technical similarity, this work is motivated differently and is set to achieve a different goal: #TARGET_REF tried to enforce the transitivity structure, while the current work attempts to use imperfect signals (e.g., partially annotated) taken from additional data, and learn in the incidental supervision framework.",
                "The P used in this work is TBAQ, where only 12% of the edges are annotated.",
                "In practice, every annotation comes at a cost, either time or the expenses paid to annotators, and as more edges are annotated, the marginal \"benefit\" of one edge is going down (an extreme case is that an edge is of no value if it can be inferred from existing edges)."
            ],
            "label": [
                "DIFFERENCES",
                "SIMILARITY"
            ]
        },
        "input": "sent0: While incorporating transitivity constraints in inference is widely used, #REF proposed to incorporate these constraints in the learning phase as well.\n sent1: One of the algorithms proposed in #REF is based on #REF 's constraint-driven learning (CoDL), which is the same as our intermediate System 7 in Table 2 ; the fact that System 7 is better than System 1 can thus be considered as a reproduction of #REF .\n sent2: Despite the technical similarity, this work is motivated differently and is set to achieve a different goal: #TARGET_REF tried to enforce the transitivity structure, while the current work attempts to use imperfect signals (e.g., partially annotated) taken from additional data, and learn in the incidental supervision framework.\n sent3: The P used in this work is TBAQ, where only 12% of the edges are annotated.\n sent4: In practice, every annotation comes at a cost, either time or the expenses paid to annotators, and as more edges are annotated, the marginal \"benefit\" of one edge is going down (an extreme case is that an edge is of no value if it can be inferred from existing edges).\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"SIMILARITY\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The answer type taxonomy consists of 17 types, and the training data is TREC-8 and TREC-9 data.",
                "Testing data is TREC-10.",
                "In the experiment, questions are represented by 13 features, 9 of which are semantic features based on WordNet.",
                "#TARGET_REF use a Sparse Network of Winnows (SNoW) to classify questions with respect to their expected answer type.",
                "The taxonomy consists of 6 coarse and 50 fine semantic classes."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The answer type taxonomy consists of 17 types, and the training data is TREC-8 and TREC-9 data.\n sent1: Testing data is TREC-10.\n sent2: In the experiment, questions are represented by 13 features, 9 of which are semantic features based on WordNet.\n sent3: #TARGET_REF use a Sparse Network of Winnows (SNoW) to classify questions with respect to their expected answer type.\n sent4: The taxonomy consists of 6 coarse and 50 fine semantic classes.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The features used were words, part-ofspeech tags, chunks, named entities, head chunks (e.g. the first noun chunk in a sentence), and semantically related words (words that often occur with a specific question class).",
                "Apart from these primitive features, a set of operators were used to compose more complex features.",
                "#REF used the same taxonomy as #TARGET_REF , as well as the same training and testing data.",
                "In an initial experiment they compared different machine learning approaches with regards to the question classification problem: Nearest Neighbors (NN), Naïve Bayes (NB), Decision Trees (DT), SNoW, and Support Vector Machines.",
                "The feature extracted and used as input to the machine learning algorithms in the initial experiment was bag-of-words and bag-of-ngrams (all continuous word sequences in the question)."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The features used were words, part-ofspeech tags, chunks, named entities, head chunks (e.g. the first noun chunk in a sentence), and semantically related words (words that often occur with a specific question class).\n sent1: Apart from these primitive features, a set of operators were used to compose more complex features.\n sent2: #REF used the same taxonomy as #TARGET_REF , as well as the same training and testing data.\n sent3: In an initial experiment they compared different machine learning approaches with regards to the question classification problem: Nearest Neighbors (NN), Naïve Bayes (NB), Decision Trees (DT), SNoW, and Support Vector Machines.\n sent4: The feature extracted and used as input to the machine learning algorithms in the initial experiment was bag-of-words and bag-of-ngrams (all continuous word sequences in the question).\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The taxonomy used is the taxonomy proposed by #REF .",
                "This taxonomy has been chosen since it is the most frequently used one in earlier work in the field #TARGET_REF; #REF; #REF) .",
                "The corpora used is both the corpus constructed and tagged by #REF , as well as a newly tagged corpus extracted from the AnswerBus logs.",
                "AnswerBus is a question answering system that has been online and logged real users questions.",
                "The AnswerBus corpus consists of 25,000 questions."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The taxonomy used is the taxonomy proposed by #REF .\n sent1: This taxonomy has been chosen since it is the most frequently used one in earlier work in the field #TARGET_REF; #REF; #REF) .\n sent2: The corpora used is both the corpus constructed and tagged by #REF , as well as a newly tagged corpus extracted from the AnswerBus logs.\n sent3: AnswerBus is a question answering system that has been online and logged real users questions.\n sent4: The AnswerBus corpus consists of 25,000 questions.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The results in this paper indicate that some of the results found in previous work #TARGET_REF; #REF; #REF) on question classification might be incorrect due to an unbiased training and test corpus.",
                "This bias stems from the fact that the training corpus is derived exclusively from TREC-10 data, while the training data stems from other sources.",
                "Since the TREC conferences have an explicit agenda that shifts from year to year this is perhaps no surprise.",
                "In relation to this, TREC material is maybe not the best source of information if one is interested in how different machine learners might perform on actual user data."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The results in this paper indicate that some of the results found in previous work #TARGET_REF; #REF; #REF) on question classification might be incorrect due to an unbiased training and test corpus.\n sent1: This bias stems from the fact that the training corpus is derived exclusively from TREC-10 data, while the training data stems from other sources.\n sent2: Since the TREC conferences have an explicit agenda that shifts from year to year this is perhaps no surprise.\n sent3: In relation to this, TREC material is maybe not the best source of information if one is interested in how different machine learners might perform on actual user data.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "In essence each class is assigned a codeword of 1's and -1's of length m, where m equals or is greater than the number of classes.",
                "This splits the multi-class data into m binary class data.",
                "Therefore, m SVM classifiers can be designed and their output combined.",
                "The SVM:s also used linear kernels.",
                "The same taxonomy, training and testing data was used as in #TARGET_REF"
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: In essence each class is assigned a codeword of 1's and -1's of length m, where m equals or is greater than the number of classes.\n sent1: This splits the multi-class data into m binary class data.\n sent2: Therefore, m SVM classifiers can be designed and their output combined.\n sent3: The SVM:s also used linear kernels.\n sent4: The same taxonomy, training and testing data was used as in #TARGET_REF\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Thses were originally developed for the text categorization task, but as question classification bears many resemblances and can be seen as a special case of text categorization.",
                "The taxonomy used is the taxonomy proposed by #REF .",
                "This taxonomy has been chosen since it is the most frequently used one in earlier work in the field (#REF; #REF; #REF) .",
                "The corpora used is both the corpus constructed and tagged by #TARGET_REF , as well as a newly tagged corpus extracted from the AnswerBus logs.",
                "AnswerBus is a question answering system that has been online and logged real users questions."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: Thses were originally developed for the text categorization task, but as question classification bears many resemblances and can be seen as a special case of text categorization.\n sent1: The taxonomy used is the taxonomy proposed by #REF .\n sent2: This taxonomy has been chosen since it is the most frequently used one in earlier work in the field (#REF; #REF; #REF) .\n sent3: The corpora used is both the corpus constructed and tagged by #TARGET_REF , as well as a newly tagged corpus extracted from the AnswerBus logs.\n sent4: AnswerBus is a question answering system that has been online and logged real users questions.\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "For present purposes the micro and macro sign tests established by #REF have been used.",
                "Thses were originally developed for the text categorization task, but as question classification bears many resemblances and can be seen as a special case of text categorization.",
                "The taxonomy used is the taxonomy proposed by #TARGET_REF .",
                "This taxonomy has been chosen since it is the most frequently used one in earlier work in the field (#REF; #REF; #REF) .",
                "The corpora used is both the corpus constructed and tagged by #REF , as well as a newly tagged corpus extracted from the AnswerBus logs."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: For present purposes the micro and macro sign tests established by #REF have been used.\n sent1: Thses were originally developed for the text categorization task, but as question classification bears many resemblances and can be seen as a special case of text categorization.\n sent2: The taxonomy used is the taxonomy proposed by #TARGET_REF .\n sent3: This taxonomy has been chosen since it is the most frequently used one in earlier work in the field (#REF; #REF; #REF) .\n sent4: The corpora used is both the corpus constructed and tagged by #REF , as well as a newly tagged corpus extracted from the AnswerBus logs.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The first experiment is intended to be a straightforward re-examination of previous work to establish what differences in performance there really are between machine learners.",
                "This experiment has been done under two different settings.",
                "First, we have used the corpus originally developed by #TARGET_REF , but since the test corpus used consists of questions solely from TREC-10 and the TREC conferences have a specific agenda the test corpus might be slightly different from the training data.",
                "Therefore, a second setting was used where the questions from the training and test corpora were pooled together and a randomized test corpus was extracted.",
                "This will be refered to as the repartitioned corpus."
            ],
            "label": [
                "DIFFERENCES",
                "EXTENDS"
            ]
        },
        "input": "sent0: The first experiment is intended to be a straightforward re-examination of previous work to establish what differences in performance there really are between machine learners.\n sent1: This experiment has been done under two different settings.\n sent2: First, we have used the corpus originally developed by #TARGET_REF , but since the test corpus used consists of questions solely from TREC-10 and the TREC conferences have a specific agenda the test corpus might be slightly different from the training data.\n sent3: Therefore, a second setting was used where the questions from the training and test corpora were pooled together and a randomized test corpus was extracted.\n sent4: This will be refered to as the repartitioned corpus.\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"EXTENDS\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF introduced the first deep end-to-end model for SRL using a stacked Bi-LSTM network with a conditional random field (CRF) as the top layer.",
                "#REF simplified their architecture using a highway Bi-LSTM network.",
                "More recently, #TARGET_REF replaced the common recurrent architecture with a self-attention network, directly capturing relationships between tokens regardless of their distance, resulting in better results and faster training.",
                "The work in deep end-to-end SRL has focused heavily on applying deep learning advances without considering the multilingual aspect.",
                "However, language-specific characteristics and the available amount of training data highly influence the optimal model structure."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #REF introduced the first deep end-to-end model for SRL using a stacked Bi-LSTM network with a conditional random field (CRF) as the top layer.\n sent1: #REF simplified their architecture using a highway Bi-LSTM network.\n sent2: More recently, #TARGET_REF replaced the common recurrent architecture with a self-attention network, directly capturing relationships between tokens regardless of their distance, resulting in better results and faster training.\n sent3: The work in deep end-to-end SRL has focused heavily on applying deep learning advances without considering the multilingual aspect.\n sent4: However, language-specific characteristics and the available amount of training data highly influence the optimal model structure.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "However, language-specific characteristics and the available amount of training data highly influence the optimal model structure.",
                "DAMESRL facilitates exploration and fair evaluation of new SRL models for different languages by providing flexible neural model construction on different modeling levels, the handling of various input and output formats, and clear output visualization.",
                "Beyond the existing state-of-the-art models (#REF; #REF; #TARGET_REF ), we exploit character-level modeling, beneficial when considering multiple languages.",
                "To demonstrate the merits of easy cross-lingual exploration and evaluation of model structures for SRL provided by DAMESRL, we report performance of several distinct models integrated into our framework for English, German and Arabic, as they have very different linguistic characteristics.",
                "by w p ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: However, language-specific characteristics and the available amount of training data highly influence the optimal model structure.\n sent1: DAMESRL facilitates exploration and fair evaluation of new SRL models for different languages by providing flexible neural model construction on different modeling levels, the handling of various input and output formats, and clear output visualization.\n sent2: Beyond the existing state-of-the-art models (#REF; #REF; #TARGET_REF ), we exploit character-level modeling, beneficial when considering multiple languages.\n sent3: To demonstrate the merits of easy cross-lingual exploration and evaluation of model structures for SRL provided by DAMESRL, we report performance of several distinct models integrated into our framework for English, German and Arabic, as they have very different linguistic characteristics.\n sent4: by w p .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "DAMESRL provides a Bi-LSTM network to learn character-level word representations helping for languages where important SRL cues are given through inflections, such as case markings in German and Arabic.",
                "Despite the foreseen importance, character-level embeddings have not been used in previous work (#REF; #REF; #TARGET_REF) .",
                "Phase II: As core sequence representation component, users can choose between a self-attention encoding (#REF) , a regular Bi-LSTM (#REF) or a highway Bi-LSTM (#REF; #REF) .",
                "Phase III: To compute model probabilities, users can choose a regular softmax, or a linear chain CRF as proposed by (#REF) , which can be useful for languages where word order is an important SRL cue, such as English, or when less training data is available (shown in Section 4).",
                "Phase IV: The inference phase provides two options for label inference from the computed model probabilities including greedy prediction and Viterbi decoding."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: DAMESRL provides a Bi-LSTM network to learn character-level word representations helping for languages where important SRL cues are given through inflections, such as case markings in German and Arabic.\n sent1: Despite the foreseen importance, character-level embeddings have not been used in previous work (#REF; #REF; #TARGET_REF) .\n sent2: Phase II: As core sequence representation component, users can choose between a self-attention encoding (#REF) , a regular Bi-LSTM (#REF) or a highway Bi-LSTM (#REF; #REF) .\n sent3: Phase III: To compute model probabilities, users can choose a regular softmax, or a linear chain CRF as proposed by (#REF) , which can be useful for languages where word order is an important SRL cue, such as English, or when less training data is available (shown in Section 4).\n sent4: Phase IV: The inference phase provides two options for label inference from the computed model probabilities including greedy prediction and Viterbi decoding.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "DAMESRL provides a Bi-LSTM network to learn character-level word representations helping for languages where important SRL cues are given through inflections, such as case markings in German and Arabic.",
                "Despite the foreseen importance, character-level embeddings have not been used in previous work (#REF; #REF; #REF) .",
                "Phase II: As core sequence representation component, users can choose between a self-attention encoding ( #TARGET_REF) , a regular Bi-LSTM (#REF) or a highway Bi-LSTM (#REF; #REF) .",
                "Phase III: To compute model probabilities, users can choose a regular softmax, or a linear chain CRF as proposed by (#REF) , which can be useful for languages where word order is an important SRL cue, such as English, or when less training data is available (shown in Section 4).",
                "Phase IV: The inference phase provides two options for label inference from the computed model probabilities including greedy prediction and Viterbi decoding."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: DAMESRL provides a Bi-LSTM network to learn character-level word representations helping for languages where important SRL cues are given through inflections, such as case markings in German and Arabic.\n sent1: Despite the foreseen importance, character-level embeddings have not been used in previous work (#REF; #REF; #REF) .\n sent2: Phase II: As core sequence representation component, users can choose between a self-attention encoding ( #TARGET_REF) , a regular Bi-LSTM (#REF) or a highway Bi-LSTM (#REF; #REF) .\n sent3: Phase III: To compute model probabilities, users can choose a regular softmax, or a linear chain CRF as proposed by (#REF) , which can be useful for languages where word order is an important SRL cue, such as English, or when less training data is available (shown in Section 4).\n sent4: Phase IV: The inference phase provides two options for label inference from the computed model probabilities including greedy prediction and Viterbi decoding.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF break down fake news into three categories, hoax, propaganda and satire.",
                "A hoax article typically tries to convince the reader about a cookedup story while propaganda ones usually mislead the reader into believing a false political or social agenda.",
                "#REF defines a satirical article as the one which deliberately exposes real-world individuals, organisations and events to ridicule.",
                "Previous works #TARGET_REF; #REF) rely on various linguistic and handcrafted semantic features for differentiating between news articles.",
                "However, none of them try to model the interaction of sentences within the document."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #REF break down fake news into three categories, hoax, propaganda and satire.\n sent1: A hoax article typically tries to convince the reader about a cookedup story while propaganda ones usually mislead the reader into believing a false political or social agenda.\n sent2: #REF defines a satirical article as the one which deliberately exposes real-world individuals, organisations and events to ridicule.\n sent3: Previous works #TARGET_REF; #REF) rely on various linguistic and handcrafted semantic features for differentiating between news articles.\n sent4: However, none of them try to model the interaction of sentences within the document.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Satire, according to #REF , is complicated because it occupies more than one place in the framework for humor, proposed by #REF : it clearly has an aggressive and social function, and often expresses an intellectual aspect as well.",
                "#TARGET_REF defines news satire as a genre of satire that mimics the format and style of journalistic reporting.",
                "Datasets created for the task of identifying satirical news articles from the trusted ones are often constructed by collecting documents from different online sources (#REF) .",
                "#REF hypothesized that this encourages the models to learn characteristics for different publication sources rather than characteristics of satire.",
                "In this work, we show that our proposed model generalizes to articles from unseen publication sources."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Satire, according to #REF , is complicated because it occupies more than one place in the framework for humor, proposed by #REF : it clearly has an aggressive and social function, and often expresses an intellectual aspect as well.\n sent1: #TARGET_REF defines news satire as a genre of satire that mimics the format and style of journalistic reporting.\n sent2: Datasets created for the task of identifying satirical news articles from the trusted ones are often constructed by collecting documents from different online sources (#REF) .\n sent3: #REF hypothesized that this encourages the models to learn characteristics for different publication sources rather than characteristics of satire.\n sent4: In this work, we show that our proposed model generalizes to articles from unseen publication sources.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Satire, according to #REF , is complicated because it occupies more than one place in the framework for humor, proposed by #REF : it clearly has an aggressive and social function, and often expresses an intellectual aspect as well.",
                "#REF defines news satire as a genre of satire that mimics the format and style of journalistic reporting.",
                "Datasets created for the task of identifying satirical news articles from the trusted ones are often constructed by collecting documents from different online sources #TARGET_REF .",
                "#REF hypothesized that this encourages the models to learn characteristics for different publication sources rather than characteristics of satire.",
                "In this work, we show that our proposed model generalizes to articles from unseen publication sources."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Satire, according to #REF , is complicated because it occupies more than one place in the framework for humor, proposed by #REF : it clearly has an aggressive and social function, and often expresses an intellectual aspect as well.\n sent1: #REF defines news satire as a genre of satire that mimics the format and style of journalistic reporting.\n sent2: Datasets created for the task of identifying satirical news articles from the trusted ones are often constructed by collecting documents from different online sources #TARGET_REF .\n sent3: #REF hypothesized that this encourages the models to learn characteristics for different publication sources rather than characteristics of satire.\n sent4: In this work, we show that our proposed model generalizes to articles from unseen publication sources.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Specifically, satirical articles had a more coherent story and thus all the sentences in the document seemed similar to each other.",
                "On the other hand, the trusted news articles were also coherent but the similarity between sentences from different parts of the document was not that strong, as depicted in Figure 1 .",
                "We believe that the reason for such kind of behaviour is the presence of factual jumps across sections in a trusted document.",
                "In this work, we propose a graph neural network-based model to classify news articles while capturing the interaction of sentences across the document.",
                "We present a series of experiments on News Corpus with Varying Reliability dataset (#REF) and Satirical Legitimate News dataset #TARGET_REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Specifically, satirical articles had a more coherent story and thus all the sentences in the document seemed similar to each other.\n sent1: On the other hand, the trusted news articles were also coherent but the similarity between sentences from different parts of the document was not that strong, as depicted in Figure 1 .\n sent2: We believe that the reason for such kind of behaviour is the presence of factual jumps across sections in a trusted document.\n sent3: In this work, we propose a graph neural network-based model to classify news articles while capturing the interaction of sentences across the document.\n sent4: We present a series of experiments on News Corpus with Varying Reliability dataset (#REF) and Satirical Legitimate News dataset #TARGET_REF .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "We use SLN: Satirical and Legitimate News Database #TARGET_REF , RPN: Random Political News Dataset (#REF) and LUN: Labeled Unreliable News Dataset #REF for our experiments.",
                "Table 1 shows the statistics.",
                "Since all of the previous methods on the aforementioned datasets are non-neural, we implement the following neural baselines,",
                "• CNN: In this model, we apply a 1-d CNN (Convolutional Neural Network) layer (#REF) with filter size 3 over the word embeddings of the sentences within a document.",
                "This is followed by a max-pooling layer to get a single document vector which is passed to a fully connected projection layer to get the logits over output classes."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We use SLN: Satirical and Legitimate News Database #TARGET_REF , RPN: Random Political News Dataset (#REF) and LUN: Labeled Unreliable News Dataset #REF for our experiments.\n sent1: Table 1 shows the statistics.\n sent2: Since all of the previous methods on the aforementioned datasets are non-neural, we implement the following neural baselines,\n sent3: • CNN: In this model, we apply a 1-d CNN (Convolutional Neural Network) layer (#REF) with filter size 3 over the word embeddings of the sentences within a document.\n sent4: This is followed by a max-pooling layer to get a single document vector which is passed to a fully connected projection layer to get the logits over output classes.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "In this work, we show that our proposed model generalizes to articles from unseen publication sources.",
                "#TARGET_REF 's work by offering a quantitative study of linguistic differences found in articles of different types of fake news such as hoax, propaganda and satire.",
                "They also proposed predictive models for graded deception across multiple domains.",
                "#REF found that neural methods didn't perform well for this task and proposed to use a Max-Entropy classifier.",
                "We show that our proposed neural network based on graph convolutional layers can outperform this model."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: In this work, we show that our proposed model generalizes to articles from unseen publication sources.\n sent1: #TARGET_REF 's work by offering a quantitative study of linguistic differences found in articles of different types of fake news such as hoax, propaganda and satire.\n sent2: They also proposed predictive models for graded deception across multiple domains.\n sent3: #REF found that neural methods didn't perform well for this task and proposed to use a Max-Entropy classifier.\n sent4: We show that our proposed neural network based on graph convolutional layers can outperform this model.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent1\", \"sent2\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "We only report F1-score following the SoTA paper.",
                "similarity model does not seem to have much impact on the GCN model, and considering the computing cost, we don't experiment with it for the 4-way classification scenario.",
                "Given that we use SLN as an out of domain test set (just one overlapping source, no overlap in articles), whereas the SoTA paper #TARGET_REF reports a 10fold cross validation number on SLN.",
                "We believe that our results are quite strong, the GAT + 2 Attn Heads model achieves an accuracy of 87% on the entire RPN dataset when used as an out-of-domain test set.",
                "The SoTA paper (#REF) on RPN reports a 5-fold cross validation accuracy of 91%."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: We only report F1-score following the SoTA paper.\n sent1: similarity model does not seem to have much impact on the GCN model, and considering the computing cost, we don't experiment with it for the 4-way classification scenario.\n sent2: Given that we use SLN as an out of domain test set (just one overlapping source, no overlap in articles), whereas the SoTA paper #TARGET_REF reports a 10fold cross validation number on SLN.\n sent3: We believe that our results are quite strong, the GAT + 2 Attn Heads model achieves an accuracy of 87% on the entire RPN dataset when used as an out-of-domain test set.\n sent4: The SoTA paper (#REF) on RPN reports a 5-fold cross validation accuracy of 91%.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Figure 2 depicts these results.",
                "The correlation between our scores and changes in workforce participation rates are similar to the correlation between the scores from (#REF) and the same (r = 0.8, p = 0.01 and r = 0.81, p < 0.01, respectively, for gender occupation bias; r = 0.84, p < 0.01 and r = 0.79, p = 0.01, respectively, for Asian/White occupation bias).",
                "Qualitative inspection of Figure 2 suggests that our model also produces smoother decade-by-decade scores, suggesting that it not only identifies attribute- #TARGET_REF and our model (blue dotted and green dashed lines, respectively) compared to actual workforce participation rates (solid lines) for gender (top) and Asian/White (bottom) linguistic biases.",
                "To compare all values on a single yaxis, we standardize both sets of bias scores and workforce participation rates by subtracting the mean and dividing by the standard deviation across decades.",
                "specific fluctuations in word semantics, but also, may provide a more general, regularized model for learning attribute-conditioned word embeddings."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Figure 2 depicts these results.\n sent1: The correlation between our scores and changes in workforce participation rates are similar to the correlation between the scores from (#REF) and the same (r = 0.8, p = 0.01 and r = 0.81, p < 0.01, respectively, for gender occupation bias; r = 0.84, p < 0.01 and r = 0.79, p = 0.01, respectively, for Asian/White occupation bias).\n sent2: Qualitative inspection of Figure 2 suggests that our model also produces smoother decade-by-decade scores, suggesting that it not only identifies attribute- #TARGET_REF and our model (blue dotted and green dashed lines, respectively) compared to actual workforce participation rates (solid lines) for gender (top) and Asian/White (bottom) linguistic biases.\n sent3: To compare all values on a single yaxis, we standardize both sets of bias scores and workforce participation rates by subtracting the mean and dividing by the standard deviation across decades.\n sent4: specific fluctuations in word semantics, but also, may provide a more general, regularized model for learning attribute-conditioned word embeddings.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "As a comparison, we also compute bias scores by training one Word2Vec model per day and projecting all day-by-day models into the same vector space using orthogonal Procrustes alignment 6 similar to (#REF) .",
                "The resulting scores from this non-dynamic model are depicted in 3(a).",
                "From qualitative inspection, the day-byday scores produced by the non-dynamic model appear much less smooth, and hence, fail to show the relative shift in discourse that likely occurred in response to a major refugee-related news event.",
                "One possible reason for this is that the median number of words for each day in the talk radio corpus is 4 million-over 5x fewer than a median of 22 million words per decade used to train each decade-specific model in #TARGET_REF .",
                "These results suggest that using our dynamic embedding approach is particularly valuable when data is sparse for any given attribute."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: As a comparison, we also compute bias scores by training one Word2Vec model per day and projecting all day-by-day models into the same vector space using orthogonal Procrustes alignment 6 similar to (#REF) .\n sent1: The resulting scores from this non-dynamic model are depicted in 3(a).\n sent2: From qualitative inspection, the day-byday scores produced by the non-dynamic model appear much less smooth, and hence, fail to show the relative shift in discourse that likely occurred in response to a major refugee-related news event.\n sent3: One possible reason for this is that the median number of words for each day in the talk radio corpus is 4 million-over 5x fewer than a median of 22 million words per decade used to train each decade-specific model in #TARGET_REF .\n sent4: These results suggest that using our dynamic embedding approach is particularly valuable when data is sparse for any given attribute.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Traditional methods rely on linguistic or semantic features (#REF; #REF) , or kernels based on syntax or sequences (#REFa,b; #REF) to represent sentences of relations.",
                "More recently, deep neural nets start to show promising results.",
                "Most rely on convolutional neural nets (#REF (#REF #REF, 2016; #TARGET_REF or recurrent neural nets (#REF; #REF; #REF) to learn the representation of relations.",
                "Our supervised base model will be similar to (#REF) .",
                "Our initial experiments did not use syntactic features (#REF; #REF ) that require additional parsers."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Traditional methods rely on linguistic or semantic features (#REF; #REF) , or kernels based on syntax or sequences (#REFa,b; #REF) to represent sentences of relations.\n sent1: More recently, deep neural nets start to show promising results.\n sent2: Most rely on convolutional neural nets (#REF (#REF #REF, 2016; #TARGET_REF or recurrent neural nets (#REF; #REF; #REF) to learn the representation of relations.\n sent3: Our supervised base model will be similar to (#REF) .\n sent4: Our initial experiments did not use syntactic features (#REF; #REF ) that require additional parsers.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The supervised neural model on a single dataset was introduced by #REF and followed by many others (#REF; #REF; #REF; #REF; #TARGET_REF .",
                "We use a similar model as our base model.",
                "It takes word tokens, position of arguments and their entity types as input.",
                "Some work (#REF; #REF) used extra syntax features as input.",
                "However, the parsers that produce syntax features could have errors and vary depending on the domain of text."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The supervised neural model on a single dataset was introduced by #REF and followed by many others (#REF; #REF; #REF; #REF; #TARGET_REF .\n sent1: We use a similar model as our base model.\n sent2: It takes word tokens, position of arguments and their entity types as input.\n sent3: Some work (#REF; #REF) used extra syntax features as input.\n sent4: However, the parsers that produce syntax features could have errors and vary depending on the domain of text.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "The supervised neural model on a single dataset was introduced by #REF and followed by many others (#REF; #REF; #REF; #REF; #REF) .",
                "We use a similar model as our base model.",
                "It takes word tokens, position of arguments and their entity types as input.",
                "Some work (#REF; #TARGET_REF used extra syntax features as input.",
                "However, the parsers that produce syntax features could have errors and vary depending on the domain of text."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The supervised neural model on a single dataset was introduced by #REF and followed by many others (#REF; #REF; #REF; #REF; #REF) .\n sent1: We use a similar model as our base model.\n sent2: It takes word tokens, position of arguments and their entity types as input.\n sent3: Some work (#REF; #TARGET_REF used extra syntax features as input.\n sent4: However, the parsers that produce syntax features could have errors and vary depending on the domain of text.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Then we obtain the high level summarization φ(x) for the relation example.",
                "The decoder uses this high level representation as features for relation classification.",
                "It usually contains one hidden layer (#REF; #REF; #TARGET_REF ) and a softmax output layer.",
                "We use the same structure which can be formalized as the following:",
                "where W h and b h are the weights for the hidden"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Then we obtain the high level summarization φ(x) for the relation example.\n sent1: The decoder uses this high level representation as features for relation classification.\n sent2: It usually contains one hidden layer (#REF; #REF; #TARGET_REF ) and a softmax output layer.\n sent3: We use the same structure which can be formalized as the following:\n sent4: where W h and b h are the weights for the hidden\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The ACE05 dataset provides a cross-domain evaluation setting .",
                "It contains 6 domains: broadcast conversation (bc), broadcast news (bn), telephone conversation (cts), newswire (nw), usenet (un) and weblogs (wl).",
                "Previous work (#REF; #REF; #TARGET_REF set, and the other half of bc, cts and wl as the test sets.",
                "We followed their split of documents and their split of the relation types for asymmetric relations.",
                "The ERE dataset has a similar relation schema to ACE05, but is different in some annotation guidelines (#REF) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The ACE05 dataset provides a cross-domain evaluation setting .\n sent1: It contains 6 domains: broadcast conversation (bc), broadcast news (bn), telephone conversation (cts), newswire (nw), usenet (un) and weblogs (wl).\n sent2: Previous work (#REF; #REF; #TARGET_REF set, and the other half of bc, cts and wl as the test sets.\n sent3: We followed their split of documents and their split of the relation types for asymmetric relations.\n sent4: The ERE dataset has a similar relation schema to ACE05, but is different in some annotation guidelines (#REF) .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Our supervised base model will be similar to (#REF) .",
                "Our initial experiments did not use syntactic features (#REF; #TARGET_REF ) that require additional parsers.",
                "In order to further improve the representation learning for relation extraction, tried to transfer knowledge through bilingual representation.",
                "They used their multi-task model to train on the bilingual ACE05 datasets and obtained improvement when there is less training available (10%-50%).",
                "Our experiments will show our multitask model can make significant improvement on the full training set."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Our supervised base model will be similar to (#REF) .\n sent1: Our initial experiments did not use syntactic features (#REF; #TARGET_REF ) that require additional parsers.\n sent2: In order to further improve the representation learning for relation extraction, tried to transfer knowledge through bilingual representation.\n sent3: They used their multi-task model to train on the bilingual ACE05 datasets and obtained improvement when there is less training available (10%-50%).\n sent4: Our experiments will show our multitask model can make significant improvement on the full training set.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The setup of word embedding and position embedding was introduced by #REF .",
                "The entity embedding (#REF; #TARGET_REF ) is included for arguments that are entities rather than common nouns.",
                "At the end, each token is converted to an embedding w i as the concatenation of these three types of embeddings, where i ∈ [0, T ), T is the length of the sentence.",
                "A wide range of encoders have been proposed for relation extraction.",
                "Most of them fall into categories of CNN (#REF) , RNN (#REF) and TreeRNN (#REF) ."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: The setup of word embedding and position embedding was introduced by #REF .\n sent1: The entity embedding (#REF; #TARGET_REF ) is included for arguments that are entities rather than common nouns.\n sent2: At the end, each token is converted to an embedding w i as the concatenation of these three types of embeddings, where i ∈ [0, T ), T is the length of the sentence.\n sent3: A wide range of encoders have been proposed for relation extraction.\n sent4: Most of them fall into categories of CNN (#REF) , RNN (#REF) and TreeRNN (#REF) .\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Training separately on the two corpora (row \"Supervised\" in Table 1 ), we obtain results on ACE05 comparable to previous work (#REF) with substantially fewer features.",
                "With syntactic features as (#REF; #TARGET_REF did, it could be further improved.",
                "In this paper, however, we want to focus on representation learning from scratch first.",
                "Our experiments focus on whether we can improve the representation with more sources of data.",
                "A common way to do so is pre-training."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: Training separately on the two corpora (row \"Supervised\" in Table 1 ), we obtain results on ACE05 comparable to previous work (#REF) with substantially fewer features.\n sent1: With syntactic features as (#REF; #TARGET_REF did, it could be further improved.\n sent2: In this paper, however, we want to focus on representation learning from scratch first.\n sent3: Our experiments focus on whether we can improve the representation with more sources of data.\n sent4: A common way to do so is pre-training.\n",
        "output": "{\"label\": [null], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Therefore, citation texts have been previously used to enhance many downstream tasks in IR/NLP such as search and summarization (e.g. [2, 15, 16] ).",
                "While useful, citation texts might lack the appropriate context from the reference article #TARGET_REF 5, 18] .",
                "For example, details of the methods, assumptions or conditions for the obtained results are often not mentioned.",
                "Furthermore, in many cases the citing author might misunderstand or misquote the referenced paper and ascribe contributions to it that are not intended in that form.",
                "Hence, sometimes the citation text is not su ciently informative or in other cases, even inaccurate [17] ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Therefore, citation texts have been previously used to enhance many downstream tasks in IR/NLP such as search and summarization (e.g. [2, 15, 16] ).\n sent1: While useful, citation texts might lack the appropriate context from the reference article #TARGET_REF 5, 18] .\n sent2: For example, details of the methods, assumptions or conditions for the obtained results are often not mentioned.\n sent3: Furthermore, in many cases the citing author might misunderstand or misquote the referenced paper and ascribe contributions to it that are not intended in that form.\n sent4: Hence, sometimes the citation text is not su ciently informative or in other cases, even inaccurate [17] .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "This dataset contains 220 scienti c biomedical journal articles and 313 total citation texts where the relevant contexts for each citation text are annotated by 4 experts.",
                "Baselines.",
                "To our knowledge, the only published results on TAC 201 #TARGET_REF is #TARGET_REF , where the authors utilized query reformulation (QR) based on UMLS ontology.",
                "In addition to [4] , we also implement several other strong baselines to better evaluate the e ectiveness of our model: 1) BM25; 2) VSM: Vector Space Model that was used in [4] ; 3) DESM: Dual Embedding Space Model which is a recent embedding based retrieval model [12] ; and 4) LMD-LDA: Language modeling with LDA smoothing which is a recent extension of the LMD to also account for the latent topics [10] .",
                "All the baseline parameters are tuned for the best performance, and the same preprocessing is applied to all the baselines and our methods."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This dataset contains 220 scienti c biomedical journal articles and 313 total citation texts where the relevant contexts for each citation text are annotated by 4 experts.\n sent1: Baselines.\n sent2: To our knowledge, the only published results on TAC 201 #TARGET_REF is #TARGET_REF , where the authors utilized query reformulation (QR) based on UMLS ontology.\n sent3: In addition to [4] , we also implement several other strong baselines to better evaluate the e ectiveness of our model: 1) BM25; 2) VSM: Vector Space Model that was used in [4] ; 3) DESM: Dual Embedding Space Model which is a recent embedding based retrieval model [12] ; and 4) LMD-LDA: Language modeling with LDA smoothing which is a recent extension of the LMD to also account for the latent topics [10] .\n sent4: All the baseline parameters are tuned for the best performance, and the same preprocessing is applied to all the baselines and our methods.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "This shows the e ectiveness of our models viewed from di erent aspects in comparison with the baselines.",
                "The best baseline performance is the query reformulation (QR) method by #TARGET_REF which improves over other baselines.",
                "We observe that using general domain embeddings does not provide much advantage in comparison with the best baseline (compare WE wiki and QR in the Table) .",
                "However, using the domain speci c embeddings (WE Bio ) results in 10% c-F improvement over the best baseline.",
                "This is expected since word relations in the biomedical context are better captured with biomedical embeddings."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This shows the e ectiveness of our models viewed from di erent aspects in comparison with the baselines.\n sent1: The best baseline performance is the query reformulation (QR) method by #TARGET_REF which improves over other baselines.\n sent2: We observe that using general domain embeddings does not provide much advantage in comparison with the best baseline (compare WE wiki and QR in the Table) .\n sent3: However, using the domain speci c embeddings (WE Bio ) results in 10% c-F improvement over the best baseline.\n sent4: This is expected since word relations in the biomedical context are better captured with biomedical embeddings.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "To our knowledge, the only published results on TAC 2014 is [4] , where the authors utilized query reformulation (QR) based on UMLS ontology.",
                "In addition to #TARGET_REF , we also implement several other strong baselines to better evaluate the e ectiveness of our model: 1) BM25; 2) VSM: Vector Space Model that was used in #TARGET_REF ; 3) DESM: Dual Embedding Space Model which is a recent embedding based retrieval model [12] ; and #TARGET_REF) LMD-LDA: Language modeling with LDA smoothing which is a recent extension of the LMD to also account for the latent topics [10] .",
                "All the baseline parameters are tuned for the best performance, and the same preprocessing is applied to all the baselines and our methods.",
                "Our methods.",
                "We rst report results based on training the embeddings on Wikipedia (WE Wiki )."
            ],
            "label": [
                "DIFFERENCES",
                "EXTENDS"
            ]
        },
        "input": "sent0: To our knowledge, the only published results on TAC 2014 is [4] , where the authors utilized query reformulation (QR) based on UMLS ontology.\n sent1: In addition to #TARGET_REF , we also implement several other strong baselines to better evaluate the e ectiveness of our model: 1) BM25; 2) VSM: Vector Space Model that was used in #TARGET_REF ; 3) DESM: Dual Embedding Space Model which is a recent embedding based retrieval model [12] ; and #TARGET_REF) LMD-LDA: Language modeling with LDA smoothing which is a recent extension of the LMD to also account for the latent topics [10] .\n sent2: All the baseline parameters are tuned for the best performance, and the same preprocessing is applied to all the baselines and our methods.\n sent3: Our methods.\n sent4: We rst report results based on training the embeddings on Wikipedia (WE Wiki ).\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"EXTENDS\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF proposed dual embedded space model that predicts document aboutness by comparing the centroid of word vectors to query terms.",
                "#REF used embeddings to transform term weights in a translation model for retrieval.",
                "Their model uses embeddings to expand documents and use co-occurrences for estimation.",
                "Unlike these works, we directly use embeddings in estimating the likelihood of query given documents; we furthermore incorporate ways to utilize domain speci c knowledge in our model.",
                "The most relevant prior work to ours is #TARGET_REF where the authors approached the problem using a vector space model similarity ranking and query reformulations."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: #REF proposed dual embedded space model that predicts document aboutness by comparing the centroid of word vectors to query terms.\n sent1: #REF used embeddings to transform term weights in a translation model for retrieval.\n sent2: Their model uses embeddings to expand documents and use co-occurrences for estimation.\n sent3: Unlike these works, we directly use embeddings in estimating the likelihood of query given documents; we furthermore incorporate ways to utilize domain speci c knowledge in our model.\n sent4: The most relevant prior work to ours is #TARGET_REF where the authors approached the problem using a vector space model similarity ranking and query reformulations.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Mutual expectation (#REF) involves deriving a normalized statistic that reflects the extent to which a phrase resists the omission of any constituent word.",
                "Similarly, the lexical predictability ratio (LPR) of #TARGET_REF is an association measure applicable to any possible syntactic pattern, which is calculated by discounting syntactic predictability from the overall conditional probability for each word given the other words in the phrase.",
                "Though most association measures involve only usage statistics of the phrase and its subparts, the DRUID measure (#REF) is an exception which uses distributional semantics around the phrase to identify how easily an n-gram could be replaced by a single word.",
                "Typically multiword lexicons are created by ranking n-grams according to an association measure and applying a threshold.",
                "The algorithm of da #REF is somewhat more sophisticated, in that it identifies the local maxima of association measures across subsuming n-grams within a sentence to identify MWEs of unrestricted length and syntactic composition; its effectiveness beyond noun phrases, however, seems relatively limited (#REF) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Mutual expectation (#REF) involves deriving a normalized statistic that reflects the extent to which a phrase resists the omission of any constituent word.\n sent1: Similarly, the lexical predictability ratio (LPR) of #TARGET_REF is an association measure applicable to any possible syntactic pattern, which is calculated by discounting syntactic predictability from the overall conditional probability for each word given the other words in the phrase.\n sent2: Though most association measures involve only usage statistics of the phrase and its subparts, the DRUID measure (#REF) is an exception which uses distributional semantics around the phrase to identify how easily an n-gram could be replaced by a single word.\n sent3: Typically multiword lexicons are created by ranking n-grams according to an association measure and applying a threshold.\n sent4: The algorithm of da #REF is somewhat more sophisticated, in that it identifies the local maxima of association measures across subsuming n-grams within a sentence to identify MWEs of unrestricted length and syntactic composition; its effectiveness beyond noun phrases, however, seems relatively limited (#REF) .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "One major motivation for looking beyond MWEs is the ability to carry out broader linguistic analyses.",
                "Within corpus linguistics, multiword sequences have been studied in the form of lexical bundles (#REF) , which are simply n-grams that occur above a certain frequency threshold.",
                "Like FS, lexical bundles generally involve larger phrasal chunks that would be missed by traditional MWE extraction, and so research in this area has tended to focus on how particular formulaic phrases (e.g., if you look at) are indicative of particular genres (e.g., university lectures).",
                "Lexical bundles have been applied, in particular, to learner language: for example, #REF show that non-native student writers use a severely restricted range of lexical bundle types, and tend to overuse those types, while #REF investigate the role of proficiency, demonstrating that intermediate learners underuse lower-frequency bigrams and overuse high-frequency bigrams relative to advanced learners.",
                "#REF demonstrate that improving fluency (closely linked to the use of linguistic formulas) is more important than improving strict grammaticality with respect to native speaker judgments of non-native productions; #TARGET_REF explicitly argue for FS lexicons as a way to identify, track, and improve learner proficiency."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: One major motivation for looking beyond MWEs is the ability to carry out broader linguistic analyses.\n sent1: Within corpus linguistics, multiword sequences have been studied in the form of lexical bundles (#REF) , which are simply n-grams that occur above a certain frequency threshold.\n sent2: Like FS, lexical bundles generally involve larger phrasal chunks that would be missed by traditional MWE extraction, and so research in this area has tended to focus on how particular formulaic phrases (e.g., if you look at) are indicative of particular genres (e.g., university lectures).\n sent3: Lexical bundles have been applied, in particular, to learner language: for example, #REF show that non-native student writers use a severely restricted range of lexical bundle types, and tend to overuse those types, while #REF investigate the role of proficiency, demonstrating that intermediate learners underuse lower-frequency bigrams and overuse high-frequency bigrams relative to advanced learners.\n sent4: #REF demonstrate that improving fluency (closely linked to the use of linguistic formulas) is more important than improving strict grammaticality with respect to native speaker judgments of non-native productions; #TARGET_REF explicitly argue for FS lexicons as a way to identify, track, and improve learner proficiency.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Our approach to FS identification involves optimization of the total explanatory power of a lattice, where each node corresponds to an n-gram type.",
                "The explanatory power of the whole lattice is defined simply as a product of the explainedness of the individual nodes.",
                "Each node can be considered either \"on\" (is an FS) or \"off\" (is not an FS).",
                "The basis of the calculation of explainedness is the syntax-sensitive LPR association measure of #TARGET_REF , but it is calculated differently depending on the on/off status of the node as well as the status of the nodes in its vicinity.",
                "Nodes are linked based on n-gram subsumption and corpus overlap relationships (see Figure 2), with \"on\" nodes typically explaining other nodes."
            ],
            "label": [
                "USE",
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Our approach to FS identification involves optimization of the total explanatory power of a lattice, where each node corresponds to an n-gram type.\n sent1: The explanatory power of the whole lattice is defined simply as a product of the explainedness of the individual nodes.\n sent2: Each node can be considered either \"on\" (is an FS) or \"off\" (is not an FS).\n sent3: The basis of the calculation of explainedness is the syntax-sensitive LPR association measure of #TARGET_REF , but it is calculated differently depending on the on/off status of the node as well as the status of the nodes in its vicinity.\n sent4: Nodes are linked based on n-gram subsumption and corpus overlap relationships (see Figure 2), with \"on\" nodes typically explaining other nodes.\n",
        "output": "{\"label\": [\"USE\", \"DIFFERENCES\"], \"context\": [\"sent0\", \"sent1\", \"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Since our primary association measure is an adaption of LPR, our approach in this section mostly follows #REF up until the last stage.",
                "An initial requirement of any such method is an n-gram frequency threshold, which we set to 1 instance per 10 million words, following #TARGET_REF .",
                "3 We include gapped or non-contiguous n-grams in our analysis, in acknowledgment of the fact that many languages have MWEs where the components can be \"separated\", including verb particle constructions in English (Dehé, 2002) , and noun-verb idioms in Japanese .",
                "Having said this, there are generally strong syntactic and length restrictions on what can constitute a gap (#REF) , which we capture in the form of a language-specific POS-based regular expression (see Section 4 for details).",
                "This greatly lowers the number of potentially gapped n-gram types, increasing precision and efficiency for negligible loss of recall."
            ],
            "label": [
                "USE",
                "SIMILARITY"
            ]
        },
        "input": "sent0: Since our primary association measure is an adaption of LPR, our approach in this section mostly follows #REF up until the last stage.\n sent1: An initial requirement of any such method is an n-gram frequency threshold, which we set to 1 instance per 10 million words, following #TARGET_REF .\n sent2: 3 We include gapped or non-contiguous n-grams in our analysis, in acknowledgment of the fact that many languages have MWEs where the components can be \"separated\", including verb particle constructions in English (Dehé, 2002) , and noun-verb idioms in Japanese .\n sent3: Having said this, there are generally strong syntactic and length restrictions on what can constitute a gap (#REF) , which we capture in the form of a language-specific POS-based regular expression (see Section 4 for details).\n sent4: This greatly lowers the number of potentially gapped n-gram types, increasing precision and efficiency for negligible loss of recall.\n",
        "output": "{\"label\": [\"USE\", \"SIMILARITY\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We evaluate our approach across three different languages, including evaluation sets derived from four different corpora selected for their size and linguistic diversity.",
                "In English, we follow #TARGET_REF in using a 890M token filtered portion of the ICWSM blog corpus (#REF ) tagged with the Tree Tagger (#REF) .",
                "To facilitate a comparison with #REF , which does not scale up to a corpus as large as the ICWSM, we also build a lexicon using the 100M token British National Corpus (#REF) , using the standard CLAWS-derived POS tags for the corpus.",
                "Lemmatization included removing all inflectional marking from both words and POS tags.",
                "For English, gaps are identified using the same POS regex used in #REF , which includes simple nouns and portions thereof, up to a maximum of 4 words."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We evaluate our approach across three different languages, including evaluation sets derived from four different corpora selected for their size and linguistic diversity.\n sent1: In English, we follow #TARGET_REF in using a 890M token filtered portion of the ICWSM blog corpus (#REF ) tagged with the Tree Tagger (#REF) .\n sent2: To facilitate a comparison with #REF , which does not scale up to a corpus as large as the ICWSM, we also build a lexicon using the 100M token British National Corpus (#REF) , using the standard CLAWS-derived POS tags for the corpus.\n sent3: Lemmatization included removing all inflectional marking from both words and POS tags.\n sent4: For English, gaps are identified using the same POS regex used in #REF , which includes simple nouns and portions thereof, up to a maximum of 4 words.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "In English, we follow #REF in using a 890M token filtered portion of the ICWSM blog corpus (#REF ) tagged with the Tree Tagger (#REF) .",
                "To facilitate a comparison with #REF , which does not scale up to a corpus as large as the ICWSM, we also build a lexicon using the 100M token British National Corpus (#REF) , using the standard CLAWS-derived POS tags for the corpus.",
                "Lemmatization included removing all inflectional marking from both words and POS tags.",
                "For English, gaps are identified using the same POS regex used in #TARGET_REF , which includes simple nouns and portions thereof, up to a maximum of 4 words.",
                "The other two languages we include in our evaluation are Croatian and Japanese."
            ],
            "label": [
                "USE",
                "SIMILARITY"
            ]
        },
        "input": "sent0: In English, we follow #REF in using a 890M token filtered portion of the ICWSM blog corpus (#REF ) tagged with the Tree Tagger (#REF) .\n sent1: To facilitate a comparison with #REF , which does not scale up to a corpus as large as the ICWSM, we also build a lexicon using the 100M token British National Corpus (#REF) , using the standard CLAWS-derived POS tags for the corpus.\n sent2: Lemmatization included removing all inflectional marking from both words and POS tags.\n sent3: For English, gaps are identified using the same POS regex used in #TARGET_REF , which includes simple nouns and portions thereof, up to a maximum of 4 words.\n sent4: The other two languages we include in our evaluation are Croatian and Japanese.\n",
        "output": "{\"label\": [\"USE\", \"SIMILARITY\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Note that the identification of the best ratio across all possible choices of context, not just the largest, is important for longer FS, where the entire POS context alone might uniquely identify the phrase, resulting in the minimum LPR of 1 even for entirely formulaic sequences-an undesirable result.",
                "In the segmentation approach of #TARGET_REF , LPR for an entire span is calculated as a product of the individual LPRs, but here we will use the minimum LPR across the words in the sequence:",
                "Here, minLPR for a particular n-gram does not reflect the overall degree to which it holds together, but rather focuses on the word which is its weakest link.",
                "For example, in the case of be keep * under wraps (Figure 2 ), a general statistical metric might assign it a high score due to the strong association between keep and under or under and wraps, but minLPR is focused on the weaker relationship between be and the rest of the phrase.",
                "This makes it particularly suited to use in a lattice model of competing n-grams, where the choice of be keep * under wraps versus keep * under wraps should be based exactly on the extent to which be is an essential part of the phrase; the other affinities are, in effect, irrelevant, because they occur in the smaller n-gram as well."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Note that the identification of the best ratio across all possible choices of context, not just the largest, is important for longer FS, where the entire POS context alone might uniquely identify the phrase, resulting in the minimum LPR of 1 even for entirely formulaic sequences-an undesirable result.\n sent1: In the segmentation approach of #TARGET_REF , LPR for an entire span is calculated as a product of the individual LPRs, but here we will use the minimum LPR across the words in the sequence:\n sent2: Here, minLPR for a particular n-gram does not reflect the overall degree to which it holds together, but rather focuses on the word which is its weakest link.\n sent3: For example, in the case of be keep * under wraps (Figure 2 ), a general statistical metric might assign it a high score due to the strong association between keep and under or under and wraps, but minLPR is focused on the weaker relationship between be and the rest of the phrase.\n sent4: This makes it particularly suited to use in a lattice model of competing n-grams, where the choice of be keep * under wraps versus keep * under wraps should be based exactly on the extent to which be is an essential part of the phrase; the other affinities are, in effect, irrelevant, because they occur in the smaller n-gram as well.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We use the annotation of 2000 n-grams in the ICWSM corpus from that earlier work, and applied the same annotation methodology to the other three corpora: after training and based on written guidelines derived from the definitions of #REF, three native-speaker, educated annotators judged 500 contiguous n-grams and another 500 gapped n-grams for each corpus.",
                "Other than the inclusion of new languages, our test sets differ from #TARGET_REF in two ways.",
                "One advantage of a type-based annotation approach, particularly with regards to annotation with a known subjective component, is that it is quite sensible to simply discard borderline cases, improving reliability at the cost of some representativeness.",
                "As such we entirely excluded from our test set n-grams which just one annotator marked as FS.",
                "Table 1 contains the counts for the four test sets after this filtering step and Fleiss' Kappa scores before (\"Pre\") and after (\"Post\")."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: We use the annotation of 2000 n-grams in the ICWSM corpus from that earlier work, and applied the same annotation methodology to the other three corpora: after training and based on written guidelines derived from the definitions of #REF, three native-speaker, educated annotators judged 500 contiguous n-grams and another 500 gapped n-grams for each corpus.\n sent1: Other than the inclusion of new languages, our test sets differ from #TARGET_REF in two ways.\n sent2: One advantage of a type-based annotation approach, particularly with regards to annotation with a known subjective component, is that it is quite sensible to simply discard borderline cases, improving reliability at the cost of some representativeness.\n sent3: As such we entirely excluded from our test set n-grams which just one annotator marked as FS.\n sent4: Table 1 contains the counts for the four test sets after this filtering step and Fleiss' Kappa scores before (\"Pre\") and after (\"Post\").\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The main results for FS acquisition across the four corpora are shown in Table 2 .",
                "As noted in Section 2, simple statistical association measures like PMI do poorly when faced with syntactically-unrestricted n-grams of variable length: minLPR is clearly a much better statistic for this purpose.",
                "The LPRseg method of #TARGET_REF consistently outperforms simple ranking, and the lattice method proposed here does better still, with a margin that is fairly consistent across the languages.",
                "Generally, clearing and overlap node interactions provide a relatively large increase in precision at the cost of a smaller drop in recall, though the change is fairly symmetrical in Croatian.",
                "When only covering is used, the results are fairly similar to #REF , which is unsurprising given the extent to which decomposition and covering are related."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: The main results for FS acquisition across the four corpora are shown in Table 2 .\n sent1: As noted in Section 2, simple statistical association measures like PMI do poorly when faced with syntactically-unrestricted n-grams of variable length: minLPR is clearly a much better statistic for this purpose.\n sent2: The LPRseg method of #TARGET_REF consistently outperforms simple ranking, and the lattice method proposed here does better still, with a margin that is fairly consistent across the languages.\n sent3: Generally, clearing and overlap node interactions provide a relatively large increase in precision at the cost of a smaller drop in recall, though the change is fairly symmetrical in Croatian.\n sent4: When only covering is used, the results are fairly similar to #REF , which is unsurprising given the extent to which decomposition and covering are related.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Bringing in other information such as simple distributional statistics might help the model identify non-compositional semantics, and could, in combination with the existing lattice competition, focus the model on MWEs which could provide a reliable basis for generalization.",
                "For all four corpora, the lattice optimization algorithm converged within 10 iterations.",
                "Although the optimization of the lattice is several orders of magnitude more complex than the decomposition heuristics of #TARGET_REF , the time needed to build and optimize the lattice is a fraction of the time required to collect the statistics for LPR calculation, and so the end-to-end runtimes of the two methods are comparable.",
                "In the BNC, the full lattice method was much faster than LocalMaxs and DP-Seg, though direct runtime comparisons to these methods are of modest value due to differences in both scope and implementation.",
                "Finally, though the model was designed specifically for FS extraction, we note that it could be useful for related tasks such as unsupervised learning of morphological lexicons, particularly for agglutinative languages."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Bringing in other information such as simple distributional statistics might help the model identify non-compositional semantics, and could, in combination with the existing lattice competition, focus the model on MWEs which could provide a reliable basis for generalization.\n sent1: For all four corpora, the lattice optimization algorithm converged within 10 iterations.\n sent2: Although the optimization of the lattice is several orders of magnitude more complex than the decomposition heuristics of #TARGET_REF , the time needed to build and optimize the lattice is a fraction of the time required to collect the statistics for LPR calculation, and so the end-to-end runtimes of the two methods are comparable.\n sent3: In the BNC, the full lattice method was much faster than LocalMaxs and DP-Seg, though direct runtime comparisons to these methods are of modest value due to differences in both scope and implementation.\n sent4: Finally, though the model was designed specifically for FS extraction, we note that it could be useful for related tasks such as unsupervised learning of morphological lexicons, particularly for agglutinative languages.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The first step in the process is to derive a set of ngrams and related statistics from a large, unlabeled corpus of text.",
                "Since our primary association measure is an adaption of LPR, our approach in this section mostly follows #TARGET_REF up until the last stage.",
                "An initial requirement of any such method is an n-gram frequency threshold, which we set to 1 instance per 10 million words, following #REF .",
                "3 We include gapped or non-contiguous n-grams in our analysis, in acknowledgment of the fact that many languages have MWEs where the components can be \"separated\", including verb particle constructions in English (Dehé, 2002) , and noun-verb idioms in Japanese .",
                "Having said this, there are generally strong syntactic and length restrictions on what can constitute a gap (#REF) , which we capture in the form of a language-specific POS-based regular expression (see Section 4 for details)."
            ],
            "label": [
                "SIMILARITY",
                "EXTENDS"
            ]
        },
        "input": "sent0: The first step in the process is to derive a set of ngrams and related statistics from a large, unlabeled corpus of text.\n sent1: Since our primary association measure is an adaption of LPR, our approach in this section mostly follows #TARGET_REF up until the last stage.\n sent2: An initial requirement of any such method is an n-gram frequency threshold, which we set to 1 instance per 10 million words, following #REF .\n sent3: 3 We include gapped or non-contiguous n-grams in our analysis, in acknowledgment of the fact that many languages have MWEs where the components can be \"separated\", including verb particle constructions in English (Dehé, 2002) , and noun-verb idioms in Japanese .\n sent4: Having said this, there are generally strong syntactic and length restrictions on what can constitute a gap (#REF) , which we capture in the form of a language-specific POS-based regular expression (see Section 4 for details).\n",
        "output": "{\"label\": [\"SIMILARITY\", \"EXTENDS\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The LPRseg method of #REF consistently outperforms simple ranking, and the lattice method proposed here does better still, with a margin that is fairly consistent across the languages.",
                "Generally, clearing and overlap node interactions provide a relatively large increase in precision at the cost of a smaller drop in recall, though the change is fairly symmetrical in Croatian.",
                "When only covering is used, the results are fairly similar to #TARGET_REF , which is unsurprising given the extent to which decomposition and covering are related.",
                "The Japanese and ICWSM corpora have relatively high precision and low recall, whereas both the BNC and Croatian corpora have low precision and high recall.",
                "In the contiguous FS test set for the BNC (Ta- Table 2 : Results of FS identification in various test sets: Countrank = ranking with frequency; PMIrank = PMI-based ranking; minLPRrank = ranking with minLPR; LPRseg = the method of #REF ; \"−cl\" = no clearing; \"−ovr\" = no penalization of overlaps; \"P\" = Precision; \"R\" = Recall; and \"F\" = F-score."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: The LPRseg method of #REF consistently outperforms simple ranking, and the lattice method proposed here does better still, with a margin that is fairly consistent across the languages.\n sent1: Generally, clearing and overlap node interactions provide a relatively large increase in precision at the cost of a smaller drop in recall, though the change is fairly symmetrical in Croatian.\n sent2: When only covering is used, the results are fairly similar to #TARGET_REF , which is unsurprising given the extent to which decomposition and covering are related.\n sent3: The Japanese and ICWSM corpora have relatively high precision and low recall, whereas both the BNC and Croatian corpora have low precision and high recall.\n sent4: In the contiguous FS test set for the BNC (Ta- Table 2 : Results of FS identification in various test sets: Countrank = ranking with frequency; PMIrank = PMI-based ranking; minLPRrank = ranking with minLPR; LPRseg = the method of #REF ; \"−cl\" = no clearing; \"−ovr\" = no penalization of overlaps; \"P\" = Precision; \"R\" = Recall; and \"F\" = F-score.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "This is done as a preprocessing step, and greatly improves the tractability of the iterative optimization of the lattice.",
                "Of course, a threshold for hard covering must be chosen: during development we found that a ratio of 2/3 (corresponding to a significant majority of the counts of a lower node corresponding to the higher node) worked well.",
                "We also use the concept of hard covering to address the issue of pronouns, based on the observation that specific pronouns often have high LPR values due to pragmatic biases #TARGET_REF ; for instance, private state verbs like feel tend to have first person singular subjects.",
                "In the lattice, n-grams with pronouns are considered covered (inactive) unless they cover at least one other node which does not have a pronoun, which allows us to limit FS with pronouns without excluding them entirely: they are included only in cases where they are definitively formulaic.",
                "Soft covering is used in cases when a single ngram does not entirely account for another, but a turned-on n-gram to some extent may explain some of the statistical irregularity of one lower in the lattice."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: This is done as a preprocessing step, and greatly improves the tractability of the iterative optimization of the lattice.\n sent1: Of course, a threshold for hard covering must be chosen: during development we found that a ratio of 2/3 (corresponding to a significant majority of the counts of a lower node corresponding to the higher node) worked well.\n sent2: We also use the concept of hard covering to address the issue of pronouns, based on the observation that specific pronouns often have high LPR values due to pragmatic biases #TARGET_REF ; for instance, private state verbs like feel tend to have first person singular subjects.\n sent3: In the lattice, n-grams with pronouns are considered covered (inactive) unless they cover at least one other node which does not have a pronoun, which allows us to limit FS with pronouns without excluding them entirely: they are included only in cases where they are definitively formulaic.\n sent4: Soft covering is used in cases when a single ngram does not entirely account for another, but a turned-on n-gram to some extent may explain some of the statistical irregularity of one lower in the lattice.\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The rationale is that the number of positive gapped examples is too low to provide a reliable independent F-score.",
                "Our primary comparison is with the heuristic LPR model of #TARGET_REF , which is scalable to large corpora and includes gapped n-grams.",
                "For the BNC, we also benchmark against the DP-seg model of #REF with recommended settings, and the LocalMaxs algorithm of da #REF using SCP; neither of these methods scale to the larger corpora.",
                "4 Because these other approaches only generate sequential multiword units, we use only the sequential part of the BNC test set for this evaluation.",
                "All comparison approaches have themselves been previously compared against a wide range of association measures."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: The rationale is that the number of positive gapped examples is too low to provide a reliable independent F-score.\n sent1: Our primary comparison is with the heuristic LPR model of #TARGET_REF , which is scalable to large corpora and includes gapped n-grams.\n sent2: For the BNC, we also benchmark against the DP-seg model of #REF with recommended settings, and the LocalMaxs algorithm of da #REF using SCP; neither of these methods scale to the larger corpora.\n sent3: 4 Because these other approaches only generate sequential multiword units, we use only the sequential part of the BNC test set for this evaluation.\n sent4: All comparison approaches have themselves been previously compared against a wide range of association measures.\n",
        "output": "{\"label\": [null], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "introducing trainable parameters for the components that differentiate multi-task learning approaches.",
                "We build on recent work trying to learn where to split merged networks [21] , as well as work trying to learn how best to combine private and shared subspaces [5, 18] .",
                "Our model is empirically justified and deals with the dirtiness [16] of loosely related tasks.",
                "We show that it is a generalization of various multi-task learning algorithms such as hard parameter sharing [7] , low supervision #TARGET_REF , and cross-stitch networks [21] , as well as transfer learning algorithms such as frustratingly easy domain adaptation [9] .",
                "Moreover, we study what task properties predict gains, and what properties correlate with learning certain types of sharing, as well as the inductive bias of the resulting architecture."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: introducing trainable parameters for the components that differentiate multi-task learning approaches.\n sent1: We build on recent work trying to learn where to split merged networks [21] , as well as work trying to learn how best to combine private and shared subspaces [5, 18] .\n sent2: Our model is empirically justified and deals with the dirtiness [16] of loosely related tasks.\n sent3: We show that it is a generalization of various multi-task learning algorithms such as hard parameter sharing [7] , low supervision #TARGET_REF , and cross-stitch networks [21] , as well as transfer learning algorithms such as frustratingly easy domain adaptation [9] .\n sent4: Moreover, we study what task properties predict gains, and what properties correlate with learning certain types of sharing, as well as the inductive bias of the resulting architecture.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The architecture is very flexible and can be seen as a generalization over several existing algorithms for transfer and multi-task learning, including [7, 9, #TARGET_REF 21] .",
                "We show how to derive each of these below.",
                "Hard Parameter Sharing in the two networks appears if all α values are set to the same constant [7, 8] .",
                "This is equivalent to a mean-constrained 0-regularizer Ω(·) = | · |w i 0 and i λiLi < 1. If the sum of weighted losses are smaller than 1, the loss with penalty is always the highest when all parameters are shared.",
                "Group Lasso The 1/ 2 group lasso regularizer is G g=1 ||G1,i,g||2, a weighted sum over the 2 norms of the groups, often used to enforce subspace sharing [33, 26] ."
            ],
            "label": [
                "USE",
                "SIMILARITY"
            ]
        },
        "input": "sent0: The architecture is very flexible and can be seen as a generalization over several existing algorithms for transfer and multi-task learning, including [7, 9, #TARGET_REF 21] .\n sent1: We show how to derive each of these below.\n sent2: Hard Parameter Sharing in the two networks appears if all α values are set to the same constant [7, 8] .\n sent3: This is equivalent to a mean-constrained 0-regularizer Ω(·) = | · |w i 0 and i λiLi < 1. If the sum of weighted losses are smaller than 1, the loss with penalty is always the highest when all parameters are shared.\n sent4: Group Lasso The 1/ 2 group lasso regularizer is G g=1 ||G1,i,g||2, a weighted sum over the 2 norms of the groups, often used to enforce subspace sharing [33, 26] .\n",
        "output": "{\"label\": [\"USE\", \"SIMILARITY\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "We present experiments with the English portions of datasets, for which we show statistics in Table 1.",
                "2 Tasks In multi-task learning, one task is usually considered the main task, while other tasks are used as auxiliary tasks to improve performance on the main task.",
                "As main tasks, we use chunking (CHUNK), named entity recognition (NER), and a simplified version of semantic role labeling (SRL) where we only identify headwords, and pair them with part-of-speech tagging (POS) as an auxiliary task, following #TARGET_REF .",
                "Example annotations for each task can be found in Table 2 .",
                "Model We use a state-ofthe-art BiLSTM-based sequence labeling model [23] as the building block of our model."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We present experiments with the English portions of datasets, for which we show statistics in Table 1.\n sent1: 2 Tasks In multi-task learning, one task is usually considered the main task, while other tasks are used as auxiliary tasks to improve performance on the main task.\n sent2: As main tasks, we use chunking (CHUNK), named entity recognition (NER), and a simplified version of semantic role labeling (SRL) where we only identify headwords, and pair them with part-of-speech tagging (POS) as an auxiliary task, following #TARGET_REF .\n sent3: Example annotations for each task can be found in Table 2 .\n sent4: Model We use a state-ofthe-art BiLSTM-based sequence labeling model [23] as the building block of our model.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We use the same hyperparameters for all comparison models across all domains.",
                "We train our models on each domain and evaluate them both on the in-domain test set as well as on the test sets of all other domains to evaluate their out-of-domain generalization ability.",
                "Baseline Models As baselines, we compare against i) a single-task model only trained on chunking; ii) the low supervision model by #TARGET_REF , which predicts the auxiliary task at the first layer; iii) an MTL model based on hard parameter sharing [6] ; and iv) cross-stitch networks [21] .",
                "We compare these against our complete sluice network with subspace constraints and learned α and β parameters.",
                "We provide a detailed ablation analysis of our model in Section 5."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We use the same hyperparameters for all comparison models across all domains.\n sent1: We train our models on each domain and evaluate them both on the in-domain test set as well as on the test sets of all other domains to evaluate their out-of-domain generalization ability.\n sent2: Baseline Models As baselines, we compare against i) a single-task model only trained on chunking; ii) the low supervision model by #TARGET_REF , which predicts the auxiliary task at the first layer; iii) an MTL model based on hard parameter sharing [6] ; and iv) cross-stitch networks [21] .\n sent3: We compare these against our complete sluice network with subspace constraints and learned α and β parameters.\n sent4: We provide a detailed ablation analysis of our model in Section 5.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "L K .",
                "We assume that all the deep networks have the same hyper-parameters at the outset.",
                "With loosely related tasks, one task may be better modeled with one hidden layer; another one with two #TARGET_REF .",
                "Our architecture, however, is flexible enough to learn this, if we initially associate each task with the union of the a priori task networks.",
                "Let W ∈ R M ×D be a matrix in which each row i corresponds to a model θ i with D parameters."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: L K .\n sent1: We assume that all the deep networks have the same hyper-parameters at the outset.\n sent2: With loosely related tasks, one task may be better modeled with one hidden layer; another one with two #TARGET_REF .\n sent3: Our architecture, however, is flexible enough to learn this, if we initially associate each task with the union of the a priori task networks.\n sent4: Let W ∈ R M ×D be a matrix in which each row i corresponds to a model θ i with D parameters.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The approach to domain adaptation in [9] , which relies on a shared and a private space for each task or domain, can be encoded in sluice networks by setting all αA,B-and αB,A-weights associated with G i,k,1 to 0, while setting all αA,B-weights associated with G i,k,2 to αB,B, and αB,A-weights associated with G i,k,2 to αA,A. Note that [9] discusses three subspaces.",
                "We split the space in two, leading to three subspaces, if we only share one half across the two networks.",
                "Low Supervision #TARGET_REF propose a model where only the inner layers of two deep recurrent works are shared.",
                "This is obtained using heavy mean-constrained L0 regularization over the first layer Li,1, e.g., Ω(W ) = K i ||Li,1||0 with i λiL(i) < 1, while for the auxiliary task, only the first layer β parameter is set to 1.",
                "Cross-Stitch #REF introduce cross-stitch networks that have α values control the flow between layers of two convolutional neural networks."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The approach to domain adaptation in [9] , which relies on a shared and a private space for each task or domain, can be encoded in sluice networks by setting all αA,B-and αB,A-weights associated with G i,k,1 to 0, while setting all αA,B-weights associated with G i,k,2 to αB,B, and αB,A-weights associated with G i,k,2 to αA,A. Note that [9] discusses three subspaces.\n sent1: We split the space in two, leading to three subspaces, if we only share one half across the two networks.\n sent2: Low Supervision #TARGET_REF propose a model where only the inner layers of two deep recurrent works are shared.\n sent3: This is obtained using heavy mean-constrained L0 regularization over the first layer Li,1, e.g., Ω(W ) = K i ||Li,1||0 with i λiL(i) < 1, while for the auxiliary task, only the first layer β parameter is set to 1.\n sent4: Cross-Stitch #REF introduce cross-stitch networks that have α values control the flow between layers of two convolutional neural networks.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Sluice networks not only learn the parameters in W , but also some of the parameters of the regularizer Ω, through the α weights, while the β weights are used to learn the parameters of the mixture functions f (·).",
                "Recall that our architecture is partly motivated by the observation that for loosely related tasks, only certain features in specific layers should be shared, while many of the layers and subspaces may remain more taskspecific #TARGET_REF .",
                "We want to learn what to share while inducing models for the different tasks.",
                "For simplicity, we ignore subspaces at first and assume only two tasks A and B. The outputs h A,k,t and h B,k,t of the k-th layer for time step t for task A and B respectively interact through what [21] refer to as cross-stitch units α (see Figure  1 ).",
                "Omitting t for simplicity, the output of the α layers is:"
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Sluice networks not only learn the parameters in W , but also some of the parameters of the regularizer Ω, through the α weights, while the β weights are used to learn the parameters of the mixture functions f (·).\n sent1: Recall that our architecture is partly motivated by the observation that for loosely related tasks, only certain features in specific layers should be shared, while many of the layers and subspaces may remain more taskspecific #TARGET_REF .\n sent2: We want to learn what to share while inducing models for the different tasks.\n sent3: For simplicity, we ignore subspaces at first and assume only two tasks A and B. The outputs h A,k,t and h B,k,t of the k-th layer for time step t for task A and B respectively interact through what [21] refer to as cross-stitch units α (see Figure  1 ).\n sent4: Omitting t for simplicity, the output of the α layers is:\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "**LEARNING MIXTURES MANY TASKS HAVE AN IMPLICIT HIERARCHY THAT INFORMS THEIR INTERACTION. RATHER THAN**",
                "predefining it #TARGET_REF 11] , we enable our model to learn hierarchical relations by associating different tasks with different layers if this is beneficial for learning.",
                "Inspired by advances in residual learning [12] , we employ skip-connections from each layer, controlled using β parameters.",
                "This layer acts as a mixture model, returning a mixture of expert predictions:",
                "where h A,k is the output of layer k of model A, while hA,t is the linear combination of all layer outputs of model A that is fed into the final softmax layer."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: **LEARNING MIXTURES MANY TASKS HAVE AN IMPLICIT HIERARCHY THAT INFORMS THEIR INTERACTION. RATHER THAN**\n sent1: predefining it #TARGET_REF 11] , we enable our model to learn hierarchical relations by associating different tasks with different layers if this is beneficial for learning.\n sent2: Inspired by advances in residual learning [12] , we employ skip-connections from each layer, controlled using β parameters.\n sent3: This layer acts as a mixture model, returning a mixture of expert predictions:\n sent4: where h A,k is the output of layer k of model A, while hA,t is the linear combination of all layer outputs of model A that is fed into the final softmax layer.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Finally, concatenation of layer outputs is a viable form to share information across layers.",
                "Chunking, NER, and SRL.",
                "We present inner, middle, and outer layer left to right.",
                "Figure 2 presents the final α weights in the sluice networks for Chunking, NER, and SRL, trained with newswire as training data.",
                "We see that a) for the low-level simplified SRL, there is more sharing at inner layers, which is in line with #TARGET_REF , while Chunking and NER also rely on the outer layer, and b) more information is shared from the more complex target tasks than vice versa."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: Finally, concatenation of layer outputs is a viable form to share information across layers.\n sent1: Chunking, NER, and SRL.\n sent2: We present inner, middle, and outer layer left to right.\n sent3: Figure 2 presents the final α weights in the sluice networks for Chunking, NER, and SRL, trained with newswire as training data.\n sent4: We see that a) for the low-level simplified SRL, there is more sharing at inner layers, which is in line with #TARGET_REF , while Chunking and NER also rely on the outer layer, and b) more information is shared from the more complex target tasks than vice versa.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Ability to Fit Noise Sluice networks can learn to disregard sharing completely, so we expect them to be as good as single-task networks to fit random noise, potentially even better.",
                "We verify this by computing a learning curve for random relabelings of 200 sentences annotated with syntactic chunking brackets, as well as 100 gold standard POS-annotated sentences.",
                "The figure in 3 shows that hard parameter sharing, while learning faster because of the smoother loss surface in multi-task learning, is a good regularizer, confirming the findings in #TARGET_REF , whereas the sluice network is even better at fitting noise than the single-task models.",
                "While ability to fit noise is not necessarily a problem [32] , this means that it can be beneficial to add inductive bias to the regularizer, especially when working with small amounts of data."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: Ability to Fit Noise Sluice networks can learn to disregard sharing completely, so we expect them to be as good as single-task networks to fit random noise, potentially even better.\n sent1: We verify this by computing a learning curve for random relabelings of 200 sentences annotated with syntactic chunking brackets, as well as 100 gold standard POS-annotated sentences.\n sent2: The figure in 3 shows that hard parameter sharing, while learning faster because of the smoother loss surface in multi-task learning, is a good regularizer, confirming the findings in #TARGET_REF , whereas the sluice network is even better at fitting noise than the single-task models.\n sent3: While ability to fit noise is not necessarily a problem [32] , this means that it can be beneficial to add inductive bias to the regularizer, especially when working with small amounts of data.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Deep neural network models have demonstrated strong performance on a number of challenging tasks, such as image classification (#REF) , speech recognition (#REF) , and various natural language processing tasks #REF; #REF) .",
                "Recently, the augmentation of neural networks with external memory components has been shown to be a powerful means of capturing context of different types (#REF #REF) .",
                "Of particular interest to this work is the work by #TARGET_REF , on end-toend memory networks (N2Ns), which exhibit remarkable reasoning capabilities, e.g. for reasoning and goal-oriented dialogue tasks .",
                "Typically, such tasks consist of three key components: a sequence of supporting facts (the story), a question, and its answer.",
                "An example task is given in Figure 1 ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Deep neural network models have demonstrated strong performance on a number of challenging tasks, such as image classification (#REF) , speech recognition (#REF) , and various natural language processing tasks #REF; #REF) .\n sent1: Recently, the augmentation of neural networks with external memory components has been shown to be a powerful means of capturing context of different types (#REF #REF) .\n sent2: Of particular interest to this work is the work by #TARGET_REF , on end-toend memory networks (N2Ns), which exhibit remarkable reasoning capabilities, e.g. for reasoning and goal-oriented dialogue tasks .\n sent3: Typically, such tasks consist of three key components: a sequence of supporting facts (the story), a question, and its answer.\n sent4: An example task is given in Figure 1 .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "End-to-End Memory Networks: Building on top of memory networks , #TARGET_REF ing the memory position supervision and making the model trainable in an end-to-end fashion, through the advent of supporting memories and a memory access controller.",
                "Representations of the context sentences x 1 , . . . , x n in the story are encoded using two sets of embedding matrices A and C (both of size d ⇥ |V | where d is the embedding size and |V | the vocabulary size), and stored in the input and output memory cells m 1 , . . . , m n and c 1 , . . . , c n , each of which is obtained via",
                "is a function that maps the input into a bag of dimension |V |.",
                "The input question q is encoded with another embedding matrix B 2 R d⇥|V | such that u = B (q).",
                "N2N utilises the question embedding u and the input memory representations m i to measure the relevance between the question and each supporting context sentence, resulting in a vector of attention weights:"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: End-to-End Memory Networks: Building on top of memory networks , #TARGET_REF ing the memory position supervision and making the model trainable in an end-to-end fashion, through the advent of supporting memories and a memory access controller.\n sent1: Representations of the context sentences x 1 , . . . , x n in the story are encoded using two sets of embedding matrices A and C (both of size d ⇥ |V | where d is the embedding size and |V | the vocabulary size), and stored in the input and output memory cells m 1 , . . . , m n and c 1 , . . . , c n , each of which is obtained via\n sent2: is a function that maps the input into a bag of dimension |V |.\n sent3: The input question q is encoded with another embedding matrix B 2 R d⇥|V | such that u = B (q).\n sent4: N2N utilises the question embedding u and the input memory representations m i to measure the relevance between the question and each supporting context sentence, resulting in a vector of attention weights:\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "N2N utilises the question embedding u and the input memory representations m i to measure the relevance between the question and each supporting context sentence, resulting in a vector of attention weights:",
                "where softmax(a i ) = e a i P j e a j .",
                "Once the attention weights have been computed, the memory access controller receives the response o in the form of a weighted sum over the output memory representations:",
                "To enhance the model's ability to cope with more challenging tasks requiring multiple supporting facts from the memory, #TARGET_REF further extended the model by stacking multiple memory layers (also known as \"hops\"), in which case the output of the k th hop is taken as input to the (k + 1) th hop:",
                "Lastly, N2N predicts the answer to question q using a softmax function: whereŷ is the predicted answer distribution, W 2 R |V |⇥d is a parameter matrix for the model to learn (note that in the context of bAbI tasks, answers are single words), and K is the total number of hops."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: N2N utilises the question embedding u and the input memory representations m i to measure the relevance between the question and each supporting context sentence, resulting in a vector of attention weights:\n sent1: where softmax(a i ) = e a i P j e a j .\n sent2: Once the attention weights have been computed, the memory access controller receives the response o in the form of a weighted sum over the output memory representations:\n sent3: To enhance the model's ability to cope with more challenging tasks requiring multiple supporting facts from the memory, #TARGET_REF further extended the model by stacking multiple memory layers (also known as \"hops\"), in which case the output of the k th hop is taken as input to the (k + 1) th hop:\n sent4: Lastly, N2N predicts the answer to question q using a softmax function: whereŷ is the predicted answer distribution, W 2 R |V |⇥d is a parameter matrix for the model to learn (note that in the context of bAbI tasks, answers are single words), and K is the total number of hops.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "In #TARGET_REF , two types of weight tying were explored for N2N, namely adjacent (\"ADJ\") and layer-wise (\"LW\").",
                "With LW, the input and output embedding matrices are shared across different hops (i.e., A 1 = A 2 = . . . = A K and C 1 = C 2 = . . . = C K ), resembling RNNs.",
                "With ADJ, on the other hand, not only is the output embedding for a given layer shared with the corresponding input embedding (i.e., A k+1 = C k ), the answer prediction matrix W and question embedding matrix B are also constrained such that W > = C K and B = A 1 .",
                "While both ADJ and LW work well, achieving comparable overall performance in terms of mean error over the 20 bAbI tasks, their performance on a subset of the tasks (i.e., tasks 3, 16, 17 and 19, as shown in Table 1 ) is inconsistent, with one performing very well, and the other performing poorly.",
                "Based on this observation, we propose a unified weight tying mechanism exploiting the benefits of both ADJ and LW, and capable of dynamically determining the best weight tying approach for a given task."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In #TARGET_REF , two types of weight tying were explored for N2N, namely adjacent (\"ADJ\") and layer-wise (\"LW\").\n sent1: With LW, the input and output embedding matrices are shared across different hops (i.e., A 1 = A 2 = . . . = A K and C 1 = C 2 = . . . = C K ), resembling RNNs.\n sent2: With ADJ, on the other hand, not only is the output embedding for a given layer shared with the corresponding input embedding (i.e., A k+1 = C k ), the answer prediction matrix W and question embedding matrix B are also constrained such that W > = C K and B = A 1 .\n sent3: While both ADJ and LW work well, achieving comparable overall performance in terms of mean error over the 20 bAbI tasks, their performance on a subset of the tasks (i.e., tasks 3, 16, 17 and 19, as shown in Table 1 ) is inconsistent, with one performing very well, and the other performing poorly.\n sent4: Based on this observation, we propose a unified weight tying mechanism exploiting the benefits of both ADJ and LW, and capable of dynamically determining the best weight tying approach for a given task.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "While both ADJ and LW work well, achieving comparable overall performance in terms of mean error over the 20 bAbI tasks, their performance on a subset of the tasks (i.e., tasks 3, 16, 17 and 19, as shown in Table 1 ) is inconsistent, with one performing very well, and the other performing poorly.",
                "Based on this observation, we propose a unified weight tying mechanism exploiting the benefits of both ADJ and LW, and capable of dynamically determining the best weight tying approach for a given task.",
                "Table 1 : Accuracy (%) reported in #TARGET_REF on a selected subset of the 20 bAbI 10k tasks.",
                "Note that performance in the LW column is obtained with a larger embedding size d = 100 and ReLU non-linearity applied to the internal state after each hop.",
                "Related reasoning models: Gated End-to-End Memory Networks (GN2Ns) (#REF) are a variant of N2N with a simple yet effective gating mechanism on the connections between hops, allowing the model to dynamically regulate the information flow between the controller and the memory."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: While both ADJ and LW work well, achieving comparable overall performance in terms of mean error over the 20 bAbI tasks, their performance on a subset of the tasks (i.e., tasks 3, 16, 17 and 19, as shown in Table 1 ) is inconsistent, with one performing very well, and the other performing poorly.\n sent1: Based on this observation, we propose a unified weight tying mechanism exploiting the benefits of both ADJ and LW, and capable of dynamically determining the best weight tying approach for a given task.\n sent2: Table 1 : Accuracy (%) reported in #TARGET_REF on a selected subset of the 20 bAbI 10k tasks.\n sent3: Note that performance in the LW column is obtained with a larger embedding size d = 100 and ReLU non-linearity applied to the internal state after each hop.\n sent4: Related reasoning models: Gated End-to-End Memory Networks (GN2Ns) (#REF) are a variant of N2N with a simple yet effective gating mechanism on the connections between hops, allowing the model to dynamically regulate the information flow between the controller and the memory.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "One drawback of N2Ns is the problem of choosing between two types of weight tying (adjacent and layer-wise; see Section 2 for a technical description).",
                "While N2Ns generally work well with either weight tying approach, as reported in #TARGET_REF , the performance is uneven on some difficult tasks.",
                "That is, for some tasks, one weight tying approach attains nearperfect accuracy and the other performs poorly, but for other tasks, this trend is reversed.",
                "In this paper, focusing on improving N2N, we propose a unified model, UN2N, capable of dynamically determining the appropriate type of weight tying for a given task.",
                "This is realised through the use of a gating vector, inspired by #REF ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: One drawback of N2Ns is the problem of choosing between two types of weight tying (adjacent and layer-wise; see Section 2 for a technical description).\n sent1: While N2Ns generally work well with either weight tying approach, as reported in #TARGET_REF , the performance is uneven on some difficult tasks.\n sent2: That is, for some tasks, one weight tying approach attains nearperfect accuracy and the other performs poorly, but for other tasks, this trend is reversed.\n sent3: In this paper, focusing on improving N2N, we propose a unified model, UN2N, capable of dynamically determining the appropriate type of weight tying for a given task.\n sent4: This is realised through the use of a gating vector, inspired by #REF .\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Dim. of u 0 and h T .",
                "Essentially, the gating vector z is now dependent on not only the question u 0 , but also the context sentences in the memory encoded in h T .",
                "Note that the gating vector z can be replaced by a gating scalar z, but we choose to use a vector for more fine-grained control as in LSTMs (#REF) and GRUs .",
                "To simplify the model, we constrain B and W > to share the same parameters as A 1 and C K .",
                "Moreover, following #TARGET_REF , we add a linear mapping H 2 R d⇥d to the update connection between memory hops, but in our case, down-weight it by 1 z, resulting in:"
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Dim. of u 0 and h T .\n sent1: Essentially, the gating vector z is now dependent on not only the question u 0 , but also the context sentences in the memory encoded in h T .\n sent2: Note that the gating vector z can be replaced by a gating scalar z, but we choose to use a vector for more fine-grained control as in LSTMs (#REF) and GRUs .\n sent3: To simplify the model, we constrain B and W > to share the same parameters as A 1 and C K .\n sent4: Moreover, following #TARGET_REF , we add a linear mapping H 2 R d⇥d to the update connection between memory hops, but in our case, down-weight it by 1 z, resulting in:\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "either 1k or 10k training instances per task.",
                "In this work, we focus exclusively on the 10k version.",
                "3",
                "Training Details: Following #TARGET_REF, we hold out 10% of the bAbI training set to form a development set.",
                "Position encoding and temporal encoding (with 10% random noise) are also incorporated into the model."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: either 1k or 10k training instances per task.\n sent1: In this work, we focus exclusively on the 10k version.\n sent2: 3\n sent3: Training Details: Following #TARGET_REF, we hold out 10% of the bAbI training set to form a development set.\n sent4: Position encoding and temporal encoding (with 10% random noise) are also incorporated into the model.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Training Details: Following #REF, we hold out 10% of the bAbI training set to form a development set.",
                "Position encoding and temporal encoding (with 10% random noise) are also incorporated into the model.",
                "Training is performed over 100 epochs with a batch size of 32 using the Adam optimiser (#REF) with a learning rate of 0.005.",
                "Following #TARGET_REF , linear start is employed in all our experiments for the first 20 epochs.",
                "All weight parameters are initialised based on a Gaussian distribution with zero mean and = 0.1."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Training Details: Following #REF, we hold out 10% of the bAbI training set to form a development set.\n sent1: Position encoding and temporal encoding (with 10% random noise) are also incorporated into the model.\n sent2: Training is performed over 100 epochs with a batch size of 32 using the Adam optimiser (#REF) with a learning rate of 0.005.\n sent3: Following #TARGET_REF , linear start is employed in all our experiments for the first 20 epochs.\n sent4: All weight parameters are initialised based on a Gaussian distribution with zero mean and = 0.1.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Training is performed over 100 epochs with a batch size of 32 using the Adam optimiser (#REF) with a learning rate of 0.005.",
                "Following #REF , linear start is employed in all our experiments for the first 20 epochs.",
                "All weight parameters are initialised based on a Gaussian distribution with zero mean and = 0.1.",
                "Gradients with an`2 norm of 40 are divided by a scalar to have norm 40.",
                "Also following #TARGET_REF , we use only the most recent 50 sentences as the memory and set the number of memory hops to 3, the embedding size to 20, and to"
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Training is performed over 100 epochs with a batch size of 32 using the Adam optimiser (#REF) with a learning rate of 0.005.\n sent1: Following #REF , linear start is employed in all our experiments for the first 20 epochs.\n sent2: All weight parameters are initialised based on a Gaussian distribution with zero mean and = 0.1.\n sent3: Gradients with an`2 norm of 40 are divided by a scalar to have norm 40.\n sent4: Also following #TARGET_REF , we use only the most recent 50 sentences as the memory and set the number of memory hops to 3, the embedding size to 20, and to\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "The results on the 20 bAbI QA tasks are presented in Table 3 .",
                "We benchmark against other memory network based models: (1) N2N with ADJ and LW #TARGET_REF ; (2) DMN (#REF) and its improved version DMN+ (#REF) ; and (3) GN2N (#REF) .",
                "Major improvements on the difficult tasks.",
                "The most noticeable performance gains are over tasks 16, 17 and 19 where, compared with the vanilla N2N, UN2N achieves much better results than the worst of ADJ and LW, surpassing both ADJ and LW in the case of tasks 17 and 18.",
                "This confirms the validity of the model."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The results on the 20 bAbI QA tasks are presented in Table 3 .\n sent1: We benchmark against other memory network based models: (1) N2N with ADJ and LW #TARGET_REF ; (2) DMN (#REF) and its improved version DMN+ (#REF) ; and (3) GN2N (#REF) .\n sent2: Major improvements on the difficult tasks.\n sent3: The most noticeable performance gains are over tasks 16, 17 and 19 where, compared with the vanilla N2N, UN2N achieves much better results than the worst of ADJ and LW, surpassing both ADJ and LW in the case of tasks 17 and 18.\n sent4: This confirms the validity of the model.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "In terms of the training procedure, experiments are carried out with the same configuration as described in Section 4.1.",
                "As a large variance can be observed due to how sensitive memory-based models are to parameter initialisation, following #TARGET_REF and (#REF) , we repeat each training 10 times using the Table 4 : Per-response accuracy on the Dialog bAbI tasks.",
                "N2N: .",
                "GN2N: (#REF) .",
                "+match suggests the use of the match features in Section 5.1."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In terms of the training procedure, experiments are carried out with the same configuration as described in Section 4.1.\n sent1: As a large variance can be observed due to how sensitive memory-based models are to parameter initialisation, following #TARGET_REF and (#REF) , we repeat each training 10 times using the Table 4 : Per-response accuracy on the Dialog bAbI tasks.\n sent2: N2N: .\n sent3: GN2N: (#REF) .\n sent4: +match suggests the use of the match features in Section 5.1.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The results on the Dialog bAbI tasks are shown in Table 4 .",
                "In terms of baselines, we benchmark against other memory network-based models: 4 (1) N2N #TARGET_REF ; and (2) GN2N #TARGET_REF .",
                "While the results of GN2N is achieved with ADJ, the type of weight tying for N2N is not reported in .",
                "Improvements on task 5.",
                "It can be observed that UN2N offers consistent performance boost on task 5 across all experiments settings, especially in the non-OOV group."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The results on the Dialog bAbI tasks are shown in Table 4 .\n sent1: In terms of baselines, we benchmark against other memory network-based models: 4 (1) N2N #TARGET_REF ; and (2) GN2N #TARGET_REF .\n sent2: While the results of GN2N is achieved with ADJ, the type of weight tying for N2N is not reported in .\n sent3: Improvements on task 5.\n sent4: It can be observed that UN2N offers consistent performance boost on task 5 across all experiments settings, especially in the non-OOV group.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We describe a machine learning based method to identify incorrect entries in translation memories.",
                "It extends previous work by #TARGET_REF through incorporating recall-based machine translation and part-of-speech-tagging features.",
                "Our system ranked first in the Binary Classification (II) task for two out of three language pairs: English-Italian and English-Spanish."
            ],
            "label": [
                "DIFFERENCES",
                "EXTENDS"
            ]
        },
        "input": "sent0: We describe a machine learning based method to identify incorrect entries in translation memories.\n sent1: It extends previous work by #TARGET_REF through incorporating recall-based machine translation and part-of-speech-tagging features.\n sent2: Our system ranked first in the Binary Classification (II) task for two out of three language pairs: English-Italian and English-Spanish.\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"EXTENDS\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Rather than tailoring it to individual languages, we aimed at a languageindependent solution to cover all of the language pairs in this shared task or, looking to the future, Autodesk's production environments.",
                "The system is based on previous work by #TARGET_REF and uses language-independent features with language-specific plug-ins, such as machine translation, part-of-speech tagging, and language classification.",
                "Specifics about previous work are given in the next section.",
                "In Section 3, we describe our method and, in Section 4, show how it compares to Barbu's (2015) approach as well as other submissions to this shared task.",
                "Lastly, we offer preliminary conclusions and outline future work in Section 5."
            ],
            "label": [
                "DIFFERENCES",
                "EXTENDS"
            ]
        },
        "input": "sent0: Rather than tailoring it to individual languages, we aimed at a languageindependent solution to cover all of the language pairs in this shared task or, looking to the future, Autodesk's production environments.\n sent1: The system is based on previous work by #TARGET_REF and uses language-independent features with language-specific plug-ins, such as machine translation, part-of-speech tagging, and language classification.\n sent2: Specifics about previous work are given in the next section.\n sent3: In Section 3, we describe our method and, in Section 4, show how it compares to Barbu's (2015) approach as well as other submissions to this shared task.\n sent4: Lastly, we offer preliminary conclusions and outline future work in Section 5.\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"EXTENDS\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Rather than tailoring it to individual languages, we aimed at a languageindependent solution to cover all of the language pairs in this shared task or, looking to the future, Autodesk's production environments.",
                "The system is based on previous work by #REF and uses language-independent features with language-specific plug-ins, such as machine translation, part-of-speech tagging, and language classification.",
                "Specifics about previous work are given in the next section.",
                "In Section 3, we describe our method and, in Section 4, show how it compares to #TARGET_REF approach as well as other submissions to this shared task.",
                "Lastly, we offer preliminary conclusions and outline future work in Section 5."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: Rather than tailoring it to individual languages, we aimed at a languageindependent solution to cover all of the language pairs in this shared task or, looking to the future, Autodesk's production environments.\n sent1: The system is based on previous work by #REF and uses language-independent features with language-specific plug-ins, such as machine translation, part-of-speech tagging, and language classification.\n sent2: Specifics about previous work are given in the next section.\n sent3: In Section 3, we describe our method and, in Section 4, show how it compares to #TARGET_REF approach as well as other submissions to this shared task.\n sent4: Lastly, we offer preliminary conclusions and outline future work in Section 5.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The \"most important\" of them, according to the author, were bisegment_similarity and lang_diff : the former is defined as the cosine similarity between a target segment and its machine translated source segment, while the latter denotes whether the language codes declared in a translation unit correspond with the codes detected by a language detector.",
                "The best classifier, a support vector machine with linear kernel, achieved 82% precision and 81% recall on a held-out test set of 309 TUs.",
                "To the best of our knowledge, Barbu provided the first and so far only research contribution on automatic TM cleaning, which the author himself described as \"a neglected research area\" #TARGET_REF .",
                "With our participation to this shared task, we seek to extend his work by examining new features based on statistical MT and POS tagging.",
                "As outlined above, comparing machine translated source segments to their actual target segments has proven effective in Barbu's (2015) experiments."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: The \"most important\" of them, according to the author, were bisegment_similarity and lang_diff : the former is defined as the cosine similarity between a target segment and its machine translated source segment, while the latter denotes whether the language codes declared in a translation unit correspond with the codes detected by a language detector.\n sent1: The best classifier, a support vector machine with linear kernel, achieved 82% precision and 81% recall on a held-out test set of 309 TUs.\n sent2: To the best of our knowledge, Barbu provided the first and so far only research contribution on automatic TM cleaning, which the author himself described as \"a neglected research area\" #TARGET_REF .\n sent3: With our participation to this shared task, we seek to extend his work by examining new features based on statistical MT and POS tagging.\n sent4: As outlined above, comparing machine translated source segments to their actual target segments has proven effective in Barbu's (2015) experiments.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Our feature extraction pipeline, including #TARGET_REF as well as our own features (see Section 3.1), is implemented in Scala.",
                "This pipeline is used to transform translation units into feature vectors and train classifiers using the scikitlearn framework (#REF) .",
                "From the various classification algorithms we tested, Random Forests performed best with our selection of features (see below)."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: Our feature extraction pipeline, including #TARGET_REF as well as our own features (see Section 3.1), is implemented in Scala.\n sent1: This pipeline is used to transform translation units into feature vectors and train classifiers using the scikitlearn framework (#REF) .\n sent2: From the various classification algorithms we tested, Random Forests performed best with our selection of features (see below).\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "For the reasons mentioned in Section 1, we aimed at finding a combination of features that would perform well with all language pairs rather than tailoring solutions to individual languages.",
                "We focused on gearing our classifiers to distinguish correct or almost correct (classes 1, 2) from incorrect TUs (class 3) -i.e., the Binary Classification (II) task -by optimising the weighted F 1 -score (F 1 ) on training data (see Tables 2a and 2b) .",
                "From the various feature combinations we tested, we found the following to be most successful: ratio_words, pos_sim_all, language_detection, mt_cfs, mt_bleu, ratio_chars (as described in Section 3.1), alongside cg_score, only_capletters_dif, and punctuation_similarity (from #TARGET_REF .",
                "Evaluation results are given in the next section."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: For the reasons mentioned in Section 1, we aimed at finding a combination of features that would perform well with all language pairs rather than tailoring solutions to individual languages.\n sent1: We focused on gearing our classifiers to distinguish correct or almost correct (classes 1, 2) from incorrect TUs (class 3) -i.e., the Binary Classification (II) task -by optimising the weighted F 1 -score (F 1 ) on training data (see Tables 2a and 2b) .\n sent2: From the various feature combinations we tested, we found the following to be most successful: ratio_words, pos_sim_all, language_detection, mt_cfs, mt_bleu, ratio_chars (as described in Section 3.1), alongside cg_score, only_capletters_dif, and punctuation_similarity (from #TARGET_REF .\n sent3: Evaluation results are given in the next section.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Our rationale for focusing on telling apart correct or almost correct from incorrect TUs was that a first application of our method, if successful, would most likely be the filtering of TM data for MT training.",
                "While eliminating almost correct TUs might decrease rather than increase MT quality, filtering out incorrect segments can have a positive impact (#REF) .",
                "Prior to submission, we benchmarked our system against the two baselines provided by the organizers: a dummy classifier assigning random classes according to the overall class distribution in the training data (Baseline 1), and a classifier based on the Church-Gale algorithm as adapted by #TARGET_REF (Baseline 2).",
                "More importantly, however, we compared our system to Barbu's (2015) approach, using the classification algorithms which reportedly worked best with the 17 features in his work.",
                "Our system performed well in this comparison, surpassing Barbu's approach in all language pairs except en-de, where both systems were en par."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: Our rationale for focusing on telling apart correct or almost correct from incorrect TUs was that a first application of our method, if successful, would most likely be the filtering of TM data for MT training.\n sent1: While eliminating almost correct TUs might decrease rather than increase MT quality, filtering out incorrect segments can have a positive impact (#REF) .\n sent2: Prior to submission, we benchmarked our system against the two baselines provided by the organizers: a dummy classifier assigning random classes according to the overall class distribution in the training data (Baseline 1), and a classifier based on the Church-Gale algorithm as adapted by #TARGET_REF (Baseline 2).\n sent3: More importantly, however, we compared our system to Barbu's (2015) approach, using the classification algorithms which reportedly worked best with the 17 features in his work.\n sent4: Our system performed well in this comparison, surpassing Barbu's approach in all language pairs except en-de, where both systems were en par.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Our rationale for focusing on telling apart correct or almost correct from incorrect TUs was that a first application of our method, if successful, would most likely be the filtering of TM data for MT training.",
                "While eliminating almost correct TUs might decrease rather than increase MT quality, filtering out incorrect segments can have a positive impact (#REF) .",
                "Prior to submission, we benchmarked our system against the two baselines provided by the organizers: a dummy classifier assigning random classes according to the overall class distribution in the training data (Baseline 1), and a classifier based on the Church-Gale algorithm as adapted by #REF (Baseline 2).",
                "More importantly, however, we compared our system to #TARGET_REF approach, using the classification algorithms which reportedly worked best with the 17 features in his work.",
                "Our system performed well in this comparison, surpassing Barbu's approach in all language pairs except en-de, where both systems were en par."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: Our rationale for focusing on telling apart correct or almost correct from incorrect TUs was that a first application of our method, if successful, would most likely be the filtering of TM data for MT training.\n sent1: While eliminating almost correct TUs might decrease rather than increase MT quality, filtering out incorrect segments can have a positive impact (#REF) .\n sent2: Prior to submission, we benchmarked our system against the two baselines provided by the organizers: a dummy classifier assigning random classes according to the overall class distribution in the training data (Baseline 1), and a classifier based on the Church-Gale algorithm as adapted by #REF (Baseline 2).\n sent3: More importantly, however, we compared our system to #TARGET_REF approach, using the classification algorithms which reportedly worked best with the 17 features in his work.\n sent4: Our system performed well in this comparison, surpassing Barbu's approach in all language pairs except en-de, where both systems were en par.\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Although geared to the Binary Classification (II) task (see above), we also assessed our system on the Fine-Grained Classification task.",
                "Here, the goal was to distinguish between all of the three classes, i.e., determine whether a TU is correct, almost correct, or incorrect.",
                "Again, we compared our system's performance to #TARGET_REF method, using 2 /3-1 /3 splits of the training data (5-fold cross-validation).",
                "The results, shown in Table 2b , implied that the nine features we selected would not suffice for a more fine-grained classification of TUs.",
                "This was confirmed in the official evaluation and ranking: our system scored low on en-de and mediocre on en-es and en-it."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: Although geared to the Binary Classification (II) task (see above), we also assessed our system on the Fine-Grained Classification task.\n sent1: Here, the goal was to distinguish between all of the three classes, i.e., determine whether a TU is correct, almost correct, or incorrect.\n sent2: Again, we compared our system's performance to #TARGET_REF method, using 2 /3-1 /3 splits of the training data (5-fold cross-validation).\n sent3: The results, shown in Table 2b , implied that the nine features we selected would not suffice for a more fine-grained classification of TUs.\n sent4: This was confirmed in the official evaluation and ranking: our system scored low on en-de and mediocre on en-es and en-it.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "As outlined above, comparing machine translated source segments to their actual target segments has proven effective in #TARGET_REF experiments.",
                "We propose to complement or replace the similarity function used for this comparison (cosine similarity) by two automatic MT evaluation metrics, Bleu (#REF) and characterbased Levenshtein distance, in order to reward higher-order n-gram (n > 1) and partial word overlaps, respectively.",
                "Furthermore, we introduce a recall-based MT feature that takes multiple MT hypotheses (n-best translations) of a given source segment into account, based on the assumption that alternative translations of words (such as \"buy\" and \"purchase\") or phrases (such as \"despite\" and \"in spite of\") should not be punished.",
                "We also experiment with part-of-speech information to identify spurious translation units.",
                "With closely related languages in particular, the rationale would be that adjectives (to name an example) in a source segment are likely to be reflected in the corresponding target segment in case of a valid translation."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: As outlined above, comparing machine translated source segments to their actual target segments has proven effective in #TARGET_REF experiments.\n sent1: We propose to complement or replace the similarity function used for this comparison (cosine similarity) by two automatic MT evaluation metrics, Bleu (#REF) and characterbased Levenshtein distance, in order to reward higher-order n-gram (n > 1) and partial word overlaps, respectively.\n sent2: Furthermore, we introduce a recall-based MT feature that takes multiple MT hypotheses (n-best translations) of a given source segment into account, based on the assumption that alternative translations of words (such as \"buy\" and \"purchase\") or phrases (such as \"despite\" and \"in spite of\") should not be punished.\n sent3: We also experiment with part-of-speech information to identify spurious translation units.\n sent4: With closely related languages in particular, the rationale would be that adjectives (to name an example) in a source segment are likely to be reflected in the corresponding target segment in case of a valid translation.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "First, for each record of the i th row and j th column in the table, we utilize 1-layer MLP to encode the embeddings of each record's four types of information into a dense vector r i,j , r i,j = ReLU (W a [r i,j .e; r i,j .c; r i,j .v; r i,j .f ] + b a ).",
                "W a and b a are trainable parameters.",
                "The word embeddings for each type of information are trainable and randomly initialized before training following #TARGET_REF .",
                "[; ] denotes the vector concatenation.",
                "Then, we use a LSTM decoder with attention and conditional copy to model the conditional probability P (y t |y <t , S)."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: First, for each record of the i th row and j th column in the table, we utilize 1-layer MLP to encode the embeddings of each record's four types of information into a dense vector r i,j , r i,j = ReLU (W a [r i,j .e; r i,j .c; r i,j .v; r i,j .f ] + b a ).\n sent1: W a and b a are trainable parameters.\n sent2: The word embeddings for each type of information are trainable and randomly initialized before training following #TARGET_REF .\n sent3: [; ] denotes the vector concatenation.\n sent4: Then, we use a LSTM decoder with attention and conditional copy to model the conditional probability P (y t |y <t , S).\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The one with LSTM cell is similar to the one in #REF with 1 layer from {1,2,3}. The one with CNN cell (#REF) has kernel width 3 from {3, 5} and 10 layer from {5,10,15,20}. The one with transformer-style encoder (MHSA) (#REF) has 8 head from {8, 10} and 5 layer from {2,3,4,5,6}. The heads and layers mentioned above were for both record-level encoder and rowlevel encoder respectively.",
                "The self-attention (SA) cell we used, as described in Section 3, achieved better overall performance in terms of F1% of CS, CO and BLEU among the hierarchical encoders.",
                "Also we implemented a template system same as the one used in #TARGET_REF which outputted eight sentences: an introductory sentence (two teams' points and who win), six top players' statistics (ranked by their points) and a conclusion sentence.",
                "We refer the readers to #REF 's paper for more detailed information on templates.",
                "The gold reference's result is also included in Table 1 ."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: The one with LSTM cell is similar to the one in #REF with 1 layer from {1,2,3}. The one with CNN cell (#REF) has kernel width 3 from {3, 5} and 10 layer from {5,10,15,20}. The one with transformer-style encoder (MHSA) (#REF) has 8 head from {8, 10} and 5 layer from {2,3,4,5,6}. The heads and layers mentioned above were for both record-level encoder and rowlevel encoder respectively.\n sent1: The self-attention (SA) cell we used, as described in Section 3, achieved better overall performance in terms of F1% of CS, CO and BLEU among the hierarchical encoders.\n sent2: Also we implemented a template system same as the one used in #TARGET_REF which outputted eight sentences: an introductory sentence (two teams' points and who win), six top players' statistics (ranked by their points) and a conclusion sentence.\n sent3: We refer the readers to #REF 's paper for more detailed information on templates.\n sent4: The gold reference's result is also included in Table 1 .\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We conducted experiments on ROTOWIRE #TARGET_REF .",
                "For each example, it provides three tables as described in Section 2.1 which consists of 628 records in total with a long game summary.",
                "The average length of game summary is 337.1.",
                "In this paper, we followed the data split introduced in #REF"
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We conducted experiments on ROTOWIRE #TARGET_REF .\n sent1: For each example, it provides three tables as described in Section 2.1 which consists of 628 records in total with a long game summary.\n sent2: The average length of game summary is 337.1.\n sent3: In this paper, we followed the data split introduced in #REF\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We conducted experiments on ROTOWIRE (#REF) .",
                "For each example, it provides three tables as described in Section 2.1 which consists of 628 records in total with a long game summary.",
                "The average length of game summary is 337.1.",
                "In this paper, we followed the data split introduced in #TARGET_REF"
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We conducted experiments on ROTOWIRE (#REF) .\n sent1: For each example, it provides three tables as described in Section 2.1 which consists of 628 records in total with a long game summary.\n sent2: The average length of game summary is 337.1.\n sent3: In this paper, we followed the data split introduced in #TARGET_REF\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Code of our model can be found at https://github.com/ernestgong/data2text-three-dimensions/. Table 1 displays the automatic evaluation results on both development and test set.",
                "We chose Conditional Copy (CC) model as our baseline, which is the best model in #TARGET_REF .",
                "We included reported scores with updated IE model by #REF and our implementation's result on CC in this paper.",
                "Also, we compared our models with other existing works on this dataset including OpATT (#REF) and Neural Content Planning with conditional copy (NCP+CC) (#REF) .",
                "In addition, we implemented three other hierarchical encoders that encoded tables' row dimension information in both record-level and row-level to compare with the hierarchical structure of encoder in our model."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Code of our model can be found at https://github.com/ernestgong/data2text-three-dimensions/. Table 1 displays the automatic evaluation results on both development and test set.\n sent1: We chose Conditional Copy (CC) model as our baseline, which is the best model in #TARGET_REF .\n sent2: We included reported scores with updated IE model by #REF and our implementation's result on CC in this paper.\n sent3: Also, we compared our models with other existing works on this dataset including OpATT (#REF) and Neural Content Planning with conditional copy (NCP+CC) (#REF) .\n sent4: In addition, we implemented three other hierarchical encoders that encoded tables' row dimension information in both record-level and row-level to compare with the hierarchical structure of encoder in our model.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Row, column and time dimension information are important to the modeling of tables because subtracting any of them will result in performance Table 2 : Automatic evaluation results on test set.",
                "Results were obtained using #TARGET_REF 's trained extractive evaluation models with relexicalization (#REF) .",
                "* We include delayed copy (DEL)'s result in the paper (#REF) for comparison.",
                "drop.",
                "Also, position embedding is critical when modeling time dimension information according to the results."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Row, column and time dimension information are important to the modeling of tables because subtracting any of them will result in performance Table 2 : Automatic evaluation results on test set.\n sent1: Results were obtained using #TARGET_REF 's trained extractive evaluation models with relexicalization (#REF) .\n sent2: * We include delayed copy (DEL)'s result in the paper (#REF) for comparison.\n sent3: drop.\n sent4: Also, position embedding is critical when modeling time dimension information according to the results.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "In addition, we compare our model with delayed copy model (DEL) (#REF) along with gold text, template system (TEM), conditional copy (CC) (#REF) and NCP+CC (NCP) (#REF) .",
                "#REF 's model generate a template at first and then fill in the slots with delayed copy mechanism.",
                "Since its result in #REF 's paper was evaluated by IE model trained by #TARGET_REF and \"relexicalization\" by #REF , we adopted the corresponding IE model and re-implement \"relexicalization\" as suggested by #REF for fair comparison.",
                "Please note that CC's evaluation results via our reimplemented \"relexicalization\" is comparable to the reported result in #REF .",
                "We applied them on models other than DEL as shown in Table 2 and report DEL's result from (#REF) 's paper."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In addition, we compare our model with delayed copy model (DEL) (#REF) along with gold text, template system (TEM), conditional copy (CC) (#REF) and NCP+CC (NCP) (#REF) .\n sent1: #REF 's model generate a template at first and then fill in the slots with delayed copy mechanism.\n sent2: Since its result in #REF 's paper was evaluated by IE model trained by #TARGET_REF and \"relexicalization\" by #REF , we adopted the corresponding IE model and re-implement \"relexicalization\" as suggested by #REF for fair comparison.\n sent3: Please note that CC's evaluation results via our reimplemented \"relexicalization\" is comparable to the reported result in #REF .\n sent4: We applied them on models other than DEL as shown in Table 2 and report DEL's result from (#REF) 's paper.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "This indicates that cell and time dimension representation are important in representing the tables.",
                "Compared to reported baseline result in #TARGET_REF , we achieved improvement of 22.27% in terms of RG, 26.84% in terms of CS F1%, 35.28% in terms of CO and 18.75% in terms of BLEU on test set.",
                "Unsurprisingly, template system achieves best on RG P% and CS R% due to the included domain knowledge.",
                "Also, the high RG # and low CS P% indicates that template will include vast information while many of them are deemed redundant.",
                "In addition, the low CO and low BLEU indicates that the rigid structure of the template will produce texts that aren't as adaptive to the given tables and natural as those produced by neural models."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: This indicates that cell and time dimension representation are important in representing the tables.\n sent1: Compared to reported baseline result in #TARGET_REF , we achieved improvement of 22.27% in terms of RG, 26.84% in terms of CS F1%, 35.28% in terms of CO and 18.75% in terms of BLEU on test set.\n sent2: Unsurprisingly, template system achieves best on RG P% and CS R% due to the included domain knowledge.\n sent3: Also, the high RG # and low CS P% indicates that template will include vast information while many of them are deemed redundant.\n sent4: In addition, the low CO and low BLEU indicates that the rigid structure of the template will produce texts that aren't as adaptive to the given tables and natural as those produced by neural models.\n",
        "output": "{\"label\": [null], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "For Chinese, treebanks have been made available under various segmentation granularities (#REF; #REF; #REF) .",
                "These give rise to the research problem * Work done when the first author was visiting SUTD.",
                "of effectively making use of multiple treebanks under heterogeneous annotations for improving output accuracies (#REF; #REF; #TARGET_REF .",
                "The task has been tackled using two typical approaches.",
                "The first is based on stacking (#REF; #REF; #REF) ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: For Chinese, treebanks have been made available under various segmentation granularities (#REF; #REF; #REF) .\n sent1: These give rise to the research problem * Work done when the first author was visiting SUTD.\n sent2: of effectively making use of multiple treebanks under heterogeneous annotations for improving output accuracies (#REF; #REF; #TARGET_REF .\n sent3: The task has been tackled using two typical approaches.\n sent4: The first is based on stacking (#REF; #REF; #REF) .\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The task has been tackled using two typical approaches.",
                "The first is based on stacking (#REF; #REF; #REF) .",
                "As shown in Figure 1 (a), the main idea is to have a model trained using a source treebank, which is then used to guide a target treebank model by offering source-style features.",
                "This method has been used for leveraging two different treebanks for word segmentation (#REF; #REF) and dependency parsing (#REF; #REF) .",
                "The second approach is based on multi-view learning (#REF; #TARGET_REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The task has been tackled using two typical approaches.\n sent1: The first is based on stacking (#REF; #REF; #REF) .\n sent2: As shown in Figure 1 (a), the main idea is to have a model trained using a source treebank, which is then used to guide a target treebank model by offering source-style features.\n sent3: This method has been used for leveraging two different treebanks for word segmentation (#REF; #REF) and dependency parsing (#REF; #REF) .\n sent4: The second approach is based on multi-view learning (#REF; #TARGET_REF .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "It remains an interesting research question how effective multiple treebanks can be utilized by neural NLP models, and we aim to investigate this empirically.",
                "We follow #TARGET_REF , taking POS-tagging for case study, using the methods of #REF and #TARGET_REF as the discrete stacking and multi-view training baselines, respectively, and building neural network counterparts to their models for empirical comparison.",
                "The base tagger is a neural CRF model #REF) , which gives competitive accuracies to discrete CRF taggers.",
                "Results show that neural stacking allows deeper integration of the source model beyond one-best outputs, and further the fine-tuning of the source model during the target model training.",
                "In addition, the advantage of neural multi-view learning over its discrete counterpart are many-fold."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: It remains an interesting research question how effective multiple treebanks can be utilized by neural NLP models, and we aim to investigate this empirically.\n sent1: We follow #TARGET_REF , taking POS-tagging for case study, using the methods of #REF and #TARGET_REF as the discrete stacking and multi-view training baselines, respectively, and building neural network counterparts to their models for empirical comparison.\n sent2: The base tagger is a neural CRF model #REF) , which gives competitive accuracies to discrete CRF taggers.\n sent3: Results show that neural stacking allows deeper integration of the source model beyond one-best outputs, and further the fine-tuning of the source model during the target model training.\n sent4: In addition, the advantage of neural multi-view learning over its discrete counterpart are many-fold.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "As shown in Figure 1(b) , multi-view learning #TARGET_REF utilizes corpus A and corpus B simultaneously for training.",
                "The coupled tagger directly learns the logistic correspondences between both corpora, therefore can lead a more comprehensive usage of corpus A compared with stacking.",
                "In order to better capture such correlation, specifically designed feature templates between two tag sets are essential.",
                "For each training instances, both A and B labels are needed.",
                "However, one type of tag is missing."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: As shown in Figure 1(b) , multi-view learning #TARGET_REF utilizes corpus A and corpus B simultaneously for training.\n sent1: The coupled tagger directly learns the logistic correspondences between both corpora, therefore can lead a more comprehensive usage of corpus A compared with stacking.\n sent2: In order to better capture such correlation, specifically designed feature templates between two tag sets are essential.\n sent3: For each training instances, both A and B labels are needed.\n sent4: However, one type of tag is missing.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Input: Two training datasets: where ⃗ y is the model output, s(⃗ y|⃗ x) = logP (⃗ y|⃗ x) is the log probability of ⃗ y and δ(⃗ y, ⃗ y d ) is the Hamming distance between ⃗ y and ⃗ y d .",
                "We adopt online learning, updating parameters using AdaGrad (#REF) .",
                "To train the neural stacking model, we first train a base tagger using corpus A. Then, we train the stacked tagger with corpus B, where the parameters of the A tagger has been pretrained from corpus A and the B tagger is randomly initialized.",
                "For neural multi-view model, we follow #TARGET_REF and take a the corpus-weighting strategy to sample a number of training instances from both corpora for each training iteration, as shown in Algorithm 1.",
                "At each epoch, we randomly sample from the two datasets according to a corpus weights ratio, namely the ratio between the number of sentences in each dataset used for training, to form a training set for the epoch."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Input: Two training datasets: where ⃗ y is the model output, s(⃗ y|⃗ x) = logP (⃗ y|⃗ x) is the log probability of ⃗ y and δ(⃗ y, ⃗ y d ) is the Hamming distance between ⃗ y and ⃗ y d .\n sent1: We adopt online learning, updating parameters using AdaGrad (#REF) .\n sent2: To train the neural stacking model, we first train a base tagger using corpus A. Then, we train the stacked tagger with corpus B, where the parameters of the A tagger has been pretrained from corpus A and the B tagger is randomly initialized.\n sent3: For neural multi-view model, we follow #TARGET_REF and take a the corpus-weighting strategy to sample a number of training instances from both corpora for each training iteration, as shown in Algorithm 1.\n sent4: At each epoch, we randomly sample from the two datasets according to a corpus weights ratio, namely the ratio between the number of sentences in each dataset used for training, to form a training set for the epoch.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "We adopt the Penn Chinese Treebank version 5.0 (CTB5) (#REF) as our main corpus, with the standard data split following previous work (#REF; #TARGET_REF .",
                "People's Daily (PD) is used as second corpus with a different scheme.",
                "We filter out PD sentences longer than 200 words.",
                "Details of the datasets are listed in Table  1 .",
                "The standard token-wise POS tagging accuracy is used as the evaluation metric."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We adopt the Penn Chinese Treebank version 5.0 (CTB5) (#REF) as our main corpus, with the standard data split following previous work (#REF; #TARGET_REF .\n sent1: People's Daily (PD) is used as second corpus with a different scheme.\n sent2: We filter out PD sentences longer than 200 words.\n sent3: Details of the datasets are listed in Table  1 .\n sent4: The standard token-wise POS tagging accuracy is used as the evaluation metric.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "This indicates that the NN models can overfit the training data without dropout.",
                "However, when a 50% dropout rate is used, the initial performances are significantly worse, which implies that the 50% dropout rate can be too large and leads to underfitting.",
                "As a result, we choose a dropout rate of 20% for the remaining experiments, which strikes the balance between over-System Accuracy CRF Baseline #TARGET_REF 94.10 CRF Stacking #TARGET_REF 94.81 CRF Multi-view #TARGET_REF 95 Table 2 : Accuracies on CTB-test.",
                "fitting and underfitting.",
                "Figure 5 also shows that the batch size has a relative small influence on the accuracies, which varies according to the dropout rate."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This indicates that the NN models can overfit the training data without dropout.\n sent1: However, when a 50% dropout rate is used, the initial performances are significantly worse, which implies that the 50% dropout rate can be too large and leads to underfitting.\n sent2: As a result, we choose a dropout rate of 20% for the remaining experiments, which strikes the balance between over-System Accuracy CRF Baseline #TARGET_REF 94.10 CRF Stacking #TARGET_REF 94.81 CRF Multi-view #TARGET_REF 95 Table 2 : Accuracies on CTB-test.\n sent3: fitting and underfitting.\n sent4: Figure 5 also shows that the batch size has a relative small influence on the accuracies, which varies according to the dropout rate.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The ratios of 1:1 and 1:4 give comparable performances.",
                "We choose the former for our final tests because it is a much faster choice.",
                "Table 2 shows the final results on the CTB test data.",
                "We lists the results of stacking method of #REF re-implemented by #TARGET_REF , and CRF multi-view method reported by #TARGET_REF .",
                "We adopt pair-wise significance test (#REF) when comparing the results between two different models."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The ratios of 1:1 and 1:4 give comparable performances.\n sent1: We choose the former for our final tests because it is a much faster choice.\n sent2: Table 2 shows the final results on the CTB test data.\n sent3: We lists the results of stacking method of #REF re-implemented by #TARGET_REF , and CRF multi-view method reported by #TARGET_REF .\n sent4: We adopt pair-wise significance test (#REF) when comparing the results between two different models.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "We compare the efficiencies of neural and discrete multi-view training by running our models and the model of #TARGET_REF 4 with default configurations on the CTB5 training data.",
                "The CRF baseline is adapted from #REF .",
                "All the systems are implemented in C++ running on an Intel E5-1620 CPU.",
                "The results are shown in Table 3 .",
                "The NN baseline model is slower than the CRF baseline model."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We compare the efficiencies of neural and discrete multi-view training by running our models and the model of #TARGET_REF 4 with default configurations on the CTB5 training data.\n sent1: The CRF baseline is adapted from #REF .\n sent2: All the systems are implemented in C++ running on an Intel E5-1620 CPU.\n sent3: The results are shown in Table 3 .\n sent4: The NN baseline model is slower than the CRF baseline model.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "The base tagger is a neural CRF model #REF) , which gives competitive accuracies to discrete CRF taggers.",
                "Results show that neural stacking allows deeper integration of the source model beyond one-best outputs, and further the fine-tuning of the source model during the target model training.",
                "In addition, the advantage of neural multi-view learning over its discrete counterpart are many-fold.",
                "First, it is free from the necessity of manual cross-labelset interactive feature engineering, which is far from trivial for representing annotation correspondence #TARGET_REF .",
                "Second, compared to discrete model, parameter sharing in deep neural network eliminates the issue of exponential growth of search space, and allows separated training of each label type, in the same way as multi-task learning (#REF) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The base tagger is a neural CRF model #REF) , which gives competitive accuracies to discrete CRF taggers.\n sent1: Results show that neural stacking allows deeper integration of the source model beyond one-best outputs, and further the fine-tuning of the source model during the target model training.\n sent2: In addition, the advantage of neural multi-view learning over its discrete counterpart are many-fold.\n sent3: First, it is free from the necessity of manual cross-labelset interactive feature engineering, which is far from trivial for representing annotation correspondence #TARGET_REF .\n sent4: Second, compared to discrete model, parameter sharing in deep neural network eliminates the issue of exponential growth of search space, and allows separated training of each label type, in the same way as multi-task learning (#REF) .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "This shows that neural stacking is a preferred choice for stacking.",
                "Multi-view training.",
                "With respect of the multiview training method, the NN model improves over the NN baseline from 94.24 to 95.40, by a margin of +1.16, which is higher than that of 0.90 brought by discrete method of #TARGET_REF over its baseline, from 94.10 to 95.00.",
                "NN multi-view training method gives relatively higher improvements compared with NN stacking method.",
                "This is consistent with the observation of #REF , who showed that discrete label coupling training gives slightly better improvement compared with discrete stacking."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: This shows that neural stacking is a preferred choice for stacking.\n sent1: Multi-view training.\n sent2: With respect of the multiview training method, the NN model improves over the NN baseline from 94.24 to 95.40, by a margin of +1.16, which is higher than that of 0.90 brought by discrete method of #TARGET_REF over its baseline, from 94.10 to 95.00.\n sent3: NN multi-view training method gives relatively higher improvements compared with NN stacking method.\n sent4: This is consistent with the observation of #REF , who showed that discrete label coupling training gives slightly better improvement compared with discrete stacking.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "With respect of the multiview training method, the NN model improves over the NN baseline from 94.24 to 95.40, by a margin of +1.16, which is higher than that of 0.90 brought by discrete method of #REF over its baseline, from 94.10 to 95.00.",
                "NN multi-view training method gives relatively higher improvements compared with NN stacking method.",
                "This is consistent with the observation of #TARGET_REF , who showed that discrete label coupling training gives slightly better improvement compared with discrete stacking.",
                "The final accuracies of NN multi-view training is also higher than that of its CRF counterpart, namely 95.00 vs 95.40 at the confidence level p < 10 −3 .",
                "The difference between the final NN multi-view training result of 95.40 and the final NN stacking results is not significant."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: With respect of the multiview training method, the NN model improves over the NN baseline from 94.24 to 95.40, by a margin of +1.16, which is higher than that of 0.90 brought by discrete method of #REF over its baseline, from 94.10 to 95.00.\n sent1: NN multi-view training method gives relatively higher improvements compared with NN stacking method.\n sent2: This is consistent with the observation of #TARGET_REF , who showed that discrete label coupling training gives slightly better improvement compared with discrete stacking.\n sent3: The final accuracies of NN multi-view training is also higher than that of its CRF counterpart, namely 95.00 vs 95.40 at the confidence level p < 10 −3 .\n sent4: The difference between the final NN multi-view training result of 95.40 and the final NN stacking results is not significant.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We compare the efficiencies of neural and discrete multi-view training by running our models and the model of #REF 4 with default configurations on the CTB5 training data.",
                "The CRF baseline is adapted from #TARGET_REF .",
                "All the systems are implemented in C++ running on an Intel E5-1620 CPU.",
                "The results are shown in Table 3 .",
                "The NN baseline model is slower than the CRF baseline model."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: We compare the efficiencies of neural and discrete multi-view training by running our models and the model of #REF 4 with default configurations on the CTB5 training data.\n sent1: The CRF baseline is adapted from #TARGET_REF .\n sent2: All the systems are implemented in C++ running on an Intel E5-1620 CPU.\n sent3: The results are shown in Table 3 .\n sent4: The NN baseline model is slower than the CRF baseline model.\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Recent work has argued instead for image paragraph captioning with the aim of generating a (usually 5-8 sentence) paragraph describing an image.",
                "Compared with single-sentence captioning, paragraph captioning is a relatively new task.",
                "The main paragraph captioning dataset is the Visual Genome corpus, introduced by #TARGET_REF .",
                "When strong single-sentence captioning models are trained on this dataset, they produce repetitive paragraphs that are unable to describe diverse aspects of images.",
                "The generated paragraphs repeat a slight variant of the same sentence multiple times, even when beam search is used."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Recent work has argued instead for image paragraph captioning with the aim of generating a (usually 5-8 sentence) paragraph describing an image.\n sent1: Compared with single-sentence captioning, paragraph captioning is a relatively new task.\n sent2: The main paragraph captioning dataset is the Visual Genome corpus, introduced by #TARGET_REF .\n sent3: When strong single-sentence captioning models are trained on this dataset, they produce repetitive paragraphs that are unable to describe diverse aspects of images.\n sent4: The generated paragraphs repeat a slight variant of the same sentence multiple times, even when beam search is used.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "CIDEr and BLEU measure accuracy with n-gram overlaps, with CIDEr weighting n-grams by TF-IDF (termfrequency inverse-document-frequency), and ME-TEOR uses unigram overlap, incorporating synonym and paraphrase matches.",
                "We discuss these metrics in greater detail when analyzing our experiments.",
                "#TARGET_REF introduced the first large-scale paragraph captioning dataset, a subset of the Visual Genome dataset, along with a number of models for paragraph captioning.",
                "Empirically, they showed that paragraphs contain significantly more pronouns, verbs, coreferences, and greater overall \"diversity\" than singlesentence captions.",
                "Whereas most single-sentence captions in the MSCOCO dataset describe only the most important object or action in an image, paragraph captions usually touch on multiple objects and actions."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: CIDEr and BLEU measure accuracy with n-gram overlaps, with CIDEr weighting n-grams by TF-IDF (termfrequency inverse-document-frequency), and ME-TEOR uses unigram overlap, incorporating synonym and paraphrase matches.\n sent1: We discuss these metrics in greater detail when analyzing our experiments.\n sent2: #TARGET_REF introduced the first large-scale paragraph captioning dataset, a subset of the Visual Genome dataset, along with a number of models for paragraph captioning.\n sent3: Empirically, they showed that paragraphs contain significantly more pronouns, verbs, coreferences, and greater overall \"diversity\" than singlesentence captions.\n sent4: Whereas most single-sentence captions in the MSCOCO dataset describe only the most important object or action in an image, paragraph captions usually touch on multiple objects and actions.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The paragraph captioning models proposed by #TARGET_REF included template-based (nonneural) approaches and two encoder-decoder models.",
                "In both neural models, the encoder is an object detector pre-trained for dense captioning.",
                "In the first model, called the flat model, the decoder is a single LSTM which outputs an entire paragraph word-by-word.",
                "In the second model, called the hierarchical model, the decoder is composed of two LSTMs, where the output of one sentencelevel LSTM is used as input to the other word-level LSTM.",
                "Recently, #REF extended this model with a third (paragraph-level) LSTM and added adversarial training."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The paragraph captioning models proposed by #TARGET_REF included template-based (nonneural) approaches and two encoder-decoder models.\n sent1: In both neural models, the encoder is an object detector pre-trained for dense captioning.\n sent2: In the first model, called the flat model, the decoder is a single LSTM which outputs an entire paragraph word-by-word.\n sent3: In the second model, called the hierarchical model, the decoder is composed of two LSTMs, where the output of one sentencelevel LSTM is used as input to the other word-level LSTM.\n sent4: Recently, #REF extended this model with a third (paragraph-level) LSTM and added adversarial training.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Recently, #REF extended this model with a third (paragraph-level) LSTM and added adversarial training.",
                "In total, their model (RTT-GAN) incorporates three LSTMs, two attention mechanisms, a phrase copy mechanism, and two adversarial discriminators.",
                "To the best of our knowledge, this model achieves state-ofthe-art performance of 16.9 CIDEr on the Visual Genome dataset (without external data).",
                "For our experiments, we use the top-down single-sentence captioning model in #REF .",
                "This model is similar to the \"flat\" model in #TARGET_REF , except that it incorporates attention with a top-down mechanism."
            ],
            "label": [
                "BACKGROUND",
                "SIMILARITY"
            ]
        },
        "input": "sent0: Recently, #REF extended this model with a third (paragraph-level) LSTM and added adversarial training.\n sent1: In total, their model (RTT-GAN) incorporates three LSTMs, two attention mechanisms, a phrase copy mechanism, and two adversarial discriminators.\n sent2: To the best of our knowledge, this model achieves state-ofthe-art performance of 16.9 CIDEr on the Visual Genome dataset (without external data).\n sent3: For our experiments, we use the top-down single-sentence captioning model in #REF .\n sent4: This model is similar to the \"flat\" model in #TARGET_REF , except that it incorporates attention with a top-down mechanism.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"SIMILARITY\"], \"context\": [\"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "For our paragraph captioning model we use the top-down model from #REF .",
                "Our encoder is a convolutional network pretrained for object detection (as opposed to dense captioning, as in #TARGET_REF and #REF ).",
                "METEOR CIDEr BLEU-1 BLEU-2 BLEU-3 BLEU-4 Krause et al. (Template) The encoder extracts between 10 and 100 objects per image and applies spatial max-pooling to yield a single feature vector of dimension 2048 per object.",
                "The decoder is a 1-layer LSTM with hidden dimension 512 and top-down attention.",
                "Evaluation is done on the Visual Genome dataset with the splits provided by #REF ."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: For our paragraph captioning model we use the top-down model from #REF .\n sent1: Our encoder is a convolutional network pretrained for object detection (as opposed to dense captioning, as in #TARGET_REF and #REF ).\n sent2: METEOR CIDEr BLEU-1 BLEU-2 BLEU-3 BLEU-4 Krause et al. (Template) The encoder extracts between 10 and 100 objects per image and applies spatial max-pooling to yield a single feature vector of dimension 2048 per object.\n sent3: The decoder is a 1-layer LSTM with hidden dimension 512 and top-down attention.\n sent4: Evaluation is done on the Visual Genome dataset with the splits provided by #REF .\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "METEOR CIDEr BLEU-1 BLEU-2 BLEU-3 BLEU-4 Krause et al. (Template) The encoder extracts between 10 and 100 objects per image and applies spatial max-pooling to yield a single feature vector of dimension 2048 per object.",
                "The decoder is a 1-layer LSTM with hidden dimension 512 and top-down attention.",
                "Evaluation is done on the Visual Genome dataset with the splits provided by #TARGET_REF .",
                "We first train for 25 epochs with crossentropy (XE) loss, using Adam with learning rate 5 · 10 −4 .",
                "We then train an additional 25 epochs with repetition-penalized SCST targeting a CIDEr-based reward, using Adam with learning rate 5 · 10 −5 ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: METEOR CIDEr BLEU-1 BLEU-2 BLEU-3 BLEU-4 Krause et al. (Template) The encoder extracts between 10 and 100 objects per image and applies spatial max-pooling to yield a single feature vector of dimension 2048 per object.\n sent1: The decoder is a 1-layer LSTM with hidden dimension 512 and top-down attention.\n sent2: Evaluation is done on the Visual Genome dataset with the splits provided by #TARGET_REF .\n sent3: We first train for 25 epochs with crossentropy (XE) loss, using Adam with learning rate 5 · 10 −4 .\n sent4: We then train an additional 25 epochs with repetition-penalized SCST targeting a CIDEr-based reward, using Adam with learning rate 5 · 10 −5 .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "After analyzing the alignment matrices generated by GroundHog #TARGET_REF , we find that modeling the structural divergence of natural languages is so challenging that unidirectional models can only capture part of alignment regularities.",
                "This finding inspires us to improve attention-based NMT by combining two unidirectional models.",
                "In this work, we only apply agreement-based joint learning to GroundHog.",
                "As our approach does not assume specific network architectures, it is possible to apply it to the models proposed by Luong et al. [2015] ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: After analyzing the alignment matrices generated by GroundHog #TARGET_REF , we find that modeling the structural divergence of natural languages is so challenging that unidirectional models can only capture part of alignment regularities.\n sent1: This finding inspires us to improve attention-based NMT by combining two unidirectional models.\n sent2: In this work, we only apply agreement-based joint learning to GroundHog.\n sent3: As our approach does not assume specific network architectures, it is possible to apply it to the models proposed by Luong et al. [2015] .\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Efforts in agent-human negotiations involving free-form natural language as a means of communication are rather sparse.",
                "Researchers #TARGET_REF recently studied natural language negotiations in buyer-seller bargaining setup, which is comparatively less restricted than previously studied game environments (#REF; #REF) .",
                "Lack of a well-defined structure in such negotiations allows humans or agents to express themselves more freely, which better emulates a realistic scenario.",
                "Interestingly, this also provides an exciting research opportunity: how can an agent leverage the behavioral cues in natural language to direct its negotiation strategies? Understanding the impact of natural language on negotiation outcomes through a data-driven neural framework is the primary objective of this work.",
                "We focus on buyer-seller negotiations (#REF) where two individuals negotiate the price of a given product."
            ],
            "label": [
                "BACKGROUND",
                "MOTIVATION"
            ]
        },
        "input": "sent0: Efforts in agent-human negotiations involving free-form natural language as a means of communication are rather sparse.\n sent1: Researchers #TARGET_REF recently studied natural language negotiations in buyer-seller bargaining setup, which is comparatively less restricted than previously studied game environments (#REF; #REF) .\n sent2: Lack of a well-defined structure in such negotiations allows humans or agents to express themselves more freely, which better emulates a realistic scenario.\n sent3: Interestingly, this also provides an exciting research opportunity: how can an agent leverage the behavioral cues in natural language to direct its negotiation strategies? Understanding the impact of natural language on negotiation outcomes through a data-driven neural framework is the primary objective of this work.\n sent4: We focus on buyer-seller negotiations (#REF) where two individuals negotiate the price of a given product.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"MOTIVATION\"], \"context\": [\"sent1\", \"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "This number is lower than average for Housing, Bike and Car, resulting in relative better performance of Priceonly model for these categories over others.",
                "The models also show evidence of capturing buyer interest.",
                "By constructing artificial negotiations, we observe that the model predictions at f =0.2 increase when the buyer shows more interest in the product, indicating more willingness to pay.",
                "With the capability to incorporate cues from natural language, such a framework can be used in the future to get negotiation feedback, in order to guide the planning of a negotiating agent.",
                "This can be a viable middleground between following the average human behavior through supervised learning or exploring the wild by optimizing on rewards using reinforcement learning (#REF; #TARGET_REF ."
            ],
            "label": [
                "BACKGROUND",
                null
            ]
        },
        "input": "sent0: This number is lower than average for Housing, Bike and Car, resulting in relative better performance of Priceonly model for these categories over others.\n sent1: The models also show evidence of capturing buyer interest.\n sent2: By constructing artificial negotiations, we observe that the model predictions at f =0.2 increase when the buyer shows more interest in the product, indicating more willingness to pay.\n sent3: With the capability to incorporate cues from natural language, such a framework can be used in the future to get negotiation feedback, in order to guide the planning of a negotiating agent.\n sent4: This can be a viable middleground between following the average human behavior through supervised learning or exploring the wild by optimizing on rewards using reinforcement learning (#REF; #TARGET_REF .\n",
        "output": "{\"label\": [\"BACKGROUND\", null], \"context\": [\"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Lack of a well-defined structure in such negotiations allows humans or agents to express themselves more freely, which better emulates a realistic scenario.",
                "Interestingly, this also provides an exciting research opportunity: how can an agent leverage the behavioral cues in natural language to direct its negotiation strategies? Understanding the impact of natural language on negotiation outcomes through a data-driven neural framework is the primary objective of this work.",
                "We focus on buyer-seller negotiations #TARGET_REF where two individuals negotiate the price of a given product.",
                "Leveraging the recent advancements (#REF; #REF) in pre-trained language encoders, we attempt to predict negotiation outcomes early on in the conversation, in a completely data-driven manner ( Figure  1 ).",
                "Early prediction of outcomes is essential for effective planning of an automatically negotiating agent."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Lack of a well-defined structure in such negotiations allows humans or agents to express themselves more freely, which better emulates a realistic scenario.\n sent1: Interestingly, this also provides an exciting research opportunity: how can an agent leverage the behavioral cues in natural language to direct its negotiation strategies? Understanding the impact of natural language on negotiation outcomes through a data-driven neural framework is the primary objective of this work.\n sent2: We focus on buyer-seller negotiations #TARGET_REF where two individuals negotiate the price of a given product.\n sent3: Leveraging the recent advancements (#REF; #REF) in pre-trained language encoders, we attempt to predict negotiation outcomes early on in the conversation, in a completely data-driven manner ( Figure  1 ).\n sent4: Early prediction of outcomes is essential for effective planning of an automatically negotiating agent.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Early prediction of outcomes is essential for effective planning of an automatically negotiating agent.",
                "Although there have been attempts to gain insights into negotiations (#REF; #REF) , to the best of our knowledge, we are the first to study early natural language cues through a datadriven neural system (Section 3).",
                "Our evaluations show that natural language allows the models to make better predictions by looking at only a fraction of the negotiation.",
                "Rather than just realizing the strategy in natural language, our empirical results suggest that language can be crucial in the planning as well.",
                "We provide a sample negotiation from the test set #TARGET_REF ) along with our model predictions in Table 1 ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Early prediction of outcomes is essential for effective planning of an automatically negotiating agent.\n sent1: Although there have been attempts to gain insights into negotiations (#REF; #REF) , to the best of our knowledge, we are the first to study early natural language cues through a datadriven neural system (Section 3).\n sent2: Our evaluations show that natural language allows the models to make better predictions by looking at only a fraction of the negotiation.\n sent3: Rather than just realizing the strategy in natural language, our empirical results suggest that language can be crucial in the planning as well.\n sent4: We provide a sample negotiation from the test set #TARGET_REF ) along with our model predictions in Table 1 .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "We study human-human negotiations in the buyerseller bargaining scenario, which has been a key research area in the literature (#REF) .",
                "In this section, we first describe our problem setup and key terminologies by discussing the dataset used.",
                "Later, we formalize our problem definition.",
                "Dataset: For our explorations, we use the Craigslist Bargaining dataset (CB) introduced by #TARGET_REF .",
                "Instead of focusing on the previously studied game environments (#REF; #REF) , the dataset considers a more realistic setup: negotiating the price of products listed on Craigslist 1 ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We study human-human negotiations in the buyerseller bargaining scenario, which has been a key research area in the literature (#REF) .\n sent1: In this section, we first describe our problem setup and key terminologies by discussing the dataset used.\n sent2: Later, we formalize our problem definition.\n sent3: Dataset: For our explorations, we use the Craigslist Bargaining dataset (CB) introduced by #TARGET_REF .\n sent4: Instead of focusing on the previously studied game environments (#REF; #REF) , the dataset considers a more realistic setup: negotiating the price of products listed on Craigslist 1 .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Approaches to lexical simplification generally follow a standard pipeline consisting of two main steps: generation and ranking.",
                "In the generation step, a set of possible substitutions for the target word is commonly created by querying semantic databases such as Wordnet (#REF) , learning substitution rules from sentence-aligned parallel corpora of complex-simple texts (#REF; #TARGET_REF , and learning word embeddings from a large corpora to obtain similar words of the complex word (Glavaš andŠtajner, 2015; #REF; #REFa, 2017) .",
                "In the ranking step, features that discriminate a substitution candidate from other substitution candidates are leveraged and the candidates are ranked with respect to their simplicity and contextual fitness.",
                "* This research was conducted while the first author was a Post Doctoral Fellow at the City University of Hong Kong.",
                "The ranking step is challenging because the substitution candidates usually have similar meaning to the target word, and thus share similar context features."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Approaches to lexical simplification generally follow a standard pipeline consisting of two main steps: generation and ranking.\n sent1: In the generation step, a set of possible substitutions for the target word is commonly created by querying semantic databases such as Wordnet (#REF) , learning substitution rules from sentence-aligned parallel corpora of complex-simple texts (#REF; #TARGET_REF , and learning word embeddings from a large corpora to obtain similar words of the complex word (Glavaš andŠtajner, 2015; #REF; #REFa, 2017) .\n sent2: In the ranking step, features that discriminate a substitution candidate from other substitution candidates are leveraged and the candidates are ranked with respect to their simplicity and contextual fitness.\n sent3: * This research was conducted while the first author was a Post Doctoral Fellow at the City University of Hong Kong.\n sent4: The ranking step is challenging because the substitution candidates usually have similar meaning to the target word, and thus share similar context features.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "For generating substitution candidates, we utilize the method proposed by #REF , which was recently shown to be the state-of-art method for generating substitution candidates.",
                "They exploit a hybrid substitution generation approach where candidates are first extracted from 550,644 simple-complex aligned sentences from the Newsela corpus (#REF) .",
                "Then, these candidates are complemented with candidates generated with a retrofitted word embedding model.",
                "The word embedding model is retrofitted over WordNet's synonym pairs (for details, please refer to #TARGET_REF ).",
                "For ranking substitution candidates, we use a DSSM, which we elaborate in the next section."
            ],
            "label": [
                "BACKGROUND",
                "USE"
            ]
        },
        "input": "sent0: For generating substitution candidates, we utilize the method proposed by #REF , which was recently shown to be the state-of-art method for generating substitution candidates.\n sent1: They exploit a hybrid substitution generation approach where candidates are first extracted from 550,644 simple-complex aligned sentences from the Newsela corpus (#REF) .\n sent2: Then, these candidates are complemented with candidates generated with a retrofitted word embedding model.\n sent3: The word embedding model is retrofitted over WordNet's synonym pairs (for details, please refer to #TARGET_REF ).\n sent4: For ranking substitution candidates, we use a DSSM, which we elaborate in the next section.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "We focus on the ranking step of the standard lexical simplification pipeline.",
                "Given a dataset of tar-get words, their sentential contexts and substitution candidates for the target words, the goal is to train a model that accurately ranks the candidates based on their simplicity and semantic matching.",
                "For generating substitution candidates, we utilize the method proposed by #TARGET_REF , which was recently shown to be the state-of-art method for generating substitution candidates.",
                "They exploit a hybrid substitution generation approach where candidates are first extracted from 550,644 simple-complex aligned sentences from the Newsela corpus (#REF) .",
                "Then, these candidates are complemented with candidates generated with a retrofitted word embedding model."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We focus on the ranking step of the standard lexical simplification pipeline.\n sent1: Given a dataset of tar-get words, their sentential contexts and substitution candidates for the target words, the goal is to train a model that accurately ranks the candidates based on their simplicity and semantic matching.\n sent2: For generating substitution candidates, we utilize the method proposed by #TARGET_REF , which was recently shown to be the state-of-art method for generating substitution candidates.\n sent3: They exploit a hybrid substitution generation approach where candidates are first extracted from 550,644 simple-complex aligned sentences from the Newsela corpus (#REF) .\n sent4: Then, these candidates are complemented with candidates generated with a retrofitted word embedding model.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "As baseline features, we use the same n-gram probability features as in #TARGET_REF , who also employ a neural network to rank substitution candidates.",
                "As in #REF , the features were extracted using the SubIMDB corpus (#REF) .",
                "We also experiment with additional features that have been reported as useful in this task.",
                "For each target word and a substitution candidate word we also compute: cosine similarity, word length, and alignment probability in the sentence-aligned Normal-Simple Wikipedia corpus (#REF) .",
                "The cosine similarity feature is computed using the SubIMDB corpus."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: As baseline features, we use the same n-gram probability features as in #TARGET_REF , who also employ a neural network to rank substitution candidates.\n sent1: As in #REF , the features were extracted using the SubIMDB corpus (#REF) .\n sent2: We also experiment with additional features that have been reported as useful in this task.\n sent3: For each target word and a substitution candidate word we also compute: cosine similarity, word length, and alignment probability in the sentence-aligned Normal-Simple Wikipedia corpus (#REF) .\n sent4: The cosine similarity feature is computed using the SubIMDB corpus.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "As baseline features, we use the same n-gram probability features as in #REF , who also employ a neural network to rank substitution candidates.",
                "As in #TARGET_REF , the features were extracted using the SubIMDB corpus (#REF) .",
                "We also experiment with additional features that have been reported as useful in this task.",
                "For each target word and a substitution candidate word we also compute: cosine similarity, word length, and alignment probability in the sentence-aligned Normal-Simple Wikipedia corpus (#REF) .",
                "The cosine similarity feature is computed using the SubIMDB corpus."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: As baseline features, we use the same n-gram probability features as in #REF , who also employ a neural network to rank substitution candidates.\n sent1: As in #TARGET_REF , the features were extracted using the SubIMDB corpus (#REF) .\n sent2: We also experiment with additional features that have been reported as useful in this task.\n sent3: For each target word and a substitution candidate word we also compute: cosine similarity, word length, and alignment probability in the sentence-aligned Normal-Simple Wikipedia corpus (#REF) .\n sent4: The cosine similarity feature is computed using the SubIMDB corpus.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Following previous works that used supervised machine learning for ranking in lexical simplification (#REF; #TARGET_REF , we train the DSSM using the LexMTurk dataset (#REF) , which contains 500 instances composed of a sentence, a target word and substitution candidates ranked by simplicity #TARGET_REF .",
                "In order to learn the parameters W t and W s (Figure 1 ) of the DSSM, we use the standard backpropagation algorithm (#REF) .",
                "The objective used in this paper follows the pair-wise learning-to-rank paradigm outlined in (#REF) .",
                "Given a target word and its sentential context T , we obtain a list of candidates L. We set different positive values to the candidates based on their simplicity rankings.",
                "E.g., if the list of the candidates is ordered by simplificity as, L = {A + > B + > C + }, the labels are first constructed as L = {y A + = 3, y B + = 2, y C + = 1}. The values are then normalized by dividing by the maximum value in the list: L = {y A + = 1, y B + = 0.667, y C + = 0.333}. If the target word was not originally in L, we add it with label 0."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Following previous works that used supervised machine learning for ranking in lexical simplification (#REF; #TARGET_REF , we train the DSSM using the LexMTurk dataset (#REF) , which contains 500 instances composed of a sentence, a target word and substitution candidates ranked by simplicity #TARGET_REF .\n sent1: In order to learn the parameters W t and W s (Figure 1 ) of the DSSM, we use the standard backpropagation algorithm (#REF) .\n sent2: The objective used in this paper follows the pair-wise learning-to-rank paradigm outlined in (#REF) .\n sent3: Given a target word and its sentential context T , we obtain a list of candidates L. We set different positive values to the candidates based on their simplicity rankings.\n sent4: E.g., if the list of the candidates is ordered by simplificity as, L = {A + > B + > C + }, the labels are first constructed as L = {y A + = 3, y B + = 2, y C + = 1}. The values are then normalized by dividing by the maximum value in the list: L = {y A + = 1, y B + = 0.667, y C + = 0.333}. If the target word was not originally in L, we add it with label 0.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "To evaluate the proposed model, we conduct experiments on two common datasets for lexical simplification: BenchLS (#REFb) , which contains 929 instances, and NNSEval (#REFa) , which contains 239 instances.",
                "Each instance is composed of a sentence, a target word, and a set of gold candidates ranked by simplicity #TARGET_REF .",
                "Since both datasets contain instances from the LexMturk dataset (#REF) , which we use for training the DNN, we remove the overlap instances between training and test datasets 1 .",
                "We finally obtain 429 remaining instances in the BenchLS dataset, and 78 instances in the NNEval dataset, which are used in our evaluation.",
                "We adopt the same evaluation metrics featured in Glavaš andŠtajner (2015) and #REF : 1) precision: ratio of correct simplifications out of all the simplifications made by the system; 2) accuracy: ratio of correct simplifications out of all words that should have been simplified; and 3) changed: ratio of target words changed by the system."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: To evaluate the proposed model, we conduct experiments on two common datasets for lexical simplification: BenchLS (#REFb) , which contains 929 instances, and NNSEval (#REFa) , which contains 239 instances.\n sent1: Each instance is composed of a sentence, a target word, and a set of gold candidates ranked by simplicity #TARGET_REF .\n sent2: Since both datasets contain instances from the LexMturk dataset (#REF) , which we use for training the DNN, we remove the overlap instances between training and test datasets 1 .\n sent3: We finally obtain 429 remaining instances in the BenchLS dataset, and 78 instances in the NNEval dataset, which are used in our evaluation.\n sent4: We adopt the same evaluation metrics featured in Glavaš andŠtajner (2015) and #REF : 1) precision: ratio of correct simplifications out of all the simplifications made by the system; 2) accuracy: ratio of correct simplifications out of all words that should have been simplified; and 3) changed: ratio of target words changed by the system.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We compared the proposed model (DSSM Ranking) to two state-of-the-art approaches to ranking in lexical simplification that exploit supervised machine learning-based methods.",
                "The first baseline is the Neural Substitution Ranking (NSR) approach described in #TARGET_REF , which employs a multi-layer perceptron neural network.",
                "We reimplement their model as part of the LEXenstein toolkit (#REF) .",
                "The network has 3 hidden layers with 8 nodes each.",
                "Unlike the proposed model, they treat ranking in lexical simplification as a standard classification problem."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We compared the proposed model (DSSM Ranking) to two state-of-the-art approaches to ranking in lexical simplification that exploit supervised machine learning-based methods.\n sent1: The first baseline is the Neural Substitution Ranking (NSR) approach described in #TARGET_REF , which employs a multi-layer perceptron neural network.\n sent2: We reimplement their model as part of the LEXenstein toolkit (#REF) .\n sent3: The network has 3 hidden layers with 8 nodes each.\n sent4: Unlike the proposed model, they treat ranking in lexical simplification as a standard classification problem.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "n-gram probs.",
                "denotes the n-gram probability features described in #REF , and all denotes all features described in Section 2.3.",
                "All values marked in bold are significantly higher compared to the best baseline, SVM rank , measured by t-test at p-value of 0.05.",
                "with default parameters) for ranking substitution candidates, similar to the method described in (#REF) .",
                "All the three models employ the n-gram probability features extracted from the SubIMDB corpus (#REF) , as described in #TARGET_REF , and are trained using the LexMTurk dataset."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: n-gram probs.\n sent1: denotes the n-gram probability features described in #REF , and all denotes all features described in Section 2.3.\n sent2: All values marked in bold are significantly higher compared to the best baseline, SVM rank , measured by t-test at p-value of 0.05.\n sent3: with default parameters) for ranking substitution candidates, similar to the method described in (#REF) .\n sent4: All the three models employ the n-gram probability features extracted from the SubIMDB corpus (#REF) , as described in #TARGET_REF , and are trained using the LexMTurk dataset.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "We follow the Unsupervised Boundary Ranking Substitution Selection method described in #TARGET_REF , which ranks candidates according to how well they fit the context of the target word, and discards 50% of the worst ranking candidates.",
                "The bottom part of the table 1 (Selection Step + Substitution Candidates Ranking) summarizes the results of all ranking systems after performing the selection step on generated substitution candidates.",
                "We obtain similar tendency in the results, with the DSSM Ranking outperforming both baselines.",
                "The results indicate the advantage of using a deep architecture, and of building a semantic representation of the whole sentence on top of the characters.",
                "To illustrate by examples, Table 2 lists the top candidate ranked by the systems for different input sentences."
            ],
            "label": [
                "USE",
                "SIMILARITY",
                "DIFFERENCES"
            ]
        },
        "input": "sent0: We follow the Unsupervised Boundary Ranking Substitution Selection method described in #TARGET_REF , which ranks candidates according to how well they fit the context of the target word, and discards 50% of the worst ranking candidates.\n sent1: The bottom part of the table 1 (Selection Step + Substitution Candidates Ranking) summarizes the results of all ranking systems after performing the selection step on generated substitution candidates.\n sent2: We obtain similar tendency in the results, with the DSSM Ranking outperforming both baselines.\n sent3: The results indicate the advantage of using a deep architecture, and of building a semantic representation of the whole sentence on top of the characters.\n sent4: To illustrate by examples, Table 2 lists the top candidate ranked by the systems for different input sentences.\n",
        "output": "{\"label\": [\"USE\", \"SIMILARITY\", \"DIFFERENCES\"], \"context\": [\"sent0\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "However, from an SMT perspective it is important that the models reflect probability distributions which are preferred by the decoding process, i.e., phrase translations which are likely to be used frequently to achieve better translations should get higher scores and phrases which are less likely to be used should get low scores.",
                "In addition, the heuristic extraction algorithm generates all possible, consistent phrases including overlapping phrases.",
                "This means that translation probabilities are distributed over a very large number of phrase translation candidates most of which never lead to the best possible translation of a sentence.",
                "In this paper, we propose a novel solution which is to re-estimate the models from the best BLEU translation of each source sentence in the bitext.",
                "An important contribution of our approach is that unlike previous approaches such as forced alignment #TARGET_REF , reordering and language models can also be re-estimated."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: However, from an SMT perspective it is important that the models reflect probability distributions which are preferred by the decoding process, i.e., phrase translations which are likely to be used frequently to achieve better translations should get higher scores and phrases which are less likely to be used should get low scores.\n sent1: In addition, the heuristic extraction algorithm generates all possible, consistent phrases including overlapping phrases.\n sent2: This means that translation probabilities are distributed over a very large number of phrase translation candidates most of which never lead to the best possible translation of a sentence.\n sent3: In this paper, we propose a novel solution which is to re-estimate the models from the best BLEU translation of each source sentence in the bitext.\n sent4: An important contribution of our approach is that unlike previous approaches such as forced alignment #TARGET_REF , reordering and language models can also be re-estimated.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Within forced decoding, #TARGET_REF address this problem by using a leave-one-out approach where they modify the phrase translation probabilities for each sentence pair by removing the counts of all phrases that were extracted from that particular sentence.",
                "However, in our approach, we do not impose a constraint to produce the exact translation, instead we use the highest BLEU translations which may be very different from the references.",
                "Thus it is not strictly necessary to apply leave-one-out in our approach as a solution to over-fitting.",
                "Instead, we handle the problem by simply removing all the phrase pairs below a threshold count which in our case is 2,",
                "therefore removing phrase pairs with high probability but low frequency."
            ],
            "label": [
                "DIFFERENCES",
                "BACKGROUND"
            ]
        },
        "input": "sent0: Within forced decoding, #TARGET_REF address this problem by using a leave-one-out approach where they modify the phrase translation probabilities for each sentence pair by removing the counts of all phrases that were extracted from that particular sentence.\n sent1: However, in our approach, we do not impose a constraint to produce the exact translation, instead we use the highest BLEU translations which may be very different from the references.\n sent2: Thus it is not strictly necessary to apply leave-one-out in our approach as a solution to over-fitting.\n sent3: Instead, we handle the problem by simply removing all the phrase pairs below a threshold count which in our case is 2,\n sent4: therefore removing phrase pairs with high probability but low frequency.\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"BACKGROUND\"], \"context\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The best improvements over the baseline are obtained by using only 1-best (n= 1) alignments as shown in Table 1 .",
                "Surprisingly, this is in contrast with forced decoding as discussed in #TARGET_REF , where the best improvements are obtained for n = 100.",
                "Table 2 provides a comparison between BLEU improvements achieved by forced decoding (n = 100 best) and our oracle-BLEU re-estimation approach (n = 1 best) over the baseline for different models.",
                "One can see in Table 2 that while phrase table re-estimation drops substantially for forced decoding for all test sets (up to -1.4 for MT09), oracle-BLEU phrase table re-estimation shows either slight improvements or negligible drops compared to the baseline.",
                "For the linear interpolation of the re-estimated phrase table with the baseline, forced decoding shows only a slight improvement for MT06, MT08 and MT09 and still suffers from a substantial drop for MT05."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: The best improvements over the baseline are obtained by using only 1-best (n= 1) alignments as shown in Table 1 .\n sent1: Surprisingly, this is in contrast with forced decoding as discussed in #TARGET_REF , where the best improvements are obtained for n = 100.\n sent2: Table 2 provides a comparison between BLEU improvements achieved by forced decoding (n = 100 best) and our oracle-BLEU re-estimation approach (n = 1 best) over the baseline for different models.\n sent3: One can see in Table 2 that while phrase table re-estimation drops substantially for forced decoding for all test sets (up to -1.4 for MT09), oracle-BLEU phrase table re-estimation shows either slight improvements or negligible drops compared to the baseline.\n sent4: For the linear interpolation of the re-estimated phrase table with the baseline, forced decoding shows only a slight improvement for MT06, MT08 and MT09 and still suffers from a substantial drop for MT05.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The forced alignment technique of #TARGET_REF forms the main motivation for our work.",
                "In forced alignment, given a sentence pair (F, E), a decoder determines the best phrase segmentation and alignment which will result in a translation of F into E. The best segmentation is defined as the one which maximizes the probability of translating the source sentence into the given target sentence.",
                "At the end, the phrase table is re-estimated using the phrase pair segmentations obtained from forced decoding.",
                "Thus forced alignment is a reestimation technique where translation probabilities are calculated based on their frequency in best-scoring hypotheses instead of the frequencies of all possible phrase pairs in the bitext.",
                "However, one limitation of forced alignment is that only the phrase translation model can be re-estimated since it is restricted to align the source sentence to the given target reference, thus fixing the choice of reordering decisions."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: The forced alignment technique of #TARGET_REF forms the main motivation for our work.\n sent1: In forced alignment, given a sentence pair (F, E), a decoder determines the best phrase segmentation and alignment which will result in a translation of F into E. The best segmentation is defined as the one which maximizes the probability of translating the source sentence into the given target sentence.\n sent2: At the end, the phrase table is re-estimated using the phrase pair segmentations obtained from forced decoding.\n sent3: Thus forced alignment is a reestimation technique where translation probabilities are calculated based on their frequency in best-scoring hypotheses instead of the frequencies of all possible phrase pairs in the bitext.\n sent4: However, one limitation of forced alignment is that only the phrase translation model can be re-estimated since it is restricted to align the source sentence to the given target reference, thus fixing the choice of reordering decisions.\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "A similar line of work is proposed by and who use a self-enhancing strategy to utilize additional mono- lingual source language data by aligning it to its target language translation obtained by using an SMT system to rank sentence translation probabilities.",
                "However, the main focus of their work is translation model adaptation by augmenting the bitext with additional training data and not the reestimation of the translation models trained on the parallel data.",
                "In this work, we propose that aligning source sentences to their oracle BLEU translations provides a more realistic estimate of the models from the decoding perspective instead of aligning them to high quality human translations as in forced decoding.",
                "Another relevant line of research relates tuning (weight optimisation), where our work lies between forced decoding #TARGET_REF and the bold updating approach of (#REF) .",
                "However, our approach specifically proposes a novel method for training models using oracle BLEU translations."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: A similar line of work is proposed by and who use a self-enhancing strategy to utilize additional mono- lingual source language data by aligning it to its target language translation obtained by using an SMT system to rank sentence translation probabilities.\n sent1: However, the main focus of their work is translation model adaptation by augmenting the bitext with additional training data and not the reestimation of the translation models trained on the parallel data.\n sent2: In this work, we propose that aligning source sentences to their oracle BLEU translations provides a more realistic estimate of the models from the decoding perspective instead of aligning them to high quality human translations as in forced decoding.\n sent3: Another relevant line of research relates tuning (weight optimisation), where our work lies between forced decoding #TARGET_REF and the bold updating approach of (#REF) .\n sent4: However, our approach specifically proposes a novel method for training models using oracle BLEU translations.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The highest BLEU improvement of +0.8 is achieved by using a re-estimated BiLM and an interpolated phrase ysis, we experimented with the interpolation of both the re-estimated phrase table (forced decoding and oracle-BLEU) with the baseline.",
                "However, improvements achieved with this interpolation did not surpass the best result obtained for the oracle-BLEU re-estimation.",
                "Additionally, we also compare oracle-BLEU re-estimation to forced decoding with leave-oneout #TARGET_REF by evaluating both on a concatenation of 5 test sets (MT03, MT05-MT09).",
                "As shown in Table 3 , even with leaveone-out, forced decoding performance drops below the baseline by -0.3 BLEU.",
                "In contrast, phrase tables re-estimated from oracle-BLEU translation achieves the same performance as the baseline."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The highest BLEU improvement of +0.8 is achieved by using a re-estimated BiLM and an interpolated phrase ysis, we experimented with the interpolation of both the re-estimated phrase table (forced decoding and oracle-BLEU) with the baseline.\n sent1: However, improvements achieved with this interpolation did not surpass the best result obtained for the oracle-BLEU re-estimation.\n sent2: Additionally, we also compare oracle-BLEU re-estimation to forced decoding with leave-oneout #TARGET_REF by evaluating both on a concatenation of 5 test sets (MT03, MT05-MT09).\n sent3: As shown in Table 3 , even with leaveone-out, forced decoding performance drops below the baseline by -0.3 BLEU.\n sent4: In contrast, phrase tables re-estimated from oracle-BLEU translation achieves the same performance as the baseline.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Current state-of-the-art parsers score over 90% on the standard newswire evaluation, but the remaining errors are difficult to overcome using only the training corpus.",
                "Features from n-gram counts over resources like Web1T (#REF ) have proven to be useful proxies for syntax #TARGET_REF; #REF) , but they enforce linear word order, and are unable to distinguish between syntactic and non-syntactic co-occurrences.",
                "Longer n-grams are also noisier and sparser, limiting the range of potential features.",
                "In this paper we develop new features for the graph-based MSTParser (#REF) from the Google Syntactic Ngrams corpus (#REF) , a collection of Stanford dependency subtree counts.",
                "These features capture information collated across millions of subtrees produced by a shift-reduce parser, trading off potential systemic parser errors for data that is better aligned with the parsing task."
            ],
            "label": [
                "MOTIVATION",
                "BACKGROUND"
            ]
        },
        "input": "sent0: Current state-of-the-art parsers score over 90% on the standard newswire evaluation, but the remaining errors are difficult to overcome using only the training corpus.\n sent1: Features from n-gram counts over resources like Web1T (#REF ) have proven to be useful proxies for syntax #TARGET_REF; #REF) , but they enforce linear word order, and are unable to distinguish between syntactic and non-syntactic co-occurrences.\n sent2: Longer n-grams are also noisier and sparser, limiting the range of potential features.\n sent3: In this paper we develop new features for the graph-based MSTParser (#REF) from the Google Syntactic Ngrams corpus (#REF) , a collection of Stanford dependency subtree counts.\n sent4: These features capture information collated across millions of subtrees produced by a shift-reduce parser, trading off potential systemic parser errors for data that is better aligned with the parsing task.\n",
        "output": "{\"label\": [\"MOTIVATION\", \"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "3 Surface n-gram Features #TARGET_REF demonstrate that features generated from bucketing simple surface n-gram counts and collecting the top paraphrase-based contextual words over Web1T are useful for almost all attachment decisions, boosting dependency parsing accuracy by up to 0.6%.",
                "However, this technique is restricted to counts based purely on the linear order of the adjacent words, and is unable to incorporate disambiguating information such as POS tags to avoid spurious counts.",
                "#REF also tested only on in-domain text, though these external count features should be useful out of domain.",
                "We extract #REF 's affinity and paraphrase-style first-order features from the Google Books English Ngrams corpus, and compare their performance against Web1T counts.",
                "Both corpora are very large, contain different types of noise, and are sourced from very different underlying texts."
            ],
            "label": [
                "MOTIVATION",
                "BACKGROUND"
            ]
        },
        "input": "sent0: 3 Surface n-gram Features #TARGET_REF demonstrate that features generated from bucketing simple surface n-gram counts and collecting the top paraphrase-based contextual words over Web1T are useful for almost all attachment decisions, boosting dependency parsing accuracy by up to 0.6%.\n sent1: However, this technique is restricted to counts based purely on the linear order of the adjacent words, and is unable to incorporate disambiguating information such as POS tags to avoid spurious counts.\n sent2: #REF also tested only on in-domain text, though these external count features should be useful out of domain.\n sent3: We extract #REF 's affinity and paraphrase-style first-order features from the Google Books English Ngrams corpus, and compare their performance against Web1T counts.\n sent4: Both corpora are very large, contain different types of noise, and are sourced from very different underlying texts.\n",
        "output": "{\"label\": [\"MOTIVATION\", \"BACKGROUND\"], \"context\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Affinity features rely on the intuition that frequently co-occurring words in large unlabeled text collections are likely to be in a syntactic relationship (#REF; #TARGET_REF .",
                "N-gram resources such as Web1T and Google Books provide large offline collections from which these co-occurrence statistics can be harvested; given each head and argument ambiguity in a training and test corpus, the corpora can be linearly scanned ahead of parsing time to reduce the impact of querying in the parser.",
                "When scanning, the head and argument word may appear immediately adjacent to one another in linear order (CONTIG), or with up to three intervening words (GAP1, GAP2, and GAP3) as the maximum n-gram length is five.",
                "The total count is then discretized as per Equation 1 previously.",
                "The final parser features include the POS tags of the potential head and argument, the discretized count, directionality, and the binned length of the dependency."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Affinity features rely on the intuition that frequently co-occurring words in large unlabeled text collections are likely to be in a syntactic relationship (#REF; #TARGET_REF .\n sent1: N-gram resources such as Web1T and Google Books provide large offline collections from which these co-occurrence statistics can be harvested; given each head and argument ambiguity in a training and test corpus, the corpora can be linearly scanned ahead of parsing time to reduce the impact of querying in the parser.\n sent2: When scanning, the head and argument word may appear immediately adjacent to one another in linear order (CONTIG), or with up to three intervening words (GAP1, GAP2, and GAP3) as the maximum n-gram length is five.\n sent3: The total count is then discretized as per Equation 1 previously.\n sent4: The final parser features include the POS tags of the potential head and argument, the discretized count, directionality, and the binned length of the dependency.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "In #TARGET_REF , paraphrase features are generated for all full-parse attachment ambiguities from the surface n-gram corpus.",
                "For each attachment ambiguity, 3-grams of the form ( q 1 q 2 ), (q 1 q 2 ), and (q 1 q 2 ) are extracted, where q 1 and q 2 are the head and argument in their linear order of appearance in the original sentence, and is any single context word appearing before, in between, or after the query words.",
                "Then the most frequent words appearing in each of these configurations for each head-argument ambiguity is encoded as a feature with the POS tags of the head and argument 2 .",
                "Given the arc hold → hearing in Figure 2 , public is the most frequent word appearing in the n-gram (hold hearing) in Web1T.",
                "Thus, the final encoded feature is POS (hold) ∧ POS (hearing) ∧ public ∧ mid."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In #TARGET_REF , paraphrase features are generated for all full-parse attachment ambiguities from the surface n-gram corpus.\n sent1: For each attachment ambiguity, 3-grams of the form ( q 1 q 2 ), (q 1 q 2 ), and (q 1 q 2 ) are extracted, where q 1 and q 2 are the head and argument in their linear order of appearance in the original sentence, and is any single context word appearing before, in between, or after the query words.\n sent2: Then the most frequent words appearing in each of these configurations for each head-argument ambiguity is encoded as a feature with the POS tags of the head and argument 2 .\n sent3: Given the arc hold → hearing in Figure 2 , public is the most frequent word appearing in the n-gram (hold hearing) in Web1T.\n sent4: Thus, the final encoded feature is POS (hold) ∧ POS (hearing) ∧ public ∧ mid.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Surface n-gram counts from large web corpora have been used to address NP and PP attachment errors (#REF; #REF) Aside from #TARGET_REF , other feature-based approaches to improving dependency parsing include #REF , who exploits Brown clusters and point-wise mutual information of surface n-gram counts to specifically address PP and coordination errors.",
                "#REF describe a novel way of generating meta-features that work to emphasise important feature types used by the parser.",
                "#REF generate subtree-based features that are similar to ours.",
                "However, they use the in-domain BLLIP newswire corpus to generate their subtree counts, whereas the Syntactic Ngrams corpus is out-of-domain and an order of magnitude larger.",
                "They also use the same underlying parser to generate the BLLIP subtree counts and as the final test-time parser, while Syntactic Ngrams is parsed with a simpler, shift-reduce parser compared to the graph-based MSTParser used during test time."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Surface n-gram counts from large web corpora have been used to address NP and PP attachment errors (#REF; #REF) Aside from #TARGET_REF , other feature-based approaches to improving dependency parsing include #REF , who exploits Brown clusters and point-wise mutual information of surface n-gram counts to specifically address PP and coordination errors.\n sent1: #REF describe a novel way of generating meta-features that work to emphasise important feature types used by the parser.\n sent2: #REF generate subtree-based features that are similar to ours.\n sent3: However, they use the in-domain BLLIP newswire corpus to generate their subtree counts, whereas the Syntactic Ngrams corpus is out-of-domain and an order of magnitude larger.\n sent4: They also use the same underlying parser to generate the BLLIP subtree counts and as the final test-time parser, while Syntactic Ngrams is parsed with a simpler, shift-reduce parser compared to the graph-based MSTParser used during test time.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Features from n-gram counts over resources like Web1T (#REF ) have proven to be useful proxies for syntax (#REF; #REF) , but they enforce linear word order, and are unable to distinguish between syntactic and non-syntactic co-occurrences.",
                "Longer n-grams are also noisier and sparser, limiting the range of potential features.",
                "In this paper we develop new features for the graph-based MSTParser (#REF) from the Google Syntactic Ngrams corpus (#REF) , a collection of Stanford dependency subtree counts.",
                "These features capture information collated across millions of subtrees produced by a shift-reduce parser, trading off potential systemic parser errors for data that is better aligned with the parsing task.",
                "We compare the performance of our syntactic n-gram features against the surface n-gram features of #TARGET_REF in-domain on newswire and out-of-domain on the English Web Treebank (#REF) across CoNLL-style (LTH) dependencies."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Features from n-gram counts over resources like Web1T (#REF ) have proven to be useful proxies for syntax (#REF; #REF) , but they enforce linear word order, and are unable to distinguish between syntactic and non-syntactic co-occurrences.\n sent1: Longer n-grams are also noisier and sparser, limiting the range of potential features.\n sent2: In this paper we develop new features for the graph-based MSTParser (#REF) from the Google Syntactic Ngrams corpus (#REF) , a collection of Stanford dependency subtree counts.\n sent3: These features capture information collated across millions of subtrees produced by a shift-reduce parser, trading off potential systemic parser errors for data that is better aligned with the parsing task.\n sent4: We compare the performance of our syntactic n-gram features against the surface n-gram features of #TARGET_REF in-domain on newswire and out-of-domain on the English Web Treebank (#REF) across CoNLL-style (LTH) dependencies.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Table 1 summarizes the first-order features extracted from the dependency hold → hearing depicted in Figure 1 .",
                "The final feature encodes the POS tags of the head and argument, directionality, the binned distance between the head and argument, and a bucketed frequency of the syntactic n-gram calculated as per Equation 1, creating bucket labels from 0 in increments of 5 (0, 5, 10, etc.) .",
                "Additional features for each bucket value up to the maximum are also encoded.",
                "We also develop paraphrase-style features like those of #TARGET_REF based on the most frequently occurring words and POS tags before, in between, and after each head-argument ambiguity (see Section 3.2).",
                "Figure 1 depicts the potential context words available the hold → hearing dependency."
            ],
            "label": [
                "USE",
                "SIMILARITY"
            ]
        },
        "input": "sent0: Table 1 summarizes the first-order features extracted from the dependency hold → hearing depicted in Figure 1 .\n sent1: The final feature encodes the POS tags of the head and argument, directionality, the binned distance between the head and argument, and a bucketed frequency of the syntactic n-gram calculated as per Equation 1, creating bucket labels from 0 in increments of 5 (0, 5, 10, etc.) .\n sent2: Additional features for each bucket value up to the maximum are also encoded.\n sent3: We also develop paraphrase-style features like those of #TARGET_REF based on the most frequently occurring words and POS tags before, in between, and after each head-argument ambiguity (see Section 3.2).\n sent4: Figure 1 depicts the potential context words available the hold → hearing dependency.\n",
        "output": "{\"label\": [\"USE\", \"SIMILARITY\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "However, this technique is restricted to counts based purely on the linear order of the adjacent words, and is unable to incorporate disambiguating information such as POS tags to avoid spurious counts.",
                "#REF also tested only on in-domain text, though these external count features should be useful out of domain.",
                "We extract #REF 's affinity and paraphrase-style first-order features from the Google Books English Ngrams corpus, and compare their performance against Web1T counts.",
                "Both corpora are very large, contain different types of noise, and are sourced from very different underlying texts.",
                "We also extend #TARGET_REF Klein's affinity and paraphrase features to second-order."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: However, this technique is restricted to counts based purely on the linear order of the adjacent words, and is unable to incorporate disambiguating information such as POS tags to avoid spurious counts.\n sent1: #REF also tested only on in-domain text, though these external count features should be useful out of domain.\n sent2: We extract #REF 's affinity and paraphrase-style first-order features from the Google Books English Ngrams corpus, and compare their performance against Web1T counts.\n sent3: Both corpora are very large, contain different types of noise, and are sourced from very different underlying texts.\n sent4: We also extend #TARGET_REF Klein's affinity and paraphrase features to second-order.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "As with #TARGET_REF #REF , we convert the Penn Treebank to dependencies using pennconverter 3 (#REF) (henceforth LTH) and generate POS tags with MX-POST (#REF) .",
                "We used sections 02-21 of the WSJ for training, 22 for development, and 23 for final testing.",
                "The test sections of the answers, newsgroups, and reviews sections of the English Web Treebank as per the SANCL 2012 Shared Task (#REF) were converted to LTH and used for out-of-domain evaluation.",
                "We used MSTParser (#REF) , trained with the parameters order:2, training-k:5, iters:10, and loss-type:nopunc.",
                "We omit labeled attachment scores in this paper for brevity, but they are consistent with the reported UAS scores."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: As with #TARGET_REF #REF , we convert the Penn Treebank to dependencies using pennconverter 3 (#REF) (henceforth LTH) and generate POS tags with MX-POST (#REF) .\n sent1: We used sections 02-21 of the WSJ for training, 22 for development, and 23 for final testing.\n sent2: The test sections of the answers, newsgroups, and reviews sections of the English Web Treebank as per the SANCL 2012 Shared Task (#REF) were converted to LTH and used for out-of-domain evaluation.\n sent3: We used MSTParser (#REF) , trained with the parameters order:2, training-k:5, iters:10, and loss-type:nopunc.\n sent4: We omit labeled attachment scores in this paper for brevity, but they are consistent with the reported UAS scores.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "However, this technique is restricted to counts based purely on the linear order of the adjacent words, and is unable to incorporate disambiguating information such as POS tags to avoid spurious counts.",
                "#REF also tested only on in-domain text, though these external count features should be useful out of domain.",
                "We extract #TARGET_REF 's affinity and paraphrase-style first-order features from the Google Books English Ngrams corpus, and compare their performance against Web1T counts.",
                "Both corpora are very large, contain different types of noise, and are sourced from very different underlying texts.",
                "We also extend Bansal and Klein's affinity and paraphrase features to second-order."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: However, this technique is restricted to counts based purely on the linear order of the adjacent words, and is unable to incorporate disambiguating information such as POS tags to avoid spurious counts.\n sent1: #REF also tested only on in-domain text, though these external count features should be useful out of domain.\n sent2: We extract #TARGET_REF 's affinity and paraphrase-style first-order features from the Google Books English Ngrams corpus, and compare their performance against Web1T counts.\n sent3: Both corpora are very large, contain different types of noise, and are sourced from very different underlying texts.\n sent4: We also extend Bansal and Klein's affinity and paraphrase features to second-order.\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The premier example for word embeddings is skip-gram negative sampling, which is part of the word2vec family of algorithms (#REF) .",
                "The random processes involved in training these embeddings lead to a lack of reliability which is dangerous during interpretationexperiments cannot be repeated without predicting severely different relationships between words #REFa, 2017) .",
                "Word embeddings based on singular value decomposition (SVD; historically popular in the form of Latent Semantic Analysis (#REF) ) are not affected by this problem.",
                "#REF created SVD PPMI after investigating the implicit operations performed while training neural word embeddings (#REF) .",
                "As SVD PPMI performs very similar to word2vec on evaluation tasks while avoiding reliability problems we deem it the best currently available word embedding method for applying distributional semantics in the Digital Humanities #TARGET_REF; #REFa) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The premier example for word embeddings is skip-gram negative sampling, which is part of the word2vec family of algorithms (#REF) .\n sent1: The random processes involved in training these embeddings lead to a lack of reliability which is dangerous during interpretationexperiments cannot be repeated without predicting severely different relationships between words #REFa, 2017) .\n sent2: Word embeddings based on singular value decomposition (SVD; historically popular in the form of Latent Semantic Analysis (#REF) ) are not affected by this problem.\n sent3: #REF created SVD PPMI after investigating the implicit operations performed while training neural word embeddings (#REF) .\n sent4: As SVD PPMI performs very similar to word2vec on evaluation tasks while avoiding reliability problems we deem it the best currently available word embedding method for applying distributional semantics in the Digital Humanities #TARGET_REF; #REFa) .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Meanwhile, word embeddings and their application to diachronic semantics have become a novel state-of-the-art methodology lacking, however, off-the-shelves analysis tools easy to use for a typically non-technical audience.",
                "Most work is centered around word2vec (e.g., #REF ; #REF ; Hellrich and Hahn (2016b) ), whereas alternative approaches are rare, e.g., #REF using GloVe (#REF) and #TARGET_REF using SVD PPMI .",
                "Embeddings trained on corpora specific for multiple time spans can be used for two research purposes, namely, screening the semantic evolution of lexical items over time (#REF; #REF; #REF) and exploring the meaning of lexical items during a specific time span by finding their closest neighbors in embedding space.",
                "This information can then be exploited for automatic (#REF) or manual (#REF) interpretation.",
                "research."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Meanwhile, word embeddings and their application to diachronic semantics have become a novel state-of-the-art methodology lacking, however, off-the-shelves analysis tools easy to use for a typically non-technical audience.\n sent1: Most work is centered around word2vec (e.g., #REF ; #REF ; Hellrich and Hahn (2016b) ), whereas alternative approaches are rare, e.g., #REF using GloVe (#REF) and #TARGET_REF using SVD PPMI .\n sent2: Embeddings trained on corpora specific for multiple time spans can be used for two research purposes, namely, screening the semantic evolution of lexical items over time (#REF; #REF; #REF) and exploring the meaning of lexical items during a specific time span by finding their closest neighbors in embedding space.\n sent3: This information can then be exploited for automatic (#REF) or manual (#REF) interpretation.\n sent4: research.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "There exist already several tools for performing statistical analysis on user provided corpora, e.g., WORDSMITH 3 or the UCS TOOLKIT, 4 as well as interactive websites for exploring precompiled corpora, e.g., the \"advanced\" interface for Google Books (#REF) or DIACOLLO (#REF) .",
                "Meanwhile, word embeddings and their application to diachronic semantics have become a novel state-of-the-art methodology lacking, however, off-the-shelves analysis tools easy to use for a typically non-technical audience.",
                "Most work is centered around word2vec (e.g., #REF ; #REF ; Hellrich and Hahn (2016b) ), whereas alternative approaches are rare, e.g., #REF using GloVe (#REF) and #REF using SVD PPMI .",
                "Embeddings trained on corpora specific for multiple time spans can be used for two research purposes, namely, screening the semantic evolution of lexical items over time (#REF; #REF; #TARGET_REF and exploring the meaning of lexical items during a specific time span by finding their closest neighbors in embedding space.",
                "This information can then be exploited for automatic (#REF) or manual (#REF) interpretation."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: There exist already several tools for performing statistical analysis on user provided corpora, e.g., WORDSMITH 3 or the UCS TOOLKIT, 4 as well as interactive websites for exploring precompiled corpora, e.g., the \"advanced\" interface for Google Books (#REF) or DIACOLLO (#REF) .\n sent1: Meanwhile, word embeddings and their application to diachronic semantics have become a novel state-of-the-art methodology lacking, however, off-the-shelves analysis tools easy to use for a typically non-technical audience.\n sent2: Most work is centered around word2vec (e.g., #REF ; #REF ; Hellrich and Hahn (2016b) ), whereas alternative approaches are rare, e.g., #REF using GloVe (#REF) and #REF using SVD PPMI .\n sent3: Embeddings trained on corpora specific for multiple time spans can be used for two research purposes, namely, screening the semantic evolution of lexical items over time (#REF; #REF; #TARGET_REF and exploring the meaning of lexical items during a specific time span by finding their closest neighbors in embedding space.\n sent4: This information can then be exploited for automatic (#REF) or manual (#REF) interpretation.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The resulting models have a size of 32 GB and are available for download on JESEME's Help page.",
                "9 To ensure JESEME's responsiveness, we finally pre-computed similarity (by cosine between word embeddings), as well as context specificity based on PPMI and χ 2 .",
                "These values are stored in a POSTGRESQL 10 database, occupying about 60GB of space.",
                "Due to both space constraints (scaling with O(n 2 ) for vocabulary size n) and the lower quality of representations for infrequent words, we limited this step to words which were among the 10k most frequent words for all slices of a corpus, resulting in 3,1k -6,5k words per corpus.",
                "In accordance with this limit, we also discarded slices with less than 10k (5k for RSC) 7 Parameters were chosen in accordance with #REF and #TARGET_REF words above the minimum frequency threshold used during PPMI and χ 2 calculation, e.g., the 1810s and 1820s COHA slices."
            ],
            "label": [
                "USE",
                "SIMILARITY"
            ]
        },
        "input": "sent0: The resulting models have a size of 32 GB and are available for download on JESEME's Help page.\n sent1: 9 To ensure JESEME's responsiveness, we finally pre-computed similarity (by cosine between word embeddings), as well as context specificity based on PPMI and χ 2 .\n sent2: These values are stored in a POSTGRESQL 10 database, occupying about 60GB of space.\n sent3: Due to both space constraints (scaling with O(n 2 ) for vocabulary size n) and the lower quality of representations for infrequent words, we limited this step to words which were among the 10k most frequent words for all slices of a corpus, resulting in 3,1k -6,5k words per corpus.\n sent4: In accordance with this limit, we also discarded slices with less than 10k (5k for RSC) 7 Parameters were chosen in accordance with #REF and #TARGET_REF words above the minimum frequency threshold used during PPMI and χ 2 calculation, e.g., the 1810s and 1820s COHA slices.\n",
        "output": "{\"label\": [\"USE\", \"SIMILARITY\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Query words are automatically lowercased or lemmatized, depending on the respective corpus (see Section 4).",
                "The result page provides three kinds of graphs, i.e., Similar Words, Typical Context and Relative Frequency.",
                "Similar Words depicts the words with the highest similarity relative to the query term for the first and last time slice and how their similarity values changed over time.",
                "We follow #REF in choosing such a visualization, while we refrain from using the two-dimensional projection used in other studies (#REF; #TARGET_REF .",
                "We stipulate that the latter could Figure 2 : Screenshot of JESEME's result page when searching for the lexical item \"heart\" in COHA."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Query words are automatically lowercased or lemmatized, depending on the respective corpus (see Section 4).\n sent1: The result page provides three kinds of graphs, i.e., Similar Words, Typical Context and Relative Frequency.\n sent2: Similar Words depicts the words with the highest similarity relative to the query term for the first and last time slice and how their similarity values changed over time.\n sent3: We follow #REF in choosing such a visualization, while we refrain from using the two-dimensional projection used in other studies (#REF; #TARGET_REF .\n sent4: We stipulate that the latter could Figure 2 : Screenshot of JESEME's result page when searching for the lexical item \"heart\" in COHA.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "We presented JESEME, the Jena Semantic Explorer, an interactive website and REST API for exploring changes in lexical semantics over long periods of time.",
                "In contrast to other corpus exploration tools, JESEME is based on cutting-edge word embedding technology (#REF; #TARGET_REF; #REFa, 2017) and provides access to five popular corpora for the English and German language.",
                "JESEME is also the first tool of its kind and under continuous development.",
                "Future technical work will add functionality to compare words across corpora which might require a mapping between embeddings (#REF; #REF) and provide optional stemming routines.",
                "Both goals come with an increase in precomputed similarity values and will thus necessitate storage optimizations to ensure long-term availability."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: We presented JESEME, the Jena Semantic Explorer, an interactive website and REST API for exploring changes in lexical semantics over long periods of time.\n sent1: In contrast to other corpus exploration tools, JESEME is based on cutting-edge word embedding technology (#REF; #TARGET_REF; #REFa, 2017) and provides access to five popular corpora for the English and German language.\n sent2: JESEME is also the first tool of its kind and under continuous development.\n sent3: Future technical work will add functionality to compare words across corpora which might require a mapping between embeddings (#REF; #REF) and provide optional stemming routines.\n sent4: Both goals come with an increase in precomputed similarity values and will thus necessitate storage optimizations to ensure long-term availability.\n",
        "output": "{\"label\": [null], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "In contrast to other corpus exploration tools, JESEME is based on cutting-edge word embedding technology (#REF; #REF; #REFa, 2017) and provides access to five popular corpora for the English and German language.",
                "JESEME is also the first tool of its kind and under continuous development.",
                "Future technical work will add functionality to compare words across corpora which might require a mapping between embeddings (#REF; #TARGET_REF and provide optional stemming routines.",
                "Both goals come with an increase in precomputed similarity values and will thus necessitate storage optimizations to ensure long-term availability.",
                "Finally, we will conduct a user study to investigate JESEME's potential for the Digital Humanities community."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: In contrast to other corpus exploration tools, JESEME is based on cutting-edge word embedding technology (#REF; #REF; #REFa, 2017) and provides access to five popular corpora for the English and German language.\n sent1: JESEME is also the first tool of its kind and under continuous development.\n sent2: Future technical work will add functionality to compare words across corpora which might require a mapping between embeddings (#REF; #TARGET_REF and provide optional stemming routines.\n sent3: Both goals come with an increase in precomputed similarity values and will thus necessitate storage optimizations to ensure long-term availability.\n sent4: Finally, we will conduct a user study to investigate JESEME's potential for the Digital Humanities community.\n",
        "output": "{\"label\": [null], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "This limitation is probably partly due to the fact that these analyses were not conducted in the field of automatic scoring, but in applied linguistics.",
                "In automatic scoring, phraseological expres-sions have long been used almost exclusively for detecting errors, a task for which they have been very useful (e.g., #REF; #REF; #REF) .",
                "It is noteworthy that a feature tracking the correct use of collocations was considered for inclusion in eRater, but its usefulness for predicting text quality seems rather limited (#REF) .",
                "Very recently, however, #REF and #TARGET_REF Even if these results were extremely promising, they leave a number of questions unanswered.",
                "First, they were obtained by studying short oral responses."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: This limitation is probably partly due to the fact that these analyses were not conducted in the field of automatic scoring, but in applied linguistics.\n sent1: In automatic scoring, phraseological expres-sions have long been used almost exclusively for detecting errors, a task for which they have been very useful (e.g., #REF; #REF; #REF) .\n sent2: It is noteworthy that a feature tracking the correct use of collocations was considered for inclusion in eRater, but its usefulness for predicting text quality seems rather limited (#REF) .\n sent3: Very recently, however, #REF and #TARGET_REF Even if these results were extremely promising, they leave a number of questions unanswered.\n sent4: First, they were obtained by studying short oral responses.\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Mutual rank ratio (mrr, #REF), a nonparametric measure that has been successful in detecting collocation errors in EFL texts (#REF), 6 . logDice (#REF), a logarithmic transformation of the Dice coefficient used in the Sketch Engine (#REF) .",
                "In order to extract more information from the distribution of the ASs in each text than the mean or the median, #REF and #TARGET_REF used a standard procedure in descriptive statistics and automatic information processing known as discretization, binning or quantization (#REF) .",
                "It divides a continuous variable into bins and counts the proportion of scores that fall into each bin.",
                "In their analyses, the boundaries of the bins were manually and arbitrarily defined.",
                "This approach can be used for any AS, but it makes the comparison of the effectiveness of them difficult because a weaker performance may come from a less effective AS or from poorly chosen bin boundaries."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Mutual rank ratio (mrr, #REF), a nonparametric measure that has been successful in detecting collocation errors in EFL texts (#REF), 6 . logDice (#REF), a logarithmic transformation of the Dice coefficient used in the Sketch Engine (#REF) .\n sent1: In order to extract more information from the distribution of the ASs in each text than the mean or the median, #REF and #TARGET_REF used a standard procedure in descriptive statistics and automatic information processing known as discretization, binning or quantization (#REF) .\n sent2: It divides a continuous variable into bins and counts the proportion of scores that fall into each bin.\n sent3: In their analyses, the boundaries of the bins were manually and arbitrarily defined.\n sent4: This approach can be used for any AS, but it makes the comparison of the effectiveness of them difficult because a weaker performance may come from a less effective AS or from poorly chosen bin boundaries.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Dataset: The analyses were conducted on the First Certificate in English (FCE) ESOL examination scripts described in Yannakoudakis et al. (2011 Yannakoudakis et al. ( , 2012 .",
                "Extracted from the Cambridge Learner Corpus, this dataset consists of 1238 texts of between 200 and 400 words, to which an overall mark has been assigned.",
                "As in #REF , the 1141 texts from the year 2000 were used for training, while the 97 texts from the year 2001 were used for testing.",
                "Collocational Features: The global statistical features in #TARGET_REF and were used: the mean, the median, the maximum and the minimum of the ASs, and the proportion of bigrams that are present in the learner text but absent from the reference corpus.",
                "Because the best number of bins for discretizing the distributions was not known, the following ones were compared: 3, 5, 8, 10, 15, 20, 25, 33, 50, 75 and 100. To get all these features, each learner text was tokenized and POS-tagged by means of CLAWS7 2 and all bigrams were extracted."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Dataset: The analyses were conducted on the First Certificate in English (FCE) ESOL examination scripts described in Yannakoudakis et al. (2011 Yannakoudakis et al. ( , 2012 .\n sent1: Extracted from the Cambridge Learner Corpus, this dataset consists of 1238 texts of between 200 and 400 words, to which an overall mark has been assigned.\n sent2: As in #REF , the 1141 texts from the year 2000 were used for training, while the 97 texts from the year 2001 were used for testing.\n sent3: Collocational Features: The global statistical features in #TARGET_REF and were used: the mean, the median, the maximum and the minimum of the ASs, and the proportion of bigrams that are present in the learner text but absent from the reference corpus.\n sent4: Because the best number of bins for discretizing the distributions was not known, the following ones were compared: 3, 5, 8, 10, 15, 20, 25, 33, 50, 75 and 100. To get all these features, each learner text was tokenized and POS-tagged by means of CLAWS7 2 and all bigrams were extracted.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "From eight bins and beyond, using all the ASs gave the best result, but the gain was relatively small.",
                "Regarding the number of bins, at least five seems necessary, but using many more did not harm performance.",
                "It is noteworthy that all the correlations reported in table 1 are much larger that the correlation of a baseline system based purely on length (r = 0.27).",
                "To determine if the automatic procedure for discretizing the ASs is at least as effective as the bin boundaries manually set by #TARGET_REF , I used them instead of the automatic bins for the model with eight bins based on MI.",
                "The correlation obtained was 0.60, a value slightly lower than that reported in Table 1 (0.61)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: From eight bins and beyond, using all the ASs gave the best result, but the gain was relatively small.\n sent1: Regarding the number of bins, at least five seems necessary, but using many more did not harm performance.\n sent2: It is noteworthy that all the correlations reported in table 1 are much larger that the correlation of a baseline system based purely on length (r = 0.27).\n sent3: To determine if the automatic procedure for discretizing the ASs is at least as effective as the bin boundaries manually set by #TARGET_REF , I used them instead of the automatic bins for the model with eight bins based on MI.\n sent4: The correlation obtained was 0.60, a value slightly lower than that reported in Table 1 (0.61).\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "It is also necessary to determine whether the collocational features can improve not only the baseline used here, but also a predictive model that includes many other features known for their effectiveness.",
                "Further developments are worth mentioning.",
                "Unlike #TARGET_REF , I only used bigrams' collocational features.",
                "Whether adding trigrams would further improve the performance is an open question.",
                "Trying to answer it requires a thorough study of the association measures for ngrams longer than two words since they have received much less attention (#REF; #REF) ."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: It is also necessary to determine whether the collocational features can improve not only the baseline used here, but also a predictive model that includes many other features known for their effectiveness.\n sent1: Further developments are worth mentioning.\n sent2: Unlike #TARGET_REF , I only used bigrams' collocational features.\n sent3: Whether adding trigrams would further improve the performance is an open question.\n sent4: Trying to answer it requires a thorough study of the association measures for ngrams longer than two words since they have received much less attention (#REF; #REF) .\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Unsupervised speech representation learning [2, 3, 4, 5, 6, #TARGET_REF 8, 9, 10] is effective in extracting high-level properties from speech.",
                "SLP downstream tasks can be improved through speech representations because surface features such as log Mel-spectrograms or waveform can poorly reveal the abundant information within speech.",
                "Contrastive Predictive Coding (CPC) [5] and wav2vec [7] use a multi-layer CNN to encode past context, representations are learned by predicting the future in latent space under a contrastive binary classification task.",
                "Autoregressive Predictive Coding (APC) [6] uses autoregressive models to encode temporal information of past acoustic sequences; the model predicts future frames like an RNN-based language model [11] , optimized with reconstruction loss.",
                "Unidirectional models are commonly used in the previous approaches [2, 3, 4, 5, 6, 7] ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Unsupervised speech representation learning [2, 3, 4, 5, 6, #TARGET_REF 8, 9, 10] is effective in extracting high-level properties from speech.\n sent1: SLP downstream tasks can be improved through speech representations because surface features such as log Mel-spectrograms or waveform can poorly reveal the abundant information within speech.\n sent2: Contrastive Predictive Coding (CPC) [5] and wav2vec [7] use a multi-layer CNN to encode past context, representations are learned by predicting the future in latent space under a contrastive binary classification task.\n sent3: Autoregressive Predictive Coding (APC) [6] uses autoregressive models to encode temporal information of past acoustic sequences; the model predicts future frames like an RNN-based language model [11] , optimized with reconstruction loss.\n sent4: Unidirectional models are commonly used in the previous approaches [2, 3, 4, 5, 6, 7] .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Contrastive Predictive Coding (CPC) [5] and wav2vec #TARGET_REF use a multi-layer CNN to encode past context, representations are learned by predicting the future in latent space under a contrastive binary classification task.",
                "Autoregressive Predictive Coding (APC) [6] uses autoregressive models to encode temporal information of past acoustic sequences; the model predicts future frames like an RNN-based language model [11] , optimized with reconstruction loss.",
                "Unidirectional models are commonly used in the previous approaches [2, 3, 4, 5, 6, 7] .",
                "However, this constraint on model architectures limits the potential of speech representation learning.",
                "The recently proposed vq-wav2vec [8] approach attempts to apply the well-performing Natural Language Processing (NLP) algorithm BERT [12] on continuous speech."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Contrastive Predictive Coding (CPC) [5] and wav2vec #TARGET_REF use a multi-layer CNN to encode past context, representations are learned by predicting the future in latent space under a contrastive binary classification task.\n sent1: Autoregressive Predictive Coding (APC) [6] uses autoregressive models to encode temporal information of past acoustic sequences; the model predicts future frames like an RNN-based language model [11] , optimized with reconstruction loss.\n sent2: Unidirectional models are commonly used in the previous approaches [2, 3, 4, 5, 6, 7] .\n sent3: However, this constraint on model architectures limits the potential of speech representation learning.\n sent4: The recently proposed vq-wav2vec [8] approach attempts to apply the well-performing Natural Language Processing (NLP) algorithm BERT [12] on continuous speech.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Autoregressive Predictive Coding (APC) [6] uses autoregressive models to encode temporal information of past acoustic sequences; the model predicts future frames like an RNN-based language model [11] , optimized with reconstruction loss.",
                "Unidirectional models are commonly used in the previous approaches [2, 3, 4, 5, 6, #TARGET_REF .",
                "However, this constraint on model architectures limits the potential of speech representation learning.",
                "The recently proposed vq-wav2vec [8] approach attempts to apply the well-performing Natural Language Processing (NLP) algorithm BERT [12] on continuous speech.",
                "Input speech is discretized to a K-way quantized embedding space, so continuous speech could act like discrete units similar to word tokens in NLP tasks."
            ],
            "label": [
                "BACKGROUND",
                "MOTIVATION"
            ]
        },
        "input": "sent0: Autoregressive Predictive Coding (APC) [6] uses autoregressive models to encode temporal information of past acoustic sequences; the model predicts future frames like an RNN-based language model [11] , optimized with reconstruction loss.\n sent1: Unidirectional models are commonly used in the previous approaches [2, 3, 4, 5, 6, #TARGET_REF .\n sent2: However, this constraint on model architectures limits the potential of speech representation learning.\n sent3: The recently proposed vq-wav2vec [8] approach attempts to apply the well-performing Natural Language Processing (NLP) algorithm BERT [12] on continuous speech.\n sent4: Input speech is discretized to a K-way quantized embedding space, so continuous speech could act like discrete units similar to word tokens in NLP tasks.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"MOTIVATION\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Unlike previous left-to-right unidirectional approaches that only consider past sequences to predict information about future frames, the proposed method allows us to train a bidirectional speech representation model, alleviating the unidirectionality constraint of previous methods.",
                "As a result, the Mockingjay model obtains substantial improvements in several SLP tasks.",
                "Moreover, as previous approaches restrict the power of the pre-trained models to representation extraction only [5, 6, #TARGET_REF 8] , the proposed method is robust and can be fine-tuned easily on downstream tasks.",
                "We show that finetuning for 2 epochs easily acquires significant improvement.",
                "The proposed approach outperforms other representations and features."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Unlike previous left-to-right unidirectional approaches that only consider past sequences to predict information about future frames, the proposed method allows us to train a bidirectional speech representation model, alleviating the unidirectionality constraint of previous methods.\n sent1: As a result, the Mockingjay model obtains substantial improvements in several SLP tasks.\n sent2: Moreover, as previous approaches restrict the power of the pre-trained models to representation extraction only [5, 6, #TARGET_REF 8] , the proposed method is robust and can be fine-tuned easily on downstream tasks.\n sent3: We show that finetuning for 2 epochs easily acquires significant improvement.\n sent4: The proposed approach outperforms other representations and features.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "As reported in [6] , the APC approach outperformed CPC representations [5, #TARGET_REF 9] in both two tasks, which makes APC suitable as a strong baseline.",
                "APC uses an unidirectional autoregressive model.",
                "We compare the proposed approach with APC to show that our bidirectional approach has advantages in speech representation learning.",
                "For fair comparison, we pre-train APC using their official implementations with the reported ideal parameters and settings, but expand the model's hidden size to H dim =768 to match ours.",
                "We also report results on 160-dimensional log Mel-features, which helps evaluate the accessibility of speech information from regular acoustic features."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: As reported in [6] , the APC approach outperformed CPC representations [5, #TARGET_REF 9] in both two tasks, which makes APC suitable as a strong baseline.\n sent1: APC uses an unidirectional autoregressive model.\n sent2: We compare the proposed approach with APC to show that our bidirectional approach has advantages in speech representation learning.\n sent3: For fair comparison, we pre-train APC using their official implementations with the reported ideal parameters and settings, but expand the model's hidden size to H dim =768 to match ours.\n sent4: We also report results on 160-dimensional log Mel-features, which helps evaluate the accessibility of speech information from regular acoustic features.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Following previous works [2, 3, 4, 5, 6, #TARGET_REF 8] , we evaluate different features and representations on downstream tasks, including: phoneme classification, speaker recognition, and sentiment classification on spoken content.",
                "For a fair comparison, each downstream task uses an identical model architecture and hyperparameters despite different input features.",
                "We report results from 5 of our models: 1) BASE and 2) LARGE where Mockingjay representations are extracted from the last encoder layer, 3) the BASE-FT2 where we finetune BASE with random initialized downstream models for 2 epochs, and 4) the BASE-FT500 where we fine-tune for 500k steps, and finally 5) the LARGE-WS where we incorporate hidden states from all encoder layers of the LARGE model through a learnable weighted sum.",
                "We did not fine-tune the LARGE model, as it is meant for extracting representations.",
                "Empirically we found that even with supervised training, a random initialized Mockingjay model followed by any downstream model is hard to be trained from scratch."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Following previous works [2, 3, 4, 5, 6, #TARGET_REF 8] , we evaluate different features and representations on downstream tasks, including: phoneme classification, speaker recognition, and sentiment classification on spoken content.\n sent1: For a fair comparison, each downstream task uses an identical model architecture and hyperparameters despite different input features.\n sent2: We report results from 5 of our models: 1) BASE and 2) LARGE where Mockingjay representations are extracted from the last encoder layer, 3) the BASE-FT2 where we finetune BASE with random initialized downstream models for 2 epochs, and 4) the BASE-FT500 where we fine-tune for 500k steps, and finally 5) the LARGE-WS where we incorporate hidden states from all encoder layers of the LARGE model through a learnable weighted sum.\n sent3: We did not fine-tune the LARGE model, as it is meant for extracting representations.\n sent4: Empirically we found that even with supervised training, a random initialized Mockingjay model followed by any downstream model is hard to be trained from scratch.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF propose a new architecture that avoids recurrence and convolution completely.",
                "Instead, it uses only self-attention and feed-forward layers.",
                "While the proposed architecture achieves state-of-the-art results on several machine translation tasks, it requires a large number of parameters and training iterations to converge.",
                "We propose Weighted Transformer, a Transformer with modified attention layers, that not only outperforms the baseline network in BLEU score but also converges 15 − 40% faster.",
                "Specifically, we replace the multi-head attention by multiple self-attention branches that the model learns to combine during the training process."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF propose a new architecture that avoids recurrence and convolution completely.\n sent1: Instead, it uses only self-attention and feed-forward layers.\n sent2: While the proposed architecture achieves state-of-the-art results on several machine translation tasks, it requires a large number of parameters and training iterations to converge.\n sent3: We propose Weighted Transformer, a Transformer with modified attention layers, that not only outperforms the baseline network in BLEU score but also converges 15 − 40% faster.\n sent4: Specifically, we replace the multi-head attention by multiple self-attention branches that the model learns to combine during the training process.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Several architectures have been proposed to reduce the computational load associated with recurrence-based computation (#REF; 2017; Kaiser & #REF; #REF) .",
                "Self-attention, which relies on dot-products between elements of the input sequence to compute a weighted sum (#REF; #REF; #REF) , has also been a critical ingredient in modern NMT architectures.",
                "The Transformer network #TARGET_REF avoids the recurrence completely and uses only self-attention.",
                "We propose a modified Transformer network wherein the multi-head attention layer is replaced by a branched self-attention layer.",
                "The contributions of the various branches is learned as part of the training procedure."
            ],
            "label": [
                "BACKGROUND",
                "EXTENDS"
            ]
        },
        "input": "sent0: Several architectures have been proposed to reduce the computational load associated with recurrence-based computation (#REF; 2017; Kaiser & #REF; #REF) .\n sent1: Self-attention, which relies on dot-products between elements of the input sequence to compute a weighted sum (#REF; #REF; #REF) , has also been a critical ingredient in modern NMT architectures.\n sent2: The Transformer network #TARGET_REF avoids the recurrence completely and uses only self-attention.\n sent3: We propose a modified Transformer network wherein the multi-head attention layer is replaced by a branched self-attention layer.\n sent4: The contributions of the various branches is learned as part of the training procedure.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"EXTENDS\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "where the W i and W O are parameter projection matrices that are learned.",
                "Note that",
                "where h denotes the number of heads in the multi-head attention.",
                "#TARGET_REF proportionally reduce d k = d v = d model so that the computational load of the multi-head attention is the same as simple self-attention.",
                "The second component of each layer of the Transformer network is a feed-forward network."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: where the W i and W O are parameter projection matrices that are learned.\n sent1: Note that\n sent2: where h denotes the number of heads in the multi-head attention.\n sent3: #TARGET_REF proportionally reduce d k = d v = d model so that the computational load of the multi-head attention is the same as simple self-attention.\n sent4: The second component of each layer of the Transformer network is a feed-forward network.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Given trainable weights W 1 , W 2 , b 1 , b 2 , the sub-layer is defined as:",
                "The dimension of the inner layer is d f f which is set to 2048 in their experiments.",
                "For the sake of brevity, we refer the reader to #TARGET_REF for additional details regarding the architecture.",
                "For regularization and ease of training, the network uses layer normalization (#REF) after each sub-layer and a residual connection around each full layer .",
                "Analogously, each layer of the decoder contains the two sub-layers mentioned above as well as an additional multi-head attention sub-layer that receives as inputs (V, K) from the output of the corresponding encoding layer."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Given trainable weights W 1 , W 2 , b 1 , b 2 , the sub-layer is defined as:\n sent1: The dimension of the inner layer is d f f which is set to 2048 in their experiments.\n sent2: For the sake of brevity, we refer the reader to #TARGET_REF for additional details regarding the architecture.\n sent3: For regularization and ease of training, the network uses layer normalization (#REF) after each sub-layer and a residual connection around each full layer .\n sent4: Analogously, each layer of the decoder contains the two sub-layers mentioned above as well as an additional multi-head attention sub-layer that receives as inputs (V, K) from the output of the corresponding encoding layer.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Analogously, each layer of the decoder contains the two sub-layers mentioned above as well as an additional multi-head attention sub-layer that receives as inputs (V, K) from the output of the corresponding encoding layer.",
                "In the case of the decoder multi-head attention sub-layers, the scaled dot-product attention is masked to prevent future positions from being attended to, or in other words, to prevent illegal leftward-ward information flow.",
                "One natural question regarding the Transformer network is why self-attention should be preferred to recurrent or convolutional models.",
                "#TARGET_REF state three reasons for the preference: (a) computational complexity of each layer, (b) concurrency, and (c) path length between long-range dependencies.",
                "Assuming a sequence length of n and vector dimension d, the complexity of each layer is O(n 2 d) for self-attention layers while it is O(nd 2 ) for recurrent layers."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Analogously, each layer of the decoder contains the two sub-layers mentioned above as well as an additional multi-head attention sub-layer that receives as inputs (V, K) from the output of the corresponding encoding layer.\n sent1: In the case of the decoder multi-head attention sub-layers, the scaled dot-product attention is masked to prevent future positions from being attended to, or in other words, to prevent illegal leftward-ward information flow.\n sent2: One natural question regarding the Transformer network is why self-attention should be preferred to recurrent or convolutional models.\n sent3: #TARGET_REF state three reasons for the preference: (a) computational complexity of each layer, (b) concurrency, and (c) path length between long-range dependencies.\n sent4: Assuming a sequence length of n and vector dimension d, the complexity of each layer is O(n 2 d) for self-attention layers while it is O(nd 2 ) for recurrent layers.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "We now describe the proposed architecture, the Weighted Transformer, which is more efficient to train and makes better use of representational power.",
                "In Equations (3) and (4), we described the attention layer proposed in #TARGET_REF comprising the multi-head attention sub-layer and a FFN sub-layer.",
                "For the Weighted Transformer, we propose a branched attention that modifies the entire attention layer in the Transformer network (including both the multi-head attention and the feed-forward network).",
                "The proposed attention layer can be described as:",
                "where M denotes the total number of branches, κ i , α i ∈ R + are learned parameters and W Oi ∈ R dv×dmodel ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We now describe the proposed architecture, the Weighted Transformer, which is more efficient to train and makes better use of representational power.\n sent1: In Equations (3) and (4), we described the attention layer proposed in #TARGET_REF comprising the multi-head attention sub-layer and a FFN sub-layer.\n sent2: For the Weighted Transformer, we propose a branched attention that modifies the entire attention layer in the Transformer network (including both the multi-head attention and the feed-forward network).\n sent3: The proposed attention layer can be described as:\n sent4: where M denotes the total number of branches, κ i , α i ∈ R + are learned parameters and W Oi ∈ R dv×dmodel .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Transformer (small) #TARGET_REF 27.3 38.1 Weighted Transformer (small) 28.4 38.9",
                "Transformer (large) (#REF) 28.4 41.0 Weighted Transformer (large) 28.9 41.4",
                "ByteNet (#REF) 23.7 -Deep-Att+PosUnk (#REF) -39.2 GNMT+RL (#REF) 24.6 39.9 ConvS2S (#REF) 25.2 40.5 MoE 26.0 40.6 Table 1 : Experimental results on the WMT 2014 English-to-German (EN-DE) and English-toFrench (EN-FR) translation tasks.",
                "Our proposed model outperforms the state-of-the-art models including the Transformer (#REF) .",
                "The small model corresponds to configuration (A) in Table 2 while large corresponds to configuration (B)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Transformer (small) #TARGET_REF 27.3 38.1 Weighted Transformer (small) 28.4 38.9\n sent1: Transformer (large) (#REF) 28.4 41.0 Weighted Transformer (large) 28.9 41.4\n sent2: ByteNet (#REF) 23.7 -Deep-Att+PosUnk (#REF) -39.2 GNMT+RL (#REF) 24.6 39.9 ConvS2S (#REF) 25.2 40.5 MoE 26.0 40.6 Table 1 : Experimental results on the WMT 2014 English-to-German (EN-DE) and English-toFrench (EN-FR) translation tasks.\n sent3: Our proposed model outperforms the state-of-the-art models including the Transformer (#REF) .\n sent4: The small model corresponds to configuration (A) in Table 2 while large corresponds to configuration (B).\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Transformer (small) (#REF) 27.3 38.1 Weighted Transformer (small) 28.4 38.9",
                "Transformer (large) #TARGET_REF 28.4 41.0 Weighted Transformer (large) 28.9 41.4",
                "ByteNet (#REF) 23.7 -Deep-Att+PosUnk (#REF) -39.2 GNMT+RL (#REF) 24.6 39.9 ConvS2S (#REF) 25.2 40.5 MoE 26.0 40.6 Table 1 : Experimental results on the WMT 2014 English-to-German (EN-DE) and English-toFrench (EN-FR) translation tasks.",
                "Our proposed model outperforms the state-of-the-art models including the Transformer (#REF) .",
                "The small model corresponds to configuration (A) in Table 2 while large corresponds to configuration (B)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Transformer (small) (#REF) 27.3 38.1 Weighted Transformer (small) 28.4 38.9\n sent1: Transformer (large) #TARGET_REF 28.4 41.0 Weighted Transformer (large) 28.9 41.4\n sent2: ByteNet (#REF) 23.7 -Deep-Att+PosUnk (#REF) -39.2 GNMT+RL (#REF) 24.6 39.9 ConvS2S (#REF) 25.2 40.5 MoE 26.0 40.6 Table 1 : Experimental results on the WMT 2014 English-to-German (EN-DE) and English-toFrench (EN-FR) translation tasks.\n sent3: Our proposed model outperforms the state-of-the-art models including the Transformer (#REF) .\n sent4: The small model corresponds to configuration (A) in Table 2 while large corresponds to configuration (B).\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "In addition to the layer normalization and residual connections, we use label smoothing with ls = 0.1, attention dropout, and residual dropout with probability P drop = 0.1.",
                "Attention dropout randomly drops out elements (#REF) from the softmax in (1).",
                "As in #TARGET_REF , we used the Adam optimizer (Kingma & #REF) with (β 1 , β 2 ) = (0.9, 0.98) and = 10 −9 .",
                "We also use the learning rate warm-up strategy for Adam wherein the learning rate lr takes on the form:",
                "for the all parameters except (α, κ) and"
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: In addition to the layer normalization and residual connections, we use label smoothing with ls = 0.1, attention dropout, and residual dropout with probability P drop = 0.1.\n sent1: Attention dropout randomly drops out elements (#REF) from the softmax in (1).\n sent2: As in #TARGET_REF , we used the Adam optimizer (Kingma & #REF) with (β 1 , β 2 ) = (0.9, 0.98) and = 10 −9 .\n sent3: We also use the learning rate warm-up strategy for Adam wherein the learning rate lr takes on the form:\n sent4: for the all parameters except (α, κ) and\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We train the Weighted Transformer for the respective variants for 60K and 250K iterations.",
                "We found that the objective did not significantly improve by running it for longer.",
                "Further, we do not use any averaging strategies employed in #TARGET_REF and simply return the final model for testing purposes.",
                "In order to reduce the computational load associated with padding, sentences were batched such that they were approximately of the same length.",
                "All sentences were encoded using byte-pair encoding (#REF) and shared a common vocabulary."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: We train the Weighted Transformer for the respective variants for 60K and 250K iterations.\n sent1: We found that the objective did not significantly improve by running it for longer.\n sent2: Further, we do not use any averaging strategies employed in #TARGET_REF and simply return the final model for testing purposes.\n sent3: In order to reduce the computational load associated with padding, sentences were batched such that they were approximately of the same length.\n sent4: All sentences were encoded using byte-pair encoding (#REF) and shared a common vocabulary.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Transformer (large) (#REF) 28.4 41.0 Weighted Transformer (large) 28.9 41.4",
                "ByteNet (#REF) 23.7 -Deep-Att+PosUnk (#REF) -39.2 GNMT+RL (#REF) 24.6 39.9 ConvS2S (#REF) 25.2 40.5 MoE 26.0 40.6 Table 1 : Experimental results on the WMT 2014 English-to-German (EN-DE) and English-toFrench (EN-FR) translation tasks.",
                "Our proposed model outperforms the state-of-the-art models including the Transformer #TARGET_REF .",
                "The small model corresponds to configuration (A) in Table 2 while large corresponds to configuration (B).",
                "testing losses for the same training loss."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Transformer (large) (#REF) 28.4 41.0 Weighted Transformer (large) 28.9 41.4\n sent1: ByteNet (#REF) 23.7 -Deep-Att+PosUnk (#REF) -39.2 GNMT+RL (#REF) 24.6 39.9 ConvS2S (#REF) 25.2 40.5 MoE 26.0 40.6 Table 1 : Experimental results on the WMT 2014 English-to-German (EN-DE) and English-toFrench (EN-FR) translation tasks.\n sent2: Our proposed model outperforms the state-of-the-art models including the Transformer #TARGET_REF .\n sent3: The small model corresponds to configuration (A) in Table 2 while large corresponds to configuration (B).\n sent4: testing losses for the same training loss.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We propose to learn contextualized sparse phrase representations which are also very interpretable.",
                "We demonstrate the effectiveness of our model in open-domain question answering (QA), the task of retrieving answer phrases given a web-scale collection of documents.",
                "Following #TARGET_REF , we concatenate both sparse and dense vectors to encode every phrase in Wikipedia and use maximum similarity search to find the closest candidate phrase to answer each question.",
                "We only substitute (or augment) the baseline sparse encoding which is entirely based on frequency-based embedding (tf-idf) with our contextualized sparse representation (COSPR).",
                "Our empirical results demonstrate its state-of-the-art performance in open-domain QA datasets, SQUAD OPEN and CURATEDTREC."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We propose to learn contextualized sparse phrase representations which are also very interpretable.\n sent1: We demonstrate the effectiveness of our model in open-domain question answering (QA), the task of retrieving answer phrases given a web-scale collection of documents.\n sent2: Following #TARGET_REF , we concatenate both sparse and dense vectors to encode every phrase in Wikipedia and use maximum similarity search to find the closest candidate phrase to answer each question.\n sent3: We only substitute (or augment) the baseline sparse encoding which is entirely based on frequency-based embedding (tf-idf) with our contextualized sparse representation (COSPR).\n sent4: Our empirical results demonstrate its state-of-the-art performance in open-domain QA datasets, SQUAD OPEN and CURATEDTREC.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "One can also think of this as kernel trick (in the literature of SVM (Cortes & #REF) ) that allows us to compute the loss function without explicit mapping.",
                "The loss to minimize is computed from the negative log likelihood over the sum of the dense and sparse logits:",
                "where i * , j * denote the true start and end positions of the answer phrase.",
                "While the loss above is an unbiased estimator, in practice, we adopt early loss summation as suggested by #TARGET_REF for larger gradient signals in early training.",
                "Additionally, we also add dense-only loss that omits the sparse logits (i.e. original loss in #REF ) to the final loss, in which case we find that we obtain higher-quality dense phrase representations."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: One can also think of this as kernel trick (in the literature of SVM (Cortes & #REF) ) that allows us to compute the loss function without explicit mapping.\n sent1: The loss to minimize is computed from the negative log likelihood over the sum of the dense and sparse logits:\n sent2: where i * , j * denote the true start and end positions of the answer phrase.\n sent3: While the loss above is an unbiased estimator, in practice, we adopt early loss summation as suggested by #TARGET_REF for larger gradient signals in early training.\n sent4: Additionally, we also add dense-only loss that omits the sparse logits (i.e. original loss in #REF ) to the final loss, in which case we find that we obtain higher-quality dense phrase representations.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "where i * , j * denote the true start and end positions of the answer phrase.",
                "While the loss above is an unbiased estimator, in practice, we adopt early loss summation as suggested by #REF for larger gradient signals in early training.",
                "Additionally, we also add dense-only loss that omits the sparse logits (i.e. original loss in #TARGET_REF ) to the final loss, in which case we find that we obtain higher-quality dense phrase representations.",
                "Negative Sampling We train our model on SQuAD v1.1 which always has a positive paragraph that contains the answer.",
                "To learn robust phrase representations, we concatenate negative paragraphs to the original SQuAD paragraphs."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: where i * , j * denote the true start and end positions of the answer phrase.\n sent1: While the loss above is an unbiased estimator, in practice, we adopt early loss summation as suggested by #REF for larger gradient signals in early training.\n sent2: Additionally, we also add dense-only loss that omits the sparse logits (i.e. original loss in #TARGET_REF ) to the final loss, in which case we find that we obtain higher-quality dense phrase representations.\n sent3: Negative Sampling We train our model on SQuAD v1.1 which always has a positive paragraph that contains the answer.\n sent4: To learn robust phrase representations, we concatenate negative paragraphs to the original SQuAD paragraphs.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Negative Sampling We train our model on SQuAD v1.1 which always has a positive paragraph that contains the answer.",
                "To learn robust phrase representations, we concatenate negative paragraphs to the original SQuAD paragraphs.",
                "To each paragraph x, we concatenate the paragraph x neg which was paired with the question whose dense representation h neg is most similar to the original dense question representation h , following #TARGET_REF .",
                "Note the difference, however, that we concatenate the negative example instead of considering it as an independent example with noanswer option #REF .",
                "During training, we find that adding tf-idf matching scores on the word-level logits of the negative paragraphs further improves the quality of sparse representations as our sparse models have to give stronger attentions to positively related words in this biased setting."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Negative Sampling We train our model on SQuAD v1.1 which always has a positive paragraph that contains the answer.\n sent1: To learn robust phrase representations, we concatenate negative paragraphs to the original SQuAD paragraphs.\n sent2: To each paragraph x, we concatenate the paragraph x neg which was paired with the question whose dense representation h neg is most similar to the original dense question representation h , following #TARGET_REF .\n sent3: Note the difference, however, that we concatenate the negative example instead of considering it as an independent example with noanswer option #REF .\n sent4: During training, we find that adding tf-idf matching scores on the word-level logits of the negative paragraphs further improves the quality of sparse representations as our sparse models have to give stronger attentions to positively related words in this biased setting.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "It takes 600 GPU hours to index all phrases in Wikipedia.",
                "Each phrase representation has 2d se + 2F + 1 dimensions.",
                "We use the same storage reduction and search techniques by #TARGET_REF .",
                "For storage, the total size of the index is 1.3 TB including unigram and bigram sparse representations.",
                "For search, we either perform dense search first and then rerank with sparse scores (DFS) or perform sparse search first and rerank with dense scores (SFS), and also consider a combination of both (Hybrid)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: It takes 600 GPU hours to index all phrases in Wikipedia.\n sent1: Each phrase representation has 2d se + 2F + 1 dimensions.\n sent2: We use the same storage reduction and search techniques by #TARGET_REF .\n sent3: For storage, the total size of the index is 1.3 TB including unigram and bigram sparse representations.\n sent4: For search, we either perform dense search first and then rerank with sparse scores (DFS) or perform sparse search first and rerank with dense scores (SFS), and also consider a combination of both (Hybrid).\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We compare tf-idf vectors and COSPR (uni/bigram) by showing top weighted n-grams in each representation.",
                "Note that the scale of weights in tf-idf vectors is normalized in open-domain setups to match the scale between tf-idf vectors and dense vectors.",
                "We observe that tf-idf vectors usually assign high weights on infrequent (often meaningless) n-grams, while COSPR focuses on contextually important entities such as 1991 for 415,000 or california state, state university for 12.",
                "Our sparse question representation also learns meaningful n-gram weights compared to tf-idf vectors.",
                "Table 4 shows the outputs of three OpenQA models: DrQA (#REF) , DENSPI #TARGET_REF , and our DENSPI+COSPR."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We compare tf-idf vectors and COSPR (uni/bigram) by showing top weighted n-grams in each representation.\n sent1: Note that the scale of weights in tf-idf vectors is normalized in open-domain setups to match the scale between tf-idf vectors and dense vectors.\n sent2: We observe that tf-idf vectors usually assign high weights on infrequent (often meaningless) n-grams, while COSPR focuses on contextually important entities such as 1991 for 415,000 or california state, state university for 12.\n sent3: Our sparse question representation also learns meaningful n-gram weights compared to tf-idf vectors.\n sent4: Table 4 shows the outputs of three OpenQA models: DrQA (#REF) , DENSPI #TARGET_REF , and our DENSPI+COSPR.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Notably, our method significantly outperforms DENSPI #TARGET_REF , the previous end-toend QA model, by more than 4% with negligible drop in inference speed.",
                "Moreover, our method achieves up to 2% better accuracy and x97 speedup in inference compared to pipeline (retrievalbased) approaches.",
                "Our analysis particularly shows that fine-grained sparse representation is crucial for doing well in phrase retrieval task.",
                "In summary, the contributions of our paper are:",
                "1. we show that learning sparse representations for embedding lexically important context words of a phrase can be achieved by contextualized sparse representations, 2. we introduce an efficient training strategy that leverages the kernelization of the sparse inner-product space, and 3. we achieve the state-of-the-art performance in two open-domain QA datasets with up to x97 faster inference time."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Notably, our method significantly outperforms DENSPI #TARGET_REF , the previous end-toend QA model, by more than 4% with negligible drop in inference speed.\n sent1: Moreover, our method achieves up to 2% better accuracy and x97 speedup in inference compared to pipeline (retrievalbased) approaches.\n sent2: Our analysis particularly shows that fine-grained sparse representation is crucial for doing well in phrase retrieval task.\n sent3: In summary, the contributions of our paper are:\n sent4: 1. we show that learning sparse representations for embedding lexically important context words of a phrase can be achieved by contextualized sparse representations, 2. we introduce an efficient training strategy that leverages the kernelization of the sparse inner-product space, and 3. we achieve the state-of-the-art performance in two open-domain QA datasets with up to x97 faster inference time.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "As training phrase encoders on the whole Wikipedia is computationally prohibitive, we use training examples from an extractive question answering dataset (SQuAD) to train our encoders.",
                "Given a pair of question q and a golden document x (a paragraph in the case of SQuAD), we first compute the dense logit of each phrase x i:j by l i,j = h i:j · h .",
                "Unlike #TARGET_REF , each phrase's sparse embedding is also trained, so it needs to be considered in the loss function.",
                "We define the sparse logit for phrase x i:j as l sparse",
                ". For brevity, we describe how we compute the first term s start i ·s start"
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: As training phrase encoders on the whole Wikipedia is computationally prohibitive, we use training examples from an extractive question answering dataset (SQuAD) to train our encoders.\n sent1: Given a pair of question q and a golden document x (a paragraph in the case of SQuAD), we first compute the dense logit of each phrase x i:j by l i,j = h i:j · h .\n sent2: Unlike #TARGET_REF , each phrase's sparse embedding is also trained, so it needs to be considered in the loss function.\n sent3: We define the sparse logit for phrase x i:j as l sparse\n sent4: . For brevity, we describe how we compute the first term s start i ·s start\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "To mitigate this problem, #TARGET_REF propose to learn query-agnostic representations of phrases in Wikipedia and retrieve phrases that best answers a question.",
                "While #REF have shown that encoding both dense and sparse representations for each phrase could keep the lexically important words of a phrase to some extent, their sparse representations are based on static tf-idf vectors which have globally the same weight for each n-gram.",
                "Phrase Representations In NLP, phrase representations can be either obtained in a similar manner as word representations (#REF) , or by learning a parametric function of word representations (#REF) .",
                "In extractive question answering, phrases are often referred to as spans, but most models do not consider explicitly learning phrase representations as these answer spans can be obtained by predicting only start and end positions in a paragraph (Wang & #REF; .",
                "Nevertheless, few studies have focused on directly learning and classifying phrase representations (#REF) which achieve strong performance when combined with attention mechanism."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: To mitigate this problem, #TARGET_REF propose to learn query-agnostic representations of phrases in Wikipedia and retrieve phrases that best answers a question.\n sent1: While #REF have shown that encoding both dense and sparse representations for each phrase could keep the lexically important words of a phrase to some extent, their sparse representations are based on static tf-idf vectors which have globally the same weight for each n-gram.\n sent2: Phrase Representations In NLP, phrase representations can be either obtained in a similar manner as word representations (#REF) , or by learning a parametric function of word representations (#REF) .\n sent3: In extractive question answering, phrases are often referred to as spans, but most models do not consider explicitly learning phrase representations as these answer spans can be obtained by predicting only start and end positions in a paragraph (Wang & #REF; .\n sent4: Nevertheless, few studies have focused on directly learning and classifying phrase representations (#REF) which achieve strong performance when combined with attention mechanism.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "and d c are chosen to make d = 2d se + 2d c ).",
                "Then each phrase x i:j is densely represented as follows:",
                "where · denotes inner product operation.",
                "h 1 i and h 2 j are start/end representations of a phrase, and the inner product of h 3 i and h 4 j is used for computing coherency of the phrase.",
                "Refer to #TARGET_REF for details; we mostly reuse its architecture."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: and d c are chosen to make d = 2d se + 2d c ).\n sent1: Then each phrase x i:j is densely represented as follows:\n sent2: where · denotes inner product operation.\n sent3: h 1 i and h 2 j are start/end representations of a phrase, and the inner product of h 3 i and h 4 j is used for computing coherency of the phrase.\n sent4: Refer to #TARGET_REF for details; we mostly reuse its architecture.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Due to the pipeline nature, however, these models inevitably suffer error propagation from the retrievers.",
                "To mitigate this problem, #REF propose to learn query-agnostic representations of phrases in Wikipedia and retrieve phrases that best answers a question.",
                "While #TARGET_REF have shown that encoding both dense and sparse representations for each phrase could keep the lexically important words of a phrase to some extent, their sparse representations are based on static tf-idf vectors which have globally the same weight for each n-gram.",
                "Phrase Representations In NLP, phrase representations can be either obtained in a similar manner as word representations (#REF) , or by learning a parametric function of word representations (#REF) .",
                "In extractive question answering, phrases are often referred to as spans, but most models do not consider explicitly learning phrase representations as these answer spans can be obtained by predicting only start and end positions in a paragraph (Wang & #REF; ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Due to the pipeline nature, however, these models inevitably suffer error propagation from the retrievers.\n sent1: To mitigate this problem, #REF propose to learn query-agnostic representations of phrases in Wikipedia and retrieve phrases that best answers a question.\n sent2: While #TARGET_REF have shown that encoding both dense and sparse representations for each phrase could keep the lexically important words of a phrase to some extent, their sparse representations are based on static tf-idf vectors which have globally the same weight for each n-gram.\n sent3: Phrase Representations In NLP, phrase representations can be either obtained in a similar manner as word representations (#REF) , or by learning a parametric function of word representations (#REF) .\n sent4: In extractive question answering, phrases are often referred to as spans, but most models do not consider explicitly learning phrase representations as these answer spans can be obtained by predicting only start and end positions in a paragraph (Wang & #REF; .\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We evaluate the effectiveness of COSPR by augmenting DENSPI #TARGET_REF with contextualized sparse representations (DENSPI+COSPR).",
                "We extensively compare the model with the original DENSPI and previous pipeline-based QA models.",
                "Model SQUADOPEN CURATEDTREC EM F1 Exact Match s/Q DrQA (#REF) 29.8 ** -25.4 * 35 R 3 (#REFa) 29.1 37.5 28.4 * -Paragraph Ranker (#REF) 30.2 -35.4 * -Multi-Step-Reasoner (#REF) 31.9 39.2 --BERTserini (#REF) 38.6 46.1 -115 ORQA 20.2 -30.1 -Multi-passage BERT † † (#REF) 53.0 60.9 --DENSPI (#REF) 36 (#REF) while being almost two orders of magnitude faster.",
                "We expect much bigger speed gaps between ours and other pipeline methods as most of them put additional complex components to the original pipelined methods.",
                "On CURATEDTREC, which is constructed from real user queries, our model also achieves the stateof-the-art performance."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: We evaluate the effectiveness of COSPR by augmenting DENSPI #TARGET_REF with contextualized sparse representations (DENSPI+COSPR).\n sent1: We extensively compare the model with the original DENSPI and previous pipeline-based QA models.\n sent2: Model SQUADOPEN CURATEDTREC EM F1 Exact Match s/Q DrQA (#REF) 29.8 ** -25.4 * 35 R 3 (#REFa) 29.1 37.5 28.4 * -Paragraph Ranker (#REF) 30.2 -35.4 * -Multi-Step-Reasoner (#REF) 31.9 39.2 --BERTserini (#REF) 38.6 46.1 -115 ORQA 20.2 -30.1 -Multi-passage BERT † † (#REF) 53.0 60.9 --DENSPI (#REF) 36 (#REF) while being almost two orders of magnitude faster.\n sent3: We expect much bigger speed gaps between ours and other pipeline methods as most of them put additional complex components to the original pipelined methods.\n sent4: On CURATEDTREC, which is constructed from real user queries, our model also achieves the stateof-the-art performance.\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Self-attention networks (SANs) (#REF; #REF) have shown promising empirical results in various natural language processing (NLP) tasks, such as machine translation #TARGET_REF , natural language inference (#REFa) , and acoustic modeling (#REF) .",
                "One appealing strength of SANs lies in their ability to capture dependencies regardless of distance by explicitly attending to all the elements.",
                "In addition, the performance of SANs can be improved by multi-head attention (#REF) , which projects the input sequence into multiple subspaces and applies attention to the representation in each subspace.",
                "Despite their success, SANs have two major limitations.",
                "First, the model fully take into ac- * Zhaopeng Tu is the corresponding author of the paper."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Self-attention networks (SANs) (#REF; #REF) have shown promising empirical results in various natural language processing (NLP) tasks, such as machine translation #TARGET_REF , natural language inference (#REFa) , and acoustic modeling (#REF) .\n sent1: One appealing strength of SANs lies in their ability to capture dependencies regardless of distance by explicitly attending to all the elements.\n sent2: In addition, the performance of SANs can be improved by multi-head attention (#REF) , which projects the input sequence into multiple subspaces and applies attention to the representation in each subspace.\n sent3: Despite their success, SANs have two major limitations.\n sent4: First, the model fully take into ac- * Zhaopeng Tu is the corresponding author of the paper.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "One appealing strength of SANs lies in their ability to capture dependencies regardless of distance by explicitly attending to all the elements.",
                "In addition, the performance of SANs can be improved by multi-head attention #TARGET_REF , which projects the input sequence into multiple subspaces and applies attention to the representation in each subspace.",
                "Despite their success, SANs have two major limitations.",
                "First, the model fully take into ac- * Zhaopeng Tu is the corresponding author of the paper.",
                "This work was conducted when Baosong Yang was interning at Tencent AI Lab."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: One appealing strength of SANs lies in their ability to capture dependencies regardless of distance by explicitly attending to all the elements.\n sent1: In addition, the performance of SANs can be improved by multi-head attention #TARGET_REF , which projects the input sequence into multiple subspaces and applies attention to the representation in each subspace.\n sent2: Despite their success, SANs have two major limitations.\n sent3: First, the model fully take into ac- * Zhaopeng Tu is the corresponding author of the paper.\n sent4: This work was conducted when Baosong Yang was interning at Tencent AI Lab.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Given an input sequence X = {x 1 , . . . , x I } ∈ R I×d , the model first transforms it into queries Q, keys K, and values V:",
                "where",
                "where ATT(·) is an attention model (#REF; #TARGET_REF that retrieves the keys K h with the query q h i .",
                "The final output representation O is the concatenation of outputs generated by multiple attention models:",
                "3 Approach"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Given an input sequence X = {x 1 , . . . , x I } ∈ R I×d , the model first transforms it into queries Q, keys K, and values V:\n sent1: where\n sent2: where ATT(·) is an attention model (#REF; #TARGET_REF that retrieves the keys K h with the query q h i .\n sent3: The final output representation O is the concatenation of outputs generated by multiple attention models:\n sent4: 3 Approach\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "To make a fair comparison, we re-implemented the above approaches under a same framework.",
                "Empirical results on machine translation tasks show the superiority of our approach in both translation quality and training efficiency.",
                "Multi-Head Attention Multi-head attention mechanism #TARGET_REF employs different attention heads to capture distinct features (#REF) .",
                "Along this direction, Shen et al. (2018a) explicitly used multiple attention heads to model different dependencies of the same word pair, and #REF employed different attention heads to capture different linguistic features.",
                "#REF introduced disagreement regularizations to encourage the diversity among attention heads."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: To make a fair comparison, we re-implemented the above approaches under a same framework.\n sent1: Empirical results on machine translation tasks show the superiority of our approach in both translation quality and training efficiency.\n sent2: Multi-Head Attention Multi-head attention mechanism #TARGET_REF employs different attention heads to capture distinct features (#REF) .\n sent3: Along this direction, Shen et al. (2018a) explicitly used multiple attention heads to model different dependencies of the same word pair, and #REF employed different attention heads to capture different linguistic features.\n sent4: #REF introduced disagreement regularizations to encourage the diversity among attention heads.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We expect that the interaction across different subspaces can further improve the performance of SANs.",
                "We evaluate the effectiveness of the proposed model on three widely-used translation tasks: WMT14 English-to-German, WMT17 Chineseto-English, and WAT17 Japanese-to-English.",
                "Experimental results demonstrate that our approach consistently improves performance over the strong TRANSFORMER model #TARGET_REF across language pairs.",
                "Comparing with previous work on modeling locality for SANs (e.g. #REF; #REF) , our model boosts performance on both translation quality and training efficiency.",
                "2 Multi-Head Self-Attention Networks"
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: We expect that the interaction across different subspaces can further improve the performance of SANs.\n sent1: We evaluate the effectiveness of the proposed model on three widely-used translation tasks: WMT14 English-to-German, WMT17 Chineseto-English, and WAT17 Japanese-to-English.\n sent2: Experimental results demonstrate that our approach consistently improves performance over the strong TRANSFORMER model #TARGET_REF across language pairs.\n sent3: Comparing with previous work on modeling locality for SANs (e.g. #REF; #REF) , our model boosts performance on both translation quality and training efficiency.\n sent4: 2 Multi-Head Self-Attention Networks\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We conducted experiments with the Transformer model #TARGET_REF on English⇒German (En⇒De), Chinese⇒English (Zh⇒En) and Japanese⇒English (Ja⇒En) translation tasks.",
                "For the En⇒De and Zh⇒En tasks, the models were trained on widely-used WMT14 and WMT17 corpora, consisting of around 4.5 and 20.62 million sentence pairs, respectively.",
                "Concerning Ja⇒En, we used the first two sections of WAT17 corpus as the training data, which consists of 2M sentence pairs.",
                "To reduce the vocabulary size, all the data were tokenized and segmented into subword symbols using byte-pair encoding (#REF) with 32K merge operations.",
                "Following #REF , we incorporated the proposed model into the encoder, which is a stack of 6 SAN layers."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We conducted experiments with the Transformer model #TARGET_REF on English⇒German (En⇒De), Chinese⇒English (Zh⇒En) and Japanese⇒English (Ja⇒En) translation tasks.\n sent1: For the En⇒De and Zh⇒En tasks, the models were trained on widely-used WMT14 and WMT17 corpora, consisting of around 4.5 and 20.62 million sentence pairs, respectively.\n sent2: Concerning Ja⇒En, we used the first two sections of WAT17 corpus as the training data, which consists of 2M sentence pairs.\n sent3: To reduce the vocabulary size, all the data were tokenized and segmented into subword symbols using byte-pair encoding (#REF) with 32K merge operations.\n sent4: Following #REF , we incorporated the proposed model into the encoder, which is a stack of 6 SAN layers.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Concerning Ja⇒En, we used the first two sections of WAT17 corpus as the training data, which consists of 2M sentence pairs.",
                "To reduce the vocabulary size, all the data were tokenized and segmented into subword symbols using byte-pair encoding (#REF) with 32K merge operations.",
                "Following #REF , we incorporated the proposed model into the encoder, which is a stack of 6 SAN layers.",
                "Prior studies revealed that modeling locality in lower layers can achieve better performance (#REFb; #REF; , we applied our approach to the lowest three layers of the encoder.",
                "About configurations of NMT models, we used the Base and Big settings same as #TARGET_REF , and all models were trained on 8 NVIDIA P40 GPUs with a batch of 4096 tokens."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: Concerning Ja⇒En, we used the first two sections of WAT17 corpus as the training data, which consists of 2M sentence pairs.\n sent1: To reduce the vocabulary size, all the data were tokenized and segmented into subword symbols using byte-pair encoding (#REF) with 32K merge operations.\n sent2: Following #REF , we incorporated the proposed model into the encoder, which is a stack of 6 SAN layers.\n sent3: Prior studies revealed that modeling locality in lower layers can achieve better performance (#REFb; #REF; , we applied our approach to the lowest three layers of the encoder.\n sent4: About configurations of NMT models, we used the Base and Big settings same as #TARGET_REF , and all models were trained on 8 NVIDIA P40 GPUs with a batch of 4096 tokens.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "We study the problem of jointly embedding a knowledge base and a text corpus.",
                "The key issue is the alignment model making sure the vectors of entities, relations and words are in the same space.",
                "#TARGET_REF rely on Wikipedia anchors, making the applicable scope quite limited.",
                "In this paper we propose a new alignment model based on text descriptions of entities, without dependency on anchors.",
                "We require the embedding vector of an entity not only to fit the structured constraints in KBs but also to be equal to the embedding vector computed from the text description."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: We study the problem of jointly embedding a knowledge base and a text corpus.\n sent1: The key issue is the alignment model making sure the vectors of entities, relations and words are in the same space.\n sent2: #TARGET_REF rely on Wikipedia anchors, making the applicable scope quite limited.\n sent3: In this paper we propose a new alignment model based on text descriptions of entities, without dependency on anchors.\n sent4: We require the embedding vector of an entity not only to fit the structured constraints in KBs but also to be equal to the embedding vector computed from the text description.\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "This task is to extract facts (h, r, t) from plain text. show that combing scores from TransE and some text side base extractor achieved much better precision-recall curve compared to the base extractor.",
                "#TARGET_REF confirm this observation and show that jointly embedding brings further encouraging improvement over TransE. In this experiment, we follow the same settings as #TARGET_REF to investigate the performance of our new alignment model.",
                "We use the same public dataset NYT+FB, released by #REF and used in and (#REFa) .",
                "We use Mintz (#REF ) and MIML (#REF) as our base extractors.",
                "In order to combine the score of a base extractor and the score from embeddings, we only reserve the testing triplets whose entitites and relations can be mapped to the embeddings learned from the triplet classification experiment."
            ],
            "label": [
                "MOTIVATION",
                "USE"
            ]
        },
        "input": "sent0: This task is to extract facts (h, r, t) from plain text. show that combing scores from TransE and some text side base extractor achieved much better precision-recall curve compared to the base extractor.\n sent1: #TARGET_REF confirm this observation and show that jointly embedding brings further encouraging improvement over TransE. In this experiment, we follow the same settings as #TARGET_REF to investigate the performance of our new alignment model.\n sent2: We use the same public dataset NYT+FB, released by #REF and used in and (#REFa) .\n sent3: We use Mintz (#REF ) and MIML (#REF) as our base extractors.\n sent4: In order to combine the score of a base extractor and the score from embeddings, we only reserve the testing triplets whose entitites and relations can be mapped to the embeddings learned from the triplet classification experiment.\n",
        "output": "{\"label\": [\"MOTIVATION\", \"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The key issue is the alignment model making sure the vectors of entities, relations and words are in the same space.",
                "Wang et al. (2014a) rely on Wikipedia anchors, making the applicable scope quite limited.",
                "In this paper we propose a new alignment model based on text descriptions of entities, without dependency on anchors.",
                "We require the embedding vector of an entity not only to fit the structured constraints in KBs but also to be equal to the embedding vector computed from the text description.",
                "Extensive experiments show that, the proposed approach consistently performs comparably or even better than the method of #TARGET_REF , which is encouraging as we do not use any anchor information."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: The key issue is the alignment model making sure the vectors of entities, relations and words are in the same space.\n sent1: Wang et al. (2014a) rely on Wikipedia anchors, making the applicable scope quite limited.\n sent2: In this paper we propose a new alignment model based on text descriptions of entities, without dependency on anchors.\n sent3: We require the embedding vector of an entity not only to fit the structured constraints in KBs but also to be equal to the embedding vector computed from the text description.\n sent4: Extensive experiments show that, the proposed approach consistently performs comparably or even better than the method of #TARGET_REF , which is encouraging as we do not use any anchor information.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "We only assume some entities in KBs have text descriptions, which almost always holds in practice.",
                "We require the embedding of an entity not only fits the structured constraints in KBs but also equals the vector computed from the text description.",
                "Meanwhile, if an entity has few facts, the description will provide information for embedding, thus the issue of KB sparsity is also well handled.",
                "We conduct extensive experiments on the tasks of triplet classification, link prediction, relational fact extraction, and analogical reasoning to compare with the previous approach #TARGET_REF .",
                "Results show that our approach consistently achieves better or comparable performance."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: We only assume some entities in KBs have text descriptions, which almost always holds in practice.\n sent1: We require the embedding of an entity not only fits the structured constraints in KBs but also equals the vector computed from the text description.\n sent2: Meanwhile, if an entity has few facts, the description will provide information for embedding, thus the issue of KB sparsity is also well handled.\n sent3: We conduct extensive experiments on the tasks of triplet classification, link prediction, relational fact extraction, and analogical reasoning to compare with the previous approach #TARGET_REF .\n sent4: Results show that our approach consistently achieves better or comparable performance.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "where",
                "Then the loss function of text model is",
                "Alignment Model This part is different from #TARGET_REF .",
                "For each word w in the description of entity e, we define Pr(w|e), the conditional probability of predicting w given e:",
                "Pr(w|e) = exp{z(e, w)} w∈V exp{z(e,w)} ,"
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: where\n sent1: Then the loss function of text model is\n sent2: Alignment Model This part is different from #TARGET_REF .\n sent3: For each word w in the description of entity e, we define Pr(w|e), the conditional probability of predicting w given e:\n sent4: Pr(w|e) = exp{z(e, w)} w∈V exp{z(e,w)} ,\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "(2) \"Jointly(desp)\" achieves the best results, especially for the case of \"Phrases\".",
                "Both \"Jointly(anchor)\" and \"Skip-gram\" only consider the context of words, while \"Jointly(desp)\" not only consider the context but also use the whole document to disambiguate words.",
                "Intuitively, the whole document is also a valuable resource to disambiguate words.",
                "(3) We further verify that \"Jointly(name)\", i.e., using entity names for alignment, indeed pollutes word embeddings, which is consistent with the reports in (#REFa) .",
                "The above four experiments are consistent in results: without using any anchor information, alignment by entity description is able to achieve better or comparable performance, compared to alignment by Wikipedia anchors proposed by #TARGET_REF ."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: (2) \"Jointly(desp)\" achieves the best results, especially for the case of \"Phrases\".\n sent1: Both \"Jointly(anchor)\" and \"Skip-gram\" only consider the context of words, while \"Jointly(desp)\" not only consider the context but also use the whole document to disambiguate words.\n sent2: Intuitively, the whole document is also a valuable resource to disambiguate words.\n sent3: (3) We further verify that \"Jointly(name)\", i.e., using entity names for alignment, indeed pollutes word embeddings, which is consistent with the reports in (#REFa) .\n sent4: The above four experiments are consistent in results: without using any anchor information, alignment by entity description is able to achieve better or comparable performance, compared to alignment by Wikipedia anchors proposed by #TARGET_REF .\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "For example, in fact extraction, a candidate value may be just a phrase in text.",
                "(2) KB sparsity.",
                "The above approaches are only based on structured facts of KBs, and thus cannot work well on entities with few facts.",
                "An important milestone, the approach of #TARGET_REF solves issue (1) by jointly embedding entities, relations, and words into the same vector space and hence is able to deal with words/phrases beyond entities in KBs.",
                "The key component is the so-called alignment model, which makes sure the embeddings of entities, relations, and words are in the same space."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For example, in fact extraction, a candidate value may be just a phrase in text.\n sent1: (2) KB sparsity.\n sent2: The above approaches are only based on structured facts of KBs, and thus cannot work well on entities with few facts.\n sent3: An important milestone, the approach of #TARGET_REF solves issue (1) by jointly embedding entities, relations, and words into the same vector space and hence is able to deal with words/phrases beyond entities in KBs.\n sent4: The key component is the so-called alignment model, which makes sure the embeddings of entities, relations, and words are in the same space.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "For a fact (h, r, t) in KBs, where h is the head entity, r is the relation, and t is the tail entity, TransE models the relation r as a translation vector r connecting the embeddings h and t of the two entities, i.e., h + r is close to t. The model is simple, effective and efficient.",
                "Most knowledge embedding models thereafter including this paper are variants of this model (#REFb; #TARGET_REF; #REF) .",
                "Skip-gram This is an efficient word embedding method proposed by Mikolov et al. (2013a) , which learns word embeddings from word concurrencies in text windows.",
                "Without any supervision, it amazingly recovers the semantic relations between words in a vector space such as 'King' − 'Queen' ≈ 'Man' − 'Women'.",
                "However, as it is unsupervised, it cannot tell the exact relation between two words."
            ],
            "label": [
                "BACKGROUND",
                "USE"
            ]
        },
        "input": "sent0: For a fact (h, r, t) in KBs, where h is the head entity, r is the relation, and t is the tail entity, TransE models the relation r as a translation vector r connecting the embeddings h and t of the two entities, i.e., h + r is close to t. The model is simple, effective and efficient.\n sent1: Most knowledge embedding models thereafter including this paper are variants of this model (#REFb; #TARGET_REF; #REF) .\n sent2: Skip-gram This is an efficient word embedding method proposed by Mikolov et al. (2013a) , which learns word embeddings from word concurrencies in text windows.\n sent3: Without any supervision, it amazingly recovers the semantic relations between words in a vector space such as 'King' − 'Queen' ≈ 'Man' − 'Women'.\n sent4: However, as it is unsupervised, it cannot tell the exact relation between two words.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Without any supervision, it amazingly recovers the semantic relations between words in a vector space such as 'King' − 'Queen' ≈ 'Man' − 'Women'.",
                "However, as it is unsupervised, it cannot tell the exact relation between two words.",
                "#TARGET_REF combines knowledge embedding and word embedding in a joint framework so that the entities/relations and words are in the same vector space and hence operators like inner product (similarity) between them are meaningful.",
                "This brings convenience to tasks requiring computation between knowledge bases and text.",
                "Meanwhile, jointly embedding utilizes information from both structured KBs and unstructured text and hence the knowledge embedding and word embedding can be enhanced by each other."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Without any supervision, it amazingly recovers the semantic relations between words in a vector space such as 'King' − 'Queen' ≈ 'Man' − 'Women'.\n sent1: However, as it is unsupervised, it cannot tell the exact relation between two words.\n sent2: #TARGET_REF combines knowledge embedding and word embedding in a joint framework so that the entities/relations and words are in the same vector space and hence operators like inner product (similarity) between them are meaningful.\n sent3: This brings convenience to tasks requiring computation between knowledge bases and text.\n sent4: Meanwhile, jointly embedding utilizes information from both structured KBs and unstructured text and hence the knowledge embedding and word embedding can be enhanced by each other.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We try to learn embeddings e i , r j and w l for each entity e i , relation r j and word w l respectively.",
                "The vocabulary of words is V. The union vocabulary of entities and words together is I = E ∪ V. In this paper \"word(s)\" refers to \"word(s)/phrase(s)\".",
                "We follow the jointly embedding framework of #TARGET_REF , i.e., learning optimal embeddings by minimizing the following loss",
                "where L K , L T and L A are the component loss functions of the knowledge model, text model and alignment model respectively.",
                "Our focus is on a new alignment model L A while the knowledge model L K and text model L T are the same as the counterparts in (#REFa) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We try to learn embeddings e i , r j and w l for each entity e i , relation r j and word w l respectively.\n sent1: The vocabulary of words is V. The union vocabulary of entities and words together is I = E ∪ V. In this paper \"word(s)\" refers to \"word(s)/phrase(s)\".\n sent2: We follow the jointly embedding framework of #TARGET_REF , i.e., learning optimal embeddings by minimizing the following loss\n sent3: where L K , L T and L A are the component loss functions of the knowledge model, text model and alignment model respectively.\n sent4: Our focus is on a new alignment model L A while the knowledge model L K and text model L T are the same as the counterparts in (#REFa) .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We try to learn embeddings e i , r j and w l for each entity e i , relation r j and word w l respectively.",
                "The vocabulary of words is V. The union vocabulary of entities and words together is I = E ∪ V. In this paper \"word(s)\" refers to \"word(s)/phrase(s)\".",
                "We follow the jointly embedding framework of (#REFa) , i.e., learning optimal embeddings by minimizing the following loss",
                "where L K , L T and L A are the component loss functions of the knowledge model, text model and alignment model respectively.",
                "Our focus is on a new alignment model L A while the knowledge model L K and text model L T are the same as the counterparts in #TARGET_REF ."
            ],
            "label": [
                "USE",
                "SIMILARITY"
            ]
        },
        "input": "sent0: We try to learn embeddings e i , r j and w l for each entity e i , relation r j and word w l respectively.\n sent1: The vocabulary of words is V. The union vocabulary of entities and words together is I = E ∪ V. In this paper \"word(s)\" refers to \"word(s)/phrase(s)\".\n sent2: We follow the jointly embedding framework of (#REFa) , i.e., learning optimal embeddings by minimizing the following loss\n sent3: where L K , L T and L A are the component loss functions of the knowledge model, text model and alignment model respectively.\n sent4: Our focus is on a new alignment model L A while the knowledge model L K and text model L T are the same as the counterparts in #TARGET_REF .\n",
        "output": "{\"label\": [\"USE\", \"SIMILARITY\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "However, to make the content self-contained, we still need to briefly explain L K and L T .",
                "Knowledge Model Describes the plausibility of a triplet (h, r, t) by defining",
                "where z(h, r, t) = b − 0.5 · h + r − t 2 2 , b = 7 as suggested by #TARGET_REF .",
                "Pr(r|h, t) and Pr(t|h, r) are defined in the same way.",
                "The loss function of knowledge model is then defined as"
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: However, to make the content self-contained, we still need to briefly explain L K and L T .\n sent1: Knowledge Model Describes the plausibility of a triplet (h, r, t) by defining\n sent2: where z(h, r, t) = b − 0.5 · h + r − t 2 2 , b = 7 as suggested by #TARGET_REF .\n sent3: Pr(r|h, t) and Pr(t|h, r) are defined in the same way.\n sent4: The loss function of knowledge model is then defined as\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The last one evaluates quality of word embeddings.",
                "We try to study whether the proposed alignment model, without using any anchor information, is able to achieve comparable or better performance than alignment by anchors.",
                "As to the methods, \"Separately\" denotes the method of separately embedding knowledge bases and text. \"Jointly(anchor)\" and \"Jointly(name)\" denote the jointly embedding methods based on Alignment by Wikipedia Anchors and Alignment by Entity Names in #TARGET_REF respectively.",
                "\"Jointly(desp)\" is the joint embedding method based on alignment by entity descriptions.",
                "Data For link prediction, FB15K from ) is used as the knowledge base."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The last one evaluates quality of word embeddings.\n sent1: We try to study whether the proposed alignment model, without using any anchor information, is able to achieve comparable or better performance than alignment by anchors.\n sent2: As to the methods, \"Separately\" denotes the method of separately embedding knowledge bases and text. \"Jointly(anchor)\" and \"Jointly(name)\" denote the jointly embedding methods based on Alignment by Wikipedia Anchors and Alignment by Entity Names in #TARGET_REF respectively.\n sent3: \"Jointly(desp)\" is the joint embedding method based on alignment by entity descriptions.\n sent4: Data For link prediction, FB15K from ) is used as the knowledge base.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Data For link prediction, FB15K from ) is used as the knowledge base.",
                "For triplet classification, a large dataset provided by #TARGET_REF is used as the knowledge base.",
                "Both sets are subsets of Freebase.",
                "For all tasks, Wikipedia articles are used as the text corpus.",
                "As many Wikipedia articles can be mapped to Freebase entities, we regard a Wikipedia article as the description for the corresponding entity in Freebase."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Data For link prediction, FB15K from ) is used as the knowledge base.\n sent1: For triplet classification, a large dataset provided by #TARGET_REF is used as the knowledge base.\n sent2: Both sets are subsets of Freebase.\n sent3: For all tasks, Wikipedia articles are used as the text corpus.\n sent4: As many Wikipedia articles can be mapped to Freebase entities, we regard a Wikipedia article as the description for the corresponding entity in Freebase.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "For all tasks, Wikipedia articles are used as the text corpus.",
                "As many Wikipedia articles can be mapped to Freebase entities, we regard a Wikipedia article as the description for the corresponding entity in Freebase.",
                "Following the settings in #TARGET_REF , we apply the same preprocessing steps, including sentence segmentation, tokenization, and named entity recognition.",
                "We combine the consecutive tokens covered by an anchor or identically tagged as \"Location/Person/Organization\" and regard them as phrases.",
                "Link Prediction This task aims to complete a fact (h, r, t) in absence of h or t, simply based on h + r − t ."
            ],
            "label": [
                "USE",
                "SIMILARITY"
            ]
        },
        "input": "sent0: For all tasks, Wikipedia articles are used as the text corpus.\n sent1: As many Wikipedia articles can be mapped to Freebase entities, we regard a Wikipedia article as the description for the corresponding entity in Freebase.\n sent2: Following the settings in #TARGET_REF , we apply the same preprocessing steps, including sentence segmentation, tokenization, and named entity recognition.\n sent3: We combine the consecutive tokens covered by an anchor or identically tagged as \"Location/Person/Organization\" and regard them as phrases.\n sent4: Link Prediction This task aims to complete a fact (h, r, t) in absence of h or t, simply based on h + r − t .\n",
        "output": "{\"label\": [\"USE\", \"SIMILARITY\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "In other settings \"Jointly(desp)\" wins.",
                "Triplet Classification This is a binary classification task, predicting whether a candidate triplet (h, r, t) is a correct fact or not.",
                "It is used in (#REF; #REFb; #REFa) .",
                "We follow the same protocol in #TARGET_REF .",
                "We train their models via our own implementation on our dataset."
            ],
            "label": [
                "USE",
                "SIMILARITY"
            ]
        },
        "input": "sent0: In other settings \"Jointly(desp)\" wins.\n sent1: Triplet Classification This is a binary classification task, predicting whether a candidate triplet (h, r, t) is a correct fact or not.\n sent2: It is used in (#REF; #REFb; #REFa) .\n sent3: We follow the same protocol in #TARGET_REF .\n sent4: We train their models via our own implementation on our dataset.\n",
        "output": "{\"label\": [\"USE\", \"SIMILARITY\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "This task is to extract facts (h, r, t) from plain text. show that combing scores from TransE and some text side base extractor achieved much better precision-recall curve compared to the base extractor.",
                "Wang et al. (2014a) confirm this observation and show that jointly embedding brings further encouraging improvement over TransE. In this experiment, we follow the same settings as (#REFa) to investigate the performance of our new alignment model.",
                "We use the same public dataset NYT+FB, released by #REF and used in and #TARGET_REF .",
                "We use Mintz (#REF ) and MIML (#REF) as our base extractors.",
                "In order to combine the score of a base extractor and the score from embeddings, we only reserve the testing triplets whose entitites and relations can be mapped to the embeddings learned from the triplet classification experiment."
            ],
            "label": [
                "USE",
                "SIMILARITY"
            ]
        },
        "input": "sent0: This task is to extract facts (h, r, t) from plain text. show that combing scores from TransE and some text side base extractor achieved much better precision-recall curve compared to the base extractor.\n sent1: Wang et al. (2014a) confirm this observation and show that jointly embedding brings further encouraging improvement over TransE. In this experiment, we follow the same settings as (#REFa) to investigate the performance of our new alignment model.\n sent2: We use the same public dataset NYT+FB, released by #REF and used in and #TARGET_REF .\n sent3: We use Mintz (#REF ) and MIML (#REF) as our base extractors.\n sent4: In order to combine the score of a base extractor and the score from embeddings, we only reserve the testing triplets whose entitites and relations can be mapped to the embeddings learned from the triplet classification experiment.\n",
        "output": "{\"label\": [\"USE\", \"SIMILARITY\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "In order to combine the score of a base extractor and the score from embeddings, we only reserve the testing triplets whose entitites and relations can be mapped to the embeddings learned from the triplet classification experiment.",
                "Since both Mintz and MIML are probabilistic models, we use the same method in #TARGET_REF to linearly combine the scores.",
                "The precision-recall curves are plot in Fig. (1) .",
                "On both base extractors, the jointly embedding methods outperform separate embedding.",
                "Moreover, \"Jointly(desp)\" is slightly better than \"Jointly(anchor)\", which is in accordance with the results from the link prediction experiment and the triplet classification experiment."
            ],
            "label": [
                "USE",
                "SIMILARITY"
            ]
        },
        "input": "sent0: In order to combine the score of a base extractor and the score from embeddings, we only reserve the testing triplets whose entitites and relations can be mapped to the embeddings learned from the triplet classification experiment.\n sent1: Since both Mintz and MIML are probabilistic models, we use the same method in #TARGET_REF to linearly combine the scores.\n sent2: The precision-recall curves are plot in Fig. (1) .\n sent3: On both base extractors, the jointly embedding methods outperform separate embedding.\n sent4: Moreover, \"Jointly(desp)\" is slightly better than \"Jointly(anchor)\", which is in accordance with the results from the link prediction experiment and the triplet classification experiment.\n",
        "output": "{\"label\": [\"USE\", \"SIMILARITY\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "On both base extractors, the jointly embedding methods outperform separate embedding.",
                "Moreover, \"Jointly(desp)\" is slightly better than \"Jointly(anchor)\", which is in accordance with the results from the link prediction experiment and the triplet classification experiment.",
                "Analogical Reasoning This task evaluates the quality of word embeddings (#REFb) .",
                "We use the original dataset released by (#REFb) and follow the same evaluation protocol of #TARGET_REF .",
                "For a true analogical pair like (\"France\", \"Paris\") and (\"China\", \"Beijing\"), we hide \"Beijing\" and predict it by selecting the word from the vocabulary whose vector has highest similarity with the vector of \"China\" + \"Paris\" -\"France\"."
            ],
            "label": [
                "USE",
                "SIMILARITY"
            ]
        },
        "input": "sent0: On both base extractors, the jointly embedding methods outperform separate embedding.\n sent1: Moreover, \"Jointly(desp)\" is slightly better than \"Jointly(anchor)\", which is in accordance with the results from the link prediction experiment and the triplet classification experiment.\n sent2: Analogical Reasoning This task evaluates the quality of word embeddings (#REFb) .\n sent3: We use the original dataset released by (#REFb) and follow the same evaluation protocol of #TARGET_REF .\n sent4: For a true analogical pair like (\"France\", \"Paris\") and (\"China\", \"Beijing\"), we hide \"Beijing\" and predict it by selecting the word from the vocabulary whose vector has highest similarity with the vector of \"China\" + \"Paris\" -\"France\".\n",
        "output": "{\"label\": [\"USE\", \"SIMILARITY\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "(2) \"Jointly(desp)\" achieves the best results, especially for the case of \"Phrases\".",
                "Both \"Jointly(anchor)\" and \"Skip-gram\" only consider the context of words, while \"Jointly(desp)\" not only consider the context but also use the whole document to disambiguate words.",
                "Intuitively, the whole document is also a valuable resource to disambiguate words.",
                "(3) We further verify that \"Jointly(name)\", i.e., using entity names for alignment, indeed pollutes word embeddings, which is consistent with the reports in #TARGET_REF .",
                "The above four experiments are consistent in results: without using any anchor information, alignment by entity description is able to achieve better or comparable performance, compared to alignment by Wikipedia anchors proposed by Wang et al. (2014a) ."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: (2) \"Jointly(desp)\" achieves the best results, especially for the case of \"Phrases\".\n sent1: Both \"Jointly(anchor)\" and \"Skip-gram\" only consider the context of words, while \"Jointly(desp)\" not only consider the context but also use the whole document to disambiguate words.\n sent2: Intuitively, the whole document is also a valuable resource to disambiguate words.\n sent3: (3) We further verify that \"Jointly(name)\", i.e., using entity names for alignment, indeed pollutes word embeddings, which is consistent with the reports in #TARGET_REF .\n sent4: The above four experiments are consistent in results: without using any anchor information, alignment by entity description is able to achieve better or comparable performance, compared to alignment by Wikipedia anchors proposed by Wang et al. (2014a) .\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "These models broadly fall into two categories: text-based and network-based methods.",
                "Orthogonally, the geolocation task can be viewed as a regression task over real-valued geographical coordinates, or a classification task over discretised region-based locations.",
                "Most previous research on user geolocation has focused either on text-based classification approaches (#REF; #REF; #TARGET_REF; #REF) or, to a lesser extent, network-based regression approaches (#REF; #REF; #TARGET_REF .",
                "Methods which combine the two, however, are rare.",
                "In this paper, we present work on Twitter user geolocation using both text and network information."
            ],
            "label": [
                "BACKGROUND",
                "MOTIVATION"
            ]
        },
        "input": "sent0: These models broadly fall into two categories: text-based and network-based methods.\n sent1: Orthogonally, the geolocation task can be viewed as a regression task over real-valued geographical coordinates, or a classification task over discretised region-based locations.\n sent2: Most previous research on user geolocation has focused either on text-based classification approaches (#REF; #REF; #TARGET_REF; #REF) or, to a lesser extent, network-based regression approaches (#REF; #REF; #TARGET_REF .\n sent3: Methods which combine the two, however, are rare.\n sent4: In this paper, we present work on Twitter user geolocation using both text and network information.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"MOTIVATION\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF and #REF used a Twitter reciprocal mention network, and geolocated users based on the geographical coordinates of their friends, by minimising the weighted distance of a given user to their friends.",
                "For a reciprocal mention network to be effective, however, a huge amount of Twitter data is required.",
                "#REF showed that this assumption could be relaxed to use an undirected mention network for smaller datasets, and still attain state-of-theart results.",
                "The greatest shortcoming of networkbased models is that they completely fail to geolocate users who are not connected to geolocated components of the graph.",
                "As shown by #TARGET_REF , geolocation predictions from text can be used as a backoff for disconnected users, but there has been little work that has investigated a more integrated text-and network-based approach to user geolocation."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #REF and #REF used a Twitter reciprocal mention network, and geolocated users based on the geographical coordinates of their friends, by minimising the weighted distance of a given user to their friends.\n sent1: For a reciprocal mention network to be effective, however, a huge amount of Twitter data is required.\n sent2: #REF showed that this assumption could be relaxed to use an undirected mention network for smaller datasets, and still attain state-of-theart results.\n sent3: The greatest shortcoming of networkbased models is that they completely fail to geolocate users who are not connected to geolocated components of the graph.\n sent4: As shown by #TARGET_REF , geolocation predictions from text can be used as a backoff for disconnected users, but there has been little work that has investigated a more integrated text-and network-based approach to user geolocation.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Orthogonally, the geolocation task can be viewed as a regression task over real-valued geographical coordinates, or a classification task over discretised region-based locations.",
                "Most previous research on user geolocation has focused either on text-based classification approaches (#REF; #REF; #REF; #REF) or, to a lesser extent, network-based regression approaches (#REF; #REF; #REF) .",
                "Methods which combine the two, however, are rare.",
                "In this paper, we present work on Twitter user geolocation using both text and network information.",
                "Our contributions are as follows: (1) we propose the use of Modified Adsorption (#REF) as a baseline networkbased geolocation model, and show that it outperforms previous network-based approaches (#REF; #TARGET_REF ; (2) we demonstrate that removing \"celebrity\" nodes (nodes with high in-degrees) from the network increases geolocation accuracy and dramatically decreases network edge size; and (3) we integrate textbased geolocation priors into Modified Adsorption, and show that our unified geolocation model outperforms both text-only and network-only approaches, and achieves state-of-the-art results over three standard datasets."
            ],
            "label": [
                "DIFFERENCES",
                "USE"
            ]
        },
        "input": "sent0: Orthogonally, the geolocation task can be viewed as a regression task over real-valued geographical coordinates, or a classification task over discretised region-based locations.\n sent1: Most previous research on user geolocation has focused either on text-based classification approaches (#REF; #REF; #REF; #REF) or, to a lesser extent, network-based regression approaches (#REF; #REF; #REF) .\n sent2: Methods which combine the two, however, are rare.\n sent3: In this paper, we present work on Twitter user geolocation using both text and network information.\n sent4: Our contributions are as follows: (1) we propose the use of Modified Adsorption (#REF) as a baseline networkbased geolocation model, and show that it outperforms previous network-based approaches (#REF; #TARGET_REF ; (2) we demonstrate that removing \"celebrity\" nodes (nodes with high in-degrees) from the network increases geolocation accuracy and dramatically decreases network edge size; and (3) we integrate textbased geolocation priors into Modified Adsorption, and show that our unified geolocation model outperforms both text-only and network-only approaches, and achieves state-of-the-art results over three standard datasets.\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Our baseline network-based model of MAD-B outperforms the text-based models and also previous network-based models (#REF; #REF; #TARGET_REF .",
                "The inference, however, is intractable for TWITTER-US and TWITTER-WORLD due to the size of the network.",
                "Celebrity removal in MADCEL-B and MADCEL-W has a positive effect on geolocation accuracy, and results in a 47% reduction in Median over GEOTEXT.",
                "It also makes graph inference over TWITTER-US and TWITTER-WORLD tractable, and results in superior Acc@161 and Median, but slightly inferior Mean, compared to the state-of-the-art results of LR, based on text-based classification (#REF) .",
                "MADCEL-W (weighted graph) outperforms MADCEL-B (binary graph) over the smaller GEOTEXT dataset where it compensates for the sparsity of network information, but doesn't improve the results for the two larger datasets where network information is denser."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Our baseline network-based model of MAD-B outperforms the text-based models and also previous network-based models (#REF; #REF; #TARGET_REF .\n sent1: The inference, however, is intractable for TWITTER-US and TWITTER-WORLD due to the size of the network.\n sent2: Celebrity removal in MADCEL-B and MADCEL-W has a positive effect on geolocation accuracy, and results in a 47% reduction in Median over GEOTEXT.\n sent3: It also makes graph inference over TWITTER-US and TWITTER-WORLD tractable, and results in superior Acc@161 and Median, but slightly inferior Mean, compared to the state-of-the-art results of LR, based on text-based classification (#REF) .\n sent4: MADCEL-W (weighted graph) outperforms MADCEL-B (binary graph) over the smaller GEOTEXT dataset where it compensates for the sparsity of network information, but doesn't improve the results for the two larger datasets where network information is denser.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "The inference, however, is intractable for TWITTER-US and TWITTER-WORLD due to the size of the network.",
                "Celebrity removal in MADCEL-B and MADCEL-W has a positive effect on geolocation accuracy, and results in a 47% reduction in Median over GEOTEXT.",
                "It also makes graph inference over TWITTER-US and TWITTER-WORLD tractable, and results in superior Acc@161 and Median, but slightly inferior Mean, compared to the state-of-the-art results of LR, based on text-based classification #TARGET_REF .",
                "MADCEL-W (weighted graph) outperforms MADCEL-B (binary graph) over the smaller GEOTEXT dataset where it compensates for the sparsity of network information, but doesn't improve the results for the two larger datasets where network information is denser.",
                "Adding text to the network-based geolocation models in the form of MADCEL-B-LR (binary edges) and MADCEL-W-LR (weighted edges), we achieve state-of-the-art results over all three datasets."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: The inference, however, is intractable for TWITTER-US and TWITTER-WORLD due to the size of the network.\n sent1: Celebrity removal in MADCEL-B and MADCEL-W has a positive effect on geolocation accuracy, and results in a 47% reduction in Median over GEOTEXT.\n sent2: It also makes graph inference over TWITTER-US and TWITTER-WORLD tractable, and results in superior Acc@161 and Median, but slightly inferior Mean, compared to the state-of-the-art results of LR, based on text-based classification #TARGET_REF .\n sent3: MADCEL-W (weighted graph) outperforms MADCEL-B (binary graph) over the smaller GEOTEXT dataset where it compensates for the sparsity of network information, but doesn't improve the results for the two larger datasets where network information is denser.\n sent4: Adding text to the network-based geolocation models in the form of MADCEL-B-LR (binary edges) and MADCEL-W-LR (weighted edges), we achieve state-of-the-art results over all three datasets.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We evaluate our models over three pre-existing geotagged Twitter datasets: (1) GEOTEXT (Eisen-stein et al., 2010), (2) TWITTER-US #TARGET_REF , and (3) TWITTER-WORLD (#REF) .",
                "In each dataset, users are represented by a single meta-document, generated by concatenating their tweets.",
                "The datasets are pre-partitioned into training, development and test sets, and rebuilt from the original version to include mention information.",
                "The first two datasets were constructed to contain mostly English messages.",
                "GEOTEXT consists of tweets from 9.5K users: 1895 users are held out for each of development and test data."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We evaluate our models over three pre-existing geotagged Twitter datasets: (1) GEOTEXT (Eisen-stein et al., 2010), (2) TWITTER-US #TARGET_REF , and (3) TWITTER-WORLD (#REF) .\n sent1: In each dataset, users are represented by a single meta-document, generated by concatenating their tweets.\n sent2: The datasets are pre-partitioned into training, development and test sets, and rebuilt from the original version to include mention information.\n sent3: The first two datasets were constructed to contain mostly English messages.\n sent4: GEOTEXT consists of tweets from 9.5K users: 1895 users are held out for each of development and test data.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "We apply celebrity removal over both binary (\"MADCEL-B\") and weighted (\"MADCEL-W\") networks (using the respective T for each dataset).",
                "The effect of celebrity removal over the development set of TWITTER-US is shown in Figure 2 where it dramatically reduces the graph edge size and simultaneously leads to an improvement in the mean error.",
                "A Unified Geolocation Model To address the issue of disconnected test users, we incorporate text information into the model by attaching a labelled dongle node to every test node (#REF; #REF) .",
                "The label for the dongle node is based on a textbased l 1 regularised logistic regression model, using the method of #TARGET_REF .",
                "The dongle nodes with their corresponding label confidences are added to the seed set, and are treated in the same way as other labelled nodes (i.e. the training nodes)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We apply celebrity removal over both binary (\"MADCEL-B\") and weighted (\"MADCEL-W\") networks (using the respective T for each dataset).\n sent1: The effect of celebrity removal over the development set of TWITTER-US is shown in Figure 2 where it dramatically reduces the graph edge size and simultaneously leads to an improvement in the mean error.\n sent2: A Unified Geolocation Model To address the issue of disconnected test users, we incorporate text information into the model by attaching a labelled dongle node to every test node (#REF; #REF) .\n sent3: The label for the dongle node is based on a textbased l 1 regularised logistic regression model, using the method of #TARGET_REF .\n sent4: The dongle nodes with their corresponding label confidences are added to the seed set, and are treated in the same way as other labelled nodes (i.e. the training nodes).\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Following #REF and #REF , we evaluate using the mean and median error (in km) over all test users (\"Mean\" and \"Median\", resp.), and also accuracy within 161km of the actual location (\"Acc@161\").",
                "Note that higher numbers are better for Acc@161, but lower numbers are better for mean and median error, with a lower bound of 0 and no (theoretical) upper bound.",
                "To generate a continuous-valued latitude/longitude coordinate for a given user from the k-d tree cell, we use the median coordinates of all training points in the predicted region.",
                "Table 1 shows the performance of MAD-B, MADCEL-B, MADCEL-W, MADCEL-B-LR and MADCEL-W-LR over the GEOTEXT, TWITTER-US and TWITTER-WORLD datasets.",
                "The results are also compared with prior work on network-based geolocation using label propagation (LP) #TARGET_REF , text-based classification models (#REF; #REF; #TARGET_REF; #REF) , textbased graphical models (#REF) , and network-text hybrid models (LP-LR) #TARGET_REF ."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: Following #REF and #REF , we evaluate using the mean and median error (in km) over all test users (\"Mean\" and \"Median\", resp.), and also accuracy within 161km of the actual location (\"Acc@161\").\n sent1: Note that higher numbers are better for Acc@161, but lower numbers are better for mean and median error, with a lower bound of 0 and no (theoretical) upper bound.\n sent2: To generate a continuous-valued latitude/longitude coordinate for a given user from the k-d tree cell, we use the median coordinates of all training points in the predicted region.\n sent3: Table 1 shows the performance of MAD-B, MADCEL-B, MADCEL-W, MADCEL-B-LR and MADCEL-W-LR over the GEOTEXT, TWITTER-US and TWITTER-WORLD datasets.\n sent4: The results are also compared with prior work on network-based geolocation using label propagation (LP) #TARGET_REF , text-based classification models (#REF; #REF; #TARGET_REF; #REF) , textbased graphical models (#REF) , and network-text hybrid models (LP-LR) #TARGET_REF .\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "However, the main focus is to identify only the \"correct\" answers that support a given question.",
                "Since the ability to refute is as important as to support, it does not fully address the verification problem of factchecking.",
                "Recently, #TARGET_REF proposed a public dataset to explore the complete process of the large-scale fact-checking.",
                "It is designed not only to verify claims but also to extract sets of related evidence.",
                "Nevertheless, the pipeline solution proposed in that paper suffers from following problems: 1) The overall performance (30.88% accuracy) still needs further improvement to be applicable to the evidence selection and classification, which also highlights the challenging nature of this task."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: However, the main focus is to identify only the \"correct\" answers that support a given question.\n sent1: Since the ability to refute is as important as to support, it does not fully address the verification problem of factchecking.\n sent2: Recently, #TARGET_REF proposed a public dataset to explore the complete process of the large-scale fact-checking.\n sent3: It is designed not only to verify claims but also to extract sets of related evidence.\n sent4: Nevertheless, the pipeline solution proposed in that paper suffers from following problems: 1) The overall performance (30.88% accuracy) still needs further improvement to be applicable to the evidence selection and classification, which also highlights the challenging nature of this task.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "In this module, l sentences are extracted as possible evidence for the claim.",
                "Instead of selecting the sentences by recomputing sentence-level TF-IDF features between claim and document text as in #TARGET_REF , we propose a neural ranker using decomposable attention (DA) model (#REF) to perform evidence selection.",
                "DA model does not require the input text to be parsed syntactically, nor is an ensemble, and it is faster without any recurrent structure.",
                "In general, using neural methods is better for the following reasons: 1) The TF-IDF may have limited ability to capture semantics compared to word representation learning 2) Faster inference time compared to TF-IDF methods that need real-time reconstruction.",
                "The neural ranker DA rank is trained using a fake task, which is to classify whether a given sentence is an evidence of a given claim or not."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: In this module, l sentences are extracted as possible evidence for the claim.\n sent1: Instead of selecting the sentences by recomputing sentence-level TF-IDF features between claim and document text as in #TARGET_REF , we propose a neural ranker using decomposable attention (DA) model (#REF) to perform evidence selection.\n sent2: DA model does not require the input text to be parsed syntactically, nor is an ensemble, and it is faster without any recurrent structure.\n sent3: In general, using neural methods is better for the following reasons: 1) The TF-IDF may have limited ability to capture semantics compared to word representation learning 2) Faster inference time compared to TF-IDF methods that need real-time reconstruction.\n sent4: The neural ranker DA rank is trained using a fake task, which is to classify whether a given sentence is an evidence of a given claim or not.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Given a claim and l possible evidence, a DA rte classifier is trained to recognize the textual entailment to be support, refute or not enough information to verify (NEI).",
                "Same as #TARGET_REF , we use the decomposable attention (DA) between the claim and the evidence for RTE.",
                "DA model decomposes the RTE problem into subproblems, which can be considered as bi-direction wordlevel attention features.",
                "Note that the DA model is utilized over other models such as as Chen et al. (2017b) ; #REF , because it is a simple but effective model.",
                "Our DA rte model must correctly decide whether a claim is NEI, when the evidence retrieved is irrelevant and insufficient."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: Given a claim and l possible evidence, a DA rte classifier is trained to recognize the textual entailment to be support, refute or not enough information to verify (NEI).\n sent1: Same as #TARGET_REF , we use the decomposable attention (DA) between the claim and the evidence for RTE.\n sent2: DA model decomposes the RTE problem into subproblems, which can be considered as bi-direction wordlevel attention features.\n sent3: Note that the DA model is utilized over other models such as as Chen et al. (2017b) ; #REF , because it is a simple but effective model.\n sent4: Our DA rte model must correctly decide whether a claim is NEI, when the evidence retrieved is irrelevant and insufficient.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Note that the DA model is utilized over other models such as as Chen et al. (2017b) ; #REF , because it is a simple but effective model.",
                "Our DA rte model must correctly decide whether a claim is NEI, when the evidence retrieved is irrelevant and insufficient.",
                "However, NEI claims have no annotated evidence, thus cannot be used to train RTE.",
                "To overcome this issue, same as #TARGET_REF , the most probable NEI evidence are simulated by sampling sentences from the nearest page to the claim using the document retrieval module.",
                "MLP DA rte DA rte +NER Accuracy (%) 63.2 78.4 79.9 Table 4 : Oracle RTE classification accuracy in the test set using gold evidence."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: Note that the DA model is utilized over other models such as as Chen et al. (2017b) ; #REF , because it is a simple but effective model.\n sent1: Our DA rte model must correctly decide whether a claim is NEI, when the evidence retrieved is irrelevant and insufficient.\n sent2: However, NEI claims have no annotated evidence, thus cannot be used to train RTE.\n sent3: To overcome this issue, same as #TARGET_REF , the most probable NEI evidence are simulated by sampling sentences from the nearest page to the claim using the document retrieval module.\n sent4: MLP DA rte DA rte +NER Accuracy (%) 63.2 78.4 79.9 Table 4 : Oracle RTE classification accuracy in the test set using gold evidence.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Some works have investigated fact verification using PolitiFact data (#REF; #REF) or FakeNews challenge (Pomerleau and Rao) .",
                "Most closely related to our work, #TARGET_REF addresses large-scale fact extraction and verification task using a pipeline approach.",
                "In addition, question answering (#REF; #REFa; #REF; #REF) and task-oriented dialog systems (#REF;  Table 7 : Full-pipeline evaluation on the test set using k = 2 and th = 0.6.",
                "The first and the second one (with *) are the baselines from #REF .",
                "#REF) also have similar aspects to these works, although aiming at a different goal."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: Some works have investigated fact verification using PolitiFact data (#REF; #REF) or FakeNews challenge (Pomerleau and Rao) .\n sent1: Most closely related to our work, #TARGET_REF addresses large-scale fact extraction and verification task using a pipeline approach.\n sent2: In addition, question answering (#REF; #REFa; #REF; #REF) and task-oriented dialog systems (#REF;  Table 7 : Full-pipeline evaluation on the test set using k = 2 and th = 0.6.\n sent3: The first and the second one (with *) are the baselines from #REF .\n sent4: #REF) also have similar aspects to these works, although aiming at a different goal.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Dataset: FEVER dataset #TARGET_REF ) is a relatively large-scale dataset compared to other previous fact extraction and verification works, with around 5.4M Wikipedia documents and 185k samples.",
                "The claims are generated by altering sentences extracted from Wikipedia, with humanannotated evidence sentences and verification labels (e.g. Table 1 ).",
                "The training/validation/test sets of these three datasets are split in advance by the providers.",
                "Note that the test-set was equally split into 3 classes: #REF, #REF, NEI (3333).",
                "Training: We trained our models end-to-end using Adagrad optimizer (#REF) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Dataset: FEVER dataset #TARGET_REF ) is a relatively large-scale dataset compared to other previous fact extraction and verification works, with around 5.4M Wikipedia documents and 185k samples.\n sent1: The claims are generated by altering sentences extracted from Wikipedia, with humanannotated evidence sentences and verification labels (e.g. Table 1 ).\n sent2: The training/validation/test sets of these three datasets are split in advance by the providers.\n sent3: Note that the test-set was equally split into 3 classes: #REF, #REF, NEI (3333).\n sent4: Training: We trained our models end-to-end using Adagrad optimizer (#REF) .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Evaluation: For each module, we independently measure oracle performance, where we assume gold standard documents and set of evidence are provided (oracle evaluation).",
                "For the final fullpipeline, we compare to and follow the metric defined in #TARGET_REF .",
                "NoScoreEv is a simple classification accuracy that only considers the correctness of the verification label.",
                "On the other hand, ScoreEv is a stricter measure that also considers the correctness of the retrieved evidence.",
                "Hence, it is a more meaningful measure because it considers the classification to be correct only if appropriate evidence is provided to justify the classification."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Evaluation: For each module, we independently measure oracle performance, where we assume gold standard documents and set of evidence are provided (oracle evaluation).\n sent1: For the final fullpipeline, we compare to and follow the metric defined in #TARGET_REF .\n sent2: NoScoreEv is a simple classification accuracy that only considers the correctness of the verification label.\n sent3: On the other hand, ScoreEv is a stricter measure that also considers the correctness of the retrieved evidence.\n sent4: Hence, it is a more meaningful measure because it considers the classification to be correct only if appropriate evidence is provided to justify the classification.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "This is because providing a succinct set of evidence makes the verification task easier for the RTE module.",
                "Therefore, we choose DA rank +NER model with th = 0.6 for the fullpipeline.",
                "Recognizing Textual Entailment: The oracle classification accuracy for RTE is shown in Table 4.",
                "The MLP is a simple multi-layer perceptron using TF and TF-IDF cosine similarity between the claim and evidence as features as shown in #TARGET_REF .",
                "The highest accuracy achieved is 79.9% using DA rte with NER information, thus, we further evaluate the full-pipeline accuracy on this setting."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This is because providing a succinct set of evidence makes the verification task easier for the RTE module.\n sent1: Therefore, we choose DA rank +NER model with th = 0.6 for the fullpipeline.\n sent2: Recognizing Textual Entailment: The oracle classification accuracy for RTE is shown in Table 4.\n sent3: The MLP is a simple multi-layer perceptron using TF and TF-IDF cosine similarity between the claim and evidence as features as shown in #TARGET_REF .\n sent4: The highest accuracy achieved is 79.9% using DA rte with NER information, thus, we further evaluate the full-pipeline accuracy on this setting.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Recently, translation scholars have made some general claims about translation properties.",
                "Some of these are source language independent while others are not.",
                "#TARGET_REF performed empirical studies to validate both types of properties using English source texts and other texts translated into English.",
                "Obviously, corpora of this sort, which focus on a single language, are not adequate for claiming universality of translation properties.",
                "In this paper, we are validating both types of translation properties using original and translated texts from six European languages."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Recently, translation scholars have made some general claims about translation properties.\n sent1: Some of these are source language independent while others are not.\n sent2: #TARGET_REF performed empirical studies to validate both types of properties using English source texts and other texts translated into English.\n sent3: Obviously, corpora of this sort, which focus on a single language, are not adequate for claiming universality of translation properties.\n sent4: In this paper, we are validating both types of translation properties using original and translated texts from six European languages.\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent0\", \"sent1\", \"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF have built a classifier that can identify the correct source of the translated text (given different possible source languages).",
                "They have built another classifier which can identify source text and translated text.",
                "Furthermore, they have shown that the degree of difference between two translated texts, translated from two different languages into the same target language reflects, the degree of difference of the source languages.",
                "They have gained impressive results for both of the tasks.",
                "However, the limitation of this study is that they only used a corpus of English original text and English text translated from various European languages."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: #TARGET_REF have built a classifier that can identify the correct source of the translated text (given different possible source languages).\n sent1: They have built another classifier which can identify source text and translated text.\n sent2: Furthermore, they have shown that the degree of difference between two translated texts, translated from two different languages into the same target language reflects, the degree of difference of the source languages.\n sent3: They have gained impressive results for both of the tasks.\n sent4: However, the limitation of this study is that they only used a corpus of English original text and English text translated from various European languages.\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent0\", \"sent1\", \"sent2\", \"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "The system works on a character level rather than on a word level.",
                "The system performs poorly when the source language of the training corpus is different from the one of the test corpus.",
                "We can not compare our findings directly with #TARGET_REF even though we use text from the same corpus and similar techniques.",
                "The English language is not considered for this study due to unavailability of English translations for some languages included in this work.",
                "Furthermore, instead of the list of 300 function words used by #REF , we used the 100 most frequent words for each candidate language."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: The system works on a character level rather than on a word level.\n sent1: The system performs poorly when the source language of the training corpus is different from the one of the test corpus.\n sent2: We can not compare our findings directly with #TARGET_REF even though we use text from the same corpus and similar techniques.\n sent3: The English language is not considered for this study due to unavailability of English translations for some languages included in this work.\n sent4: Furthermore, instead of the list of 300 function words used by #REF , we used the 100 most frequent words for each candidate language.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "However, this corpus is not suitable for the experiment we are performing here.",
                "We extract a suitable corpus from the Europarl corpus in a way similar to #REF and #REF .",
                "Our target is to extract texts that are translated from and to the languages considered here.",
                "We trust the source language marker that has been put by the respective translator, as did #REF and #TARGET_REF .",
                "To experiment with stylistic differences in translated text, a list of function words and their"
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: However, this corpus is not suitable for the experiment we are performing here.\n sent1: We extract a suitable corpus from the Europarl corpus in a way similar to #REF and #REF .\n sent2: Our target is to extract texts that are translated from and to the languages considered here.\n sent3: We trust the source language marker that has been put by the respective translator, as did #REF and #TARGET_REF .\n sent4: To experiment with stylistic differences in translated text, a list of function words and their\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Table 1 shows the statistics of the corpus used for source language identification experiments.",
                "Later, each corpus is divided into a number of chunks (see Table 2 ).",
                "Each chunk contains at least seven sentences.",
                "Our hypothesis is again similar to #TARGET_REF , that is, if the classifier's accuracy is close to 20%, then we cannot say that there is an interference effect in translated text.",
                "If the classifier's accuracy is close to 100% then our conclusion will be that interference effects exist in translated text."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: Table 1 shows the statistics of the corpus used for source language identification experiments.\n sent1: Later, each corpus is divided into a number of chunks (see Table 2 ).\n sent2: Each chunk contains at least seven sentences.\n sent3: Our hypothesis is again similar to #TARGET_REF , that is, if the classifier's accuracy is close to 20%, then we cannot say that there is an interference effect in translated text.\n sent4: If the classifier's accuracy is close to 100% then our conclusion will be that interference effects exist in translated text.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "The results show that training a classifier based on the 100 most frequent words of a language is sufficient to obtain interpretable results.",
                "We find our results to be compatible with #TARGET_REF who used 300 function words.",
                "A list of the 100 most frequent words is easily obtainable for a vast number of languages, while lists consisting strictly of function words are rare and cannot be produced without considerable additional effort.",
                "While the 100 most frequent words of a language are sufficient to train a classifier for Germanic or Romance languages, it fails to perform equally well for Slavic languages.",
                "#REF claim that Toury's (1995) findings of interference of a translation hold true; we find the assumption to be too simplistic, since for Slavic text either as a source or target language this statement cannot supported."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: The results show that training a classifier based on the 100 most frequent words of a language is sufficient to obtain interpretable results.\n sent1: We find our results to be compatible with #TARGET_REF who used 300 function words.\n sent2: A list of the 100 most frequent words is easily obtainable for a vast number of languages, while lists consisting strictly of function words are rare and cannot be produced without considerable additional effort.\n sent3: While the 100 most frequent words of a language are sufficient to train a classifier for Germanic or Romance languages, it fails to perform equally well for Slavic languages.\n sent4: #REF claim that Toury's (1995) findings of interference of a translation hold true; we find the assumption to be too simplistic, since for Slavic text either as a source or target language this statement cannot supported.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The system works on a character level rather than on a word level.",
                "The system performs poorly when the source language of the training corpus is different from the one of the test corpus.",
                "We can not compare our findings directly with #REF even though we use text from the same corpus and similar techniques.",
                "The English language is not considered for this study due to unavailability of English translations for some languages included in this work.",
                "Furthermore, instead of the list of 300 function words used by #TARGET_REF , we used the 100 most frequent words for each candidate language."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: The system works on a character level rather than on a word level.\n sent1: The system performs poorly when the source language of the training corpus is different from the one of the test corpus.\n sent2: We can not compare our findings directly with #REF even though we use text from the same corpus and similar techniques.\n sent3: The English language is not considered for this study due to unavailability of English translations for some languages included in this work.\n sent4: Furthermore, instead of the list of 300 function words used by #TARGET_REF , we used the 100 most frequent words for each candidate language.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "That is, 80% of the data is randomly extracted for training and the rest of the data is used for testing.",
                "Expected F-Scores are calculated from 100 samples.",
                "Table 5 shows the evaluation results.",
                "Even though the classifier for German achieves around 99% accuracy, we cannot compare the result with #TARGET_REF as the amount of chunks for the classes are different.",
                "The classifiers for other languages also display very high accuracy."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: That is, 80% of the data is randomly extracted for training and the rest of the data is used for testing.\n sent1: Expected F-Scores are calculated from 100 samples.\n sent2: Table 5 shows the evaluation results.\n sent3: Even though the classifier for German achieves around 99% accuracy, we cannot compare the result with #TARGET_REF as the amount of chunks for the classes are different.\n sent4: The classifiers for other languages also display very high accuracy.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "A customized version of the Europarl corpus (#REF ) is freely available for corpus-based translation studies.",
                "However, this corpus is not suitable for the experiment we are performing here.",
                "We extract a suitable corpus from the Europarl corpus in a way similar to #REF and #TARGET_REF .",
                "Our target is to extract texts that are translated from and to the languages considered here.",
                "We trust the source language marker that has been put by the respective translator, as did #REF and #REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: A customized version of the Europarl corpus (#REF ) is freely available for corpus-based translation studies.\n sent1: However, this corpus is not suitable for the experiment we are performing here.\n sent2: We extract a suitable corpus from the Europarl corpus in a way similar to #REF and #TARGET_REF .\n sent3: Our target is to extract texts that are translated from and to the languages considered here.\n sent4: We trust the source language marker that has been put by the respective translator, as did #REF and #REF .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "A list of the 100 most frequent words is easily obtainable for a vast number of languages, while lists consisting strictly of function words are rare and cannot be produced without considerable additional effort.",
                "While the 100 most frequent words of a language are sufficient to train a classifier for Germanic or Romance languages, it fails to perform equally well for Slavic languages.",
                "#TARGET_REF claim that Toury's (1995) findings of interference of a translation hold true; we find the assumption to be too simplistic, since for Slavic text either as a source or target language this statement cannot supported.",
                "Although function words do exist in all the languages we examined, the language families differ in the degree to which it is necessary to use them.",
                "For instance, French lacks a case system (#REF) , and makes instead use of prepositions."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: A list of the 100 most frequent words is easily obtainable for a vast number of languages, while lists consisting strictly of function words are rare and cannot be produced without considerable additional effort.\n sent1: While the 100 most frequent words of a language are sufficient to train a classifier for Germanic or Romance languages, it fails to perform equally well for Slavic languages.\n sent2: #TARGET_REF claim that Toury's (1995) findings of interference of a translation hold true; we find the assumption to be too simplistic, since for Slavic text either as a source or target language this statement cannot supported.\n sent3: Although function words do exist in all the languages we examined, the language families differ in the degree to which it is necessary to use them.\n sent4: For instance, French lacks a case system (#REF) , and makes instead use of prepositions.\n",
        "output": "{\"label\": [null], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Typically, each sentence carries a full message and transmits an idea in contrast with an isolated word.",
                "Therefore, mapping a text into a time series of sentence lengths is a natural way to investigate text structures.",
                "Recent methods applied to study texts at sentence level include probability distributions [24, 25, 26, 27] and correlations [26, 27, 17, #TARGET_REF .",
                "In recent studies, sentence length analysis have been related to style and authorship [29, 26, 27] .",
                "In general, sentence lengths have been quantified by the number of words [24, 29, 25, 28] or characters [30, 31, 26, 27] ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Typically, each sentence carries a full message and transmits an idea in contrast with an isolated word.\n sent1: Therefore, mapping a text into a time series of sentence lengths is a natural way to investigate text structures.\n sent2: Recent methods applied to study texts at sentence level include probability distributions [24, 25, 26, 27] and correlations [26, 27, 17, #TARGET_REF .\n sent3: In recent studies, sentence length analysis have been related to style and authorship [29, 26, 27] .\n sent4: In general, sentence lengths have been quantified by the number of words [24, 29, 25, 28] or characters [30, 31, 26, 27] .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Therefore, mapping a text into a time series of sentence lengths is a natural way to investigate text structures.",
                "Recent methods applied to study texts at sentence level include probability distributions [24, 25, 26, 27] and correlations [26, 27, 17, 28] .",
                "In recent studies, sentence length analysis have been related to style and authorship [29, 26, 27] .",
                "In general, sentence lengths have been quantified by the number of words [24, 29, 25, #TARGET_REF or characters [30, 31, 26, 27] .",
                "Also, note that the recurrence time of full stops also quantifies the sentence length [3] ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Therefore, mapping a text into a time series of sentence lengths is a natural way to investigate text structures.\n sent1: Recent methods applied to study texts at sentence level include probability distributions [24, 25, 26, 27] and correlations [26, 27, 17, 28] .\n sent2: In recent studies, sentence length analysis have been related to style and authorship [29, 26, 27] .\n sent3: In general, sentence lengths have been quantified by the number of words [24, 29, 25, #TARGET_REF or characters [30, 31, 26, 27] .\n sent4: Also, note that the recurrence time of full stops also quantifies the sentence length [3] .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "All comparisons for all books rejected the null hypothesis for these tests.",
                "Before concluding this subsection, we discuss Fig. 2a in connection with the MenzerathAltmann law.",
                "Basically, this law states that the bigger the whole, the smaller its parts and vice-versa.",
                "However, an issue about this relationship was addressed in [37] : \"Somewhat more problematic is the relation of sentence length to the word length.\" This comment is consistent with the one in #TARGET_REF asserting that the Menzerath-Altmann law does not hold if the sentence length is measured in terms of characters instead of the number of words.",
                "In a similar way, the data of Fig. 2a indicates, in the context of this law, that the relationship between the number of words and characters is also problematic."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: All comparisons for all books rejected the null hypothesis for these tests.\n sent1: Before concluding this subsection, we discuss Fig. 2a in connection with the MenzerathAltmann law.\n sent2: Basically, this law states that the bigger the whole, the smaller its parts and vice-versa.\n sent3: However, an issue about this relationship was addressed in [37] : \"Somewhat more problematic is the relation of sentence length to the word length.\" This comment is consistent with the one in #TARGET_REF asserting that the Menzerath-Altmann law does not hold if the sentence length is measured in terms of characters instead of the number of words.\n sent4: In a similar way, the data of Fig. 2a indicates, in the context of this law, that the relationship between the number of words and characters is also problematic.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "We can infer that h differs very little from one series to another and that their values are close to 0.8, with h * ∼ 0.5, implying in long-range correlations.",
                "All the series from the other books reflects this behavior (h ∼ 0.75).",
                "This result is consistent with the multifractal analysis performed in #TARGET_REF .",
                "In addition, the values for the difference between Hurst exponents (∆h) of a given book are shown in Fig. 4c , where we can observe that the variation of the Hurst exponents is small from one series to another.",
                "Also, the standard deviation for h within each book was close to 0.001."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: We can infer that h differs very little from one series to another and that their values are close to 0.8, with h * ∼ 0.5, implying in long-range correlations.\n sent1: All the series from the other books reflects this behavior (h ∼ 0.75).\n sent2: This result is consistent with the multifractal analysis performed in #TARGET_REF .\n sent3: In addition, the values for the difference between Hurst exponents (∆h) of a given book are shown in Fig. 4c , where we can observe that the variation of the Hurst exponents is small from one series to another.\n sent4: Also, the standard deviation for h within each book was close to 0.001.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The present work differs from #REF in two respects, viz. we focus solely on textual information and we directly address the problem of tutorial dialogue.",
                "In this study we apply the methods of #REF , #TARGET_REF Hearst ( , 1997 , and a new technique utilizing an orthonormal basis to topic segmentation of tutorial dialogue.",
                "All three are vector space methods that measure lexical cohesion to determine topic shifts.",
                "Our results show that the new using an orthonormal basis significantly outperforms the other methods.",
                "Section 2 reviews previous work, and Section 3 reviews the vector space model."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The present work differs from #REF in two respects, viz. we focus solely on textual information and we directly address the problem of tutorial dialogue.\n sent1: In this study we apply the methods of #REF , #TARGET_REF Hearst ( , 1997 , and a new technique utilizing an orthonormal basis to topic segmentation of tutorial dialogue.\n sent2: All three are vector space methods that measure lexical cohesion to determine topic shifts.\n sent3: Our results show that the new using an orthonormal basis significantly outperforms the other methods.\n sent4: Section 2 reviews previous work, and Section 3 reviews the vector space model.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The JTextTile software was used to implement #TARGET_REF on dialogue.",
                "As with #REF , a text unit and window size had to be determined for dialogue.",
                "#REF recommends using the average paragraph size as the window size.",
                "Using the development corpus's average topic length of 16 utterances as a reference point, F-measures were calculated for the combinations of window size and text unit size in Table 1 .",
                "The optimal combination of parameters (Fmeasure = .17) is a unit size of 16 words and a window size of 16 units."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The JTextTile software was used to implement #TARGET_REF on dialogue.\n sent1: As with #REF , a text unit and window size had to be determined for dialogue.\n sent2: #REF recommends using the average paragraph size as the window size.\n sent3: Using the development corpus's average topic length of 16 utterances as a reference point, F-measures were calculated for the combinations of window size and text unit size in Table 1 .\n sent4: The optimal combination of parameters (Fmeasure = .17) is a unit size of 16 words and a window size of 16 units.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "The optimal combination of parameters (Fmeasure = .17) is a unit size of 16 words and a window size of 16 units.",
                "This combination matches #TARGET_REF 's heuristic of choosing the window size to be the average paragraph length.",
                "On the test set, this combination of parameters yielded an F-measure of .14 as opposed to the Fmeasure for monologue reported by #REF , .70.",
                "For dialogue, the algorithm is 20% as effective as it is for monologue.",
                "It is unclear, however, exactly what part of the algorithm contributes to this poor performance."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The optimal combination of parameters (Fmeasure = .17) is a unit size of 16 words and a window size of 16 units.\n sent1: This combination matches #TARGET_REF 's heuristic of choosing the window size to be the average paragraph length.\n sent2: On the test set, this combination of parameters yielded an F-measure of .14 as opposed to the Fmeasure for monologue reported by #REF , .70.\n sent3: For dialogue, the algorithm is 20% as effective as it is for monologue.\n sent4: It is unclear, however, exactly what part of the algorithm contributes to this poor performance.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "An implementation must define how to represent units of text, compare the cohesion between units, and determine whether the results of comparison indicate a new text segment.",
                "Both #TARGET_REF Hearst ( , 1997 and #REF use vector space methods discussed below to represent and compare units of text.",
                "The comparisons can be characterized by a moving window, where successive overlapping comparisons are advanced by one unit of text.",
                "However, Hearst (1994 Hearst ( , 1997 and #REF differ on how text units are defined and on how to interpret the results of a comparison.",
                "The text unit's definition in Hearst (1994 Hearst ( , 1997 and #REF is generally task dependent, depending on what size gives the best results."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: An implementation must define how to represent units of text, compare the cohesion between units, and determine whether the results of comparison indicate a new text segment.\n sent1: Both #TARGET_REF Hearst ( , 1997 and #REF use vector space methods discussed below to represent and compare units of text.\n sent2: The comparisons can be characterized by a moving window, where successive overlapping comparisons are advanced by one unit of text.\n sent3: However, Hearst (1994 Hearst ( , 1997 and #REF differ on how text units are defined and on how to interpret the results of a comparison.\n sent4: The text unit's definition in Hearst (1994 Hearst ( , 1997 and #REF is generally task dependent, depending on what size gives the best results.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The comparisons can be characterized by a moving window, where successive overlapping comparisons are advanced by one unit of text.",
                "However, #TARGET_REF Hearst ( , 1997 and #REF differ on how text units are defined and on how to interpret the results of a comparison.",
                "The text unit's definition in Hearst (1994 Hearst ( , 1997 and #REF is generally task dependent, depending on what size gives the best results.",
                "For example, when measuring comprehension, use the unit of the sentence, as opposed to the more standard unit of the proposition, because LSA is most correlated with comprehension at that level.",
                "However, when using LSA to segment text, #REF use the paragraph as the unit, to \"smooth out\" the local changes in cohesion and become more sensitive to more global changes of cohesion."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The comparisons can be characterized by a moving window, where successive overlapping comparisons are advanced by one unit of text.\n sent1: However, #TARGET_REF Hearst ( , 1997 and #REF differ on how text units are defined and on how to interpret the results of a comparison.\n sent2: The text unit's definition in Hearst (1994 Hearst ( , 1997 and #REF is generally task dependent, depending on what size gives the best results.\n sent3: For example, when measuring comprehension, use the unit of the sentence, as opposed to the more standard unit of the proposition, because LSA is most correlated with comprehension at that level.\n sent4: However, when using LSA to segment text, #REF use the paragraph as the unit, to \"smooth out\" the local changes in cohesion and become more sensitive to more global changes of cohesion.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "An implementation must define how to represent units of text, compare the cohesion between units, and determine whether the results of comparison indicate a new text segment.",
                "Both Hearst (1994 Hearst ( , 1997 and #REF use vector space methods discussed below to represent and compare units of text.",
                "The comparisons can be characterized by a moving window, where successive overlapping comparisons are advanced by one unit of text.",
                "However, Hearst (1994 Hearst ( , 1997 and #REF differ on how text units are defined and on how to interpret the results of a comparison.",
                "The text unit's definition in #TARGET_REF Hearst ( , 1997 and #REF is generally task dependent, depending on what size gives the best results."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: An implementation must define how to represent units of text, compare the cohesion between units, and determine whether the results of comparison indicate a new text segment.\n sent1: Both Hearst (1994 Hearst ( , 1997 and #REF use vector space methods discussed below to represent and compare units of text.\n sent2: The comparisons can be characterized by a moving window, where successive overlapping comparisons are advanced by one unit of text.\n sent3: However, Hearst (1994 Hearst ( , 1997 and #REF differ on how text units are defined and on how to interpret the results of a comparison.\n sent4: The text unit's definition in #TARGET_REF Hearst ( , 1997 and #REF is generally task dependent, depending on what size gives the best results.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "However, when using LSA to segment text, #REF use the paragraph as the unit, to \"smooth out\" the local changes in cohesion and become more sensitive to more global changes of cohesion.",
                "Hearst likewise chooses a large unit, 6 token-sequences of 20 tokens #TARGET_REF , but varies these parameters dependent on the characteristics of the text to be segmented, e.g. paragraph size.",
                "Under a vector space model, comparisons are performed by calculating the cosine of vectors representing text.",
                "As stated previously, these comparisons reflect the cohesion between units of text.",
                "In order to use these comparisons to segment text, however, one must have a criterion in place."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: However, when using LSA to segment text, #REF use the paragraph as the unit, to \"smooth out\" the local changes in cohesion and become more sensitive to more global changes of cohesion.\n sent1: Hearst likewise chooses a large unit, 6 token-sequences of 20 tokens #TARGET_REF , but varies these parameters dependent on the characteristics of the text to be segmented, e.g. paragraph size.\n sent2: Under a vector space model, comparisons are performed by calculating the cosine of vectors representing text.\n sent3: As stated previously, these comparisons reflect the cohesion between units of text.\n sent4: In order to use these comparisons to segment text, however, one must have a criterion in place.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "For example, a moving window size greater than 16 utterances implies that, in the majority of occurrences, the moving window straddles three topics as opposed to the desired two.",
                "To replicate #REF , software was written in Java that created a moving window of varying sizes on the input text, and the software retrieved the LSA vector and calculated the cosine of each window.",
                "Hearst (1994 Hearst ( , 1997 was replicated using the JTextTile (#REF ) Java software.",
                "A variant of #TARGET_REF Hearst ( , 1997 was created by using LSA instead of the standard vector space method.",
                "The orthonormal basis method also used a moving window; however, in contrast to the previous methods, the window is not treated just as a large block of text."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: For example, a moving window size greater than 16 utterances implies that, in the majority of occurrences, the moving window straddles three topics as opposed to the desired two.\n sent1: To replicate #REF , software was written in Java that created a moving window of varying sizes on the input text, and the software retrieved the LSA vector and calculated the cosine of each window.\n sent2: Hearst (1994 Hearst ( , 1997 was replicated using the JTextTile (#REF ) Java software.\n sent3: A variant of #TARGET_REF Hearst ( , 1997 was created by using LSA instead of the standard vector space method.\n sent4: The orthonormal basis method also used a moving window; however, in contrast to the previous methods, the window is not treated just as a large block of text.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "While this result is not statistically significant, the direction of the result supports the use of an \"inferencing\" vector space method for segmenting dialogue.",
                "Unfortunately, the large difference in F-measure between the Foltz algorithm and the Hearst + LSA algorithm is more difficult to explain.",
                "These two methods differ by their segmentation criterion and by their training (Foltz is a regression model and Hearst is not).",
                "It may be that #TARGET_REF Hearst ( , 1997 )'s segmentation criterion, i.e. depth scores, do not translate well to dialogue.",
                "Perhaps the assignment of segment boundaries based on the relative difference between a candidate score and its surrounding peaks is highly sensitive to cohesion gaps created by conversational implicatures."
            ],
            "label": [
                "DIFFERENCES",
                null
            ]
        },
        "input": "sent0: While this result is not statistically significant, the direction of the result supports the use of an \"inferencing\" vector space method for segmenting dialogue.\n sent1: Unfortunately, the large difference in F-measure between the Foltz algorithm and the Hearst + LSA algorithm is more difficult to explain.\n sent2: These two methods differ by their segmentation criterion and by their training (Foltz is a regression model and Hearst is not).\n sent3: It may be that #TARGET_REF Hearst ( , 1997 )'s segmentation criterion, i.e. depth scores, do not translate well to dialogue.\n sent4: Perhaps the assignment of segment boundaries based on the relative difference between a candidate score and its surrounding peaks is highly sensitive to cohesion gaps created by conversational implicatures.\n",
        "output": "{\"label\": [\"DIFFERENCES\", null], \"context\": [\"sent1\", \"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "To be able to answer the questions \"What causes ebola?\", \"What are the duties of a medical doctor?\", \"What are the differences between a terrorist and a victim?\", \"Which are the animals that have wings but cannot fly?\" one requires knowledge about verb-based relations.",
                "Over the years, researchers have developed various relation learning algorithms.",
                "Some (#REF; #REF) targeted specific relations like BornInYear, CorporationAcquired, others (#REF; #TARGET_REF ) extracted any phrase denoting a relation in an English sentence.",
                "(#REF) used labeled data to learn relations, (#REF) used information encoded in the structured Wikipedia documents, (#REF) bootstrapped patterns.",
                "As a result various knowledge bases have been produced like TopicSignatures (#REF) , ConceptNet (#REF) , Yago (#REF) , NELL (#REF) and ReVerb (#REF) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: To be able to answer the questions \"What causes ebola?\", \"What are the duties of a medical doctor?\", \"What are the differences between a terrorist and a victim?\", \"Which are the animals that have wings but cannot fly?\" one requires knowledge about verb-based relations.\n sent1: Over the years, researchers have developed various relation learning algorithms.\n sent2: Some (#REF; #REF) targeted specific relations like BornInYear, CorporationAcquired, others (#REF; #TARGET_REF ) extracted any phrase denoting a relation in an English sentence.\n sent3: (#REF) used labeled data to learn relations, (#REF) used information encoded in the structured Wikipedia documents, (#REF) bootstrapped patterns.\n sent4: As a result various knowledge bases have been produced like TopicSignatures (#REF) , ConceptNet (#REF) , Yago (#REF) , NELL (#REF) and ReVerb (#REF) .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "(#REF) used labeled data to learn relations, (#REF) used information encoded in the structured Wikipedia documents, (#REF) bootstrapped patterns.",
                "As a result various knowledge bases have been produced like TopicSignatures (#REF) , ConceptNet (#REF) , Yago (#REF) , NELL (#REF) and ReVerb #TARGET_REF .",
                "Despite the many efforts to date, yet there is no universal repository (or even a system), which for a given term it can immediately return all verb relations related to the term.",
                "However, one would still like to dispose of an automated procedure, which on the fly can accurately and quickly produce such information for any term.",
                "If available, such resource can aid different natural language processing tasks such as preposition sense disambiguation (#REF) , selectional preferences (#REF; #REF) , question answering (#REF) and textual entailment (#REF) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: (#REF) used labeled data to learn relations, (#REF) used information encoded in the structured Wikipedia documents, (#REF) bootstrapped patterns.\n sent1: As a result various knowledge bases have been produced like TopicSignatures (#REF) , ConceptNet (#REF) , Yago (#REF) , NELL (#REF) and ReVerb #TARGET_REF .\n sent2: Despite the many efforts to date, yet there is no universal repository (or even a system), which for a given term it can immediately return all verb relations related to the term.\n sent3: However, one would still like to dispose of an automated procedure, which on the fly can accurately and quickly produce such information for any term.\n sent4: If available, such resource can aid different natural language processing tasks such as preposition sense disambiguation (#REF) , selectional preferences (#REF; #REF) , question answering (#REF) and textual entailment (#REF) .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Lots of attention has been payed on learning is-a and part-of relations (#REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF) .",
                "Others (#REF; #REF) have focused on learning specific relations like BornInYear, EmployedBy and CorporationAcquired.",
                "However to build a system that can learn a richer set of relations is not trivial, because often labeled training data is required (#REF; #REF) and most methods do not scale to corpora where the number of relations is very large or when the relations are not specified in advance #TARGET_REF .",
                "However, recently developed OpenIE systems like TextRunner (#REF; #REF) and ReVerb (#REF) surmount the necessity of labeled data by extracting arbitrary phrases denoting relations in English sentences.",
                "(#REF; #REF ) define relation to be any verb-prep, adj-noun construction."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Lots of attention has been payed on learning is-a and part-of relations (#REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF) .\n sent1: Others (#REF; #REF) have focused on learning specific relations like BornInYear, EmployedBy and CorporationAcquired.\n sent2: However to build a system that can learn a richer set of relations is not trivial, because often labeled training data is required (#REF; #REF) and most methods do not scale to corpora where the number of relations is very large or when the relations are not specified in advance #TARGET_REF .\n sent3: However, recently developed OpenIE systems like TextRunner (#REF; #REF) and ReVerb (#REF) surmount the necessity of labeled data by extracting arbitrary phrases denoting relations in English sentences.\n sent4: (#REF; #REF ) define relation to be any verb-prep, adj-noun construction.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Lots of attention has been payed on learning is-a and part-of relations (#REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF) .",
                "Others (#REF; #REF) have focused on learning specific relations like BornInYear, EmployedBy and CorporationAcquired.",
                "However to build a system that can learn a richer set of relations is not trivial, because often labeled training data is required (#REF; #REF) and most methods do not scale to corpora where the number of relations is very large or when the relations are not specified in advance (#REF) .",
                "However, recently developed OpenIE systems like TextRunner (#REF; #REF) and ReVerb #TARGET_REF surmount the necessity of labeled data by extracting arbitrary phrases denoting relations in English sentences.",
                "(#REF; #REF ) define relation to be any verb-prep, adj-noun construction."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Lots of attention has been payed on learning is-a and part-of relations (#REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF) .\n sent1: Others (#REF; #REF) have focused on learning specific relations like BornInYear, EmployedBy and CorporationAcquired.\n sent2: However to build a system that can learn a richer set of relations is not trivial, because often labeled training data is required (#REF; #REF) and most methods do not scale to corpora where the number of relations is very large or when the relations are not specified in advance (#REF) .\n sent3: However, recently developed OpenIE systems like TextRunner (#REF; #REF) and ReVerb #TARGET_REF surmount the necessity of labeled data by extracting arbitrary phrases denoting relations in English sentences.\n sent4: (#REF; #REF ) define relation to be any verb-prep, adj-noun construction.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "For our comparative study with existing systems, we used ReVerb 4 (#REF) , which similarly to our approach was specifically designed to learn verb-based relations from unstructured texts.",
                "Currently, ReVerb has extracted relations from ClueWeb09 5 and Wikipedia, which have been freely distributed to the public.",
                "ReVerb learns relations by taking as input any document and applies POS-tagging, NP-chunking and a set of rules over all sentences in the document to generate triples containing the verbs and the arguments associated with them.",
                "According to #TARGET_REF ReVerb outperforms TextRunner (#REF) and the open Wikipedia extractor WOE (#REF) in terms of the quantity and quality of the learned relations.",
                "For comparison, we took five terms from our experiment: ant, bomb, president, terrorists, virus and collected all verbs found by ReVerb in the ClueWeb09 and Wikipedia triples."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For our comparative study with existing systems, we used ReVerb 4 (#REF) , which similarly to our approach was specifically designed to learn verb-based relations from unstructured texts.\n sent1: Currently, ReVerb has extracted relations from ClueWeb09 5 and Wikipedia, which have been freely distributed to the public.\n sent2: ReVerb learns relations by taking as input any document and applies POS-tagging, NP-chunking and a set of rules over all sentences in the document to generate triples containing the verbs and the arguments associated with them.\n sent3: According to #TARGET_REF ReVerb outperforms TextRunner (#REF) and the open Wikipedia extractor WOE (#REF) in terms of the quantity and quality of the learned relations.\n sent4: For comparison, we took five terms from our experiment: ant, bomb, president, terrorists, virus and collected all verbs found by ReVerb in the ClueWeb09 and Wikipedia triples.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "• We develop an automatic procedure, which on the fly can learn a diverse set of verb and verb-preposition relations for a given term.",
                "• We establish the effectiveness of our approach through human-based evaluation.",
                "• We conduct a comparative study with the verb-based relation extraction system ReVerb #TARGET_REF and show that our approach accurately extracts more verb-based relations.",
                "• We also compare the verb relations produced by our system with those available in existing knowledge bases, and observe that despite their completeness these repositories lack many verb-based relations.",
                "The rest of the paper is organized as follows."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: • We develop an automatic procedure, which on the fly can learn a diverse set of verb and verb-preposition relations for a given term.\n sent1: • We establish the effectiveness of our approach through human-based evaluation.\n sent2: • We conduct a comparative study with the verb-based relation extraction system ReVerb #TARGET_REF and show that our approach accurately extracts more verb-based relations.\n sent3: • We also compare the verb relations produced by our system with those available in existing knowledge bases, and observe that despite their completeness these repositories lack many verb-based relations.\n sent4: The rest of the paper is organized as follows.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "For our comparative study with existing systems, we used ReVerb 4 #TARGET_REF , which similarly to our approach was specifically designed to learn verb-based relations from unstructured texts.",
                "Currently, ReVerb has extracted relations from ClueWeb09 5 and Wikipedia, which have been freely distributed to the public.",
                "ReVerb learns relations by taking as input any document and applies POS-tagging, NP-chunking and a set of rules over all sentences in the document to generate triples containing the verbs and the arguments associated with them.",
                "According to (#REF) ReVerb outperforms TextRunner (#REF) and the open Wikipedia extractor WOE (#REF) in terms of the quantity and quality of the learned relations.",
                "For comparison, we took five terms from our experiment: ant, bomb, president, terrorists, virus and collected all verbs found by ReVerb in the ClueWeb09 and Wikipedia triples."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: For our comparative study with existing systems, we used ReVerb 4 #TARGET_REF , which similarly to our approach was specifically designed to learn verb-based relations from unstructured texts.\n sent1: Currently, ReVerb has extracted relations from ClueWeb09 5 and Wikipedia, which have been freely distributed to the public.\n sent2: ReVerb learns relations by taking as input any document and applies POS-tagging, NP-chunking and a set of rules over all sentences in the document to generate triples containing the verbs and the arguments associated with them.\n sent3: According to (#REF) ReVerb outperforms TextRunner (#REF) and the open Wikipedia extractor WOE (#REF) in terms of the quantity and quality of the learned relations.\n sent4: For comparison, we took five terms from our experiment: ant, bomb, president, terrorists, virus and collected all verbs found by ReVerb in the ClueWeb09 and Wikipedia triples.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Our key contribution is the development of a semi-supervised procedure, which starts with a term and a verb to learn from Web documents a large and diverse set of verb relations.",
                "We have conducted an experimental evaluation with 36 terms and have collected 26, 678 unique candidate verbs and 1, 040, 651 candidate argument fillers.",
                "We have evaluated the accuracy of our approach using human based evaluation and have compared results against the ReVerb #TARGET_REF system and existing knowledge bases like NELL (#REF) , Yago (#REF) and ConceptNet (#REF) .",
                "Our study showed that despite their completeness these resources lack verb-based information and there is plenty of room for improvement since they can be further enriched with verbs using our harvesting procedure.",
                "In the future, we would like to test the usefulness of the generated resources in NLP applications."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: Our key contribution is the development of a semi-supervised procedure, which starts with a term and a verb to learn from Web documents a large and diverse set of verb relations.\n sent1: We have conducted an experimental evaluation with 36 terms and have collected 26, 678 unique candidate verbs and 1, 040, 651 candidate argument fillers.\n sent2: We have evaluated the accuracy of our approach using human based evaluation and have compared results against the ReVerb #TARGET_REF system and existing knowledge bases like NELL (#REF) , Yago (#REF) and ConceptNet (#REF) .\n sent3: Our study showed that despite their completeness these resources lack verb-based information and there is plenty of room for improvement since they can be further enriched with verbs using our harvesting procedure.\n sent4: In the future, we would like to test the usefulness of the generated resources in NLP applications.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Unfortunately, this approach requires a control of the samples variance, which can lead otherwise to unstable learning [12] .",
                "In a similar work, Mnih et al. #TARGET_REF proposed to use Noise Contrastive Estimation (NCE) [14] to speed-up the training.",
                "NCE treats the learning as a binary classification problem between a target word and noise samples, which are drawn from a noise distribution.",
                "Moreover, NCE considers the normalization term as an additional parameter that can be learned during training or fixed beforehand.",
                "In this case, the network learns to self-normalize."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Unfortunately, this approach requires a control of the samples variance, which can lead otherwise to unstable learning [12] .\n sent1: In a similar work, Mnih et al. #TARGET_REF proposed to use Noise Contrastive Estimation (NCE) [14] to speed-up the training.\n sent2: NCE treats the learning as a binary classification problem between a target word and noise samples, which are drawn from a noise distribution.\n sent3: Moreover, NCE considers the normalization term as an additional parameter that can be learned during training or fixed beforehand.\n sent4: In this case, the network learns to self-normalize.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The results reported, however, showed a minor difference in performance (2.4 points in perplexity).",
                "Moreover, training using IS can be very difficult and requires a careful control of the samples variance, which can lead otherwise to unstable learning as was reported in [12] .",
                "Hence, an adaptive IS may use a large number of samples to solve this problem whereas NCE is more stable and requires a fixed small number of noise samples (e.g., 100) to achieve a good performance #TARGET_REF 16] .",
                "Furthermore, the network learns to self-normalize during training using NCE.",
                "As a results, and on the contrary to IS, the softmax is no longer required during evaluation, which makes NCE an attractive choice to train large vocabulary NNLM."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The results reported, however, showed a minor difference in performance (2.4 points in perplexity).\n sent1: Moreover, training using IS can be very difficult and requires a careful control of the samples variance, which can lead otherwise to unstable learning as was reported in [12] .\n sent2: Hence, an adaptive IS may use a large number of samples to solve this problem whereas NCE is more stable and requires a fixed small number of noise samples (e.g., 100) to achieve a good performance #TARGET_REF 16] .\n sent3: Furthermore, the network learns to self-normalize during training using NCE.\n sent4: As a results, and on the contrary to IS, the softmax is no longer required during evaluation, which makes NCE an attractive choice to train large vocabulary NNLM.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "This paper proposes an extension of NCE to batch mode (B-NCE) training.",
                "This approach does not require any sampling and can be formulated using dense matrix operations.",
                "Furthermore, we can show that this solution optimally approximates the sampling from a unigram distribution, which has been shown to be a good noise distribution choice #TARGET_REF 16] .",
                "The main idea here is to restrict the vocabulary, at each forward-backward pass, to the target words in the batch (words to predict) and then replace the softmax function by NCE.",
                "In particular, these words play alternatively the role of targets and noise samples."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This paper proposes an extension of NCE to batch mode (B-NCE) training.\n sent1: This approach does not require any sampling and can be formulated using dense matrix operations.\n sent2: Furthermore, we can show that this solution optimally approximates the sampling from a unigram distribution, which has been shown to be a good noise distribution choice #TARGET_REF 16] .\n sent3: The main idea here is to restrict the vocabulary, at each forward-backward pass, to the target words in the batch (words to predict) and then replace the softmax function by NCE.\n sent4: In particular, these words play alternatively the role of targets and noise samples.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "For the LTCB corpus, we also report results of the models trained with the full softmax function.",
                "This is the primary motive for using this corpus.",
                "We would like also to highlight that the goal of this paper is not about improving LMs performance but rather showing how a significant training speed-up can be achieved without compromising the models performance for large vocabulary LMs.",
                "Hence, we solely focus our experiments on NCE as a major approach to achieve this goal [17, #TARGET_REF 16] in comparison to the reference full softmax function.",
                "Comparison to other training approaches such as importance sampling will be conducted in future work."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: For the LTCB corpus, we also report results of the models trained with the full softmax function.\n sent1: This is the primary motive for using this corpus.\n sent2: We would like also to highlight that the goal of this paper is not about improving LMs performance but rather showing how a significant training speed-up can be achieved without compromising the models performance for large vocabulary LMs.\n sent3: Hence, we solely focus our experiments on NCE as a major approach to achieve this goal [17, #TARGET_REF 16] in comparison to the reference full softmax function.\n sent4: Comparison to other training approaches such as importance sampling will be conducted in future work.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The batch size is fixed at 400 and the initial learning rate is set to 0.4.",
                "The latter is halved when no improvement on the validation data is observed for an additional 7 epochs.",
                "We also use a norm-based gradient clipping with a threshold of 5 but we do not use dropout.",
                "Moreover, B-NCE and S-NCE use the unigram as noise distribution pn.",
                "Following the setup proposed in #TARGET_REF 16] , S-NCE uses K = 100 noise samples, whereas B-NCE uses only the target words in the batch (K=0)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The batch size is fixed at 400 and the initial learning rate is set to 0.4.\n sent1: The latter is halved when no improvement on the validation data is observed for an additional 7 epochs.\n sent2: We also use a norm-based gradient clipping with a threshold of 5 but we do not use dropout.\n sent3: Moreover, B-NCE and S-NCE use the unigram as noise distribution pn.\n sent4: Following the setup proposed in #TARGET_REF 16] , S-NCE uses K = 100 noise samples, whereas B-NCE uses only the target words in the batch (K=0).\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Topic models are a newer development in machine learning that play an important role in document analysis and information retrieval.",
                "It turns out there is a surprising connection between the two that suggests novel ways of extending both grammars and topic models.",
                "After explaining this connection, I go on to describe extensions which identify topical multiword collocations and automatically learn the internal structure of namedentity phrases.",
                "The adaptor grammar framework is a nonparametric extension of probabilistic context-free grammars (#REF) , which was initially intended to allow fast prototyping of models of unsupervised language acquisition (#REF), but it has been shown to have applications in text data mining and information retrieval as well #TARGET_REF) .",
                "We'll see how learning the referents of words and learning the roles of social cues in language acquisition (#REF) can be viewed as a kind of topic modelling problem that can be reduced to a grammatical inference problem using the techniques described in this talk."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Topic models are a newer development in machine learning that play an important role in document analysis and information retrieval.\n sent1: It turns out there is a surprising connection between the two that suggests novel ways of extending both grammars and topic models.\n sent2: After explaining this connection, I go on to describe extensions which identify topical multiword collocations and automatically learn the internal structure of namedentity phrases.\n sent3: The adaptor grammar framework is a nonparametric extension of probabilistic context-free grammars (#REF) , which was initially intended to allow fast prototyping of models of unsupervised language acquisition (#REF), but it has been shown to have applications in text data mining and information retrieval as well #TARGET_REF) .\n sent4: We'll see how learning the referents of words and learning the roles of social cues in language acquisition (#REF) can be viewed as a kind of topic modelling problem that can be reduced to a grammatical inference problem using the techniques described in this talk.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The techniques for solving this problem can be applied to a variety of NLP tasks, such as query expansion, word sense disambiguation, machine translation, information extraction and question answering.",
                "Previous work addressing the problem can be roughly classified into three categories: (1) learning word embeddings from large collections of text using variants of neural networks (Mikolov et al. (2013a) ; Mikolov et al. (2013b) ; Mikolov et al. (2013c) ; #TARGET_REF ) or global matrix factorization (#REF ; #REF ); (2) extracting knowledge from existing semantic networks, such as WordNet (#REF ; #REF ; #REF ) and ConceptNet (#REF ); (3) combining the above two models by various ways (#REF ; #REF ; #REF ; #REF ).",
                "The empirical evidence shows that the word representations learned from neural network models do an especially good job in capturing not only attributional similarities between words but also similarities between pairs of words (Mikolov et al. (2013c) ).",
                "#REF generalize the skip-gram model with negative sampling to include arbitrary word contexts and present the dependency-based word embeddings, which are learned from syntactic contexts derived from dependency parse-trees.",
                "Qualitative and quantitative analysis demonstrates that the word2vec model with linear bag-of-words contexts can yield broad topical similarity while the dependency-based word embeddings with syntactic contexts can capture more functional similarity."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The techniques for solving this problem can be applied to a variety of NLP tasks, such as query expansion, word sense disambiguation, machine translation, information extraction and question answering.\n sent1: Previous work addressing the problem can be roughly classified into three categories: (1) learning word embeddings from large collections of text using variants of neural networks (Mikolov et al. (2013a) ; Mikolov et al. (2013b) ; Mikolov et al. (2013c) ; #TARGET_REF ) or global matrix factorization (#REF ; #REF ); (2) extracting knowledge from existing semantic networks, such as WordNet (#REF ; #REF ; #REF ) and ConceptNet (#REF ); (3) combining the above two models by various ways (#REF ; #REF ; #REF ; #REF ).\n sent2: The empirical evidence shows that the word representations learned from neural network models do an especially good job in capturing not only attributional similarities between words but also similarities between pairs of words (Mikolov et al. (2013c) ).\n sent3: #REF generalize the skip-gram model with negative sampling to include arbitrary word contexts and present the dependency-based word embeddings, which are learned from syntactic contexts derived from dependency parse-trees.\n sent4: Qualitative and quantitative analysis demonstrates that the word2vec model with linear bag-of-words contexts can yield broad topical similarity while the dependency-based word embeddings with syntactic contexts can capture more functional similarity.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "So for k =2, the contexts of the target word w t are w t−2 , w t−1 , w t+1, w t+2 and we are predicting each of these from the word w t .",
                "However, a context window with a smaller size k may miss some important contexts while including some accidental ones.",
                "Recently, Levy et al. propose the dependency-based word embeddings (DEP), which generalize the skip-gram model with negative sampling, and move from linear bag-of-words contexts to syntactic contexts that are derived from automatically produced dependency parse-trees.",
                "Embeddings produced from different kinds of contexts can induce different word similarities.",
                "The original skip-gram embeddings can yield broad topical similarities, while the dependency-based word embeddings can capture more functional similarities #TARGET_REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: So for k =2, the contexts of the target word w t are w t−2 , w t−1 , w t+1, w t+2 and we are predicting each of these from the word w t .\n sent1: However, a context window with a smaller size k may miss some important contexts while including some accidental ones.\n sent2: Recently, Levy et al. propose the dependency-based word embeddings (DEP), which generalize the skip-gram model with negative sampling, and move from linear bag-of-words contexts to syntactic contexts that are derived from automatically produced dependency parse-trees.\n sent3: Embeddings produced from different kinds of contexts can induce different word similarities.\n sent4: The original skip-gram embeddings can yield broad topical similarities, while the dependency-based word embeddings can capture more functional similarities #TARGET_REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "The first is the advent of deep learning techniques (#REF) , which allow manipulation of natural language beyond its surface forms and generalize beyond relatively small amounts of labeled data.",
                "The second factor is the formulation of standard machine comprehension benchmarks based on Cloze-style queries #TARGET_REF; #REF) , which permit fast integration loops between model conception and experimental evaluation.",
                "Cloze-style queries (#REF) are created by deleting a particular word in a natural-language statement.",
                "The task is to guess which word was deleted.",
                "In a pragmatic approach, recent work (#REF) formed such questions by extracting a sentence from a larger document."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The first is the advent of deep learning techniques (#REF) , which allow manipulation of natural language beyond its surface forms and generalize beyond relatively small amounts of labeled data.\n sent1: The second factor is the formulation of standard machine comprehension benchmarks based on Cloze-style queries #TARGET_REF; #REF) , which permit fast integration loops between model conception and experimental evaluation.\n sent2: Cloze-style queries (#REF) are created by deleting a particular word in a natural-language statement.\n sent3: The task is to guess which word was deleted.\n sent4: In a pragmatic approach, recent work (#REF) formed such questions by extracting a sentence from a larger document.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The second factor is the formulation of standard machine comprehension benchmarks based on Cloze-style queries (#REF; #REF) , which permit fast integration loops between model conception and experimental evaluation.",
                "Cloze-style queries (#REF) are created by deleting a particular word in a natural-language statement.",
                "The task is to guess which word was deleted.",
                "In a pragmatic approach, recent work #TARGET_REF formed such questions by extracting a sentence from a larger document.",
                "In contrast to considering a stand-alone statement, the system is now required to handle a larger amount of information that may possibly influence the prediction of the missing word."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The second factor is the formulation of standard machine comprehension benchmarks based on Cloze-style queries (#REF; #REF) , which permit fast integration loops between model conception and experimental evaluation.\n sent1: Cloze-style queries (#REF) are created by deleting a particular word in a natural-language statement.\n sent2: The task is to guess which word was deleted.\n sent3: In a pragmatic approach, recent work #TARGET_REF formed such questions by extracting a sentence from a larger document.\n sent4: In contrast to considering a stand-alone statement, the system is now required to handle a larger amount of information that may possibly influence the prediction of the missing word.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "One of the advantages of using Cloze-style questions to evaluate machine comprehension systems is that a sufficient amount of training and test data can be obtained without human intervention.",
                "The CBT #TARGET_REF and CNN (#REF) corpora are two such datasets.",
                "The CBT 1 corpus was generated from well-known children's books available through Project Gutenberg.",
                "Documents consist of 20-sentence excerpts from these books.",
                "The related query is formed from an excerpt's 21st sentence by replacing a single word with an anonymous placeholder token."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: One of the advantages of using Cloze-style questions to evaluate machine comprehension systems is that a sufficient amount of training and test data can be obtained without human intervention.\n sent1: The CBT #TARGET_REF and CNN (#REF) corpora are two such datasets.\n sent2: The CBT 1 corpus was generated from well-known children's books available through Project Gutenberg.\n sent3: Documents consist of 20-sentence excerpts from these books.\n sent4: The related query is formed from an excerpt's 21st sentence by replacing a single word with an anonymous placeholder token.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The result of this alternating search is fed back into the iterative inference process to seed the next search step.",
                "This permits our model to reason about different parts of the query in a sequential way, based on the information that has been gathered previously from the document.",
                "After a fixed number of iterations, the model uses a summary of its inference process to predict the answer.",
                "This paper makes the following contributions.",
                "We present a novel iterative, alternating attention mechanism that, unlike existing models #TARGET_REF; #REF) , does not compress the query to a single representation, but instead alternates its attention between the query and the document to obtain a fine-grained query representation within a fixed computation time."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: The result of this alternating search is fed back into the iterative inference process to seed the next search step.\n sent1: This permits our model to reason about different parts of the query in a sequential way, based on the information that has been gathered previously from the document.\n sent2: After a fixed number of iterations, the model uses a summary of its inference process to predict the answer.\n sent3: This paper makes the following contributions.\n sent4: We present a novel iterative, alternating attention mechanism that, unlike existing models #TARGET_REF; #REF) , does not compress the query to a single representation, but instead alternates its attention between the query and the document to obtain a fine-grained query representation within a fixed computation time.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "The attention we use here is similar to the formulation used in #TARGET_REF; #REF) , but with two differences.",
                "First, we use a bilinear term instead of a simple dot product in order to compute the importance of each query term in the current time step.",
                "This simple bilinear attention has been successfully used in (#REF) .",
                "Second, we add a term a q that allows to bias the attention mechanism towards words which tend to be important across the questions independently of the search key s t−1 .",
                "This is similar to what is achieved by the original attention mechanism proposed in (#REF) without the burden of the additional tanh layer."
            ],
            "label": [
                "DIFFERENCES",
                "SIMILARITY"
            ]
        },
        "input": "sent0: The attention we use here is similar to the formulation used in #TARGET_REF; #REF) , but with two differences.\n sent1: First, we use a bilinear term instead of a simple dot product in order to compute the importance of each query term in the current time step.\n sent2: This simple bilinear attention has been successfully used in (#REF) .\n sent3: Second, we add a term a q that allows to bias the attention mechanism towards words which tend to be important across the questions independently of the search key s t−1 .\n sent4: This is similar to what is achieved by the original attention mechanism proposed in (#REF) without the burden of the additional tanh layer.\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"SIMILARITY\"], \"context\": [\"sent0\", \"sent1\", \"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "After a fixed number of iterations, the model uses a summary of its inference process to predict the answer.",
                "This paper makes the following contributions.",
                "We present a novel iterative, alternating attention mechanism that, unlike existing models (#REF; #REF) , does not compress the query to a single representation, but instead alternates its attention between the query and the document to obtain a fine-grained query representation within a fixed computation time.",
                "Our architecture tightly integrates previous ideas related to bidirectional readers (#REF) and iterative attention processes #TARGET_REF; #REF) .",
                "It obtains state-of-theart results on two machine comprehension datasets and shows promise for application to a broad range of natural language processing tasks."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: After a fixed number of iterations, the model uses a summary of its inference process to predict the answer.\n sent1: This paper makes the following contributions.\n sent2: We present a novel iterative, alternating attention mechanism that, unlike existing models (#REF; #REF) , does not compress the query to a single representation, but instead alternates its attention between the query and the document to obtain a fine-grained query representation within a fixed computation time.\n sent3: Our architecture tightly integrates previous ideas related to bidirectional readers (#REF) and iterative attention processes #TARGET_REF; #REF) .\n sent4: It obtains state-of-theart results on two machine comprehension datasets and shows promise for application to a broad range of natural language processing tasks.\n",
        "output": "{\"label\": [null], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "In order to stabilize the learning, we clip the gradients if their norm is greater than 5 (#REF) .",
                "We performed a hyperparameter search with embedding regularization in {0.001, 0.0001}, inference steps T ∈ {3, 5, 8}, embedding size d ∈ {256, 384}, encoder size h ∈ {128, 256} and the inference GRU size s ∈ {256, 512}. We regularize our model by applying a dropout (#REF) Table 2 : Results on the CBT-NE (named entity) and CBT-CN (common noun) datasets.",
                "Results marked with 1 are from #TARGET_REF and those marked with 2 are from (#REF) .",
                "the inputs to both the query and the document attention mechanisms.",
                "We found that setting embedding regularization to 0.0001, T = 8, d = 384, h = 128, s = 512 worked robustly across the datasets."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: In order to stabilize the learning, we clip the gradients if their norm is greater than 5 (#REF) .\n sent1: We performed a hyperparameter search with embedding regularization in {0.001, 0.0001}, inference steps T ∈ {3, 5, 8}, embedding size d ∈ {256, 384}, encoder size h ∈ {128, 256} and the inference GRU size s ∈ {256, 512}. We regularize our model by applying a dropout (#REF) Table 2 : Results on the CBT-NE (named entity) and CBT-CN (common noun) datasets.\n sent2: Results marked with 1 are from #TARGET_REF and those marked with 2 are from (#REF) .\n sent3: the inputs to both the query and the document attention mechanisms.\n sent4: We found that setting embedding regularization to 0.0001, T = 8, d = 384, h = 128, s = 512 worked robustly across the datasets.\n",
        "output": "{\"label\": [null], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The subsets are named entity, common noun, verb, and preposition.",
                "We will focus our evaluation solely on the first two subsets, i.e. CBT-NE (named entity) and CBT-CN (common nouns), since the latter two are relatively simple as demonstrated by #TARGET_REF .",
                "The CNN 2 corpus was generated from news articles available through the CNN website.",
                "The documents are given by the full articles themselves, which are accompanied by short, bullet-point summary statements.",
                "Instead of extracting a query from the articles themselves, the authors replace a named entity within each article summary with an anonymous placeholder token."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: The subsets are named entity, common noun, verb, and preposition.\n sent1: We will focus our evaluation solely on the first two subsets, i.e. CBT-NE (named entity) and CBT-CN (common nouns), since the latter two are relatively simple as demonstrated by #TARGET_REF .\n sent2: The CNN 2 corpus was generated from news articles available through the CNN website.\n sent3: The documents are given by the full articles themselves, which are accompanied by short, bullet-point summary statements.\n sent4: Instead of extracting a query from the articles themselves, the authors replace a named entity within each article summary with an anonymous placeholder token.\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "In our case, the memory is represented by the set of document and query contextual encodings.",
                "Our model is closely related to (#REF; #REF; #REF; #REF; #TARGET_REF , which were also applied to question answering.",
                "The pointer-style attention mechanism that we use to perform the final answer prediction has been proposed by (#REF) , which in turn was based on the earlier Pointer Networks of (#REF) .",
                "However, differently from our work, (#REF) perform only one attention step and embed the query into a single vector representation, corresponding to the concatenation of the last state of the forward and backward GRU networks.",
                "To our knowledge, embedding the query into a single vector representation is a choice that is shared by most machine reading comprehension models."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: In our case, the memory is represented by the set of document and query contextual encodings.\n sent1: Our model is closely related to (#REF; #REF; #REF; #REF; #TARGET_REF , which were also applied to question answering.\n sent2: The pointer-style attention mechanism that we use to perform the final answer prediction has been proposed by (#REF) , which in turn was based on the earlier Pointer Networks of (#REF) .\n sent3: However, differently from our work, (#REF) perform only one attention step and embed the query into a single vector representation, corresponding to the concatenation of the last state of the forward and backward GRU networks.\n sent4: To our knowledge, embedding the query into a single vector representation is a choice that is shared by most machine reading comprehension models.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The inference network is responsible for making sense of the current attention step with respect to what has been gathered before.",
                "In addition to achieving state-ofthe-art performance, this technique may also prove to be more scalable than alternative query attention models.",
                "Finally, our iterative inference process shares similarities to the iterative hops in Memory Networks (#REF; #TARGET_REF .",
                "In that model, the query representation is updated iteratively from hop to hop, although its different components are not attended to separately.",
                "Moreover, we substitute the simple linear update with a GRU network."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: The inference network is responsible for making sense of the current attention step with respect to what has been gathered before.\n sent1: In addition to achieving state-ofthe-art performance, this technique may also prove to be more scalable than alternative query attention models.\n sent2: Finally, our iterative inference process shares similarities to the iterative hops in Memory Networks (#REF; #TARGET_REF .\n sent3: In that model, the query representation is updated iteratively from hop to hop, although its different components are not attended to separately.\n sent4: Moreover, we substitute the simple linear update with a GRU network.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We report the results of our model on the CBT-CN, CBT-NE and CNN datasets, previously described in Section 2.",
                "Table 2 reports our results on the CBT-CN and CBT-NE dataset.",
                "The Humans, LSTMs and Memory Networks (MemNNs) results are taken from #TARGET_REF and the Attention-Sum Reader (AS Reader) is a state-of-the-art result recently obtained by (#REF) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We report the results of our model on the CBT-CN, CBT-NE and CNN datasets, previously described in Section 2.\n sent1: Table 2 reports our results on the CBT-CN and CBT-NE dataset.\n sent2: The Humans, LSTMs and Memory Networks (MemNNs) results are taken from #TARGET_REF and the Attention-Sum Reader (AS Reader) is a state-of-the-art result recently obtained by (#REF) .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Recent work that incorporated Dirichlet process (DP) nonparametric models into TSGs has provided an efficient solution to the problem of segmenting training data trees into elementary parse tree fragments to form the grammar (#REF; #TARGET_REF; #REF) .",
                "DP inference tackles this problem by exploring the space of all possible segmentations of the data, in search for fragments that are on the one hand large enough so that they incorporate the useful dependencies, and on the other small enough so that they recur and have a chance to be useful in analyzing unseen data.",
                "The elementary trees combined in a TSG are, intuitively, primitives of the language, yet certain linguistic phenomena (notably various forms of modification) \"split them up\", preventing their reuse, leading to less sparse grammars than might be ideal.",
                "For instance, imagine modeling the following set of structures:",
                "• A natural recurring structure here would be the structure \"[ N P the [ N N president]]\", yet it occurs not at all in the data."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Recent work that incorporated Dirichlet process (DP) nonparametric models into TSGs has provided an efficient solution to the problem of segmenting training data trees into elementary parse tree fragments to form the grammar (#REF; #TARGET_REF; #REF) .\n sent1: DP inference tackles this problem by exploring the space of all possible segmentations of the data, in search for fragments that are on the one hand large enough so that they incorporate the useful dependencies, and on the other small enough so that they recur and have a chance to be useful in analyzing unseen data.\n sent2: The elementary trees combined in a TSG are, intuitively, primitives of the language, yet certain linguistic phenomena (notably various forms of modification) \"split them up\", preventing their reuse, leading to less sparse grammars than might be ideal.\n sent3: For instance, imagine modeling the following set of structures:\n sent4: • A natural recurring structure here would be the structure \"[ N P the [ N N president]]\", yet it occurs not at all in the data.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "In the basic nonparametric TSG model, there is an independent DP for every grammar category (such as c = N P ), each of which uses a base distribution P 0 that generates an initial tree by making stepwise decisions.",
                "The canonical P 0 uses a probabilistic CFGP that is fixed a priori to sample CFG rules top-down and Bernoulli variables for determining where substitutions should occur (#REF; #TARGET_REF .",
                "We extend this model by adding specialized DPs for left and right auxiliary trees.",
                "3",
                "Therefore, we have an exchangeable process for generating right auxiliary trees"
            ],
            "label": [
                "BACKGROUND",
                "EXTENDS"
            ]
        },
        "input": "sent0: In the basic nonparametric TSG model, there is an independent DP for every grammar category (such as c = N P ), each of which uses a base distribution P 0 that generates an initial tree by making stepwise decisions.\n sent1: The canonical P 0 uses a probabilistic CFGP that is fixed a priori to sample CFG rules top-down and Bernoulli variables for determining where substitutions should occur (#REF; #TARGET_REF .\n sent2: We extend this model by adding specialized DPs for left and right auxiliary trees.\n sent3: 3\n sent4: Therefore, we have an exchangeable process for generating right auxiliary trees\n",
        "output": "{\"label\": [\"BACKGROUND\", \"EXTENDS\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Since TIG derivations are highly structured objects, a basic sampling strategy based on local node-level moves such as Gibbs sampling (#REF) would not hold much promise.",
                "Following previous work, we design a blocked Metropolis-Hastings sampler that samples derivations per entire parse trees all at once in a joint fashion #TARGET_REF; #REF) .",
                "This is achieved by proposing derivations from an approximating distribution and stochastically correcting via accept/reject to achieve convergence into the correct posterior (#REF) .",
                "Since our base distributions factorize over levels of tree, CFG is the most convenient choice for a CFG rule CFG probability proposal distribution.",
                "Fortunately, #REF provide an (exact) transformation from a fully general TIG into a TSG that generates the same string languages."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Since TIG derivations are highly structured objects, a basic sampling strategy based on local node-level moves such as Gibbs sampling (#REF) would not hold much promise.\n sent1: Following previous work, we design a blocked Metropolis-Hastings sampler that samples derivations per entire parse trees all at once in a joint fashion #TARGET_REF; #REF) .\n sent2: This is achieved by proposing derivations from an approximating distribution and stochastically correcting via accept/reject to achieve convergence into the correct posterior (#REF) .\n sent3: Since our base distributions factorize over levels of tree, CFG is the most convenient choice for a CFG rule CFG probability proposal distribution.\n sent4: Fortunately, #REF provide an (exact) transformation from a fully general TIG into a TSG that generates the same string languages.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "This is achieved by proposing derivations from an approximating distribution and stochastically correcting via accept/reject to achieve convergence into the correct posterior (#REF) .",
                "Since our base distributions factorize over levels of tree, CFG is the most convenient choice for a CFG rule CFG probability proposal distribution.",
                "Fortunately, #REF provide an (exact) transformation from a fully general TIG into a TSG that generates the same string languages.",
                "It is then straightforward to represent this TSG as a CFG using the Goodman transform (#REF; #TARGET_REF .",
                "Figure 4 lists the additional CFG productions we have designed, as well as the rules used that trigger them."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This is achieved by proposing derivations from an approximating distribution and stochastically correcting via accept/reject to achieve convergence into the correct posterior (#REF) .\n sent1: Since our base distributions factorize over levels of tree, CFG is the most convenient choice for a CFG rule CFG probability proposal distribution.\n sent2: Fortunately, #REF provide an (exact) transformation from a fully general TIG into a TSG that generates the same string languages.\n sent3: It is then straightforward to represent this TSG as a CFG using the Goodman transform (#REF; #TARGET_REF .\n sent4: Figure 4 lists the additional CFG productions we have designed, as well as the rules used that trigger them.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "All reported numbers are averages over three runs.",
                "Parsing results are based on the maximum probability parse which was obtained by sampling derivations under the transform CFG.",
                "We compare our system (referred to as TIG) to our implementation of the TSG system of #TARGET_REF ) (referred to as TSG) and the constrained TIG variant of (#REF ) (referred to as TIG 0 ).",
                "The upshot of our experiments is that, while on the large training set all models have similar performance (85.6, 85.3, 85 .4 for TSG, TIG 0 and TIG respectively), on the small dataset insertion helps nonparametric model to find more compact and generalizable representations for the data, which affects parsing performance (Figure 4 ).",
                "Although TIG 0 has performance close to TIG, note that TIG achieves this performance using a more succinct representation and extracting a rich set of auxiliary trees."
            ],
            "label": [
                "USE",
                "SIMILARITY"
            ]
        },
        "input": "sent0: All reported numbers are averages over three runs.\n sent1: Parsing results are based on the maximum probability parse which was obtained by sampling derivations under the transform CFG.\n sent2: We compare our system (referred to as TIG) to our implementation of the TSG system of #TARGET_REF ) (referred to as TSG) and the constrained TIG variant of (#REF ) (referred to as TIG 0 ).\n sent3: The upshot of our experiments is that, while on the large training set all models have similar performance (85.6, 85.3, 85 .4 for TSG, TIG 0 and TIG respectively), on the small dataset insertion helps nonparametric model to find more compact and generalizable representations for the data, which affects parsing performance (Figure 4 ).\n sent4: Although TIG 0 has performance close to TIG, note that TIG achieves this performance using a more succinct representation and extracting a rich set of auxiliary trees.\n",
        "output": "{\"label\": [\"USE\", \"SIMILARITY\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Pilots could learn how many birds are in their path to decide whether to change course and so avoid costly, life-threatening collisions.",
                "These examples illustrate several of the interests from a visual question answering (VQA) system, including tackling problems that involve classification, detection, and counting.",
                "More generally, the goal for VQA is to have a single system that can accurately answer any natural language question about an image or video [2] , #TARGET_REF , [4] .",
                "Entangled in the dream of a VQA system is an unavoidable issue that, when asking multiple people a visual question, sometimes they all agree on a single answer while other times they offer different answers ( Figure 1) .",
                "In fact, as we show in the paper, these two outcomes arise in approximately equal proportions in today's largest publicly-shared VQA benchmark that contains over 450,000 visual questions."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Pilots could learn how many birds are in their path to decide whether to change course and so avoid costly, life-threatening collisions.\n sent1: These examples illustrate several of the interests from a visual question answering (VQA) system, including tackling problems that involve classification, detection, and counting.\n sent2: More generally, the goal for VQA is to have a single system that can accurately answer any natural language question about an image or video [2] , #TARGET_REF , [4] .\n sent3: Entangled in the dream of a VQA system is an unavoidable issue that, when asking multiple people a visual question, sometimes they all agree on a single answer while other times they offer different answers ( Figure 1) .\n sent4: In fact, as we show in the paper, these two outcomes arise in approximately equal proportions in today's largest publicly-shared VQA benchmark that contains over 450,000 visual questions.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Today's status quo is to assume a fixed number of human responses per visual question and so a fixed cost, delay, and potential diversity of answers for every visual question #TARGET_REF , [1] , [5] .",
                "We instead propose to dynamically solicit the number of human responses based on each visual question.",
                "In particular, we aim to accrue additional costs and delays from collecting extra answers only when extra responses are needed to discover all plausible answers.",
                "We show in our experiments that our system saves 19 40-hour work weeks and $1800 to answer 121,512 visual questions, compared to today's status quo approach [1] .",
                "Our work is also inspired by the goal to improve how to employ crowds to produce the information needed to train and evaluate automated methods."
            ],
            "label": [
                "BACKGROUND",
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Today's status quo is to assume a fixed number of human responses per visual question and so a fixed cost, delay, and potential diversity of answers for every visual question #TARGET_REF , [1] , [5] .\n sent1: We instead propose to dynamically solicit the number of human responses based on each visual question.\n sent2: In particular, we aim to accrue additional costs and delays from collecting extra answers only when extra responses are needed to discover all plausible answers.\n sent3: We show in our experiments that our system saves 19 40-hour work weeks and $1800 to answer 121,512 visual questions, compared to today's status quo approach [1] .\n sent4: Our work is also inspired by the goal to improve how to employ crowds to produce the information needed to train and evaluate automated methods.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"DIFFERENCES\"], \"context\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We show in our experiments that our system saves 19 40-hour work weeks and $1800 to answer 121,512 visual questions, compared to today's status quo approach [1] .",
                "Our work is also inspired by the goal to improve how to employ crowds to produce the information needed to train and evaluate automated methods.",
                "Specifically, researchers in fields as diverse as computer vision #TARGET_REF , computational linguistics [2] , and machine learning [4] rely on large datasets to improve their VQA algorithms.",
                "These datasets include visual questions and human-supplied answers.",
                "Such data is critical for teaching machine learning algorithms how to answer questions by example."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: We show in our experiments that our system saves 19 40-hour work weeks and $1800 to answer 121,512 visual questions, compared to today's status quo approach [1] .\n sent1: Our work is also inspired by the goal to improve how to employ crowds to produce the information needed to train and evaluate automated methods.\n sent2: Specifically, researchers in fields as diverse as computer vision #TARGET_REF , computational linguistics [2] , and machine learning [4] rely on large datasets to improve their VQA algorithms.\n sent3: These datasets include visual questions and human-supplied answers.\n sent4: Such data is critical for teaching machine learning algorithms how to answer questions by example.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "In general, \"bigger\" data is better.",
                "Current methods to create these datasets assume a fixed number of human answers per visual question #TARGET_REF , [5] , thereby either compromising on quality by not collecting all plausible answers or cost by collecting additional answers when they are redundant.",
                "We offer an economical way to spend a human budget to collect answers from crowd workers.",
                "In particular, we aim to actively allocate additional answers only to visual questions likely to have multiple answers.",
                "The key contributions of our work are as follows:"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In general, \"bigger\" data is better.\n sent1: Current methods to create these datasets assume a fixed number of human answers per visual question #TARGET_REF , [5] , thereby either compromising on quality by not collecting all plausible answers or cost by collecting additional answers when they are redundant.\n sent2: We offer an economical way to spend a human budget to collect answers from crowd workers.\n sent3: In particular, we aim to actively allocate additional answers only to visual questions likely to have multiple answers.\n sent4: The key contributions of our work are as follows:\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Visual Question Answering Services: Researchers spanning communities as diverse as human computer interaction, machine learning, computational linguistics, and computer vision have proposed a variety of ways to answer questions about images [2] , #TARGET_REF , [1] , [4] .",
                "Yet a commonality across these communities is they adopt a one-size-fits-all approach when deciding the number of answers for any visual question.",
                "For example, crowd-powered systems aim to supply a prespecified, fixed number of answers per visual question [1] and automated systems return a single answer for every visual question [2] , [3] , [4] .",
                "Inspired by the observation that there can be multiple plausible answers per visual question, we propose a richer representation of visual question answering that accounts for whether different people would agree on a single answer.",
                "We propose a system that automatically predicts whether humans will disagree."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Visual Question Answering Services: Researchers spanning communities as diverse as human computer interaction, machine learning, computational linguistics, and computer vision have proposed a variety of ways to answer questions about images [2] , #TARGET_REF , [1] , [4] .\n sent1: Yet a commonality across these communities is they adopt a one-size-fits-all approach when deciding the number of answers for any visual question.\n sent2: For example, crowd-powered systems aim to supply a prespecified, fixed number of answers per visual question [1] and automated systems return a single answer for every visual question [2] , [3] , [4] .\n sent3: Inspired by the observation that there can be multiple plausible answers per visual question, we propose a richer representation of visual question answering that accounts for whether different people would agree on a single answer.\n sent4: We propose a system that automatically predicts whether humans will disagree.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Visual Question Answering Services: Researchers spanning communities as diverse as human computer interaction, machine learning, computational linguistics, and computer vision have proposed a variety of ways to answer questions about images [2] , [3] , [1] , [4] .",
                "Yet a commonality across these communities is they adopt a one-size-fits-all approach when deciding the number of answers for any visual question.",
                "For example, crowd-powered systems aim to supply a prespecified, fixed number of answers per visual question [1] and automated systems return a single answer for every visual question [2] , #TARGET_REF , [4] .",
                "Inspired by the observation that there can be multiple plausible answers per visual question, we propose a richer representation of visual question answering that accounts for whether different people would agree on a single answer.",
                "We propose a system that automatically predicts whether humans will disagree."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Visual Question Answering Services: Researchers spanning communities as diverse as human computer interaction, machine learning, computational linguistics, and computer vision have proposed a variety of ways to answer questions about images [2] , [3] , [1] , [4] .\n sent1: Yet a commonality across these communities is they adopt a one-size-fits-all approach when deciding the number of answers for any visual question.\n sent2: For example, crowd-powered systems aim to supply a prespecified, fixed number of answers per visual question [1] and automated systems return a single answer for every visual question [2] , #TARGET_REF , [4] .\n sent3: Inspired by the observation that there can be multiple plausible answers per visual question, we propose a richer representation of visual question answering that accounts for whether different people would agree on a single answer.\n sent4: We propose a system that automatically predicts whether humans will disagree.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "For those systems that treat response time as a first priority, a variable number of answers may arise but this is due to varying crowdsourcing conditions such as the available supply of workers [1] , [6] .",
                "Other systems ensure a fixed number of answers are collected per visual question #TARGET_REF , [5] .",
                "Unlike prior work, our goal is to collect answers in a way that is both economical and complete in capturing the diversity of plausible answers for all visual questions.",
                "To our knowledge, our work is the first to predict the number of answers to collect for a visual question.",
                "Experiments demonstrate that our disagreement predictions are useful to significantly reduce human effort for capturing the diversity of valid answers for 121,512 visual questions."
            ],
            "label": [
                "BACKGROUND",
                "DIFFERENCES"
            ]
        },
        "input": "sent0: For those systems that treat response time as a first priority, a variable number of answers may arise but this is due to varying crowdsourcing conditions such as the available supply of workers [1] , [6] .\n sent1: Other systems ensure a fixed number of answers are collected per visual question #TARGET_REF , [5] .\n sent2: Unlike prior work, our goal is to collect answers in a way that is both economical and complete in capturing the diversity of plausible answers for all visual questions.\n sent3: To our knowledge, our work is the first to predict the number of answers to collect for a visual question.\n sent4: Experiments demonstrate that our disagreement predictions are useful to significantly reduce human effort for capturing the diversity of valid answers for 121,512 visual questions.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"DIFFERENCES\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We chose this benchmark because it both represents a diversity of visual questions and includes many crowdsourced answers for every visual question.",
                "The benchmark consists of two datasets that reflect VQAs for real images and abstract scenes.",
                "Specifically, 80% (i.e., 369,861) of VQAs are about real images that show 91 types of objects that would be \"easily recognizable by a 4 year old\" in their natural context [21] .",
                "The remaining 90,000 VQAs are about abstract scenes that were created with clipart and show 100 types of everyday objects often observed in real images #TARGET_REF .",
                "The benchmark includes a diversity of visual questions intentionally collected to be both grounded in images and taskindependent."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: We chose this benchmark because it both represents a diversity of visual questions and includes many crowdsourced answers for every visual question.\n sent1: The benchmark consists of two datasets that reflect VQAs for real images and abstract scenes.\n sent2: Specifically, 80% (i.e., 369,861) of VQAs are about real images that show 91 types of objects that would be \"easily recognizable by a 4 year old\" in their natural context [21] .\n sent3: The remaining 90,000 VQAs are about abstract scenes that were created with clipart and show 100 types of everyday objects often observed in real images #TARGET_REF .\n sent4: The benchmark includes a diversity of visual questions intentionally collected to be both grounded in images and taskindependent.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "We pre-process each answer by converting all letters to lower case, converting numbers to digits, and removing punctuation and articles (i.e., \"a\", \"an\", \"the\"), as was done in prior work [3] .",
                "While this approach does not fully resolve all conceptually equivalent responses, it does reveal an upper bound of expected answer diversity.",
                "In other words, more lenient agreement schemes (e.g., employing sophisticated natural language processing methods) would lead to either the same or less answer diversity.",
                "We establish valid answers by tallying the number of times each unique answer is given and then only accepting answers observed from at least m people, where m is an application-specific parameter to set.",
                "In practice, prior work deems answers as 100% valid using blind trust (i.e., m = 1 person) [22] as well as more conservative answer validation schemes (i.e., m = 3 people) #TARGET_REF ."
            ],
            "label": [
                "BACKGROUND",
                "DIFFERENCES"
            ]
        },
        "input": "sent0: We pre-process each answer by converting all letters to lower case, converting numbers to digits, and removing punctuation and articles (i.e., \"a\", \"an\", \"the\"), as was done in prior work [3] .\n sent1: While this approach does not fully resolve all conceptually equivalent responses, it does reveal an upper bound of expected answer diversity.\n sent2: In other words, more lenient agreement schemes (e.g., employing sophisticated natural language processing methods) would lead to either the same or less answer diversity.\n sent3: We establish valid answers by tallying the number of times each unique answer is given and then only accepting answers observed from at least m people, where m is an application-specific parameter to set.\n sent4: In practice, prior work deems answers as 100% valid using blind trust (i.e., m = 1 person) [22] as well as more conservative answer validation schemes (i.e., m = 3 people) #TARGET_REF .\n",
        "output": "{\"label\": [\"BACKGROUND\", \"DIFFERENCES\"], \"context\": [\"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "We next present a novel resource allocation system for efficiently capturing the diversity of true answers for a batch of visual questions.",
                "Today's status quo is to either uniformly collect N answers for every visual question #TARGET_REF or collect multiple answers where the number is determined by external crowdsourcing conditions [1] .",
                "Our system instead spends a human budget by predicting the number of answers to collect Fig. 6 : We propose a novel application of predicting the number of redundant answers to collect from the crowd per visual question to efficiently capture the diversity of all answers for all visual questions.",
                "(a) For a batch of visual questions, our system first produces a relative ordering using the predicted confidence in whether a crowd would agree on an answer (upper half).",
                "Then, the system allocates a minimum number of annotations to all visual questions (bottom, left half) and then the extra available human budget to visual questions most confidently predicted to lead to crowd disagreement (bottom, right half)."
            ],
            "label": [
                "BACKGROUND",
                "DIFFERENCES"
            ]
        },
        "input": "sent0: We next present a novel resource allocation system for efficiently capturing the diversity of true answers for a batch of visual questions.\n sent1: Today's status quo is to either uniformly collect N answers for every visual question #TARGET_REF or collect multiple answers where the number is determined by external crowdsourcing conditions [1] .\n sent2: Our system instead spends a human budget by predicting the number of answers to collect Fig. 6 : We propose a novel application of predicting the number of redundant answers to collect from the crowd per visual question to efficiently capture the diversity of all answers for all visual questions.\n sent3: (a) For a batch of visual questions, our system first produces a relative ordering using the predicted confidence in whether a crowd would agree on an answer (upper half).\n sent4: Then, the system allocates a minimum number of annotations to all visual questions (bottom, left half) and then the extra available human budget to visual questions most confidently predicted to lead to crowd disagreement (bottom, right half).\n",
        "output": "{\"label\": [\"BACKGROUND\", \"DIFFERENCES\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Status Quo:",
                "The system randomly prioritizes which images receive redundancy.",
                "This predictor illustrates the best a user can achieve today with crowd-powered systems [1] , [6] or with current dataset collection methods #TARGET_REF , [5] .",
                "Evaluation Methodology: We quantify the total diversity of answers captured by a resource allocation system for a batch of visual questions Q as follows:",
                "where q i represents the set of all true answers for the i-th visual question, r i represents the set of unique answers captured in the R answers collected for the i-th visual question, and s j represents the set of unique answers captured in the S answers collected for the j-th visual question."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Status Quo:\n sent1: The system randomly prioritizes which images receive redundancy.\n sent2: This predictor illustrates the best a user can achieve today with crowd-powered systems [1] , [6] or with current dataset collection methods #TARGET_REF , [5] .\n sent3: Evaluation Methodology: We quantify the total diversity of answers captured by a resource allocation system for a batch of visual questions Q as follows:\n sent4: where q i represents the set of all true answers for the i-th visual question, r i represents the set of unique answers captured in the R answers collected for the i-th visual question, and s j represents the set of unique answers captured in the S answers collected for the j-th visual question.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Inspired by the observation that there can be multiple plausible answers per visual question, we propose a richer representation of visual question answering that accounts for whether different people would agree on a single answer.",
                "We propose a system that automatically predicts whether humans will disagree.",
                "We demonstrate the predictive advantage of our system over relying on the uncertainty of a VQA algorithm in its predicted answer #TARGET_REF .",
                "Answer Collection from a Crowd: Our work relates to methods that propose how to employ crowd workers to answer questions about images.",
                "Such approaches aim to collect a pre-specified, fixed number of answers per visual question."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Inspired by the observation that there can be multiple plausible answers per visual question, we propose a richer representation of visual question answering that accounts for whether different people would agree on a single answer.\n sent1: We propose a system that automatically predicts whether humans will disagree.\n sent2: We demonstrate the predictive advantage of our system over relying on the uncertainty of a VQA algorithm in its predicted answer #TARGET_REF .\n sent3: Answer Collection from a Crowd: Our work relates to methods that propose how to employ crowd workers to answer questions about images.\n sent4: Such approaches aim to collect a pre-specified, fixed number of answers per visual question.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Therefore, we employ as a baseline a related VQA algorithm [25] , #TARGET_REF which produces for a given visual question an answer with a confidence score.",
                "This system parallels the deep learning architecture we adapt.",
                "However, it predicts the system's uncertainty in its own answer, whereas we are interested in the humans' collective disagreement on the answer.",
                "Still, it is a useful baseline to see if an existing algorithm could serve our purpose.",
                "Classification Performance: We evaluate the predictive power of the classification systems based on each classifier's predictions for the 121,512 visual questions in the test dataset."
            ],
            "label": [
                "DIFFERENCES",
                "USE"
            ]
        },
        "input": "sent0: Therefore, we employ as a baseline a related VQA algorithm [25] , #TARGET_REF which produces for a given visual question an answer with a confidence score.\n sent1: This system parallels the deep learning architecture we adapt.\n sent2: However, it predicts the system's uncertainty in its own answer, whereas we are interested in the humans' collective disagreement on the answer.\n sent3: Still, it is a useful baseline to see if an existing algorithm could serve our purpose.\n sent4: Classification Performance: We evaluate the predictive power of the classification systems based on each classifier's predictions for the 121,512 visual questions in the test dataset.\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"USE\"], \"context\": [\"sent0\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We also report the average precision (AP), which indicates the area under a precision-recall curve.",
                "AP values range from 0 to 1 with better-performing prediction systems having larger values.",
                "Figure 4a shows precision-recall curves for all prediction systems.",
                "Both our proposed classification systems outperform the VQA Algorithm #TARGET_REF baseline; e.g., Ours -RF yields a 12 percentage point improvement with respect to AP.",
                "This is interesting because it shows there is value in learning the disagreement task specifically, rather than employing an algorithm's confidence in its answers."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: We also report the average precision (AP), which indicates the area under a precision-recall curve.\n sent1: AP values range from 0 to 1 with better-performing prediction systems having larger values.\n sent2: Figure 4a shows precision-recall curves for all prediction systems.\n sent3: Both our proposed classification systems outperform the VQA Algorithm #TARGET_REF baseline; e.g., Ours -RF yields a 12 percentage point improvement with respect to AP.\n sent4: This is interesting because it shows there is value in learning the disagreement task specifically, rather than employing an algorithm's confidence in its answers.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "This translates to eliminating 19 40-hour work weeks and saving over $1800, assuming workers are paid $0.02 per answer and take 30 seconds to answer a visual question.",
                "Our approach fills an important gap in the crowdsourcing answer collection literature for targeting the allocation of extra answers only to visual questions where a diversity of answers is expected.",
                "Figure 6b also illustrates the advantage of our system over a related VQA algorithm #TARGET_REF for our novel application of costsensitive answer collection from a crowd.",
                "As observed, relying on an algorithm's confidence in its answer offers a valuable indicator over today's status quo of passively budgeting.",
                "While we acknowledge this method is not intended for our task specifically, it still serves as an important baseline (as discussed above)."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: This translates to eliminating 19 40-hour work weeks and saving over $1800, assuming workers are paid $0.02 per answer and take 30 seconds to answer a visual question.\n sent1: Our approach fills an important gap in the crowdsourcing answer collection literature for targeting the allocation of extra answers only to visual questions where a diversity of answers is expected.\n sent2: Figure 6b also illustrates the advantage of our system over a related VQA algorithm #TARGET_REF for our novel application of costsensitive answer collection from a crowd.\n sent3: As observed, relying on an algorithm's confidence in its answer offers a valuable indicator over today's status quo of passively budgeting.\n sent4: While we acknowledge this method is not intended for our task specifically, it still serves as an important baseline (as discussed above).\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Our first aim is to answer the following questions: 1) How much answer diversity arises for visual questions? and 2) Why do people disagree?",
                "VQA Datasets: We conduct our analysis on a total of 459,861 visual questions and 4,598,610 answers coming from today's largest freely-available VQA benchmark #TARGET_REF .",
                "We chose this benchmark because it both represents a diversity of visual questions and includes many crowdsourced answers for every visual question.",
                "The benchmark consists of two datasets that reflect VQAs for real images and abstract scenes.",
                "Specifically, 80% (i.e., 369,861) of VQAs are about real images that show 91 types of objects that would be \"easily recognizable by a 4 year old\" in their natural context [21] ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Our first aim is to answer the following questions: 1) How much answer diversity arises for visual questions? and 2) Why do people disagree?\n sent1: VQA Datasets: We conduct our analysis on a total of 459,861 visual questions and 4,598,610 answers coming from today's largest freely-available VQA benchmark #TARGET_REF .\n sent2: We chose this benchmark because it both represents a diversity of visual questions and includes many crowdsourced answers for every visual question.\n sent3: The benchmark consists of two datasets that reflect VQAs for real images and abstract scenes.\n sent4: Specifically, 80% (i.e., 369,861) of VQAs are about real images that show 91 types of objects that would be \"easily recognizable by a 4 year old\" in their natural context [21] .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The benchmark includes a diversity of visual questions intentionally collected to be both grounded in images and taskindependent.",
                "Towards this aim, visual questions were collected by asking three Amazon Mechanical Turk (AMT) crowd workers to look at a given image and generate a text-based question about it that would \"stump a smart robot\" #TARGET_REF .",
                "Three open-ended questions were collected about each of 153,287 images, resulting in a total of 459,861 visual questions.",
                "The benchmark also includes 10 open-ended natural language answers from 10 AMT crowd workers per visual question.",
                "Each answer was collected by showing a worker an image with associated question and asking him/her to respond with \"a brief phrase and not a complete sentence\" [3] ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The benchmark includes a diversity of visual questions intentionally collected to be both grounded in images and taskindependent.\n sent1: Towards this aim, visual questions were collected by asking three Amazon Mechanical Turk (AMT) crowd workers to look at a given image and generate a text-based question about it that would \"stump a smart robot\" #TARGET_REF .\n sent2: Three open-ended questions were collected about each of 153,287 images, resulting in a total of 459,861 visual questions.\n sent3: The benchmark also includes 10 open-ended natural language answers from 10 AMT crowd workers per visual question.\n sent4: Each answer was collected by showing a worker an image with associated question and asking him/her to respond with \"a brief phrase and not a complete sentence\" [3] .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Towards this aim, visual questions were collected by asking three Amazon Mechanical Turk (AMT) crowd workers to look at a given image and generate a text-based question about it that would \"stump a smart robot\" [3] .",
                "Three open-ended questions were collected about each of 153,287 images, resulting in a total of 459,861 visual questions.",
                "The benchmark also includes 10 open-ended natural language answers from 10 AMT crowd workers per visual question.",
                "Each answer was collected by showing a worker an image with associated question and asking him/her to respond with \"a brief phrase and not a complete sentence\" #TARGET_REF .",
                "Finally, to enrich our analysis, we leverage the included labels which indicate the type of answer elicited for each visual question."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Towards this aim, visual questions were collected by asking three Amazon Mechanical Turk (AMT) crowd workers to look at a given image and generate a text-based question about it that would \"stump a smart robot\" [3] .\n sent1: Three open-ended questions were collected about each of 153,287 images, resulting in a total of 459,861 visual questions.\n sent2: The benchmark also includes 10 open-ended natural language answers from 10 AMT crowd workers per visual question.\n sent3: Each answer was collected by showing a worker an image with associated question and asking him/her to respond with \"a brief phrase and not a complete sentence\" #TARGET_REF .\n sent4: Finally, to enrich our analysis, we leverage the included labels which indicate the type of answer elicited for each visual question.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "We derive results using the 10 crowdsourced answers per visual question.",
                "We establish unique answers by pre-processing each answer to eliminate cosmetic differences and then applying exact string matching to identify the number of different answers.",
                "We pre-process each answer by converting all letters to lower case, converting numbers to digits, and removing punctuation and articles (i.e., \"a\", \"an\", \"the\"), as was done in prior work #TARGET_REF .",
                "While this approach does not fully resolve all conceptually equivalent responses, it does reveal an upper bound of expected answer diversity.",
                "In other words, more lenient agreement schemes (e.g., employing sophisticated natural language processing methods) would lead to either the same or less answer diversity."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We derive results using the 10 crowdsourced answers per visual question.\n sent1: We establish unique answers by pre-processing each answer to eliminate cosmetic differences and then applying exact string matching to identify the number of different answers.\n sent2: We pre-process each answer by converting all letters to lower case, converting numbers to digits, and removing punctuation and articles (i.e., \"a\", \"an\", \"the\"), as was done in prior work #TARGET_REF .\n sent3: While this approach does not fully resolve all conceptually equivalent responses, it does reveal an upper bound of expected answer diversity.\n sent4: In other words, more lenient agreement schemes (e.g., employing sophisticated natural language processing methods) would lead to either the same or less answer diversity.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We now describe our studies to assess the predictive power of our classification systems to decide whether visual questions will lead to answer (dis)agreement.",
                "We capitalize on today's largest visual question answering dataset #TARGET_REF to evaluate our prediction system, which includes 369,861 visual questions about real images.",
                "Of these, 248,349 visual questions (i.e., Training questions 2015 v1.0) are kept for for training and the remaining 121,512 visual questions (i.e., Validation questions 2015 v1.0) are employed for testing our classification system.",
                "This separation of training and testing samples enables us to estimate how well a classifier will generalize when applied to an unseen, independent set of visual questions.",
                "To our knowledge, no prior work has directly addressed predicting answer (dis)agreement for visual questions."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We now describe our studies to assess the predictive power of our classification systems to decide whether visual questions will lead to answer (dis)agreement.\n sent1: We capitalize on today's largest visual question answering dataset #TARGET_REF to evaluate our prediction system, which includes 369,861 visual questions about real images.\n sent2: Of these, 248,349 visual questions (i.e., Training questions 2015 v1.0) are kept for for training and the remaining 121,512 visual questions (i.e., Validation questions 2015 v1.0) are employed for testing our classification system.\n sent3: This separation of training and testing samples enables us to estimate how well a classifier will generalize when applied to an unseen, independent set of visual questions.\n sent4: To our knowledge, no prior work has directly addressed predicting answer (dis)agreement for visual questions.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We simulate answer collection by randomly selecting answers from the 10 crowd answers per visual question.",
                "Baselines: We compare our approach to the following baselines:",
                "VQA Algorithm #TARGET_REF :",
                "As in the previous section, we leverage the output confidence score from the publicly-shared model [25] learned from a LSTM-CNN deep learning architecture to rank the order of priority for visual questions to receive redundancy.",
                "Status Quo:"
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We simulate answer collection by randomly selecting answers from the 10 crowd answers per visual question.\n sent1: Baselines: We compare our approach to the following baselines:\n sent2: VQA Algorithm #TARGET_REF :\n sent3: As in the previous section, we leverage the output confidence score from the publicly-shared model [25] learned from a LSTM-CNN deep learning architecture to rank the order of priority for visual questions to receive redundancy.\n sent4: Status Quo:\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We observe that question-based features yield greater predictive performance than image-based features for all visual questions, when comparing AP scores for Q and I classification results ( Figure 5 ).",
                "In fact, image features contribute to performance improvements only for our random forest classifier for visual questions that lead to \"number\" answers, as illustrated by comparing AP scores for Our RF: Q+I and Our RF: Q (Figure 5b) .",
                "Our overall finding that most of the predictive power stems from language-based features parallels feature analysis findings in the automated VQA literature #TARGET_REF , [22] .",
                "This does not mean, however, that the image content is not predictive.",
                "Further work improving visual content cues for VQA agreement is warranted."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: We observe that question-based features yield greater predictive performance than image-based features for all visual questions, when comparing AP scores for Q and I classification results ( Figure 5 ).\n sent1: In fact, image features contribute to performance improvements only for our random forest classifier for visual questions that lead to \"number\" answers, as illustrated by comparing AP scores for Our RF: Q+I and Our RF: Q (Figure 5b) .\n sent2: Our overall finding that most of the predictive power stems from language-based features parallels feature analysis findings in the automated VQA literature #TARGET_REF , [22] .\n sent3: This does not mean, however, that the image content is not predictive.\n sent4: Further work improving visual content cues for VQA agreement is warranted.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We present a simple and efficient unsupervised method for pairwise matching of documents from heterogeneous collections.",
                "Following #TARGET_REF , we consider two document collections heterogeneous if #TARGET_REF documents differ systematically with respect to vocabulary and / or level of abstraction.",
                "With these defining differences, there often also comes a difference in length, which, however, by itself does not make document collections heterogeneous.",
                "Examples include collections in which expert answers are mapped to non-expert questions (e.g. InsuranceQA by #REF ), but also so-called community QA collections (#REF ), where the lexical mismatch between Q and A documents is often less pronounced than the length difference.",
                "Like many other approaches, the proposed method is based on word embeddings as universal meaning representations, and on vector cosine as the similarity metric."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: We present a simple and efficient unsupervised method for pairwise matching of documents from heterogeneous collections.\n sent1: Following #TARGET_REF , we consider two document collections heterogeneous if #TARGET_REF documents differ systematically with respect to vocabulary and / or level of abstraction.\n sent2: With these defining differences, there often also comes a difference in length, which, however, by itself does not make document collections heterogeneous.\n sent3: Examples include collections in which expert answers are mapped to non-expert questions (e.g. InsuranceQA by #REF ), but also so-called community QA collections (#REF ), where the lexical mismatch between Q and A documents is often less pronounced than the length difference.\n sent4: Like many other approaches, the proposed method is based on word embeddings as universal meaning representations, and on vector cosine as the similarity metric.\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF do not provide any specification, or annotation guidelines, of the semantics of the 'matches' relation to be annotated.",
                "Instead, they create gold standard annotations based on a majority vote of three manual annotations.",
                "Figure 1 provides an example of a matching C-P pair.",
                "The concept labels can be very specific, potentially introducing vocabulary that is not present in the actual concept descriptions.",
                "The extent to which this information is used by #TARGET_REF is not entirely clear, so we experiment with several setups (cf. Section 4)."
            ],
            "label": [
                "MOTIVATION",
                null,
                "EXTENDS"
            ]
        },
        "input": "sent0: #REF do not provide any specification, or annotation guidelines, of the semantics of the 'matches' relation to be annotated.\n sent1: Instead, they create gold standard annotations based on a majority vote of three manual annotations.\n sent2: Figure 1 provides an example of a matching C-P pair.\n sent3: The concept labels can be very specific, potentially introducing vocabulary that is not present in the actual concept descriptions.\n sent4: The extent to which this information is used by #TARGET_REF is not entirely clear, so we experiment with several setups (cf. Section 4).\n",
        "output": "{\"label\": [\"MOTIVATION\", null, \"EXTENDS\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "We presented a simple method for semantic matching of documents from heterogeneous collections as a solution to the Concept-Project matching task by #TARGET_REF .",
                "Although much simpler, our method clearly outperformed the original system in most input settings.",
                "Another result is that, contrary to the claim made by #REF , the standard averaging approach does indeed work very well even for heterogeneous document collections, if appropriate weighting is applied.",
                "Due to its simplicity, we believe that our method can also be applied to other text matching tasks, including more 'standard' ones which do not necessarily involve heterogeneous document collections.",
                "This seems desirable because our method offers additional transparency by providing not only a similarity score, but also the subset of words on which the similarity score is based."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: We presented a simple method for semantic matching of documents from heterogeneous collections as a solution to the Concept-Project matching task by #TARGET_REF .\n sent1: Although much simpler, our method clearly outperformed the original system in most input settings.\n sent2: Another result is that, contrary to the claim made by #REF , the standard averaging approach does indeed work very well even for heterogeneous document collections, if appropriate weighting is applied.\n sent3: Due to its simplicity, we believe that our method can also be applied to other text matching tasks, including more 'standard' ones which do not necessarily involve heterogeneous document collections.\n sent4: This seems desirable because our method offers additional transparency by providing not only a similarity score, but also the subset of words on which the similarity score is based.\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Examples include collections in which expert answers are mapped to non-expert questions (e.g. InsuranceQA by #REF ), but also so-called community QA collections (#REF ), where the lexical mismatch between Q and A documents is often less pronounced than the length difference.",
                "Like many other approaches, the proposed method is based on word embeddings as universal meaning representations, and on vector cosine as the similarity metric.",
                "However, instead of computing pairs of document representations and measuring their similarity, our method assesses the document-pair similarity on the basis of selected pairwise word similarities.",
                "This has the following advantages, which make our method a viable candidate for practical, real-world applications: efficiency, because pairwise word similarities can be efficiently (pre-)computed and cached, and transparency, because the selected words from each document are available as evidence for what the similarity computation was based on.",
                "We demonstrate our method with the Concept-Project matching task ( #TARGET_REF ), which is described in the next section."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Examples include collections in which expert answers are mapped to non-expert questions (e.g. InsuranceQA by #REF ), but also so-called community QA collections (#REF ), where the lexical mismatch between Q and A documents is often less pronounced than the length difference.\n sent1: Like many other approaches, the proposed method is based on word embeddings as universal meaning representations, and on vector cosine as the similarity metric.\n sent2: However, instead of computing pairs of document representations and measuring their similarity, our method assesses the document-pair similarity on the basis of selected pairwise word similarities.\n sent3: This has the following advantages, which make our method a viable candidate for practical, real-world applications: efficiency, because pairwise word similarities can be efficiently (pre-)computed and cached, and transparency, because the selected words from each document are available as evidence for what the similarity computation was based on.\n sent4: We demonstrate our method with the Concept-Project matching task ( #TARGET_REF ), which is described in the next section.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "As a second baseline, they use Word Mover's Distance (#REF ), which is based on word-level distances, rather than distance of global document representations, but which also fails to be competitive with their topic-based method.",
                "#REF use two different sets of word embeddings: One (topic wiki) was trained on a full English Wikipedia dump, the other (wiki science) on a smaller subset of the former dump which only contained science articles.",
                "the similarity of two documents (i.e. word sequences) c and p is to average over the word embeddings for each sequence first, and to compute the cosine similarity between the two averages afterwards.",
                "In the first step, weighting can be applied by multiplying a vector with the TF, IDF, or TF*IDF score of its pertaining word.",
                "We implement this standard measure (AVG COS SIM) as a baseline for both our method and for the method by #TARGET_REF ."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: As a second baseline, they use Word Mover's Distance (#REF ), which is based on word-level distances, rather than distance of global document representations, but which also fails to be competitive with their topic-based method.\n sent1: #REF use two different sets of word embeddings: One (topic wiki) was trained on a full English Wikipedia dump, the other (wiki science) on a smaller subset of the former dump which only contained science articles.\n sent2: the similarity of two documents (i.e. word sequences) c and p is to average over the word embeddings for each sequence first, and to compute the cosine similarity between the two averages afterwards.\n sent3: In the first step, weighting can be applied by multiplying a vector with the TF, IDF, or TF*IDF score of its pertaining word.\n sent4: We implement this standard measure (AVG COS SIM) as a baseline for both our method and for the method by #TARGET_REF .\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "The results can be found in Table 3 .",
                "For comparison, the two top rows provide the best results of #REF .",
                "The first interesting finding is that the AVG COS SIM measure again performs very well: In all three settings, it beats both the system based on general-purpose embeddings (topic wiki) and the one that is adapted to the science domain (topic science), with again the Both setting yielding the best overall result (.926).",
                "Note that our Both setting is probably the one most similar to the concept input used by #TARGET_REF .",
                "This result corroborates our findings on the tuning data, and clearly contradicts the (implicit) claim made by #REF regarding the infeasibility of document-level matching for documents of different lengths."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: The results can be found in Table 3 .\n sent1: For comparison, the two top rows provide the best results of #REF .\n sent2: The first interesting finding is that the AVG COS SIM measure again performs very well: In all three settings, it beats both the system based on general-purpose embeddings (topic wiki) and the one that is adapted to the science domain (topic science), with again the Both setting yielding the best overall result (.926).\n sent3: Note that our Both setting is probably the one most similar to the concept input used by #TARGET_REF .\n sent4: This result corroborates our findings on the tuning data, and clearly contradicts the (implicit) claim made by #REF regarding the infeasibility of document-level matching for documents of different lengths.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "This result corroborates our findings on the tuning data, and clearly contradicts the (implicit) claim made by #REF regarding the infeasibility of document-level matching for documents of different lengths.",
                "The second, more important finding is that our proposed TOP n COS SIM AVG measure is also very competitive, as it also outperforms both systems by #TARGET_REF in two out of three settings.",
                "It only fails in the setting using only the Description input.",
                "8 This is the more important as we exclusively employ off-the-shelf, general-purpose embeddings, while #REF reach their best results with a much more sophisticated system and with embeddings that were custom-trained for the science domain.",
                "Thus, while the performance of our proposed TOP n COS SIM AVG method is superior to the approach by #REF , it is itself outperformed by the 'baseline' AVG COS SIM method with appropriate weighting."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: This result corroborates our findings on the tuning data, and clearly contradicts the (implicit) claim made by #REF regarding the infeasibility of document-level matching for documents of different lengths.\n sent1: The second, more important finding is that our proposed TOP n COS SIM AVG measure is also very competitive, as it also outperforms both systems by #TARGET_REF in two out of three settings.\n sent2: It only fails in the setting using only the Description input.\n sent3: 8 This is the more important as we exclusively employ off-the-shelf, general-purpose embeddings, while #REF reach their best results with a much more sophisticated system and with embeddings that were custom-trained for the science domain.\n sent4: Thus, while the performance of our proposed TOP n COS SIM AVG method is superior to the approach by #REF , it is itself outperformed by the 'baseline' AVG COS SIM method with appropriate weighting.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Starting from word embeddings, researchers proposed various ways of aggregating word embedding vectors to obtain efficient sentence-level or documentlevel representations #REF; #REF; #REF; #REF; #REF; #TARGET_REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF #REF .",
                "Although the mean (or sum) of word vectors is commonly adopted because of its simplicity (#REF) , it seems that more complex approaches usually yield better performance (#REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF #REF .",
                "To this end, we propose a simple yet effective approach for aggregating word embeddings into document embeddings.",
                "Our approach is inspired by the Vector of Locally-Aggregated Descriptors (VLAD) (Jégou et al., 2010 (Jégou et al., , 2012 used in computer vision to efficiently represent images for various image classification and retrieval tasks.",
                "To our knowledge, we are the first to adapt and use VLAD in the text domain."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Starting from word embeddings, researchers proposed various ways of aggregating word embedding vectors to obtain efficient sentence-level or documentlevel representations #REF; #REF; #REF; #REF; #REF; #TARGET_REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF #REF .\n sent1: Although the mean (or sum) of word vectors is commonly adopted because of its simplicity (#REF) , it seems that more complex approaches usually yield better performance (#REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF #REF .\n sent2: To this end, we propose a simple yet effective approach for aggregating word embeddings into document embeddings.\n sent3: Our approach is inspired by the Vector of Locally-Aggregated Descriptors (VLAD) (Jégou et al., 2010 (Jégou et al., , 2012 used in computer vision to efficiently represent images for various image classification and retrieval tasks.\n sent4: To our knowledge, we are the first to adapt and use VLAD in the text domain.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "In recent years, word embeddings (#REF; #REF; #REF; #REF) have had a huge impact in natural language processing (NLP) and related fields, being used in many tasks including sentiment analysis (Dos #REF; #REF) , information retrieval (#REF; #REF) and word sense disambiguation (#REF; #REF; #REF) , among many others.",
                "Starting from word embeddings, researchers proposed various ways of aggregating word embedding vectors to obtain efficient sentence-level or documentlevel representations #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF #REF .",
                "Although the mean (or sum) of word vectors is commonly adopted because of its simplicity (#REF) , it seems that more complex approaches usually yield better performance (#REF; #REF; #REF; #REF; #TARGET_REF; #REF; #REF; #REF; #REF #REF .",
                "To this end, we propose a simple yet effective approach for aggregating word embeddings into document embeddings.",
                "Our approach is inspired by the Vector of Locally-Aggregated Descriptors (VLAD) (Jégou et al., 2010 (Jégou et al., , 2012 used in computer vision to efficiently represent images for various image classification and retrieval tasks."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In recent years, word embeddings (#REF; #REF; #REF; #REF) have had a huge impact in natural language processing (NLP) and related fields, being used in many tasks including sentiment analysis (Dos #REF; #REF) , information retrieval (#REF; #REF) and word sense disambiguation (#REF; #REF; #REF) , among many others.\n sent1: Starting from word embeddings, researchers proposed various ways of aggregating word embedding vectors to obtain efficient sentence-level or documentlevel representations #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF #REF .\n sent2: Although the mean (or sum) of word vectors is commonly adopted because of its simplicity (#REF) , it seems that more complex approaches usually yield better performance (#REF; #REF; #REF; #REF; #TARGET_REF; #REF; #REF; #REF; #REF #REF .\n sent3: To this end, we propose a simple yet effective approach for aggregating word embeddings into document embeddings.\n sent4: Our approach is inspired by the Vector of Locally-Aggregated Descriptors (VLAD) (Jégou et al., 2010 (Jégou et al., , 2012 used in computer vision to efficiently represent images for various image classification and retrieval tasks.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We consider five benchmark data sets: #REF8 (#REF) , RT-2k (#REF) , MR (#REF) , TREC (#REF) and Subj (#REF) .",
                "We compare VLAWE with recent stateof-the-art methods #REF; #REF; #TARGET_REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF #REF , demonstrating the effectiveness of our approach.",
                "Furthermore, we obtain a considerable improvement on the Movie Review (MR) data set, surpassing the state-of-the-art approach of #REF by almost 10%.",
                "The rest of the paper is organized as follows.",
                "We present related works on learning documentlevel representations in Section 2."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: We consider five benchmark data sets: #REF8 (#REF) , RT-2k (#REF) , MR (#REF) , TREC (#REF) and Subj (#REF) .\n sent1: We compare VLAWE with recent stateof-the-art methods #REF; #REF; #TARGET_REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF #REF , demonstrating the effectiveness of our approach.\n sent2: Furthermore, we obtain a considerable improvement on the Movie Review (MR) data set, surpassing the state-of-the-art approach of #REF by almost 10%.\n sent3: The rest of the paper is organized as follows.\n sent4: We present related works on learning documentlevel representations in Section 2.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "There are various works #REF; #REF; #REF; #TARGET_REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF ) that propose to build effective sentence-level or document-level representations based on word embeddings.",
                "While most of these approaches are based on deep learning (#REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF) , there have been some approaches that are inspired by computer vision research, namely by the bag-of-visual-words and by Fisher Vectors (#REF) .",
                "The relationship between the bag-of-visual-words, Fisher Vectors and VLAD is discussed in (Jégou et al., 2012) .",
                "The discussion can be transferred to describe the relantionship of our work and the closely-related works of and #REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: There are various works #REF; #REF; #REF; #TARGET_REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF ) that propose to build effective sentence-level or document-level representations based on word embeddings.\n sent1: While most of these approaches are based on deep learning (#REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF) , there have been some approaches that are inspired by computer vision research, namely by the bag-of-visual-words and by Fisher Vectors (#REF) .\n sent2: The relationship between the bag-of-visual-words, Fisher Vectors and VLAD is discussed in (Jégou et al., 2012) .\n sent3: The discussion can be transferred to describe the relantionship of our work and the closely-related works of and #REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "There are various works #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF ) that propose to build effective sentence-level or document-level representations based on word embeddings.",
                "While most of these approaches are based on deep learning (#REF; #REF; #TARGET_REF; #REF; #REF; #REF; #REF; #REF; #REF) , there have been some approaches that are inspired by computer vision research, namely by the bag-of-visual-words and by Fisher Vectors (#REF) .",
                "The relationship between the bag-of-visual-words, Fisher Vectors and VLAD is discussed in (Jégou et al., 2012) .",
                "The discussion can be transferred to describe the relantionship of our work and the closely-related works of and #REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: There are various works #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF ) that propose to build effective sentence-level or document-level representations based on word embeddings.\n sent1: While most of these approaches are based on deep learning (#REF; #REF; #TARGET_REF; #REF; #REF; #REF; #REF; #REF; #REF) , there have been some approaches that are inspired by computer vision research, namely by the bag-of-visual-words and by Fisher Vectors (#REF) .\n sent2: The relationship between the bag-of-visual-words, Fisher Vectors and VLAD is discussed in (Jégou et al., 2012) .\n sent3: The discussion can be transferred to describe the relantionship of our work and the closely-related works of and #REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "In the learning stage, we employ the Support Vector Machines (SVM) implementation provided by LibSVM (#REF Table 1 : Performance results (in %) of our approach (VLAWE) versus several state-of-the-art methods #REF; #REF; #TARGET_REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF #REF on the #REF8, RT-2k, MR, TREC and Subj data sets.",
                "The top three results on each data set are highlighted in red, green and blue, respectively.",
                "Best viewed in color.",
                "set the SVM regularization parameter to C = 1 in all our experiments.",
                "In the SVM, we use the linear kernel."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In the learning stage, we employ the Support Vector Machines (SVM) implementation provided by LibSVM (#REF Table 1 : Performance results (in %) of our approach (VLAWE) versus several state-of-the-art methods #REF; #REF; #TARGET_REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF #REF on the #REF8, RT-2k, MR, TREC and Subj data sets.\n sent1: The top three results on each data set are highlighted in red, green and blue, respectively.\n sent2: Best viewed in color.\n sent3: set the SVM regularization parameter to C = 1 in all our experiments.\n sent4: In the SVM, we use the linear kernel.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "We compare VLAWE with several state-of-theart methods #REF; #REF; #TARGET_REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF #REF as well as two baseline methods, namely the average of word embeddings and the standard bag-of-words (BOW).",
                "The corresponding results are presented in Table 1 .",
                "First, we notice that our approach outperforms both baselines on all data sets, unlike other related methods (#REF; #REF) .",
                "In most cases, our improvements over the baselines are higher than 5%.",
                "On the #REF8 data set, we surpass the closely-related approach"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: We compare VLAWE with several state-of-theart methods #REF; #REF; #TARGET_REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF #REF as well as two baseline methods, namely the average of word embeddings and the standard bag-of-words (BOW).\n sent1: The corresponding results are presented in Table 1 .\n sent2: First, we notice that our approach outperforms both baselines on all data sets, unlike other related methods (#REF; #REF) .\n sent3: In most cases, our improvements over the baselines are higher than 5%.\n sent4: On the #REF8 data set, we surpass the closely-related approach\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Best viewed in color.",
                "set the SVM regularization parameter to C = 1 in all our experiments.",
                "In the SVM, we use the linear kernel.",
                "For optimal results, the VLAWE representation is combined with the BOSWE representation , which is based on the PQ kernel #REF, 2015b) .",
                "We follow the same evaluation procedure as #REF and #TARGET_REF , using 10-fold cross-validation when a train and test split is not pre-defined for a given data set."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: Best viewed in color.\n sent1: set the SVM regularization parameter to C = 1 in all our experiments.\n sent2: In the SVM, we use the linear kernel.\n sent3: For optimal results, the VLAWE representation is combined with the BOSWE representation , which is based on the PQ kernel #REF, 2015b) .\n sent4: We follow the same evaluation procedure as #REF and #TARGET_REF , using 10-fold cross-validation when a train and test split is not pre-defined for a given data set.\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "The corresponding results are presented in Table 1 .",
                "First, we notice that our approach outperforms both baselines on all data sets, unlike other related methods (#REF; #TARGET_REF .",
                "In most cases, our improvements over the baselines are higher than 5%.",
                "On the #REF8 data set, we surpass the closely-related approach",
                "93.2 VLAWE (full, k = 10) 93.3 Table 2 : Performance results (in %) of the full VLAWE representation (with k = 10) versus two compact versions of VLAWE, obtained either by setting k = 2 or by applying PCA."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: The corresponding results are presented in Table 1 .\n sent1: First, we notice that our approach outperforms both baselines on all data sets, unlike other related methods (#REF; #TARGET_REF .\n sent2: In most cases, our improvements over the baselines are higher than 5%.\n sent3: On the #REF8 data set, we surpass the closely-related approach\n sent4: 93.2 VLAWE (full, k = 10) 93.3 Table 2 : Performance results (in %) of the full VLAWE representation (with k = 10) versus two compact versions of VLAWE, obtained either by setting k = 2 or by applying PCA.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The title of our talk-an implicit reference to the English cliché like a spider weaving her webintends to attract one's attention to the metaphor that can be drawn between the dance of a spider weaving her web and a new lexicographic gesture that is gradually emerging from the work on Net-like lexical resources (#REF; #REF; #TARGET_REF .",
                "Our claim is that the inherent graph structure of natural language lexicons not only determine vocabulary acquisition and use (#REF) , but also lexicographic activity.",
                "In that respect, reflecting on new ways to implement the task of building lexical resources is essential for lexicographers themselves, but also for anyone interested is lexicons as mental structures.",
                "After all, lexicographers and language learners are those who have the most direct contact with lexical structures, through closely related activities: describing a natural phenomenon is a form of learning through explicit conceptualization.",
                "Lexicographers often experience the fact that by completing the description of a word they achieve a form of understanding and mastering of this word."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The title of our talk-an implicit reference to the English cliché like a spider weaving her webintends to attract one's attention to the metaphor that can be drawn between the dance of a spider weaving her web and a new lexicographic gesture that is gradually emerging from the work on Net-like lexical resources (#REF; #REF; #TARGET_REF .\n sent1: Our claim is that the inherent graph structure of natural language lexicons not only determine vocabulary acquisition and use (#REF) , but also lexicographic activity.\n sent2: In that respect, reflecting on new ways to implement the task of building lexical resources is essential for lexicographers themselves, but also for anyone interested is lexicons as mental structures.\n sent3: After all, lexicographers and language learners are those who have the most direct contact with lexical structures, through closely related activities: describing a natural phenomenon is a form of learning through explicit conceptualization.\n sent4: Lexicographers often experience the fact that by completing the description of a word they achieve a form of understanding and mastering of this word.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "The title of our talk-an implicit reference to the English cliché like a spider weaving her webintends to attract one's attention to the metaphor that can be drawn between the dance of a spider weaving her web and a new lexicographic gesture that is gradually emerging from the work on Net-like lexical resources (#REF; #REF; #TARGET_REF .",
                "Our claim is that the inherent graph structure of natural language lexicons not only determine vocabulary acquisition and use (#REF) , but also lexicographic activity.",
                "In that respect, reflecting on new ways to implement the task of building lexical resources is essential for lexicographers themselves, but also for anyone interested is lexicons as mental structures.",
                "After all, lexicographers and language learners are those who have the most direct contact with lexical structures, through closely related activities: describing a natural phenomenon is a form of learning through explicit conceptualization.",
                "Lexicographers often experience the fact that by completing the description of a word they achieve a form of understanding and mastering of this word."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The title of our talk-an implicit reference to the English cliché like a spider weaving her webintends to attract one's attention to the metaphor that can be drawn between the dance of a spider weaving her web and a new lexicographic gesture that is gradually emerging from the work on Net-like lexical resources (#REF; #REF; #TARGET_REF .\n sent1: Our claim is that the inherent graph structure of natural language lexicons not only determine vocabulary acquisition and use (#REF) , but also lexicographic activity.\n sent2: In that respect, reflecting on new ways to implement the task of building lexical resources is essential for lexicographers themselves, but also for anyone interested is lexicons as mental structures.\n sent3: After all, lexicographers and language learners are those who have the most direct contact with lexical structures, through closely related activities: describing a natural phenomenon is a form of learning through explicit conceptualization.\n sent4: Lexicographers often experience the fact that by completing the description of a word they achieve a form of understanding and mastering of this word.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "In our talk, we take the above observations as given, including the fact that lexicography should indeed be targeting virtual dictionaries, generated from non-textual lexical models .",
                "We illustrate how the lexicographic process of building graph-based lexical models can benefit from tools that allow lexicographers to wade through the lexical web, following paradigmatic and syntagmatic paths, while simultaneously weaving new links and incrementing the lexical description.",
                "Work performed on the French Lexical Network #TARGET_REF ) will serve to demonstrate how the lexicographic process can be made closer to actual navigation through lexical knowledge by the speaker.",
                "The main theoretical and descriptive tool that makes such navigation possible is the system of lexical functions proposed by the Meaning-Text linguistic approach (Mel'čuk, 1996) .",
                "It induces the multidimensional and non-hierarchical graph structure of the FLN that, we believe, is far better suited for designing lexical resources than hyperonymy-based structures."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: In our talk, we take the above observations as given, including the fact that lexicography should indeed be targeting virtual dictionaries, generated from non-textual lexical models .\n sent1: We illustrate how the lexicographic process of building graph-based lexical models can benefit from tools that allow lexicographers to wade through the lexical web, following paradigmatic and syntagmatic paths, while simultaneously weaving new links and incrementing the lexical description.\n sent2: Work performed on the French Lexical Network #TARGET_REF ) will serve to demonstrate how the lexicographic process can be made closer to actual navigation through lexical knowledge by the speaker.\n sent3: The main theoretical and descriptive tool that makes such navigation possible is the system of lexical functions proposed by the Meaning-Text linguistic approach (Mel'čuk, 1996) .\n sent4: It induces the multidimensional and non-hierarchical graph structure of the FLN that, we believe, is far better suited for designing lexical resources than hyperonymy-based structures.\n",
        "output": "{\"label\": [null], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Work performed on the French Lexical Network (#REF ) will serve to demonstrate how the lexicographic process can be made closer to actual navigation through lexical knowledge by the speaker.",
                "The main theoretical and descriptive tool that makes such navigation possible is the system of lexical functions proposed by the Meaning-Text linguistic approach (Mel'čuk, 1996) .",
                "It induces the multidimensional and non-hierarchical graph structure of the FLN that, we believe, is far better suited for designing lexical resources than hyperonymy-based structures.",
                "Computational aspects of the work on the French Lexical Network are dealt with in #TARGET_REF .",
                "In our presentation, we focus on the actual process of weaving lexical relations."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Work performed on the French Lexical Network (#REF ) will serve to demonstrate how the lexicographic process can be made closer to actual navigation through lexical knowledge by the speaker.\n sent1: The main theoretical and descriptive tool that makes such navigation possible is the system of lexical functions proposed by the Meaning-Text linguistic approach (Mel'čuk, 1996) .\n sent2: It induces the multidimensional and non-hierarchical graph structure of the FLN that, we believe, is far better suited for designing lexical resources than hyperonymy-based structures.\n sent3: Computational aspects of the work on the French Lexical Network are dealt with in #TARGET_REF .\n sent4: In our presentation, we focus on the actual process of weaving lexical relations.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "For these reasons, to obtain a more interesting comparison, we modified CoMiC-EN to perform scoring instead of meaning comparison.",
                "This means that the memory-based learning approach CoMiC-EN had employed so far was no longer applicable and had to be replaced with a regression-capable learning strategy.",
                "We chose Support Vector Regression (SVR) using libSVM 4 since that is one of the methods employed by #TARGET_REF .",
                "However, all other parts of CoMiC-EN such as the processing pipeline and the alignment approach and the extracted features remained the same.",
                "The evaluation procedure was carried out as a 12-fold cross-validation due to the 12 assignments in the Texas corpus."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: For these reasons, to obtain a more interesting comparison, we modified CoMiC-EN to perform scoring instead of meaning comparison.\n sent1: This means that the memory-based learning approach CoMiC-EN had employed so far was no longer applicable and had to be replaced with a regression-capable learning strategy.\n sent2: We chose Support Vector Regression (SVR) using libSVM 4 since that is one of the methods employed by #TARGET_REF .\n sent3: However, all other parts of CoMiC-EN such as the processing pipeline and the alignment approach and the extracted features remained the same.\n sent4: The evaluation procedure was carried out as a 12-fold cross-validation due to the 12 assignments in the Texas corpus.\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent0\", \"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "This approach is also supported by recent literature (cf., e.g., ).",
                "However, for the Texas corpus, #TARGET_REF have opted to use the arithmetic mean of the two graders as gold standard.",
                "While mathematically a viable solution, it seems questionable whether the mean is reliable with only two graders, especially if they have not operated on the grounds of explicit guidelines.",
                "It would be interesting to see whether in this case, a system trained on more, singly annotated data would perform better than one on less, doubly annotated data, as argued for by #REF .",
                "In any case, if many disagreements occur, one should ask the question whether the annotation task is defined well enough and whether machines should really be expected to perform it consistently if humans have trouble doing so."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: This approach is also supported by recent literature (cf., e.g., ).\n sent1: However, for the Texas corpus, #TARGET_REF have opted to use the arithmetic mean of the two graders as gold standard.\n sent2: While mathematically a viable solution, it seems questionable whether the mean is reliable with only two graders, especially if they have not operated on the grounds of explicit guidelines.\n sent3: It would be interesting to see whether in this case, a system trained on more, singly annotated data would perform better than one on less, doubly annotated data, as argued for by #REF .\n sent4: In any case, if many disagreements occur, one should ask the question whether the annotation task is defined well enough and whether machines should really be expected to perform it consistently if humans have trouble doing so.\n",
        "output": "{\"label\": [null], \"context\": [\"sent1\", \"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Their results also reveal the differences between these methodologies in their assessment of topic coherence.",
                "A hyper-parameter in all these methodologies is the number of topic words, or its cardinality.",
                "These methodologies evaluate coherence over the top-N topic words, where N is selected arbitrarily: for #REF , N = 5, whereas for #TARGET_REF , #REF and #REF , N = 10.",
                "The germ of this paper came when using the automatic word intrusion methodology (#REF) , and noticing that introducing one extra word to a given topic can dramatically change the accuracy of intruder word prediction.",
                "This forms the kernel of this paper: to better understand the impact of the topic cardinality hyper-parameter on the evaluation of topic coherence."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Their results also reveal the differences between these methodologies in their assessment of topic coherence.\n sent1: A hyper-parameter in all these methodologies is the number of topic words, or its cardinality.\n sent2: These methodologies evaluate coherence over the top-N topic words, where N is selected arbitrarily: for #REF , N = 5, whereas for #TARGET_REF , #REF and #REF , N = 10.\n sent3: The germ of this paper came when using the automatic word intrusion methodology (#REF) , and noticing that introducing one extra word to a given topic can dramatically change the accuracy of intruder word prediction.\n sent4: This forms the kernel of this paper: to better understand the impact of the topic cardinality hyper-parameter on the evaluation of topic coherence.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "To examine the relationship between topic cardinality and topic coherence, we require a dataset that has topics for a range of cardinality settings.",
                "Although there are existing datasets with human-annotated coherence scores #TARGET_REF; #REF; #REF; #REF) , these topics were annotated using a fixed cardinality setting (e.g. 5 or 10).",
                "We thus develop a new dataset for this experiment.",
                "Following #REF , we use two domains: (1) WIKI, a collection of 3.3 million English Wikipedia articles (retrieved November 28th 2009); and (2) NEWS, a collection of 1.2 million New York Times articles from 1994 to 2004 (English Gigaword).",
                "We sub-sample approximately 50M tokens (100K and 50K articles for WIKI and NEWS respectively) from both domains to create two smaller document collections."
            ],
            "label": [
                "BACKGROUND",
                "DIFFERENCES"
            ]
        },
        "input": "sent0: To examine the relationship between topic cardinality and topic coherence, we require a dataset that has topics for a range of cardinality settings.\n sent1: Although there are existing datasets with human-annotated coherence scores #TARGET_REF; #REF; #REF; #REF) , these topics were annotated using a fixed cardinality setting (e.g. 5 or 10).\n sent2: We thus develop a new dataset for this experiment.\n sent3: Following #REF , we use two domains: (1) WIKI, a collection of 3.3 million English Wikipedia articles (retrieved November 28th 2009); and (2) NEWS, a collection of 1.2 million New York Times articles from 1994 to 2004 (English Gigaword).\n sent4: We sub-sample approximately 50M tokens (100K and 50K articles for WIKI and NEWS respectively) from both domains to create two smaller document collections.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"DIFFERENCES\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We then generate 300 LDA topics for each of the sub-sampled collection.",
                "2 There are two primary approaches to assessing topic coherence: (1) via word intrusion (Chang et (2) by directly measuring observed coherence #TARGET_REF; #REF) .",
                "With the first method, #REF injects an intruder word into the top-5 topic words, shuffles the topic words, and sets the task of selecting the single intruder word out of the 6 words.",
                "In preliminary experiments, we found that the word intrusion task becomes unreasonably difficult for human annotators when the topic cardinality is high, e.g. when N = 20.",
                "As such, we use the second approach as the means for generating our gold standard, asking users to judge topic coherence directly over different topic cardinalities."
            ],
            "label": [
                "BACKGROUND",
                "USE"
            ]
        },
        "input": "sent0: We then generate 300 LDA topics for each of the sub-sampled collection.\n sent1: 2 There are two primary approaches to assessing topic coherence: (1) via word intrusion (Chang et (2) by directly measuring observed coherence #TARGET_REF; #REF) .\n sent2: With the first method, #REF injects an intruder word into the top-5 topic words, shuffles the topic words, and sets the task of selecting the single intruder word out of the 6 words.\n sent3: In preliminary experiments, we found that the word intrusion task becomes unreasonably difficult for human annotators when the topic cardinality is high, e.g. when N = 20.\n sent4: As such, we use the second approach as the means for generating our gold standard, asking users to judge topic coherence directly over different topic cardinalities.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"USE\"], \"context\": [\"sent1\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "To investigate this, we develop a new dataset with human-annotated coherence judgements for a range of cardinality settings (N = {5, 10, 15, 20}).",
                "We experiment with the automatic word intrusion (#REF) and discover that correlation with human ratings decreases systematically as cardinality increases.",
                "We also test the PMI methodology #TARGET_REF and make the same observation.",
                "To remedy this, we show that performance can be substantially improved if system scores and human ratings are aggregated over different cardinality settings before computing the correlation.",
                "This has broad implications for topic model evaluation."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: To investigate this, we develop a new dataset with human-annotated coherence judgements for a range of cardinality settings (N = {5, 10, 15, 20}).\n sent1: We experiment with the automatic word intrusion (#REF) and discover that correlation with human ratings decreases systematically as cardinality increases.\n sent2: We also test the PMI methodology #TARGET_REF and make the same observation.\n sent3: To remedy this, we show that performance can be substantially improved if system scores and human ratings are aggregated over different cardinality settings before computing the correlation.\n sent4: This has broad implications for topic model evaluation.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "With the first method, #REF injects an intruder word into the top-5 topic words, shuffles the topic words, and sets the task of selecting the single intruder word out of the 6 words.",
                "In preliminary experiments, we found that the word intrusion task becomes unreasonably difficult for human annotators when the topic cardinality is high, e.g. when N = 20.",
                "As such, we use the second approach as the means for generating our gold standard, asking users to judge topic coherence directly over different topic cardinalities.",
                "3 To collect the coherence judgements, we used Amazon Mechanical Turk and asked Turkers to rate topics in terms of coherence using a 3-point ordinal scale, where 1 indicates incoherent and 3 very coherent #TARGET_REF .",
                "For each topic (600 topics in total) we experiment with 4 cardinality settings: N = {5, 10, 15, 20}. For example, for N = 5, we display the top-5 topic words for coherence judgement."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: With the first method, #REF injects an intruder word into the top-5 topic words, shuffles the topic words, and sets the task of selecting the single intruder word out of the 6 words.\n sent1: In preliminary experiments, we found that the word intrusion task becomes unreasonably difficult for human annotators when the topic cardinality is high, e.g. when N = 20.\n sent2: As such, we use the second approach as the means for generating our gold standard, asking users to judge topic coherence directly over different topic cardinalities.\n sent3: 3 To collect the coherence judgements, we used Amazon Mechanical Turk and asked Turkers to rate topics in terms of coherence using a 3-point ordinal scale, where 1 indicates incoherent and 3 very coherent #TARGET_REF .\n sent4: For each topic (600 topics in total) we experiment with 4 cardinality settings: N = {5, 10, 15, 20}. For example, for N = 5, we display the top-5 topic words for coherence judgement.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Traditional relation-extraction systems rely on manual annotations or domain-specific rules provided by experts, both of which are scarce resources that are not portable across domains.",
                "To remedy these problems, recent years have seen interest in the distant supervision approach for relation extraction (#REF; #TARGET_REF .",
                "The input to distant supervision is a set of seed facts for the target relation together with an (unlabeled) text corpus, and the output is a set of (noisy) annotations that can be used by any machine learning technique to train a statistical model for the target relation.",
                "For example, given the target relation birthPlace(person, place) and a seed fact birthPlace(John, Springfield), the sentence \"John and his wife were born in Springfield in 1946\" (S1) would qualify as a positive training example.",
                "Distant supervision replaces the expensive process of manually acquiring annotations that is required by direct supervision with resources that already exist in many scenarios (seed facts and a text corpus)."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Traditional relation-extraction systems rely on manual annotations or domain-specific rules provided by experts, both of which are scarce resources that are not portable across domains.\n sent1: To remedy these problems, recent years have seen interest in the distant supervision approach for relation extraction (#REF; #TARGET_REF .\n sent2: The input to distant supervision is a set of seed facts for the target relation together with an (unlabeled) text corpus, and the output is a set of (noisy) annotations that can be used by any machine learning technique to train a statistical model for the target relation.\n sent3: For example, given the target relation birthPlace(person, place) and a seed fact birthPlace(John, Springfield), the sentence \"John and his wife were born in Springfield in 1946\" (S1) would qualify as a positive training example.\n sent4: Distant supervision replaces the expensive process of manually acquiring annotations that is required by direct supervision with resources that already exist in many scenarios (seed facts and a text corpus).\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We study this question with input data sets that are orders of magnitude larger than those in prior work.",
                "While the largest corpus (Wikipedia and New York Times) employed by recent work on distant supervision #TARGET_REF; #REF) contain about 2M documents, we run experiments on a 100M-document (50X more) corpus drawn from ClueWeb.",
                "1 While prior work (#REF) on crowdsourcing for distant supervision used thousands of human feedback units, we acquire tens of thousands of human-provided labels.",
                "Despite the large scale, we follow state-of-the-art distant supervision approaches and use deep linguistic features, e.g., part-of-speech tags and dependency parsing.",
                "2 Our experiments shed insight on the following two questions:"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: We study this question with input data sets that are orders of magnitude larger than those in prior work.\n sent1: While the largest corpus (Wikipedia and New York Times) employed by recent work on distant supervision #TARGET_REF; #REF) contain about 2M documents, we run experiments on a 100M-document (50X more) corpus drawn from ClueWeb.\n sent2: 1 While prior work (#REF) on crowdsourcing for distant supervision used thousands of human feedback units, we acquire tens of thousands of human-provided labels.\n sent3: Despite the large scale, we follow state-of-the-art distant supervision approaches and use deep linguistic features, e.g., part-of-speech tags and dependency parsing.\n sent4: 2 Our experiments shed insight on the following two questions:\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The idea of using entity-level structured data (e.g., facts in a database) to generate mention-level training data (e.g., in English text) is a classic one: researchers have used variants of this idea to extract entities of a certain type from webpages (#REF; #REF) .",
                "More closely related to relation extraction is the work of #REF that uses dependency paths to find answers that express the same relation as in a question.",
                "Since #TARGET_REF coined the name \"distant supervision,\" there has been growing interest in this technique.",
                "For example, distant supervision has been used for the TAC-KBP slot-filling tasks and other relation-extraction tasks (#REF; #REF; #REFa; #REFb) .",
                "In contrast, we study how increasing input size (and incorporating human feedback) improves the result quality of distant supervision."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The idea of using entity-level structured data (e.g., facts in a database) to generate mention-level training data (e.g., in English text) is a classic one: researchers have used variants of this idea to extract entities of a certain type from webpages (#REF; #REF) .\n sent1: More closely related to relation extraction is the work of #REF that uses dependency paths to find answers that express the same relation as in a question.\n sent2: Since #TARGET_REF coined the name \"distant supervision,\" there has been growing interest in this technique.\n sent3: For example, distant supervision has been used for the TAC-KBP slot-filling tasks and other relation-extraction tasks (#REF; #REF; #REFa; #REFb) .\n sent4: In contrast, we study how increasing input size (and incorporating human feedback) improves the result quality of distant supervision.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "3 taining mentions of named entities, our goal is to learn a classifier for R(x, y) using linguistic features of x and y, e.g., dependency-path information.",
                "The problem is that we lack the large amount of labeled examples that are typically required to apply supervised learning techniques.",
                "We describe an overview of these techniques and the methodological choices we made to implement our study.",
                "Figure 1 illustrates the overall workflow of a distant supervision system.",
                "At each step of the distant supervision process, we closely follow the recent literature #TARGET_REF; ."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: 3 taining mentions of named entities, our goal is to learn a classifier for R(x, y) using linguistic features of x and y, e.g., dependency-path information.\n sent1: The problem is that we lack the large amount of labeled examples that are typically required to apply supervised learning techniques.\n sent2: We describe an overview of these techniques and the methodological choices we made to implement our study.\n sent3: Figure 1 illustrates the overall workflow of a distant supervision system.\n sent4: At each step of the distant supervision process, we closely follow the recent literature #TARGET_REF; .\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "4 We use the facts in R i combined with C to generate examples.",
                "Following recent work #TARGET_REF; #REF) , we use Freebase 5 as the knowledge base for seed facts.",
                "We use two text corpora: (1) the TAC-KBP 6 2010 corpus that 4 We only consider binary predicates in this work.",
                "5 http://freebase.com 6 KBP stands for \"Knowledge-Base Population.\" consists of 1.8M newswire and blog articles 7 , and (2) the ClueWeb09 corpus that is a 2009 snapshot of 500M webpages.",
                "We use the TAC-KBP slot filling task and select those TAC-KBP relations that are present in the Freebase schema as targets (20 relations on people and organization)."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: 4 We use the facts in R i combined with C to generate examples.\n sent1: Following recent work #TARGET_REF; #REF) , we use Freebase 5 as the knowledge base for seed facts.\n sent2: We use two text corpora: (1) the TAC-KBP 6 2010 corpus that 4 We only consider binary predicates in this work.\n sent3: 5 http://freebase.com 6 KBP stands for \"Knowledge-Base Population.\" consists of 1.8M newswire and blog articles 7 , and (2) the ClueWeb09 corpus that is a 2009 snapshot of 500M webpages.\n sent4: We use the TAC-KBP slot filling task and select those TAC-KBP relations that are present in the Freebase schema as targets (20 relations on people and organization).\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "In turn, this implies that these pairs cannot be used directly as training examples for our classifier.",
                "To generate training examples, we need to map the entities back to mentions in the corpus.",
                "We denote the relation that describes this mapping as the relation EL(e, m) where e ∈ E is an entity in the database D and m is a mention in the corpus C. For each relation R i , we generate a set of (noisy) positive examples denoted R",
                "As in previous work, we impose the constraint that both mentions (m 1 , m 2 ) ∈ R + i are contained in the same sentence #TARGET_REF; #REF) .",
                "To generate negative examples for each relation, we follow the assumption in #REF that relations are disjoint and sample from other relations, i.e., R"
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: In turn, this implies that these pairs cannot be used directly as training examples for our classifier.\n sent1: To generate training examples, we need to map the entities back to mentions in the corpus.\n sent2: We denote the relation that describes this mapping as the relation EL(e, m) where e ∈ E is an entity in the database D and m is a mention in the corpus C. For each relation R i , we generate a set of (noisy) positive examples denoted R\n sent3: As in previous work, we impose the constraint that both mentions (m 1 , m 2 ) ∈ R + i are contained in the same sentence #TARGET_REF; #REF) .\n sent4: To generate negative examples for each relation, we follow the assumption in #REF that relations are disjoint and sample from other relations, i.e., R\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "In turn, this implies that these pairs cannot be used directly as training examples for our classifier.",
                "To generate training examples, we need to map the entities back to mentions in the corpus.",
                "We denote the relation that describes this mapping as the relation EL(e, m) where e ∈ E is an entity in the database D and m is a mention in the corpus C. For each relation R i , we generate a set of (noisy) positive examples denoted R",
                "As in previous work, we impose the constraint that both mentions (m 1 , m 2 ) ∈ R + i are contained in the same sentence (#REF; #REF) .",
                "To generate negative examples for each relation, we follow the assumption in #TARGET_REF that relations are disjoint and sample from other relations, i.e., R"
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: In turn, this implies that these pairs cannot be used directly as training examples for our classifier.\n sent1: To generate training examples, we need to map the entities back to mentions in the corpus.\n sent2: We denote the relation that describes this mapping as the relation EL(e, m) where e ∈ E is an entity in the database D and m is a mention in the corpus C. For each relation R i , we generate a set of (noisy) positive examples denoted R\n sent3: As in previous work, we impose the constraint that both mentions (m 1 , m 2 ) ∈ R + i are contained in the same sentence (#REF; #REF) .\n sent4: To generate negative examples for each relation, we follow the assumption in #TARGET_REF that relations are disjoint and sample from other relations, i.e., R\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Once we have constructed the set of possible mention pairs, the state-of-the-art technique to generate feature vectors uses linguistic tools such as partof-speech taggers, named-entity recognizers, dependency parsers, and string features.",
                "Following recent work on distant supervision #TARGET_REF; #REF) , we use both lexical and syntactic features.",
                "After this stage, we have a well-defined machine learning problem that is solvable using standard supervised techniques.",
                "We use sparse logistic regression ( 1 regularized) (#REF) , which is used in previous studies.",
                "Our feature extraction process consists of three steps:"
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: Once we have constructed the set of possible mention pairs, the state-of-the-art technique to generate feature vectors uses linguistic tools such as partof-speech taggers, named-entity recognizers, dependency parsers, and string features.\n sent1: Following recent work on distant supervision #TARGET_REF; #REF) , we use both lexical and syntactic features.\n sent2: After this stage, we have a well-defined machine learning problem that is solvable using standard supervised techniques.\n sent3: We use sparse logistic regression ( 1 regularized) (#REF) , which is used in previous studies.\n sent4: Our feature extraction process consists of three steps:\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Just as direct training data are scarce, ground truth for relation extraction is scarce as well.",
                "As a result, prior work mainly considers two types of evaluation methods: (1) randomly sample a small portion of predictions (e.g., top-k) and manually evaluate precision/recall; and (2) use a held-out portion of seed facts (usually Freebase) as a kind of \"distant\" ground truth.",
                "We replace manual evaluation with a standardized relation-extraction benchmark: TAC-KBP 2010.",
                "TAC-KBP asks for extractions of 46 relations on a given set of 100 entities.",
                "Interestingly, the Freebase held-out metric #TARGET_REF; #REF ) turns out to be heavily biased toward distantly labeled data (e.g., increasing human feedback hurts precision; see Section 4.6)."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Just as direct training data are scarce, ground truth for relation extraction is scarce as well.\n sent1: As a result, prior work mainly considers two types of evaluation methods: (1) randomly sample a small portion of predictions (e.g., top-k) and manually evaluate precision/recall; and (2) use a held-out portion of seed facts (usually Freebase) as a kind of \"distant\" ground truth.\n sent2: We replace manual evaluation with a standardized relation-extraction benchmark: TAC-KBP 2010.\n sent3: TAC-KBP asks for extractions of 46 relations on a given set of 100 entities.\n sent4: Interestingly, the Freebase held-out metric #TARGET_REF; #REF ) turns out to be heavily biased toward distantly labeled data (e.g., increasing human feedback hurts precision; see Section 4.6).\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "In addition to the TAC-KBP benchmark, we also follow prior work #TARGET_REF; #REF) and measure the quality using held-out data from Freebase.",
                "We randomly partition both Freebase and the corpus into two halves.",
                "One database-corpus pair is used for training and the other pair for testing.",
                "We evaluate the precision over the 10 3 highest-probability predictions on the test set.",
                "In Figure 5 , we vary the size of the corpus in the train pair and the number of human labels; the precision reaches a dramatic peak when we the corpus size is above 10 5 and uses little human feedback."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: In addition to the TAC-KBP benchmark, we also follow prior work #TARGET_REF; #REF) and measure the quality using held-out data from Freebase.\n sent1: We randomly partition both Freebase and the corpus into two halves.\n sent2: One database-corpus pair is used for training and the other pair for testing.\n sent3: We evaluate the precision over the 10 3 highest-probability predictions on the test set.\n sent4: In Figure 5 , we vary the size of the corpus in the train pair and the number of human labels; the precision reaches a dramatic peak when we the corpus size is above 10 5 and uses little human feedback.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "From those 4 QA pairs discussing a Microsoft mouse, we know that the Microsoft mouse is compatible with \"Microsoft Surface Pro 3\" and \"Windows 10\" but incompatible with \"iPad\".",
                "Furthermore, we have no idea whether \"Samsung Galaxy Tab 2 10.0\" is compatible or not with this mouse.",
                "Similar to our previous work in product reviews #TARGET_REF , we call the mouse target entity and those 4 products complementary entities of the target entity.",
                "Each complementary entity forms a complementary relation with the target entity.",
                "Each yes/no answer further assigns a compatibility label to each complementary entity."
            ],
            "label": [
                "SIMILARITY",
                "BACKGROUND"
            ]
        },
        "input": "sent0: From those 4 QA pairs discussing a Microsoft mouse, we know that the Microsoft mouse is compatible with \"Microsoft Surface Pro 3\" and \"Windows 10\" but incompatible with \"iPad\".\n sent1: Furthermore, we have no idea whether \"Samsung Galaxy Tab 2 10.0\" is compatible or not with this mouse.\n sent2: Similar to our previous work in product reviews #TARGET_REF , we call the mouse target entity and those 4 products complementary entities of the target entity.\n sent3: Each complementary entity forms a complementary relation with the target entity.\n sent4: Each yes/no answer further assigns a compatibility label to each complementary entity.\n",
        "output": "{\"label\": [\"SIMILARITY\", \"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "This is because customers tend to ask specific questions in PCQA.",
                "We leave the work of mining compatible/incompatible products on open questions to future work.",
                "Given the structure of a QA pair, our method naturally has a twostage framework: Complementary Entity Recognition (CER) (#REF) and yes/no answer classification.",
                "For the first stage, we employ a similar approach as in #TARGET_REF ; for the second stage, it is reduced to a yes/no answer classification problem (#REF) .",
                "We observe that the second stage provides further research opportunity since the polarities of many yes/no answers are implicit."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: This is because customers tend to ask specific questions in PCQA.\n sent1: We leave the work of mining compatible/incompatible products on open questions to future work.\n sent2: Given the structure of a QA pair, our method naturally has a twostage framework: Complementary Entity Recognition (CER) (#REF) and yes/no answer classification.\n sent3: For the first stage, we employ a similar approach as in #TARGET_REF ; for the second stage, it is reduced to a yes/no answer classification problem (#REF) .\n sent4: We observe that the second stage provides further research opportunity since the polarities of many yes/no answers are implicit.\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The problem of Complementary Entity Recognition (CER) is first proposed by Xu et.",
                "al. #TARGET_REF .",
                "However, our previous work focuses on product reviews and consider CER as a special kind of aspect extraction problem (#REF) .",
                "Determining the polarities of compatibility is reduced to a traditional sentiment classification problem.",
                "This paper focuses on yes/no QAs in PCQA and the polarities of compatibility is a yes/no answer classification problem."
            ],
            "label": [
                "BACKGROUND",
                "MOTIVATION"
            ]
        },
        "input": "sent0: The problem of Complementary Entity Recognition (CER) is first proposed by Xu et.\n sent1: al. #TARGET_REF .\n sent2: However, our previous work focuses on product reviews and consider CER as a special kind of aspect extraction problem (#REF) .\n sent3: Determining the polarities of compatibility is reduced to a traditional sentiment classification problem.\n sent4: This paper focuses on yes/no QAs in PCQA and the polarities of compatibility is a yes/no answer classification problem.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"MOTIVATION\"], \"context\": [\"sent0\", \"sent1\", \"sent2\", \"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Complementary entities are also studied as a social network problem in recommender systems (#REF; #REF) .",
                "We discussed the benefit of CER over social network problem in #TARGET_REF ) so we omit here but keep a performance comparison in Section 5.",
                "Community Question and Answering (CQA) has been well studied in literature (#REF; #REF; #REF; #REF) .",
                "More specifically, product Community Question and Answering (PCQA) is studied in (#REF; #REF) .",
                "They both try to find relevance between reviews and questions."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Complementary entities are also studied as a social network problem in recommender systems (#REF; #REF) .\n sent1: We discussed the benefit of CER over social network problem in #TARGET_REF ) so we omit here but keep a performance comparison in Section 5.\n sent2: Community Question and Answering (CQA) has been well studied in literature (#REF; #REF; #REF; #REF) .\n sent3: More specifically, product Community Question and Answering (PCQA) is studied in (#REF; #REF) .\n sent4: They both try to find relevance between reviews and questions.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We leave the work of mining compatible/incompatible products on open questions to future work.",
                "Given the structure of a QA pair, our method naturally has a twostage framework: Complementary Entity Recognition (CER) #TARGET_REF and yes/no answer classification.",
                "For the first stage, we employ a similar approach as in (#REF) ; for the second stage, it is reduced to a yes/no answer classification problem (#REF) .",
                "We observe that the second stage provides further research opportunity since the polarities of many yes/no answers are implicit.",
                "For example, \"Will it work with Surface Pro 3? It works.\" has no explicit \"Yes\" but it is still a yes answer."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We leave the work of mining compatible/incompatible products on open questions to future work.\n sent1: Given the structure of a QA pair, our method naturally has a twostage framework: Complementary Entity Recognition (CER) #TARGET_REF and yes/no answer classification.\n sent2: For the first stage, we employ a similar approach as in (#REF) ; for the second stage, it is reduced to a yes/no answer classification problem (#REF) .\n sent3: We observe that the second stage provides further research opportunity since the polarities of many yes/no answers are implicit.\n sent4: For example, \"Will it work with Surface Pro 3? It works.\" has no explicit \"Yes\" but it is still a yes answer.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "This is because the number of complementary products for a target product can be unlimited so it is impractical to cover all of them.",
                "We also bring out the test dataset used in #TARGET_REF for a comparison (Section 5).",
                "We notice that PCQA addresses compatibility issues in a different perspective compared to product reviews.",
                "PCQA tends to be specific on compatibility issues; reviews are free to talk about their experiences (e.g, opinions on features/aspects).",
                "For example, customers tend to ask more specific questions like \"Will it work with Surface Pro 3\" rather than \"Will it work with my tablet?\" since the latter question is pointless; reviews are typical datasets for opinion mining and aspects extraction (#REF) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This is because the number of complementary products for a target product can be unlimited so it is impractical to cover all of them.\n sent1: We also bring out the test dataset used in #TARGET_REF for a comparison (Section 5).\n sent2: We notice that PCQA addresses compatibility issues in a different perspective compared to product reviews.\n sent3: PCQA tends to be specific on compatibility issues; reviews are free to talk about their experiences (e.g, opinions on features/aspects).\n sent4: For example, customers tend to ask more specific questions like \"Will it work with Surface Pro 3\" rather than \"Will it work with my tablet?\" since the latter question is pointless; reviews are typical datasets for opinion mining and aspects extraction (#REF) .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "In this section, we first introduce the two-stage framework of the proposed method.",
                "Then we briefly introduce the method for CER in #TARGET_REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In this section, we first introduce the two-stage framework of the proposed method.\n sent1: Then we briefly introduce the method for CER in #TARGET_REF .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Since complementary entities are mentioned in yes/no questions and their polarities of compatibility information are in answers, the proposed method naturally has a two-stage framework: Complementary Entity Recognition: we extract complementary entities from questions using dependency paths almost the same as in #TARGET_REF .",
                "It utilizes a large amount of unlabeled reviews under the same category as the target entity to expand knowledge about domain-specific verbs.",
                "Identifying Polarities of Yes/No Answers: then we determine the polarity (yes, no or neutral) of yes/no answers for each question with complementary entity and assign a compatibility label (compatible, incompatible or unknown) to it.",
                "We form this 3-class classification via PU-learning and a binary SVM classifier in Section 4."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Since complementary entities are mentioned in yes/no questions and their polarities of compatibility information are in answers, the proposed method naturally has a two-stage framework: Complementary Entity Recognition: we extract complementary entities from questions using dependency paths almost the same as in #TARGET_REF .\n sent1: It utilizes a large amount of unlabeled reviews under the same category as the target entity to expand knowledge about domain-specific verbs.\n sent2: Identifying Polarities of Yes/No Answers: then we determine the polarity (yes, no or neutral) of yes/no answers for each question with complementary entity and assign a compatibility label (compatible, incompatible or unknown) to it.\n sent3: We form this 3-class classification via PU-learning and a binary SVM classifier in Section 4.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "We briefly introduce the method used in #TARGET_REF and how the dependency paths can be used in questions of PCQA (details of dependency paths can be found in the original paper).",
                "The basic idea is to use dependency paths to identify the context of complementary relations around complementary entities.",
                "Dependency paths can match dependency relations parsed through dependency parsing 1 , which parses a sentence into a set of dependency relations.",
                "In our previous work, we notice that the verbs used to indicate a complementary relation can be unlimited and product specific.",
                "So we utilize another novel set of dependency paths that are in high precision but low recall to expand knowledge about complementary entities on a large amount of unlabeled review."
            ],
            "label": [
                "USE",
                "EXTENDS"
            ]
        },
        "input": "sent0: We briefly introduce the method used in #TARGET_REF and how the dependency paths can be used in questions of PCQA (details of dependency paths can be found in the original paper).\n sent1: The basic idea is to use dependency paths to identify the context of complementary relations around complementary entities.\n sent2: Dependency paths can match dependency relations parsed through dependency parsing 1 , which parses a sentence into a set of dependency relations.\n sent3: In our previous work, we notice that the verbs used to indicate a complementary relation can be unlimited and product specific.\n sent4: So we utilize another novel set of dependency paths that are in high precision but low recall to expand knowledge about complementary entities on a large amount of unlabeled review.\n",
        "output": "{\"label\": [\"USE\", \"EXTENDS\"], \"context\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Then disagreements are discussed and final agreements are reached among all annotators.",
                "To obtain knowledge about domainspecific verbs, we use 6000 reviews for each product similar as in #TARGET_REF .",
                "We also select about 220 reviews for each product and label them in a similar way to show the difference between product QA community and reviews.",
                "The agreement for reviews is 82%.",
                "The statistics of the datasets 2 can be found in Table 1 ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Then disagreements are discussed and final agreements are reached among all annotators.\n sent1: To obtain knowledge about domainspecific verbs, we use 6000 reviews for each product similar as in #TARGET_REF .\n sent2: We also select about 220 reviews for each product and label them in a similar way to show the difference between product QA community and reviews.\n sent3: The agreement for reviews is 82%.\n sent4: The statistics of the datasets 2 can be found in Table 1 .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "CER6K: This method is the method proposed in #TARGET_REF .",
                "Specifically, it uses 6000 reviews to expand domain-specific verbs.",
                "Next, we perform a separate evaluation on yes/no answer classification.",
                "We assume the accuracies of complementary entities extraction are 100% and errors do not affect answer classification.",
                "We only classify answers to questions that have labeled complementary entities."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: CER6K: This method is the method proposed in #TARGET_REF .\n sent1: Specifically, it uses 6000 reviews to expand domain-specific verbs.\n sent2: Next, we perform a separate evaluation on yes/no answer classification.\n sent3: We assume the accuracies of complementary entities extraction are 100% and errors do not affect answer classification.\n sent4: We only classify answers to questions that have labeled complementary entities.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\", \"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "For sentiment analysis on twitter the best performing approaches (#REF; #REF) have used a set of rich lexical features.",
                "However, the development of lexica can be time consuming and is not always suitable when shifting between domains, which examine new topics and user populations (#REF) .",
                "Excitingly, the state of the art has recently shifted toward novel semi-supervised techniques such as the incorporation of word embeddings to represent the context of words and concepts #TARGET_REF .",
                "Moreover, it is important to be able to identify sentiment in relation to particular entities, topics or events (aspect-based sentiment).",
                "We have followed a hybrid approach which incorporates traditional lexica, unigrams and bigrams as well as word embeddings using word2vec (#REF) to train classifiers for subtasks A and B. For subtask C, sentiment targeted towards a particular topic, we have developed a set of different strategies which use either syntactic dependencies or token-level associations with the topic word in combination with our A classifier to produce sentiment annotations."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For sentiment analysis on twitter the best performing approaches (#REF; #REF) have used a set of rich lexical features.\n sent1: However, the development of lexica can be time consuming and is not always suitable when shifting between domains, which examine new topics and user populations (#REF) .\n sent2: Excitingly, the state of the art has recently shifted toward novel semi-supervised techniques such as the incorporation of word embeddings to represent the context of words and concepts #TARGET_REF .\n sent3: Moreover, it is important to be able to identify sentiment in relation to particular entities, topics or events (aspect-based sentiment).\n sent4: We have followed a hybrid approach which incorporates traditional lexica, unigrams and bigrams as well as word embeddings using word2vec (#REF) to train classifiers for subtasks A and B. For subtask C, sentiment targeted towards a particular topic, we have developed a set of different strategies which use either syntactic dependencies or token-level associations with the topic word in combination with our A classifier to produce sentiment annotations.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Here we employ a combination of lexical features and word embeddings to maximise our performance in task A. We build phrase-based classifiers both with an emphasis on the distinction between positive and negative sentiment, which conforms to the distribution of training data in task A, as well as phrasebased classifiers trained on a balanced set of positive, negative and neutral tweets.",
                "We use the latter to identify sentiment in the vicinity of topic words in task C, for targeted sentiment assignment.",
                "In previous work (#REFa; #TARGET_REF sentiment-specific word embeddings have been used as features for identification of tweet-level sentiment but not phrase-level sentiment.",
                "Other work which considered word embeddings for phrase level sentiment (dos #REF) did not focus on producing sentiment-specific representations and the embeddings learnt were a combination of character and word embeddings, where the relative contribution of the word embeddings is not clear.",
                "In this work we present two different strategies for learning phrase level sentiment specific word embeddings."
            ],
            "label": [
                "BACKGROUND",
                "MOTIVATION"
            ]
        },
        "input": "sent0: Here we employ a combination of lexical features and word embeddings to maximise our performance in task A. We build phrase-based classifiers both with an emphasis on the distinction between positive and negative sentiment, which conforms to the distribution of training data in task A, as well as phrasebased classifiers trained on a balanced set of positive, negative and neutral tweets.\n sent1: We use the latter to identify sentiment in the vicinity of topic words in task C, for targeted sentiment assignment.\n sent2: In previous work (#REFa; #TARGET_REF sentiment-specific word embeddings have been used as features for identification of tweet-level sentiment but not phrase-level sentiment.\n sent3: Other work which considered word embeddings for phrase level sentiment (dos #REF) did not focus on producing sentiment-specific representations and the embeddings learnt were a combination of character and word embeddings, where the relative contribution of the word embeddings is not clear.\n sent4: In this work we present two different strategies for learning phrase level sentiment specific word embeddings.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"MOTIVATION\"], \"context\": [\"sent2\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "In both cases, we created representations of length equal to 100 2 .",
                "For each strategy, class and dimension, we used the functions suggested by #TARGET_REF ) (average, maximum and minimum), resulting in 2,400 features.",
                "Extra Features: We used several features, potentially indicative of sentiment, a subset of those in (#REF) .",
                "These include: the total number of words of the target phrase, its position within the tweet (\"start\", \"end\", or \"other\"), the average word length of the target/context and the presence of elongated words, URLs and user mentions.",
                "We manually labelled various emoticons as positive (strong/weak), negative (strong/weak) and \"other\" and counted how many times each label appeared in the target and its context."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In both cases, we created representations of length equal to 100 2 .\n sent1: For each strategy, class and dimension, we used the functions suggested by #TARGET_REF ) (average, maximum and minimum), resulting in 2,400 features.\n sent2: Extra Features: We used several features, potentially indicative of sentiment, a subset of those in (#REF) .\n sent3: These include: the total number of words of the target phrase, its position within the tweet (\"start\", \"end\", or \"other\"), the average word length of the target/context and the presence of elongated words, URLs and user mentions.\n sent4: We manually labelled various emoticons as positive (strong/weak), negative (strong/weak) and \"other\" and counted how many times each label appeared in the target and its context.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We set the word embeddings dimension to 100 in order to gain enough semantic information whilst reducing training time.",
                "We also employed the word embeddings encoding sentiment information generated through the unified models in #TARGET_REF .",
                "Similar to Tang, we represent each tweet by the min, average, max and sum on each dimension of the word embeddings of all the words in the tweet.",
                "In the end, the number of our word embeddings features is 4 ⇥ 100 = 400.",
                "A tweet's representations of word embeddings generated from the HAPPY and non-HAPPY subset of tweets and the embeddings generated by Tang et al. were incorporated into the feature set."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We set the word embeddings dimension to 100 in order to gain enough semantic information whilst reducing training time.\n sent1: We also employed the word embeddings encoding sentiment information generated through the unified models in #TARGET_REF .\n sent2: Similar to Tang, we represent each tweet by the min, average, max and sum on each dimension of the word embeddings of all the words in the tweet.\n sent3: In the end, the number of our word embeddings features is 4 ⇥ 100 = 400.\n sent4: A tweet's representations of word embeddings generated from the HAPPY and non-HAPPY subset of tweets and the embeddings generated by Tang et al. were incorporated into the feature set.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The results we submitted were generated by the second classifier.",
                "Table 2 demonstrates that representing the tweet with positive and negative word embeddings is the most effective feature (performance is affected the most when we remove these) followed by the manually generated lexicon-based features.",
                "This combined with a 2% reduction in F1 score when the embeddings are removed, indicates that the embeddings improve sentiment analysis performance.",
                "Contrary to the approach by #TARGET_REF , we didn't integrate the sentiment information in the word embeddings training process, but rather the sentiment-specific nature of the embeddings was reflected in the choice of different training datasets, yielding different word embedding features for positive and negative tweets.",
                "To measure the contributions of our word embeddings and Tang's sentiment-specific word embeddings separately in the F1 score, we performed a further test."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: The results we submitted were generated by the second classifier.\n sent1: Table 2 demonstrates that representing the tweet with positive and negative word embeddings is the most effective feature (performance is affected the most when we remove these) followed by the manually generated lexicon-based features.\n sent2: This combined with a 2% reduction in F1 score when the embeddings are removed, indicates that the embeddings improve sentiment analysis performance.\n sent3: Contrary to the approach by #TARGET_REF , we didn't integrate the sentiment information in the word embeddings training process, but rather the sentiment-specific nature of the embeddings was reflected in the choice of different training datasets, yielding different word embedding features for positive and negative tweets.\n sent4: To measure the contributions of our word embeddings and Tang's sentiment-specific word embeddings separately in the F1 score, we performed a further test.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "One of the tasks in aspect-based sentiment analysis is to extract aspect and opinion terms from review text.",
                "Our study focuses on evaluating transfer learning using BERT #TARGET_REF to classify tokens from hotel reviews in bahasa Indonesia.",
                "We show that the default BERT model failed to outperform a simple argmax method.",
                "However, changing the default BERT tokenizer to our custom one can improve the F 1 scores on our labels of interest by at least 5%.",
                "For I-ASPECT and B-SENTIMENT, it can even increased the F 1 scores by 11%."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: One of the tasks in aspect-based sentiment analysis is to extract aspect and opinion terms from review text.\n sent1: Our study focuses on evaluating transfer learning using BERT #TARGET_REF to classify tokens from hotel reviews in bahasa Indonesia.\n sent2: We show that the default BERT model failed to outperform a simple argmax method.\n sent3: However, changing the default BERT tokenizer to our custom one can improve the F 1 scores on our labels of interest by at least 5%.\n sent4: For I-ASPECT and B-SENTIMENT, it can even increased the F 1 scores by 11%.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The staff couldn't have been more professional, they really helped get us a taxi when our pre arranged pickup ran late.\" In this review, some of the sentiment terms are \"excellent\", \"typical\", \"clean\", and \"professional\".",
                "In this study, we are focusing on the aspect and opinion term extraction from the reviews to do aspect-based sentiment analysis (#REF) .",
                "While some work has been done in this task #REF; #REF) , we have not seen a transfer learning approach (#REF) employed, which should need much less training effort.",
                "Using transfer learning is especially helpful for low-resource languages (#REF) , such as bahasa Indonesia.",
                "Our main contribution in this study is evaluating BERT #TARGET_REF as a pretrained transformer model on this token classification task on hotel reviews in bahasa Indonesia."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The staff couldn't have been more professional, they really helped get us a taxi when our pre arranged pickup ran late.\" In this review, some of the sentiment terms are \"excellent\", \"typical\", \"clean\", and \"professional\".\n sent1: In this study, we are focusing on the aspect and opinion term extraction from the reviews to do aspect-based sentiment analysis (#REF) .\n sent2: While some work has been done in this task #REF; #REF) , we have not seen a transfer learning approach (#REF) employed, which should need much less training effort.\n sent3: Using transfer learning is especially helpful for low-resource languages (#REF) , such as bahasa Indonesia.\n sent4: Our main contribution in this study is evaluating BERT #TARGET_REF as a pretrained transformer model on this token classification task on hotel reviews in bahasa Indonesia.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "In the argmax method, we classify a token as the most probable  Label   Train  Test   B-ASPECT  7005  1758  I-ASPECT  2292  584  B-SENTIMENT  9646  2384  I-SENTIMENT  4265  1067  OTHER  39897  9706 #REF5 15499 Table 1 : Label distribution label in the training set.",
                "For fastText implementation, we use the skip-gram model and produce 100-dimensional vectors.",
                "We proposed to use transfer learning from pretrained BERT-Base, Multilingual Cased #TARGET_REF for this token classification problem.",
                "We used the implementation in PyTorch by Hugging #REF 3 .",
                "We found out that the multilingual cased tokenizer of BERT does not recognize some common terms in our dataset, such as \"kamar\" (room), \"kendala\" (issue), \"wifi\", \"koneksi\" (connection), \"bagus\" (good), \"bersih\" (clean)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In the argmax method, we classify a token as the most probable  Label   Train  Test   B-ASPECT  7005  1758  I-ASPECT  2292  584  B-SENTIMENT  9646  2384  I-SENTIMENT  4265  1067  OTHER  39897  9706 #REF5 15499 Table 1 : Label distribution label in the training set.\n sent1: For fastText implementation, we use the skip-gram model and produce 100-dimensional vectors.\n sent2: We proposed to use transfer learning from pretrained BERT-Base, Multilingual Cased #TARGET_REF for this token classification problem.\n sent3: We used the implementation in PyTorch by Hugging #REF 3 .\n sent4: We found out that the multilingual cased tokenizer of BERT does not recognize some common terms in our dataset, such as \"kamar\" (room), \"kendala\" (issue), \"wifi\", \"koneksi\" (connection), \"bagus\" (good), \"bersih\" (clean).\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The work by #REF itself is an improvement from what their prior work on the same task (#REF) .",
                "Thus, we only included the work by #REF because they show that we can get the best result by combining the latest work by and #REF .",
                "In their paper, #TARGET_REF show that they can achieve state-of-the-art performance not only on sentence-level, but also on token-level tasks, such as for named entity recognition (NER).",
                "This motivates us to explore BERT in our study.",
                "This way, we do not need to use dependency parsers or any feature engineering."
            ],
            "label": [
                "BACKGROUND",
                "MOTIVATION"
            ]
        },
        "input": "sent0: The work by #REF itself is an improvement from what their prior work on the same task (#REF) .\n sent1: Thus, we only included the work by #REF because they show that we can get the best result by combining the latest work by and #REF .\n sent2: In their paper, #TARGET_REF show that they can achieve state-of-the-art performance not only on sentence-level, but also on token-level tasks, such as for named entity recognition (NER).\n sent3: This motivates us to explore BERT in our study.\n sent4: This way, we do not need to use dependency parsers or any feature engineering.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"MOTIVATION\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Certain applications, however, appear to require even more flexibility than is provided by TL-MCTAG.",
                "Scrambling is one well-known example (#REF) .",
                "In addition, in the semantics domain, the use of a new TAG operation, flexible composition, is used to perform certain semantic operations that seemingly cannot be modeled with TL-MCTAG alone #TARGET_REF and in work in synchronous TAG semantics, constructions such as nested quantifiers require a set-local MCTAG (SL-MCTAG) analysis (#REF) .",
                "In this paper we suggest a shift in focus from constraining locality and complexity through restrictions that all trees in a tree set must adjoin within a single tree or tree set to constraining locality and complexity through restrictions on the derivational distance between trees in the same tree set in a valid derivation.",
                "We examine three formalisms, two of them introduced in this work for the first time, that use derivational distance to constrain locality and demonstrate by construction of parsers their relationship to TL-MCTAG in both expressivity and complexity."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Certain applications, however, appear to require even more flexibility than is provided by TL-MCTAG.\n sent1: Scrambling is one well-known example (#REF) .\n sent2: In addition, in the semantics domain, the use of a new TAG operation, flexible composition, is used to perform certain semantic operations that seemingly cannot be modeled with TL-MCTAG alone #TARGET_REF and in work in synchronous TAG semantics, constructions such as nested quantifiers require a set-local MCTAG (SL-MCTAG) analysis (#REF) .\n sent3: In this paper we suggest a shift in focus from constraining locality and complexity through restrictions that all trees in a tree set must adjoin within a single tree or tree set to constraining locality and complexity through restrictions on the derivational distance between trees in the same tree set in a valid derivation.\n sent4: We examine three formalisms, two of them introduced in this work for the first time, that use derivational distance to constrain locality and demonstrate by construction of parsers their relationship to TL-MCTAG in both expressivity and complexity.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "In addition to lexical locality constraints the definition of MCTAG requires that all trees from a set adjoin simultaneously.",
                "In terms of well-formed derivation trees, this amounts to disallowing derivations in which a tree from a given set is the ancestor of a tree from the same tree set.",
                "For most linguistic applications of TAG, this requirement seems natural and is strictly obeyed.",
                "There are a few applications, including flexible composition and scrambling in free-word order languages that benefit from TAG-based grammars that drop the simultaneity requirement #TARGET_REF; #REF) .",
                "From a complexity perspective, however, checking the simultaneity requirement is expensive (#REF) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In addition to lexical locality constraints the definition of MCTAG requires that all trees from a set adjoin simultaneously.\n sent1: In terms of well-formed derivation trees, this amounts to disallowing derivations in which a tree from a given set is the ancestor of a tree from the same tree set.\n sent2: For most linguistic applications of TAG, this requirement seems natural and is strictly obeyed.\n sent3: There are a few applications, including flexible composition and scrambling in free-word order languages that benefit from TAG-based grammars that drop the simultaneity requirement #TARGET_REF; #REF) .\n sent4: From a complexity perspective, however, checking the simultaneity requirement is expensive (#REF) .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The Filter side condition accordingly expunges trees that are the top tree in the dominance chain of their tree vector.",
                "The side conditions for the Adjoin non-base rule enforce that the dominance constraints are satisfied and that the derivational distance from the base of a tree vector to its currently highest adjoined tree is maintained accurately.",
                "We note that in order to allow a non-total ordering of the trees in a vector we would simply have to record all trees in a tree vector in the histories as is done in the delayed TL-MCTAG parser.",
                "7 Delayed TL-MCTAG #TARGET_REF introduce the delayed TL-MCTAG formalism which makes use of a derivational distance restriction in a somewhat different way.",
                "Rather than restricting the absolute distance between the trees of a set and their nearest common ancestor, given a node α in a derivation tree, delayed TL-MCTAG restricts the number of tree sets that are not fully dominated by α."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The Filter side condition accordingly expunges trees that are the top tree in the dominance chain of their tree vector.\n sent1: The side conditions for the Adjoin non-base rule enforce that the dominance constraints are satisfied and that the derivational distance from the base of a tree vector to its currently highest adjoined tree is maintained accurately.\n sent2: We note that in order to allow a non-total ordering of the trees in a vector we would simply have to record all trees in a tree vector in the histories as is done in the delayed TL-MCTAG parser.\n sent3: 7 Delayed TL-MCTAG #TARGET_REF introduce the delayed TL-MCTAG formalism which makes use of a derivational distance restriction in a somewhat different way.\n sent4: Rather than restricting the absolute distance between the trees of a set and their nearest common ancestor, given a node α in a derivation tree, delayed TL-MCTAG restricts the number of tree sets that are not fully dominated by α.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Section 4 briefly addresses the simultaneity requirement present in MCTAG formalisms but not in Vector- TAG formalisms and argues for dropping the requirement.",
                "In Sections 5 and 6 we introduce two novel formalisms, restricted non-simultaneous MC-TAG and restricted Vector-TAG, respectively, and define CKY-style parsers for them.",
                "In Section 7 we recall the delayed TL-MCTAG formalism introduced by #TARGET_REF and define a CKY-style parser for it as well.",
                "In Section 8 we explore the complexity of all three parsers and the relationship between the formalisms.",
                "In Section 9 we discuss the linguistic applications of these formalisms and show that they permit analyses of some of the hard cases that have led researchers to look beyond TL-MCTAG."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Section 4 briefly addresses the simultaneity requirement present in MCTAG formalisms but not in Vector- TAG formalisms and argues for dropping the requirement.\n sent1: In Sections 5 and 6 we introduce two novel formalisms, restricted non-simultaneous MC-TAG and restricted Vector-TAG, respectively, and define CKY-style parsers for them.\n sent2: In Section 7 we recall the delayed TL-MCTAG formalism introduced by #TARGET_REF and define a CKY-style parser for it as well.\n sent3: In Section 8 we explore the complexity of all three parsers and the relationship between the formalisms.\n sent4: In Section 9 we discuss the linguistic applications of these formalisms and show that they permit analyses of some of the hard cases that have led researchers to look beyond TL-MCTAG.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "7 Delayed TL-MCTAG #REF introduce the delayed TL-MCTAG formalism which makes use of a derivational distance restriction in a somewhat different way.",
                "Rather than restricting the absolute distance between the trees of a set and their nearest common ancestor, given a node α in a derivation tree, delayed TL-MCTAG restricts the number of tree sets that are not fully dominated by α.",
                "Borrowing directly from #TARGET_REF , Figure 7 gives two examples.",
                "Parsing for delayed TL-MCTAG is not discussed by #REF but can be accomplished using a similar CKY-style strategy as in the two parsers above.",
                "We present a parser in Figure 6 ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: 7 Delayed TL-MCTAG #REF introduce the delayed TL-MCTAG formalism which makes use of a derivational distance restriction in a somewhat different way.\n sent1: Rather than restricting the absolute distance between the trees of a set and their nearest common ancestor, given a node α in a derivation tree, delayed TL-MCTAG restricts the number of tree sets that are not fully dominated by α.\n sent2: Borrowing directly from #TARGET_REF , Figure 7 gives two examples.\n sent3: Parsing for delayed TL-MCTAG is not discussed by #REF but can be accomplished using a similar CKY-style strategy as in the two parsers above.\n sent4: We present a parser in Figure 6 .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Borrowing directly from #REF , Figure 7 gives two examples.",
                "Parsing for delayed TL-MCTAG is not discussed by #TARGET_REF but can be accomplished using a similar CKY-style strategy as in the two parsers above.",
                "We present a parser in Figure 6 .",
                "Rather than keeping histories that record derivational distance, we keep an active delay list for each item that records the delays that are active (by recording the identities of the trees that have adjoined) for the tree of which the current node is a part.",
                "At the root of each tree the active delay list is filtered using the Filter side condition to remove all tree sets that are fully dominated and the resulting list is checked using the Size to ensure that it contains no more than d distinct tree sets where d is the specified delay for the grammar."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Borrowing directly from #REF , Figure 7 gives two examples.\n sent1: Parsing for delayed TL-MCTAG is not discussed by #TARGET_REF but can be accomplished using a similar CKY-style strategy as in the two parsers above.\n sent2: We present a parser in Figure 6 .\n sent3: Rather than keeping histories that record derivational distance, we keep an active delay list for each item that records the delays that are active (by recording the identities of the trees that have adjoined) for the tree of which the current node is a part.\n sent4: At the root of each tree the active delay list is filtered using the Filter side condition to remove all tree sets that are fully dominated and the resulting list is checked using the Size to ensure that it contains no more than d distinct tree sets where d is the specified delay for the grammar.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Recent work on neural constituency parsing #TARGET_REF; #REF) has found multiple cases where generative scoring models for which inference is complex outperform base models for which inference is simpler.",
                "Let A be a parser that we want to parse with (here one of the generative models), and let B be a base parser that we use to propose candidate parses which are then scored by the less-tractable parser A. We denote this cross-scoring setup by B → A. The papers above repeatedly saw that the cross-scoring setup B → A under which their generative models were applied outperformed the standard singleparser setup B → B. We term this a cross-scoring gain.",
                "This paper asks two questions.",
                "First, why do recent discriminative-to-generative cross-scoring se- * Equal contribution.",
                "tups B → A outperform their base parsers B?"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Recent work on neural constituency parsing #TARGET_REF; #REF) has found multiple cases where generative scoring models for which inference is complex outperform base models for which inference is simpler.\n sent1: Let A be a parser that we want to parse with (here one of the generative models), and let B be a base parser that we use to propose candidate parses which are then scored by the less-tractable parser A. We denote this cross-scoring setup by B → A. The papers above repeatedly saw that the cross-scoring setup B → A under which their generative models were applied outperformed the standard singleparser setup B → B. We term this a cross-scoring gain.\n sent2: This paper asks two questions.\n sent3: First, why do recent discriminative-to-generative cross-scoring se- * Equal contribution.\n sent4: tups B → A outperform their base parsers B?\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Past work on discriminative neural constituency parsers has shown the effectiveness of beam search with a small beam (#REF) or even greedy search, as in the case of RD #TARGET_REF .",
                "The standard beam search procedure, which we refer to as action-synchronous, maintains a beam of K partially-completed parses that all have the same number of actions taken.",
                "At each stage, a pool of successors is constructed by extending each candidate in the beam with each of its possible next actions.",
                "The K highest-probability successors are chosen as the next beam.",
                "Unfortunately, we find that action-synchronous beam search breaks down for both generative models we explore in this work, failing to find parses that are high scoring under the model."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Past work on discriminative neural constituency parsers has shown the effectiveness of beam search with a small beam (#REF) or even greedy search, as in the case of RD #TARGET_REF .\n sent1: The standard beam search procedure, which we refer to as action-synchronous, maintains a beam of K partially-completed parses that all have the same number of actions taken.\n sent2: At each stage, a pool of successors is constructed by extending each candidate in the beam with each of its possible next actions.\n sent3: The K highest-probability successors are chosen as the next beam.\n sent4: Unfortunately, we find that action-synchronous beam search breaks down for both generative models we explore in this work, failing to find parses that are high scoring under the model.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Of course, many real hybrids will exhibit both reranking and model combination gains.",
                "In this paper, we present experiments to isolate the degree to which each gain occurs for each of two state-of-the-art generative neural parsing models: the Recurrent Neural Network Grammar generative parser (RG) of #TARGET_REF , and the LSTM language modeling generative parser (LM) of #REF .",
                "In particular, we present and use a beam-based search procedure with an augmented state space that can search directly in the generative models, allowing us to explore A → A for these generative parsers A independent of any base parsers.",
                "Our findings suggest the presence of model combination effects in both generative parsers: when parses found by searching directly in the generative parser are added to a list of candidates from a strong base parser (the RNNG discriminative parser, RD (#REF) ), performance decreases when compared to using just candidates from the base parser, i.e., B ∪ A → A has lower evaluation performance than B → A (Section 3.1).",
                "This result suggests that both generative models benefit from fortuitous search errors in the rescoring setting -there are trees with higher probability under the generative model than any tree proposed by the base parser, but which would decrease evaluation performance if selected."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Of course, many real hybrids will exhibit both reranking and model combination gains.\n sent1: In this paper, we present experiments to isolate the degree to which each gain occurs for each of two state-of-the-art generative neural parsing models: the Recurrent Neural Network Grammar generative parser (RG) of #TARGET_REF , and the LSTM language modeling generative parser (LM) of #REF .\n sent2: In particular, we present and use a beam-based search procedure with an augmented state space that can search directly in the generative models, allowing us to explore A → A for these generative parsers A independent of any base parsers.\n sent3: Our findings suggest the presence of model combination effects in both generative parsers: when parses found by searching directly in the generative parser are added to a list of candidates from a strong base parser (the RNNG discriminative parser, RD (#REF) ), performance decreases when compared to using just candidates from the base parser, i.e., B ∪ A → A has lower evaluation performance than B → A (Section 3.1).\n sent4: This result suggests that both generative models benefit from fortuitous search errors in the rescoring setting -there are trees with higher probability under the generative model than any tree proposed by the base parser, but which would decrease evaluation performance if selected.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Of course, many real hybrids will exhibit both reranking and model combination gains.",
                "In this paper, we present experiments to isolate the degree to which each gain occurs for each of two state-of-the-art generative neural parsing models: the Recurrent Neural Network Grammar generative parser (RG) of #REF , and the LSTM language modeling generative parser (LM) of #REF .",
                "In particular, we present and use a beam-based search procedure with an augmented state space that can search directly in the generative models, allowing us to explore A → A for these generative parsers A independent of any base parsers.",
                "Our findings suggest the presence of model combination effects in both generative parsers: when parses found by searching directly in the generative parser are added to a list of candidates from a strong base parser (the RNNG discriminative parser, RD #TARGET_REF ), performance decreases when compared to using just candidates from the base parser, i.e., B ∪ A → A has lower evaluation performance than B → A (Section 3.1).",
                "This result suggests that both generative models benefit from fortuitous search errors in the rescoring setting -there are trees with higher probability under the generative model than any tree proposed by the base parser, but which would decrease evaluation performance if selected."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Of course, many real hybrids will exhibit both reranking and model combination gains.\n sent1: In this paper, we present experiments to isolate the degree to which each gain occurs for each of two state-of-the-art generative neural parsing models: the Recurrent Neural Network Grammar generative parser (RG) of #REF , and the LSTM language modeling generative parser (LM) of #REF .\n sent2: In particular, we present and use a beam-based search procedure with an augmented state space that can search directly in the generative models, allowing us to explore A → A for these generative parsers A independent of any base parsers.\n sent3: Our findings suggest the presence of model combination effects in both generative parsers: when parses found by searching directly in the generative parser are added to a list of candidates from a strong base parser (the RNNG discriminative parser, RD #TARGET_REF ), performance decreases when compared to using just candidates from the base parser, i.e., B ∪ A → A has lower evaluation performance than B → A (Section 3.1).\n sent4: This result suggests that both generative models benefit from fortuitous search errors in the rescoring setting -there are trees with higher probability under the generative model than any tree proposed by the base parser, but which would decrease evaluation performance if selected.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "All of the parsers we investigate in this work (the discriminative parser RD, and the two generative parsers RG and LM, see Section 1) produce parse trees in a depth-first, left-to-right traversal, using the same basic actions: NT(X), which opens a new constituent with the non-terminal symbol X; SHIFT / GEN(w), which adds a word; and RE-DUCE, which closes the current constituent.",
                "We refer to #TARGET_REF for a complete description of these actions, and the constraints on them necessary to ensure valid parse trees.",
                "1 The primary difference between the actions in the discriminative and generative models is that, whereas the discriminative model uses a SHIFT action which is fixed to produce the next word in the sentence, the generative models use GEN(w) to define a distribution over all possible words w in the lexicon.",
                "This stems from the generative model's definition of a joint probability p(x, y) over all possible sentences x and parses y. To use a generative model as a parser, we are interested in finding the maximum probability parse for a given sentence.",
                "This is made more complicated by not having an explicit representation for p(y|x), as we do in the discriminative setting."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: All of the parsers we investigate in this work (the discriminative parser RD, and the two generative parsers RG and LM, see Section 1) produce parse trees in a depth-first, left-to-right traversal, using the same basic actions: NT(X), which opens a new constituent with the non-terminal symbol X; SHIFT / GEN(w), which adds a word; and RE-DUCE, which closes the current constituent.\n sent1: We refer to #TARGET_REF for a complete description of these actions, and the constraints on them necessary to ensure valid parse trees.\n sent2: 1 The primary difference between the actions in the discriminative and generative models is that, whereas the discriminative model uses a SHIFT action which is fixed to produce the next word in the sentence, the generative models use GEN(w) to define a distribution over all possible words w in the lexicon.\n sent3: This stems from the generative model's definition of a joint probability p(x, y) over all possible sentences x and parses y. To use a generative model as a parser, we are interested in finding the maximum probability parse for a given sentence.\n sent4: This is made more complicated by not having an explicit representation for p(y|x), as we do in the discriminative setting.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Using the above decoding procedures, we attempt to separate reranking effects from model combination effects through a set of reranking experiments.",
                "Our base experiments are performed on the Penn Treebank (#REF) , using sections 2-21 for training, section 22 for development, and section 23 for testing.",
                "For the LSTM generative model (LM), we use the pre-trained model released by #REF .",
                "We train RNNG discriminative (RD) and generative (RG) models, following #TARGET_REF by using the same hyperparameter settings, and using pretrained word embeddings from #REF for the discriminative model.",
                "The automaticallypredicted part-of-speech tags we use as input for RD are the same as those used by #REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Using the above decoding procedures, we attempt to separate reranking effects from model combination effects through a set of reranking experiments.\n sent1: Our base experiments are performed on the Penn Treebank (#REF) , using sections 2-21 for training, section 22 for development, and section 23 for testing.\n sent2: For the LSTM generative model (LM), we use the pre-trained model released by #REF .\n sent3: We train RNNG discriminative (RD) and generative (RG) models, following #TARGET_REF by using the same hyperparameter settings, and using pretrained word embeddings from #REF for the discriminative model.\n sent4: The automaticallypredicted part-of-speech tags we use as input for RD are the same as those used by #REF .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The e i,t and α i,t are calculated in each time step t of decoding.",
                "The t-th hidden state s t in dynamic attention-based decoder can be calculated as follows:",
                "The main difference between our proposed conversational response generation model and the above two state-of-the-art models is the two attention mechanisms for obtaining the contextual representation of a conversation.",
                "Rather than use a hierarchical attention neural network #TARGET_REF to obtain the contextual representation of a conversation, we propose two utterance-level attentions for weighting the importance of each utterance in the context, which is more simple in structure and has less number of parameters than the hierarchical attention approach.",
                "Meanwhile, rather than use a heuristic approach to weigh the importance of each utterance in the context (#REF) , in our proposed approach, the weights of utterance in the context are learned by two attention mechanisms from the data, which is more reasonable and flexible than the heuristic based approach."
            ],
            "label": [
                "DIFFERENCES",
                "EXTENDS"
            ]
        },
        "input": "sent0: The e i,t and α i,t are calculated in each time step t of decoding.\n sent1: The t-th hidden state s t in dynamic attention-based decoder can be calculated as follows:\n sent2: The main difference between our proposed conversational response generation model and the above two state-of-the-art models is the two attention mechanisms for obtaining the contextual representation of a conversation.\n sent3: Rather than use a hierarchical attention neural network #TARGET_REF to obtain the contextual representation of a conversation, we propose two utterance-level attentions for weighting the importance of each utterance in the context, which is more simple in structure and has less number of parameters than the hierarchical attention approach.\n sent4: Meanwhile, rather than use a heuristic approach to weigh the importance of each utterance in the context (#REF) , in our proposed approach, the weights of utterance in the context are learned by two attention mechanisms from the data, which is more reasonable and flexible than the heuristic based approach.\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"EXTENDS\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Topic segmentation likewise has multiple educational applications, such as question answering, detecting student initiative, and assessing student answers.",
                "There have been essentially two approaches to topic segmentation in the past.",
                "The first of these, lexical cohesion, may be used for either linear segmentation (#REF; #TARGET_REF or hierarchical segmentation (#REF; #REF) .",
                "The essential idea behind the lexical cohesion approaches is that different topics will have different vocabularies.",
                "Therefore the lexical cohesion within topics will be higher than the lexical cohesion between topics, and gaps in cohesion may mark topic boundaries."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Topic segmentation likewise has multiple educational applications, such as question answering, detecting student initiative, and assessing student answers.\n sent1: There have been essentially two approaches to topic segmentation in the past.\n sent2: The first of these, lexical cohesion, may be used for either linear segmentation (#REF; #TARGET_REF or hierarchical segmentation (#REF; #REF) .\n sent3: The essential idea behind the lexical cohesion approaches is that different topics will have different vocabularies.\n sent4: Therefore the lexical cohesion within topics will be higher than the lexical cohesion between topics, and gaps in cohesion may mark topic boundaries.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Though the idea of using lexical cohesion to segment text has the advantages of simplicity and intuitive appeal, it lacks a unique implementation.",
                "An implementation must define how to represent units of text, compare the cohesion between units, and determine whether the results of comparison indicate a new text segment.",
                "Both Hearst (1994 #TARGET_REF and #REF use vector space methods discussed below to represent and compare units of text.",
                "The comparisons can be characterized by a moving window, where successive overlapping comparisons are advanced by one unit of text.",
                "However, Hearst (1994 Hearst ( , 1997 and #REF differ on how text units are defined and on how to interpret the results of a comparison."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Though the idea of using lexical cohesion to segment text has the advantages of simplicity and intuitive appeal, it lacks a unique implementation.\n sent1: An implementation must define how to represent units of text, compare the cohesion between units, and determine whether the results of comparison indicate a new text segment.\n sent2: Both Hearst (1994 #TARGET_REF and #REF use vector space methods discussed below to represent and compare units of text.\n sent3: The comparisons can be characterized by a moving window, where successive overlapping comparisons are advanced by one unit of text.\n sent4: However, Hearst (1994 Hearst ( , 1997 and #REF differ on how text units are defined and on how to interpret the results of a comparison.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "However, Hearst (1994 #TARGET_REF and #REF differ on how text units are defined and on how to interpret the results of a comparison.",
                "The text unit's definition in Hearst (1994 Hearst ( , 1997 and #REF is generally task dependent, depending on what size gives the best results.",
                "For example, when measuring comprehension, use the unit of the sentence, as opposed to the more standard unit of the proposition, because LSA is most correlated with comprehension at that level.",
                "However, when using LSA to segment text, #REF use the paragraph as the unit, to \"smooth out\" the local changes in cohesion and become more sensitive to more global changes of cohesion.",
                "Hearst likewise chooses a large unit, 6 token-sequences of 20 tokens (#REF) , but varies these parameters dependent on the characteristics of the text to be segmented, e.g. paragraph size."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: However, Hearst (1994 #TARGET_REF and #REF differ on how text units are defined and on how to interpret the results of a comparison.\n sent1: The text unit's definition in Hearst (1994 Hearst ( , 1997 and #REF is generally task dependent, depending on what size gives the best results.\n sent2: For example, when measuring comprehension, use the unit of the sentence, as opposed to the more standard unit of the proposition, because LSA is most correlated with comprehension at that level.\n sent3: However, when using LSA to segment text, #REF use the paragraph as the unit, to \"smooth out\" the local changes in cohesion and become more sensitive to more global changes of cohesion.\n sent4: Hearst likewise chooses a large unit, 6 token-sequences of 20 tokens (#REF) , but varies these parameters dependent on the characteristics of the text to be segmented, e.g. paragraph size.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "The comparisons can be characterized by a moving window, where successive overlapping comparisons are advanced by one unit of text.",
                "However, Hearst (1994 Hearst ( , 1997 and #REF differ on how text units are defined and on how to interpret the results of a comparison.",
                "The text unit's definition in Hearst (1994 #TARGET_REF and #REF is generally task dependent, depending on what size gives the best results.",
                "For example, when measuring comprehension, use the unit of the sentence, as opposed to the more standard unit of the proposition, because LSA is most correlated with comprehension at that level.",
                "However, when using LSA to segment text, #REF use the paragraph as the unit, to \"smooth out\" the local changes in cohesion and become more sensitive to more global changes of cohesion."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The comparisons can be characterized by a moving window, where successive overlapping comparisons are advanced by one unit of text.\n sent1: However, Hearst (1994 Hearst ( , 1997 and #REF differ on how text units are defined and on how to interpret the results of a comparison.\n sent2: The text unit's definition in Hearst (1994 #TARGET_REF and #REF is generally task dependent, depending on what size gives the best results.\n sent3: For example, when measuring comprehension, use the unit of the sentence, as opposed to the more standard unit of the proposition, because LSA is most correlated with comprehension at that level.\n sent4: However, when using LSA to segment text, #REF use the paragraph as the unit, to \"smooth out\" the local changes in cohesion and become more sensitive to more global changes of cohesion.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "In order to use these comparisons to segment text, however, one must have a criterion in place.",
                "#REF , noting mean cosines of .16 for boundaries and .43 for non-boundaries, choose a threshold criterion of .15, which is two standard deviations below the boundary mean of .43.",
                "Using LSA and this criterion, #REF detected chapter boundaries with an F-measure of .33 (see Manning and Schütze (1999) for a definition of Fmeasure).",
                "Hearst (1994 #TARGET_REF in contrast uses a relative comparison of cohesion, by recasting vector comparisons as depth scores.",
                "A depth score is computed as the difference between a given vector comparison and its surrounding peaks, i.e. the local maxima of vector comparisons on either side of the given vector comparison."
            ],
            "label": [
                "BACKGROUND",
                "DIFFERENCES"
            ]
        },
        "input": "sent0: In order to use these comparisons to segment text, however, one must have a criterion in place.\n sent1: #REF , noting mean cosines of .16 for boundaries and .43 for non-boundaries, choose a threshold criterion of .15, which is two standard deviations below the boundary mean of .43.\n sent2: Using LSA and this criterion, #REF detected chapter boundaries with an F-measure of .33 (see Manning and Schütze (1999) for a definition of Fmeasure).\n sent3: Hearst (1994 #TARGET_REF in contrast uses a relative comparison of cohesion, by recasting vector comparisons as depth scores.\n sent4: A depth score is computed as the difference between a given vector comparison and its surrounding peaks, i.e. the local maxima of vector comparisons on either side of the given vector comparison.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"DIFFERENCES\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The greater the difference between a given comparison and its surrounding peaks, the higher the depth score.",
                "Once all the depth scores are calculated for a text, those that are higher than one standard deviation below the mean are taken as topic boundaries.",
                "Using a vector space method without singular value decomposition, #TARGET_REF reports an F-measure of .70 when detecting topic shifts between paragraphs.",
                "Thus previous work suggests that the #REF method is superior to that of #REF , having roughly twice the accuracy indicated by F-measure.",
                "Although these two results used different data sets and are therefore not directly comparable, one would predict based on this limited evidence that the Hearst algorithm would outperform the Foltz algorithm on other topic segmentation tasks."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The greater the difference between a given comparison and its surrounding peaks, the higher the depth score.\n sent1: Once all the depth scores are calculated for a text, those that are higher than one standard deviation below the mean are taken as topic boundaries.\n sent2: Using a vector space method without singular value decomposition, #TARGET_REF reports an F-measure of .70 when detecting topic shifts between paragraphs.\n sent3: Thus previous work suggests that the #REF method is superior to that of #REF , having roughly twice the accuracy indicated by F-measure.\n sent4: Although these two results used different data sets and are therefore not directly comparable, one would predict based on this limited evidence that the Hearst algorithm would outperform the Foltz algorithm on other topic segmentation tasks.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The greater the difference between a given comparison and its surrounding peaks, the higher the depth score.",
                "Once all the depth scores are calculated for a text, those that are higher than one standard deviation below the mean are taken as topic boundaries.",
                "Using a vector space method without singular value decomposition, #REF reports an F-measure of .70 when detecting topic shifts between paragraphs.",
                "Thus previous work suggests that #TARGET_REF method is superior to that of #REF , having roughly twice the accuracy indicated by F-measure.",
                "Although these two results used different data sets and are therefore not directly comparable, one would predict based on this limited evidence that the Hearst algorithm would outperform the Foltz algorithm on other topic segmentation tasks."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The greater the difference between a given comparison and its surrounding peaks, the higher the depth score.\n sent1: Once all the depth scores are calculated for a text, those that are higher than one standard deviation below the mean are taken as topic boundaries.\n sent2: Using a vector space method without singular value decomposition, #REF reports an F-measure of .70 when detecting topic shifts between paragraphs.\n sent3: Thus previous work suggests that #TARGET_REF method is superior to that of #REF , having roughly twice the accuracy indicated by F-measure.\n sent4: Although these two results used different data sets and are therefore not directly comparable, one would predict based on this limited evidence that the Hearst algorithm would outperform the Foltz algorithm on other topic segmentation tasks.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Beyond relative performance rankings, more support for the above reasoning can be found in the difference between Hearst and Hearst + LSA.",
                "Recall that in monologue, #TARGET_REF reports a much larger F-measure than #REF , .70 vs. .33, albeit on different data sets.",
                "In the present dialogue corpus, these roles are reversed, .14 vs. .52.",
                "Possible reasons for this reversal are the segmentation criterion, the vector space method, or the fact that Foltz has been trained on similar data via regression and Hearst has not.",
                "However, comparing the Hearst algorithm with the Hearst + LSA algorithm indicates that a 57% improvement stems from the addition of LSA, keeping all other factors constant."
            ],
            "label": [
                "BACKGROUND",
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Beyond relative performance rankings, more support for the above reasoning can be found in the difference between Hearst and Hearst + LSA.\n sent1: Recall that in monologue, #TARGET_REF reports a much larger F-measure than #REF , .70 vs. .33, albeit on different data sets.\n sent2: In the present dialogue corpus, these roles are reversed, .14 vs. .52.\n sent3: Possible reasons for this reversal are the segmentation criterion, the vector space method, or the fact that Foltz has been trained on similar data via regression and Hearst has not.\n sent4: However, comparing the Hearst algorithm with the Hearst + LSA algorithm indicates that a 57% improvement stems from the addition of LSA, keeping all other factors constant.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"DIFFERENCES\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "While this result is not statistically significant, the direction of the result supports the use of an \"inferencing\" vector space method for segmenting dialogue.",
                "Unfortunately, the large difference in F-measure between the Foltz algorithm and the Hearst + LSA algorithm is more difficult to explain.",
                "These two methods differ by their segmentation criterion and by their training (Foltz is a regression model and Hearst is not).",
                "It may be that Hearst (1994 #TARGET_REF )'s segmentation criterion, i.e. depth scores, do not translate well to dialogue.",
                "Perhaps the assignment of segment boundaries based on the relative difference between a candidate score and its surrounding peaks is highly sensitive to cohesion gaps created by conversational implicatures."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: While this result is not statistically significant, the direction of the result supports the use of an \"inferencing\" vector space method for segmenting dialogue.\n sent1: Unfortunately, the large difference in F-measure between the Foltz algorithm and the Hearst + LSA algorithm is more difficult to explain.\n sent2: These two methods differ by their segmentation criterion and by their training (Foltz is a regression model and Hearst is not).\n sent3: It may be that Hearst (1994 #TARGET_REF )'s segmentation criterion, i.e. depth scores, do not translate well to dialogue.\n sent4: Perhaps the assignment of segment boundaries based on the relative difference between a candidate score and its surrounding peaks is highly sensitive to cohesion gaps created by conversational implicatures.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Using the development corpus's average topic length of 16 utterances as a reference point, F-measures were calculated for the combinations of window size and text unit size in Table 1 .",
                "The optimal combination of parameters (Fmeasure = .17) is a unit size of 16 words and a window size of 16 units.",
                "This combination matches #REF 's heuristic of choosing the window size to be the average paragraph length.",
                "On the test set, this combination of parameters yielded an F-measure of .14 as opposed to the Fmeasure for monologue reported by #TARGET_REF , .70.",
                "For dialogue, the algorithm is 20% as effective as it is for monologue."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Using the development corpus's average topic length of 16 utterances as a reference point, F-measures were calculated for the combinations of window size and text unit size in Table 1 .\n sent1: The optimal combination of parameters (Fmeasure = .17) is a unit size of 16 words and a window size of 16 units.\n sent2: This combination matches #REF 's heuristic of choosing the window size to be the average paragraph length.\n sent3: On the test set, this combination of parameters yielded an F-measure of .14 as opposed to the Fmeasure for monologue reported by #TARGET_REF , .70.\n sent4: For dialogue, the algorithm is 20% as effective as it is for monologue.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent0\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Along with this, many authors have worked on producing datasets for eliminating bias, strengthening the performance of the model by robust question-answer pairs which try to cover the various types of questions, testing the visual and language understanding of the system.",
                "In this survey, first we cover major datasets published for validating the Visual Question Answering task, such as VQA dataset [1] , DAQUAR [8] , Visual7W [9] and most recent datasets up to 2019 include Tally-QA [10] and KVQA [11] .",
                "Next, we discuss the stateof-the-art architectures designed for the task of Visual Question Answering such as Vanilla VQA [1] , Stacked Attention #REF and Pythia v1.0 [13] .",
                "Next we present some of our computed results over the three architectures: vanilla VQA model [1] , Stacked Attention Network (SAN) [12] and Teney et al. model #TARGET_REF .",
                "Finally, we discuss the observations and future directions."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Along with this, many authors have worked on producing datasets for eliminating bias, strengthening the performance of the model by robust question-answer pairs which try to cover the various types of questions, testing the visual and language understanding of the system.\n sent1: In this survey, first we cover major datasets published for validating the Visual Question Answering task, such as VQA dataset [1] , DAQUAR [8] , Visual7W [9] and most recent datasets up to 2019 include Tally-QA [10] and KVQA [11] .\n sent2: Next, we discuss the stateof-the-art architectures designed for the task of Visual Question Answering such as Vanilla VQA [1] , Stacked Attention #REF and Pythia v1.0 [13] .\n sent3: Next we present some of our computed results over the three architectures: vanilla VQA model [1] , Stacked Attention Network (SAN) [12] and Teney et al. model #TARGET_REF .\n sent4: Finally, we discuss the observations and future directions.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The Differentail Network is the very recent method proposed for VQA task and shows very promising performance over different datasets.",
                "As part of this survey, we also implemented different methods over different datasets and performed the experiments.",
                "We considered the following three models for our experiments, 1) the baseline Vanilla VQA model [1] which uses the VGG16 CNN architecture [3] and LST#REF , 2) the Stacked Attention #REF architecture, and 3) the 2017 VQA challenge winner Teney et al. model #TARGET_REF .",
                "We considered the widely adapted datasets such as standard VQA dataset [1] and Visual7W dataset [9] for the experiments.",
                "We used the Adam Optimizer for all models with Cross-Entropy loss function."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The Differentail Network is the very recent method proposed for VQA task and shows very promising performance over different datasets.\n sent1: As part of this survey, we also implemented different methods over different datasets and performed the experiments.\n sent2: We considered the following three models for our experiments, 1) the baseline Vanilla VQA model [1] which uses the VGG16 CNN architecture [3] and LST#REF , 2) the Stacked Attention #REF architecture, and 3) the 2017 VQA challenge winner Teney et al. model #TARGET_REF .\n sent3: We considered the widely adapted datasets such as standard VQA dataset [1] and Visual7W dataset [9] for the experiments.\n sent4: We used the Adam Optimizer for all models with Cross-Entropy loss function.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Each model is trained for 100 epochs for each dataset.",
                "The experimental results are presented in Table III in terms of the accuracy for three models over two datasets.",
                "In the experiments, we found that the Teney et al. #TARGET_REF is the best performing model on both VQA and Visual7W Dataset.",
                "The accuracies obtained over the Teney et al. model are 67.23% and 65.82% over VQA and Visual7W datasets for the openended question-answering task, respectively.",
                "The above results re-affirmed that the Teney et al. model is the best performing model till 2018 which has been pushed by Pythia v1.0 [13] , recently, where they have utilized the same model with more layers to boost the performance."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Each model is trained for 100 epochs for each dataset.\n sent1: The experimental results are presented in Table III in terms of the accuracy for three models over two datasets.\n sent2: In the experiments, we found that the Teney et al. #TARGET_REF is the best performing model on both VQA and Visual7W Dataset.\n sent3: The accuracies obtained over the Teney et al. model are 67.23% and 65.82% over VQA and Visual7W datasets for the openended question-answering task, respectively.\n sent4: The above results re-affirmed that the Teney et al. model is the best performing model till 2018 which has been pushed by Pythia v1.0 [13] , recently, where they have utilized the same model with more layers to boost the performance.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "This model is better suited for the VQA in videos which has more use cases than images.",
                "[20] VQA [1] , TDIUC [29] , COCO-QA [21] Faster-RCNN [22] , Differential #REF , GRU [31] 68.59 (VQA-v2), 86.73 (TDIUC), 69.36 (COCO-QA) AAAI 2019 Pythia v1.0 [28] : Pythia v1.0 is the award winning architecture for VQA #REF",
                "1 . The architecture is similar to Teney et al. #TARGET_REF with reduced computations with elementwise multiplication, use of GloVe vectors [23] , and ensemble of 30 models.",
                "Differential #REF : This model uses the differences between forward propagation steps to reduce the noise and to learn the interdependency between features.",
                "Image features are extracted using Faster-RCNN [22] ."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: This model is better suited for the VQA in videos which has more use cases than images.\n sent1: [20] VQA [1] , TDIUC [29] , COCO-QA [21] Faster-RCNN [22] , Differential #REF , GRU [31] 68.59 (VQA-v2), 86.73 (TDIUC), 69.36 (COCO-QA) AAAI 2019 Pythia v1.0 [28] : Pythia v1.0 is the award winning architecture for VQA #REF\n sent2: 1 . The architecture is similar to Teney et al. #TARGET_REF with reduced computations with elementwise multiplication, use of GloVe vectors [23] , and ensemble of 30 models.\n sent3: Differential #REF : This model uses the differences between forward propagation steps to reduce the noise and to learn the interdependency between features.\n sent4: Image features are extracted using Faster-RCNN [22] .\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Decipherment of homophonic substitution ciphers using language models (LMs) is a wellstudied task in NLP.",
                "Previous work in this topic scores short local spans of possible plaintext decipherments using n-gram LMs.",
                "The most widely used technique is the use of beam search with n-gram LMs proposed by #TARGET_REF .",
                "We propose a beam search algorithm that scores the entire candidate plaintext at each step of the decipherment using a neural LM.",
                "We augment beam search with a novel rest cost estimation that exploits the prediction power of a neural LM."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Decipherment of homophonic substitution ciphers using language models (LMs) is a wellstudied task in NLP.\n sent1: Previous work in this topic scores short local spans of possible plaintext decipherments using n-gram LMs.\n sent2: The most widely used technique is the use of beam search with n-gram LMs proposed by #TARGET_REF .\n sent3: We propose a beam search algorithm that scores the entire candidate plaintext at each step of the decipherment using a neural LM.\n sent4: We augment beam search with a novel rest cost estimation that exploits the prediction power of a neural LM.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Breaking substitution ciphers recovers the plaintext from a ciphertext that uses a 1:1 or homophonic cipher key.",
                "Previous work using pretrained language models (LMs) for decipherment use n-gram LMs (#REF; #TARGET_REF .",
                "Some methods use the ExpectationMaximization (EM) algorithm (#REF) while most state-of-the-art approaches for decipherment of 1:1 and homophonic substitution ciphers use beam search and rely on the clever use of n-gram LMs (#REF; #REF) .",
                "Neural LMs globally score the entire candidate plaintext sequence (#REF) .",
                "However, using a neural LM for decipherment is not trivial because scoring the entire candidate partially deciphered plaintext is computationally challenging."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Breaking substitution ciphers recovers the plaintext from a ciphertext that uses a 1:1 or homophonic cipher key.\n sent1: Previous work using pretrained language models (LMs) for decipherment use n-gram LMs (#REF; #TARGET_REF .\n sent2: Some methods use the ExpectationMaximization (EM) algorithm (#REF) while most state-of-the-art approaches for decipherment of 1:1 and homophonic substitution ciphers use beam search and rely on the clever use of n-gram LMs (#REF; #REF) .\n sent3: Neural LMs globally score the entire candidate plaintext sequence (#REF) .\n sent4: However, using a neural LM for decipherment is not trivial because scoring the entire candidate partially deciphered plaintext is computationally challenging.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The number of f s that are fixed in φ is given by its cardinality.",
                "φ is called an extension of φ, if f is fixed in φ such that δ(φ (f ), φ(f )) yields true ∀f ∈ V f which are already fixed in φ where δ is Kronecker delta.",
                "Decipherment is then the task of finding the φ for which the probability of the deciphered text is maximized.",
                "where p(.) is the language model (LM).",
                "Finding this argmax is solved using a beam search algorithm #TARGET_REF which incrementally finds the most likely substitutions using the language model scores as the ranking."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The number of f s that are fixed in φ is given by its cardinality.\n sent1: φ is called an extension of φ, if f is fixed in φ such that δ(φ (f ), φ(f )) yields true ∀f ∈ V f which are already fixed in φ where δ is Kronecker delta.\n sent2: Decipherment is then the task of finding the φ for which the probability of the deciphered text is maximized.\n sent3: where p(.) is the language model (LM).\n sent4: Finding this argmax is solved using a beam search algorithm #TARGET_REF which incrementally finds the most likely substitutions using the language model scores as the ranking.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Algorithm 1 is the beam search algorithm #TARGET_REF (#REF Hs.",
                "ADD((∅,0)) 5:",
                "while",
                "for all φ ∈ Hs do 8:",
                "for all e ∈ Ve do 9:"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Algorithm 1 is the beam search algorithm #TARGET_REF (#REF Hs.\n sent1: ADD((∅,0)) 5:\n sent2: while\n sent3: for all φ ∈ Hs do 8:\n sent4: for all e ∈ Ve do 9:\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Zodiac-408, a homophonic cipher, is commonly used to evaluate decipherment algorithms.",
                "Our neural LM model with global rest cost estimation and frequency matching heuristic with a beam size of 1M has SER of 1.2% compared to the beam search algorithm #TARGET_REF with beam size of 10M with a 6-gram LM which gives an SER of 2%.",
                "The improved beam search (#REF) with an 8-gram LM, however, gets 52 out of 54 mappings correct on the Zodiac-408 cipher."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Zodiac-408, a homophonic cipher, is commonly used to evaluate decipherment algorithms.\n sent1: Our neural LM model with global rest cost estimation and frequency matching heuristic with a beam size of 1M has SER of 1.2% compared to the beam search algorithm #TARGET_REF with beam size of 10M with a 6-gram LM which gives an SER of 2%.\n sent2: The improved beam search (#REF) with an 8-gram LM, however, gets 52 out of 54 mappings correct on the Zodiac-408 cipher.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF presented an efficient A* search algorithm to solve letter substitution ciphers.",
                "#REF produce better results in faster time compared to ILP and EM-based decipherment methods by employing a higher order language model and an iterative beam search algorithm.",
                "#REF present various improvements to the beam search algorithm in #TARGET_REF including improved rest cost estimation and an optimized strategy for ordering decipherment of the cipher symbols.",
                "#REF propose a novel approach for solving mono-alphabetic substitution ciphers which combines character-level and word-level language model.",
                "They formulate decipherment as a tree search problem, and use Monte Carlo Tree Search (MCTS) as an alternative to beam search."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #REF presented an efficient A* search algorithm to solve letter substitution ciphers.\n sent1: #REF produce better results in faster time compared to ILP and EM-based decipherment methods by employing a higher order language model and an iterative beam search algorithm.\n sent2: #REF present various improvements to the beam search algorithm in #TARGET_REF including improved rest cost estimation and an optimized strategy for ordering decipherment of the cipher symbols.\n sent3: #REF propose a novel approach for solving mono-alphabetic substitution ciphers which combines character-level and word-level language model.\n sent4: They formulate decipherment as a tree search problem, and use Monte Carlo Tree Search (MCTS) as an alternative to beam search.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We use the notation from #TARGET_REF .",
                "Ciphertext f N 1 = f 1 ..f i ..f N and plaintext e N 1 = e 1 ..e i ..e N consist of vocabularies f i ∈ V f and e i ∈ V e respectively.",
                "The beginning tokens in the ciphertext (f 0 ) and plaintext (e 0 ) are set to \"$\" denoting the beginning of a sentence.",
                "The substitutions are represented by a function φ : V f → V e such that 1:1 substitutions are bijective while homophonic substitutions are general.",
                "A cipher function φ which does not have every φ(f ) fixed is called a partial cipher function (#REF) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We use the notation from #TARGET_REF .\n sent1: Ciphertext f N 1 = f 1 ..f i ..f N and plaintext e N 1 = e 1 ..e i ..e N consist of vocabularies f i ∈ V f and e i ∈ V e respectively.\n sent2: The beginning tokens in the ciphertext (f 0 ) and plaintext (e 0 ) are set to \"$\" denoting the beginning of a sentence.\n sent3: The substitutions are represented by a function φ : V f → V e such that 1:1 substitutions are bijective while homophonic substitutions are general.\n sent4: A cipher function φ which does not have every φ(f ) fixed is called a partial cipher function (#REF) .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "In this experiment we use a synthetic 1:1 letter substitution cipher dataset following #REF , #TARGET_REF and #REF .",
                "The text is from English Wikipedia articles about history 3 , preprocessed by stripping the text of all images, tables, then lower-casing all characters, and removing all non-alphabetic and non-space characters.",
                "We create 50 cryptograms for each length 16, 32, 64, 128 and 256 using a random Caesar-cipher 1:1 substitution."
            ],
            "label": [
                "USE",
                "SIMILARITY"
            ]
        },
        "input": "sent0: In this experiment we use a synthetic 1:1 letter substitution cipher dataset following #REF , #TARGET_REF and #REF .\n sent1: The text is from English Wikipedia articles about history 3 , preprocessed by stripping the text of all images, tables, then lower-casing all characters, and removing all non-alphabetic and non-space characters.\n sent2: We create 50 cryptograms for each length 16, 32, 64, 128 and 256 using a random Caesar-cipher 1:1 substitution.\n",
        "output": "{\"label\": [\"USE\", \"SIMILARITY\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "This paper presents, to our knowledge, the first application of large pre-trained neural LMs to the decipherment problem.",
                "We modify the beam search algorithm for decipherment from #TARGET_REF; and extend it to use global scoring of the plaintext message using neural LMs.",
                "To enable full plaintext scoring we use the neural LM to sample plaintext characters which reduces the beam size required.",
                "For challenging ciphers such as Beale Pt 2 we obtain lower error rates with smaller beam sizes when compared to the state of the art in decipherment for such ciphers."
            ],
            "label": [
                "DIFFERENCES",
                "EXTENDS"
            ]
        },
        "input": "sent0: This paper presents, to our knowledge, the first application of large pre-trained neural LMs to the decipherment problem.\n sent1: We modify the beam search algorithm for decipherment from #TARGET_REF; and extend it to use global scoring of the plaintext message using neural LMs.\n sent2: To enable full plaintext scoring we use the neural LM to sample plaintext characters which reduces the beam size required.\n sent3: For challenging ciphers such as Beale Pt 2 we obtain lower error rates with smaller beam sizes when compared to the state of the art in decipherment for such ciphers.\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"EXTENDS\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Not only do such deep models outperform traditional machine learning methods, they also come with the benefit of not requiring difficult feature engineering.",
                "For instance, both #REF and #TARGET_REF propose end-to-end models for sequence labelling task and achieve state-of-the-art results.",
                "* https://github.com/minghao-wu/CRF-AE † Work carried out at The University of Melbourne",
                "Orthogonal to the advances in deep learning is the effort spent on feature engineering.",
                "A representative example is the task of named entity recognition (NER), one that requires both lexical and syntactic knowledge, where, until recently, most models heavily rely on statistical sequential labelling models taking in manually engineered features (#REF; #REF; #REF) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Not only do such deep models outperform traditional machine learning methods, they also come with the benefit of not requiring difficult feature engineering.\n sent1: For instance, both #REF and #TARGET_REF propose end-to-end models for sequence labelling task and achieve state-of-the-art results.\n sent2: * https://github.com/minghao-wu/CRF-AE † Work carried out at The University of Melbourne\n sent3: Orthogonal to the advances in deep learning is the effort spent on feature engineering.\n sent4: A representative example is the task of named entity recognition (NER), one that requires both lexical and syntactic knowledge, where, until recently, most models heavily rely on statistical sequential labelling models taking in manually engineered features (#REF; #REF; #REF) .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Inspired by this, taking NER as a case study, we investigate the utility of hand-crafted features in deep learning models, challenging conventional wisdom in an attempt to refute the utility of manually-engineered features.",
                "Of particular interest to this paper is the work by #TARGET_REF introduce a strong end-to-end model combining a bi-directional Long Short-Term Memory (Bi-LSTM) network with Convolutional Neural Network (CNN) character encoding in a Conditional Random Field (CRF).",
                "Their model is highly capable of capturing not only word-but also characterlevel features.",
                "We extend this model by integrating an auto-encoder loss, allowing the model to take hand-crafted features as input and re-construct them as output, and show that, even with such a highly competitive model, incorporating linguistic features is still beneficial.",
                "Perhaps the closest to this study is the works by #REF and , who show how CRFs can be framed as auto-encoders in unsupervised or semisupervised settings."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Inspired by this, taking NER as a case study, we investigate the utility of hand-crafted features in deep learning models, challenging conventional wisdom in an attempt to refute the utility of manually-engineered features.\n sent1: Of particular interest to this paper is the work by #TARGET_REF introduce a strong end-to-end model combining a bi-directional Long Short-Term Memory (Bi-LSTM) network with Convolutional Neural Network (CNN) character encoding in a Conditional Random Field (CRF).\n sent2: Their model is highly capable of capturing not only word-but also characterlevel features.\n sent3: We extend this model by integrating an auto-encoder loss, allowing the model to take hand-crafted features as input and re-construct them as output, and show that, even with such a highly competitive model, incorporating linguistic features is still beneficial.\n sent4: Perhaps the closest to this study is the works by #REF and , who show how CRFs can be framed as auto-encoders in unsupervised or semisupervised settings.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Specifically, our model, referred to as Neural-CRF+AE, consists of four major components: (1) a character-level CNN (char-CNN); (2) a word-level bi-directional LSTM (Bi-LSTM); (3) a conditional random field (CRF); and (4) an auto-encoder auxiliary loss.",
                "An illustration of the model architecture is presented in Figure 1 .",
                "#REF; #REF; #TARGET_REF have demonstrated that CNNs are highly capable of capturing character-level features.",
                "Here, our character-level CNN is similar to that used in #REF but differs in that we use a ReLU activation (#REF) .",
                "1"
            ],
            "label": [
                "BACKGROUND",
                "MOTIVATION"
            ]
        },
        "input": "sent0: Specifically, our model, referred to as Neural-CRF+AE, consists of four major components: (1) a character-level CNN (char-CNN); (2) a word-level bi-directional LSTM (Bi-LSTM); (3) a conditional random field (CRF); and (4) an auto-encoder auxiliary loss.\n sent1: An illustration of the model architecture is presented in Figure 1 .\n sent2: #REF; #REF; #TARGET_REF have demonstrated that CNNs are highly capable of capturing character-level features.\n sent3: Here, our character-level CNN is similar to that used in #REF but differs in that we use a ReLU activation (#REF) .\n sent4: 1\n",
        "output": "{\"label\": [\"BACKGROUND\", \"MOTIVATION\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We build on a highly competitive sequence labelling model, namely Bi-LSTM-CNN-CRF, first introduced by #TARGET_REF .",
                "Given an input sequence of x = {x 1 , x 2 , . . . , x T } of length T , the model is capable of tagging each input with a predicted labelŷ, resulting in a sequence ofŷ = {ŷ 1 ,ŷ 2 , . . . ,ŷ T } closely matching the gold label sequence y = {y 1 , y 2 , . . . , y T }.",
                "Here, we extend the model by incorporating an auto-encoder loss taking hand-crafted features as in/output, thereby forcing the model to preserve crucial information stored in such features and allowing us to evaluate the impacts of each feature on model performance.",
                "Specifically, our model, referred to as Neural-CRF+AE, consists of four major components: (1) a character-level CNN (char-CNN); (2) a word-level bi-directional LSTM (Bi-LSTM); (3) a conditional random field (CRF); and (4) an auto-encoder auxiliary loss.",
                "An illustration of the model architecture is presented in Figure 1 ."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: We build on a highly competitive sequence labelling model, namely Bi-LSTM-CNN-CRF, first introduced by #TARGET_REF .\n sent1: Given an input sequence of x = {x 1 , x 2 , . . . , x T } of length T , the model is capable of tagging each input with a predicted labelŷ, resulting in a sequence ofŷ = {ŷ 1 ,ŷ 2 , . . . ,ŷ T } closely matching the gold label sequence y = {y 1 , y 2 , . . . , y T }.\n sent2: Here, we extend the model by incorporating an auto-encoder loss taking hand-crafted features as in/output, thereby forcing the model to preserve crucial information stored in such features and allowing us to evaluate the impacts of each feature on model performance.\n sent3: Specifically, our model, referred to as Neural-CRF+AE, consists of four major components: (1) a character-level CNN (char-CNN); (2) a word-level bi-directional LSTM (Bi-LSTM); (3) a conditional random field (CRF); and (4) an auto-encoder auxiliary loss.\n sent4: An illustration of the model architecture is presented in Figure 1 .\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "We use the CoNLL 2003 NER shared task dataset, consisting of 14,041/3,250/3,453 sentences in the training/development/test set respectively, all extracted from Reuters news articles during the period from 1996 to 1997.",
                "The dataset is annotated with four categories of name entities: PERSON, LOCATION, ORGANIZATION and MISC.",
                "We use the IOBES tagging scheme, as previous study have shown that this scheme provides a modest improvement to the model performance (#REF; #REF; #REF; #TARGET_REF .",
                "Model configuration.",
                "Following the work of #REF , we initialise word embeddings with GloVe (#REF ) (300-dimensional, trained on a 6B-token corpus)."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: We use the CoNLL 2003 NER shared task dataset, consisting of 14,041/3,250/3,453 sentences in the training/development/test set respectively, all extracted from Reuters news articles during the period from 1996 to 1997.\n sent1: The dataset is annotated with four categories of name entities: PERSON, LOCATION, ORGANIZATION and MISC.\n sent2: We use the IOBES tagging scheme, as previous study have shown that this scheme provides a modest improvement to the model performance (#REF; #REF; #REF; #TARGET_REF .\n sent3: Model configuration.\n sent4: Following the work of #REF , we initialise word embeddings with GloVe (#REF ) (300-dimensional, trained on a 6B-token corpus).\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Model configuration.",
                "Following the work of #TARGET_REF , we initialise word embeddings with GloVe (#REF ) (300-dimensional, trained on a 6B-token corpus).",
                "Character embeddings are 30-dimensional and randomly initialised with a uniform distribution in",
                "Parameters are optimised with stochastic gradient descent (SGD) with an initial learning rate of η = 0.015 and momentum of 0.9.",
                "Exponential learning rate decay is applied every 5 epochs with a factor of 0.8."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: Model configuration.\n sent1: Following the work of #TARGET_REF , we initialise word embeddings with GloVe (#REF ) (300-dimensional, trained on a 6B-token corpus).\n sent2: Character embeddings are 30-dimensional and randomly initialised with a uniform distribution in\n sent3: Parameters are optimised with stochastic gradient descent (SGD) with an initial learning rate of η = 0.015 and momentum of 0.9.\n sent4: Exponential learning rate decay is applied every 5 epochs with a factor of 0.8.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Evaluation.",
                "We measure model performance with the official CoNLL evaluation script and report span-level named entity F-score on the test set using early stopping based on the performance on the validation set.",
                "We report average F-scores and standard deviation over 5 runs for our model.",
                "Baseline.",
                "In addition to reporting a number of prior results of competitive baseline models, as listed in Table 2 , we also re-implement the Bi-LSTM-CNN-CRF model by #TARGET_REF (referred to as Neural-CRF in Table 2 ) and report its average performance."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: Evaluation.\n sent1: We measure model performance with the official CoNLL evaluation script and report span-level named entity F-score on the test set using early stopping based on the performance on the validation set.\n sent2: We report average F-scores and standard deviation over 5 runs for our model.\n sent3: Baseline.\n sent4: In addition to reporting a number of prior results of competitive baseline models, as listed in Table 2 , we also re-implement the Bi-LSTM-CNN-CRF model by #TARGET_REF (referred to as Neural-CRF in Table 2 ) and report its average performance.\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Specifically, our model, referred to as Neural-CRF+AE, consists of four major components: (1) a character-level CNN (char-CNN); (2) a word-level bi-directional LSTM (Bi-LSTM); (3) a conditional random field (CRF); and (4) an auto-encoder auxiliary loss.",
                "An illustration of the model architecture is presented in Figure 1 .",
                "#REF; #REF; #REF) have demonstrated that CNNs are highly capable of capturing character-level features.",
                "Here, our character-level CNN is similar to that used in #TARGET_REF but differs in that we use a ReLU activation (#REF) .",
                "1"
            ],
            "label": [
                "DIFFERENCES",
                "EXTENDS"
            ]
        },
        "input": "sent0: Specifically, our model, referred to as Neural-CRF+AE, consists of four major components: (1) a character-level CNN (char-CNN); (2) a word-level bi-directional LSTM (Bi-LSTM); (3) a conditional random field (CRF); and (4) an auto-encoder auxiliary loss.\n sent1: An illustration of the model architecture is presented in Figure 1 .\n sent2: #REF; #REF; #REF) have demonstrated that CNNs are highly capable of capturing character-level features.\n sent3: Here, our character-level CNN is similar to that used in #TARGET_REF but differs in that we use a ReLU activation (#REF) .\n sent4: 1\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"EXTENDS\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Parsing morphologically rich languages (MRLs) is difficult due to the complex relationship of syntax to morphology.",
                "But the success of neural networks offer an appealing solution to this problem by computing word representation from characters.",
                "Character-level models (#REF; #REF) learn relationship between similar word forms and have shown to be effective for parsing MRLs (#REF; #REF; #REF; Björkelund et al., 2017) .",
                "Does that mean that we can do away with explicit modeling of morphology altogether? Consider two challenges in parsing MRLs raised by #TARGET_REF Tsarfaty et al. ( , 2013 :",
                "• Can we represent words abstractly so as to reflect shared morphological aspects between them? • Which types of morphological information should we include in the parsing model?"
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Parsing morphologically rich languages (MRLs) is difficult due to the complex relationship of syntax to morphology.\n sent1: But the success of neural networks offer an appealing solution to this problem by computing word representation from characters.\n sent2: Character-level models (#REF; #REF) learn relationship between similar word forms and have shown to be effective for parsing MRLs (#REF; #REF; #REF; Björkelund et al., 2017) .\n sent3: Does that mean that we can do away with explicit modeling of morphology altogether? Consider two challenges in parsing MRLs raised by #TARGET_REF Tsarfaty et al. ( , 2013 :\n sent4: • Can we represent words abstractly so as to reflect shared morphological aspects between them? • Which types of morphological information should we include in the parsing model?\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Does that mean that we can do away with explicit modeling of morphology altogether? Consider two challenges in parsing MRLs raised by Tsarfaty et al. (2010 Tsarfaty et al. ( , 2013 :",
                "• Can we represent words abstractly so as to reflect shared morphological aspects between them? • Which types of morphological information should we include in the parsing model?",
                "It is tempting to hypothesize that character-level models effectively solve the first problem.",
                "For the second, #TARGET_REF and #REF reported that morphological case is beneficial across morphologically rich languages with extensive case systems, where case syncretism is pervasive and often hurts parsing performance. But #TARGET_REF focus on vintage parsers; do neural parsers with character-level representations also solve this second problem?",
                "We attempt to answer this question by asking whether an explicit model of morphological case helps dependency parsing, and our results show that it does."
            ],
            "label": [
                "MOTIVATION",
                "BACKGROUND"
            ]
        },
        "input": "sent0: Does that mean that we can do away with explicit modeling of morphology altogether? Consider two challenges in parsing MRLs raised by Tsarfaty et al. (2010 Tsarfaty et al. ( , 2013 :\n sent1: • Can we represent words abstractly so as to reflect shared morphological aspects between them? • Which types of morphological information should we include in the parsing model?\n sent2: It is tempting to hypothesize that character-level models effectively solve the first problem.\n sent3: For the second, #TARGET_REF and #REF reported that morphological case is beneficial across morphologically rich languages with extensive case systems, where case syncretism is pervasive and often hurts parsing performance. But #TARGET_REF focus on vintage parsers; do neural parsers with character-level representations also solve this second problem?\n sent4: We attempt to answer this question by asking whether an explicit model of morphological case helps dependency parsing, and our results show that it does.\n",
        "output": "{\"label\": [\"MOTIVATION\", \"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "We train a morphological tagger to predict case information.",
                "The tagger has the same structure as the parser's encoder, with an additional feedforward neural network with one hidden layer followed by a softmax layer.",
                "We found that predicted case improves accuracy, although the effect is different across languages.",
                "These results are interesting, since in vintage parsers, predicted case usually harmed accuracy ( #TARGET_REF) .",
                "However, we note that our taggers use gold POS, which might help."
            ],
            "label": [
                "BACKGROUND",
                "DIFFERENCES"
            ]
        },
        "input": "sent0: We train a morphological tagger to predict case information.\n sent1: The tagger has the same structure as the parser's encoder, with an additional feedforward neural network with one hidden layer followed by a softmax layer.\n sent2: We found that predicted case improves accuracy, although the effect is different across languages.\n sent3: These results are interesting, since in vintage parsers, predicted case usually harmed accuracy ( #TARGET_REF) .\n sent4: However, we note that our taggers use gold POS, which might help.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"DIFFERENCES\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "2. Ambiguity -for written text, a change in context may require a different normalization.",
                "For example, \"2/3\" can be verbalized as a date or fraction depending on the meaning of the sentence.",
                "Traditionally, the task of NSW normalization has been approached by manually authoring grammars in the form of finite-state transducers (#REF; #REF) such as integer grammars (e.g., \"26\" → \"twenty six\") or time grammars (e.g., \"5:26\" → \"five twenty six\").",
                "Constructing such grammars is time consuming and error-prone and requires extensive linguistic knowledge and programming proficiency.",
                "Recently, with the rise of machine learning and especially deep learning techniques, researchers are starting to bring more data-driven approaches to this field #TARGET_REF ."
            ],
            "label": [
                "BACKGROUND",
                "MOTIVATION"
            ]
        },
        "input": "sent0: 2. Ambiguity -for written text, a change in context may require a different normalization.\n sent1: For example, \"2/3\" can be verbalized as a date or fraction depending on the meaning of the sentence.\n sent2: Traditionally, the task of NSW normalization has been approached by manually authoring grammars in the form of finite-state transducers (#REF; #REF) such as integer grammars (e.g., \"26\" → \"twenty six\") or time grammars (e.g., \"5:26\" → \"five twenty six\").\n sent3: Constructing such grammars is time consuming and error-prone and requires extensive linguistic knowledge and programming proficiency.\n sent4: Recently, with the rise of machine learning and especially deep learning techniques, researchers are starting to bring more data-driven approaches to this field #TARGET_REF .\n",
        "output": "{\"label\": [\"BACKGROUND\", \"MOTIVATION\"], \"context\": [\"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Recently, methods based on neural networks have been applied to TN and ITN #TARGET_REF; #REF; #REF) .",
                "To overcome one of the biggest problems -a lack of supervision, WFSTs have been used to transform large amounts of written-form text to its spoken form.",
                "Researchers hope a vast amount of such data can counteract the errors inherited in WFST-based models.",
                "Recent data-driven approaches examine window-based sequence-to-sequence (seq2seq) models and convolutional neural networks (CNN) to normalize a central piece of text with the help of context (#REF; #REF) .",
                "Window-based methods have the advantage of limiting the output vocabulary size, as most tokens that do not need to be transformed are labeled with a special <self> token."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Recently, methods based on neural networks have been applied to TN and ITN #TARGET_REF; #REF; #REF) .\n sent1: To overcome one of the biggest problems -a lack of supervision, WFSTs have been used to transform large amounts of written-form text to its spoken form.\n sent2: Researchers hope a vast amount of such data can counteract the errors inherited in WFST-based models.\n sent3: Recent data-driven approaches examine window-based sequence-to-sequence (seq2seq) models and convolutional neural networks (CNN) to normalize a central piece of text with the help of context (#REF; #REF) .\n sent4: Window-based methods have the advantage of limiting the output vocabulary size, as most tokens that do not need to be transformed are labeled with a special <self> token.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Recently, methods based on neural networks have been applied to TN and ITN (#REF; #REF; #REF) .",
                "To overcome one of the biggest problems -a lack of supervision, WFSTs have been used to transform large amounts of written-form text to its spoken form.",
                "Researchers hope a vast amount of such data can counteract the errors inherited in WFST-based models.",
                "Recent data-driven approaches examine window-based sequence-to-sequence (seq2seq) models and convolutional neural networks (CNN) to normalize a central piece of text with the help of context #TARGET_REF; #REF) .",
                "Window-based methods have the advantage of limiting the output vocabulary size, as most tokens that do not need to be transformed are labeled with a special <self> token."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Recently, methods based on neural networks have been applied to TN and ITN (#REF; #REF; #REF) .\n sent1: To overcome one of the biggest problems -a lack of supervision, WFSTs have been used to transform large amounts of written-form text to its spoken form.\n sent2: Researchers hope a vast amount of such data can counteract the errors inherited in WFST-based models.\n sent3: Recent data-driven approaches examine window-based sequence-to-sequence (seq2seq) models and convolutional neural networks (CNN) to normalize a central piece of text with the help of context #TARGET_REF; #REF) .\n sent4: Window-based methods have the advantage of limiting the output vocabulary size, as most tokens that do not need to be transformed are labeled with a special <self> token.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Following #TARGET_REF, we implement a seq2seq model trained on window-based data.",
                "Table 1 illustrates the window-based model's training examples corresponding to one sentence \"wake me up at 8 AM .\" which is broken down into 6 pairs.",
                "<n> and </n> indicate the center of the window.",
                "A window center might contain 1 or more words (e.g., \"8 AM\") and the grouping is provided by the dataset where each input sentence is segmented into chunks corresponding to labels such as TIME, DATE, ORDINAL (#REF) .",
                "The model outputs tokens which correspond to the center of the window."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Following #TARGET_REF, we implement a seq2seq model trained on window-based data.\n sent1: Table 1 illustrates the window-based model's training examples corresponding to one sentence \"wake me up at 8 AM .\" which is broken down into 6 pairs.\n sent2: <n> and </n> indicate the center of the window.\n sent3: A window center might contain 1 or more words (e.g., \"8 AM\") and the grouping is provided by the dataset where each input sentence is segmented into chunks corresponding to labels such as TIME, DATE, ORDINAL (#REF) .\n sent4: The model outputs tokens which correspond to the center of the window.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Table 1 illustrates the window-based model's training examples corresponding to one sentence \"wake me up at 8 AM .\" which is broken down into 6 pairs.",
                "<n> and </n> indicate the center of the window.",
                "A window center might contain 1 or more words (e.g., \"8 AM\") and the grouping is provided by the dataset where each input sentence is segmented into chunks corresponding to labels such as TIME, DATE, ORDINAL #TARGET_REF .",
                "The model outputs tokens which correspond to the center of the window.",
                "Input Output <n> wake </n> me <self> wake <n> me </n> up <self> me <n> up </n> at <self>"
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Table 1 illustrates the window-based model's training examples corresponding to one sentence \"wake me up at 8 AM .\" which is broken down into 6 pairs.\n sent1: <n> and </n> indicate the center of the window.\n sent2: A window center might contain 1 or more words (e.g., \"8 AM\") and the grouping is provided by the dataset where each input sentence is segmented into chunks corresponding to labels such as TIME, DATE, ORDINAL #TARGET_REF .\n sent3: The model outputs tokens which correspond to the center of the window.\n sent4: Input Output <n> wake </n> me <self> wake <n> me </n> up <self> me <n> up </n> at <self>\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\", \"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The data for the window-based seq2seq model and full sentence seq2seq were generated from the publicly available release of parallel written/speech formatted text from #TARGET_REF .",
                "The set consists of Wikipedia text which was processed through Google TTS's Kestrel text normalization system relying primarily on handcrafted rules to produce speech-formatted text.",
                "Although a large parallel dataset is available for English, we consider the feasibility of developing neural models for other languages which may not have text normalization systems in place.",
                "Therefore, we choose to scale the training data size to a limited set of text which could be generated by annotators in a reasonable time frame.",
                "As summarized in Table 2 , both window-based and sentencebased models are trained with 500K training instances."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The data for the window-based seq2seq model and full sentence seq2seq were generated from the publicly available release of parallel written/speech formatted text from #TARGET_REF .\n sent1: The set consists of Wikipedia text which was processed through Google TTS's Kestrel text normalization system relying primarily on handcrafted rules to produce speech-formatted text.\n sent2: Although a large parallel dataset is available for English, we consider the feasibility of developing neural models for other languages which may not have text normalization systems in place.\n sent3: Therefore, we choose to scale the training data size to a limited set of text which could be generated by annotators in a reasonable time frame.\n sent4: As summarized in Table 2 , both window-based and sentencebased models are trained with 500K training instances.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Therefore, we choose to scale the training data size to a limited set of text which could be generated by annotators in a reasonable time frame.",
                "As summarized in Table 2 , both window-based and sentencebased models are trained with 500K training instances.",
                "Our datasets were randomly sampled from a set of 4.9M sentences in the training data portion of the #TARGET_REF data release and split into training, validation, and test data.",
                "However, the training data for window-based and sentencebased models are not identical due to differences in input configurations.",
                "While the window-based model uses 500K randomly sampled windows, the sentence-based models use 500K sentences."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Therefore, we choose to scale the training data size to a limited set of text which could be generated by annotators in a reasonable time frame.\n sent1: As summarized in Table 2 , both window-based and sentencebased models are trained with 500K training instances.\n sent2: Our datasets were randomly sampled from a set of 4.9M sentences in the training data portion of the #TARGET_REF data release and split into training, validation, and test data.\n sent3: However, the training data for window-based and sentencebased models are not identical due to differences in input configurations.\n sent4: While the window-based model uses 500K randomly sampled windows, the sentence-based models use 500K sentences.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "There are large numbers of <self> tokens present in the dataset.",
                "We follow #TARGET_REF in down-sampling window-based training data to constrain the proportion of \"<self>\" tokens to 10% of the data.",
                "For training sentence-based models, the source sentence is segmented into characters while the target sentence is broken into tokens.",
                "For the subword model, both the source and target sentences are segmented into subword sequences.",
                "Subword units are concatenated to words for evaluation."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: There are large numbers of <self> tokens present in the dataset.\n sent1: We follow #TARGET_REF in down-sampling window-based training data to constrain the proportion of \"<self>\" tokens to 10% of the data.\n sent2: For training sentence-based models, the source sentence is segmented into characters while the target sentence is broken into tokens.\n sent3: For the subword model, both the source and target sentences are segmented into subword sequences.\n sent4: Subword units are concatenated to words for evaluation.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Our first approach replicates the window-based seq2seq model of #TARGET_REF .",
                "The model encodes the central piece of text (1 or more tokens) including its context of N previous and following tokens at the character level.",
                "The output is a target token or a sequence of tokens.",
                "The input vocabulary consists of 250 common characters including letters, digits and symbols (e.g., $).",
                "The decoder vocabulary consists of 1K tokens including <self> and <sil>, the latter of which is used to normalize punctuation."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Our first approach replicates the window-based seq2seq model of #TARGET_REF .\n sent1: The model encodes the central piece of text (1 or more tokens) including its context of N previous and following tokens at the character level.\n sent2: The output is a target token or a sequence of tokens.\n sent3: The input vocabulary consists of 250 common characters including letters, digits and symbols (e.g., $).\n sent4: The decoder vocabulary consists of 1K tokens including <self> and <sil>, the latter of which is used to normalize punctuation.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "upper, lower, mixed, nonalphanumerical, foreign characters; 2) position: (#REF) .",
                "Edit labels are the most expensive to obtain in real life.",
                "Our labels are generated directly from the Google FST #TARGET_REF .",
                "Each type of feature is represented by a one-hot encoding.",
                "To combine linguistic features with subword units, one can add or concatenate each subword's embedding with its corresponding linguistic feature embedding and feed a combined embedding to the bi-LSTM encoder."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: upper, lower, mixed, nonalphanumerical, foreign characters; 2) position: (#REF) .\n sent1: Edit labels are the most expensive to obtain in real life.\n sent2: Our labels are generated directly from the Google FST #TARGET_REF .\n sent3: Each type of feature is represented by a one-hot encoding.\n sent4: To combine linguistic features with subword units, one can add or concatenate each subword's embedding with its corresponding linguistic feature embedding and feed a combined embedding to the bi-LSTM encoder.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "A dropout of 0.3 was used across all models.",
                "Figure 2: Evaluation of the window-based model.",
                "Categories are sorted by frequency.",
                "* TELEPHONE is not reported in #TARGET_REF but included in the dataset; ** we removed ELECTRONIC category.",
                "As shown in Figure 2 , our replicated windowbased model achieves reasonable performance compared with #REF , considering our training set is much smaller."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: A dropout of 0.3 was used across all models.\n sent1: Figure 2: Evaluation of the window-based model.\n sent2: Categories are sorted by frequency.\n sent3: * TELEPHONE is not reported in #TARGET_REF but included in the dataset; ** we removed ELECTRONIC category.\n sent4: As shown in Figure 2 , our replicated windowbased model achieves reasonable performance compared with #REF , considering our training set is much smaller.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "There are 16 different edit labels shown.",
                "Data with TELEPHONE labels were not included in the initial analysis of #TARGET_REF , but were made available in the dataset release.",
                "For our second baseline model which operates on whole sentences, on the input side, we still use 250 common characters.",
                "However, due to the removal of the <self> token, the output space is drastically extended from 1K tokens to 45K tokens.",
                "Thus, it becomes increasingly difficult for the model to learn and predict."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: There are 16 different edit labels shown.\n sent1: Data with TELEPHONE labels were not included in the initial analysis of #TARGET_REF , but were made available in the dataset release.\n sent2: For our second baseline model which operates on whole sentences, on the input side, we still use 250 common characters.\n sent3: However, due to the removal of the <self> token, the output space is drastically extended from 1K tokens to 45K tokens.\n sent4: Thus, it becomes increasingly difficult for the model to learn and predict.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Figure 2: Evaluation of the window-based model.",
                "Categories are sorted by frequency.",
                "* TELEPHONE is not reported in #REF but included in the dataset; ** we removed ELECTRONIC category.",
                "As shown in Figure 2 , our replicated windowbased model achieves reasonable performance compared with #TARGET_REF , considering our training set is much smaller.",
                "There are 16 different edit labels shown."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: Figure 2: Evaluation of the window-based model.\n sent1: Categories are sorted by frequency.\n sent2: * TELEPHONE is not reported in #REF but included in the dataset; ** we removed ELECTRONIC category.\n sent3: As shown in Figure 2 , our replicated windowbased model achieves reasonable performance compared with #TARGET_REF , considering our training set is much smaller.\n sent4: There are 16 different edit labels shown.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The introduction of pre-trained language models, such as BERT #TARGET_REF and Open-GPT [3] , among many others, has brought tremendous progress to the NLP research and industrial communities.",
                "The contribution of these models can be categorized into two aspects.",
                "First, pre-trained language models allow modelers to achieve reasonable accuracy without the need an excessive amount of manually labeled data.",
                "This strategy is in contrast with the classical deep learning methods, which requires a multitude more data to reach comparable results.",
                "Second, for many NLP tasks, including but not limited to, SQuAD [4] , CoQA [5] , named entity recognition [6] , #REF , machine translation [8] , pre-trained model allows the creation of new state-of-art, given a reasonable amount of labelled data."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The introduction of pre-trained language models, such as BERT #TARGET_REF and Open-GPT [3] , among many others, has brought tremendous progress to the NLP research and industrial communities.\n sent1: The contribution of these models can be categorized into two aspects.\n sent2: First, pre-trained language models allow modelers to achieve reasonable accuracy without the need an excessive amount of manually labeled data.\n sent3: This strategy is in contrast with the classical deep learning methods, which requires a multitude more data to reach comparable results.\n sent4: Second, for many NLP tasks, including but not limited to, SQuAD [4] , CoQA [5] , named entity recognition [6] , #REF , machine translation [8] , pre-trained model allows the creation of new state-of-art, given a reasonable amount of labelled data.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "It is obvious that the word \"bank\" represents completely different meaning, which the word embeddings techniques fail to capture.",
                "The aforementioned deficiencies prompt researchers to propose deep neural networks that are able to be trained in an unsupervised fashion while being able to capture the contextual meaning of the words presented in the texts.",
                "Some early attempts include pre-trained models includes, #REF , CVT [13, 14] , EL#REF and ULMFiT [16] .",
                "However, the most successful ones are BERT #TARGET_REF and Open-GPT [3] .",
                "Unlike standard NLP deep learning model, BERT and Open-GPT are built on top of transformer [17] structures, instead of LSTM [18] or GRU [19] ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: It is obvious that the word \"bank\" represents completely different meaning, which the word embeddings techniques fail to capture.\n sent1: The aforementioned deficiencies prompt researchers to propose deep neural networks that are able to be trained in an unsupervised fashion while being able to capture the contextual meaning of the words presented in the texts.\n sent2: Some early attempts include pre-trained models includes, #REF , CVT [13, 14] , EL#REF and ULMFiT [16] .\n sent3: However, the most successful ones are BERT #TARGET_REF and Open-GPT [3] .\n sent4: Unlike standard NLP deep learning model, BERT and Open-GPT are built on top of transformer [17] structures, instead of LSTM [18] or GRU [19] .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "In this line of work, #REF investigated the linguistic knowledge and transferability of contextual representations by comparing BERT [2] with EL#REF , and concluded that while the higher levels of LSTM's are more task-specific, this trend does not exhibit in transformer based models.",
                "#REF invented projected attention layer for multi-task learning using BERT, which results in an improvement in various state-of-the-art results compared to the original work of Devlin et al. #TARGET_REF .",
                "#REF propose a \"post-training\" algorithms, which does not directly fine-tune BERT, but rather first \"post-train\" BERT on the task related corpus using the masked language prediction task next sentence prediction task, which helps to reduce the bias in the training corpus.",
                "Finally, #REF added additional fine-tuning tasks based on multi-task training, which further improves the prediction power of BERT in the tasks of text classification.",
                "In this aspect, however, there is a simple yet crucial question that needs to be addressed."
            ],
            "label": [
                "BACKGROUND",
                "MOTIVATION"
            ]
        },
        "input": "sent0: In this line of work, #REF investigated the linguistic knowledge and transferability of contextual representations by comparing BERT [2] with EL#REF , and concluded that while the higher levels of LSTM's are more task-specific, this trend does not exhibit in transformer based models.\n sent1: #REF invented projected attention layer for multi-task learning using BERT, which results in an improvement in various state-of-the-art results compared to the original work of Devlin et al. #TARGET_REF .\n sent2: #REF propose a \"post-training\" algorithms, which does not directly fine-tune BERT, but rather first \"post-train\" BERT on the task related corpus using the masked language prediction task next sentence prediction task, which helps to reduce the bias in the training corpus.\n sent3: Finally, #REF added additional fine-tuning tasks based on multi-task training, which further improves the prediction power of BERT in the tasks of text classification.\n sent4: In this aspect, however, there is a simple yet crucial question that needs to be addressed.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"MOTIVATION\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Second, the core computational unit is matrix multiplications, which allows researchers to utilize the full computational potential of TPU [20] .",
                "After training on a large corpus, both BERT and Open-GPT are able to renew the SOTA of many important natural language tasks, such as such as SQuAD [4] , CoQA [5] , named entity recognition [6] , #REF , machine translation [8] .",
                "In the presence of the success of pre-trained language models, especially BERT [2] , it is natural to ask how to best utilize the pre-trained language models to achieve new state-of-the-art results.",
                "In this line of work, #REF investigated the linguistic knowledge and transferability of contextual representations by comparing BERT #TARGET_REF with EL#REF , and concluded that while the higher levels of LSTM's are more task-specific, this trend does not exhibit in transformer based models.",
                "#REF invented projected attention layer for multi-task learning using BERT, which results in an improvement in various state-of-the-art results compared to the original work of #REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Second, the core computational unit is matrix multiplications, which allows researchers to utilize the full computational potential of TPU [20] .\n sent1: After training on a large corpus, both BERT and Open-GPT are able to renew the SOTA of many important natural language tasks, such as such as SQuAD [4] , CoQA [5] , named entity recognition [6] , #REF , machine translation [8] .\n sent2: In the presence of the success of pre-trained language models, especially BERT [2] , it is natural to ask how to best utilize the pre-trained language models to achieve new state-of-the-art results.\n sent3: In this line of work, #REF investigated the linguistic knowledge and transferability of contextual representations by comparing BERT #TARGET_REF with EL#REF , and concluded that while the higher levels of LSTM's are more task-specific, this trend does not exhibit in transformer based models.\n sent4: #REF invented projected attention layer for multi-task learning using BERT, which results in an improvement in various state-of-the-art results compared to the original work of #REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Pre-training models have been used to obtain more effective word representations through the study of a large number of corpora.",
                "In the paradigm proposed in the original work by Devlin et al. #TARGET_REF , the author directly trained BERT along with with a light-weighted task-specific head.",
                "In our case though, we top BERT with a more complex network structure, using Kaiming initialization [29] .",
                "If one would fine-tune directly the top models along with the weights in BERT, one is faced with the following dilemma: on the one hand, if the learning rate is too large, it is likely to disturb the structure innate to the pre-trained language models; on the other hand, if the learning rate is too small, since we top BERT with relatively complex models, the convergence of the top models might be impeded.",
                "Therefore, in the first phase we fix the weights in the pre-training language models, and only train the model on top of it."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Pre-training models have been used to obtain more effective word representations through the study of a large number of corpora.\n sent1: In the paradigm proposed in the original work by Devlin et al. #TARGET_REF , the author directly trained BERT along with with a light-weighted task-specific head.\n sent2: In our case though, we top BERT with a more complex network structure, using Kaiming initialization [29] .\n sent3: If one would fine-tune directly the top models along with the weights in BERT, one is faced with the following dilemma: on the one hand, if the learning rate is too large, it is likely to disturb the structure innate to the pre-trained language models; on the other hand, if the learning rate is too small, since we top BERT with relatively complex models, the convergence of the top models might be impeded.\n sent4: Therefore, in the first phase we fix the weights in the pre-training language models, and only train the model on top of it.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Second, the core computational unit is matrix multiplications, which allows researchers to utilize the full computational potential of TPU [20] .",
                "After training on a large corpus, both BERT and Open-GPT are able to renew the SOTA of many important natural language tasks, such as such as SQuAD [4] , CoQA [5] , named entity recognition [6] , #REF , machine translation [8] .",
                "In the presence of the success of pre-trained language models, especially BERT #TARGET_REF , it is natural to ask how to best utilize the pre-trained language models to achieve new state-of-the-art results.",
                "In this line of work, #REF investigated the linguistic knowledge and transferability of contextual representations by comparing BERT [2] with EL#REF , and concluded that while the higher levels of LSTM's are more task-specific, this trend does not exhibit in transformer based models.",
                "#REF invented projected attention layer for multi-task learning using BERT, which results in an improvement in various state-of-the-art results compared to the original work of #REF ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Second, the core computational unit is matrix multiplications, which allows researchers to utilize the full computational potential of TPU [20] .\n sent1: After training on a large corpus, both BERT and Open-GPT are able to renew the SOTA of many important natural language tasks, such as such as SQuAD [4] , CoQA [5] , named entity recognition [6] , #REF , machine translation [8] .\n sent2: In the presence of the success of pre-trained language models, especially BERT #TARGET_REF , it is natural to ask how to best utilize the pre-trained language models to achieve new state-of-the-art results.\n sent3: In this line of work, #REF investigated the linguistic knowledge and transferability of contextual representations by comparing BERT [2] with EL#REF , and concluded that while the higher levels of LSTM's are more task-specific, this trend does not exhibit in transformer based models.\n sent4: #REF invented projected attention layer for multi-task learning using BERT, which results in an improvement in various state-of-the-art results compared to the original work of #REF .\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "In the next two verification tasks, we use \"[CLS]\" for prediction and add two fully connected layers subsequently.",
                "Under our strategy stackand-finetune, we set different learning rates for the two phases.",
                "We tried to set the learning rate of the first stage to 1e −1 ,1e −2 ,5e −3 ,1e −3 and 5e −4 , and set it to a smaller number in the latter stage, such as 1e −3 ,1e −4 ,5e −5 and 1e −5 .",
                "After our experiments, we found that it gets better results while the learning rate is set to 0.001 in the stage of training only the upper model and set to 5e −5 in the later stage.",
                "Since BERT-Adam #TARGET_REF has excellent performance, in our experiments, we use it as an optimizer with β 1 = 0.9, β 2 = 0.999,L 2 -weight decay of 0.01.We apply a dropout trick on all layers and set the dropout probability as 0.1."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: In the next two verification tasks, we use \"[CLS]\" for prediction and add two fully connected layers subsequently.\n sent1: Under our strategy stackand-finetune, we set different learning rates for the two phases.\n sent2: We tried to set the learning rate of the first stage to 1e −1 ,1e −2 ,5e −3 ,1e −3 and 5e −4 , and set it to a smaller number in the latter stage, such as 1e −3 ,1e −4 ,5e −5 and 1e −5 .\n sent3: After our experiments, we found that it gets better results while the learning rate is set to 0.001 in the stage of training only the upper model and set to 5e −5 in the later stage.\n sent4: Since BERT-Adam #TARGET_REF has excellent performance, in our experiments, we use it as an optimizer with β 1 = 0.9, β 2 = 0.999,L 2 -weight decay of 0.01.We apply a dropout trick on all layers and set the dropout probability as 0.1.\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "In the sequence labeling task,we explore sub-task named entity recognition using CoNLL03 dataset [6] , which is a public available used in many studies to test the accuracy of their proposed methods [30, 31, 32, 33, #TARGET_REF .",
                "For strategy finetune-only and strategy stack-and-finetune, we implemented two models: one with BERT and the other with BERT adding a Bi-LSTM on top.",
                "Eval measure is accuracy and F1 score."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: In the sequence labeling task,we explore sub-task named entity recognition using CoNLL03 dataset [6] , which is a public available used in many studies to test the accuracy of their proposed methods [30, 31, 32, 33, #TARGET_REF .\n sent1: For strategy finetune-only and strategy stack-and-finetune, we implemented two models: one with BERT and the other with BERT adding a Bi-LSTM on top.\n sent2: Eval measure is accuracy and F1 score.\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Recent successes in statistical syntactic parsing based on supervised learning techniques trained on a large corpus of syntactic trees (#REF; #REF; #TARGET_REF have brought forth the hope that the same approaches could be applied to the more ambitious goal of recovering the propositional content and the frame semantics of a sentence.",
                "Moving towards a shallow semantic level of representation is a first initial step towards the distant goal of natural language understanding and has immediate applications in question-answering and information extraction.",
                "For example, an automatic flight reservation system processing the sentence I want to book a flight from Geneva to Trento will need to know that from Geneva denotes the origin of the flight and to Trento denotes its destination. Knowing that these two phrases are prepositional phrases, the information provided by a syntactic parser, is only moderately useful.",
                "The growing interest in learning deeper information is to a large extent supported and due to the recent development of semantically annotated databases such as FrameNet (#REF) or the Proposition Bank , that can be used as training resources for a number of supervised learning paradigms.",
                "We focus here on the Proposition Bank (PropBank)."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Recent successes in statistical syntactic parsing based on supervised learning techniques trained on a large corpus of syntactic trees (#REF; #REF; #TARGET_REF have brought forth the hope that the same approaches could be applied to the more ambitious goal of recovering the propositional content and the frame semantics of a sentence.\n sent1: Moving towards a shallow semantic level of representation is a first initial step towards the distant goal of natural language understanding and has immediate applications in question-answering and information extraction.\n sent2: For example, an automatic flight reservation system processing the sentence I want to book a flight from Geneva to Trento will need to know that from Geneva denotes the origin of the flight and to Trento denotes its destination. Knowing that these two phrases are prepositional phrases, the information provided by a syntactic parser, is only moderately useful.\n sent3: The growing interest in learning deeper information is to a large extent supported and due to the recent development of semantically annotated databases such as FrameNet (#REF) or the Proposition Bank , that can be used as training resources for a number of supervised learning paradigms.\n sent4: We focus here on the Proposition Bank (PropBank).\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "The representation h(d 1 , . . . , d i−1 ) is computed from a set f of features of the derivation move d i−1 and from a finite set D of recent history representations h(d 1 , . . . , d j ), where j < i − 1.",
                "Because the history representation computed for the move i − 1 is included in the inputs to the computation of the representation for the next move i, virtually any information about the derivation history could flow from history representation to history representation and be used to estimate the probability of a derivation move.",
                "However, the recency preference exhibited by recursively defined neural networks biases learning towards information which flows through fewer history representations.",
                "#TARGET_REF exploits this bias by directly inputting information which is considered relevant at a given step to the history representation of the constituent on the top of the stack before that step.",
                "In addition to history representations, the inputs to h(d 1 , . . . , d i−1 ) include handcrafted features of the derivation history that are meant to be relevant to the move to be chosen at step i. For each of the experiments reported here, the set D that is input to the computation of the history representation of the derivation moves d 1 , . . . , d i−1 includes the most recent history representation of the following nodes: top i , the node on top of the pushdown stack before the ith move; the left-corner ancestor of top i (that is, the second top-most node on the parser's stack); the leftmost child of top i ; and the most recent child of top i , if any."
            ],
            "label": [
                "BACKGROUND",
                "MOTIVATION"
            ]
        },
        "input": "sent0: The representation h(d 1 , . . . , d i−1 ) is computed from a set f of features of the derivation move d i−1 and from a finite set D of recent history representations h(d 1 , . . . , d j ), where j < i − 1.\n sent1: Because the history representation computed for the move i − 1 is included in the inputs to the computation of the representation for the next move i, virtually any information about the derivation history could flow from history representation to history representation and be used to estimate the probability of a derivation move.\n sent2: However, the recency preference exhibited by recursively defined neural networks biases learning towards information which flows through fewer history representations.\n sent3: #TARGET_REF exploits this bias by directly inputting information which is considered relevant at a given step to the history representation of the constituent on the top of the stack before that step.\n sent4: In addition to history representations, the inputs to h(d 1 , . . . , d i−1 ) include handcrafted features of the derivation history that are meant to be relevant to the move to be chosen at step i. For each of the experiments reported here, the set D that is input to the computation of the history representation of the derivation moves d 1 , . . . , d i−1 includes the most recent history representation of the following nodes: top i , the node on top of the pushdown stack before the ith move; the left-corner ancestor of top i (that is, the second top-most node on the parser's stack); the leftmost child of top i ; and the most recent child of top i , if any.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"MOTIVATION\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The VP node is assumed to be on the top of the parser's stack, and the S one is supposed to be its leftcorner ancestor.",
                "The directed arcs represent the information that flows from one node to another.",
                "According to the original SSN model in #TARGET_REF , only the information carried over by the leftmost child and the most recent child of a constituent directly flows to that constituent.",
                "In the figure above, only the information conveyed by the nodes α and δ is directly input to the node S. Similarly, the only bottom-up information directly input to the VP node is conveyed by the child nodes ǫ and θ.",
                "In the original SSN models, nodes bearing a function label such as φ 1 and φ 2 are not directly input to their respective parents."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The VP node is assumed to be on the top of the parser's stack, and the S one is supposed to be its leftcorner ancestor.\n sent1: The directed arcs represent the information that flows from one node to another.\n sent2: According to the original SSN model in #TARGET_REF , only the information carried over by the leftmost child and the most recent child of a constituent directly flows to that constituent.\n sent3: In the figure above, only the information conveyed by the nodes α and δ is directly input to the node S. Similarly, the only bottom-up information directly input to the VP node is conveyed by the child nodes ǫ and θ.\n sent4: In the original SSN models, nodes bearing a function label such as φ 1 and φ 2 are not directly input to their respective parents.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "While some of these models are based on full parse trees (#REF; #REF) , other methods have been proposed that eschew the need for a full parse (CoNNL, 2004; CoNLL, 2005) .",
                "Because of the way the problem has been formulated -as a pipeline of parsing (or chunking) feeding into labelling -specific investigations of integrated approaches that solve both the parsing and the semantic role labelling problems at the same time have not been studied.",
                "We present work to test the hypothesis that a current statistical parser #TARGET_REF can output richer information robustly, that is without any significant degradation of the parser's accuracy on the original parsing task, by explicitly modelling semantic role labels as the interface between syntax and semantics.",
                "We achieve promising results both on the simple parsing task, where the accuracy of the parser is measured on the standard Parseval measures, and also on the parsing task where the more complex labels of PropBank are taken into account.",
                "We will call the former task Penn Treebank parsing (PTB parsing) and the latter task PropBank parsing below."
            ],
            "label": [
                "USE",
                "EXTENDS"
            ]
        },
        "input": "sent0: While some of these models are based on full parse trees (#REF; #REF) , other methods have been proposed that eschew the need for a full parse (CoNNL, 2004; CoNLL, 2005) .\n sent1: Because of the way the problem has been formulated -as a pipeline of parsing (or chunking) feeding into labelling -specific investigations of integrated approaches that solve both the parsing and the semantic role labelling problems at the same time have not been studied.\n sent2: We present work to test the hypothesis that a current statistical parser #TARGET_REF can output richer information robustly, that is without any significant degradation of the parser's accuracy on the original parsing task, by explicitly modelling semantic role labels as the interface between syntax and semantics.\n sent3: We achieve promising results both on the simple parsing task, where the accuracy of the parser is measured on the standard Parseval measures, and also on the parsing task where the more complex labels of PropBank are taken into account.\n sent4: We will call the former task Penn Treebank parsing (PTB parsing) and the latter task PropBank parsing below.\n",
        "output": "{\"label\": [\"USE\", \"EXTENDS\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "To achieve the complex task of assigning semantic role labels while parsing, we use a family of statistical parsers, the Simple Synchrony Network (SSN) parsers #TARGET_REF , which do not make any explicit independence assumptions, and are therefore likely to adapt without much modification to the current problem.",
                "This architecture has shown state-of-the-art performance.",
                "SSN parsers comprise two components, one which estimates the parameters of a stochastic model for syntactic trees, and one which searches for the most probable syntactic tree given the parameter estimates.",
                "As with many other statistical parsers (#REF; #REF) , SSN parsers use a history-based model of parsing.",
                "Events in such a model are derivation moves."
            ],
            "label": [
                "USE",
                "MOTIVATION"
            ]
        },
        "input": "sent0: To achieve the complex task of assigning semantic role labels while parsing, we use a family of statistical parsers, the Simple Synchrony Network (SSN) parsers #TARGET_REF , which do not make any explicit independence assumptions, and are therefore likely to adapt without much modification to the current problem.\n sent1: This architecture has shown state-of-the-art performance.\n sent2: SSN parsers comprise two components, one which estimates the parameters of a stochastic model for syntactic trees, and one which searches for the most probable syntactic tree given the parameter estimates.\n sent3: As with many other statistical parsers (#REF; #REF) , SSN parsers use a history-based model of parsing.\n sent4: Events in such a model are derivation moves.\n",
        "output": "{\"label\": [\"USE\", \"MOTIVATION\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "1 According to these criteria, we came up with the following four data sets covering three typologically diverse languages (exemplary entries in Table 1) .",
                "SE07: The test set of #REF Task 14 #TARGET_REF comprises 1000 English news headlines which are annotated according to six Basic Emotions, joy, anger, sadness, fear, disgust, and surprise on a [0; 100]-scale (BE6 annotation format).",
                "ANET: The Affective Norms for English Text (#REF) are an adaptation of the popular lexical database ANEW (#REF ) to short texts.",
                "The corpus comprises 120 situation description which are annotated according to Valence, Arousal, and Dominance on a 9-point scale (VAD annotation format).",
                "ANPST and MAS: The Affective Norms of Polish Short Texts (#REF) ) and the Minho Affective Sentences (#REF) can be seen as loose adaptations of ANET, very similar in methodology, but different in size and linguistic characteristics (see Table 1 )."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: 1 According to these criteria, we came up with the following four data sets covering three typologically diverse languages (exemplary entries in Table 1) .\n sent1: SE07: The test set of #REF Task 14 #TARGET_REF comprises 1000 English news headlines which are annotated according to six Basic Emotions, joy, anger, sadness, fear, disgust, and surprise on a [0; 100]-scale (BE6 annotation format).\n sent2: ANET: The Affective Norms for English Text (#REF) are an adaptation of the popular lexical database ANEW (#REF ) to short texts.\n sent3: The corpus comprises 120 situation description which are annotated according to Valence, Arousal, and Dominance on a 9-point scale (VAD annotation format).\n sent4: ANPST and MAS: The Affective Norms of Polish Short Texts (#REF) ) and the Minho Affective Sentences (#REF) can be seen as loose adaptations of ANET, very similar in methodology, but different in size and linguistic characteristics (see Table 1 ).\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "In line with that, we found in a supplemental experiment that not using pre-trained embeddings but instead learning them during training significantly reduces performance, e.g., by over 15%-points for the GRU on SE07.",
                "We now compare our best performing model against previously reported results for the SE07 corpus.",
                "Table 5 provides the performance of the winning system of the original shared task (WIN-NER; #REF ), the IAA as reported by the organizers #TARGET_REF , the performance by #REF , the highest one reported for this data set so far (BECK), as well as the results of our GRU from the 10×10-CV.",
                "As can be seen, the GRU established a new state-of-the-art result and even achieves superhuman performance.",
                "This may sound improbable at first glance."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In line with that, we found in a supplemental experiment that not using pre-trained embeddings but instead learning them during training significantly reduces performance, e.g., by over 15%-points for the GRU on SE07.\n sent1: We now compare our best performing model against previously reported results for the SE07 corpus.\n sent2: Table 5 provides the performance of the winning system of the original shared task (WIN-NER; #REF ), the IAA as reported by the organizers #TARGET_REF , the performance by #REF , the highest one reported for this data set so far (BECK), as well as the results of our GRU from the 10×10-CV.\n sent3: As can be seen, the GRU established a new state-of-the-art result and even achieves superhuman performance.\n sent4: This may sound improbable at first glance.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "CNN-LSTM on four topologically diverse data sets of sizes ranging between 1000 and only 120 instances.",
                "Counterintuitively, we found that all DL approaches performed well under every experimental condition.",
                "Our proposed GRU model even established a novel state-of-the-art result on the #REF test set #TARGET_REF outperforming human reliability.",
                "Moreover, it has been frequently argued that pre-trained word embeddings do not comprise sufficient affective information to be used verbatim in emotion analysis.",
                "We here provided evidence that in actuality the opposite holds-high-quality pre-trained word embeddings are instrumental in achieving strong results in low-resource scenarios and largely boost performance independent of model type."
            ],
            "label": [
                "USE",
                "DIFFERENCES"
            ]
        },
        "input": "sent0: CNN-LSTM on four topologically diverse data sets of sizes ranging between 1000 and only 120 instances.\n sent1: Counterintuitively, we found that all DL approaches performed well under every experimental condition.\n sent2: Our proposed GRU model even established a novel state-of-the-art result on the #REF test set #TARGET_REF outperforming human reliability.\n sent3: Moreover, it has been frequently argued that pre-trained word embeddings do not comprise sufficient affective information to be used verbatim in emotion analysis.\n sent4: We here provided evidence that in actuality the opposite holds-high-quality pre-trained word embeddings are instrumental in achieving strong results in low-resource scenarios and largely boost performance independent of model type.\n",
        "output": "{\"label\": [\"USE\", \"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Table 5 provides the performance of the winning system of the original shared task (WIN-NER; #REF ), the IAA as reported by the organizers (#REF) , the performance by #REF , the highest one reported for this data set so far (BECK), as well as the results of our GRU from the 10×10-CV.",
                "As can be seen, the GRU established a new state-of-the-art result and even achieves superhuman performance.",
                "This may sound improbable at first glance.",
                "However, #TARGET_REF employ a rather weak notion of human performance which is-broadly speaking-based on the reliability of a single human rater.",
                "5 Interestingly, the GRU shows particularly large improvements over human performance for categories where the IAA is low (anger, disgust, and surprise) which might be an effect of the additional supervision introduced by multi-task learning."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Table 5 provides the performance of the winning system of the original shared task (WIN-NER; #REF ), the IAA as reported by the organizers (#REF) , the performance by #REF , the highest one reported for this data set so far (BECK), as well as the results of our GRU from the 10×10-CV.\n sent1: As can be seen, the GRU established a new state-of-the-art result and even achieves superhuman performance.\n sent2: This may sound improbable at first glance.\n sent3: However, #TARGET_REF employ a rather weak notion of human performance which is-broadly speaking-based on the reliability of a single human rater.\n sent4: 5 Interestingly, the GRU shows particularly large improvements over human performance for categories where the IAA is low (anger, disgust, and surprise) which might be an effect of the additional supervision introduced by multi-task learning.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent1\", \"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Words and phrases that may directly mark the structure of a discourse have been termed CUE PttR.ASES, CLUE WORDS, DISCOURSE MAI:tKERS~ arid DISCOURSE PARTICLES [3, #TARGET_REF 14, 17, 19] .",
                "Some exarnpies are 'now', which marks the introduction of a new subtopic or return to a previous one, 'incidentally' and 'by the way', which indicate the beginning of a digression, and 'anyway' and 'in any case', which indicate return from a digression.",
                "In a previous study [8] , we noted that such terms are potentially ambiguous between DISCOURSE and SENTENTIAL uses [18] .",
                "So, 'now' may be used as a temporal adverbial as well as a discourse marker, 'incidentally' may also function as an adverbial, and other cue phrases similarly have one or more senses in addition to their function as markers of discourse structure.",
                "Based upon an empiricM study of 'now' in recorded speech, we proposed that such discourse and sentential uses of cue phrases can be disambiguated intonationally."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Words and phrases that may directly mark the structure of a discourse have been termed CUE PttR.ASES, CLUE WORDS, DISCOURSE MAI:tKERS~ arid DISCOURSE PARTICLES [3, #TARGET_REF 14, 17, 19] .\n sent1: Some exarnpies are 'now', which marks the introduction of a new subtopic or return to a previous one, 'incidentally' and 'by the way', which indicate the beginning of a digression, and 'anyway' and 'in any case', which indicate return from a digression.\n sent2: In a previous study [8] , we noted that such terms are potentially ambiguous between DISCOURSE and SENTENTIAL uses [18] .\n sent3: So, 'now' may be used as a temporal adverbial as well as a discourse marker, 'incidentally' may also function as an adverbial, and other cue phrases similarly have one or more senses in addition to their function as markers of discourse structure.\n sent4: Based upon an empiricM study of 'now' in recorded speech, we proposed that such discourse and sentential uses of cue phrases can be disambiguated intonationally.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "The important role that cue phrases play in understanding and generating discourse has been well documented in the computational linguistics literature.",
                "For example, by indicating the presence of a structural boundary or a relationship between parts of a discourse, cue phrases caa assist in the resolution of anaphora [5, #TARGET_REF 17] and in the identification of rhetorical relations [10, 12, 17] .",
                "Cue phrases have also been used to reduce the complexity of discourse processing and to increase textual coherence [3, 11, 21] .",
                "In Example (1) 1, interpretation of the anaphor 'it' as (correctly) co-indexed with THE SYSTEM is facilitated by the presence of the cue phrases 'say' and 'then', marking potential antecedents in '... as an EXPERT DATABASE for AN EXPERT SYSTEM ...' as structurally unavailable.",
                "2"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The important role that cue phrases play in understanding and generating discourse has been well documented in the computational linguistics literature.\n sent1: For example, by indicating the presence of a structural boundary or a relationship between parts of a discourse, cue phrases caa assist in the resolution of anaphora [5, #TARGET_REF 17] and in the identification of rhetorical relations [10, 12, 17] .\n sent2: Cue phrases have also been used to reduce the complexity of discourse processing and to increase textual coherence [3, 11, 21] .\n sent3: In Example (1) 1, interpretation of the anaphor 'it' as (correctly) co-indexed with THE SYSTEM is facilitated by the presence of the cue phrases 'say' and 'then', marking potential antecedents in '... as an EXPERT DATABASE for AN EXPERT SYSTEM ...' as structurally unavailable.\n sent4: 2\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "For example, #REF uses a taxonomy of connectives based on that of #REF to associate with each class of cue phrases a semantic function with respect to a model of argument understanding.",
                "Grosz and Sidner #TARGET_REF classify cue phrases based on changes to the attentional stack and intentional structure found in their theory of discourse.",
                "#REF classifies cue phrases into groups based on their sentential usage (e.g. conjunctive, adverbial, and clausal markers), while #REF and #REF associate groups of cue phrases with the rhetorical relationships they signal.",
                "Finally, #REF presents a taxonomy of cue phrases based on three functions relevant to her work in language generation: knowledge organization, knowledge acquisition, and affect maintenance.",
                "Once a cue phrase has been identified, however, it is not always clear whether to interpret it as a discourse marker or not [6, 4, 8, 18] ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For example, #REF uses a taxonomy of connectives based on that of #REF to associate with each class of cue phrases a semantic function with respect to a model of argument understanding.\n sent1: Grosz and Sidner #TARGET_REF classify cue phrases based on changes to the attentional stack and intentional structure found in their theory of discourse.\n sent2: #REF classifies cue phrases into groups based on their sentential usage (e.g. conjunctive, adverbial, and clausal markers), while #REF and #REF associate groups of cue phrases with the rhetorical relationships they signal.\n sent3: Finally, #REF presents a taxonomy of cue phrases based on three functions relevant to her work in language generation: knowledge organization, knowledge acquisition, and affect maintenance.\n sent4: Once a cue phrase has been identified, however, it is not always clear whether to interpret it as a discourse marker or not [6, 4, 8, 18] .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Finally, #REF presents a taxonomy of cue phrases based on three functions relevant to her work in language generation: knowledge organization, knowledge acquisition, and affect maintenance.",
                "Once a cue phrase has been identified, however, it is not always clear whether to interpret it as a discourse marker or not [6, #TARGET_REF 8, 18] .",
                "The texts in Exampie (2) are potentially ambiguous between a temporal reading of 'now' and a discourse interpretation:",
                "\"Now in AI our approach is to look at a knowledge base as a set of symbolic items that represent something.\"",
                "b. \"Now some of you may suspect from the title of this talk that this word is coming to you from Krypton or some other possible world.\""
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Finally, #REF presents a taxonomy of cue phrases based on three functions relevant to her work in language generation: knowledge organization, knowledge acquisition, and affect maintenance.\n sent1: Once a cue phrase has been identified, however, it is not always clear whether to interpret it as a discourse marker or not [6, #TARGET_REF 8, 18] .\n sent2: The texts in Exampie (2) are potentially ambiguous between a temporal reading of 'now' and a discourse interpretation:\n sent3: \"Now in AI our approach is to look at a knowledge base as a set of symbolic items that represent something.\"\n sent4: b. \"Now some of you may suspect from the title of this talk that this word is coming to you from Krypton or some other possible world.\"\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "In future work we plan to develop additional models for discourse a~nL(l aententiel uses of multl-word cue phrases, e.g 'that reminds me', 'first o] all', 'speaking off and so on.",
                "8Our set of cue phrases was derived from extensional definitions provided by ourselves and othel~ [3, #TARGET_REF 17, 18, 21] .",
                "Tim following lexicel items, although also cue phrases, are not present in the portion of the axlch-ess examined to date: 9The address was transcribed independently of our study by a meraber of the text processing pool at AT&T Bell Laboratories.",
                "We found that 20 cite phrases had been omitted by the traalscriber: 'and', 'now', 'ok', 'so', and 'well'.",
                "Significantly, ell but two of these were termed 'discourse' uses by In comparing our judgments, we were interested in areas of disagreement as well as agreement."
            ],
            "label": [
                "DIFFERENCES",
                "EXTENDS"
            ]
        },
        "input": "sent0: In future work we plan to develop additional models for discourse a~nL(l aententiel uses of multl-word cue phrases, e.g 'that reminds me', 'first o] all', 'speaking off and so on.\n sent1: 8Our set of cue phrases was derived from extensional definitions provided by ourselves and othel~ [3, #TARGET_REF 17, 18, 21] .\n sent2: Tim following lexicel items, although also cue phrases, are not present in the portion of the axlch-ess examined to date: 9The address was transcribed independently of our study by a meraber of the text processing pool at AT&T Bell Laboratories.\n sent3: We found that 20 cite phrases had been omitted by the traalscriber: 'and', 'now', 'ok', 'so', and 'well'.\n sent4: Significantly, ell but two of these were termed 'discourse' uses by In comparing our judgments, we were interested in areas of disagreement as well as agreement.\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"EXTENDS\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Explicit user geolocation metadata (e.g. GPS tags, WiFi footprint, IP address) is not usually available to third-party consumers, giving rise to the need for geolocation based on profile data, text content, friendship graphs (#REF) or some combination of these (#REFb,a) .",
                "The strong geographical bias, most obviously at the language level (e.g. Finland vs. Japan), and more subtly at the dialect level (e.g. in English used in north-west England vs. north-east USA vs. Texas, USA), clearly reflected in language use in social media services such as Twitter, has been used extensively either for geolocation of users #TARGET_REF; #REF; #REF; #REF) or dialectology #REF) .",
                "In these methods, a user is often represented by the concatenation of their tweets, and the geolocation model is trained on a very small percentage of explicitly geotagged tweets, noting the potential biases implicit in geotagged tweets (#REF) .",
                "Lexical dialectology is (in part) the converse of user geolocation (#REF) : given text associated with a variety of regions, the task is to identify terms that are distinctive of particular regions.",
                "The complexity of the task is two-fold: (1) localised named entities (e.g. sporting team names) are not of interest; and (2) without semantic knowledge it is difficult to detect terms that are in general use but have a special meaning in a region."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Explicit user geolocation metadata (e.g. GPS tags, WiFi footprint, IP address) is not usually available to third-party consumers, giving rise to the need for geolocation based on profile data, text content, friendship graphs (#REF) or some combination of these (#REFb,a) .\n sent1: The strong geographical bias, most obviously at the language level (e.g. Finland vs. Japan), and more subtly at the dialect level (e.g. in English used in north-west England vs. north-east USA vs. Texas, USA), clearly reflected in language use in social media services such as Twitter, has been used extensively either for geolocation of users #TARGET_REF; #REF; #REF; #REF) or dialectology #REF) .\n sent2: In these methods, a user is often represented by the concatenation of their tweets, and the geolocation model is trained on a very small percentage of explicitly geotagged tweets, noting the potential biases implicit in geotagged tweets (#REF) .\n sent3: Lexical dialectology is (in part) the converse of user geolocation (#REF) : given text associated with a variety of regions, the task is to identify terms that are distinctive of particular regions.\n sent4: The complexity of the task is two-fold: (1) localised named entities (e.g. sporting team names) are not of interest; and (2) without semantic knowledge it is difficult to detect terms that are in general use but have a special meaning in a region.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Related work on Twitter user geolocation falls into two categories: text-based and network-based methods.",
                "Text-based methods make use of the geographical biases of language use, and networkbased methods rely on the geospatial homophily of user-user interactions.",
                "In both cases, the assumption is that users who live in the same geographic area share similar features (linguistic or interactional).",
                "Three main text-based approaches are: (1) the use of gazetteers #REF) ; (2) unsupervised text clustering based on topic models or similar #TARGET_REF; #REF; #REF) ; and (3) supervised classification (#REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF) , which unlike gazetteers can be applied to informal text and compared to topic models, scales better.",
                "The classification models often rely on less than 1% of geotagged tweets for supervision and discretise real-valued coordinates into equalsized grids (#REF ), administrative regions (#REF; #REF; #REF; #REF , or flat (#REF) or hierarchical k-d tree clusters (#REF) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Related work on Twitter user geolocation falls into two categories: text-based and network-based methods.\n sent1: Text-based methods make use of the geographical biases of language use, and networkbased methods rely on the geospatial homophily of user-user interactions.\n sent2: In both cases, the assumption is that users who live in the same geographic area share similar features (linguistic or interactional).\n sent3: Three main text-based approaches are: (1) the use of gazetteers #REF) ; (2) unsupervised text clustering based on topic models or similar #TARGET_REF; #REF; #REF) ; and (3) supervised classification (#REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF) , which unlike gazetteers can be applied to informal text and compared to topic models, scales better.\n sent4: The classification models often rely on less than 1% of geotagged tweets for supervision and discretise real-valued coordinates into equalsized grids (#REF ), administrative regions (#REF; #REF; #REF; #REF , or flat (#REF) or hierarchical k-d tree clusters (#REF) .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Dialect is a variety of language shared by a group of speakers (#REF) .",
                "Our focus here is on geographical dialects which are spoken (and written in social media) by people from particular areas.",
                "The traditional approach to dialectology is to find the geographical distribution of known lexical alternatives (e.g. you, yall and yinz: (#REF; #REF; Gonçalves and Sánchez, 2014; #REF; #REF; #REF) ), the shortcoming of which is that the alternative lexical variables must be known beforehand.",
                "There have also been attempts to automatically identify such words from geotagged documents #TARGET_REF; #REF; #REF) .",
                "The main idea is to find lexical variables that are disproportionately distributed in different locations either via model-based or statistical methods (#REF) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Dialect is a variety of language shared by a group of speakers (#REF) .\n sent1: Our focus here is on geographical dialects which are spoken (and written in social media) by people from particular areas.\n sent2: The traditional approach to dialectology is to find the geographical distribution of known lexical alternatives (e.g. you, yall and yinz: (#REF; #REF; Gonçalves and Sánchez, 2014; #REF; #REF; #REF) ), the shortcoming of which is that the alternative lexical variables must be known beforehand.\n sent3: There have also been attempts to automatically identify such words from geotagged documents #TARGET_REF; #REF; #REF) .\n sent4: The main idea is to find lexical variables that are disproportionately distributed in different locations either via model-based or statistical methods (#REF) .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "We use three existing Twitter user geolocation datasets: (1) GEOTEXT #TARGET_REF , (2) TWITTER-US (#REF) , and (3) TWITTER-WORLD (#REF) .",
                "These datasets have been used widely for training and evaluation of geolocation models.",
                "They are all prepartitioned into training, development and test sets.",
                "Each user is represented by the concatenation of their tweets, and labeled with the latitude/longitude of the first collected geotagged tweet in the case of GEOTEXT and TWITTER-US, and the centre of the closest city in the case of TWITTER-WORLD.",
                "1 GEOTEXT and TWITTER-US cover the continental US, and TWITTER-WORLD covers the whole world, with 9k, 449k and 1.3m users, respectively as shown in Figure 1 ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We use three existing Twitter user geolocation datasets: (1) GEOTEXT #TARGET_REF , (2) TWITTER-US (#REF) , and (3) TWITTER-WORLD (#REF) .\n sent1: These datasets have been used widely for training and evaluation of geolocation models.\n sent2: They are all prepartitioned into training, development and test sets.\n sent3: Each user is represented by the concatenation of their tweets, and labeled with the latitude/longitude of the first collected geotagged tweet in the case of GEOTEXT and TWITTER-US, and the centre of the closest city in the case of TWITTER-WORLD.\n sent4: 1 GEOTEXT and TWITTER-US cover the continental US, and TWITTER-WORLD covers the whole world, with 9k, 449k and 1.3m users, respectively as shown in Figure 1 .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "The number of regions, regularisation strength, hidden layer and mini-batch size are tuned over development data and set to (32, 10 −5 , 896, 100), (256, 10 −6 , 2048, 10000) and (930, 10 −6 , 3720, 10000) for GEOTEXT, TWITTER-US and TWITTER-WORLD, respectively.",
                "The parameters are optimised using Adamx (#REF) using Lasagne/Theano (Theano Development #REF) .",
                "Following #REF and #TARGET_REF , we evaluated the geolocation model using mean and median error in km (\"Mean\" and \"Median\" resp.) and accuracy within 161km of the actual location (\"Acc@161\").",
                "Note that lower numbers are better for Mean and Median, and higher numbers better for Acc@161.",
                "4 The results reported in Rahimi et al. (2015b; 2015a) for TWITTER-WORLD were over a superset of the dataset; the results reported here are based on the actual dataset."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The number of regions, regularisation strength, hidden layer and mini-batch size are tuned over development data and set to (32, 10 −5 , 896, 100), (256, 10 −6 , 2048, 10000) and (930, 10 −6 , 3720, 10000) for GEOTEXT, TWITTER-US and TWITTER-WORLD, respectively.\n sent1: The parameters are optimised using Adamx (#REF) using Lasagne/Theano (Theano Development #REF) .\n sent2: Following #REF and #TARGET_REF , we evaluated the geolocation model using mean and median error in km (\"Mean\" and \"Median\" resp.) and accuracy within 161km of the actual location (\"Acc@161\").\n sent3: Note that lower numbers are better for Mean and Median, and higher numbers better for Acc@161.\n sent4: 4 The results reported in Rahimi et al. (2015b; 2015a) for TWITTER-WORLD were over a superset of the dataset; the results reported here are based on the actual dataset.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "* The authors contributed equally Originally introduced for images, GANs leverage a discriminator, which is trained to discriminate between real images and generated images via an adversarial loss.",
                "In such a framework, the generator is not directly exposed to the ground truth data, but instead learns to imitate it using global feedback from the discriminator.",
                "This has led to several attempts to use GANs for text generation, with a generator using either a recurrent neural network (RNN) #REF; #TARGET_REF; #REF) , or a Convolutional Neural Network (CNN) (#REF; #REF) .",
                "However, evaluating GANs is more difficult than evaluating LMs.",
                "While in language modeling, evaluation is based on the log-probability of a model on held-out text, this cannot be straightforwardly extended to GAN-based text generation, because the generator outputs discrete tokens, rather than a probability distribution."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: * The authors contributed equally Originally introduced for images, GANs leverage a discriminator, which is trained to discriminate between real images and generated images via an adversarial loss.\n sent1: In such a framework, the generator is not directly exposed to the ground truth data, but instead learns to imitate it using global feedback from the discriminator.\n sent2: This has led to several attempts to use GANs for text generation, with a generator using either a recurrent neural network (RNN) #REF; #TARGET_REF; #REF) , or a Convolutional Neural Network (CNN) (#REF; #REF) .\n sent3: However, evaluating GANs is more difficult than evaluating LMs.\n sent4: While in language modeling, evaluation is based on the log-probability of a model on held-out text, this cannot be straightforwardly extended to GAN-based text generation, because the generator outputs discrete tokens, rather than a probability distribution.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "A main challenge in applying GANs for text is that generating discrete symbols is a nondifferentiable operation.",
                "One solution is to perform a continuous relaxation of the GAN output, which leads to generators that emit a nearly discrete continuous distribution #TARGET_REF .",
                "This keeps the model differentiable and enables end-to-end training through the discriminator.",
                "Alternatively, SeqGAN and Leak-GAN (#REF) used policy gradient methods to overcome the differentiablity requirement.",
                "We apply our approximation to both model types."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: A main challenge in applying GANs for text is that generating discrete symbols is a nondifferentiable operation.\n sent1: One solution is to perform a continuous relaxation of the GAN output, which leads to generators that emit a nearly discrete continuous distribution #TARGET_REF .\n sent2: This keeps the model differentiable and enables end-to-end training through the discriminator.\n sent3: Alternatively, SeqGAN and Leak-GAN (#REF) used policy gradient methods to overcome the differentiablity requirement.\n sent4: We apply our approximation to both model types.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "As a result, LM metrics cannot be applied to evaluate the generated text.",
                "Consequently, other metrics have been proposed:",
                "• N-gram overlap: #TARGET_REF : Inspired by BLEU (#REF) , this measures whether n-grams generated by the model appear in a held-out corpus.",
                "A major drawback is that this metric favors conservative models that always generate very common text (e.g., \"it is\").",
                "To mitigate this, self-BLEU has been proposed (#REF) as an additional metric, where overlap is measured between two independently sampled texts from the model."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: As a result, LM metrics cannot be applied to evaluate the generated text.\n sent1: Consequently, other metrics have been proposed:\n sent2: • N-gram overlap: #TARGET_REF : Inspired by BLEU (#REF) , this measures whether n-grams generated by the model appear in a held-out corpus.\n sent3: A major drawback is that this metric favors conservative models that always generate very common text (e.g., \"it is\").\n sent4: To mitigate this, self-BLEU has been proposed (#REF) as an additional metric, where overlap is measured between two independently sampled texts from the model.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The inputs to an RNN at time step t, are the state vector h t and the current input token x t .",
                "The output token (one-hot) is denoted by o t .",
                "In RNNbased GANs, the previous output token is used at inference time as the input x t #REF; #TARGET_REF; #REF) .",
                "In contrast, when evaluating with BPC or perplexity, the gold token x t is given as input.",
                "Hence, LM-based evaluation neutralizes the problem of exposure bias addressed by GANs."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The inputs to an RNN at time step t, are the state vector h t and the current input token x t .\n sent1: The output token (one-hot) is denoted by o t .\n sent2: In RNNbased GANs, the previous output token is used at inference time as the input x t #REF; #TARGET_REF; #REF) .\n sent3: In contrast, when evaluating with BPC or perplexity, the gold token x t is given as input.\n sent4: Hence, LM-based evaluation neutralizes the problem of exposure bias addressed by GANs.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We tuned hyper-parameters on the validation set, including sequence length to generate at test time (7 for #TARGET_REF , 1000 for ).",
                "We chose the number of samples N empirically for each model, as described in Section 4.2.",
                "We set α to 10, and the boundary to γ = 10 −3 as a good trade-off between accuracy and run-time.",
                "Figure 2 plots the approximate error G t,N −α −G t,N ∞ as a function of N .",
                "For both models, N > 1600 satisfies this condition (red line in Figure 2 )."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: We tuned hyper-parameters on the validation set, including sequence length to generate at test time (7 for #TARGET_REF , 1000 for ).\n sent1: We chose the number of samples N empirically for each model, as described in Section 4.2.\n sent2: We set α to 10, and the boundary to γ = 10 −3 as a good trade-off between accuracy and run-time.\n sent3: Figure 2 plots the approximate error G t,N −α −G t,N ∞ as a function of N .\n sent4: For both models, N > 1600 satisfies this condition (red line in Figure 2 ).\n",
        "output": "{\"label\": [null], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "GAN-based models perform worse than stateof-the-art LMs by a large margin.",
                "Moreover, in SeqGAN, the pre-trained LM performs better than the fully trained model with approximate BPC scores of 1.95 and 2.06, respectively, and the BPC deteriorates as adversarial training continues.",
                "Finally, we note that generating sequences larger than 7 characters hurts the BPC of #TARGET_REF .",
                "It is difficult to assess the quality of generation with such short sequences.",
                "In Table 2 we present a few randomly generated samples from each model."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: GAN-based models perform worse than stateof-the-art LMs by a large margin.\n sent1: Moreover, in SeqGAN, the pre-trained LM performs better than the fully trained model with approximate BPC scores of 1.95 and 2.06, respectively, and the BPC deteriorates as adversarial training continues.\n sent2: Finally, we note that generating sequences larger than 7 characters hurts the BPC of #TARGET_REF .\n sent3: It is difficult to assess the quality of generation with such short sequences.\n sent4: In Table 2 we present a few randomly generated samples from each model.\n",
        "output": "{\"label\": [null], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Moreover, in SeqGAN, the pre-trained LM performs better than the fully trained model with approximate BPC scores of 1.95 and 2.06, respectively, and the BPC deteriorates as adversarial training continues.",
                "Finally, we note that generating sequences larger than 7 characters hurts the BPC of #REF .",
                "It is difficult to assess the quality of generation with such short sequences.",
                "In Table 2 we present a few randomly generated samples from each model.",
                "We indeed observe that adversarial training slightly reduces the quality of generated text for SeqGAN, and find that the quality of 100-character long sequences generated from #TARGET_REF is low."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: Moreover, in SeqGAN, the pre-trained LM performs better than the fully trained model with approximate BPC scores of 1.95 and 2.06, respectively, and the BPC deteriorates as adversarial training continues.\n sent1: Finally, we note that generating sequences larger than 7 characters hurts the BPC of #REF .\n sent2: It is difficult to assess the quality of generation with such short sequences.\n sent3: In Table 2 we present a few randomly generated samples from each model.\n sent4: We indeed observe that adversarial training slightly reduces the quality of generated text for SeqGAN, and find that the quality of 100-character long sequences generated from #TARGET_REF is low.\n",
        "output": "{\"label\": [null], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "The rest of the corpus was automatically validated synthetic material using general data from Leipzig (#REF Engine customization The data was cleaned using the Bicleaner tool (Sánchez-#REF) .",
                "The data was lowercased and extra embeddings were added in order to keep the case information.",
                "The tokenization used was the one provided by OpenNMT 3 and words were divided in subwords according to the BPE #TARGET_REF approach.",
                "The models were trained with multi-domain data and we improved performance following a domainmixing approach (#REF) .",
                "The domain information was prepended with special tokens for each target sequence."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The rest of the corpus was automatically validated synthetic material using general data from Leipzig (#REF Engine customization The data was cleaned using the Bicleaner tool (Sánchez-#REF) .\n sent1: The data was lowercased and extra embeddings were added in order to keep the case information.\n sent2: The tokenization used was the one provided by OpenNMT 3 and words were divided in subwords according to the BPE #TARGET_REF approach.\n sent3: The models were trained with multi-domain data and we improved performance following a domainmixing approach (#REF) .\n sent4: The domain information was prepended with special tokens for each target sequence.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The main part of the training corpora (approximately 75%) was part of Pangeanic's own repository harvested through web crawling and also OpenSubtitles (#REF) .",
                "The rest of the corpus was automatically validated synthetic material using general data from Leipzig (#REF) .",
                "Engine customization The data was cleaned using the Bicleaner tool (Sánchez-#REF) .",
                "The data was lowercased and extra embeddings were added in order to keep the case information.",
                "The tokenization used was the one provided by OpenNMT 3 and words were divided in subwords according to the BPE #TARGET_REF approach."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The main part of the training corpora (approximately 75%) was part of Pangeanic's own repository harvested through web crawling and also OpenSubtitles (#REF) .\n sent1: The rest of the corpus was automatically validated synthetic material using general data from Leipzig (#REF) .\n sent2: Engine customization The data was cleaned using the Bicleaner tool (Sánchez-#REF) .\n sent3: The data was lowercased and extra embeddings were added in order to keep the case information.\n sent4: The tokenization used was the one provided by OpenNMT 3 and words were divided in subwords according to the BPE #TARGET_REF approach.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Dependency parsing is a topic that has engendered increasing interest in recent years.",
                "One promising approach is based on exact search and structural learning #TARGET_REF; #REF) .",
                "In this work we also pursue this approach.",
                "Our system makes no provisions for non-projective edges.",
                "In contrast to previous work, we aim to learn labelled dependency trees at one fell swoop."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Dependency parsing is a topic that has engendered increasing interest in recent years.\n sent1: One promising approach is based on exact search and structural learning #TARGET_REF; #REF) .\n sent2: In this work we also pursue this approach.\n sent3: Our system makes no provisions for non-projective edges.\n sent4: In contrast to previous work, we aim to learn labelled dependency trees at one fell swoop.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "In contrast to previous work, we aim to learn labelled dependency trees at one fell swoop.",
                "This is done by maintaining several copies of feature vectors that capture the features' impact on predicting different dependency relations (deprels).",
                "In order to preserve the strength of #TARGET_REF 's approach in terms of unlabelled attachment score, we add feature vectors for generalizations over deprels.",
                "We also employ various reversible transformations to reach treebank formats that better match our feature representation and that reduce the complexity of the learning task.",
                "The paper first presents the methodology used, goes on to describing experiments and results and finally concludes."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: In contrast to previous work, we aim to learn labelled dependency trees at one fell swoop.\n sent1: This is done by maintaining several copies of feature vectors that capture the features' impact on predicting different dependency relations (deprels).\n sent2: In order to preserve the strength of #TARGET_REF 's approach in terms of unlabelled attachment score, we add feature vectors for generalizations over deprels.\n sent3: We also employ various reversible transformations to reach treebank formats that better match our feature representation and that reduce the complexity of the learning task.\n sent4: The paper first presents the methodology used, goes on to describing experiments and results and finally concludes.\n",
        "output": "{\"label\": [null], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "In our approach, we adopt #REF 's bottomup chart-parsing algorithm in #TARGET_REF 's formulation, which finds the best projective dependency tree for an input string",
                "We assume that every possible headdependent pair ¨ is described by a feature vector",
                "Eisner's algorithm achieves optimal tree packing by storing partial structures in two matrices and .",
                "First the diagonals of the matrices are initiated with 0; then all other cells are filled according to eqs. (1) and (2) and their symmetric variants."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: In our approach, we adopt #REF 's bottomup chart-parsing algorithm in #TARGET_REF 's formulation, which finds the best projective dependency tree for an input string\n sent1: We assume that every possible headdependent pair ¨ is described by a feature vector\n sent2: Eisner's algorithm achieves optimal tree packing by storing partial structures in two matrices and .\n sent3: First the diagonals of the matrices are initiated with 0; then all other cells are filled according to eqs. (1) and (2) and their symmetric variants.\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "In deriving features, we used all information given in the treebanks, i.e. words (w), fine-grained POS tags (fp), combinations of lemmas and coarsegrained POS tags (lcp), and whether two tokens agree 1 (agr = yes, no, don't know).",
                "We essentially employ the same set of features as #TARGET_REF :"
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In deriving features, we used all information given in the treebanks, i.e. words (w), fine-grained POS tags (fp), combinations of lemmas and coarsegrained POS tags (lcp), and whether two tokens agree 1 (agr = yes, no, don't know).\n sent1: We essentially employ the same set of features as #TARGET_REF :\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "It tries to change weights as little as possible (passiveness), while ensuring that (1)",
                "1 Agreement was computed from morphological features, viz. gender, number and person, and case.",
                "In languages with subject-verb agreement, we added a nominative case feature to finite verbs.",
                "In Basque, agreement is case-specific (absolutive, dative, ergative, other case).",
                "Having a closed-form solution, OPAL is easier to implement and more efficient than the MIRA algorithm used by #TARGET_REF , although it achieves a performance comparable to MIRA's on many problems (#REF) ."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: It tries to change weights as little as possible (passiveness), while ensuring that (1)\n sent1: 1 Agreement was computed from morphological features, viz. gender, number and person, and case.\n sent2: In languages with subject-verb agreement, we added a nominative case feature to finite verbs.\n sent3: In Basque, agreement is case-specific (absolutive, dative, ergative, other case).\n sent4: Having a closed-form solution, OPAL is easier to implement and more efficient than the MIRA algorithm used by #TARGET_REF , although it achieves a performance comparable to MIRA's on many problems (#REF) .\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "So far, the presented system, which follows closely the approach of #TARGET_REF , only predicts unlabelled dependency trees.",
                "To derive a labeling, we departed from their approach: We split each feature along the deprel label dimension, so that each deprel U is associated with its own feature vector (cf.",
                "eq. (4), where V is the tensor product and",
                "In parsing, we only consider the best deprel label.",
                "On its own, this simple approach led to a severe degradation of performance, so we took a step back by re-introducing features for unlabelled trees."
            ],
            "label": [
                "DIFFERENCES",
                "MOTIVATION"
            ]
        },
        "input": "sent0: So far, the presented system, which follows closely the approach of #TARGET_REF , only predicts unlabelled dependency trees.\n sent1: To derive a labeling, we departed from their approach: We split each feature along the deprel label dimension, so that each deprel U is associated with its own feature vector (cf.\n sent2: eq. (4), where V is the tensor product and\n sent3: In parsing, we only consider the best deprel label.\n sent4: On its own, this simple approach led to a severe degradation of performance, so we took a step back by re-introducing features for unlabelled trees.\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"MOTIVATION\"], \"context\": [\"sent0\", \"sent1\", \"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "As an alternative, we contrast this design with a direct classification approach that relies only on textual examples and effectively combines the dialogue policy with NLU.",
                "In our case study evaluation, we find that this approach offers superior performance, owing to the high frequency of NLU errors in the two step pipeline.",
                "The research presented in this paper extends our previous work.",
                "As we summarize in Section 2, this paper relies on the same data set and evaluation metric as #TARGET_REF , which reports results for learned policies based on maximum entropy models.",
                "In this paper, we add a comparison to a hand-authored policy (Rules) and a new policy based on relevance models (RM)."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: As an alternative, we contrast this design with a direct classification approach that relies only on textual examples and effectively combines the dialogue policy with NLU.\n sent1: In our case study evaluation, we find that this approach offers superior performance, owing to the high frequency of NLU errors in the two step pipeline.\n sent2: The research presented in this paper extends our previous work.\n sent3: As we summarize in Section 2, this paper relies on the same data set and evaluation metric as #TARGET_REF , which reports results for learned policies based on maximum entropy models.\n sent4: In this paper, we add a comparison to a hand-authored policy (Rules) and a new policy based on relevance models (RM).\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "We perform our experiments and evaluation using an existing set of 19 annotated Amani dialogues #TARGET_REF .",
                "The dialogues were collected through teletype-based role play.",
                "Each dialogue turn includes a single user utterance followed by the response chosen by a human role player in the role of Amani.",
                "There are a total of 296 turns, for an average of 15.6 turns/dialogue.",
                "The task of Amani's dialogue manager (DM) is to select the most appropriate system SA to use in response to a user utterance."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: We perform our experiments and evaluation using an existing set of 19 annotated Amani dialogues #TARGET_REF .\n sent1: The dialogues were collected through teletype-based role play.\n sent2: Each dialogue turn includes a single user utterance followed by the response chosen by a human role player in the role of Amani.\n sent3: There are a total of 296 turns, for an average of 15.6 turns/dialogue.\n sent4: The task of Amani's dialogue manager (DM) is to select the most appropriate system SA to use in response to a user utterance.\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "In each fold, we hold out one dialogue and use the remaining 18 dialogues as training data.",
                "To measure the performance of the dialogue policy, we follow the approach of #TARGET_REF , which counts an automatically produced system SA as correct if that SA was chosen by at least one referee for that dialogue turn in the data set.",
                "We then count the proportion of the correct SAs among all the SAs produced across all 19 dialogues, and use this measure of weak accuracy to score competing dialogue policies.",
                "We can use the weak accuracy of one referee, measured against all the others, to establish a performance ceiling for this metric.",
                "(We do not expect that an automatic system would outperform a human referee.) This score is .79; see #REF for discussion."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: In each fold, we hold out one dialogue and use the remaining 18 dialogues as training data.\n sent1: To measure the performance of the dialogue policy, we follow the approach of #TARGET_REF , which counts an automatically produced system SA as correct if that SA was chosen by at least one referee for that dialogue turn in the data set.\n sent2: We then count the proportion of the correct SAs among all the SAs produced across all 19 dialogues, and use this measure of weak accuracy to score competing dialogue policies.\n sent3: We can use the weak accuracy of one referee, measured against all the others, to establish a performance ceiling for this metric.\n sent4: (We do not expect that an automatic system would outperform a human referee.) This score is .79; see #REF for discussion.\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We begin by summarizing our research setting, data set, and evaluation metric.",
                "We refer the reader to #TARGET_REF for additional details.",
                "We use an existing virtual human scenario designed for Tactical Questioning (TACQ) (#REF) , where military personnel interview individuals for information of military value.",
                "TACQ characters are designed to be non-cooperative at times.",
                "They may answer some of the interviewer's questions, but either lie or refuse to answer others until certain conditions are met ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: We begin by summarizing our research setting, data set, and evaluation metric.\n sent1: We refer the reader to #TARGET_REF for additional details.\n sent2: We use an existing virtual human scenario designed for Tactical Questioning (TACQ) (#REF) , where military personnel interview individuals for information of military value.\n sent3: TACQ characters are designed to be non-cooperative at times.\n sent4: They may answer some of the interviewer's questions, but either lie or refuse to answer others until certain conditions are met .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We call the DM's decision process a dialogue policy.",
                "The system builders' intended policy for Amani is detailed in #TARGET_REF .",
                "Because Amani has only a fixed set of system responses, the policy problem looks like a traditional classification task.",
                "However, there are two sources of uncertainty that complicate the task.",
                "Firstly, the mapping between the user's utterance and an appropriate system SA is often one-tomany."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: We call the DM's decision process a dialogue policy.\n sent1: The system builders' intended policy for Amani is detailed in #TARGET_REF .\n sent2: Because Amani has only a fixed set of system responses, the policy problem looks like a traditional classification task.\n sent3: However, there are two sources of uncertainty that complicate the task.\n sent4: Firstly, the mapping between the user's utterance and an appropriate system SA is often one-tomany.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "In our data set, 6 referees independently linked each user utterance to the best system SA response.",
                "In Figure 1 , we provide an example in which three different system SAs were selected by the 6 referees.",
                "In other cases, up to 6 different system SAs were selected #TARGET_REF .",
                "Our first experimental question is therefore: how well can a dialogue policy select an appropriate system SA, if it is provided with an accurate user SA?",
                "Would a statistical classification-based policy perform as well as a rule-based policy?"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In our data set, 6 referees independently linked each user utterance to the best system SA response.\n sent1: In Figure 1 , we provide an example in which three different system SAs were selected by the 6 referees.\n sent2: In other cases, up to 6 different system SAs were selected #TARGET_REF .\n sent3: Our first experimental question is therefore: how well can a dialogue policy select an appropriate system SA, if it is provided with an accurate user SA?\n sent4: Would a statistical classification-based policy perform as well as a rule-based policy?\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "In each fold, we hold out one dialogue and use the remaining 18 dialogues as training data.",
                "To measure the performance of the dialogue policy, we follow the approach of #REF , which counts an automatically produced system SA as correct if that SA was chosen by at least one referee for that dialogue turn in the data set.",
                "We then count the proportion of the correct SAs among all the SAs produced across all 19 dialogues, and use this measure of weak accuracy to score competing dialogue policies.",
                "We can use the weak accuracy of one referee, measured against all the others, to establish a performance ceiling for this metric.",
                "(We do not expect that an automatic system would outperform a human referee.) This score is .79; see #TARGET_REF for discussion."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In each fold, we hold out one dialogue and use the remaining 18 dialogues as training data.\n sent1: To measure the performance of the dialogue policy, we follow the approach of #REF , which counts an automatically produced system SA as correct if that SA was chosen by at least one referee for that dialogue turn in the data set.\n sent2: We then count the proportion of the correct SAs among all the SAs produced across all 19 dialogues, and use this measure of weak accuracy to score competing dialogue policies.\n sent3: We can use the weak accuracy of one referee, measured against all the others, to establish a performance ceiling for this metric.\n sent4: (We do not expect that an automatic system would outperform a human referee.) This score is .79; see #TARGET_REF for discussion.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Note that .71 is an inferior performance to the .79 achieved with G-SA/Rules, indicating that MaxEnt does not learn a policy as effective as the hand-authored Rules, even if it is trained and evaluated on gold SA labels.",
                "As previously reported in #TARGET_REF , a performance of .66 is achieved with the MaxEnt policy when trained on text-based features.",
                "It is interesting to see here, however, that this .66 performance is significantly higher than the .58 that is achieved using Rules together with run-time SAs.",
                "In fact, the accuracy of the NLU-SA labels in this data set, with respect to the gold SAs, is 53%.",
                "Thus, while Rules can achieve very good performance with gold SAs, the high frequency of NLU errors causes a significant degradation in policy performance."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Note that .71 is an inferior performance to the .79 achieved with G-SA/Rules, indicating that MaxEnt does not learn a policy as effective as the hand-authored Rules, even if it is trained and evaluated on gold SA labels.\n sent1: As previously reported in #TARGET_REF , a performance of .66 is achieved with the MaxEnt policy when trained on text-based features.\n sent2: It is interesting to see here, however, that this .66 performance is significantly higher than the .58 that is achieved using Rules together with run-time SAs.\n sent3: In fact, the accuracy of the NLU-SA labels in this data set, with respect to the gold SAs, is 53%.\n sent4: Thus, while Rules can achieve very good performance with gold SAs, the high frequency of NLU errors causes a significant degradation in policy performance.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "In the context of RL the objective function is also called the \"reward\" (#REF) .",
                "Despite its central aspect for RL, quality assurance for objective functions has received little attention so far.",
                "In fact, the reward function is one of the most handcoded aspects in RL (#REF) .",
                "In this paper we propose a new method for meta-evaluation of the objective function.",
                "We bring together two strands of research: one strand uses Reinforcement Learning to automatically optimise dialogue strategies, e.g. (#REF) , (#REF) , (#REFa; #TARGET_REF ; the other other focuses on automatic evaluation of dialogue strategies, e.g. the PARADISE framework (#REF) , and meta-evaluation of dialogue metrics, e.g. (Engelbrecht and Möller, 2007; #REF) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In the context of RL the objective function is also called the \"reward\" (#REF) .\n sent1: Despite its central aspect for RL, quality assurance for objective functions has received little attention so far.\n sent2: In fact, the reward function is one of the most handcoded aspects in RL (#REF) .\n sent3: In this paper we propose a new method for meta-evaluation of the objective function.\n sent4: We bring together two strands of research: one strand uses Reinforcement Learning to automatically optimise dialogue strategies, e.g. (#REF) , (#REF) , (#REFa; #TARGET_REF ; the other other focuses on automatic evaluation of dialogue strategies, e.g. the PARADISE framework (#REF) , and meta-evaluation of dialogue metrics, e.g. (Engelbrecht and Möller, 2007; #REF) .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "The models obtained with PARADISE usually fit the data poorly (Engelbrecht and Möller, 2007) .",
                "It is also not clear how general they are across different systems and user groups (#REF) , (#REF) .",
                "Furthermore, it is not clear how they perform when being used for automatic strategy optimisation within the RL framework.",
                "In the following we evaluate different aspects of an objective function obtained from Wizard-of-Oz (WOZ) data #TARGET_REF .",
                "We proceed as follows: The next Section shortly summarises the overall dialogue system design."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The models obtained with PARADISE usually fit the data poorly (Engelbrecht and Möller, 2007) .\n sent1: It is also not clear how general they are across different systems and user groups (#REF) , (#REF) .\n sent2: Furthermore, it is not clear how they perform when being used for automatic strategy optimisation within the RL framework.\n sent3: In the following we evaluate different aspects of an objective function obtained from Wizard-of-Oz (WOZ) data #TARGET_REF .\n sent4: We proceed as follows: The next Section shortly summarises the overall dialogue system design.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The structure of information seeking dialogues consists of an information acquisition dialogue and an information presentation sub-dialogue (see Figure 1) .",
                "For information acquisition the task of the dialogue policy is to gather 'enough' search constraints from the user, and then, 'at the right time', to start the information presentation phase where the task is to present 'the right amount' of information -either on the screen or listing the items verbally. What this actually means depends on the dialogue context and the preferences of our users as reflected in the objective function.",
                "We therefore formulate dialogue learning as a hierarchical optimisation problem #TARGET_REF .",
                "The applied objective function follows this structure as well.",
                "Figure 1 : Hierarchical dialogue structure for information seeking multimodal systems."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The structure of information seeking dialogues consists of an information acquisition dialogue and an information presentation sub-dialogue (see Figure 1) .\n sent1: For information acquisition the task of the dialogue policy is to gather 'enough' search constraints from the user, and then, 'at the right time', to start the information presentation phase where the task is to present 'the right amount' of information -either on the screen or listing the items verbally. What this actually means depends on the dialogue context and the preferences of our users as reflected in the objective function.\n sent2: We therefore formulate dialogue learning as a hierarchical optimisation problem #TARGET_REF .\n sent3: The applied objective function follows this structure as well.\n sent4: Figure 1 : Hierarchical dialogue structure for information seeking multimodal systems.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We choose Task Ease as the ultimate measure to be optimised following (#REF) 's principle of the least effort which says: \"All things being equal, agents try to minimize their effort in doing what they intend to do\".",
                "The PARADISE regression model is constructed from 3 different corpora: the SAMMIE WOZ experiment (#REF) , and the iTalk system used for the user tests #TARGET_REF running the supervised baseline policy and the RL-based policy.",
                "By replicating the regression model on different data sets we test whether the automatic estimate of Task Ease generalises beyond the conditions and assumptions of a particular experimental design.",
                "The resulting models are shown in Equations 1-3 , where T askEase W OZ is the regression model obtained from the WOZ data, T askEase SL is obtained from the user test data running the supervised policy, and T askEase RL is obtained from the user test data running the RL-based policy.",
                "They all reflect the same trends: longer dialogues (measured in turns) predict a lower Task Ease, whereas a good performance in the multimodal information presentation phase (multimodal score) will positively influence Task Ease."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We choose Task Ease as the ultimate measure to be optimised following (#REF) 's principle of the least effort which says: \"All things being equal, agents try to minimize their effort in doing what they intend to do\".\n sent1: The PARADISE regression model is constructed from 3 different corpora: the SAMMIE WOZ experiment (#REF) , and the iTalk system used for the user tests #TARGET_REF running the supervised baseline policy and the RL-based policy.\n sent2: By replicating the regression model on different data sets we test whether the automatic estimate of Task Ease generalises beyond the conditions and assumptions of a particular experimental design.\n sent3: The resulting models are shown in Equations 1-3 , where T askEase W OZ is the regression model obtained from the WOZ data, T askEase SL is obtained from the user test data running the supervised policy, and T askEase RL is obtained from the user test data running the RL-based policy.\n sent4: They all reflect the same trends: longer dialogues (measured in turns) predict a lower Task Ease, whereas a good performance in the multimodal information presentation phase (multimodal score) will positively influence Task Ease.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "In the following the overall method is shortly summarised. Please see #TARGET_REF; #REF) for details.",
                "1. We obtain an objective function from the WOZ data of (#REF) according to the PARADISE framework.",
                "In PARADISE multivariate linear regression is applied to experimental dialogue data in order to develop predictive models of user preferences (obtained from questionnaires) as a linear weighted function of dialogue performance measures (such as dialogue length).",
                "This predictive model is used to automatically evaluate dialogues.",
                "For RL this function is used as the \"reward\" for training."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In the following the overall method is shortly summarised. Please see #TARGET_REF; #REF) for details.\n sent1: 1. We obtain an objective function from the WOZ data of (#REF) according to the PARADISE framework.\n sent2: In PARADISE multivariate linear regression is applied to experimental dialogue data in order to develop predictive models of user preferences (obtained from questionnaires) as a linear weighted function of dialogue performance measures (such as dialogue length).\n sent3: This predictive model is used to automatically evaluate dialogues.\n sent4: For RL this function is used as the \"reward\" for training.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "In previous work we showed that the RL-based policy significantly outperforms the supervised policy in terms of improved user ratings and dialogue performance measures #TARGET_REF .",
                "Here, we test the relationship between improved user ratings and dialogue behaviour, i.e. we investigate which factors lead the users to give higher scores, and whether this was correctly reflected in the original reward function.",
                "We concentrate on the information presentation phase, since there is a simple two-way relationship between user scores and the number of presented items.",
                "To estimate this relationship we use curve fitting, which is used as an alternative model to linear regression in cases where the relationship between two variables can also be non-linear.",
                "For each presentation mode (verbal vs. multimodal) we select the (simplest) model with the closest fit to the data (R 2 )."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In previous work we showed that the RL-based policy significantly outperforms the supervised policy in terms of improved user ratings and dialogue performance measures #TARGET_REF .\n sent1: Here, we test the relationship between improved user ratings and dialogue behaviour, i.e. we investigate which factors lead the users to give higher scores, and whether this was correctly reflected in the original reward function.\n sent2: We concentrate on the information presentation phase, since there is a simple two-way relationship between user scores and the number of presented items.\n sent3: To estimate this relationship we use curve fitting, which is used as an alternative model to linear regression in cases where the relationship between two variables can also be non-linear.\n sent4: For each presentation mode (verbal vs. multimodal) we select the (simplest) model with the closest fit to the data (R 2 ).\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "For verbal presentation both learning schemes (RL and SL) were able to learn a policy from the WOZ data which received consistently good ratings from the users (between 6-5 for RL, and 5-4 for SL on a 7-point Likert scale).",
                "For multimodal presentation the WOZ objective function has a turning point at 14.8 (see Figure 3) .",
                "The RL-based policy learned to maximise the returned reward by displaying no more than 15 items.",
                "The SL policy, in contrast, did not learn an upper boundary for when to show items on the screen (since the wizards did not follow a specific pattern, #TARGET_REF ).",
                "When relating number of items to user scores, the RL policy produces a linear (slightly declining) line between 7 and 6 (Table 3 , bottom right), indicating that the applied policy reflected the users' preferences."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: For verbal presentation both learning schemes (RL and SL) were able to learn a policy from the WOZ data which received consistently good ratings from the users (between 6-5 for RL, and 5-4 for SL on a 7-point Likert scale).\n sent1: For multimodal presentation the WOZ objective function has a turning point at 14.8 (see Figure 3) .\n sent2: The RL-based policy learned to maximise the returned reward by displaying no more than 15 items.\n sent3: The SL policy, in contrast, did not learn an upper boundary for when to show items on the screen (since the wizards did not follow a specific pattern, #TARGET_REF ).\n sent4: When relating number of items to user scores, the RL policy produces a linear (slightly declining) line between 7 and 6 (Table 3 , bottom right), indicating that the applied policy reflected the users' preferences.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "We use XLNet #TARGET_REF to attempt to capture long range language dependencies.",
                "At the time of this writing, XLNet provides the best accuracy for many downstream tasks that require language modeling pre-training, including question-answering, text classification, and other natural language understanding tasks.",
                "We also attempt to take advantage of a transformer's parallel properties to make some performance optimizations when re-scoring our lattices."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We use XLNet #TARGET_REF to attempt to capture long range language dependencies.\n sent1: At the time of this writing, XLNet provides the best accuracy for many downstream tasks that require language modeling pre-training, including question-answering, text classification, and other natural language understanding tasks.\n sent2: We also attempt to take advantage of a transformer's parallel properties to make some performance optimizations when re-scoring our lattices.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "An acoustic model and n-gram language model are trained to provide a baseline word-error rate.",
                "We use a library that contains a pre-trained version of XLNet, an implementation of the transformer-XL architecture [14] .",
                "The model is fairly large with 110M parameters.",
                "It was previously trained on #REF and English Wikipedia which have 13GB of plain text combined #TARGET_REF .",
                "We run a transfer learning step using PyTorch on the TED-LIUM dataset."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: An acoustic model and n-gram language model are trained to provide a baseline word-error rate.\n sent1: We use a library that contains a pre-trained version of XLNet, an implementation of the transformer-XL architecture [14] .\n sent2: The model is fairly large with 110M parameters.\n sent3: It was previously trained on #REF and English Wikipedia which have 13GB of plain text combined #TARGET_REF .\n sent4: We run a transfer learning step using PyTorch on the TED-LIUM dataset.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "However, our results in table 1 show how difficult it is to make a dent in the WER with such a large XLNet model.",
                "The RNNLM still gives a much better score.",
                "We suspect that this is due to a few things: firstly, the XLNet is 110M parameters and was trained on approximately 13GB of text compared to 25MB worth of text for TED-LIUM.",
                "Given the size of the model, and the fact that it was pre-trained on 512 TPUs #TARGET_REF , we expect that training for 20 epochs on TED-LIUM's text is not enough to overcome the differences between written text and conversational speech.",
                "Without fine-tuning, adding memory seems to have an adverse effect on the test set."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: However, our results in table 1 show how difficult it is to make a dent in the WER with such a large XLNet model.\n sent1: The RNNLM still gives a much better score.\n sent2: We suspect that this is due to a few things: firstly, the XLNet is 110M parameters and was trained on approximately 13GB of text compared to 25MB worth of text for TED-LIUM.\n sent3: Given the size of the model, and the fact that it was pre-trained on 512 TPUs #TARGET_REF , we expect that training for 20 epochs on TED-LIUM's text is not enough to overcome the differences between written text and conversational speech.\n sent4: Without fine-tuning, adding memory seems to have an adverse effect on the test set.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "XLNet is a generalized auto-regressive model that can be used for language modeling based on the transformer-XL architecture #TARGET_REF .",
                "This means that the outputs of XLNet depend strictly on the previous outputs.",
                "This is different from other state-of-the-art language models like BERT (Bidirectional Encoder Representations from Transformers) which rely on conditioning the probabilities given surrounding words.",
                "In BERT, the model tries to predict a masked word by looking at all surrounding unmasked words ( figure 3 ).",
                "The concept of Fig. 3 ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: XLNet is a generalized auto-regressive model that can be used for language modeling based on the transformer-XL architecture #TARGET_REF .\n sent1: This means that the outputs of XLNet depend strictly on the previous outputs.\n sent2: This is different from other state-of-the-art language models like BERT (Bidirectional Encoder Representations from Transformers) which rely on conditioning the probabilities given surrounding words.\n sent3: In BERT, the model tries to predict a masked word by looking at all surrounding unmasked words ( figure 3 ).\n sent4: The concept of Fig. 3 .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "The concept of Fig. 3 .",
                "BERT attempts to predict the masked word use both left and right contexts.",
                "During training, a certain percentage of words are masked for use in prediction.",
                "If both \"San\" and \"Francisco\" were masked, BERT would not be able to use information when decoding one of the words to help in decoding the other.",
                "masking the input introduces a few disadvantages mentioned in #TARGET_REF ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: The concept of Fig. 3 .\n sent1: BERT attempts to predict the masked word use both left and right contexts.\n sent2: During training, a certain percentage of words are masked for use in prediction.\n sent3: If both \"San\" and \"Francisco\" were masked, BERT would not be able to use information when decoding one of the words to help in decoding the other.\n sent4: masking the input introduces a few disadvantages mentioned in #TARGET_REF .\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent2\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Here, we describe briefly the underlying framework, called RNN Encoder-Decoder, proposed by #TARGET_REF and (#REF) upon which we build a machine transliteration model that learns to transliterate end-to-end.",
                "The enoder is a character-based recurrent neural network that learns a highly nonlinear mapping from a spelling to the phonetic of the input sequence.",
                "This network reads the source name x = (x 1 , . . . , x T ) and encodes it into a sequence of hidden states h = (h 1 , · · · , h T ):",
                "Each hidden state h i is a bidirectional recurrent representation with forward and backward sequence information around the ith character.",
                "The representation of a forward sequence and a backward sequence of the input character sequence is estimated and concatenated to form a context set C = {h 1 , h 2 , ..., h T } (#REF; #REF) ."
            ],
            "label": [
                "EXTENDS",
                "BACKGROUND"
            ]
        },
        "input": "sent0: Here, we describe briefly the underlying framework, called RNN Encoder-Decoder, proposed by #TARGET_REF and (#REF) upon which we build a machine transliteration model that learns to transliterate end-to-end.\n sent1: The enoder is a character-based recurrent neural network that learns a highly nonlinear mapping from a spelling to the phonetic of the input sequence.\n sent2: This network reads the source name x = (x 1 , . . . , x T ) and encodes it into a sequence of hidden states h = (h 1 , · · · , h T ):\n sent3: Each hidden state h i is a bidirectional recurrent representation with forward and backward sequence information around the ith character.\n sent4: The representation of a forward sequence and a backward sequence of the input character sequence is estimated and concatenated to form a context set C = {h 1 , h 2 , ..., h T } (#REF; #REF) .\n",
        "output": "{\"label\": [\"EXTENDS\", \"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Using characters instead of words leads to longer sequences, so Gated Recurrent Units (#REFa) have been used for the encoder network to model long term dependencies.",
                "The encoder has 128 hidden units for each direction (forward and backward), and the decoder has 128 hidden units with soft attention mechanism #TARGET_REF .",
                "We train the model using stochastic gradient descent with Adam (#REF).",
                "Each update is computed using a minibatch of 128 sequence pairs.",
                "The norm of the gradient is clipped with a threshold 1 (#REF) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Using characters instead of words leads to longer sequences, so Gated Recurrent Units (#REFa) have been used for the encoder network to model long term dependencies.\n sent1: The encoder has 128 hidden units for each direction (forward and backward), and the decoder has 128 hidden units with soft attention mechanism #TARGET_REF .\n sent2: We train the model using stochastic gradient descent with Adam (#REF).\n sent3: Each update is computed using a minibatch of 128 sequence pairs.\n sent4: The norm of the gradient is clipped with a threshold 1 (#REF) .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We conducted a set of experiments to show the effectiveness of RNN Encoder-Decoder model #TARGET_REF; #REF) in the task of machine transliteration using standard benchmark datasets provided by NEWS 2015-16 shared task .",
                "Table 1 shows different datasets in our experiments.",
                "Each dataset covers different levels of difficulty and training set size.",
                "The proposed model has been applied on .",
                "each dataset without tuning the algorithm for each specific language pairs."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We conducted a set of experiments to show the effectiveness of RNN Encoder-Decoder model #TARGET_REF; #REF) in the task of machine transliteration using standard benchmark datasets provided by NEWS 2015-16 shared task .\n sent1: Table 1 shows different datasets in our experiments.\n sent2: Each dataset covers different levels of difficulty and training set size.\n sent3: The proposed model has been applied on .\n sent4: each dataset without tuning the algorithm for each specific language pairs.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "each dataset without tuning the algorithm for each specific language pairs.",
                "Also, we don't apply any preprocessing on the source or target language in order to evaluate the effectiveness of the proposed model in a fair situation.",
                "'TaskID' is a unique identifier in the following experiments.",
                "We leveraged a character-based encoderdecoder model (#REF; #REF) with soft attention mechanism #TARGET_REF .",
                "In this model, input sequences in both source and target languages have been represented as characters."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: each dataset without tuning the algorithm for each specific language pairs.\n sent1: Also, we don't apply any preprocessing on the source or target language in order to evaluate the effectiveness of the proposed model in a fair situation.\n sent2: 'TaskID' is a unique identifier in the following experiments.\n sent3: We leveraged a character-based encoderdecoder model (#REF; #REF) with soft attention mechanism #TARGET_REF .\n sent4: In this model, input sequences in both source and target languages have been represented as characters.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "For example, the modern English word said might be realized as sayed, seyd, said, sayd, etc.",
                "Spellings change over time, but also vary within a single time period and even within a single author, since orthography only became standardized in many languages fairly recently.",
                "Over the years, researchers have proposed normalization methods based on rules and/or edit distances (#REF; #REF; #REF; #REF; #REFa; #REF; #TARGET_REF , statistical machine translation (#REFb; #REF) , and most recently neural network models (Bollmann and Søgaard, 2016; #REF; #REF) .",
                "However, most of these systems have been developed and tested on a single language (or even a single corpus), and many have not been compared to the naïve but strong baseline that only changes words seen in the training data, normalizing each to its most frequent modern form observed during training.",
                "1 These issues make it hard to tell which methods generalize across languages and corpora, and how they compare to each other."
            ],
            "label": [
                "BACKGROUND",
                "MOTIVATION"
            ]
        },
        "input": "sent0: For example, the modern English word said might be realized as sayed, seyd, said, sayd, etc.\n sent1: Spellings change over time, but also vary within a single time period and even within a single author, since orthography only became standardized in many languages fairly recently.\n sent2: Over the years, researchers have proposed normalization methods based on rules and/or edit distances (#REF; #REF; #REF; #REF; #REFa; #REF; #TARGET_REF , statistical machine translation (#REFb; #REF) , and most recently neural network models (Bollmann and Søgaard, 2016; #REF; #REF) .\n sent3: However, most of these systems have been developed and tested on a single language (or even a single corpus), and many have not been compared to the naïve but strong baseline that only changes words seen in the training data, normalizing each to its most frequent modern form observed during training.\n sent4: 1 These issues make it hard to tell which methods generalize across languages and corpora, and how they compare to each other.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"MOTIVATION\"], \"context\": [\"sent2\", \"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Both are encoderdecoder models; the former with soft attention, and the latter with hard monotonic attention.",
                "We present results on five languages, for both seen and unseen words and for various amounts of training data.",
                "The soft attention model performs surprisingly poorly on seen words, so that its overall performance is worse than the naïve baseline and several earlier models #TARGET_REF .",
                "However, on unseen words (which we argue are what matters), both neural models do well.",
                "Unfortunately, these positive results did not"
            ],
            "label": [
                "BACKGROUND",
                "MOTIVATION"
            ]
        },
        "input": "sent0: Both are encoderdecoder models; the former with soft attention, and the latter with hard monotonic attention.\n sent1: We present results on five languages, for both seen and unseen words and for various amounts of training data.\n sent2: The soft attention model performs surprisingly poorly on seen words, so that its overall performance is worse than the naïve baseline and several earlier models #TARGET_REF .\n sent3: However, on unseen words (which we argue are what matters), both neural models do well.\n sent4: Unfortunately, these positive results did not\n",
        "output": "{\"label\": [\"BACKGROUND\", \"MOTIVATION\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "In describing the training and test sets, researchers should not only report the number of types and tokens, but also the per-centage of unseen tokens in the test (or dev) set and the percentage of training items (h, m) where h = m. This last statistic measures the degree of spelling variation, which varies considerably between corpora.",
                "As for reporting results, we have argued that accuracy should be reported separately for seen vs unseen tokens, and overall results compared to the naïve memorization baseline.",
                "Since historical spelling normalization is typically a low-resource task, systems should also ideally be tested with varying amounts of training data to assess how much annotation might be required for a new corpus #TARGET_REF; Bollmann and Søgaard, 2016; #REF) .",
                "Finally, since these systems may be deployed on corpora other than those they were trained on, and used as preprocessing for other tasks, we advocate reporting performance on a downstream task and/or different corpus.",
                "To our knowledge the only previous supervised learning system to do so is Pettersson et al. (2013b) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In describing the training and test sets, researchers should not only report the number of types and tokens, but also the per-centage of unseen tokens in the test (or dev) set and the percentage of training items (h, m) where h = m. This last statistic measures the degree of spelling variation, which varies considerably between corpora.\n sent1: As for reporting results, we have argued that accuracy should be reported separately for seen vs unseen tokens, and overall results compared to the naïve memorization baseline.\n sent2: Since historical spelling normalization is typically a low-resource task, systems should also ideally be tested with varying amounts of training data to assess how much annotation might be required for a new corpus #TARGET_REF; Bollmann and Søgaard, 2016; #REF) .\n sent3: Finally, since these systems may be deployed on corpora other than those they were trained on, and used as preprocessing for other tasks, we advocate reporting performance on a downstream task and/or different corpus.\n sent4: To our knowledge the only previous supervised learning system to do so is Pettersson et al. (2013b) .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We focus on two neural encoder-decoder models for spelling normalization, comparing them against the memorization baseline and to previous results from #TARGET_REF .",
                "The first model (#REF) 4 uses a fairly standard architecture with a bi-directional LSTM encoder and an LSTM decoder with soft attention (#REF) , and is trained using cross-entropy loss.",
                "The second model is a new approach to spelling normalization, which adapts the morphological reinflection system of #REF .",
                "5 The reinflection model generates the characters in an inflected wordform (y 1:n ), given the characters of its lemma (x 1:m ) and a set of corresponding morphological features (f ).",
                "Rather than using a soft attention mechanism that computes a weight vector over the entire sequence, this model exploits the generally monotonic character alignment between x 1:m and y 1:n and attends to only a single encoded input character at a time during decoding."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: We focus on two neural encoder-decoder models for spelling normalization, comparing them against the memorization baseline and to previous results from #TARGET_REF .\n sent1: The first model (#REF) 4 uses a fairly standard architecture with a bi-directional LSTM encoder and an LSTM decoder with soft attention (#REF) , and is trained using cross-entropy loss.\n sent2: The second model is a new approach to spelling normalization, which adapts the morphological reinflection system of #REF .\n sent3: 5 The reinflection model generates the characters in an inflected wordform (y 1:n ), given the characters of its lemma (x 1:m ) and a set of corresponding morphological features (f ).\n sent4: Rather than using a soft attention mechanism that computes a weight vector over the entire sequence, this model exploits the generally monotonic character alignment between x 1:m and y 1:n and attends to only a single encoded input character at a time during decoding.\n",
        "output": "{\"label\": [null], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "We use the same datasets as #TARGET_REF , with data from five languages over a range of historical periods.",
                "6 We use the same train/dev/test splits as Pettersson; dataset statistics are shown in Table  1 . Because we do no hyperparameter tuning, we do not use the development sets, and all results are reported on the test sets.",
                "Each system was tested as recommended above, with accuracy reported separately on seen and unseen items, and for different training data sizes.",
                "To evaluate the downstream effects of normalization, we applied the models to a collection of unseen documents and then tagged them with the Stan-ford POS tagger, which comes pre-trained on modern English.",
                "The documents are from the Parsed Corpus of Early English Correspondence (PCEEC) (#REF) , comprised of 84 letter collections from the 15th-17th centuries."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We use the same datasets as #TARGET_REF , with data from five languages over a range of historical periods.\n sent1: 6 We use the same train/dev/test splits as Pettersson; dataset statistics are shown in Table  1 . Because we do no hyperparameter tuning, we do not use the development sets, and all results are reported on the test sets.\n sent2: Each system was tested as recommended above, with accuracy reported separately on seen and unseen items, and for different training data sizes.\n sent3: To evaluate the downstream effects of normalization, we applied the models to a collection of unseen documents and then tagged them with the Stan-ford POS tagger, which comes pre-trained on modern English.\n sent4: The documents are from the Parsed Corpus of Early English Correspondence (PCEEC) (#REF) , comprised of 84 letter collections from the 15th-17th centuries.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "8 When we varied the training data sizes, we found that the soft attention model actually gets worse on seen tokens in all languages as the training data increases beyond a relatively small size.",
                "We have no good explanation for this, and it's possible that tuning the parameters would help.",
                "Table 2 : Tokens normalized correctly (%) for each dataset.",
                "Upper half: results on (A)ll tokens reported by #TARGET_REF for a hybrid model (apply memorization baseline to seen tokens and an edit-distance-based model to unseen tokens) and two SMT models (which align character unigrams and bigrams, respectively).",
                "Lower half: results from our experiments, including accuracy reported separately on (S)een and (U)nseen tokens. and presumably more difficult as training data size increases, so the baseline gets worse."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: 8 When we varied the training data sizes, we found that the soft attention model actually gets worse on seen tokens in all languages as the training data increases beyond a relatively small size.\n sent1: We have no good explanation for this, and it's possible that tuning the parameters would help.\n sent2: Table 2 : Tokens normalized correctly (%) for each dataset.\n sent3: Upper half: results on (A)ll tokens reported by #TARGET_REF for a hybrid model (apply memorization baseline to seen tokens and an edit-distance-based model to unseen tokens) and two SMT models (which align character unigrams and bigrams, respectively).\n sent4: Lower half: results from our experiments, including accuracy reported separately on (S)een and (U)nseen tokens. and presumably more difficult as training data size increases, so the baseline gets worse.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The three most widely studied modalities in current multimodal sentiment analysis research are: vocal (e.g., speech acoustics), visual (e.g., facial expressions), and verbal (e.g., lexical content).",
                "These are sometimes referred to as \"the three Vs\" of communication (#REF) .",
                "Multimodal sentiment analysis research focuses on understanding how an individual modality conveys sentiment information (intra-modal dynamics), and how they interact with each other (intermodal dynamics).",
                "It is a challenging research area and state-of-the-art performance of automatic sentiment prediction has room for improvement compared to human performance #TARGET_REF .",
                "While multimodal approaches to sentiment analysis are relatively new in NLP, multimodal emotion recognition has long been a focus of Affective Computing."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The three most widely studied modalities in current multimodal sentiment analysis research are: vocal (e.g., speech acoustics), visual (e.g., facial expressions), and verbal (e.g., lexical content).\n sent1: These are sometimes referred to as \"the three Vs\" of communication (#REF) .\n sent2: Multimodal sentiment analysis research focuses on understanding how an individual modality conveys sentiment information (intra-modal dynamics), and how they interact with each other (intermodal dynamics).\n sent3: It is a challenging research area and state-of-the-art performance of automatic sentiment prediction has room for improvement compared to human performance #TARGET_REF .\n sent4: While multimodal approaches to sentiment analysis are relatively new in NLP, multimodal emotion recognition has long been a focus of Affective Computing.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "In the LF model, as shown in Figure 5 , we concatenate the unimodal model top layers as the multimodal inputs.",
                "In the HF model, unimodal information is used in a hierarchy where the top layer of the lower unimodal model is concatenated with the input layer of the higher unimodal model, as shown in Figure 6 .",
                "We use the vocal modality at the bottom of the hierarchy while using the verbal modality at the top in HF fusion.",
                "This is because in previous studies (e.g., #TARGET_REF ) the verbal modality was shown to be the most effective for unimodal sentiment analysis, while the vocal modality was shown to be the least effective.",
                "4"
            ],
            "label": [
                "BACKGROUND",
                "SIMILARITY"
            ]
        },
        "input": "sent0: In the LF model, as shown in Figure 5 , we concatenate the unimodal model top layers as the multimodal inputs.\n sent1: In the HF model, unimodal information is used in a hierarchy where the top layer of the lower unimodal model is concatenated with the input layer of the higher unimodal model, as shown in Figure 6 .\n sent2: We use the vocal modality at the bottom of the hierarchy while using the verbal modality at the top in HF fusion.\n sent3: This is because in previous studies (e.g., #TARGET_REF ) the verbal modality was shown to be the most effective for unimodal sentiment analysis, while the vocal modality was shown to be the least effective.\n sent4: 4\n",
        "output": "{\"label\": [\"BACKGROUND\", \"SIMILARITY\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Structures of the single-task and multi-task learning models only differ at the output layer: for sentiment score regression the output is a single node with tanh activation; for polarity classification the output is a single node with sigmoid activation; for intensity classification the output is 4 nodes with softmax activation.",
                "The main task uses mean absolute error as the loss function, while polarity classification uses binary cross-entropy as the loss function, and intensity classification uses categorical crossentropy as the loss function.",
                "Following state-ofthe-art on the CMU-MOSI database , during training we used Adam as the optimization function with a learning rate of 0.0005.",
                "We use the CMU Multimodal Data Software Development Kit (SDK) #TARGET_REF to load and pre-process the CMU-MOSI database, which splits the 2199 opinion segments into training (1283 segments), validation (229 segments), and test (686 segments) sets.",
                "1 We implement the sentiment analysis models using the Keras deep learning library (#REF) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Structures of the single-task and multi-task learning models only differ at the output layer: for sentiment score regression the output is a single node with tanh activation; for polarity classification the output is a single node with sigmoid activation; for intensity classification the output is 4 nodes with softmax activation.\n sent1: The main task uses mean absolute error as the loss function, while polarity classification uses binary cross-entropy as the loss function, and intensity classification uses categorical crossentropy as the loss function.\n sent2: Following state-ofthe-art on the CMU-MOSI database , during training we used Adam as the optimization function with a learning rate of 0.0005.\n sent3: We use the CMU Multimodal Data Software Development Kit (SDK) #TARGET_REF to load and pre-process the CMU-MOSI database, which splits the 2199 opinion segments into training (1283 segments), validation (229 segments), and test (686 segments) sets.\n sent4: 1 We implement the sentiment analysis models using the Keras deep learning library (#REF) .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Here we report our sentiment score prediction experiments.",
                "2 In Tables 2 and 3 , \"S\" is the singletask learning model; \"S+P\" is the bi-task learning model with polarity classification as the auxillary task; \"S+I\" is the bi-task learning model with intensity classification as the auxillary task; \"S+P+I\" is the tri-task learning model.",
                "To evaluate the performance of sentiment score prediction, following previous work #TARGET_REF , we Tables 2 and 3 , the numbers in bold are the best performance for each modality or fusion strategy.",
                "To identify the significant differences in results, we perform a two-sample Wilcoxon test on the sentiment score predictions given by each pair of models being compared and consider p < 0.05 as significant.",
                "We also include random prediction as a baseline and the human performance reported by ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Here we report our sentiment score prediction experiments.\n sent1: 2 In Tables 2 and 3 , \"S\" is the singletask learning model; \"S+P\" is the bi-task learning model with polarity classification as the auxillary task; \"S+I\" is the bi-task learning model with intensity classification as the auxillary task; \"S+P+I\" is the tri-task learning model.\n sent2: To evaluate the performance of sentiment score prediction, following previous work #TARGET_REF , we Tables 2 and 3 , the numbers in bold are the best performance for each modality or fusion strategy.\n sent3: To identify the significant differences in results, we perform a two-sample Wilcoxon test on the sentiment score predictions given by each pair of models being compared and consider p < 0.05 as significant.\n sent4: We also include random prediction as a baseline and the human performance reported by .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The results of unimodal sentiment prediction experiments are shown in Table 2 .",
                "3 The verbal models have the best performance here, which is consistent with previous sentiment analysis studies on multiple databases (e.g., #TARGET_REF ).",
                "This suggests that lexical information remains the most effective for sentiment analysis.",
                "On each modality, the best performance is achieved by a multi-task learning model.",
                "This answers our first research question and suggests that sentiment analysis can benefit from multi-task learning."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: The results of unimodal sentiment prediction experiments are shown in Table 2 .\n sent1: 3 The verbal models have the best performance here, which is consistent with previous sentiment analysis studies on multiple databases (e.g., #TARGET_REF ).\n sent2: This suggests that lexical information remains the most effective for sentiment analysis.\n sent3: On each modality, the best performance is achieved by a multi-task learning model.\n sent4: This answers our first research question and suggests that sentiment analysis can benefit from multi-task learning.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Traditional methods assess readability using surface features and shallow linguistic features such as the ones mentioned in the preceding paragraph.",
                "Cognitively motivated methods take into account the cohesion and coherence of text, its latent topic structure, Kintsch's propositions, etc [1] , [8] , [13] .",
                "Finally, machine learning methods utilize sophisticated structures such as language models [3] , [4] , [18] , query logs [15] , and several other features to predict the readability of open-domain text data.",
                "There are very few studies on readability assessment in Bengali texts.",
                "We found only three lines of work that specifically looked into Bengali readability [6] , [11] , #TARGET_REF ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Traditional methods assess readability using surface features and shallow linguistic features such as the ones mentioned in the preceding paragraph.\n sent1: Cognitively motivated methods take into account the cohesion and coherence of text, its latent topic structure, Kintsch's propositions, etc [1] , [8] , [13] .\n sent2: Finally, machine learning methods utilize sophisticated structures such as language models [3] , [4] , [18] , query logs [15] , and several other features to predict the readability of open-domain text data.\n sent3: There are very few studies on readability assessment in Bengali texts.\n sent4: We found only three lines of work that specifically looked into Bengali readability [6] , [11] , #TARGET_REF .\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "We plan to release this dataset for future research.",
                "We are working on readability modeling in Bengali, and this dataset will be very helpful.",
                "An important limitation of our study is the small corpus size.",
                "We only have 30 annotated passages at our disposal, whereas #REF had around 300. But Islam et al.'s dataset is not annotated in as fine-grained a fashion as ours.",
                "Note also that our dataset is larger than both #TARGET_REF 16document dataset #TARGET_REF , and Das and Roychoudhury's seven document dataset [6] ."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: We plan to release this dataset for future research.\n sent1: We are working on readability modeling in Bengali, and this dataset will be very helpful.\n sent2: An important limitation of our study is the small corpus size.\n sent3: We only have 30 annotated passages at our disposal, whereas #REF had around 300. But Islam et al.'s dataset is not annotated in as fine-grained a fashion as ours.\n sent4: Note also that our dataset is larger than both #TARGET_REF 16document dataset #TARGET_REF , and Das and Roychoudhury's seven document dataset [6] .\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Low-resource automatic speech-to-text translation (AST) has recently gained traction as a way to bring NLP tools to under-represented languages.",
                "An end-to-end approach [1] [2] [3] [4] #TARGET_REF [6] [7] is particularly appealing for source languages with no written form, or for endangered languages where translations into a high-resource language may be easier to collect than transcriptions [8] .",
                "However, building high-quality endto-end AST with little parallel data is challenging, and has led researchers to explore how other sources of data could be used to help.",
                "A number of methods have been investigated.",
                "Several of these use transcribed source language audio and/or translated source language text in a multitask learning scenario [4, 6, 9] or to pre-train parts of the model before fine-tuning on the end-to-end AST task [4] ."
            ],
            "label": [
                "BACKGROUND",
                "MOTIVATION"
            ]
        },
        "input": "sent0: Low-resource automatic speech-to-text translation (AST) has recently gained traction as a way to bring NLP tools to under-represented languages.\n sent1: An end-to-end approach [1] [2] [3] [4] #TARGET_REF [6] [7] is particularly appealing for source languages with no written form, or for endangered languages where translations into a high-resource language may be easier to collect than transcriptions [8] .\n sent2: However, building high-quality endto-end AST with little parallel data is challenging, and has led researchers to explore how other sources of data could be used to help.\n sent3: A number of methods have been investigated.\n sent4: Several of these use transcribed source language audio and/or translated source language text in a multitask learning scenario [4, 6, 9] or to pre-train parts of the model before fine-tuning on the end-to-end AST task [4] .\n",
        "output": "{\"label\": [\"BACKGROUND\", \"MOTIVATION\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Several of these use transcribed source language audio and/or translated source language text in a multitask learning scenario [4, 6, 9] or to pre-train parts of the model before fine-tuning on the end-to-end AST task [4] .",
                "Others assume, as we do here, that no additional source language resources are available, in which case transfer learning using data from language(s) other than the source language is a good option.",
                "In particular, several researchers have shown that low-resource AST can be improved by pretraining on an ASR task in some other language, then transferring the encoder parameters to initialize the AST model.",
                "For example, Bansal et al. #TARGET_REF showed that pre-training on either English or French ASR improved their Spanish-English AST system (trained on 20 hours of parallel data) and #REF got improvements on an 8-hour Swahili-English AST dataset using English ASR pretraining.",
                "Overall these results show that pretraining helps, but leave open the question of what factors affect the degree of improvement."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Several of these use transcribed source language audio and/or translated source language text in a multitask learning scenario [4, 6, 9] or to pre-train parts of the model before fine-tuning on the end-to-end AST task [4] .\n sent1: Others assume, as we do here, that no additional source language resources are available, in which case transfer learning using data from language(s) other than the source language is a good option.\n sent2: In particular, several researchers have shown that low-resource AST can be improved by pretraining on an ASR task in some other language, then transferring the encoder parameters to initialize the AST model.\n sent3: For example, Bansal et al. #TARGET_REF showed that pre-training on either English or French ASR improved their Spanish-English AST system (trained on 20 hours of parallel data) and #REF got improvements on an 8-hour Swahili-English AST dataset using English ASR pretraining.\n sent4: Overall these results show that pretraining helps, but leave open the question of what factors affect the degree of improvement.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "After pretraining an ASR model, we transfer only its encoder parameters to the AST task.",
                "Previous experiments #TARGET_REF showed that the encoder accounts for most of the benefits of transferring the parameters.",
                "Transferring also the decoder and attention mechanism does bring some improvements, but is only feasible when the ASR pretraining language is the same as the AST target language, which is not true in most of our experiments.",
                "In addition to pretraining, we experimented with data augmentation.",
                "Specifically, we augmented the AST data using Kaldi's [13] 3-way speed perturbation, adding versions of the AST data where the audio is sped down and up by a factor of 0.9 and 1.1, respectively."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: After pretraining an ASR model, we transfer only its encoder parameters to the AST task.\n sent1: Previous experiments #TARGET_REF showed that the encoder accounts for most of the benefits of transferring the parameters.\n sent2: Transferring also the decoder and attention mechanism does bring some improvements, but is only feasible when the ASR pretraining language is the same as the AST target language, which is not true in most of our experiments.\n sent3: In addition to pretraining, we experimented with data augmentation.\n sent4: Specifically, we augmented the AST data using Kaldi's [13] 3-way speed perturbation, adding versions of the AST data where the audio is sped down and up by a factor of 0.9 and 1.1, respectively.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Transcriptions with annotated word boundaries are available in both Hanzi (Chinese characters) and Romanized versions, and we built models with each.",
                "To compare to the GlobalPhone data, we also created a 20-hour subset of the Romanized AISHELL (zh-ai-small) by randomly selecting utterances from a subset of the speakers (81, roughly the number present in most of the GlobalPhone datasets).",
                "Finally, to reproduce one of the experiments from [5] , we pretrained one model using 300 hours of Switchboard #REF .",
                "This data is the most similar to the AST speech data in terms of style and channel (both are conversational telephone speech).",
                "However, as noted by #TARGET_REF , the Fisher Spanish speech contains many words that are actually in English (code-switching), so pretraining on English may provide an unfair advantage relative to other languages."
            ],
            "label": [
                "BACKGROUND",
                "MOTIVATION"
            ]
        },
        "input": "sent0: Transcriptions with annotated word boundaries are available in both Hanzi (Chinese characters) and Romanized versions, and we built models with each.\n sent1: To compare to the GlobalPhone data, we also created a 20-hour subset of the Romanized AISHELL (zh-ai-small) by randomly selecting utterances from a subset of the speakers (81, roughly the number present in most of the GlobalPhone datasets).\n sent2: Finally, to reproduce one of the experiments from [5] , we pretrained one model using 300 hours of Switchboard #REF .\n sent3: This data is the most similar to the AST speech data in terms of style and channel (both are conversational telephone speech).\n sent4: However, as noted by #TARGET_REF , the Fisher Spanish speech contains many words that are actually in English (code-switching), so pretraining on English may provide an unfair advantage relative to other languages.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"MOTIVATION\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "First, can we predict what sort of pretraining data is best for a particular AST task? Does it matter if the pretraining language is related to the AST source language (defined here as part of the same language family, since phonetic similarity is difficult to measure), or is the amount of pretraining data (or some other factor) more important?",
                "Second, can pretraining be effectively combined with other methods, such as data augmentation, in order to further improve AST results?",
                "To answer these questions, we use the same AST architecture and Spanish-English parallel data as Bansal et al. #TARGET_REF , but pretrain the encoder using a number of different ASR datasets: the 150hour AISHELL corpus of Chinese as well as seven GlobalPhone languages, each with about 20 hours of data.",
                "We find that pretraining on a larger amount of data from an unrelated language is much better than pretraining on a smaller amount of data from a related language.",
                "Moreover, even when controlling for the amount of data, the WER of the ASR model from pretraining seems to be a better predictor of final AST performance than does language relatedness."
            ],
            "label": [
                "EXTENDS",
                "USE"
            ]
        },
        "input": "sent0: First, can we predict what sort of pretraining data is best for a particular AST task? Does it matter if the pretraining language is related to the AST source language (defined here as part of the same language family, since phonetic similarity is difficult to measure), or is the amount of pretraining data (or some other factor) more important?\n sent1: Second, can pretraining be effectively combined with other methods, such as data augmentation, in order to further improve AST results?\n sent2: To answer these questions, we use the same AST architecture and Spanish-English parallel data as Bansal et al. #TARGET_REF , but pretrain the encoder using a number of different ASR datasets: the 150hour AISHELL corpus of Chinese as well as seven GlobalPhone languages, each with about 20 hours of data.\n sent3: We find that pretraining on a larger amount of data from an unrelated language is much better than pretraining on a smaller amount of data from a related language.\n sent4: Moreover, even when controlling for the amount of data, the WER of the ASR model from pretraining seems to be a better predictor of final AST performance than does language relatedness.\n",
        "output": "{\"label\": [\"EXTENDS\", \"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "For both ASR and AST tasks we use the same end-to-end system architecture shown in Figure 1 : the encoder-decoder model from #TARGET_REF , which itself is adapted from [2] , [4] and [3] .",
                "Details of the architecture and training parameters are described in Section 3.4.",
                "After pretraining an ASR model, we transfer only its encoder parameters to the AST task.",
                "Previous experiments [5] showed that the encoder accounts for most of the benefits of transferring the parameters.",
                "Transferring also the decoder and attention mechanism does bring some improvements, but is only feasible when the ASR pretraining language is the same as the AST target language, which is not true in most of our experiments."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: For both ASR and AST tasks we use the same end-to-end system architecture shown in Figure 1 : the encoder-decoder model from #TARGET_REF , which itself is adapted from [2] , [4] and [3] .\n sent1: Details of the architecture and training parameters are described in Section 3.4.\n sent2: After pretraining an ASR model, we transfer only its encoder parameters to the AST task.\n sent3: Previous experiments [5] showed that the encoder accounts for most of the benefits of transferring the parameters.\n sent4: Transferring also the decoder and attention mechanism does bring some improvements, but is only feasible when the ASR pretraining language is the same as the AST target language, which is not true in most of our experiments.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Transcriptions with annotated word boundaries are available in both Hanzi (Chinese characters) and Romanized versions, and we built models with each.",
                "To compare to the GlobalPhone data, we also created a 20-hour subset of the Romanized AISHELL (zh-ai-small) by randomly selecting utterances from a subset of the speakers (81, roughly the number present in most of the GlobalPhone datasets).",
                "Finally, to reproduce one of the experiments from #TARGET_REF , we pretrained one model using 300 hours of Switchboard #REF .",
                "This data is the most similar to the AST speech data in terms of style and channel (both are conversational telephone speech).",
                "However, as noted by [5] , the Fisher Spanish speech contains many words that are actually in English (code-switching), so pretraining on English may provide an unfair advantage relative to other languages."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Transcriptions with annotated word boundaries are available in both Hanzi (Chinese characters) and Romanized versions, and we built models with each.\n sent1: To compare to the GlobalPhone data, we also created a 20-hour subset of the Romanized AISHELL (zh-ai-small) by randomly selecting utterances from a subset of the speakers (81, roughly the number present in most of the GlobalPhone datasets).\n sent2: Finally, to reproduce one of the experiments from #TARGET_REF , we pretrained one model using 300 hours of Switchboard #REF .\n sent3: This data is the most similar to the AST speech data in terms of style and channel (both are conversational telephone speech).\n sent4: However, as noted by [5] , the Fisher Spanish speech contains many words that are actually in English (code-switching), so pretraining on English may provide an unfair advantage relative to other languages.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Following the architecture and training procedure described in #TARGET_REF , input speech features are fed into a stack of two CNN layers.",
                "In each CNN layer we stride the input with a factor of 2 along time, apply The points in the circled group come from different runs on the same dataset but with different BPE or learning rate schedules.",
                "The Spearman rank correlation of these points is -0.97; the correlation is -0.92 when using test sets to compute both ASR and BLEU.",
                "ReLU activation [20] followed by batch normalization [21] .",
                "The CNN output is fed into a three-layer bi-directional long short-term memory network (LSTM) [22] , with 512 hidden layer dimensions."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Following the architecture and training procedure described in #TARGET_REF , input speech features are fed into a stack of two CNN layers.\n sent1: In each CNN layer we stride the input with a factor of 2 along time, apply The points in the circled group come from different runs on the same dataset but with different BPE or learning rate schedules.\n sent2: The Spearman rank correlation of these points is -0.97; the correlation is -0.92 when using test sets to compute both ASR and BLEU.\n sent3: ReLU activation [20] followed by batch normalization [21] .\n sent4: The CNN output is fed into a three-layer bi-directional long short-term memory network (LSTM) [22] , with 512 hidden layer dimensions.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "The CNN output is fed into a three-layer bi-directional long short-term memory network (LSTM) [22] , with 512 hidden layer dimensions.",
                "For decoding, we use the predicted token 20% of the time and the training token 80% of the time [23] as input to a 128-dimensional embedding layer followed by a three-layer LSTM, with 256 hidden layer dimensions, and combine this with the output from the attention mechanism [24] to predict the word at the current time step.",
                "We use code and hyperparameter settings from #TARGET_REF 4 : the Adam optimizer [25] with an initial learning rate of 0.001 and decay it by a factor of 0.5 based on the dev set BLEU score.",
                "When training AST models, we regularize using dropout [26] with a ratio of 0.3 over the embedding and LSTM layers [27] ; weight decay with a rate of 0.0001; and, after the first 20 epochs, 30% of the time we replace the predicted output word by a random word from the target vocabulary.",
                "At test time we use beam decoding with a beam size of 5 and length normalization [28] with a weight of 0.6."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The CNN output is fed into a three-layer bi-directional long short-term memory network (LSTM) [22] , with 512 hidden layer dimensions.\n sent1: For decoding, we use the predicted token 20% of the time and the training token 80% of the time [23] as input to a 128-dimensional embedding layer followed by a three-layer LSTM, with 256 hidden layer dimensions, and combine this with the output from the attention mechanism [24] to predict the word at the current time step.\n sent2: We use code and hyperparameter settings from #TARGET_REF 4 : the Adam optimizer [25] with an initial learning rate of 0.001 and decay it by a factor of 0.5 based on the dev set BLEU score.\n sent3: When training AST models, we regularize using dropout [26] with a ratio of 0.3 over the embedding and LSTM layers [27] ; weight decay with a rate of 0.0001; and, after the first 20 epochs, 30% of the time we replace the predicted output word by a random word from the target vocabulary.\n sent4: At test time we use beam decoding with a beam size of 5 and length normalization [28] with a weight of 0.6.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Our baseline 20-hour AST system obtains a BLEU score of 10.3 ( Table 1 , first row), 0.5 BLEU point lower than that reported by #TARGET_REF .",
                "This discrepancy might be due to differences in subsampling from the 160-hour AST dataset to create the 20-hour subset, or from Kaldi parameters when computing the MFCCs.",
                "WERs for our pre-trained models ( Table 1 ) vary from 22.5 for the large AISHELL dataset with Romanized transcript to 80.5 for Portuguese GlobalPhone.",
                "These are considerably worse than stateof-the-art ASR systems (e.g., Kaldi recipes can achieve WER of 7.5 on AISHELL and 26.5 on Portuguese GlobalPhone), but we did not optimize our architecture or hyperparameters for the ASR task since our main goal is to analyze the relationship between pretraining and AST performance (and in order to use pretraining, we must use a seq2seq model with the architecture as for AST)."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Our baseline 20-hour AST system obtains a BLEU score of 10.3 ( Table 1 , first row), 0.5 BLEU point lower than that reported by #TARGET_REF .\n sent1: This discrepancy might be due to differences in subsampling from the 160-hour AST dataset to create the 20-hour subset, or from Kaldi parameters when computing the MFCCs.\n sent2: WERs for our pre-trained models ( Table 1 ) vary from 22.5 for the large AISHELL dataset with Romanized transcript to 80.5 for Portuguese GlobalPhone.\n sent3: These are considerably worse than stateof-the-art ASR systems (e.g., Kaldi recipes can achieve WER of 7.5 on AISHELL and 26.5 on Portuguese GlobalPhone), but we did not optimize our architecture or hyperparameters for the ASR task since our main goal is to analyze the relationship between pretraining and AST performance (and in order to use pretraining, we must use a seq2seq model with the architecture as for AST).\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Moreover, pretraining on the large Chinese dataset yields a bigger improvement than either of these-4.3 BLEU points.",
                "This is nearly as much as the 6 point improvement reported by #TARGET_REF when pretraining on 100 hours of English data, which is especially surprising given not only that Chinese is very different from Spanish, but also that the Spanish data contains some English words.",
                "This finding seems to suggest that data size is more important than language relatedness for predicting the effects of pretraining.",
                "However, there are big differences even amongst the languages with similar amounts of pretraining data.",
                "Analyzing our results further, we found a striking correlation between the WER of the initial ASR model and the BLEU score of the AST system pretrained using that model, as shown in Figure 2 ."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: Moreover, pretraining on the large Chinese dataset yields a bigger improvement than either of these-4.3 BLEU points.\n sent1: This is nearly as much as the 6 point improvement reported by #TARGET_REF when pretraining on 100 hours of English data, which is especially surprising given not only that Chinese is very different from Spanish, but also that the Spanish data contains some English words.\n sent2: This finding seems to suggest that data size is more important than language relatedness for predicting the effects of pretraining.\n sent3: However, there are big differences even amongst the languages with similar amounts of pretraining data.\n sent4: Analyzing our results further, we found a striking correlation between the WER of the initial ASR model and the BLEU score of the AST system pretrained using that model, as shown in Figure 2 .\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "1 Roots combine with derivational (e.g. refut+able) and inflectional affixes (e.g. hold+ing).",
                "Computational segmentation approaches can be divided into rule-based (#REF) , supervised (#REF) , semi-supervised (Grönroos et al., 2014) , and unsupervised (#REF) .",
                "#TARGET_REF observe that some of the errors made by their otherwise highly-accurate system, such as hol-dov-er and coad-ju-tors, can be attributed to the lack of awareness of morphological boundaries, which influence syllabification.",
                "In this paper, we demonstrate that the accuracy of orthographic syllabification can be improved by considering morphology.",
                "We augment the syllabification approach of #REF , with features encoding morphological segmentation of words."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: 1 Roots combine with derivational (e.g. refut+able) and inflectional affixes (e.g. hold+ing).\n sent1: Computational segmentation approaches can be divided into rule-based (#REF) , supervised (#REF) , semi-supervised (Grönroos et al., 2014) , and unsupervised (#REF) .\n sent2: #TARGET_REF observe that some of the errors made by their otherwise highly-accurate system, such as hol-dov-er and coad-ju-tors, can be attributed to the lack of awareness of morphological boundaries, which influence syllabification.\n sent3: In this paper, we demonstrate that the accuracy of orthographic syllabification can be improved by considering morphology.\n sent4: We augment the syllabification approach of #REF , with features encoding morphological segmentation of words.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "In this section, we describe the original syllabification method of #REF , which serves as our baseline system, and discuss various approaches to incorporating morphological information.",
                "#TARGET_REF present a discriminative approach to automatic syllabification.",
                "They formulate syllabification as a tagging problem, and learn a Structured SVM tagger from labeled data (#REF) .",
                "Under the Markov assumption that each tag is dependent on its previous n tags, the tagger predicts the optimal tag sequence (#REF) .",
                "A large-margin training objective is applied to learn a weight vector to separate the correct tag sequence from other possible sequences for each training instance."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In this section, we describe the original syllabification method of #REF , which serves as our baseline system, and discuss various approaches to incorporating morphological information.\n sent1: #TARGET_REF present a discriminative approach to automatic syllabification.\n sent2: They formulate syllabification as a tagging problem, and learn a Structured SVM tagger from labeled data (#REF) .\n sent3: Under the Markov assumption that each tag is dependent on its previous n tags, the tagger predicts the optimal tag sequence (#REF) .\n sent4: A large-margin training objective is applied to learn a weight vector to separate the correct tag sequence from other possible sequences for each training instance.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF observe that some of the errors made by their otherwise highly-accurate system, such as hol-dov-er and coad-ju-tors, can be attributed to the lack of awareness of morphological boundaries, which influence syllabification.",
                "In this paper, we demonstrate that the accuracy of orthographic syllabification can be improved by considering morphology.",
                "We augment the syllabification approach of #TARGET_REF , with features encoding morphological segmentation of words.",
                "We investigate the degree of overlap between the morphological and syllable boundaries.",
                "The results of our experiments on English and German show that the incorporation of expert-annotated (gold) morphological boundaries extracted from lexical databases substantially reduces the syllabification error rate, particularly in low-resource settings."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: #REF observe that some of the errors made by their otherwise highly-accurate system, such as hol-dov-er and coad-ju-tors, can be attributed to the lack of awareness of morphological boundaries, which influence syllabification.\n sent1: In this paper, we demonstrate that the accuracy of orthographic syllabification can be improved by considering morphology.\n sent2: We augment the syllabification approach of #TARGET_REF , with features encoding morphological segmentation of words.\n sent3: We investigate the degree of overlap between the morphological and syllable boundaries.\n sent4: The results of our experiments on English and German show that the incorporation of expert-annotated (gold) morphological boundaries extracted from lexical databases substantially reduces the syllabification error rate, particularly in low-resource settings.\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "As a baseline, we replicate the experiments of #TARGET_REF , and extend them to lowresource settings.",
                "Since the training sets are of slightly different sizes, we label each training size point as specified in Table 3 .",
                "We see that correct syllabification of approximately half of the words is achieved with as few as 100 English and 50 German training examples."
            ],
            "label": [
                "EXTENDS",
                "USE"
            ]
        },
        "input": "sent0: As a baseline, we replicate the experiments of #TARGET_REF , and extend them to lowresource settings.\n sent1: Since the training sets are of slightly different sizes, we label each training size point as specified in Table 3 .\n sent2: We see that correct syllabification of approximately half of the words is achieved with as few as 100 English and 50 German training examples.\n",
        "output": "{\"label\": [\"EXTENDS\", \"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "In this section, we describe the original syllabification method of #TARGET_REF , which serves as our baseline system, and discuss various approaches to incorporating morphological information.",
                "#REF present a discriminative approach to automatic syllabification.",
                "They formulate syllabification as a tagging problem, and learn a Structured SVM tagger from labeled data (#REF) .",
                "Under the Markov assumption that each tag is dependent on its previous n tags, the tagger predicts the optimal tag sequence (#REF) .",
                "A large-margin training objective is applied to learn a weight vector to separate the correct tag sequence from other possible sequences for each training instance."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In this section, we describe the original syllabification method of #TARGET_REF , which serves as our baseline system, and discuss various approaches to incorporating morphological information.\n sent1: #REF present a discriminative approach to automatic syllabification.\n sent2: They formulate syllabification as a tagging problem, and learn a Structured SVM tagger from labeled data (#REF) .\n sent3: Under the Markov assumption that each tag is dependent on its previous n tags, the tagger predicts the optimal tag sequence (#REF) .\n sent4: A large-margin training objective is applied to learn a weight vector to separate the correct tag sequence from other possible sequences for each training instance.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "In this section, we introduce our data sets, and discuss the overlap between morphological and syllabic boundaries.",
                "We investigate the quality of the morphological segmentations of produced by various methods, and replicate the syllabification results of #TARGET_REF .",
                "Finally, we discuss the results of incorporating morphological information into the syllabification system."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: In this section, we introduce our data sets, and discuss the overlap between morphological and syllabic boundaries.\n sent1: We investigate the quality of the morphological segmentations of produced by various methods, and replicate the syllabification results of #TARGET_REF .\n sent2: Finally, we discuss the results of incorporating morphological information into the syllabification system.\n",
        "output": "{\"label\": [null], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Contextualized word embeddings have played an essential role in many NLP tasks.",
                "One could expect considerable performance boosts by simply substituting distributional word embeddings with Flair (#REF) , ELMo #TARGET_REF , and BERT (#REF) embeddings.",
                "The unique thing about contextualized word embeddings is that different representations are generated for the same word type with different topical senses.",
                "This work focuses on interpreting embedding representations for word senses.",
                "We propose an algorithm (Section 3) that learns the dimension importance in representing sense information and then mask unessential dimensions that are deemed less meaningful in word sense representations to 0."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Contextualized word embeddings have played an essential role in many NLP tasks.\n sent1: One could expect considerable performance boosts by simply substituting distributional word embeddings with Flair (#REF) , ELMo #TARGET_REF , and BERT (#REF) embeddings.\n sent2: The unique thing about contextualized word embeddings is that different representations are generated for the same word type with different topical senses.\n sent3: This work focuses on interpreting embedding representations for word senses.\n sent4: We propose an algorithm (Section 3) that learns the dimension importance in representing sense information and then mask unessential dimensions that are deemed less meaningful in word sense representations to 0.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Three popular word embedding algorithms are used for our experiments with various dimensions: ELMo, Flair, and BERT.",
                "ELMo is a deep word-level bidirectional LSTM language model with character level convolution networks along with a final linear projection output layer #TARGET_REF .",
                "Flair is a character-level bidirectional LSTM language model on sequences of characters (#REF) .",
                "BERT has an architecture of a multi-layer bidirectional transformer encoder (#REF) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Three popular word embedding algorithms are used for our experiments with various dimensions: ELMo, Flair, and BERT.\n sent1: ELMo is a deep word-level bidirectional LSTM language model with character level convolution networks along with a final linear projection output layer #TARGET_REF .\n sent2: Flair is a character-level bidirectional LSTM language model on sequences of characters (#REF) .\n sent3: BERT has an architecture of a multi-layer bidirectional transformer encoder (#REF) .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The Most Frequent Sense (MFS) heuristic is the most common baseline, which selects the most frequent sense in the training data for the target word (#REFa) .",
                "Depending on the evaluation dataset, the state-of-art in WSD varies.",
                "Raganato et al. (2017b) utilize bi-LSTM networks with attention mechanism and a softmax layer.",
                "#REF and #TARGET_REF also adopt bi-LSTM networks with KNN classifiers.",
                "Later work incorporates word features such as gloss and POS information into memory networks (#REF; #REF) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The Most Frequent Sense (MFS) heuristic is the most common baseline, which selects the most frequent sense in the training data for the target word (#REFa) .\n sent1: Depending on the evaluation dataset, the state-of-art in WSD varies.\n sent2: Raganato et al. (2017b) utilize bi-LSTM networks with attention mechanism and a softmax layer.\n sent3: #REF and #TARGET_REF also adopt bi-LSTM networks with KNN classifiers.\n sent4: Later work incorporates word features such as gloss and POS information into memory networks (#REF; #REF) .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "K-Nearest Neighbor (KNN) approach is adopted from both ELMo #TARGET_REF and con-text2vec (#REF) to establish strong baseline approaches.",
                "Sense-based KNN Adapted from ELMo (#REF) with k = 1, words that have the same senses are clustered together, and the average of that cluster is used as the sense vector, which is then fitted using a one KNN classifier.",
                "Unseen words from the test corpus fall back using the first sense from WordNet (#REF) .",
                "Word-based KNN Following context2vec (#REF), a cluster of each lemma occurrences in the training set is formed.",
                "Each word has a distinct classifier, which will assign labels based on k, where k = min(# of occurrences, 5)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: K-Nearest Neighbor (KNN) approach is adopted from both ELMo #TARGET_REF and con-text2vec (#REF) to establish strong baseline approaches.\n sent1: Sense-based KNN Adapted from ELMo (#REF) with k = 1, words that have the same senses are clustered together, and the average of that cluster is used as the sense vector, which is then fitted using a one KNN classifier.\n sent2: Unseen words from the test corpus fall back using the first sense from WordNet (#REF) .\n sent3: Word-based KNN Following context2vec (#REF), a cluster of each lemma occurrences in the training set is formed.\n sent4: Each word has a distinct classifier, which will assign labels based on k, where k = min(# of occurrences, 5).\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "K-Nearest Neighbor (KNN) approach is adopted from both ELMo (#REF) and con-text2vec (#REF) to establish strong baseline approaches.",
                "Sense-based KNN Adapted from ELMo #TARGET_REF with k = 1, words that have the same senses are clustered together, and the average of that cluster is used as the sense vector, which is then fitted using a one KNN classifier.",
                "Unseen words from the test corpus fall back using the first sense from WordNet (#REF) .",
                "Word-based KNN Following context2vec (#REF), a cluster of each lemma occurrences in the training set is formed.",
                "Each word has a distinct classifier, which will assign labels based on k, where k = min(# of occurrences, 5)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: K-Nearest Neighbor (KNN) approach is adopted from both ELMo (#REF) and con-text2vec (#REF) to establish strong baseline approaches.\n sent1: Sense-based KNN Adapted from ELMo #TARGET_REF with k = 1, words that have the same senses are clustered together, and the average of that cluster is used as the sense vector, which is then fitted using a one KNN classifier.\n sent2: Unseen words from the test corpus fall back using the first sense from WordNet (#REF) .\n sent3: Word-based KNN Following context2vec (#REF), a cluster of each lemma occurrences in the training set is formed.\n sent4: Each word has a distinct classifier, which will assign labels based on k, where k = min(# of occurrences, 5).\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Unknown words constitute a major source of difficulty for Chinese part-of-speech (POS) tagging, yet relatively little work has been done on POS guessing of Chinese unknown words.",
                "The few existing studies all attempted to develop a unified statistical model to compute the probability of a word having a particular POS category for all Chinese unknown words (#REF; #TARGET_REF; #REF) .",
                "This approach tends to miss one or more pieces of information contributed by the type, length, internal structure, or context of individual unknown words, and fails to combine the strengths of different models.",
                "The rule-based approach was rejected with the claim that rules are bound to overgenerate (#REF) .",
                "In this paper, we present a hybrid model that combines the strengths of a rule-based model with those of two statistical models for this task."
            ],
            "label": [
                "BACKGROUND",
                "MOTIVATION"
            ]
        },
        "input": "sent0: Unknown words constitute a major source of difficulty for Chinese part-of-speech (POS) tagging, yet relatively little work has been done on POS guessing of Chinese unknown words.\n sent1: The few existing studies all attempted to develop a unified statistical model to compute the probability of a word having a particular POS category for all Chinese unknown words (#REF; #TARGET_REF; #REF) .\n sent2: This approach tends to miss one or more pieces of information contributed by the type, length, internal structure, or context of individual unknown words, and fails to combine the strengths of different models.\n sent3: The rule-based approach was rejected with the claim that rules are bound to overgenerate (#REF) .\n sent4: In this paper, we present a hybrid model that combines the strengths of a rule-based model with those of two statistical models for this task.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"MOTIVATION\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "This approach tends to miss one or more pieces of information contributed by the type, length, internal structure, or context of individual unknown words, and fails to combine the strengths of different models.",
                "The rule-based approach was rejected with the claim that rules are bound to overgenerate #TARGET_REF .",
                "In this paper, we present a hybrid model that combines the strengths of a rule-based model with those of two statistical models for this task.",
                "The three models make use of different sources of information.",
                "The rule-based model is sensitive to the type, length, and internal structure of unknown words, with overgeneration controlled by additional constraints."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This approach tends to miss one or more pieces of information contributed by the type, length, internal structure, or context of individual unknown words, and fails to combine the strengths of different models.\n sent1: The rule-based approach was rejected with the claim that rules are bound to overgenerate #TARGET_REF .\n sent2: In this paper, we present a hybrid model that combines the strengths of a rule-based model with those of two statistical models for this task.\n sent3: The three models make use of different sources of information.\n sent4: The rule-based model is sensitive to the type, length, and internal structure of unknown words, with overgeneration controlled by additional constraints.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Several reasons were suggested for rejecting the rule-based approach.",
                "First, #REF claimed that it does not work because the syntactic and semantic information for each character or morpheme is unavailable.",
                "This claim does not fully hold, as the POS information about the component words or morphemes of many unknown words is available in the training lexicon.",
                "Second, #TARGET_REF argued that assigning POS to Chinese unknown words on the basis of the internal structure of those words will \"result in massive overgeneration\" (p. 48).",
                "We will show that overgeneration can be controlled by additional constraints."
            ],
            "label": [
                "BACKGROUND",
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Several reasons were suggested for rejecting the rule-based approach.\n sent1: First, #REF claimed that it does not work because the syntactic and semantic information for each character or morpheme is unavailable.\n sent2: This claim does not fully hold, as the POS information about the component words or morphemes of many unknown words is available in the training lexicon.\n sent3: Second, #TARGET_REF argued that assigning POS to Chinese unknown words on the basis of the internal structure of those words will \"result in massive overgeneration\" (p. 48).\n sent4: We will show that overgeneration can be controlled by additional constraints.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"DIFFERENCES\"], \"context\": [\"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "We propose a hybrid model that combines the strengths of different models to arrive at better results for this task.",
                "The models we will consider are a rule-based model, the trigram model, and the statistical model developed by #TARGET_REF .",
                "Combination of the three models will be based on the evaluation of their individual performances on the training data."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We propose a hybrid model that combines the strengths of different models to arrive at better results for this task.\n sent1: The models we will consider are a rule-based model, the trigram model, and the statistical model developed by #TARGET_REF .\n sent2: Combination of the three models will be based on the evaluation of their individual performances on the training data.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The situation is worse when we use beam search where the shorter translations have more chances to beat the longer ones.",
                "It is in general to normalize the model score by translation length (say length normalization) to eliminate this system bias #TARGET_REF .",
                "Though widely used, length normalization is not a perfect solution.",
                "NMT systems still have under-translation and over-translation problem even with a normalized model.",
                "It is due to the lack of the coverage model that indicates the degree a source word is translated."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The situation is worse when we use beam search where the shorter translations have more chances to beat the longer ones.\n sent1: It is in general to normalize the model score by translation length (say length normalization) to eliminate this system bias #TARGET_REF .\n sent2: Though widely used, length normalization is not a perfect solution.\n sent3: NMT systems still have under-translation and over-translation problem even with a normalized model.\n sent4: It is due to the lack of the coverage model that indicates the degree a source word is translated.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "E.g., #REF developed a coveragebased model to measure the fractional count that a source word is translated during decoding.",
                "It can be jointly learned with the NMT model.",
                "Alternatively, one can rerank the n-best outputs by coverage-sensitive models, but this method just affects the final output list which has a very limited scope #TARGET_REF .",
                "In this paper we present a simple and effective approach by introducing a coverage-based feature into NMT.",
                "Unlike previous studies, we do not resort to developing extra models nor reranking the limited n-best translations."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: E.g., #REF developed a coveragebased model to measure the fractional count that a source word is translated during decoding.\n sent1: It can be jointly learned with the NMT model.\n sent2: Alternatively, one can rerank the n-best outputs by coverage-sensitive models, but this method just affects the final output list which has a very limited scope #TARGET_REF .\n sent3: In this paper we present a simple and effective approach by introducing a coverage-based feature into NMT.\n sent4: Unlike previous studies, we do not resort to developing extra models nor reranking the limited n-best translations.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "In NMT, several good methods have been developed.",
                "The simplest of these is length normalization which penalizes short translations in decoding #TARGET_REF .",
                "More sophisticated methods focus on modeling the coverage problem with extra sub-modules in NMT and require a training process (#REF; #REF) .",
                "Perhaps the most related work to this paper is #REF .",
                "In their work, the coverage problem can be interpreted in a probability story."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In NMT, several good methods have been developed.\n sent1: The simplest of these is length normalization which penalizes short translations in decoding #TARGET_REF .\n sent2: More sophisticated methods focus on modeling the coverage problem with extra sub-modules in NMT and require a training process (#REF; #REF) .\n sent3: Perhaps the most related work to this paper is #REF .\n sent4: In their work, the coverage problem can be interpreted in a probability story.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "In NMT, several good methods have been developed.",
                "The simplest of these is length normalization which penalizes short translations in decoding (#REF) .",
                "More sophisticated methods focus on modeling the coverage problem with extra sub-modules in NMT and require a training process (#REF; #REF) .",
                "Perhaps the most related work to this paper is #TARGET_REF .",
                "In their work, the coverage problem can be interpreted in a probability story."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In NMT, several good methods have been developed.\n sent1: The simplest of these is length normalization which penalizes short translations in decoding (#REF) .\n sent2: More sophisticated methods focus on modeling the coverage problem with extra sub-modules in NMT and require a training process (#REF; #REF) .\n sent3: Perhaps the most related work to this paper is #TARGET_REF .\n sent4: In their work, the coverage problem can be interpreted in a probability story.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Here, we present a coverage score (CS) to describe to what extent the source words are translated.",
                "In principle, the coverage score should be high if the translation covers most words in source sentence, and low if it covers only a few of them.",
                "Given a source position i, we define its coverage as the sum of the past attention probabilities c i = |y| j a ij #TARGET_REF; #REF) .",
                "Then, the coverage score of the sentence pair (x, y) is defined as the sum of the truncated coverage over all positions (See Figure 1 for an 1 As the discussion of the attention mechanism is out of the scope of this work, we refer the reader to #REF ; #REF for more details.",
                "illustration):"
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: Here, we present a coverage score (CS) to describe to what extent the source words are translated.\n sent1: In principle, the coverage score should be high if the translation covers most words in source sentence, and low if it covers only a few of them.\n sent2: Given a source position i, we define its coverage as the sum of the past attention probabilities c i = |y| j a ij #TARGET_REF; #REF) .\n sent3: Then, the coverage score of the sentence pair (x, y) is defined as the sum of the truncated coverage over all positions (See Figure 1 for an 1 As the discussion of the attention mechanism is out of the scope of this work, we refer the reader to #REF ; #REF for more details.\n sent4: illustration):\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "base = base system, LN = length normalization, CP = coverage penalty, and CS = our coverage score.",
                "30k entries for both source and target vocabularies.",
                "For the English-German task, BPE (#REF) was used for better performance.",
                "For comparison, we re-implemented the length normalization (LN) and coverage penalty (CP) methods #TARGET_REF .",
                "We used grid search to tune all hyperparameters on the development set as #REF ."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: base = base system, LN = length normalization, CP = coverage penalty, and CS = our coverage score.\n sent1: 30k entries for both source and target vocabularies.\n sent2: For the English-German task, BPE (#REF) was used for better performance.\n sent3: For comparison, we re-implemented the length normalization (LN) and coverage penalty (CP) methods #TARGET_REF .\n sent4: We used grid search to tune all hyperparameters on the development set as #REF .\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "For comparison, we re-implemented the length normalization (LN) and coverage penalty (CP) methods (#REF) .",
                "We used grid search to tune all hyperparameters on the development set as #TARGET_REF .",
                "Specifically, weights for both CP and our CS are evaluated in interval [0, 1] with step 0.1, while the weight for LN is in interval [0.5, 1.5].",
                "We found that the settings determined with beam size 10 can be reliably applied to larger beam sizes in the preliminary experiments and thus we tuned all systems with beam size 10.",
                "For Chinese-English translation, we used a weight of 1.0 for both LN and CP, and set α = 0.6 and β = 0.4."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: For comparison, we re-implemented the length normalization (LN) and coverage penalty (CP) methods (#REF) .\n sent1: We used grid search to tune all hyperparameters on the development set as #TARGET_REF .\n sent2: Specifically, weights for both CP and our CS are evaluated in interval [0, 1] with step 0.1, while the weight for LN is in interval [0.5, 1.5].\n sent3: We found that the settings determined with beam size 10 can be reliably applied to larger beam sizes in the preliminary experiments and thus we tuned all systems with beam size 10.\n sent4: For Chinese-English translation, we used a weight of 1.0 for both LN and CP, and set α = 0.6 and β = 0.4.\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Note that our way of truncation is different from #TARGET_REF 's, where they clip the coverage into [0, 1] and ignore the fact that a source word may be translated into multiple target words and its coverage should be of a value larger than 1.",
                "For decoding, we incorporate the coverage score into beam search via linear combination with the NMT model score as below,",
                "where y is a partial translation generated during decoding, log P(y|x) is the model score, and α is the coefficient for linear interpolation.",
                "In standard implementation of NMT systems, once a hypothesis is finished, it is removed from the beam and the beam shrinks accordingly.",
                "Here we choose a different decoding strategy."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Note that our way of truncation is different from #TARGET_REF 's, where they clip the coverage into [0, 1] and ignore the fact that a source word may be translated into multiple target words and its coverage should be of a value larger than 1.\n sent1: For decoding, we incorporate the coverage score into beam search via linear combination with the NMT model score as below,\n sent2: where y is a partial translation generated during decoding, log P(y|x) is the model score, and α is the coefficient for linear interpolation.\n sent3: In standard implementation of NMT systems, once a hypothesis is finished, it is removed from the beam and the beam shrinks accordingly.\n sent4: Here we choose a different decoding strategy.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "However, it fails to account for the cases that one source word is translated into multiple target words and is thus of a total attention score > 1.",
                "To address this issue, we remove the probability constraint and make the coverage score interpretable for different cases.",
                "Another difference lies in that our coverage model is applied to every beam search step, while #TARGET_REF 's model affects only a small number of translation outputs.",
                "Previous work have pointed out that BLEU scores of NMT systems drop as beam size increases (#REF; #REF; #REF) , and the existing length normalization and coverage models can alleviate this problem to some extent.",
                "In this work we show that our method can do this much better."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: However, it fails to account for the cases that one source word is translated into multiple target words and is thus of a total attention score > 1.\n sent1: To address this issue, we remove the probability constraint and make the coverage score interpretable for different cases.\n sent2: Another difference lies in that our coverage model is applied to every beam search step, while #TARGET_REF 's model affects only a small number of translation outputs.\n sent3: Previous work have pointed out that BLEU scores of NMT systems drop as beam size increases (#REF; #REF; #REF) , and the existing length normalization and coverage models can alleviate this problem to some extent.\n sent4: In this work we show that our method can do this much better.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The automatic identification of information status (#REF; 1992) , i.e. categorizing discourse entities into different classes on the given-new scale, has recently been identified as an important issue in natural language processing (#REF; #TARGET_REF; .",
                "It is widely acknowledged that information status and, more generally, information structure, 1 is reflected in word order, in the form of referring expressions as well as in prosody.",
                "In computational linguistics, the ability to automatically label text with information status, therefore, could be of great benefit to many applications, including surface realization, text-to-speech synthesis, anaphora resolution, summarization, etc.",
                "The task of automatically labeling text with information status, however, is a difficult one.",
                "Part of the difficulty arises from the fact that, to a certain degree, such labeling requires world knowledge and semantic comprehension of the text, but another obstacle is simply that theoretical notions of information status are not used consistently in the literature."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: The automatic identification of information status (#REF; 1992) , i.e. categorizing discourse entities into different classes on the given-new scale, has recently been identified as an important issue in natural language processing (#REF; #TARGET_REF; .\n sent1: It is widely acknowledged that information status and, more generally, information structure, 1 is reflected in word order, in the form of referring expressions as well as in prosody.\n sent2: In computational linguistics, the ability to automatically label text with information status, therefore, could be of great benefit to many applications, including surface realization, text-to-speech synthesis, anaphora resolution, summarization, etc.\n sent3: The task of automatically labeling text with information status, however, is a difficult one.\n sent4: Part of the difficulty arises from the fact that, to a certain degree, such labeling requires world knowledge and semantic comprehension of the text, but another obstacle is simply that theoretical notions of information status are not used consistently in the literature.\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "This does not rule out further subclassification (known/unknown) or the possibility of using machine learning techniques to identify this distinction, see #REF .",
                "The fact that #TARGET_REF report the highest confusion rate between NEW and MEDIATED entities may have its roots in this issue.",
                "New.",
                "Only (specific) indefinites are labeled NEW.",
                "Generic."
            ],
            "label": [
                "MOTIVATION",
                "BACKGROUND"
            ]
        },
        "input": "sent0: This does not rule out further subclassification (known/unknown) or the possibility of using machine learning techniques to identify this distinction, see #REF .\n sent1: The fact that #TARGET_REF report the highest confusion rate between NEW and MEDIATED entities may have its roots in this issue.\n sent2: New.\n sent3: Only (specific) indefinites are labeled NEW.\n sent4: Generic.\n",
        "output": "{\"label\": [\"MOTIVATION\", \"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Identifying discourse-new expressions in texts is helpful as a precursor to coreference resolution, since, by definition, there is no need to identify antecedents for new entities.",
                "In the linguistic literature, referring expressions have been distinguished in much more detail, and there is reason to believe that this could also provide useful information for NLP applications.",
                "#REF and #TARGET_REF developed methods to automatically identify three different classes: OLD, MEDIATED and NEW expressions.",
                "This classification, which is described in #REF , has been used for annotating the Switchboard dialog corpus (#REF) , on which both studies are based.",
                "Most recently, #REF extend their automatic prediction system to a more fine-grained set of 16 subtypes."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Identifying discourse-new expressions in texts is helpful as a precursor to coreference resolution, since, by definition, there is no need to identify antecedents for new entities.\n sent1: In the linguistic literature, referring expressions have been distinguished in much more detail, and there is reason to believe that this could also provide useful information for NLP applications.\n sent2: #REF and #TARGET_REF developed methods to automatically identify three different classes: OLD, MEDIATED and NEW expressions.\n sent3: This classification, which is described in #REF , has been used for annotating the Switchboard dialog corpus (#REF) , on which both studies are based.\n sent4: Most recently, #REF extend their automatic prediction system to a more fine-grained set of 16 subtypes.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "b. Klose scored a penalty in the 80th minute.",
                "Based on work described in #REF , #TARGET_REF develop a machine learning approach to information-status determination.",
                "They develop a support vector machine (SVM) model from the annotated Switchboard dialogs in order to predict the three possible classes.",
                "In an extension of this work, #REF compare a rule-based system to a classifier with features based on the rules to predict 16 subtypes of the three basic types.",
                "On this extended label set on the dialog data, they achieve accuracy of 86.4% with gold standard coreference and 78.7% with automatically detected coreference."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: b. Klose scored a penalty in the 80th minute.\n sent1: Based on work described in #REF , #TARGET_REF develop a machine learning approach to information-status determination.\n sent2: They develop a support vector machine (SVM) model from the annotated Switchboard dialogs in order to predict the three possible classes.\n sent3: In an extension of this work, #REF compare a rule-based system to a classifier with features based on the rules to predict 16 subtypes of the three basic types.\n sent4: On this extended label set on the dialog data, they achieve accuracy of 86.4% with gold standard coreference and 78.7% with automatically detected coreference.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We parse each sentence with the German Lexical Functional Grammar of #REF using the XLE parser in order to automati- #REF show that there are asymmetries between pairs of information status labels contained in sentences, i.e. certain classes of expressions tend to precede certain other classes.",
                "We therefore treat the prediction of IS labels as a sequence labeling task.",
                "4 We train a CRF using wapiti (#REF) , with the features outlined in Table 1 .",
                "We also include a basic \"coreference\" feature, similar to the lexical features of #TARGET_REF , that fires if there is some lexical overlap of nouns (or compound nouns) in the preceding 10 sentences.",
                "The original label set described in contains 21 labels."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: We parse each sentence with the German Lexical Functional Grammar of #REF using the XLE parser in order to automati- #REF show that there are asymmetries between pairs of information status labels contained in sentences, i.e. certain classes of expressions tend to precede certain other classes.\n sent1: We therefore treat the prediction of IS labels as a sequence labeling task.\n sent2: 4 We train a CRF using wapiti (#REF) , with the features outlined in Table 1 .\n sent3: We also include a basic \"coreference\" feature, similar to the lexical features of #TARGET_REF , that fires if there is some lexical overlap of nouns (or compound nouns) in the preceding 10 sentences.\n sent4: The original label set described in contains 21 labels.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The aim of these models is to identify abusive language that directly targets certain individuals or groups, particularly people belonging to protected categories (#REF) .",
                "Bias may reduce the accuracy of these models, and at worst, will mean that the models actively discriminate against the same groups they are designed to protect.",
                "Our study focuses on racial bias in hate speech and abusive language detection datasets (#REF; #TARGET_REF; #REF; #REF) , all of which use data collected from Twitter.",
                "We train classifiers using each of the datasets and use a corpus of tweets with demographic information to compare how each classifier performs on tweets written in African-American English (AAE) versus Standard American English (SAE) (#REF) .",
                "We use bootstrap sampling (#REF) to estimate the proportion of tweets in each group that each classifier assigns to each class."
            ],
            "label": [
                "MOTIVATION",
                "BACKGROUND"
            ]
        },
        "input": "sent0: The aim of these models is to identify abusive language that directly targets certain individuals or groups, particularly people belonging to protected categories (#REF) .\n sent1: Bias may reduce the accuracy of these models, and at worst, will mean that the models actively discriminate against the same groups they are designed to protect.\n sent2: Our study focuses on racial bias in hate speech and abusive language detection datasets (#REF; #TARGET_REF; #REF; #REF) , all of which use data collected from Twitter.\n sent3: We train classifiers using each of the datasets and use a corpus of tweets with demographic information to compare how each classifier performs on tweets written in African-American English (AAE) versus Standard American English (SAE) (#REF) .\n sent4: We use bootstrap sampling (#REF) to estimate the proportion of tweets in each group that each classifier assigns to each class.\n",
        "output": "{\"label\": [\"MOTIVATION\", \"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF collected 130k tweets containing one of seventeen different terms or phrases they considered to be hateful.",
                "They then annotated a sample of these tweets themselves, using guidelines inspired by critical race theory.",
                "These annotators were then reviewed by \"a 25 year old woman studying gender studies and a nonactivist feminist\" to check for bias.",
                "This dataset consists of 16,849 tweets labeled as either racism, sexism, or neither.",
                "Most of the tweets categorized as sexist relate to debates over an Australian TV show and most of those considered as racist are anti-Muslim."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF collected 130k tweets containing one of seventeen different terms or phrases they considered to be hateful.\n sent1: They then annotated a sample of these tweets themselves, using guidelines inspired by critical race theory.\n sent2: These annotators were then reviewed by \"a 25 year old woman studying gender studies and a nonactivist feminist\" to check for bias.\n sent3: This dataset consists of 16,849 tweets labeled as either racism, sexism, or neither.\n sent4: Most of the tweets categorized as sexist relate to debates over an Australian TV show and most of those considered as racist are anti-Muslim.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "In both cases the classifiers trained upon their data are still more likely to flag black-aligned tweets as sexism.",
                "The #TARGET_REF classifier is particularly sensitive to the word \"b*tch\" with 96% of black-aligned and 94% of white-aligned tweets predicted to belong to this class.",
                "For almost all of these tweets are classified as offensive, however those in the blackaligned corpus are 1.15 times as frequently classified as hate speech.",
                "We see a very similar result for #REF compared to the previous experiment, with black-aligned tweets flagged as harassment at 1.1 times the rate of those in the white-aligned corpus.",
                "Finally, for the #REF classifier we see a substantial racial disparity, with black-aligned tweets classified as hate speech at 2.7 times the rate of white aligned ones, a higher rate than in Experiment 1."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In both cases the classifiers trained upon their data are still more likely to flag black-aligned tweets as sexism.\n sent1: The #TARGET_REF classifier is particularly sensitive to the word \"b*tch\" with 96% of black-aligned and 94% of white-aligned tweets predicted to belong to this class.\n sent2: For almost all of these tweets are classified as offensive, however those in the blackaligned corpus are 1.15 times as frequently classified as hate speech.\n sent3: We see a very similar result for #REF compared to the previous experiment, with black-aligned tweets flagged as harassment at 1.1 times the rate of those in the white-aligned corpus.\n sent4: Finally, for the #REF classifier we see a substantial racial disparity, with black-aligned tweets classified as hate speech at 2.7 times the rate of white aligned ones, a higher rate than in Experiment 1.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "In particular, future work should focus on the following three areas.",
                "First, we expect that some biases emerge at the point of data collection.",
                "Some studies sampled tweets using small, ad hoc sets of keywords created by the authors #TARGET_REF; #REF; #REF) , an approach demonstrated to produce poor results (#REF) .",
                "Others start with large crowdsourced dictionaries of keywords, which tend to include many irrelevant terms, resulting in high rates of false positives #REF) .",
                "In both cases, by using keywords to identify relevant tweets we are likely to get non-representative samples of training data that may over-or under-represent certain communities."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In particular, future work should focus on the following three areas.\n sent1: First, we expect that some biases emerge at the point of data collection.\n sent2: Some studies sampled tweets using small, ad hoc sets of keywords created by the authors #TARGET_REF; #REF; #REF) , an approach demonstrated to produce poor results (#REF) .\n sent3: Others start with large crowdsourced dictionaries of keywords, which tend to include many irrelevant terms, resulting in high rates of false positives #REF) .\n sent4: In both cases, by using keywords to identify relevant tweets we are likely to get non-representative samples of training data that may over-or under-represent certain communities.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Second, we expect that the people who annotate data have their own biases.",
                "Since individual biases in reflect societal prejudices, they aggregate into systematic biases in training data.",
                "The datasets considered here relied upon a range of different annotators, from the authors (#REF; #TARGET_REF and crowdworkers #REF) to activists (#REF) .",
                "Even the classifier trained on expert-labeled data (#REF) flags black-aligned tweets as sexist at almost twice the rate of white-aligned tweets.",
                "While we agree that there is value in working with domain-experts to annotate data, these results suggest that activists may be prone to similar biases as academics and crowdworkers."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Second, we expect that the people who annotate data have their own biases.\n sent1: Since individual biases in reflect societal prejudices, they aggregate into systematic biases in training data.\n sent2: The datasets considered here relied upon a range of different annotators, from the authors (#REF; #TARGET_REF and crowdworkers #REF) to activists (#REF) .\n sent3: Even the classifier trained on expert-labeled data (#REF) flags black-aligned tweets as sexist at almost twice the rate of white-aligned tweets.\n sent4: While we agree that there is value in working with domain-experts to annotate data, these results suggest that activists may be prone to similar biases as academics and crowdworkers.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The results of Experiment 1 are shown in Table 2.",
                "We observe substantial racial disparities in the performance of all classifiers.",
                "In all but one of the comparisons, there are statistically significant (p < 0.001) differences and in all but one of these we see that tweets in the black-aligned corpus are assigned negative labels more frequently than those by whites.",
                "The only case where blackaligned tweets are classified into a negative class less frequently than white-aligned tweets is the racism class in the #TARGET_REF classifier.",
                "Note, however, the extremely low rate at which tweets are predicted to belong to this class for both groups."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: The results of Experiment 1 are shown in Table 2.\n sent1: We observe substantial racial disparities in the performance of all classifiers.\n sent2: In all but one of the comparisons, there are statistically significant (p < 0.001) differences and in all but one of these we see that tweets in the black-aligned corpus are assigned negative labels more frequently than those by whites.\n sent3: The only case where blackaligned tweets are classified into a negative class less frequently than white-aligned tweets is the racism class in the #TARGET_REF classifier.\n sent4: Note, however, the extremely low rate at which tweets are predicted to belong to this class for both groups.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "While some of the remaining disparities are likely due to differences in the distributions of other keywords we did not condition on, we expect that other more innocuous aspects of black-aligned language may be associated with negative labels in the training data, leading classifiers to disproportionately predict that tweets by African-Americans belong to negative classes.",
                "We now discuss the results as they pertain to each of the datasets used.",
                "Classifiers trained on data from #TARGET_REF and #REF only predicted a small fraction of the tweets to be racism.",
                "We suspect that this is due to the composition of their dataset, since the majority of the racist training examples consist of anti-Muslim rather than anti- Table 4 : Experiment 2, t = \"b*tch\" black language.",
                "Across both datasets the words \"n*gger\" and \"n*gga\" appear in 4 and 10 tweets respectively."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: While some of the remaining disparities are likely due to differences in the distributions of other keywords we did not condition on, we expect that other more innocuous aspects of black-aligned language may be associated with negative labels in the training data, leading classifiers to disproportionately predict that tweets by African-Americans belong to negative classes.\n sent1: We now discuss the results as they pertain to each of the datasets used.\n sent2: Classifiers trained on data from #TARGET_REF and #REF only predicted a small fraction of the tweets to be racism.\n sent3: We suspect that this is due to the composition of their dataset, since the majority of the racist training examples consist of anti-Muslim rather than anti- Table 4 : Experiment 2, t = \"b*tch\" black language.\n sent4: Across both datasets the words \"n*gger\" and \"n*gga\" appear in 4 and 10 tweets respectively.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "It also classifies black-aligned tweets as spam 1.8 times as frequently.",
                "The results of Experiment 2 are consistent with the previous results, although there are some notable differences.",
                "In most cases the racial disparities persist, although they are generally smaller in magnitude and in some cases the direction even changes.",
                "Table 3 shows that for tweets containing the word \"n*gga\", classifiers trained on #TARGET_REF and #REF are both predict black-aligned tweets to be instances of sexism approximately 1.5 times as often as white-aligned tweets.",
                "The classifier trained on the data is significantly less likely to classify black-aligned tweets as hate speech, although it is more likely to classify them as offensive."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: It also classifies black-aligned tweets as spam 1.8 times as frequently.\n sent1: The results of Experiment 2 are consistent with the previous results, although there are some notable differences.\n sent2: In most cases the racial disparities persist, although they are generally smaller in magnitude and in some cases the direction even changes.\n sent3: Table 3 shows that for tweets containing the word \"n*gga\", classifiers trained on #TARGET_REF and #REF are both predict black-aligned tweets to be instances of sexism approximately 1.5 times as often as white-aligned tweets.\n sent4: The classifier trained on the data is significantly less likely to classify black-aligned tweets as hate speech, although it is more likely to classify them as offensive.\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "For the #REF classifier we see that black-aligned tweets are slightly less frequently considered to be hate speech but are much more frequently classified as abusive.",
                "The results for the second variation of Experiment 2 where we conditioned on the word \"b*tch\" are shown in Table 4 .",
                "We see similar results for #TARGET_REF and #REF .",
                "In both cases the classifiers trained upon their data are still more likely to flag black-aligned tweets as sexism.",
                "The #REF classifier is particularly sensitive to the word \"b*tch\" with 96% of black-aligned and 94% of white-aligned tweets predicted to belong to this class."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: For the #REF classifier we see that black-aligned tweets are slightly less frequently considered to be hate speech but are much more frequently classified as abusive.\n sent1: The results for the second variation of Experiment 2 where we conditioned on the word \"b*tch\" are shown in Table 4 .\n sent2: We see similar results for #TARGET_REF and #REF .\n sent3: In both cases the classifiers trained upon their data are still more likely to flag black-aligned tweets as sexism.\n sent4: The #REF classifier is particularly sensitive to the word \"b*tch\" with 96% of black-aligned and 94% of white-aligned tweets predicted to belong to this class.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "NER research presented here involves development of new statistical and hybrid approaches to identification and disambiguation of gene [1] , protein [2] , chemical names [3] , and clinical entities.",
                "Overwhelmingly, researchers chose statistical or hybrid approaches to the tasks at hand.",
                "This is probably the reason for growing interest in creation of annotated corpora [4] , development of methods for augmenting the existing annotation [5] , speeding up the annotation process [5] , and reducing its cost; evaluating the comparability of results obtained applying the same methods to different collections #TARGET_REF , And increasing compatibility of different annotations [7] .",
                "Increasingly sophisticated relation extraction methods [6, 8] are being applied to a broader set of iii relations [9] .",
                "Other steps towards deeper understanding of the text include methods for creation of gene profiles [10] , identification of uncertainty [11] , discourse connectivity [12] , and temporal features of clinical conditions [13] ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: NER research presented here involves development of new statistical and hybrid approaches to identification and disambiguation of gene [1] , protein [2] , chemical names [3] , and clinical entities.\n sent1: Overwhelmingly, researchers chose statistical or hybrid approaches to the tasks at hand.\n sent2: This is probably the reason for growing interest in creation of annotated corpora [4] , development of methods for augmenting the existing annotation [5] , speeding up the annotation process [5] , and reducing its cost; evaluating the comparability of results obtained applying the same methods to different collections #TARGET_REF , And increasing compatibility of different annotations [7] .\n sent3: Increasingly sophisticated relation extraction methods [6, 8] are being applied to a broader set of iii relations [9] .\n sent4: Other steps towards deeper understanding of the text include methods for creation of gene profiles [10] , identification of uncertainty [11] , discourse connectivity [12] , and temporal features of clinical conditions [13] .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "NER research presented here involves development of new statistical and hybrid approaches to identification and disambiguation of gene [1] , protein [2] , chemical names [3] , and clinical entities.",
                "Overwhelmingly, researchers chose statistical or hybrid approaches to the tasks at hand.",
                "This is probably the reason for growing interest in creation of annotated corpora [4] , development of methods for augmenting the existing annotation [5] , speeding up the annotation process [5] , and reducing its cost; evaluating the comparability of results obtained applying the same methods to different collections [6] , And increasing compatibility of different annotations [7] .",
                "Increasingly sophisticated relation extraction methods #TARGET_REF 8] are being applied to a broader set of iii relations [9] .",
                "Other steps towards deeper understanding of the text include methods for creation of gene profiles [10] , identification of uncertainty [11] , discourse connectivity [12] , and temporal features of clinical conditions [13] ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: NER research presented here involves development of new statistical and hybrid approaches to identification and disambiguation of gene [1] , protein [2] , chemical names [3] , and clinical entities.\n sent1: Overwhelmingly, researchers chose statistical or hybrid approaches to the tasks at hand.\n sent2: This is probably the reason for growing interest in creation of annotated corpora [4] , development of methods for augmenting the existing annotation [5] , speeding up the annotation process [5] , and reducing its cost; evaluating the comparability of results obtained applying the same methods to different collections [6] , And increasing compatibility of different annotations [7] .\n sent3: Increasingly sophisticated relation extraction methods #TARGET_REF 8] are being applied to a broader set of iii relations [9] .\n sent4: Other steps towards deeper understanding of the text include methods for creation of gene profiles [10] , identification of uncertainty [11] , discourse connectivity [12] , and temporal features of clinical conditions [13] .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "There are also uses of KWS where a view simple speech commands (e.g. \"on\", \"off\") are enough to interact with a device such as a voice-controlled light bulb.",
                "Conventional hybrid approaches to KWS first divide their audio signal in time frames to extract features, e.g., Mel Frequency Cepstral Coefficients (MFCC).",
                "A neural net then estimates phoneme or state posteriors of the keyword Hidden Markov Model in order to calculate the keyword probability using a Viterbi search.",
                "In recent years, end-to-end architectures gained traction that directly classify keyword posterior probabilites based on the previously extracted features, e.g., [1, #TARGET_REF 3, 4, 5] .",
                "Typical application scenarios imply that the device is powered by a battery, and possesses restricted hardware resources to reduce costs."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: There are also uses of KWS where a view simple speech commands (e.g. \"on\", \"off\") are enough to interact with a device such as a voice-controlled light bulb.\n sent1: Conventional hybrid approaches to KWS first divide their audio signal in time frames to extract features, e.g., Mel Frequency Cepstral Coefficients (MFCC).\n sent2: A neural net then estimates phoneme or state posteriors of the keyword Hidden Markov Model in order to calculate the keyword probability using a Viterbi search.\n sent3: In recent years, end-to-end architectures gained traction that directly classify keyword posterior probabilites based on the previously extracted features, e.g., [1, #TARGET_REF 3, 4, 5] .\n sent4: Typical application scenarios imply that the device is powered by a battery, and possesses restricted hardware resources to reduce costs.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "To the best of our knowledge, we are the first to apply this method to the task of KWS.",
                "The first convolutional layer of our model is inspired by SincNet and we combine it with DSCconv.",
                "DSCconvs have first been introduced in the domain of Image Processing [8, 13] and have been applied to other domains since: Zhang et al. applied DSCconv to KWS #TARGET_REF .",
                "Kaiser et al. used DSConv for neural machine translation [7] .",
                "They also introduce the \"super-separable\" convolution, a DSConv that also uses grouping and thus reduces the already small number of parameters of DSConv even further."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: To the best of our knowledge, we are the first to apply this method to the task of KWS.\n sent1: The first convolutional layer of our model is inspired by SincNet and we combine it with DSCconv.\n sent2: DSCconvs have first been introduced in the domain of Image Processing [8, 13] and have been applied to other domains since: Zhang et al. applied DSCconv to KWS #TARGET_REF .\n sent3: Kaiser et al. used DSConv for neural machine translation [7] .\n sent4: They also introduce the \"super-separable\" convolution, a DSConv that also uses grouping and thus reduces the already small number of parameters of DSConv even further.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "DSConv have been successfully applied to the domain of computer vision [8, 13] , neural translation [7] and KWS #TARGET_REF .",
                "Fig. 3 provides an overview of the steps from a regular convolution to the GDSConv.",
                "The number of parameters of one DSConv layer amounts to N DSConv = k · c in + c in · c out with the kernel size k and the number of input and output channels c in and c out respectively; the first summand is determined by the depthwise convolution, the second summand by the pointwise convolution [7] .",
                "In our model configuration, the depthwise convolution only accounts for roughly 5% of parameters in this layer, the pointwise for 95%.",
                "We therefore reduced the parameters of the pointwise convolution using grouping by a factor g to N GDSConv = k · c in + cin·cout g , rather than the parameters in the depthwise convolution."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: DSConv have been successfully applied to the domain of computer vision [8, 13] , neural translation [7] and KWS #TARGET_REF .\n sent1: Fig. 3 provides an overview of the steps from a regular convolution to the GDSConv.\n sent2: The number of parameters of one DSConv layer amounts to N DSConv = k · c in + c in · c out with the kernel size k and the number of input and output channels c in and c out respectively; the first summand is determined by the depthwise convolution, the second summand by the pointwise convolution [7] .\n sent3: In our model configuration, the depthwise convolution only accounts for roughly 5% of parameters in this layer, the pointwise for 95%.\n sent4: We therefore reduced the parameters of the pointwise convolution using grouping by a factor g to N GDSConv = k · c in + cin·cout g , rather than the parameters in the depthwise convolution.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "• We propose a neural network architecture tuned towards energy efficiency in microcontrollers grounded on the observation that memory access is costly, while computation is cheap [9] .",
                "• Our keyword-spotting network classifies on raw audio employing SincConvs while at the same time reducing the number of parameters using (G)DSConvs.",
                "[3] , while keeping the number of parameters comparable to #TARGET_REF .",
                "Choi et al. build on this work as they also use a ResNet-inspired architecture.",
                "Instead of using 2D convolution over a time-frequency representation of the data they convolve along the time dimension and treat the frequency dimension as channels [4] ."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: • We propose a neural network architecture tuned towards energy efficiency in microcontrollers grounded on the observation that memory access is costly, while computation is cheap [9] .\n sent1: • Our keyword-spotting network classifies on raw audio employing SincConvs while at the same time reducing the number of parameters using (G)DSConvs.\n sent2: [3] , while keeping the number of parameters comparable to #TARGET_REF .\n sent3: Choi et al. build on this work as they also use a ResNet-inspired architecture.\n sent4: Instead of using 2D convolution over a time-frequency representation of the data they convolve along the time dimension and treat the frequency dimension as channels [4] .\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Typical application scenarios for smart devices imply that the device is powered by a battery, and possesses restricted hardware resources.",
                "The requirements for a KWS system in these scenarios are (1) very low power consumption to maximize battery life, (2) real-time or near real-time capability, (3) low memory footprint and (4) high accuracy to avoid random activations and to ensure responsiveness.",
                "Regarding real-time capability, our model is designed to operate on a single-core microcontroller capable of 50 MOps per second #TARGET_REF .",
                "We assume that in microcontrollers the memory consumption of a KWS neural network is associated with its power consumption: Reading memory values contributes most to power consumption which makes re-use of weights favorable.",
                "While in general large memory modules leak more power than small memory modules, one read operation from RAM costs far more energy than the corresponding multiply-and-accumulate computation [16, 9] ."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: Typical application scenarios for smart devices imply that the device is powered by a battery, and possesses restricted hardware resources.\n sent1: The requirements for a KWS system in these scenarios are (1) very low power consumption to maximize battery life, (2) real-time or near real-time capability, (3) low memory footprint and (4) high accuracy to avoid random activations and to ensure responsiveness.\n sent2: Regarding real-time capability, our model is designed to operate on a single-core microcontroller capable of 50 MOps per second #TARGET_REF .\n sent3: We assume that in microcontrollers the memory consumption of a KWS neural network is associated with its power consumption: Reading memory values contributes most to power consumption which makes re-use of weights favorable.\n sent4: While in general large memory modules leak more power than small memory modules, one read operation from RAM costs far more energy than the corresponding multiply-and-accumulate computation [16, 9] .\n",
        "output": "{\"label\": [null], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Table 1 lists these results in comparison with related work.",
                "Compared to the DSConv network in #TARGET_REF , our network is more efficient in terms of accuracy for a given parameter count.",
                "Their biggest model has a 1.2% lower accuracy than our base model while having about 4 times the parameters.",
                "#REF has the most competitive results while we are still able to improve upon their accuracy for a given number of parameters.",
                "They are using 1D convolution along the time dimension as well which may be evidence that this yields better performance for audio processing or at least KWS."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Table 1 lists these results in comparison with related work.\n sent1: Compared to the DSConv network in #TARGET_REF , our network is more efficient in terms of accuracy for a given parameter count.\n sent2: Their biggest model has a 1.2% lower accuracy than our base model while having about 4 times the parameters.\n sent3: #REF has the most competitive results while we are still able to improve upon their accuracy for a given number of parameters.\n sent4: They are using 1D convolution along the time dimension as well which may be evidence that this yields better performance for audio processing or at least KWS.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Large-scale distributional thesauri created automatically from corpora (#REF; #TARGET_REF; #REF; #REF) are an inexpensive and fast alternative for representing semantic relatedness between words, when manually constructed resources like WordNet (#REF ) are unavailable or lack coverage.",
                "To construct a distributional thesaurus, the (collocational or syntactic) contexts in which a target word occurs are used as the basis for calculating its similarity with other words.",
                "That is, two words are similar if they share a large proportion of contexts.",
                "Much attention has been devoted to refining thesaurus quality, improving informativeness and similarity measures (#REF; #REF; #REF) , identifying and demoting bad neighbors (#REF) , or using more relevant contexts (#REF; #REF) .",
                "For the latter in particular, as words vary in their collocational tendencies, it is difficult to determine how informative a given context is."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Large-scale distributional thesauri created automatically from corpora (#REF; #TARGET_REF; #REF; #REF) are an inexpensive and fast alternative for representing semantic relatedness between words, when manually constructed resources like WordNet (#REF ) are unavailable or lack coverage.\n sent1: To construct a distributional thesaurus, the (collocational or syntactic) contexts in which a target word occurs are used as the basis for calculating its similarity with other words.\n sent2: That is, two words are similar if they share a large proportion of contexts.\n sent3: Much attention has been devoted to refining thesaurus quality, improving informativeness and similarity measures (#REF; #REF; #REF) , identifying and demoting bad neighbors (#REF) , or using more relevant contexts (#REF; #REF) .\n sent4: For the latter in particular, as words vary in their collocational tendencies, it is difficult to determine how informative a given context is.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Large-scale distributional thesauri created automatically from corpora (#REF; #REF; #REF; #REF) are an inexpensive and fast alternative for representing semantic relatedness between words, when manually constructed resources like WordNet (#REF ) are unavailable or lack coverage.",
                "To construct a distributional thesaurus, the (collocational or syntactic) contexts in which a target word occurs are used as the basis for calculating its similarity with other words.",
                "That is, two words are similar if they share a large proportion of contexts.",
                "Much attention has been devoted to refining thesaurus quality, improving informativeness and similarity measures #TARGET_REF; #REF; #REF) , identifying and demoting bad neighbors (#REF) , or using more relevant contexts (#REF; #REF) .",
                "For the latter in particular, as words vary in their collocational tendencies, it is difficult to determine how informative a given context is."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Large-scale distributional thesauri created automatically from corpora (#REF; #REF; #REF; #REF) are an inexpensive and fast alternative for representing semantic relatedness between words, when manually constructed resources like WordNet (#REF ) are unavailable or lack coverage.\n sent1: To construct a distributional thesaurus, the (collocational or syntactic) contexts in which a target word occurs are used as the basis for calculating its similarity with other words.\n sent2: That is, two words are similar if they share a large proportion of contexts.\n sent3: Much attention has been devoted to refining thesaurus quality, improving informativeness and similarity measures #TARGET_REF; #REF; #REF) , identifying and demoting bad neighbors (#REF) , or using more relevant contexts (#REF; #REF) .\n sent4: For the latter in particular, as words vary in their collocational tendencies, it is difficult to determine how informative a given context is.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "In a nutshell, the standard approach to build a distributional thesaurus consists of: (i) the extraction of contexts for the target words from corpora, (ii) the application of an informativeness measure to represent these contexts and (iii) the application of a similarity measure to compare sets of contexts.",
                "The contexts in which a target word appears can be extracted in terms of a window of cooccurring (content) words surrounding the target (#REF; #REF; #REF) or in terms of the syntactic dependencies in which the target appears #TARGET_REF; #REF; #REF) .",
                "The informativeness of each context is calculated using measures like PMI, and t-test while the similarity between contexts is calculated using measures like Lin's (1998) , cosine, Jensen-Shannon divergence, Dice or Jaccard.",
                "Evaluation of the quality of distributional thesauri is a well know problem in the area (#REF; #REF) .",
                "For instance, for intrinsic evaluation, the agreement between thesauri has been examined, looking at the average similarity of a word in the thesauri (#REF) , and at the overlap and rank agreement between the thesauri for target words like nouns (#REF) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In a nutshell, the standard approach to build a distributional thesaurus consists of: (i) the extraction of contexts for the target words from corpora, (ii) the application of an informativeness measure to represent these contexts and (iii) the application of a similarity measure to compare sets of contexts.\n sent1: The contexts in which a target word appears can be extracted in terms of a window of cooccurring (content) words surrounding the target (#REF; #REF; #REF) or in terms of the syntactic dependencies in which the target appears #TARGET_REF; #REF; #REF) .\n sent2: The informativeness of each context is calculated using measures like PMI, and t-test while the similarity between contexts is calculated using measures like Lin's (1998) , cosine, Jensen-Shannon divergence, Dice or Jaccard.\n sent3: Evaluation of the quality of distributional thesauri is a well know problem in the area (#REF; #REF) .\n sent4: For instance, for intrinsic evaluation, the agreement between thesauri has been examined, looking at the average similarity of a word in the thesauri (#REF) , and at the overlap and rank agreement between the thesauri for target words like nouns (#REF) .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "In a nutshell, the standard approach to build a distributional thesaurus consists of: (i) the extraction of contexts for the target words from corpora, (ii) the application of an informativeness measure to represent these contexts and (iii) the application of a similarity measure to compare sets of contexts.",
                "The contexts in which a target word appears can be extracted in terms of a window of cooccurring (content) words surrounding the target (#REF; #REF; #REF) or in terms of the syntactic dependencies in which the target appears (#REF; #REF; #REF) .",
                "The informativeness of each context is calculated using measures like PMI, and t-test while the similarity between contexts is calculated using measures like #TARGET_REF , cosine, Jensen-Shannon divergence, Dice or Jaccard.",
                "Evaluation of the quality of distributional thesauri is a well know problem in the area (#REF; #REF) .",
                "For instance, for intrinsic evaluation, the agreement between thesauri has been examined, looking at the average similarity of a word in the thesauri (#REF) , and at the overlap and rank agreement between the thesauri for target words like nouns (#REF) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In a nutshell, the standard approach to build a distributional thesaurus consists of: (i) the extraction of contexts for the target words from corpora, (ii) the application of an informativeness measure to represent these contexts and (iii) the application of a similarity measure to compare sets of contexts.\n sent1: The contexts in which a target word appears can be extracted in terms of a window of cooccurring (content) words surrounding the target (#REF; #REF; #REF) or in terms of the syntactic dependencies in which the target appears (#REF; #REF; #REF) .\n sent2: The informativeness of each context is calculated using measures like PMI, and t-test while the similarity between contexts is calculated using measures like #TARGET_REF , cosine, Jensen-Shannon divergence, Dice or Jaccard.\n sent3: Evaluation of the quality of distributional thesauri is a well know problem in the area (#REF; #REF) .\n sent4: For instance, for intrinsic evaluation, the agreement between thesauri has been examined, looking at the average similarity of a word in the thesauri (#REF) , and at the overlap and rank agreement between the thesauri for target words like nouns (#REF) .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The contexts in which a target word appears can be extracted in terms of a window of cooccurring (content) words surrounding the target (#REF; #REF; #REF) or in terms of the syntactic dependencies in which the target appears (#REF; #REF; #REF) .",
                "The informativeness of each context is calculated using measures like PMI, and t-test while the similarity between contexts is calculated using measures like Lin's (1998) , cosine, Jensen-Shannon divergence, Dice or Jaccard.",
                "Evaluation of the quality of distributional thesauri is a well know problem in the area #TARGET_REF; #REF) .",
                "For instance, for intrinsic evaluation, the agreement between thesauri has been examined, looking at the average similarity of a word in the thesauri (#REF) , and at the overlap and rank agreement between the thesauri for target words like nouns (#REF) .",
                "Although much attention has been given to the evaluation of various informativeness and similarity measures, a careful assessment of the effects of filtering on the resulting thesauri is also needed."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The contexts in which a target word appears can be extracted in terms of a window of cooccurring (content) words surrounding the target (#REF; #REF; #REF) or in terms of the syntactic dependencies in which the target appears (#REF; #REF; #REF) .\n sent1: The informativeness of each context is calculated using measures like PMI, and t-test while the similarity between contexts is calculated using measures like Lin's (1998) , cosine, Jensen-Shannon divergence, Dice or Jaccard.\n sent2: Evaluation of the quality of distributional thesauri is a well know problem in the area #TARGET_REF; #REF) .\n sent3: For instance, for intrinsic evaluation, the agreement between thesauri has been examined, looking at the average similarity of a word in the thesauri (#REF) , and at the overlap and rank agreement between the thesauri for target words like nouns (#REF) .\n sent4: Although much attention has been given to the evaluation of various informativeness and similarity measures, a careful assessment of the effects of filtering on the resulting thesauri is also needed.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Contexts are extracted from syntactic dependencies generated by RASP (#REF) , using nouns (heads of NPs) which have subject and direct object relations with the target verb.",
                "Thus, each target verb is represented by a set of triples containing (i) the verb itself, (ii) a context noun and (iii) a syntactic relation (object, subject).",
                "The thesauri were constructed using #TARGET_REF method.",
                "Lin's version of the distributional hypothesis states that two words (verbs v 1 and v 2 in our case) are similar if they share a large proportion of contexts weighted by their information content, assessed with PMI (#REF; #REF) .",
                "In the literature, little attention is paid to context filters."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Contexts are extracted from syntactic dependencies generated by RASP (#REF) , using nouns (heads of NPs) which have subject and direct object relations with the target verb.\n sent1: Thus, each target verb is represented by a set of triples containing (i) the verb itself, (ii) a context noun and (iii) a syntactic relation (object, subject).\n sent2: The thesauri were constructed using #TARGET_REF method.\n sent3: Lin's version of the distributional hypothesis states that two words (verbs v 1 and v 2 in our case) are similar if they share a large proportion of contexts weighted by their information content, assessed with PMI (#REF; #REF) .\n sent4: In the literature, little attention is paid to context filters.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "However, whether or not these models are actually learning to address the tasks they are designed for is questionable.",
                "For example, #REF showed that IC models do not understand images sufficiently, as reflected by the generated captions.",
                "As a consequence, in the last few years many diagnostic tasks and datasets have been proposed aiming at investigating the capabilities of such models in more detail to determine whether and how these models are capable of exploiting visual and/or linguistic information #TARGET_REF; #REF; #REF; #REF; #REF; #REF; #REF) .",
                "FOIL (#REFb ) is one such dataset.",
                "It was proposed to evaluate the ability of V2L models in understanding the interplay of objects and their attributes in the images and their relations in an image captioning framework."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: However, whether or not these models are actually learning to address the tasks they are designed for is questionable.\n sent1: For example, #REF showed that IC models do not understand images sufficiently, as reflected by the generated captions.\n sent2: As a consequence, in the last few years many diagnostic tasks and datasets have been proposed aiming at investigating the capabilities of such models in more detail to determine whether and how these models are capable of exploiting visual and/or linguistic information #TARGET_REF; #REF; #REF; #REF; #REF; #REF; #REF) .\n sent3: FOIL (#REFb ) is one such dataset.\n sent4: It was proposed to evaluate the ability of V2L models in understanding the interplay of objects and their attributes in the images and their relations in an image captioning framework.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "For example, #REF showed that IC models do not understand images sufficiently, as reflected by the generated captions.",
                "As a consequence, in the last few years many diagnostic tasks and datasets have been proposed aiming at investigating the capabilities of such models in more detail to determine whether and how these models are capable of exploiting visual and/or linguistic information (#REFb; #REF; #REF; #REF; #REF; #REF; #REF) .",
                "FOIL #TARGET_REF ) is one such dataset.",
                "It was proposed to evaluate the ability of V2L models in understanding the interplay of objects and their attributes in the images and their relations in an image captioning framework.",
                "This is done by replacing a word in MSCOCO (#REF) captions with a 'foiled' word that is semantically similar or related to the original word (substituting dog with cat), thus rendering the image caption unfaithful to the image content, while yet linguistically valid."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For example, #REF showed that IC models do not understand images sufficiently, as reflected by the generated captions.\n sent1: As a consequence, in the last few years many diagnostic tasks and datasets have been proposed aiming at investigating the capabilities of such models in more detail to determine whether and how these models are capable of exploiting visual and/or linguistic information (#REFb; #REF; #REF; #REF; #REF; #REF; #REF) .\n sent2: FOIL #TARGET_REF ) is one such dataset.\n sent3: It was proposed to evaluate the ability of V2L models in understanding the interplay of objects and their attributes in the images and their relations in an image captioning framework.\n sent4: This is done by replacing a word in MSCOCO (#REF) captions with a 'foiled' word that is semantically similar or related to the original word (substituting dog with cat), thus rendering the image caption unfaithful to the image content, while yet linguistically valid.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "• Foiled Adjective and Adverb: Adjectives and adverbs are replaced with similar adjectives and adverbs.",
                "Here, the notion of similarity again is obtained from external resources;",
                "• Foiled Preposition: Prepositions are directly replaced with functionally similar prepositions.",
                "The Verb, Adjective, Adverb and Preposition subsets were obtained using a slightly different methodology (see Shekhar et al. (2017a) ) than that used for Nouns #TARGET_REF .",
                "Therefore, we evaluate these two groups separately."
            ],
            "label": [
                "BACKGROUND",
                "USE"
            ]
        },
        "input": "sent0: • Foiled Adjective and Adverb: Adjectives and adverbs are replaced with similar adjectives and adverbs.\n sent1: Here, the notion of similarity again is obtained from external resources;\n sent2: • Foiled Preposition: Prepositions are directly replaced with functionally similar prepositions.\n sent3: The Verb, Adjective, Adverb and Preposition subsets were obtained using a slightly different methodology (see Shekhar et al. (2017a) ) than that used for Nouns #TARGET_REF .\n sent4: Therefore, we evaluate these two groups separately.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"USE\"], \"context\": [\"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "In this section we describe the foiled caption classification task and dataset.",
                "We combine the tasks and data from #TARGET_REF and Shekhar et al. (2017a) .",
                "Given an image and a caption, in both cases the task is to learn a model that can distinguish between a REAL caption that describes the image, and a FOILed caption where a word from the original caption is swapped such that it no longer describes the image accurately.",
                "There are several sets of 'foiled captions' where words from specific parts of speech are swapped:",
                "• Foiled Noun: In this case a noun word in the original caption is replaced with another similar noun, such that the resultant caption is not the correct description for the image."
            ],
            "label": [
                "USE",
                "EXTENDS"
            ]
        },
        "input": "sent0: In this section we describe the foiled caption classification task and dataset.\n sent1: We combine the tasks and data from #TARGET_REF and Shekhar et al. (2017a) .\n sent2: Given an image and a caption, in both cases the task is to learn a model that can distinguish between a REAL caption that describes the image, and a FOILed caption where a word from the original caption is swapped such that it no longer describes the image accurately.\n sent3: There are several sets of 'foiled captions' where words from specific parts of speech are swapped:\n sent4: • Foiled Noun: In this case a noun word in the original caption is replaced with another similar noun, such that the resultant caption is not the correct description for the image.\n",
        "output": "{\"label\": [\"USE\", \"EXTENDS\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Data: We use the dataset for nouns from #TARGET_REF 1 and the datasets for other parts of speech from Shekhar et al. (2017a) 2 .",
                "Statistics about the dataset are given in Table 1 .",
                "The evaluation metric is accuracy per class and the average (overall) accuracy over the two classes."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Data: We use the dataset for nouns from #TARGET_REF 1 and the datasets for other parts of speech from Shekhar et al. (2017a) 2 .\n sent1: Statistics about the dataset are given in Table 1 .\n sent2: The evaluation metric is accuracy per class and the average (overall) accuracy over the two classes.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "bag of objects information are the best performing models across classifiers.",
                "We also note that the performance is better than human performance.",
                "We hypothesize the following reasons for this: (a) human responses were crowd-sourced, which could have resulted in some noisy annotations; (b) our gold object-based features closely resembles the information used for data-generation as described in #TARGET_REF for the foil noun dataset.",
                "The models using Predicted bag of objects from a detector are very close to the performance of Gold.",
                "The performance of models using simple bag of words (BOW) sentence representations and an MLP is better than that of models that use LSTMs."
            ],
            "label": [
                "USE",
                "SIMILARITY"
            ]
        },
        "input": "sent0: bag of objects information are the best performing models across classifiers.\n sent1: We also note that the performance is better than human performance.\n sent2: We hypothesize the following reasons for this: (a) human responses were crowd-sourced, which could have resulted in some noisy annotations; (b) our gold object-based features closely resembles the information used for data-generation as described in #TARGET_REF for the foil noun dataset.\n sent3: The models using Predicted bag of objects from a detector are very close to the performance of Gold.\n sent4: The performance of models using simple bag of words (BOW) sentence representations and an MLP is better than that of models that use LSTMs.\n",
        "output": "{\"label\": [\"USE\", \"SIMILARITY\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "In all cases, we observe that the performance is on par with human-level accuracy.",
                "Our overall accuracy is substantially higher than that reported in #TARGET_REF .",
                "Interestingly, our implementation of CNN+LSTM produced better results than their equivalent model (they reported 61.07% vs. our 87.45%).",
                "We investigate this further in Section 5.",
                "Performance on other parts of speech: For other parts of speech, we fix the image representation to Gold Frequency, and compare results using the BOW-based MLP and MM-LSTM."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: In all cases, we observe that the performance is on par with human-level accuracy.\n sent1: Our overall accuracy is substantially higher than that reported in #TARGET_REF .\n sent2: Interestingly, our implementation of CNN+LSTM produced better results than their equivalent model (they reported 61.07% vs. our 87.45%).\n sent3: We investigate this further in Section 5.\n sent4: Performance on other parts of speech: For other parts of speech, we fix the image representation to Gold Frequency, and compare results using the BOW-based MLP and MM-LSTM.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We also observe that the performance of BOW improves by adding object Frequency image information, but not CNN image embeddings.",
                "We posit that this is because there is a tighter correspondence between the bag of objects and bag of word models.",
                "In the case of LSTMs, adding either image information helps slightly.",
                "The accuracy of our models is substantially higher than that reported in #TARGET_REF , even for equivalent models.",
                "We note, however, that while the trends of image information is similar for other parts of speech datasets, the performance of BOW based models are lower than the performance of LSTM based models."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: We also observe that the performance of BOW improves by adding object Frequency image information, but not CNN image embeddings.\n sent1: We posit that this is because there is a tighter correspondence between the bag of objects and bag of word models.\n sent2: In the case of LSTMs, adding either image information helps slightly.\n sent3: The accuracy of our models is substantially higher than that reported in #TARGET_REF , even for equivalent models.\n sent4: We note, however, that while the trends of image information is similar for other parts of speech datasets, the performance of BOW based models are lower than the performance of LSTM based models.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The Vision-and-Language Navigation (VLN) task (#REF) requires an agent to navigate to a particular location in a real-world environment, following complex, context-dependent instructions written by humans (e.g. go down the second hallway on the left, enter the bedroom and stop by the mirror).",
                "The agent must navigate through the environment, conditioning on the instruction as well as the visual imagery that it observes along the route, to stop at the location specified by the instruction (e.g. the mirror).",
                "Recent state-of-the-art models (#REF; #TARGET_REF; #REF) have demonstrated large gains in accuracy on the VLN task.",
                "However, it is unclear which modality these go past the couch … Figure 1 : We factor the grounding of language instructions into visual appearance, route structure, and object detections using a mixture-of-experts approach.",
                "substantial increases in task metrics can be attributed to, and, in particular, whether the gains in performance are due to stronger grounding into visual context or e.g. simply into the discrete, geometric structure of possible routes, such as turning left or moving forward (see Fig. 1"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The Vision-and-Language Navigation (VLN) task (#REF) requires an agent to navigate to a particular location in a real-world environment, following complex, context-dependent instructions written by humans (e.g. go down the second hallway on the left, enter the bedroom and stop by the mirror).\n sent1: The agent must navigate through the environment, conditioning on the instruction as well as the visual imagery that it observes along the route, to stop at the location specified by the instruction (e.g. the mirror).\n sent2: Recent state-of-the-art models (#REF; #TARGET_REF; #REF) have demonstrated large gains in accuracy on the VLN task.\n sent3: However, it is unclear which modality these go past the couch … Figure 1 : We factor the grounding of language instructions into visual appearance, route structure, and object detections using a mixture-of-experts approach.\n sent4: substantial increases in task metrics can be attributed to, and, in particular, whether the gains in performance are due to stronger grounding into visual context or e.g. simply into the discrete, geometric structure of possible routes, such as turning left or moving forward (see Fig. 1\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "These approaches have significantly improved performance on the VLN task, when evaluated by metrics such as success rate.",
                "However, it is unclear where the high performance comes from.",
                "In this paper, we find that agents without any visual input can achieve competitive performance, matching or even outperforming their vision-based counterparts under two state-of-theart model models #TARGET_REF; #REF) .",
                "We also explore two approaches to make the agents better utilize their visual inputs.",
                "The role of vision in vision-and-language tasks."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: These approaches have significantly improved performance on the VLN task, when evaluated by metrics such as success rate.\n sent1: However, it is unclear where the high performance comes from.\n sent2: In this paper, we find that agents without any visual input can achieve competitive performance, matching or even outperforming their vision-based counterparts under two state-of-theart model models #TARGET_REF; #REF) .\n sent3: We also explore two approaches to make the agents better utilize their visual inputs.\n sent4: The role of vision in vision-and-language tasks.\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF find that image captioning models can exploit regularity in the captions, showing that a nearestneighbor matching approach can achieve competitive performance to sophisticated language generation models. and find that neural captioning models often ground object mentions into incorrect objects due to correlations in the training data, and can hallucinate non-existing objects.",
                "Recent work has also investigated singlemodality performance in vision-and-language embodiment tasks.",
                "#REF find that stateof-the-art results can be achieved on the EmbodiedQA task (#REF ) using an agent without visual inputs.",
                "Work concurrent to ours evaluates the performance of single-modality models for several embodied tasks including VLN (#REF) , finding that high performance can be achieved on the R2R dataset using a non-visual version of the baseline model (#REF) .",
                "In this paper, we show that the same trends hold for two recent state-of-the-art architectures (#REF; #TARGET_REF for the VLN task; we also analyze to what extent object-based representations and mixture-ofexperts methods can address these issues."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: #REF find that image captioning models can exploit regularity in the captions, showing that a nearestneighbor matching approach can achieve competitive performance to sophisticated language generation models. and find that neural captioning models often ground object mentions into incorrect objects due to correlations in the training data, and can hallucinate non-existing objects.\n sent1: Recent work has also investigated singlemodality performance in vision-and-language embodiment tasks.\n sent2: #REF find that stateof-the-art results can be achieved on the EmbodiedQA task (#REF ) using an agent without visual inputs.\n sent3: Work concurrent to ours evaluates the performance of single-modality models for several embodied tasks including VLN (#REF) , finding that high performance can be achieved on the R2R dataset using a non-visual version of the baseline model (#REF) .\n sent4: In this paper, we show that the same trends hold for two recent state-of-the-art architectures (#REF; #TARGET_REF for the VLN task; we also analyze to what extent object-based representations and mixture-ofexperts methods can address these issues.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "At each timestep, the agent receives the panoramic image for the viewpoint it is currently located at, and either predicts to move to one of the adjacent connected viewpoints, or to stop.",
                "When the agent predicts the stop action, it is evaluated on whether it has correctly reached the end of the route that the human annotator was asked to describe.",
                "In this work, we analyze two recent VLN models, which typify the visual grounding approaches of VLN work: the panoramic \"follower\" model from the Speaker-Follower (SF) system of #TARGET_REF and the Self-Monitoring (SM) model of #REF .",
                "These models obtained stateof-the-art results on the R2R dataset.",
                "Both models are based on the encoder-decoder approach (#REF ) and map an instruction to a sequence of actions in context by encoding the instruction with an LSTM, and outputting actions using an LSTM decoder that conditions on the encoded instruction and visual features summarizing the agent's environmental context."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: At each timestep, the agent receives the panoramic image for the viewpoint it is currently located at, and either predicts to move to one of the adjacent connected viewpoints, or to stop.\n sent1: When the agent predicts the stop action, it is evaluated on whether it has correctly reached the end of the route that the human annotator was asked to describe.\n sent2: In this work, we analyze two recent VLN models, which typify the visual grounding approaches of VLN work: the panoramic \"follower\" model from the Speaker-Follower (SF) system of #TARGET_REF and the Self-Monitoring (SM) model of #REF .\n sent3: These models obtained stateof-the-art results on the R2R dataset.\n sent4: Both models are based on the encoder-decoder approach (#REF ) and map an instruction to a sequence of actions in context by encoding the instruction with an LSTM, and outputting actions using an LSTM decoder that conditions on the encoded instruction and visual features summarizing the agent's environmental context.\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "For our objectbased representation, we use a Faster R-CNN (#REF) object detector trained on the Visual Genome dataset (#REF) .",
                "We construct a set of vectors {x obj,j } representing detected objects and their attributes.",
                "Each vector x obj,j (j-th detected object in the scene) is a concatenation of summed GloVe vectors (#REF) for the detected object label (e.g. door) and attribute labels (e.g. white) and a location vector from the object's bounding box coordinates.",
                "We then use the same visual attention mechanism as in #TARGET_REF and #REF to obtain an attended object representation x obj,att over these {x obj,j } vectors.",
                "We either substitute the ResNet CNN features x img,att (\"RN\") with our object representation x obj,att (\"Obj\"), or concatenate x img,att and x obj,att (\"RN+Obj\")."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: For our objectbased representation, we use a Faster R-CNN (#REF) object detector trained on the Visual Genome dataset (#REF) .\n sent1: We construct a set of vectors {x obj,j } representing detected objects and their attributes.\n sent2: Each vector x obj,j (j-th detected object in the scene) is a concatenation of summed GloVe vectors (#REF) for the detected object label (e.g. door) and attribute labels (e.g. white) and a location vector from the object's bounding box coordinates.\n sent3: We then use the same visual attention mechanism as in #TARGET_REF and #REF to obtain an attended object representation x obj,att over these {x obj,j } vectors.\n sent4: We either substitute the ResNet CNN features x img,att (\"RN\") with our object representation x obj,att (\"Obj\"), or concatenate x img,att and x obj,att (\"RN+Obj\").\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The Speaker-Follower (SF) model #TARGET_REF ) and the Self-Monitoring (SM) model (#REF) which we analyze both use sequenceto-sequence model (#REF) with attention (#REF) as their base instruction-following agent.",
                "Both use an encoder LSTM (#REF ) to represent the instruction text, and a decoder LSTM to predict actions sequentially.",
                "At each timestep, the decoder LSTM conditions on the action previously taken, a representation of the visual context at the agent's current location, and an attended representation of the encoded instruction.",
                "While at a high level these models are similar (at least in terms of the base sequence-tosequence models -both papers additionally develop techniques to select routes from these base models during search-based inference techniques, either using a separate language generation model in SF, or a progress-monitor in SM), they differ in the mechanism by which they combine representations of the text instruction and visual input.",
                "The SM uses a co-grounded attention mechanism, where both the visual attention on image features and the textual attention on the instruction words are generated based on previous decoder LSTM hidden state h t−1 , and then the attended visual and textual features are used as LSTM inputs to produce h t ."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: The Speaker-Follower (SF) model #TARGET_REF ) and the Self-Monitoring (SM) model (#REF) which we analyze both use sequenceto-sequence model (#REF) with attention (#REF) as their base instruction-following agent.\n sent1: Both use an encoder LSTM (#REF ) to represent the instruction text, and a decoder LSTM to predict actions sequentially.\n sent2: At each timestep, the decoder LSTM conditions on the action previously taken, a representation of the visual context at the agent's current location, and an attended representation of the encoded instruction.\n sent3: While at a high level these models are similar (at least in terms of the base sequence-tosequence models -both papers additionally develop techniques to select routes from these base models during search-based inference techniques, either using a separate language generation model in SF, or a progress-monitor in SM), they differ in the mechanism by which they combine representations of the text instruction and visual input.\n sent4: The SM uses a co-grounded attention mechanism, where both the visual attention on image features and the textual attention on the instruction words are generated based on previous decoder LSTM hidden state h t−1 , and then the attended visual and textual features are used as LSTM inputs to produce h t .\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "For SF, we use the publicly released code.",
                "For SM, we use a reimplementation without the progress monitor, which was shown to be most important for search in inference (#REF) .",
                "We investigate how well these models ground instructions into visual features of the environment, by training and evaluating them without access to the visual context: setting their visual feature vectors to zeroes during training and testing.",
                "We compare performance on the validation sets of the R2R dataset: the val-seen split, consisting of the same environments as in training, and the val- Table 1 : Success rate (SR) of the vision-based full agent (\"RN\", using ResNet) and the non-visual agent (\"no vis.\", setting all visual features to zero) on the R2R dataset under different model architectures (SpeakerFollower (SF) #TARGET_REF and Self-Monitoring (SM) (#REF) ) and training schemes.",
                "unseen split of novel environments."
            ],
            "label": [
                "DIFFERENCES",
                "EXTENDS"
            ]
        },
        "input": "sent0: For SF, we use the publicly released code.\n sent1: For SM, we use a reimplementation without the progress monitor, which was shown to be most important for search in inference (#REF) .\n sent2: We investigate how well these models ground instructions into visual features of the environment, by training and evaluating them without access to the visual context: setting their visual feature vectors to zeroes during training and testing.\n sent3: We compare performance on the validation sets of the R2R dataset: the val-seen split, consisting of the same environments as in training, and the val- Table 1 : Success rate (SR) of the vision-based full agent (\"RN\", using ResNet) and the non-visual agent (\"no vis.\", setting all visual features to zero) on the R2R dataset under different model architectures (SpeakerFollower (SF) #TARGET_REF and Self-Monitoring (SM) (#REF) ) and training schemes.\n sent4: unseen split of novel environments.\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"EXTENDS\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The earliest approach in (#REF ) used edit distance based multiple string alignment (MSA) (#REF) to build the confusion networks.",
                "The recent approaches used pair-wise alignment algorithms based on symmetric alignments from a HMM alignment model (#REF) or edit distance alignments allowing shifts #TARGET_REF .",
                "The alignment method described in this paper extends the latter by incrementally aligning the hypotheses as in MSA but also allowing shifts as in the TER alignment.",
                "The confusion networks are built around a \"skeleton\" hypothesis.",
                "The skeleton hypothesis defines the word order of the decoding output."
            ],
            "label": [
                "BACKGROUND",
                "EXTENDS"
            ]
        },
        "input": "sent0: The earliest approach in (#REF ) used edit distance based multiple string alignment (MSA) (#REF) to build the confusion networks.\n sent1: The recent approaches used pair-wise alignment algorithms based on symmetric alignments from a HMM alignment model (#REF) or edit distance alignments allowing shifts #TARGET_REF .\n sent2: The alignment method described in this paper extends the latter by incrementally aligning the hypotheses as in MSA but also allowing shifts as in the TER alignment.\n sent3: The confusion networks are built around a \"skeleton\" hypothesis.\n sent4: The skeleton hypothesis defines the word order of the decoding output.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"EXTENDS\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Second, the confusion networks are created from the union of these alignments.",
                "The incremental hypothesis alignment algorithm combines these two steps.",
                "All words from the previously aligned hypotheses are available, even if not present in the skeleton hypothesis, when aligning the following hypotheses.",
                "As in #TARGET_REF , confusion networks built around all skeletons are joined into a lattice which is expanded and rescored with language models.",
                "System weights and language model weights are tuned to optimize the quality of the decoding output on a development set."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: Second, the confusion networks are created from the union of these alignments.\n sent1: The incremental hypothesis alignment algorithm combines these two steps.\n sent2: All words from the previously aligned hypotheses are available, even if not present in the skeleton hypothesis, when aligning the following hypotheses.\n sent3: As in #TARGET_REF , confusion networks built around all skeletons are joined into a lattice which is expanded and rescored with language models.\n sent4: System weights and language model weights are tuned to optimize the quality of the decoding output on a development set.\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Finally, each insertion will generate a new node and two word arcs at the corresponding position in the network.",
                "The first word arc will have the inserted word with the confidence set as in the case of a substitution and the second word arc will have a NULL word with confidences set by assuming all previously aligned hypotheses and the skeleton generated the NULL word arc.",
                "After all hypotheses have been added into the confusion network, the system specific word arc confidences are scaled to sum to one over all arcs between 1 2 3 4 5 6 I (3) like (3) kites (1) NULL (2) NULL (1) big (1) blue (2) balloons (2) Figure 2: Network after incremental TER alignment.",
                "each set of two consecutive nodes.",
                "Other scores for the word arc are set as in #TARGET_REF ."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: Finally, each insertion will generate a new node and two word arcs at the corresponding position in the network.\n sent1: The first word arc will have the inserted word with the confidence set as in the case of a substitution and the second word arc will have a NULL word with confidences set by assuming all previously aligned hypotheses and the skeleton generated the NULL word arc.\n sent2: After all hypotheses have been added into the confusion network, the system specific word arc confidences are scaled to sum to one over all arcs between 1 2 3 4 5 6 I (3) like (3) kites (1) NULL (2) NULL (1) big (1) blue (2) balloons (2) Figure 2: Network after incremental TER alignment.\n sent3: each set of two consecutive nodes.\n sent4: Other scores for the word arc are set as in #TARGET_REF .\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "We discuss how moderation systems can be tuned, depending on the availability and workload of the moderators.",
                "We also introduce additional evaluation measures for the semi-automatic scenario.",
                "On both datasets (Gazzetta and Wikipedia comments) and for both scenarios (automatic, semiautomatic), we show that a recurrent neural network (RNN) outperforms the system of #TARGET_REF , the previous state of the art for comment moderation, which employed logistic regression or a multi-layer Perceptron (MLP), and represented each comment as a bag of (character or word) n-grams.",
                "We also propose an attention mechanism that improves the overall performance of the RNN.",
                "Our attention mechanism differs from most previous ones (#REF; #REF) in that it is used in a classification setting, where there is no previously generated output subsequence to drive the attention, unlike sequence-to-sequence models (#REF) ."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: We discuss how moderation systems can be tuned, depending on the availability and workload of the moderators.\n sent1: We also introduce additional evaluation measures for the semi-automatic scenario.\n sent2: On both datasets (Gazzetta and Wikipedia comments) and for both scenarios (automatic, semiautomatic), we show that a recurrent neural network (RNN) outperforms the system of #TARGET_REF , the previous state of the art for comment moderation, which employed logistic regression or a multi-layer Perceptron (MLP), and represented each comment as a bag of (character or word) n-grams.\n sent3: We also propose an attention mechanism that improves the overall performance of the RNN.\n sent4: Our attention mechanism differs from most previous ones (#REF; #REF) in that it is used in a classification setting, where there is no previously generated output subsequence to drive the attention, unlike sequence-to-sequence models (#REF) .\n",
        "output": "{\"label\": [null], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Unlike Wulczyn et al., we tuned the hyper-parameters by evaluating (computing AUC and Spearman, Section 4) on a random 2% of held-out comments of W-ATT-TRAIN or G-TRAIN-S, instead of the development subsets, to be able to obtain more realistic results from the development sets while developing the methods.",
                "For both Wikipedia and Gazzetta, the tuning selected character n-grams, as in the work of Wulczyn et al. Also, for both Wikipedia and Gazzetta, it preferred LR to MLP, whereas Wulczyn et al. reported slightly higher performance 8 Two of the co-authors of #TARGET_REF are with Jigsaw, who recently announced Perspective, a system to detect 'toxic' comments.",
                "Perspective is not the same as DETOX (personal communication), but we were unable to obtain scientific articles describing it.",
                "An API for Perspective is available at https://www.perspectiveapi.",
                "com/, but we did not have access to the API at the time the experiments of this paper were carried out."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: Unlike Wulczyn et al., we tuned the hyper-parameters by evaluating (computing AUC and Spearman, Section 4) on a random 2% of held-out comments of W-ATT-TRAIN or G-TRAIN-S, instead of the development subsets, to be able to obtain more realistic results from the development sets while developing the methods.\n sent1: For both Wikipedia and Gazzetta, the tuning selected character n-grams, as in the work of Wulczyn et al. Also, for both Wikipedia and Gazzetta, it preferred LR to MLP, whereas Wulczyn et al. reported slightly higher performance 8 Two of the co-authors of #TARGET_REF are with Jigsaw, who recently announced Perspective, a system to detect 'toxic' comments.\n sent2: Perspective is not the same as DETOX (personal communication), but we were unable to obtain scientific articles describing it.\n sent3: An API for Perspective is available at https://www.perspectiveapi.\n sent4: com/, but we did not have access to the API at the time the experiments of this paper were carried out.\n",
        "output": "{\"label\": [null], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Following #TARGET_REF , we report in Table 2 AUC scores (area under ROC curve), along with Spearman correlations between systemgenerated probabilities P (accept|c) and human probabilistic gold labels (Section 2.2) when probabilistic gold labels are available.",
                "18 Wulczyn et al. reported DETOX results only on W-ATT-DEV, shown in brackets.",
                "Table 2 shows that RNN is 17 More precisely, when computing F β , we reorder the development comments by time posted, and split them into batches of 100.",
                "For each ta (and tr) value, we compute F β per batch and macro-average across batches.",
                "The resulting thresholds lead to F β scores that are more stable over time."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: Following #TARGET_REF , we report in Table 2 AUC scores (area under ROC curve), along with Spearman correlations between systemgenerated probabilities P (accept|c) and human probabilistic gold labels (Section 2.2) when probabilistic gold labels are available.\n sent1: 18 Wulczyn et al. reported DETOX results only on W-ATT-DEV, shown in brackets.\n sent2: Table 2 shows that RNN is 17 More precisely, when computing F β , we reorder the development comments by time posted, and split them into batches of 100.\n sent3: For each ta (and tr) value, we compute F β per batch and macro-average across batches.\n sent4: The resulting thresholds lead to F β scores that are more stable over time.\n",
        "output": "{\"label\": [null], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "1: No explicit relation is detected in the complex sentence (left), but an explicit CONTRAST relation is identified in the simple sentence (right).",
                "The example is taken from the Simple English Wikipedia corpus #TARGET_REF connectives do not belong to any linguistic class and except for a few discourse connectives such as oh and well, most carry meaning.",
                "#REF revised this definition; even though she agreed that discourse connectives have meaning by themselves, she argued that they should contribute to the semantic interpretations of the discourse.",
                "Apart from research efforts aiming at defining discourse connectives, another line of research has focused on providing a list of discourse connectives in English (#REF; #REF; #REF; #REF; #REF; #REF) and other languages (#REF; #REF) .",
                "While most of these inventories have been built by hand, some work has attempted to identify them automatically."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: 1: No explicit relation is detected in the complex sentence (left), but an explicit CONTRAST relation is identified in the simple sentence (right).\n sent1: The example is taken from the Simple English Wikipedia corpus #TARGET_REF connectives do not belong to any linguistic class and except for a few discourse connectives such as oh and well, most carry meaning.\n sent2: #REF revised this definition; even though she agreed that discourse connectives have meaning by themselves, she argued that they should contribute to the semantic interpretations of the discourse.\n sent3: Apart from research efforts aiming at defining discourse connectives, another line of research has focused on providing a list of discourse connectives in English (#REF; #REF; #REF; #REF; #REF; #REF) and other languages (#REF; #REF) .\n sent4: While most of these inventories have been built by hand, some work has attempted to identify them automatically.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "To discover AltLexes automatically, we created two sentence-aligned data sets using standard corpora in text simplification.",
                "The first data set was created from the Simple English Wikipedia corpus #TARGET_REF ; the other was created from the Newsela corpus (#REF) .",
                "The Simple English Wikipedia (SEW) corpus (#REF) contains two sections: 1) article-aligned and 2) sentence-aligned.",
                "Here, we used the sentence-aligned section, which contains 167,686 pairs of aligned sentences.",
                "In order not to overfit to a specific corpus, we also used the Newsela (News) corpus (#REF) ."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: To discover AltLexes automatically, we created two sentence-aligned data sets using standard corpora in text simplification.\n sent1: The first data set was created from the Simple English Wikipedia corpus #TARGET_REF ; the other was created from the Newsela corpus (#REF) .\n sent2: The Simple English Wikipedia (SEW) corpus (#REF) contains two sections: 1) article-aligned and 2) sentence-aligned.\n sent3: Here, we used the sentence-aligned section, which contains 167,686 pairs of aligned sentences.\n sent4: In order not to overfit to a specific corpus, we also used the Newsela (News) corpus (#REF) .\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "To discover AltLexes automatically, we created two sentence-aligned data sets using standard corpora in text simplification.",
                "The first data set was created from the Simple English Wikipedia corpus (#REF) ; the other was created from the Newsela corpus (#REF) .",
                "The Simple English Wikipedia (SEW) corpus #TARGET_REF contains two sections: 1) article-aligned and 2) sentence-aligned.",
                "Here, we used the sentence-aligned section, which contains 167,686 pairs of aligned sentences.",
                "In order not to overfit to a specific corpus, we also used the Newsela (News) corpus (#REF) ."
            ],
            "label": [
                "EXTENDS",
                "USE"
            ]
        },
        "input": "sent0: To discover AltLexes automatically, we created two sentence-aligned data sets using standard corpora in text simplification.\n sent1: The first data set was created from the Simple English Wikipedia corpus (#REF) ; the other was created from the Newsela corpus (#REF) .\n sent2: The Simple English Wikipedia (SEW) corpus #TARGET_REF contains two sections: 1) article-aligned and 2) sentence-aligned.\n sent3: Here, we used the sentence-aligned section, which contains 167,686 pairs of aligned sentences.\n sent4: In order not to overfit to a specific corpus, we also used the Newsela (News) corpus (#REF) .\n",
        "output": "{\"label\": [\"EXTENDS\", \"USE\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "In order not to overfit to a specific corpus, we also used the Newsela (News) corpus (#REF) .",
                "This corpus contains 1,911 English news articles which have been manually re-written at most 5 times, each time with decreasing complexity level.",
                "We used this article-aligned corpus to align it at the sentence-level using an approach similar to #TARGET_REF .",
                "Then, two native English speakers evaluated the alignments.",
                "The Kappa inter-annotation agreement was 0.898 computed on 100 randomly chosen alignments."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: In order not to overfit to a specific corpus, we also used the Newsela (News) corpus (#REF) .\n sent1: This corpus contains 1,911 English news articles which have been manually re-written at most 5 times, each time with decreasing complexity level.\n sent2: We used this article-aligned corpus to align it at the sentence-level using an approach similar to #TARGET_REF .\n sent3: Then, two native English speakers evaluated the alignments.\n sent4: The Kappa inter-annotation agreement was 0.898 computed on 100 randomly chosen alignments.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Paraphrases can be extracted from non-parallel corpora using contextual similarity (#REF) .",
                "They can also be obtained from parallel corpora if such data is available ( #TARGET_REF; #REF) .",
                "Recently, there are also a number of studies that extract paraphrases from multilingual corpora (#REF; #REF) .",
                "The approach in (#REF) does not use deep linguistic analysis and therefore is suitable to noisy corpora like ours.",
                "Due to this reason, we build our technique on top of theirs."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Paraphrases can be extracted from non-parallel corpora using contextual similarity (#REF) .\n sent1: They can also be obtained from parallel corpora if such data is available ( #TARGET_REF; #REF) .\n sent2: Recently, there are also a number of studies that extract paraphrases from multilingual corpora (#REF; #REF) .\n sent3: The approach in (#REF) does not use deep linguistic analysis and therefore is suitable to noisy corpora like ours.\n sent4: Due to this reason, we build our technique on top of theirs.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Recently, there are also a number of studies that extract paraphrases from multilingual corpora (#REF; #REF) .",
                "The approach in ( #TARGET_REF) does not use deep linguistic analysis and therefore is suitable to noisy corpora like ours.",
                "Due to this reason, we build our technique on top of theirs.",
                "The following provides a summary of their technique.",
                "Two types of paraphrase patterns are defined: (1) Syntactic patterns which consist of the POS tags of the phrases."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Recently, there are also a number of studies that extract paraphrases from multilingual corpora (#REF; #REF) .\n sent1: The approach in ( #TARGET_REF) does not use deep linguistic analysis and therefore is suitable to noisy corpora like ours.\n sent2: Due to this reason, we build our technique on top of theirs.\n sent3: The following provides a summary of their technique.\n sent4: Two types of paraphrase patterns are defined: (1) Syntactic patterns which consist of the POS tags of the phrases.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "At times, some words might be detected as paraphrases incidentally due to the noise.",
                "In ( #TARGET_REF ), a paraphrase is reported as long as there is a single good supporting pair of sentences.",
                "Although this works well for a relatively clean parallel corpus considered in their work, i.e., novels, this does not work well for bug reports.",
                "Consider the context-peculiar example in Table 1 (bottom).",
                "For a context-peculiar para-phrase, there can be many sentences containing the pair of phrases but very few support them to be a paraphrase."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: At times, some words might be detected as paraphrases incidentally due to the noise.\n sent1: In ( #TARGET_REF ), a paraphrase is reported as long as there is a single good supporting pair of sentences.\n sent2: Although this works well for a relatively clean parallel corpus considered in their work, i.e., novels, this does not work well for bug reports.\n sent3: Consider the context-peculiar example in Table 1 (bottom).\n sent4: For a context-peculiar para-phrase, there can be many sentences containing the pair of phrases but very few support them to be a paraphrase.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Humor recognition refers to the task of deciding whether a sentence/spoken-utterance expresses a certain degree of humor.",
                "In most of the previous studies (#REF; #REF; #TARGET_REF , humor recognition was modeled as a binary classification task",
                "In the seminal work (#REF) , a corpus of 16,000 \"one-liners\" was created using daily joke websites to collect humorous instances while using formal writing resources (e.g., news titles) to obtain non-humorous instances.",
                "Three humor-specific stylistic features, including alliteration, antonymy, and adult slang were utilized together with content-based features to build classifiers.",
                "In a recent work (#REF) , a new corpus was constructed from a Pun of the Day website."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Humor recognition refers to the task of deciding whether a sentence/spoken-utterance expresses a certain degree of humor.\n sent1: In most of the previous studies (#REF; #REF; #TARGET_REF , humor recognition was modeled as a binary classification task\n sent2: In the seminal work (#REF) , a corpus of 16,000 \"one-liners\" was created using daily joke websites to collect humorous instances while using formal writing resources (e.g., news titles) to obtain non-humorous instances.\n sent3: Three humor-specific stylistic features, including alliteration, antonymy, and adult slang were utilized together with content-based features to build classifiers.\n sent4: In a recent work (#REF) , a new corpus was constructed from a Pun of the Day website.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "In a recent work #TARGET_REF , a new corpus was constructed from a Pun of the Day website.",
                "It systematically explained and computed latent semantic structure features based on the following four aspects: (a) Incongruity, (b) Ambiguity, (c) Interpersonal Effect, and (d) Phonetic Style.",
                "In addition, Word2Vec (#REF) distributed representations were utilized in the model building.",
                "Beyond lexical cues from text inputs, other research has also utilized speakers' acoustic cues (#REF; #REFb) .",
                "These studies have typically used audio tracks from TV shows and their corresponding captions in order to categorize characters' speaking turns as humorous or non-humorous."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In a recent work #TARGET_REF , a new corpus was constructed from a Pun of the Day website.\n sent1: It systematically explained and computed latent semantic structure features based on the following four aspects: (a) Incongruity, (b) Ambiguity, (c) Interpersonal Effect, and (d) Phonetic Style.\n sent2: In addition, Word2Vec (#REF) distributed representations were utilized in the model building.\n sent3: Beyond lexical cues from text inputs, other research has also utilized speakers' acoustic cues (#REF; #REFb) .\n sent4: These studies have typically used audio tracks from TV shows and their corresponding captions in order to categorize characters' speaking turns as humorous or non-humorous.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "The collected transcripts were split into sentences using the Stanford CoreNLP tool (#REF) .",
                "In this study, sentences containing or immediately followed by '(Laughter)' were used as humorous sentences, as shown in Figure 1 ; all other sentences were defined as non-humorous sentences.",
                "Following (#REF; #TARGET_REF , we selected the same sizes (n = 4726) of humorous and non-humorous sentences.",
                "To minimize possible topic shifts between positive and negative instances, for each positive instance, we picked up one negative instance nearby (the context window was 7 sentences in this study).",
                "For example, in Figure 1 , a negative instance (corresponding to 'sent-2') was selected from the nearby sentences ranging from 'sent-7' and 'sent+7'."
            ],
            "label": [
                "BACKGROUND",
                "USE"
            ]
        },
        "input": "sent0: The collected transcripts were split into sentences using the Stanford CoreNLP tool (#REF) .\n sent1: In this study, sentences containing or immediately followed by '(Laughter)' were used as humorous sentences, as shown in Figure 1 ; all other sentences were defined as non-humorous sentences.\n sent2: Following (#REF; #TARGET_REF , we selected the same sizes (n = 4726) of humorous and non-humorous sentences.\n sent3: To minimize possible topic shifts between positive and negative instances, for each positive instance, we picked up one negative instance nearby (the context window was 7 sentences in this study).\n sent4: For example, in Figure 1 , a negative instance (corresponding to 'sent-2') was selected from the nearby sentences ranging from 'sent-7' and 'sent+7'.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "After running 200 iterations of tweaking, we ended up with the following selection: f w is 6 (entailing that the various filter sizes are (5, 6, 7)), f n is 100, dropout 1 is 0.7 and dropout 2 is 0.35, optimization uses Adam (#REF) .",
                "When training the CNN model, we randomly selected 10% of the training data as the validation set for using early stopping to avoid overfitting.",
                "7 https://github.com/ EducationalTestingService/skll 8 https://github.com/fchollet/keras 9 The implementation will be released with the paper On the Pun data, the CNN model shows consistent improved performance over the conventional model, as suggested in #TARGET_REF .",
                "In particular, precision has been greatly increased from 0.762 to 0.864.",
                "On the TED data, we also observed that the CNN model helps to increase precision (from 0.515 to 0.582) and accuracy (from 52.0% to 58.9%)."
            ],
            "label": [
                "BACKGROUND",
                "SIMILARITY"
            ]
        },
        "input": "sent0: After running 200 iterations of tweaking, we ended up with the following selection: f w is 6 (entailing that the various filter sizes are (5, 6, 7)), f n is 100, dropout 1 is 0.7 and dropout 2 is 0.35, optimization uses Adam (#REF) .\n sent1: When training the CNN model, we randomly selected 10% of the training data as the validation set for using early stopping to avoid overfitting.\n sent2: 7 https://github.com/ EducationalTestingService/skll 8 https://github.com/fchollet/keras 9 The implementation will be released with the paper On the Pun data, the CNN model shows consistent improved performance over the conventional model, as suggested in #TARGET_REF .\n sent3: In particular, precision has been greatly increased from 0.762 to 0.864.\n sent4: On the TED data, we also observed that the CNN model helps to increase precision (from 0.515 to 0.582) and accuracy (from 52.0% to 58.9%).\n",
        "output": "{\"label\": [\"BACKGROUND\", \"SIMILARITY\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "By using Long Short Time Memory (LSTM) cells (#REF), Bertero and Fung (2016a) showed that Recurrent Neural Networks (RNNs) perform better on modeling sequential information than Conditional Random Fields (CRFs) (#REF) .",
                "From the brief review, we can find that the limited number of previously created corpora only cover one-line puns or jokes and conversations from TV comedy shows.",
                "There is a great need for an open corpus that can support investigating humor in presentations.",
                "1 CNN-based text categorization methods have been applied for humor recognition (e.g., in (#REFb) ) but with limitations: (a) a rigorous comparison with the state-of-the-art conventional method examined in #TARGET_REF is missing; (b) CNN's performance in the previous research is not quite clear 2 ; and (c) some important techniques that can improve CNN performance (e.g., using variedsized filters and dropout regularization (#REF) ) were missing.",
                "Therefore, the present study is meant to address these limitations."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: By using Long Short Time Memory (LSTM) cells (#REF), Bertero and Fung (2016a) showed that Recurrent Neural Networks (RNNs) perform better on modeling sequential information than Conditional Random Fields (CRFs) (#REF) .\n sent1: From the brief review, we can find that the limited number of previously created corpora only cover one-line puns or jokes and conversations from TV comedy shows.\n sent2: There is a great need for an open corpus that can support investigating humor in presentations.\n sent3: 1 CNN-based text categorization methods have been applied for humor recognition (e.g., in (#REFb) ) but with limitations: (a) a rigorous comparison with the state-of-the-art conventional method examined in #TARGET_REF is missing; (b) CNN's performance in the previous research is not quite clear 2 ; and (c) some important techniques that can improve CNN performance (e.g., using variedsized filters and dropout regularization (#REF) ) were missing.\n sent4: Therefore, the present study is meant to address these limitations.\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Following #TARGET_REF , we applied Random Forest (#REF ) to do humor recognition by using the following two groups of features.",
                "The first group are latent semantic structural features covering the following 4 categories 5 : Incongruity (2), Ambiguity (6), Interpersonal Effect (4), and Phonetic Pattern (4).",
                "The second group are semantic distance features, including the humor label classes from 5 sentences in the training set that are closest to this sentence (found by using a k-Nearest Neighbors (kNN) method), and each sentence's averaged Word2Vec representations (n = 300)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Following #TARGET_REF , we applied Random Forest (#REF ) to do humor recognition by using the following two groups of features.\n sent1: The first group are latent semantic structural features covering the following 4 categories 5 : Incongruity (2), Ambiguity (6), Interpersonal Effect (4), and Phonetic Pattern (4).\n sent2: The second group are semantic distance features, including the humor label classes from 5 sentences in the training set that are closest to this sentence (found by using a k-Nearest Neighbors (kNN) method), and each sentence's averaged Word2Vec representations (n = 300).\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "We used two corpora: the TED Talk corpus (denoted as TED) and the Pun of the Day corpus 6 (denoted as Pun).",
                "Note that we normalized words in the Pun data to lowercase to avoid avoid a possibly elevated result caused by a special pattern: in the original format, all negative instances started with capital letters.",
                "The Pun data allows us to verify that our implementation is consistent with the work reported in #TARGET_REF .",
                "In our experiment, we firstly divided each corpus into two parts.",
                "The smaller part (the Held-Out Partition) was used for tweaking various hyper- Table 1 : Humor recognition on both Pun and TED data sets by using (a) random prediction (Chance), conventional method (Base) and CNN method; the sizes of the dev and CV partitions are provided for each data set."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: We used two corpora: the TED Talk corpus (denoted as TED) and the Pun of the Day corpus 6 (denoted as Pun).\n sent1: Note that we normalized words in the Pun data to lowercase to avoid avoid a possibly elevated result caused by a special pattern: in the original format, all negative instances started with capital letters.\n sent2: The Pun data allows us to verify that our implementation is consistent with the work reported in #TARGET_REF .\n sent3: In our experiment, we firstly divided each corpus into two parts.\n sent4: The smaller part (the Held-Out Partition) was used for tweaking various hyper- Table 1 : Humor recognition on both Pun and TED data sets by using (a) random prediction (Chance), conventional method (Base) and CNN method; the sizes of the dev and CV partitions are provided for each data set.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) #TARGET_REF subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena.",
                "The BERT model performs remarkably well on all cases."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) #TARGET_REF subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena.\n sent1: The BERT model performs remarkably well on all cases.\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "[CLS] the game that the guard hates [MASK] bad .",
                "and compare the scores predicted for is and are.",
                "This differs from #REF and #TARGET_REF by considering the entire sentence (excluding the verb) and not just its prefix leading to the verb, and differs from #REF by conditioning the focus verb on bidirectional context.",
                "I use the PyTorch implementation of BERT, with the pre-trained models supplied by Google.",
                "4 I experiment with the bert-large-uncased and bert-base-uncased models."
            ],
            "label": [
                "MOTIVATION",
                "EXTENDS"
            ]
        },
        "input": "sent0: [CLS] the game that the guard hates [MASK] bad .\n sent1: and compare the scores predicted for is and are.\n sent2: This differs from #REF and #TARGET_REF by considering the entire sentence (excluding the verb) and not just its prefix leading to the verb, and differs from #REF by conditioning the focus verb on bidirectional context.\n sent3: I use the PyTorch implementation of BERT, with the pre-trained models supplied by Google.\n sent4: 4 I experiment with the bert-large-uncased and bert-base-uncased models.\n",
        "output": "{\"label\": [\"MOTIVATION\", \"EXTENDS\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "In particular, in (#REF) we assess the ability of LSTMs to learn subject-verb agreement patterns in English, and evaluate on naturally occurring wikipedia sentences.",
                "#TARGET_REF also consider subject-verb agreement, but in a #TARGET_REF setting in which content words in naturally occurring sentences are replaced with random words with the same partof-speech and inflection, thus ensuring a focus on syntax rather than on selectional-preferences based cues.",
                "#REF consider a wider range of syntactic phenomena (subjectverb agreement, reflexive anaphora, negative polarity items) using manually constructed stimuli, allowing for greater coverage and control than in the naturally occurring setting.",
                "The BERT model is based on the \"Transformer\" architecture (#REF) , which-in contrast to RNNs-relies purely on attention mechanisms, and does not have an explicit notion of word order beyond marking each word with its absolute-position embedding.",
                "This reliance on attention may lead one 1 to expect decreased performance on syntax-sensitive tasks compared to RNN (LSTM) models that do model word order directly, and explicitly track states across the sentence."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: In particular, in (#REF) we assess the ability of LSTMs to learn subject-verb agreement patterns in English, and evaluate on naturally occurring wikipedia sentences.\n sent1: #TARGET_REF also consider subject-verb agreement, but in a #TARGET_REF setting in which content words in naturally occurring sentences are replaced with random words with the same partof-speech and inflection, thus ensuring a focus on syntax rather than on selectional-preferences based cues.\n sent2: #REF consider a wider range of syntactic phenomena (subjectverb agreement, reflexive anaphora, negative polarity items) using manually constructed stimuli, allowing for greater coverage and control than in the naturally occurring setting.\n sent3: The BERT model is based on the \"Transformer\" architecture (#REF) , which-in contrast to RNNs-relies purely on attention mechanisms, and does not have an explicit notion of word order beyond marking each word with its absolute-position embedding.\n sent4: This reliance on attention may lead one 1 to expect decreased performance on syntax-sensitive tasks compared to RNN (LSTM) models that do model word order directly, and explicitly track states across the sentence.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "4 I experiment with the bert-large-uncased and bert-base-uncased models.",
                "Discarded Material The bi-directional setup precludes using using the NPI stimuli of #REF , in which the minimal pair differs in two words position, which I discard from the evaluation.",
                "I also discard the agreement cases involving the verbs is or are in #REF and in #TARGET_REF , because some of them are copular construction, in which strong agreement hints can be found also on the object following the verb.",
                "5 This is not an issue in the manually constructed (#REF ) stimuli due to the patterns they chose.",
                "Finally, I discard stimuli in which the focus verb or its plural/singular inflection does not appear as a single word in the BERT wordpiece-based vocabulary (and hence cannot be predicted by the model)."
            ],
            "label": [
                "DIFFERENCES",
                "EXTENDS"
            ]
        },
        "input": "sent0: 4 I experiment with the bert-large-uncased and bert-base-uncased models.\n sent1: Discarded Material The bi-directional setup precludes using using the NPI stimuli of #REF , in which the minimal pair differs in two words position, which I discard from the evaluation.\n sent2: I also discard the agreement cases involving the verbs is or are in #REF and in #TARGET_REF , because some of them are copular construction, in which strong agreement hints can be found also on the object following the verb.\n sent3: 5 This is not an issue in the manually constructed (#REF ) stimuli due to the patterns they chose.\n sent4: Finally, I discard stimuli in which the focus verb or its plural/singular inflection does not appear as a single word in the BERT wordpiece-based vocabulary (and hence cannot be predicted by the model).\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"EXTENDS\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "This reliance on attention may lead one 1 to expect decreased performance on syntax-sensitive tasks compared to RNN (LSTM) models that do model word order directly, and explicitly track states across the sentence.",
                "Indeed, #REF finds that transformerbased models perform worse than LSTM models on the #REF agreement prediction dataset.",
                "In contrast, (#REF) find that self-attention performs on par with LSTM for syntax sensitive dependencies in the context of machine-translation, and performance on syntactic tasks is correlated with the number of attention heads in multi-head attention.",
                "I adapt the evaluation protocol and stimuli of #REF , #TARGET_REF and #REF to the bidirectional setting required by BERT, and evaluate the pretrained BERT models (both the LARGE and the BASE models).",
                "Surprisingly (at least to me), the out-of-the-box models (without any task-specific fine-tuning) perform very well on all the syntactic tasks."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: This reliance on attention may lead one 1 to expect decreased performance on syntax-sensitive tasks compared to RNN (LSTM) models that do model word order directly, and explicitly track states across the sentence.\n sent1: Indeed, #REF finds that transformerbased models perform worse than LSTM models on the #REF agreement prediction dataset.\n sent2: In contrast, (#REF) find that self-attention performs on par with LSTM for syntax sensitive dependencies in the context of machine-translation, and performance on syntactic tasks is correlated with the number of attention heads in multi-head attention.\n sent3: I adapt the evaluation protocol and stimuli of #REF , #TARGET_REF and #REF to the bidirectional setting required by BERT, and evaluate the pretrained BERT models (both the LARGE and the BASE models).\n sent4: Surprisingly (at least to me), the out-of-the-box models (without any task-specific fine-tuning) perform very well on all the syntactic tasks.\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "I use the stimuli provided by (#REF; #TARGET_REF; #REF) , but change the experimental protocol to adapt it to the bidirectional nature of the BERT model.",
                "This requires discarding some of the stimuli, as described below.",
                "Thus, the numbers are not strictly comparable to those reported in previous work."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: I use the stimuli provided by (#REF; #TARGET_REF; #REF) , but change the experimental protocol to adapt it to the bidirectional nature of the BERT model.\n sent1: This requires discarding some of the stimuli, as described below.\n sent2: Thus, the numbers are not strictly comparable to those reported in previous work.\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "This include discarding #REF stimuli involving the words swims or admires, resulting in 23,368 discarded pairs (out of 152,300).",
                "I similarly discard 680 sentences from (#REF) where the focus verb or its inflection were one of 108 out-ofvocabulary tokens, 6 and 28 sentence-pairs (8 tokens 7 ) from #TARGET_REF .",
                "Limitations The BERT results are not directly comparable to the numbers reported in previous work.",
                "Beyond the differences due to bidirectionality and the discarded stimuli, the BERT models are also trained on a different and larger corpus (covering both wikipedia and books).",
                "4 https://github.com/huggingface/pytorch-pretrained-BERT 5 Results are generally a bit higher when not discarding the is/are cases."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: This include discarding #REF stimuli involving the words swims or admires, resulting in 23,368 discarded pairs (out of 152,300).\n sent1: I similarly discard 680 sentences from (#REF) where the focus verb or its inflection were one of 108 out-ofvocabulary tokens, 6 and 28 sentence-pairs (8 tokens 7 ) from #TARGET_REF .\n sent2: Limitations The BERT results are not directly comparable to the numbers reported in previous work.\n sent3: Beyond the differences due to bidirectionality and the discarded stimuli, the BERT models are also trained on a different and larger corpus (covering both wikipedia and books).\n sent4: 4 https://github.com/huggingface/pytorch-pretrained-BERT 5 Results are generally a bit higher when not discarding the is/are cases.\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Limitations The BERT results are not directly comparable to the numbers reported in previous work.",
                "Beyond the differences due to bidirectionality and the discarded stimuli, the BERT models are also trained on a different and larger corpus (covering both wikipedia and books).",
                "4 https://github.com/huggingface/pytorch-pretrained-BERT 5 Results are generally a bit higher when not discarding the is/are cases.",
                "6 blames, dislike, inhabit, exclude, revolves, governs, delete, composes, overlap, edits, embrace, compose, undertakes, disagrees, redirect, persist, recognise, rotates, accompanies, attach, undertake, earn, communicates, imagine, contradicts, specialize, accuses, obtain, caters, welcomes, interprets, await, communicate, templates, qualify, reverts, achieve, achieves, govern, restricts, violate, behave, emit, contend, adopt, overlaps, reproduces, rotate, defends, submit, revolve, lend, pertain, disagree, concentrate, detects, endorses, detect, predate, persists, consume, locates, earns, predict, interact, merge, consumes, behaves, locate, predates, enhances, predicts, integrates, inhabits, satisfy, contradict, swear, activate, restrict, satisfies, redirects, excludes, violates, interacts, admires, speculate, blame, drag, qualifies, activates, criticize, assures, welcome, depart, characterizes, defend, obtains, lends, strives, accuse, recognises, characterize, contends, perceive, complain, awaits 7 toss, spills, tosses, affirms, spill, melt, approves, affirm Table 2 : Results on the EN NONCE (#REF) stimuli.",
                "While not strictly comparable, the numbers reported by #TARGET_REF for the LSTM in this condition (on All) is 74.1 ± 1.6."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Limitations The BERT results are not directly comparable to the numbers reported in previous work.\n sent1: Beyond the differences due to bidirectionality and the discarded stimuli, the BERT models are also trained on a different and larger corpus (covering both wikipedia and books).\n sent2: 4 https://github.com/huggingface/pytorch-pretrained-BERT 5 Results are generally a bit higher when not discarding the is/are cases.\n sent3: 6 blames, dislike, inhabit, exclude, revolves, governs, delete, composes, overlap, edits, embrace, compose, undertakes, disagrees, redirect, persist, recognise, rotates, accompanies, attach, undertake, earn, communicates, imagine, contradicts, specialize, accuses, obtain, caters, welcomes, interprets, await, communicate, templates, qualify, reverts, achieve, achieves, govern, restricts, violate, behave, emit, contend, adopt, overlaps, reproduces, rotate, defends, submit, revolve, lend, pertain, disagree, concentrate, detects, endorses, detect, predate, persists, consume, locates, earns, predict, interact, merge, consumes, behaves, locate, predates, enhances, predicts, integrates, inhabits, satisfy, contradict, swear, activate, restrict, satisfies, redirects, excludes, violates, interacts, admires, speculate, blame, drag, qualifies, activates, criticize, assures, welcome, depart, characterizes, defend, obtains, lends, strives, accuse, recognises, characterize, contends, perceive, complain, awaits 7 toss, spills, tosses, affirms, spill, melt, approves, affirm Table 2 : Results on the EN NONCE (#REF) stimuli.\n sent4: While not strictly comparable, the numbers reported by #TARGET_REF for the LSTM in this condition (on All) is 74.1 ± 1.6.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "The BERT models perform remarkably well on all the syntactic test cases.",
                "I expected the attentionbased mechanism to fail on these (compared to the LSTM-based models), and am surprised by these results.",
                "The #TARGET_REF and #REF conditions rule out the possibility of overly relying on selectional preference cues or memorizing the wikipedia training data, and suggest real syntactic generalization is taking place.",
                "Exploring the extent to which deep purely-attention-based architectures such as BERT are capable of capturing hierarchy-sensitive and syntactic dependencies-as well as the mechanisms by which this is achieved-is a fascinating area for future research."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The BERT models perform remarkably well on all the syntactic test cases.\n sent1: I expected the attentionbased mechanism to fail on these (compared to the LSTM-based models), and am surprised by these results.\n sent2: The #TARGET_REF and #REF conditions rule out the possibility of overly relying on selectional preference cues or memorizing the wikipedia training data, and suggest real syntactic generalization is taking place.\n sent3: Exploring the extent to which deep purely-attention-based architectures such as BERT are capable of capturing hierarchy-sensitive and syntactic dependencies-as well as the mechanisms by which this is achieved-is a fascinating area for future research.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Qualia Structures have been originally introduced by #TARGET_REF and are used for a variety of purposes in Natural Language processing such as the analysis of compounds (#REF) , co-composition and coercion #TARGET_REF as well as for bridging reference resolution (#REF) .",
                "Further, it has also been argued that qualia structures and lexical semantic relations in general have applications in information retrieval (#REF; #REF) .",
                "One major bottleneck however is that currently Qualia Structures need to be created by hand, which is probably also the reason why there are no practical system using qualia structures, but a lot of systems using globally available resources such as WordNet (#REF) or FrameNet 1 1 http://framenet.icsi.berkeley.edu/ as source of lexical/world knowledge.",
                "The work described in this paper addresses this issue and presents an approach to automatically learning qualia structures for nominals from the Web.",
                "The approach is inspired in recent work on using the Web to identify instances of a relation of interest such as in (#REF) and (#REF) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Qualia Structures have been originally introduced by #TARGET_REF and are used for a variety of purposes in Natural Language processing such as the analysis of compounds (#REF) , co-composition and coercion #TARGET_REF as well as for bridging reference resolution (#REF) .\n sent1: Further, it has also been argued that qualia structures and lexical semantic relations in general have applications in information retrieval (#REF; #REF) .\n sent2: One major bottleneck however is that currently Qualia Structures need to be created by hand, which is probably also the reason why there are no practical system using qualia structures, but a lot of systems using globally available resources such as WordNet (#REF) or FrameNet 1 1 http://framenet.icsi.berkeley.edu/ as source of lexical/world knowledge.\n sent3: The work described in this paper addresses this issue and presents an approach to automatically learning qualia structures for nominals from the Web.\n sent4: The approach is inspired in recent work on using the Web to identify instances of a relation of interest such as in (#REF) and (#REF) .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "According to Aristotle, there are four basic factors or causes by which the nature of an object can be described (cf. (#REF) ): the material cause, i.e. the material an object is made of the agentive cause, i.e. the source of movement, creation or change the formal cause, i.e. its form or type the final cause, i.e. its purpose, intention or aim In his Generative Lexicon (GL) framework #TARGET_REF reused Aristotle's basic factors for the description of the meaning of lexical elements.",
                "In fact he introduced so called Qualia Structures by which the meaning of a lexical element is described in terms of four roles:",
                "Constitutive: describing physical properties of an object, i.e. its weight, material as well as parts and components",
                "Agentive: describing factors involved in the bringing about of an object, i.e. its creator or the causal chain leading to its creation Formal: describing that properties which distinguish an object in a larger domain, i.e. orientation, magnitude, shape and dimensionality Telic: describing the purpose or function of an object Most of the qualia structures used in (#REF) however seem to have a more restricted interpretation.",
                "In fact, in most examples the Constitutive role seems to describe the parts or components of an object, while the Agentive role is typically described by a verb denoting an action which typically brings the object in question into existence."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: According to Aristotle, there are four basic factors or causes by which the nature of an object can be described (cf. (#REF) ): the material cause, i.e. the material an object is made of the agentive cause, i.e. the source of movement, creation or change the formal cause, i.e. its form or type the final cause, i.e. its purpose, intention or aim In his Generative Lexicon (GL) framework #TARGET_REF reused Aristotle's basic factors for the description of the meaning of lexical elements.\n sent1: In fact he introduced so called Qualia Structures by which the meaning of a lexical element is described in terms of four roles:\n sent2: Constitutive: describing physical properties of an object, i.e. its weight, material as well as parts and components\n sent3: Agentive: describing factors involved in the bringing about of an object, i.e. its creator or the causal chain leading to its creation Formal: describing that properties which distinguish an object in a larger domain, i.e. orientation, magnitude, shape and dimensionality Telic: describing the purpose or function of an object Most of the qualia structures used in (#REF) however seem to have a more restricted interpretation.\n sent4: In fact, in most examples the Constitutive role seems to describe the parts or components of an object, while the Agentive role is typically described by a verb denoting an action which typically brings the object in question into existence.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "According to Aristotle, there are four basic factors or causes by which the nature of an object can be described (cf. (#REF) ): the material cause, i.e. the material an object is made of the agentive cause, i.e. the source of movement, creation or change the formal cause, i.e. its form or type the final cause, i.e. its purpose, intention or aim In his Generative Lexicon (GL) framework (#REF) reused Aristotle's basic factors for the description of the meaning of lexical elements.",
                "In fact he introduced so called Qualia Structures by which the meaning of a lexical element is described in terms of four roles:",
                "Constitutive: describing physical properties of an object, i.e. its weight, material as well as parts and components",
                "Agentive: describing factors involved in the bringing about of an object, i.e. its creator or the causal chain leading to its creation Formal: describing that properties which distinguish an object in a larger domain, i.e. orientation, magnitude, shape and dimensionality Telic: describing the purpose or function of an object Most of the qualia structures used in #TARGET_REF however seem to have a more restricted interpretation.",
                "In fact, in most examples the Constitutive role seems to describe the parts or components of an object, while the Agentive role is typically described by a verb denoting an action which typically brings the object in question into existence."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: According to Aristotle, there are four basic factors or causes by which the nature of an object can be described (cf. (#REF) ): the material cause, i.e. the material an object is made of the agentive cause, i.e. the source of movement, creation or change the formal cause, i.e. its form or type the final cause, i.e. its purpose, intention or aim In his Generative Lexicon (GL) framework (#REF) reused Aristotle's basic factors for the description of the meaning of lexical elements.\n sent1: In fact he introduced so called Qualia Structures by which the meaning of a lexical element is described in terms of four roles:\n sent2: Constitutive: describing physical properties of an object, i.e. its weight, material as well as parts and components\n sent3: Agentive: describing factors involved in the bringing about of an object, i.e. its creator or the causal chain leading to its creation Formal: describing that properties which distinguish an object in a larger domain, i.e. orientation, magnitude, shape and dimensionality Telic: describing the purpose or function of an object Most of the qualia structures used in #TARGET_REF however seem to have a more restricted interpretation.\n sent4: In fact, in most examples the Constitutive role seems to describe the parts or components of an object, while the Agentive role is typically described by a verb denoting an action which typically brings the object in question into existence.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "So instead of generating clues as above, we calculate the value",
                "for the nominal we want to acquire a qualia structure for as well as the following verbs: build, produce, make, write, plant, elect, create, cook, construct and design.",
                "If this value is over a threshold (0.0005 in our case), we assume that it is a valid filler of the Agentive qualia role.",
                "#REF) or #TARGET_REF , as well as computer, an abstract noun, i.e. conversation, as well as two very specific multi-term words, i.e. natural language processing and data mining.",
                "We give the automatically learned weighted Qualia Structures for these entries in Figures 3,  4 , 5 and 6."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: So instead of generating clues as above, we calculate the value\n sent1: for the nominal we want to acquire a qualia structure for as well as the following verbs: build, produce, make, write, plant, elect, create, cook, construct and design.\n sent2: If this value is over a threshold (0.0005 in our case), we assume that it is a valid filler of the Agentive qualia role.\n sent3: #REF) or #TARGET_REF , as well as computer, an abstract noun, i.e. conversation, as well as two very specific multi-term words, i.e. natural language processing and data mining.\n sent4: We give the automatically learned weighted Qualia Structures for these entries in Figures 3,  4 , 5 and 6.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The result is then a Weighted Qualia Structure (WQS) in which for each role the qualia elements are weighted according to this Jaccard coefficient.",
                "In what follows we describe in detail the procedure for acquiring qualia elements for each qualia role.",
                "In particular, we describe in detail the clues and lexico-syntactic patterns used.",
                "In general, the patterns have been crafted by hand, testing and refining them in an iterative process, paying attention to maximize their coverage but also accuracy.",
                "In general it is important to mention that by this approach we are not able to detect and separate multiple meanings of words, i.e. to handle polysemy, which is appropriately accounted for in the framework of the Generative Lexicon #TARGET_REF ."
            ],
            "label": [
                "DIFFERENCES",
                "EXTENDS"
            ]
        },
        "input": "sent0: The result is then a Weighted Qualia Structure (WQS) in which for each role the qualia elements are weighted according to this Jaccard coefficient.\n sent1: In what follows we describe in detail the procedure for acquiring qualia elements for each qualia role.\n sent2: In particular, we describe in detail the clues and lexico-syntactic patterns used.\n sent3: In general, the patterns have been crafted by hand, testing and refining them in an iterative process, paying attention to maximize their coverage but also accuracy.\n sent4: In general it is important to mention that by this approach we are not able to detect and separate multiple meanings of words, i.e. to handle polysemy, which is appropriately accounted for in the framework of the Generative Lexicon #TARGET_REF .\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"EXTENDS\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "In particular, we discuss more in detail the qualia structure for book, knife and beer and leave the detailed assessment of the qualia structures for computer, natural language processing, data mining and conversation to the interested reader.",
                "For book, the first four candidates of the Formal role, i.e. product, item, publication and document are very appropriate, but alluding to the physical object meaning of book as opposed to the meaning in the sense of information container (compare #TARGET_REF .",
                "As candidates for the Agentive role we have make, write and create which are appropriate, write being the ideal filler of the Agentive role according to (#REF) .",
                "For the Constitutive role of book we get -besides it at the first position which could be easily filtered out -sign (2nd position), letter (3rd position) and page (6th position), which are quite appropriate.",
                "The top four candidates for the Telic role are give, select, read and purchase."
            ],
            "label": [
                "DIFFERENCES",
                "EXTENDS"
            ]
        },
        "input": "sent0: In particular, we discuss more in detail the qualia structure for book, knife and beer and leave the detailed assessment of the qualia structures for computer, natural language processing, data mining and conversation to the interested reader.\n sent1: For book, the first four candidates of the Formal role, i.e. product, item, publication and document are very appropriate, but alluding to the physical object meaning of book as opposed to the meaning in the sense of information container (compare #TARGET_REF .\n sent2: As candidates for the Agentive role we have make, write and create which are appropriate, write being the ideal filler of the Agentive role according to (#REF) .\n sent3: For the Constitutive role of book we get -besides it at the first position which could be easily filtered out -sign (2nd position), letter (3rd position) and page (6th position), which are quite appropriate.\n sent4: The top four candidates for the Telic role are give, select, read and purchase.\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"EXTENDS\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "In this section we provide a more subjective evaluation of the automatically learned qualia structures by comparing them to ideal qualia structures discussed in the literature wherever possible.",
                "In particular, we discuss more in detail the qualia structure for book, knife and beer and leave the detailed assessment of the qualia structures for computer, natural language processing, data mining and conversation to the interested reader.",
                "For book, the first four candidates of the Formal role, i.e. product, item, publication and document are very appropriate, but alluding to the physical object meaning of book as opposed to the meaning in the sense of information container (compare (#REF) .",
                "As candidates for the Agentive role we have make, write and create which are appropriate, write being the ideal filler of the Agentive role according to #TARGET_REF .",
                "For the Constitutive role of book we get -besides it at the first position which could be easily filtered out -sign (2nd position), letter (3rd position) and page (6th position), which are quite appropriate."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: In this section we provide a more subjective evaluation of the automatically learned qualia structures by comparing them to ideal qualia structures discussed in the literature wherever possible.\n sent1: In particular, we discuss more in detail the qualia structure for book, knife and beer and leave the detailed assessment of the qualia structures for computer, natural language processing, data mining and conversation to the interested reader.\n sent2: For book, the first four candidates of the Formal role, i.e. product, item, publication and document are very appropriate, but alluding to the physical object meaning of book as opposed to the meaning in the sense of information container (compare (#REF) .\n sent3: As candidates for the Agentive role we have make, write and create which are appropriate, write being the ideal filler of the Agentive role according to #TARGET_REF .\n sent4: For the Constitutive role of book we get -besides it at the first position which could be easily filtered out -sign (2nd position), letter (3rd position) and page (6th position), which are quite appropriate.\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The top four candidates for the Telic role are give, select, read and purchase.",
                "It seems that give is emphasizing the role of a book as a gift, read is referring to the most obvious purpose of a book as specified in the ideal qualia structures of #TARGET_REF as well as (#REF) and purchase denotes the more general purpose of a book, i.e. to be bought.",
                "The first element of the Formal role of knife unfortunately denotes the material it is typically made of, i.e. steel, but the next 5 elements are definitely appropriate: weapon, item, kitchenware, object and instrument.",
                "The ideal element artifact tool (compare (#REF) ) can be found at the 10th position.",
                "The results are interesting in that on the one hand the most prominent meaning of knife according to the web is the one of a weapon."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: The top four candidates for the Telic role are give, select, read and purchase.\n sent1: It seems that give is emphasizing the role of a book as a gift, read is referring to the most obvious purpose of a book as specified in the ideal qualia structures of #TARGET_REF as well as (#REF) and purchase denotes the more general purpose of a book, i.e. to be bought.\n sent2: The first element of the Formal role of knife unfortunately denotes the material it is typically made of, i.e. steel, but the next 5 elements are definitely appropriate: weapon, item, kitchenware, object and instrument.\n sent3: The ideal element artifact tool (compare (#REF) ) can be found at the 10th position.\n sent4: The results are interesting in that on the one hand the most prominent meaning of knife according to the web is the one of a weapon.\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Finally, the top four candidates for the Telic role are kill, slit, cut and slice, whereby cut corresponds to the ideal filler of the qualia structure for knife as mentioned in (#REF) .",
                "Considering the qualia structure for beer, it is surprising that no purpose has been found.",
                "The reason is that currently no results are returned by Google for the clue a beer is used to and the four snippets returned for the purpose of a beer contain expressions of the form the purpose of a beer is to drink it which is not matched by our patterns as it is a pronoun and not matched by our NP pattern (unless it is matched by an error as in the Qualia Structure for book in Figure 4 ).",
                "Considering the results for the Formal role, the elements drink (1st), alcohol (2nd) and beverage (4th) are much more specific than liquid as given in #TARGET_REF , while thing at the 3rd position is certainly too general.",
                "Furthermore, according to the automatically learned qualia structure, beer is made of rice, malt and hop, which are perfectly reasonable results."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Finally, the top four candidates for the Telic role are kill, slit, cut and slice, whereby cut corresponds to the ideal filler of the qualia structure for knife as mentioned in (#REF) .\n sent1: Considering the qualia structure for beer, it is surprising that no purpose has been found.\n sent2: The reason is that currently no results are returned by Google for the clue a beer is used to and the four snippets returned for the purpose of a beer contain expressions of the form the purpose of a beer is to drink it which is not matched by our patterns as it is a pronoun and not matched by our NP pattern (unless it is matched by an error as in the Qualia Structure for book in Figure 4 ).\n sent3: Considering the results for the Formal role, the elements drink (1st), alcohol (2nd) and beverage (4th) are much more specific than liquid as given in #TARGET_REF , while thing at the 3rd position is certainly too general.\n sent4: Furthermore, according to the automatically learned qualia structure, beer is made of rice, malt and hop, which are perfectly reasonable results.\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Such classes can be identified across the entire lexicon, and they may also apply across languages, since their meaning components are said to be cross-linguistically applicable (#REF) .",
                "Offering a powerful tool for generalization, abstraction and prediction, VerbNet classes have been used to support many important NLP tasks, including e.g. computational lexicography, parsing, word sense disambiguation, semantic role labeling, information extraction, questionanswering, and machine translation (#REF; #REF; #REF; #REF) .",
                "However, to date their exploitation has been limited because for most languages, no Levin style classification is available.",
                "Since manual classification is costly (#REF) automatic approaches have been proposed recently which could be used to learn novel classifications in a cost-effective manner (#REF; #REF; Ó Séaghdha and #REF; #REF; #TARGET_REF ).",
                "However, most work on Levin type classification has focussed on English."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Such classes can be identified across the entire lexicon, and they may also apply across languages, since their meaning components are said to be cross-linguistically applicable (#REF) .\n sent1: Offering a powerful tool for generalization, abstraction and prediction, VerbNet classes have been used to support many important NLP tasks, including e.g. computational lexicography, parsing, word sense disambiguation, semantic role labeling, information extraction, questionanswering, and machine translation (#REF; #REF; #REF; #REF) .\n sent2: However, to date their exploitation has been limited because for most languages, no Levin style classification is available.\n sent3: Since manual classification is costly (#REF) automatic approaches have been proposed recently which could be used to learn novel classifications in a cost-effective manner (#REF; #REF; Ó Séaghdha and #REF; #REF; #TARGET_REF ).\n sent4: However, most work on Levin type classification has focussed on English.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Spectral clustering (SPEC) has proved promising in previous verb clustering experiments (Brew and Schulte im #REF; #TARGET_REF ) and other similar NLP tasks involving high dimensional feature space (#REF) .",
                "Following #REF we used the MNCut spectral clustering (#REF ) which has a wide applicability and a clear probabilistic interpretation (von #REF; #REF) .",
                "However, we extended the method to determine the optimal number of clusters automatically using the technique proposed by (#REF) .",
                "Clustering groups a given set of verbs V = {v n } N n=1 into a disjoint partition of K classes.",
                "SPEC takes a similarity matrix as input."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Spectral clustering (SPEC) has proved promising in previous verb clustering experiments (Brew and Schulte im #REF; #TARGET_REF ) and other similar NLP tasks involving high dimensional feature space (#REF) .\n sent1: Following #REF we used the MNCut spectral clustering (#REF ) which has a wide applicability and a clear probabilistic interpretation (von #REF; #REF) .\n sent2: However, we extended the method to determine the optimal number of clusters automatically using the technique proposed by (#REF) .\n sent3: Clustering groups a given set of verbs V = {v n } N n=1 into a disjoint partition of K classes.\n sent4: SPEC takes a similarity matrix as input.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "In past years, verb classification techniques -in particular unsupervised ones -have improved considerably, making investigations for a new language more feasible.",
                "We take a recent verb clustering approach developed for English #TARGET_REF ) and apply it to French -a major language for which no such experiment has been conducted yet.",
                "Basic NLP resources (corpora, taggers, parsers and subcategorization acquisition systems) are now sufficiently developed for this language for the application of a state-ofthe-art verb clustering approach to be realistic.",
                "Our investigation reveals similarities between the English and French classifications, supporting the linguistic hypothesis (#REF) and the earlier result of #REF that Levin classes have a strong cross-linguistic basis.",
                "Not only the general methodology but also best performing features are transferable between the languages, making it possible to learn useful classes for French automatically without language-specific tuning."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In past years, verb classification techniques -in particular unsupervised ones -have improved considerably, making investigations for a new language more feasible.\n sent1: We take a recent verb clustering approach developed for English #TARGET_REF ) and apply it to French -a major language for which no such experiment has been conducted yet.\n sent2: Basic NLP resources (corpora, taggers, parsers and subcategorization acquisition systems) are now sufficiently developed for this language for the application of a state-ofthe-art verb clustering approach to be realistic.\n sent3: Our investigation reveals similarities between the English and French classifications, supporting the linguistic hypothesis (#REF) and the earlier result of #REF that Levin classes have a strong cross-linguistic basis.\n sent4: Not only the general methodology but also best performing features are transferable between the languages, making it possible to learn useful classes for French automatically without language-specific tuning.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Spectral clustering (SPEC) has proved promising in previous verb clustering experiments (Brew and Schulte im #REF; #REF ) and other similar NLP tasks involving high dimensional feature space (#REF) .",
                "Following #TARGET_REF we used the MNCut spectral clustering (#REF ) which has a wide applicability and a clear probabilistic interpretation (von #REF; #REF) .",
                "However, we extended the method to determine the optimal number of clusters automatically using the technique proposed by (#REF) .",
                "Clustering groups a given set of verbs V = {v n } N n=1 into a disjoint partition of K classes.",
                "SPEC takes a similarity matrix as input."
            ],
            "label": [
                "USE",
                "SIMILARITY"
            ]
        },
        "input": "sent0: Spectral clustering (SPEC) has proved promising in previous verb clustering experiments (Brew and Schulte im #REF; #REF ) and other similar NLP tasks involving high dimensional feature space (#REF) .\n sent1: Following #TARGET_REF we used the MNCut spectral clustering (#REF ) which has a wide applicability and a clear probabilistic interpretation (von #REF; #REF) .\n sent2: However, we extended the method to determine the optimal number of clusters automatically using the technique proposed by (#REF) .\n sent3: Clustering groups a given set of verbs V = {v n } N n=1 into a disjoint partition of K classes.\n sent4: SPEC takes a similarity matrix as input.\n",
        "output": "{\"label\": [\"USE\", \"SIMILARITY\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We employ the same measures for evaluation as previously employed e.g. byÓ Séaghdha and #REF and #TARGET_REF .",
                "The first measure is modified purity (mPUR) -a global measure which evaluates the mean precision of clusters.",
                "Each cluster is associated with its prevalent class.",
                "The number of verbs in a cluster K that take this class is denoted by n prevalent (K).",
                "Verbs that do not take it are considered as errors."
            ],
            "label": [
                "USE",
                "SIMILARITY"
            ]
        },
        "input": "sent0: We employ the same measures for evaluation as previously employed e.g. byÓ Séaghdha and #REF and #TARGET_REF .\n sent1: The first measure is modified purity (mPUR) -a global measure which evaluates the mean precision of clusters.\n sent2: Each cluster is associated with its prevalent class.\n sent3: The number of verbs in a cluster K that take this class is denoted by n prevalent (K).\n sent4: Verbs that do not take it are considered as errors.\n",
        "output": "{\"label\": [\"USE\", \"SIMILARITY\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "It was acquired fully automatically from Le Monde newspaper corpus (200M words from years 1991-2000) using ASSCI -a recent subcategorization acquisition system for French (#REF) .",
                "Systems similar to ASSCI have been used in recent verb classification works e.g. (Schulte im #REF; #REF; #TARGET_REF ).",
                "Like these other systems, ASSCI takes raw corpus data as input.",
                "The data is first tagged and lemmatized using the Tree-Tagger and then parsed using Syntex (#REF) .",
                "Syntex is a shallow parser which employs a combination of statistics and heuristics to identify grammatical relations (GRs) in sentences."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: It was acquired fully automatically from Le Monde newspaper corpus (200M words from years 1991-2000) using ASSCI -a recent subcategorization acquisition system for French (#REF) .\n sent1: Systems similar to ASSCI have been used in recent verb classification works e.g. (Schulte im #REF; #REF; #TARGET_REF ).\n sent2: Like these other systems, ASSCI takes raw corpus data as input.\n sent3: The data is first tagged and lemmatized using the Tree-Tagger and then parsed using Syntex (#REF) .\n sent4: Syntex is a shallow parser which employs a combination of statistics and heuristics to identify grammatical relations (GRs) in sentences.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "In our first experiment, we evaluated 116 verbsthose which appeared in LexSchem the minimum of 150 times.",
                "We did this because English experiments had shown that due to the Zipfian nature of SCF distributions, 150 corpus occurrences are typically needed to obtain a sufficient number of frames for clustering (#REF) .",
                "Table 2 shows F-measure results for all the features.",
                "The 4th column of the table shows, for comparison, the results of #TARGET_REF obtained for English when they used the same features as us, clustered them using SPEC, and evaluated them against the English version of our gold standard, also using F-measure 2 .",
                "As expected, SPEC (the 2nd column) outperforms K-Means (the 3rd column)."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: In our first experiment, we evaluated 116 verbsthose which appeared in LexSchem the minimum of 150 times.\n sent1: We did this because English experiments had shown that due to the Zipfian nature of SCF distributions, 150 corpus occurrences are typically needed to obtain a sufficient number of frames for clustering (#REF) .\n sent2: Table 2 shows F-measure results for all the features.\n sent3: The 4th column of the table shows, for comparison, the results of #TARGET_REF obtained for English when they used the same features as us, clustered them using SPEC, and evaluated them against the English version of our gold standard, also using F-measure 2 .\n sent4: As expected, SPEC (the 2nd column) outperforms K-Means (the 3rd column).\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF occurrences per verb are used, the differences become clearer, until at the threshold of 4000, it is obvious that the most sophisticated SCF-SP feature F17 is by far the best feature for French (65.4 F) and the SCF feature F3 the second best (60.5 F).",
                "The COfeature F7 and the LP feature F13 are not nearly as good (53.4 and 51.0 F).",
                "Although the results at different thresholds are not comparable due to the different number of verbs and classes (see columns 2-3), the results for features at the same threshold are.",
                "Those results suggest that when 2000 or more occurrences per verb are used, most features perform like they performed for English in the experiment of #TARGET_REF which is not typical to many other classes.",
                "Interestingly, Levin classes 29.2, 36.1, 37.3, and 37.7 were among the best performing classes also in the supervised verb classification experiment of #REF because these classes have distinctive characteristics also in English."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: #REF occurrences per verb are used, the differences become clearer, until at the threshold of 4000, it is obvious that the most sophisticated SCF-SP feature F17 is by far the best feature for French (65.4 F) and the SCF feature F3 the second best (60.5 F).\n sent1: The COfeature F7 and the LP feature F13 are not nearly as good (53.4 and 51.0 F).\n sent2: Although the results at different thresholds are not comparable due to the different number of verbs and classes (see columns 2-3), the results for features at the same threshold are.\n sent3: Those results suggest that when 2000 or more occurrences per verb are used, most features perform like they performed for English in the experiment of #TARGET_REF which is not typical to many other classes.\n sent4: Interestingly, Levin classes 29.2, 36.1, 37.3, and 37.7 were among the best performing classes also in the supervised verb classification experiment of #REF because these classes have distinctive characteristics also in English.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "We adopt the best method of #REF where collocations (COs) are extracted from the window of words immediately preceding and following a lemmatized verb.",
                "Stop words are removed prior to extraction.",
                "We adopt a fully unsupervised approach to SP acquisition using the method of #TARGET_REF , with the difference that we determine the optimal number of SP clusters automatically following #REF .",
                "The method is introduced in the following section.",
                "The approach involves (i) taking the GRs (SUBJ, OBJ, IOBJ) associated with verbs, (ii) extracting all the argument heads in these GRs, and (iii) clustering the resulting N most frequent argument heads into M classes."
            ],
            "label": [
                "DIFFERENCES",
                "EXTENDS"
            ]
        },
        "input": "sent0: We adopt the best method of #REF where collocations (COs) are extracted from the window of words immediately preceding and following a lemmatized verb.\n sent1: Stop words are removed prior to extraction.\n sent2: We adopt a fully unsupervised approach to SP acquisition using the method of #TARGET_REF , with the difference that we determine the optimal number of SP clusters automatically following #REF .\n sent3: The method is introduced in the following section.\n sent4: The approach involves (i) taking the GRs (SUBJ, OBJ, IOBJ) associated with verbs, (ii) extracting all the argument heads in these GRs, and (iii) clustering the resulting N most frequent argument heads into M classes.\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"EXTENDS\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "When sufficient corpus data is available, there is a strong correlation between the types of features which perform the best in English and French.",
                "When the best features are used, many individual Levin classes have similar performance in the two languages.",
                "Due to differences in data sets direct comparison of performance figures for English and French is not possible.",
                "When considering the general level of performance, our best performance for French (65.4 F) is lower than the best performance for English in the experiment of #TARGET_REF .",
                "However, it does compare favourably to the performance of other stateof-the-art (even supervised) English systems (#REF; #REF; Ó Séaghdha and #REF; #REF) ."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: When sufficient corpus data is available, there is a strong correlation between the types of features which perform the best in English and French.\n sent1: When the best features are used, many individual Levin classes have similar performance in the two languages.\n sent2: Due to differences in data sets direct comparison of performance figures for English and French is not possible.\n sent3: When considering the general level of performance, our best performance for French (65.4 F) is lower than the best performance for English in the experiment of #TARGET_REF .\n sent4: However, it does compare favourably to the performance of other stateof-the-art (even supervised) English systems (#REF; #REF; Ó Séaghdha and #REF; #REF) .\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "As seen in section 7.1, such differences in data can have significant impact on performance.",
                "However, parser and feature extraction performance can also play a big role in overall accuracy, and should therefore be investigated further #TARGET_REF ).",
                "The relatively low performance of basic LP features in French suggests that at least some of the current errors are due to parsing.",
                "Future research should investigate the source of error at different stages of processing.",
                "In addition, it would be interesting to investigate whether language-specific tuning (e.g. using language specific features such as auxiliary classes) can further improve performance on French."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: As seen in section 7.1, such differences in data can have significant impact on performance.\n sent1: However, parser and feature extraction performance can also play a big role in overall accuracy, and should therefore be investigated further #TARGET_REF ).\n sent2: The relatively low performance of basic LP features in French suggests that at least some of the current errors are due to parsing.\n sent3: Future research should investigate the source of error at different stages of processing.\n sent4: In addition, it would be interesting to investigate whether language-specific tuning (e.g. using language specific features such as auxiliary classes) can further improve performance on French.\n",
        "output": "{\"label\": [null], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The earliest attempt in (#REF) concatenated all supporting documents and designed a recurrent layer to explicitly exploit the skip connections between entities given automatically gener-ated coreference annotations.",
                "Adding this layer to the neural RC models improved performance on multi-hop tasks.",
                "Recently, an attention based system #TARGET_REF utilizing both documentlevel and entity-level information achieved stateof-the-art results on WIKIHOP data set, proving that techniques like co-attention and self-attention widely employed in single-document RC tasks are also useful in multi-document RC tasks.",
                "The second type of research work is based on graph neural networks (GNN) for multi-hop reasoning.",
                "The study in #REF In this paper, we propose a new method to solve the multi-hop RC problem across multiple documents."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The earliest attempt in (#REF) concatenated all supporting documents and designed a recurrent layer to explicitly exploit the skip connections between entities given automatically gener-ated coreference annotations.\n sent1: Adding this layer to the neural RC models improved performance on multi-hop tasks.\n sent2: Recently, an attention based system #TARGET_REF utilizing both documentlevel and entity-level information achieved stateof-the-art results on WIKIHOP data set, proving that techniques like co-attention and self-attention widely employed in single-document RC tasks are also useful in multi-document RC tasks.\n sent3: The second type of research work is based on graph neural networks (GNN) for multi-hop reasoning.\n sent4: The study in #REF In this paper, we propose a new method to solve the multi-hop RC problem across multiple documents.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Each mention is treated as an entity.",
                "Then, representations of entities can be taken out from the i-th document encoding H i s .",
                "We denote an entity's representation as M ∈ R lm×h where l m is the length of the entity.",
                "Co-attention: Co-attention has achieved great success for single document reading comprehension tasks (#REF; #REF) , and recently was applied to multiple-hop reading comprehension #TARGET_REF .",
                "Coattention enables the model to combine learned query contextual information attended by document and document contextual information attended by query, with inputs of one query and one document."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Each mention is treated as an entity.\n sent1: Then, representations of entities can be taken out from the i-th document encoding H i s .\n sent2: We denote an entity's representation as M ∈ R lm×h where l m is the length of the entity.\n sent3: Co-attention: Co-attention has achieved great success for single document reading comprehension tasks (#REF; #REF) , and recently was applied to multiple-hop reading comprehension #TARGET_REF .\n sent4: Coattention enables the model to combine learned query contextual information attended by document and document contextual information attended by query, with inputs of one query and one document.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "In addition, following the success of Coarse-grain Fine-grain Coattention (CFC) network #TARGET_REF , we apply both co-attention and self-attention to learn queryaware node representations of candidates, documents and entities;",
                "• The HDE graph enables rich information interaction among different types of nodes thus facilitate accurate reasoning.",
                "Different types of nodes are connected with different types of edges to highlight the various structural information presented among query, document and candidates.",
                "Through ablation studies, we show the effectiveness of our proposed HDE graph for multi-hop multi-document RC task.",
                "Evaluated on the blind test set of WIKIHOP, our proposed end-to-end trained single neural model beats the current stateof-the-art results in (#REF) 1 , without using pretrained contextual ELMo embedding (#REF) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In addition, following the success of Coarse-grain Fine-grain Coattention (CFC) network #TARGET_REF , we apply both co-attention and self-attention to learn queryaware node representations of candidates, documents and entities;\n sent1: • The HDE graph enables rich information interaction among different types of nodes thus facilitate accurate reasoning.\n sent2: Different types of nodes are connected with different types of edges to highlight the various structural information presented among query, document and candidates.\n sent3: Through ablation studies, we show the effectiveness of our proposed HDE graph for multi-hop multi-document RC task.\n sent4: Evaluated on the blind test set of WIKIHOP, our proposed end-to-end trained single neural model beats the current stateof-the-art results in (#REF) 1 , without using pretrained contextual ELMo embedding (#REF) .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "The study presented in this paper is directly related to existing research on multi-hop reading comprehension across multiple documents (#REF; #REF; De #REF; #TARGET_REF; #REF) .",
                "The method presented in this paper is similar to previous studies using GNN for multi-hop reasoning (#REF; De #REF) .",
                "Our novelty is that we propose to use a heterogeneous graph instead of a graph with single type of nodes to incorporate different granularity levels of information.",
                "The co-attention and self-attention based encoding of multi-level information presented in each input is also inspired by the CFC model (#REF) because they show the effectiveness of attention mechanisms.",
                "Our model is very different from the other two studies (#REF; #REF) : these two studies both explicitly score the possible reasoning paths with extra NER or coreference resolution systems while our method does not require these modules and we do multi-hop reasoning over graphs."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The study presented in this paper is directly related to existing research on multi-hop reading comprehension across multiple documents (#REF; #REF; De #REF; #TARGET_REF; #REF) .\n sent1: The method presented in this paper is similar to previous studies using GNN for multi-hop reasoning (#REF; De #REF) .\n sent2: Our novelty is that we propose to use a heterogeneous graph instead of a graph with single type of nodes to incorporate different granularity levels of information.\n sent3: The co-attention and self-attention based encoding of multi-level information presented in each input is also inspired by the CFC model (#REF) because they show the effectiveness of attention mechanisms.\n sent4: Our model is very different from the other two studies (#REF; #REF) : these two studies both explicitly score the possible reasoning paths with extra NER or coreference resolution systems while our method does not require these modules and we do multi-hop reasoning over graphs.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The method presented in this paper is similar to previous studies using GNN for multi-hop reasoning (#REF; De #REF) .",
                "Our novelty is that we propose to use a heterogeneous graph instead of a graph with single type of nodes to incorporate different granularity levels of information.",
                "The co-attention and self-attention based encoding of multi-level information presented in each input is also inspired by the CFC model #TARGET_REF because they show the effectiveness of attention mechanisms.",
                "Our model is very different from the other two studies (#REF; #REF) : these two studies both explicitly score the possible reasoning paths with extra NER or coreference resolution systems while our method does not require these modules and we do multi-hop reasoning over graphs.",
                "Besides these studies, our work is also related to the following research directions."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The method presented in this paper is similar to previous studies using GNN for multi-hop reasoning (#REF; De #REF) .\n sent1: Our novelty is that we propose to use a heterogeneous graph instead of a graph with single type of nodes to incorporate different granularity levels of information.\n sent2: The co-attention and self-attention based encoding of multi-level information presented in each input is also inspired by the CFC model #TARGET_REF because they show the effectiveness of attention mechanisms.\n sent3: Our model is very different from the other two studies (#REF; #REF) : these two studies both explicitly score the possible reasoning paths with extra NER or coreference resolution systems while our method does not require these modules and we do multi-hop reasoning over graphs.\n sent4: Besides these studies, our work is also related to the following research directions.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We will show later that by including mentions of query subject the performance can be improved.",
                "We use simple exact match strategy (De #REF; #TARGET_REF to find the locations of mentions of query subject and candidates, i.e. we need the start and end positions of each mention.",
                "Each mention is treated as an entity.",
                "Then, representations of entities can be taken out from the i-th document encoding H i s .",
                "We denote an entity's representation as M ∈ R lm×h where l m is the length of the entity."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We will show later that by including mentions of query subject the performance can be improved.\n sent1: We use simple exact match strategy (De #REF; #TARGET_REF to find the locations of mentions of query subject and candidates, i.e. we need the start and end positions of each mention.\n sent2: Each mention is treated as an entity.\n sent3: Then, representations of entities can be taken out from the i-th document encoding H i s .\n sent4: We denote an entity's representation as M ∈ R lm×h where l m is the length of the entity.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We denote an entity's representation as M ∈ R lm×h where l m is the length of the entity.",
                "Co-attention: Co-attention has achieved great success for single document reading comprehension tasks (#REF; #REF) , and recently was applied to multiple-hop reading comprehension (#REF) .",
                "Coattention enables the model to combine learned query contextual information attended by document and document contextual information attended by query, with inputs of one query and one document.",
                "We follow the implementation of coattention in #TARGET_REF .",
                "We use the co-attention between a query and a supporting document for illustration."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We denote an entity's representation as M ∈ R lm×h where l m is the length of the entity.\n sent1: Co-attention: Co-attention has achieved great success for single document reading comprehension tasks (#REF; #REF) , and recently was applied to multiple-hop reading comprehension (#REF) .\n sent2: Coattention enables the model to combine learned query contextual information attended by document and document contextual information attended by query, with inputs of one query and one document.\n sent3: We follow the implementation of coattention in #TARGET_REF .\n sent4: We use the co-attention between a query and a supporting document for illustration.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "sof tmax(·) denotes column-wise normalization.",
                "We further encode the co-attended document context using a bidirectional RNN f with GRU:",
                "The final co-attention context is the columnwise concatenation of C s and D s :",
                "We expect S ca carries query-aware contextual information of supporting documents as shown by #TARGET_REF .",
                "The same co-attention module can also be applied to query and candidates, and query and entities (as shown in Figure 2 ) to get C ca and E ca . Note that we do not do coattention between query and entities corresponding to query subject because query subject is already a part of the query."
            ],
            "label": [
                "USE",
                "DIFFERENCES"
            ]
        },
        "input": "sent0: sof tmax(·) denotes column-wise normalization.\n sent1: We further encode the co-attended document context using a bidirectional RNN f with GRU:\n sent2: The final co-attention context is the columnwise concatenation of C s and D s :\n sent3: We expect S ca carries query-aware contextual information of supporting documents as shown by #TARGET_REF .\n sent4: The same co-attention module can also be applied to query and candidates, and query and entities (as shown in Figure 2 ) to get C ca and E ca . Note that we do not do coattention between query and entities corresponding to query subject because query subject is already a part of the query.\n",
        "output": "{\"label\": [\"USE\", \"DIFFERENCES\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "To keep the dimensionality consistent, we apply a single-layer multi-layer perceptron (MLP) with tanh activation function to increase the dimension of the query subject entities to 2h.",
                "Self-attentive pooling: while co-attention yields a query-aware contextual representation of documents, self-attentive pooling is designed to convert the sequential contextual representation to a fixed dimensional non-sequential feature vector by selecting important query-aware information #TARGET_REF .",
                "Self-attentive pooling summarizes the information presented in the coattention output by calculating a score for each word in the sequence.",
                "The scores are normalized and a weighted sum based pooling is applied to the sequence to get a single feature vector as the summarization of the input sequence.",
                "Formally, the self-attention module can be formulated as the following operations given S ca as input:"
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: To keep the dimensionality consistent, we apply a single-layer multi-layer perceptron (MLP) with tanh activation function to increase the dimension of the query subject entities to 2h.\n sent1: Self-attentive pooling: while co-attention yields a query-aware contextual representation of documents, self-attentive pooling is designed to convert the sequential contextual representation to a fixed dimensional non-sequential feature vector by selecting important query-aware information #TARGET_REF .\n sent2: Self-attentive pooling summarizes the information presented in the coattention output by calculating a score for each word in the sequence.\n sent3: The scores are normalized and a weighted sum based pooling is applied to the sequence to get a single feature vector as the summarization of the input sequence.\n sent4: Formally, the self-attention module can be formulated as the following operations given S ca as input:\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "In addition, following the success of Coarse-grain Fine-grain Coattention (CFC) network (#REF) , we apply both co-attention and self-attention to learn queryaware node representations of candidates, documents and entities;",
                "• The HDE graph enables rich information interaction among different types of nodes thus facilitate accurate reasoning.",
                "Different types of nodes are connected with different types of edges to highlight the various structural information presented among query, document and candidates.",
                "Through ablation studies, we show the effectiveness of our proposed HDE graph for multi-hop multi-document RC task.",
                "Evaluated on the blind test set of WIKIHOP, our proposed end-to-end trained single neural model beats the current stateof-the-art results in #TARGET_REF 1 , without using pretrained contextual ELMo embedding (#REF) ."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: In addition, following the success of Coarse-grain Fine-grain Coattention (CFC) network (#REF) , we apply both co-attention and self-attention to learn queryaware node representations of candidates, documents and entities;\n sent1: • The HDE graph enables rich information interaction among different types of nodes thus facilitate accurate reasoning.\n sent2: Different types of nodes are connected with different types of edges to highlight the various structural information presented among query, document and candidates.\n sent3: Through ablation studies, we show the effectiveness of our proposed HDE graph for multi-hop multi-document RC task.\n sent4: Evaluated on the blind test set of WIKIHOP, our proposed end-to-end trained single neural model beats the current stateof-the-art results in #TARGET_REF 1 , without using pretrained contextual ELMo embedding (#REF) .\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "where M LP (·) is a two-layer MLP with tanh as activation function.",
                "Similarly, after self-attentive pooling, we can get c sa and e sa for each candidate and entity.",
                "Our context encoding module is different from the one used in #TARGET_REF in following aspects: 1) we compute the co-attention between query and candidates which is not presented in the CFC model.",
                "2) For entity word sequences, we first calculate co-attention with query and then use selfattention to summarize each entity word sequence while #REF first do self-attention on entity word sequences to get a sequence of entity vectors in each documents.",
                "Then, they apply coattention with query."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: where M LP (·) is a two-layer MLP with tanh as activation function.\n sent1: Similarly, after self-attentive pooling, we can get c sa and e sa for each candidate and entity.\n sent2: Our context encoding module is different from the one used in #TARGET_REF in following aspects: 1) we compute the co-attention between query and candidates which is not presented in the CFC model.\n sent3: 2) For entity word sequences, we first calculate co-attention with query and then use selfattention to summarize each entity word sequence while #REF first do self-attention on entity word sequences to get a sequence of entity vectors in each documents.\n sent4: Then, they apply coattention with query.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "where M LP (·) is a two-layer MLP with tanh as activation function.",
                "Similarly, after self-attentive pooling, we can get c sa and e sa for each candidate and entity.",
                "Our context encoding module is different from the one used in #REF in following aspects: 1) we compute the co-attention between query and candidates which is not presented in the CFC model.",
                "2) For entity word sequences, we first calculate co-attention with query and then use selfattention to summarize each entity word sequence while #TARGET_REF first do self-attention on entity word sequences to get a sequence of entity vectors in each documents.",
                "Then, they apply coattention with query."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: where M LP (·) is a two-layer MLP with tanh as activation function.\n sent1: Similarly, after self-attentive pooling, we can get c sa and e sa for each candidate and entity.\n sent2: Our context encoding module is different from the one used in #REF in following aspects: 1) we compute the co-attention between query and candidates which is not presented in the CFC model.\n sent3: 2) For entity word sequences, we first calculate co-attention with query and then use selfattention to summarize each entity word sequence while #TARGET_REF first do self-attention on entity word sequences to get a sequence of entity vectors in each documents.\n sent4: Then, they apply coattention with query.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "In Table 1 , we show the results of the our proposed HDE graph based model on both development and test set and compare it with previously published results.",
                "We show that our proposed HDE graph based model improves the state-of-the-art accuracy on development set from 67.1% (#REF) to 68.1%, on the blind test set from 70.6% #TARGET_REF to 70.9%.",
                "Compared to two previous studies using GNN for multi-hop reading comprehension (#REF; De #REF) , our model surpasses them by a large margin even though we do not use better pre-trained contextual embedding ELMo (#REF) ."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: In Table 1 , we show the results of the our proposed HDE graph based model on both development and test set and compare it with previously published results.\n sent1: We show that our proposed HDE graph based model improves the state-of-the-art accuracy on development set from 67.1% (#REF) to 68.1%, on the blind test set from 70.6% #TARGET_REF to 70.9%.\n sent2: Compared to two previous studies using GNN for multi-hop reading comprehension (#REF; De #REF) , our model surpasses them by a large margin even though we do not use better pre-trained contextual embedding ELMo (#REF) .\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "These methods make limited use of the social context in which the authors are tweeting -our research question is \"Can we identify the language of a tweet using the social graph of the tweeter?\".",
                "Label propagation approaches [8] are powerful techniques for semi-supervised learning where the domain can naturally be described using an undirected graph.",
                "Each node contains a probability distribution over labels, which may be empty for unlabelled nodes, and these labels are propagated over the graph in an iterative fashion.",
                "Modified Adsorption (mad) [6] , is an extension that allows more control of the random walk through the graph.",
                "Applications of lp and mad are varied, including video recommendation [1] and sentiment analysis over Twitter #TARGET_REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: These methods make limited use of the social context in which the authors are tweeting -our research question is \"Can we identify the language of a tweet using the social graph of the tweeter?\".\n sent1: Label propagation approaches [8] are powerful techniques for semi-supervised learning where the domain can naturally be described using an undirected graph.\n sent2: Each node contains a probability distribution over labels, which may be empty for unlabelled nodes, and these labels are propagated over the graph in an iterative fashion.\n sent3: Modified Adsorption (mad) [6] , is an extension that allows more control of the random walk through the graph.\n sent4: Applications of lp and mad are varied, including video recommendation [1] and sentiment analysis over Twitter #TARGET_REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "However, as pointed out in (#REF) and #TARGET_REF , abstractive summaries are preferred to extractive ones by human judges.",
                "The possible reason for this is that extractive techniques are not well suited for the conversation summarization, since there are style differences between spoken conversations and humanauthored summaries.",
                "Abstractive conversation summarization systems, on the other hand, are mainly based on the extraction of lexical information (#REF; #REF) .",
                "The authors cluster conversation sentences/utterances into communities to identify most relevant ones and aggregate them using word-graph models.",
                "The graph paths are ranked to yield abstract sentences -a template."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: However, as pointed out in (#REF) and #TARGET_REF , abstractive summaries are preferred to extractive ones by human judges.\n sent1: The possible reason for this is that extractive techniques are not well suited for the conversation summarization, since there are style differences between spoken conversations and humanauthored summaries.\n sent2: Abstractive conversation summarization systems, on the other hand, are mainly based on the extraction of lexical information (#REF; #REF) .\n sent3: The authors cluster conversation sentences/utterances into communities to identify most relevant ones and aggregate them using word-graph models.\n sent4: The graph paths are ranked to yield abstract sentences -a template.\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Abstractive conversation summarization systems, on the other hand, are mainly based on the extraction of lexical information (#REF; #TARGET_REF .",
                "The authors cluster conversation sentences/utterances into communities to identify most relevant ones and aggregate them using word-graph models.",
                "The graph paths are ranked to yield abstract sentences -a template.",
                "And these templates are selected for population with entities extracted from a conversation.",
                "Thus the abstractive summarization systems are limited to these templates generated by supervised data sources."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Abstractive conversation summarization systems, on the other hand, are mainly based on the extraction of lexical information (#REF; #TARGET_REF .\n sent1: The authors cluster conversation sentences/utterances into communities to identify most relevant ones and aggregate them using word-graph models.\n sent2: The graph paths are ranked to yield abstract sentences -a template.\n sent3: And these templates are selected for population with entities extracted from a conversation.\n sent4: Thus the abstractive summarization systems are limited to these templates generated by supervised data sources.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "The heuristics rely on the similarity between the two, and we experiment with the cosine similarity computation on different levels of representation -raw text, text after replacing the verbs with their WordNet SynSet IDs, and the similarity computed using distributed word embeddings.",
                "The heuristics are evaluated within the template-based abstractive summarization system of #TARGET_REF .",
                "We extend this system to Italian using required NLP tools.",
                "However, the approach transparently extends to other languages with available WordNet, minimal supervised summarization corpus and running text.",
                "Heuristics are evaluated and compared on AMI meeting corpus and Italian LUNA Human-Human conversation corpus."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The heuristics rely on the similarity between the two, and we experiment with the cosine similarity computation on different levels of representation -raw text, text after replacing the verbs with their WordNet SynSet IDs, and the similarity computed using distributed word embeddings.\n sent1: The heuristics are evaluated within the template-based abstractive summarization system of #TARGET_REF .\n sent2: We extend this system to Italian using required NLP tools.\n sent3: However, the approach transparently extends to other languages with available WordNet, minimal supervised summarization corpus and running text.\n sent4: Heuristics are evaluated and compared on AMI meeting corpus and Italian LUNA Human-Human conversation corpus.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Template Generation follows the approach of #TARGET_REF and, starting from human-authored summaries, produces abstract templates applying slot labeling, summary clustering and template fusion steps.",
                "The information required for the template generation are part-of-speech (POS) tags, noun and verb phrase chunks, and root verbs from dependency parsing.",
                "For English, we use Illinois Chunker (#REF) to identify noun phrases and extract part-of-speech tags; and the the tool of (De #REF) for generating dependency parses.",
                "For Italian, on the other hand, we use TextPro 2.0 (#REF) to perform all the Natural Language Processing tasks.",
                "In the slot labeling step, noun phrases from human-authored summaries are replaced by WordNet (#REF ) SynSet IDs of the head nouns (right most for English)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Template Generation follows the approach of #TARGET_REF and, starting from human-authored summaries, produces abstract templates applying slot labeling, summary clustering and template fusion steps.\n sent1: The information required for the template generation are part-of-speech (POS) tags, noun and verb phrase chunks, and root verbs from dependency parsing.\n sent2: For English, we use Illinois Chunker (#REF) to identify noun phrases and extract part-of-speech tags; and the the tool of (De #REF) for generating dependency parses.\n sent3: For Italian, on the other hand, we use TextPro 2.0 (#REF) to perform all the Natural Language Processing tasks.\n sent4: In the slot labeling step, noun phrases from human-authored summaries are replaced by WordNet (#REF ) SynSet IDs of the head nouns (right most for English).\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "The AMI meeting corpus (#REF ) is a collection of 139 meeting records where groups of people are engaged in a 'roleplay' as a team and each speaker assumes a certain role in a team (e.g. project manager (PM)).",
                "Following #TARGET_REF , we removed 20 dialogs used by the authors for development, and use the remaining dialogs for the threefold cross-validation.",
                "The LUNA Human-Human corpus (#REF ) consists of 572 call-center dialogs where a client and an agent are engaged in a problem solving task over the phone.",
                "The 200 Italian LUNA dialogs have been annotated with summaries by 5 native speakers (5 summaries per dialog).",
                "For the Call Centre Conversation Summarization (CCCS) shared task a set of 100 dialogs was manually translated to English."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The AMI meeting corpus (#REF ) is a collection of 139 meeting records where groups of people are engaged in a 'roleplay' as a team and each speaker assumes a certain role in a team (e.g. project manager (PM)).\n sent1: Following #TARGET_REF , we removed 20 dialogs used by the authors for development, and use the remaining dialogs for the threefold cross-validation.\n sent2: The LUNA Human-Human corpus (#REF ) consists of 572 call-center dialogs where a client and an agent are engaged in a problem solving task over the phone.\n sent3: The 200 Italian LUNA dialogs have been annotated with summaries by 5 native speakers (5 summaries per dialog).\n sent4: For the Call Centre Conversation Summarization (CCCS) shared task a set of 100 dialogs was manually translated to English.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The metric considers bigram-level precision, recall and F-measure between a set of reference and hypothesis summaries.",
                "For AMI corpus, following #TARGET_REF , we report ROUGE-2 F-measures on 3-fold cross-validation.",
                "For LUNA Corpus, on the other hand, we have used the modified version of ROUGE 1.5.5 toolkit from the CCCS Shared Task , which was adapted to deal with a conversation-dependent length limit of 7%.",
                "Unlike the AMI Corpus, the official reported results for the CCCS Shared Task were recall; thus, for LUNA Corpus the reported values are ROUGE-2 recall.",
                "For statistical significance testing, we use a paired bootstrap resampling method proposed in (#REF) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The metric considers bigram-level precision, recall and F-measure between a set of reference and hypothesis summaries.\n sent1: For AMI corpus, following #TARGET_REF , we report ROUGE-2 F-measures on 3-fold cross-validation.\n sent2: For LUNA Corpus, on the other hand, we have used the modified version of ROUGE 1.5.5 toolkit from the CCCS Shared Task , which was adapted to deal with a conversation-dependent length limit of 7%.\n sent3: Unlike the AMI Corpus, the official reported results for the CCCS Shared Task were recall; thus, for LUNA Corpus the reported values are ROUGE-2 recall.\n sent4: For statistical significance testing, we use a paired bootstrap resampling method proposed in (#REF) .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "In this section we report on the results of the abstractive summarization system using the community creation heuristics described in Section 2.",
                "Following the Call-Center Conversation Summarization Shared Task at #REF , for LUNA Corpus (#REF) we compare performances to three extractive baselines: (1) the longest turn in the conversation up to the length limit (7% of a conversation) (Baseline-L), (2) the longest turn in the first 25% of the conversation up to the length limit (Baseline-LB) (#REF) , and (3) Maximal Marginal Relevance (MMR) (#REF ) with λ = 0.7.",
                "For AMI corpus, on the other hand, we compare performances to the abstractive systems reported in #TARGET_REF .",
                "The performances of the heuristics on AMI corpus are given in Table 1 .",
                "In the table we also report the performances of the previously published summarization systems that make use of the manual communities - (#REF) and (#REF) ; and our run of the system of (#REF) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In this section we report on the results of the abstractive summarization system using the community creation heuristics described in Section 2.\n sent1: Following the Call-Center Conversation Summarization Shared Task at #REF , for LUNA Corpus (#REF) we compare performances to three extractive baselines: (1) the longest turn in the conversation up to the length limit (7% of a conversation) (Baseline-L), (2) the longest turn in the first 25% of the conversation up to the length limit (Baseline-LB) (#REF) , and (3) Maximal Marginal Relevance (MMR) (#REF ) with λ = 0.7.\n sent2: For AMI corpus, on the other hand, we compare performances to the abstractive systems reported in #TARGET_REF .\n sent3: The performances of the heuristics on AMI corpus are given in Table 1 .\n sent4: In the table we also report the performances of the previously published summarization systems that make use of the manual communities - (#REF) and (#REF) ; and our run of the system of (#REF) .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "For AMI corpus, on the other hand, we compare performances to the abstractive systems reported in (#REF) .",
                "The performances of the heuristics on AMI corpus are given in Table 1 .",
                "In the table we also report the performances of the previously published summarization systems that make use of the manual communities - #TARGET_REF and (#REF) ; and our run of the system of #TARGET_REF .",
                "With manual communities we have Table 2 : ROUGE-2 recall with 7% summary length limit for the extractive baselines and abstractive summarization systems with the community creation heuristics on LUNA corpus.",
                "obtained average F-measure of 0.072."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: For AMI corpus, on the other hand, we compare performances to the abstractive systems reported in (#REF) .\n sent1: The performances of the heuristics on AMI corpus are given in Table 1 .\n sent2: In the table we also report the performances of the previously published summarization systems that make use of the manual communities - #TARGET_REF and (#REF) ; and our run of the system of #TARGET_REF .\n sent3: With manual communities we have Table 2 : ROUGE-2 recall with 7% summary length limit for the extractive baselines and abstractive summarization systems with the community creation heuristics on LUNA corpus.\n sent4: obtained average F-measure of 0.072.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The clustering of the abstract templates generated in the previous step is performed using the WordNet hierarchy of the root verb of a sentence.",
                "The similarity between verbs is computed with respect to the shortest path that connects the senses in the hypernym taxonomy of WordNet.",
                "The template graphs, created using this similarity, are then clustered using the Normalized Cuts method (#REF) .",
                "The clustered templates are further generalized using a word graph algorithm extended to templates in #TARGET_REF .",
                "The paths in the word graph are ranked using language models trained on the abstract templates and the top 10 are selected as a template for the cluster."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: The clustering of the abstract templates generated in the previous step is performed using the WordNet hierarchy of the root verb of a sentence.\n sent1: The similarity between verbs is computed with respect to the shortest path that connects the senses in the hypernym taxonomy of WordNet.\n sent2: The template graphs, created using this similarity, are then clustered using the Normalized Cuts method (#REF) .\n sent3: The clustered templates are further generalized using a word graph algorithm extended to templates in #TARGET_REF .\n sent4: The paths in the word graph are ranked using language models trained on the abstract templates and the top 10 are selected as a template for the cluster.\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Since the system produces many sentences that might repeat the same information, the final set of automatic sentences is selected from these filled templates with respect to the ranking using the token and part-of-speech tag 3-gram language models.",
                "In this paper, different from #TARGET_REF , the sentence ranking is based solely on the n-gram language models trained on the tokens and part-ofspeech tags from the human-authored summaries."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Since the system produces many sentences that might repeat the same information, the final set of automatic sentences is selected from these filled templates with respect to the ranking using the token and part-of-speech tag 3-gram language models.\n sent1: In this paper, different from #TARGET_REF , the sentence ranking is based solely on the n-gram language models trained on the tokens and part-ofspeech tags from the human-authored summaries.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Previous corpus-based sense disambiguation methods require substantial amounts of sense-tagged training data (#REF; #REF and #REF) or aligned bilingual corpora (#REF; #REF and #REF ).",
                "#TARGET_REF introduces a thesaurus-based approach to statistical sense disambiguation which works on monolingual corpora without the need for sense-tagged training data.",
                "By collecting statistical data of word occurrences in the context of different thesaurus categories from a relatively large corpus (10 million words), the system can identify salient words for each category.",
                "Using these salient words, the system is able to disambiguate polysemous words with respect to thesaurus categories.",
                "Statistical approaches like these generally suffer from the problem of data sparseness."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Previous corpus-based sense disambiguation methods require substantial amounts of sense-tagged training data (#REF; #REF and #REF) or aligned bilingual corpora (#REF; #REF and #REF ).\n sent1: #TARGET_REF introduces a thesaurus-based approach to statistical sense disambiguation which works on monolingual corpora without the need for sense-tagged training data.\n sent2: By collecting statistical data of word occurrences in the context of different thesaurus categories from a relatively large corpus (10 million words), the system can identify salient words for each category.\n sent3: Using these salient words, the system is able to disambiguate polysemous words with respect to thesaurus categories.\n sent4: Statistical approaches like these generally suffer from the problem of data sparseness.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Since the training data is not sense tagged, the data collected will contain noise due to spurious senses of polysemous words.",
                "Like the thesaurusbased approach of #TARGET_REF , our approach relies on the dilution of this noise by their distribution through all the 1792 defining concepts.",
                "Different words in the corpus have different numbers of senses and different senses have definitions of varying lengths.",
                "The principle adopted in collecting co-occurrence data is that every pair of content words which co-occur in a sentence should have equal contribution to the conceptual cooccurrence data regardless of the number of definitions (senses) of the words and the lengths of the definitions.",
                "In addition, the contribution of a word should be evenly distributed between all the senses of a word and the contribution of a sense should be evenly distributed between all the concepts in a sense."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: Since the training data is not sense tagged, the data collected will contain noise due to spurious senses of polysemous words.\n sent1: Like the thesaurusbased approach of #TARGET_REF , our approach relies on the dilution of this noise by their distribution through all the 1792 defining concepts.\n sent2: Different words in the corpus have different numbers of senses and different senses have definitions of varying lengths.\n sent3: The principle adopted in collecting co-occurrence data is that every pair of content words which co-occur in a sentence should have equal contribution to the conceptual cooccurrence data regardless of the number of definitions (senses) of the words and the lengths of the definitions.\n sent4: In addition, the contribution of a word should be evenly distributed between all the senses of a word and the contribution of a sense should be evenly distributed between all the concepts in a sense.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "6. In our experiment, the words are disambiguated between all the senses listed except the ones marked with 7.",
                "The rare senses listed in LDOCE are not listed here.",
                "For some of the words, more than one sense listed in LDOCE corresponds to a sense as used in #TARGET_REF .",
                "In these cases, the senses used by Yarowsky are adopted for easier comparison.",
                "8. All results are based on 100% recall."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: 6. In our experiment, the words are disambiguated between all the senses listed except the ones marked with 7.\n sent1: The rare senses listed in LDOCE are not listed here.\n sent2: For some of the words, more than one sense listed in LDOCE corresponds to a sense as used in #TARGET_REF .\n sent3: In these cases, the senses used by Yarowsky are adopted for easier comparison.\n sent4: 8. All results are based on 100% recall.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Our system is tested on the twelve words discussed in #TARGET_REF and previous publications on sense disambiguation.",
                "Results are shown in Table 1 .",
                "Our system achieves an average accuracy of 77% on a mean 3-way sense distinction over the twelve words.",
                "Numerically, the result is not as good as the 92% as reported in #REF .",
                "However, direct comparison between the numerical results can be misleading since the experiments are carried out on two very different corpora both in size and genre."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Our system is tested on the twelve words discussed in #TARGET_REF and previous publications on sense disambiguation.\n sent1: Results are shown in Table 1 .\n sent2: Our system achieves an average accuracy of 77% on a mean 3-way sense distinction over the twelve words.\n sent3: Numerically, the result is not as good as the 92% as reported in #REF .\n sent4: However, direct comparison between the numerical results can be misleading since the experiments are carried out on two very different corpora both in size and genre.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "1. N marks the column with the number of tcst samples for each sense.",
                "DBCC (Defmition-Bascd Conceptual Cooccurrence) and Human mark the columns with the results of our system and the human subject in disambiguating the occurrences of the 12 words in the Brown corpus, respectively.",
                "Thes.",
                "(thesaurus) marks the column with the results of #TARGET_REF tested on the Grolier's Encyclopedia.",
                "2. The \"correct\" sense of each test sample is chosen by hand disambiguation carried out by the author using the sentence as the context."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: 1. N marks the column with the number of tcst samples for each sense.\n sent1: DBCC (Defmition-Bascd Conceptual Cooccurrence) and Human mark the columns with the results of our system and the human subject in disambiguating the occurrences of the 12 words in the Brown corpus, respectively.\n sent2: Thes.\n sent3: (thesaurus) marks the column with the results of #TARGET_REF tested on the Grolier's Encyclopedia.\n sent4: 2. The \"correct\" sense of each test sample is chosen by hand disambiguation carried out by the author using the sentence as the context.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Thes.",
                "(thesaurus) marks the column with the results of #REF tested on the Grolier's Encyclopedia.",
                "2. The \"correct\" sense of each test sample is chosen by hand disambiguation carried out by the author using the sentence as the context.",
                "A small proportion of test samples cannot be disambiguated within the given context and are excluded from the experiment.",
                "3. The senses marked with * are used in #TARGET_REF but no corresponding sense is found in LDOCE."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Thes.\n sent1: (thesaurus) marks the column with the results of #REF tested on the Grolier's Encyclopedia.\n sent2: 2. The \"correct\" sense of each test sample is chosen by hand disambiguation carried out by the author using the sentence as the context.\n sent3: A small proportion of test samples cannot be disambiguated within the given context and are excluded from the experiment.\n sent4: 3. The senses marked with * are used in #TARGET_REF but no corresponding sense is found in LDOCE.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Results are shown in Table 1 .",
                "Our system achieves an average accuracy of 77% on a mean 3-way sense distinction over the twelve words.",
                "Numerically, the result is not as good as the 92% as reported in #TARGET_REF .",
                "However, direct comparison between the numerical results can be misleading since the experiments are carried out on two very different corpora both in size and genre.",
                "Firstly, Yarowsky's system is trained with the 10 million word Grolier's Encyclopedia, which is a magnitude larger than the Brown corpus used by our system."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Results are shown in Table 1 .\n sent1: Our system achieves an average accuracy of 77% on a mean 3-way sense distinction over the twelve words.\n sent2: Numerically, the result is not as good as the 92% as reported in #TARGET_REF .\n sent3: However, direct comparison between the numerical results can be misleading since the experiments are carried out on two very different corpora both in size and genre.\n sent4: Firstly, Yarowsky's system is trained with the 10 million word Grolier's Encyclopedia, which is a magnitude larger than the Brown corpus used by our system.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "s The subject has not read through the whole corpus.",
                "approaches in a large proportion of cases.",
                "9 The average sentence length in the Brown corpus is 19.41° words which is 5 times smaller than the 100 word window used in #REF and #TARGET_REF .",
                "Our approach works well even with a small \"window\" because it is based on the identification of salient concepts rather than salient words.",
                "In salient word based approaches, due to the problem of data sparseness, many less frequently occurring words which are intuitively salient to a particular word sense will not be identified in practice unless an extremely large corpus is used."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: s The subject has not read through the whole corpus.\n sent1: approaches in a large proportion of cases.\n sent2: 9 The average sentence length in the Brown corpus is 19.41° words which is 5 times smaller than the 100 word window used in #REF and #TARGET_REF .\n sent3: Our approach works well even with a small \"window\" because it is based on the identification of salient concepts rather than salient words.\n sent4: In salient word based approaches, due to the problem of data sparseness, many less frequently occurring words which are intuitively salient to a particular word sense will not be identified in practice unless an extremely large corpus is used.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "A small proportion of test samples cannot be disambiguated within the given context and are excluded from the experiment.",
                "3. The senses marked with * are used in #REF but no corresponding sense is found in LDOCE.",
                "4. The sense marked with ** is defined in LDOCE but not used in #TARGET_REF .",
                "6. In our experiment, the words are disambiguated between all the senses listed except the ones marked with 7.",
                "The rare senses listed in LDOCE are not listed here."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: A small proportion of test samples cannot be disambiguated within the given context and are excluded from the experiment.\n sent1: 3. The senses marked with * are used in #REF but no corresponding sense is found in LDOCE.\n sent2: 4. The sense marked with ** is defined in LDOCE but not used in #TARGET_REF .\n sent3: 6. In our experiment, the words are disambiguated between all the senses listed except the ones marked with 7.\n sent4: The rare senses listed in LDOCE are not listed here.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "However, deriving these sets of similar words requires a substantial amount of statistical data and thus these approaches require relatively large corpora to start with.~ 2 Our definition-based approach to statistical sense disambiguation is similar in spirit to the similaritybased approaches, with respect to the \"specificity\" of modelling individual words.",
                "However, using definitions from existing dictionaries rather than derived sets of similar words allows our method to work on corpora of much smaller sizes.",
                "In our approach, each word is modelled by its own set of defining concepts.",
                "Although only 1792 defining concepts are used, the set of all possible combinations (a power set of the defining concepts) is so huge that it is very unlikely two word senses will have the same combination of defining concepts unless they are almost identical in meaning.",
                "On the other hand, the thesaurus-based method of #TARGET_REF may suffer from loss of information (since it is semi-class-based) as well as data sparseness (since H Classes used in #REF are based on the WordNet taxonomy while classes of #REF and #REF are derived from statistical data collected from corpora."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: However, deriving these sets of similar words requires a substantial amount of statistical data and thus these approaches require relatively large corpora to start with.~ 2 Our definition-based approach to statistical sense disambiguation is similar in spirit to the similaritybased approaches, with respect to the \"specificity\" of modelling individual words.\n sent1: However, using definitions from existing dictionaries rather than derived sets of similar words allows our method to work on corpora of much smaller sizes.\n sent2: In our approach, each word is modelled by its own set of defining concepts.\n sent3: Although only 1792 defining concepts are used, the set of all possible combinations (a power set of the defining concepts) is so huge that it is very unlikely two word senses will have the same combination of defining concepts unless they are almost identical in meaning.\n sent4: On the other hand, the thesaurus-based method of #TARGET_REF may suffer from loss of information (since it is semi-class-based) as well as data sparseness (since H Classes used in #REF are based on the WordNet taxonomy while classes of #REF and #REF are derived from statistical data collected from corpora.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "This knowledge has been successfully acquired from corpora in manual or semi-automatic approaches such as that described in #REF .",
                "However, fully automatic lexically based approaches 3 #REF shows that the introduction of linguistic cues improves the performance of a statistical semantic knowledge acquisition system in the context of word grouping.",
                "such as that described in #TARGET_REF are very unlikely to be capable of acquiring this finer knowledge because the problem of data sparseness becomes even more serious with the introduction of syntactic constraints.",
                "Our approach has overcome the data sparseness problem by using the defining concepts of words.",
                "It is found to be effective in acquiring semantic coherence knowledge from a relatively small corpus."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: This knowledge has been successfully acquired from corpora in manual or semi-automatic approaches such as that described in #REF .\n sent1: However, fully automatic lexically based approaches 3 #REF shows that the introduction of linguistic cues improves the performance of a statistical semantic knowledge acquisition system in the context of word grouping.\n sent2: such as that described in #TARGET_REF are very unlikely to be capable of acquiring this finer knowledge because the problem of data sparseness becomes even more serious with the introduction of syntactic constraints.\n sent3: Our approach has overcome the data sparseness problem by using the defining concepts of words.\n sent4: It is found to be effective in acquiring semantic coherence knowledge from a relatively small corpus.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "BESTCUT has a different way of computing the cut weight than Min-Cut and a different way of stopping the cut 2 .",
                "Moreover, we have slightly modified the Min-Cut procedures.",
                "BESTCUT replaces the bottom-up search in a tree representation (as it was performed in #TARGET_REF ) with the top-down problem of obtaining the best partitioning of a graph.",
                "We start by assuming that all mentions refer to a single entity; the graph cut splits the mentions into subgraphs and the split-ting continues until each subgraph corresponds to one of the entities.",
                "The cut stopping decision has been implemented as an SVM-based classification (#REF) ."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: BESTCUT has a different way of computing the cut weight than Min-Cut and a different way of stopping the cut 2 .\n sent1: Moreover, we have slightly modified the Min-Cut procedures.\n sent2: BESTCUT replaces the bottom-up search in a tree representation (as it was performed in #TARGET_REF ) with the top-down problem of obtaining the best partitioning of a graph.\n sent3: We start by assuming that all mentions refer to a single entity; the graph cut splits the mentions into subgraphs and the split-ting continues until each subgraph corresponds to one of the entities.\n sent4: The cut stopping decision has been implemented as an SVM-based classification (#REF) .\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent0\", \"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The coreference confidence values that become the weights in the starting graphs are provided by a maximum entropy model, trained on the training datasets of the corpora used in our experiments.",
                "For maximum entropy classification we used a maxent 4 tool.",
                "Based on the data seen, a maximum entropy model (#REF) offers an expression (1) for the probability that there exists coreference C between a mention m i and a mention m j .",
                "where g k (m i , m j , C) is a feature and λ k is its weight; Z(m i , m j ) is a normalizing factor.",
                "We created the training examples in the same way as #TARGET_REF , by pairing all mentions of the same type, obtaining their feature vectors and taking the outcome (coreferent/noncoreferent) from the key files."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The coreference confidence values that become the weights in the starting graphs are provided by a maximum entropy model, trained on the training datasets of the corpora used in our experiments.\n sent1: For maximum entropy classification we used a maxent 4 tool.\n sent2: Based on the data seen, a maximum entropy model (#REF) offers an expression (1) for the probability that there exists coreference C between a mention m i and a mention m j .\n sent3: where g k (m i , m j , C) is a feature and λ k is its weight; Z(m i , m j ) is a normalizing factor.\n sent4: We created the training examples in the same way as #TARGET_REF , by pairing all mentions of the same type, obtaining their feature vectors and taking the outcome (coreferent/noncoreferent) from the key files.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "The clusterization algorithms that we implemented to evaluate in comparison with our method are #TARGET_REF 's Belltree and Link-Best (best-first clusterization) from (#REF) .",
                "The features used were described in section 2.2.",
                "We experimented on the ACE Phase 2 (NIST, 2003) and MUC6 (MUC-6, 1995) corpora.",
                "Since we aimed to measure the performance of coreference, the metrics used for evaluation are the ECM-F (#REF) and the MUC P, R and F scores (#REF) .",
                "In our first experiment, we tested the three coreference clusterization algorithms on the development-test set of the ACE Phase 2 corpus, first on true mentions (i.e. the mentions annotated in the key files), then on detected mentions (i.e. the mentions output by our mention detection system presented in section 3) and finally without any prior knowledge of the mention types."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The clusterization algorithms that we implemented to evaluate in comparison with our method are #TARGET_REF 's Belltree and Link-Best (best-first clusterization) from (#REF) .\n sent1: The features used were described in section 2.2.\n sent2: We experimented on the ACE Phase 2 (NIST, 2003) and MUC6 (MUC-6, 1995) corpora.\n sent3: Since we aimed to measure the performance of coreference, the metrics used for evaluation are the ECM-F (#REF) and the MUC P, R and F scores (#REF) .\n sent4: In our first experiment, we tested the three coreference clusterization algorithms on the development-test set of the ACE Phase 2 corpus, first on true mentions (i.e. the mentions annotated in the key files), then on detected mentions (i.e. the mentions output by our mention detection system presented in section 3) and finally without any prior knowledge of the mention types.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "The features used were described in section 2.2.",
                "We experimented on the ACE Phase 2 (NIST, 2003) and MUC6 (MUC-6, 1995) corpora.",
                "Since we aimed to measure the performance of coreference, the metrics used for evaluation are the ECM-F #TARGET_REF and the MUC P, R and F scores (#REF) .",
                "In our first experiment, we tested the three coreference clusterization algorithms on the development-test set of the ACE Phase 2 corpus, first on true mentions (i.e. the mentions annotated in the key files), then on detected mentions (i.e. the mentions output by our mention detection system presented in section 3) and finally without any prior knowledge of the mention types.",
                "The results obtained are tabulated in Table 4 ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The features used were described in section 2.2.\n sent1: We experimented on the ACE Phase 2 (NIST, 2003) and MUC6 (MUC-6, 1995) corpora.\n sent2: Since we aimed to measure the performance of coreference, the metrics used for evaluation are the ECM-F #TARGET_REF and the MUC P, R and F scores (#REF) .\n sent3: In our first experiment, we tested the three coreference clusterization algorithms on the development-test set of the ACE Phase 2 corpus, first on true mentions (i.e. the mentions annotated in the key files), then on detected mentions (i.e. the mentions output by our mention detection system presented in section 3) and finally without any prior knowledge of the mention types.\n sent4: The results obtained are tabulated in Table 4 .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Our second experiment evaluates the impact that the different categories of our added features have on the performance of the BESTCUT system.",
                "The experiment was performed with a maxent classifier on the MUC6 corpus, which was priorly converted into ACE format, and employed mention information from the key annotations.",
                "Table 5 : Impact of feature categories on BEST-CUT on MUC6.",
                "Baseline system has the #TARGET_REF features.",
                "The system was tested on key mentions."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Our second experiment evaluates the impact that the different categories of our added features have on the performance of the BESTCUT system.\n sent1: The experiment was performed with a maxent classifier on the MUC6 corpus, which was priorly converted into ACE format, and employed mention information from the key annotations.\n sent2: Table 5 : Impact of feature categories on BEST-CUT on MUC6.\n sent3: Baseline system has the #TARGET_REF features.\n sent4: The system was tested on key mentions.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "SGNNs are on-device deep learning models learned via embedding-free projection operations.",
                "We employ a modified version of the locality sensitive hashing (LSH) to reduce input dimension from millions of unique words/features to a short, fixed-length sequence of bits.",
                "This allows us to compute a projection for an incoming text very fast, on-the-fly, with a small memory footprint on the device since we do not need to store the incoming text and word embeddings.",
                "We evaluate the performance of our SGNNs on Dialogue Act classification, because (1) it is an important step towards dialog interpretation and conversational analysis aiming to understand the intent of the speaker at every utterance of the conversation and (2) deep learning methods reached state-of-the-art #TARGET_REF; #REF; #REF; #REF) .",
                "The main contributions of the paper are:"
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: SGNNs are on-device deep learning models learned via embedding-free projection operations.\n sent1: We employ a modified version of the locality sensitive hashing (LSH) to reduce input dimension from millions of unique words/features to a short, fixed-length sequence of bits.\n sent2: This allows us to compute a projection for an incoming text very fast, on-the-fly, with a small memory footprint on the device since we do not need to store the incoming text and word embeddings.\n sent3: We evaluate the performance of our SGNNs on Dialogue Act classification, because (1) it is an important step towards dialog interpretation and conversational analysis aiming to understand the intent of the speaker at every utterance of the conversation and (2) deep learning methods reached state-of-the-art #TARGET_REF; #REF; #REF; #REF) .\n sent4: The main contributions of the paper are:\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The main contributions of the paper are:",
                "• Novel Self-Governing Neural Networks (SGNNs) for on-device deep learning for short text classification.",
                "• Compression technique that effectively captures low-dimensional semantic text representation and produces compact models that save on storage and computational cost.",
                "• On the fly computation of projection vectors that eliminate the need for large pre-trained word embeddings or vocabulary pruning.",
                "• Exhaustive experimental evaluation on dialog act datasets, outperforming state-of-theart deep CNN #TARGET_REF and RNN variants (#REF; #REF )."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: The main contributions of the paper are:\n sent1: • Novel Self-Governing Neural Networks (SGNNs) for on-device deep learning for short text classification.\n sent2: • Compression technique that effectively captures low-dimensional semantic text representation and produces compact models that save on storage and computational cost.\n sent3: • On the fly computation of projection vectors that eliminate the need for large pre-trained word embeddings or vocabulary pruning.\n sent4: • Exhaustive experimental evaluation on dialog act datasets, outperforming state-of-theart deep CNN #TARGET_REF and RNN variants (#REF; #REF ).\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent0\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "For both datasets we used the following: 2-layer SGNN (P T =80,d=14 × FullyConnected 256 × FullyConnected 256 ), mini-batch size of 100, dropout rate of 0.25, learning rate was initialized to 0.025 with cosine annealing decay (#REF) .",
                "Unlike prior approaches #TARGET_REF; #REF ) that rely on pre-trained word embeddings, we learn the projection weights on the fly during training, i.e word embeddings (or vocabularies) do not need to be stored.",
                "Instead, features are computed on the fly and are dynamically compressed via the projection matrices into projection vectors.",
                "These values were chosen via a grid search on development sets, we do not perform any other dataset-specific tuning.",
                "Training is performed through stochastic gradient descent over shuffled mini-batches with Nesterov momentum optimizer (#REF) , run for 1M steps."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: For both datasets we used the following: 2-layer SGNN (P T =80,d=14 × FullyConnected 256 × FullyConnected 256 ), mini-batch size of 100, dropout rate of 0.25, learning rate was initialized to 0.025 with cosine annealing decay (#REF) .\n sent1: Unlike prior approaches #TARGET_REF; #REF ) that rely on pre-trained word embeddings, we learn the projection weights on the fly during training, i.e word embeddings (or vocabularies) do not need to be stored.\n sent2: Instead, features are computed on the fly and are dynamically compressed via the projection matrices into projection vectors.\n sent3: These values were chosen via a grid search on development sets, we do not perform any other dataset-specific tuning.\n sent4: Training is performed through stochastic gradient descent over shuffled mini-batches with Nesterov momentum optimizer (#REF) , run for 1M steps.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We compare our model against a majority class baseline and Naive Bayes classifier #TARGET_REF .",
                "Our model significantly outperforms both baselines by 12 to 35% absolute."
            ],
            "label": [
                "DIFFERENCES",
                "USE"
            ]
        },
        "input": "sent0: We compare our model against a majority class baseline and Naive Bayes classifier #TARGET_REF .\n sent1: Our model significantly outperforms both baselines by 12 to 35% absolute.\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"USE\"], \"context\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "According to (#REF) , prior work by (#REF) achieved promising results on the MRDA dataset, but since the evaluation was conducted on a different data split, it is hard to compare them directly.",
                "For both SwDA and MRDA datasets, our SGNNs obtains the best result of 83.1 and 86.7 accuracy outperforming prior state-of-the-art work.",
                "This is very impressive given that we work with very small memory footprint and we do not rely on pre-trained word embeddings.",
                "Our study also shows that the proposed method is very effective for such natural language tasks compared to more complex neural network architectures such as deep CNN #TARGET_REF and RNN variants (#REF; #REF) .",
                "We believe that the compression techniques like locality sensitive projections jointly coupled with non-linear functions are effective at capturing lowdimensional semantic text representations that are useful for text classification applications."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: According to (#REF) , prior work by (#REF) achieved promising results on the MRDA dataset, but since the evaluation was conducted on a different data split, it is hard to compare them directly.\n sent1: For both SwDA and MRDA datasets, our SGNNs obtains the best result of 83.1 and 86.7 accuracy outperforming prior state-of-the-art work.\n sent2: This is very impressive given that we work with very small memory footprint and we do not rely on pre-trained word embeddings.\n sent3: Our study also shows that the proposed method is very effective for such natural language tasks compared to more complex neural network architectures such as deep CNN #TARGET_REF and RNN variants (#REF; #REF) .\n sent4: We believe that the compression techniques like locality sensitive projections jointly coupled with non-linear functions are effective at capturing lowdimensional semantic text representations that are useful for text classification applications.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "We proposed Self-Governing Neural Networks for on-device short text classification.",
                "Experiments on multiple dialog act datasets showed that our model outperforms state-of-the-art deep leaning methods #TARGET_REF; #REF; #REF) .",
                "We introduced a compression technique that effectively captures low-dimensional semantic representation and produces compact models that significantly save on storage and computational cost.",
                "Our approach does not rely on pre-trained embeddings and efficiently computes the projection vectors on the fly.",
                "In the future, we are interested in extending this approach to more natural language tasks."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: We proposed Self-Governing Neural Networks for on-device short text classification.\n sent1: Experiments on multiple dialog act datasets showed that our model outperforms state-of-the-art deep leaning methods #TARGET_REF; #REF; #REF) .\n sent2: We introduced a compression technique that effectively captures low-dimensional semantic representation and produces compact models that significantly save on storage and computational cost.\n sent3: Our approach does not rely on pre-trained embeddings and efficiently computes the projection vectors on the fly.\n sent4: In the future, we are interested in extending this approach to more natural language tasks.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We conduct our experimental evaluation on two dialog act benchmark datasets.",
                "• SWDA: Switchboard Dialog Act Corpus (#REF; #REF) is a popular open domain dialogs corpus between two speakers with 42 dialogs acts.",
                "• MRDA: ICSI Meeting Recorder Dialog Act Corpus (#REF; #REF ) is a dialog corpus of multiparty meetings with 5 tags of dialog acts.",
                "Table 1 summarizes dataset statistics.",
                "We use the train, validation and test splits as defined in #TARGET_REF; #REF) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We conduct our experimental evaluation on two dialog act benchmark datasets.\n sent1: • SWDA: Switchboard Dialog Act Corpus (#REF; #REF) is a popular open domain dialogs corpus between two speakers with 42 dialogs acts.\n sent2: • MRDA: ICSI Meeting Recorder Dialog Act Corpus (#REF; #REF ) is a dialog corpus of multiparty meetings with 5 tags of dialog acts.\n sent3: Table 1 summarizes dataset statistics.\n sent4: We use the train, validation and test splits as defined in #TARGET_REF; #REF) .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "We also compare our performance against prior work using HMMs (#REF) and recent deep learning methods like CNN #TARGET_REF , RNN (#REF) and RNN with gated attention (#REF) .",
                "To the best of our knowledge, (#REF; #REF; #REF) are the latest approaches in dialog act classification, which also reported on the same data splits.",
                "Therefore, we compare our research against these works.",
                "According to (#REF) , prior work by (#REF) achieved promising results on the MRDA dataset, but since the evaluation was conducted on a different data split, it is hard to compare them directly.",
                "For both SwDA and MRDA datasets, our SGNNs obtains the best result of 83.1 and 86.7 accuracy outperforming prior state-of-the-art work."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We also compare our performance against prior work using HMMs (#REF) and recent deep learning methods like CNN #TARGET_REF , RNN (#REF) and RNN with gated attention (#REF) .\n sent1: To the best of our knowledge, (#REF; #REF; #REF) are the latest approaches in dialog act classification, which also reported on the same data splits.\n sent2: Therefore, we compare our research against these works.\n sent3: According to (#REF) , prior work by (#REF) achieved promising results on the MRDA dataset, but since the evaluation was conducted on a different data split, it is hard to compare them directly.\n sent4: For both SwDA and MRDA datasets, our SGNNs obtains the best result of 83.1 and 86.7 accuracy outperforming prior state-of-the-art work.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "We also compare our performance against prior work using HMMs (#REF) and recent deep learning methods like CNN (#REF) , RNN (#REF) and RNN with gated attention (#REF) .",
                "To the best of our knowledge, #TARGET_REF; #REF; #REF) are the latest approaches in dialog act classification, which also reported on the same data splits.",
                "Therefore, we compare our research against these works.",
                "According to (#REF) , prior work by (#REF) achieved promising results on the MRDA dataset, but since the evaluation was conducted on a different data split, it is hard to compare them directly.",
                "For both SwDA and MRDA datasets, our SGNNs obtains the best result of 83.1 and 86.7 accuracy outperforming prior state-of-the-art work."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: We also compare our performance against prior work using HMMs (#REF) and recent deep learning methods like CNN (#REF) , RNN (#REF) and RNN with gated attention (#REF) .\n sent1: To the best of our knowledge, #TARGET_REF; #REF; #REF) are the latest approaches in dialog act classification, which also reported on the same data splits.\n sent2: Therefore, we compare our research against these works.\n sent3: According to (#REF) , prior work by (#REF) achieved promising results on the MRDA dataset, but since the evaluation was conducted on a different data split, it is hard to compare them directly.\n sent4: For both SwDA and MRDA datasets, our SGNNs obtains the best result of 83.1 and 86.7 accuracy outperforming prior state-of-the-art work.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "How to determine the quality of an automatic transcription without reference transcripts and without confidence information?",
                "This is the key problem addressed by research on ASR quality estimation C. de #REF; #TARGET_REF , and the task for which TranscRater, the tool described in this paper, has been designed.",
                "The work on ASR quality estimation (ASR QE) has several motivations.",
                "First, the steady increase of applications involving automatic speech recognition (e.g. video/TV programs subtitling, voice search engines, voice question answering, spoken dialog systems, meeting and broadcast news transcriptions) calls for an accurate method to estimate ASR output quality at run-time.",
                "Often, indeed, the nature of such applications (consider for instance spoken dialog systems) requires quick response capabilities that are incompatible with traditional reference-based protocols."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: How to determine the quality of an automatic transcription without reference transcripts and without confidence information?\n sent1: This is the key problem addressed by research on ASR quality estimation C. de #REF; #TARGET_REF , and the task for which TranscRater, the tool described in this paper, has been designed.\n sent2: The work on ASR quality estimation (ASR QE) has several motivations.\n sent3: First, the steady increase of applications involving automatic speech recognition (e.g. video/TV programs subtitling, voice search engines, voice question answering, spoken dialog systems, meeting and broadcast news transcriptions) calls for an accurate method to estimate ASR output quality at run-time.\n sent4: Often, indeed, the nature of such applications (consider for instance spoken dialog systems) requires quick response capabilities that are incompatible with traditional reference-based protocols.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "According to the type of training labels, the problem can be approached either as a regression or as a classification task.",
                "As a consequence, also the evaluation metrics will change.",
                "Precision/recall/F1 (or other metrics, such as balanced accuracy, in case of very unbalanced distributions) will be used for classification while, similar to MT QE, the mean absolute error (MAE) or similar metrics will be used for regression.",
                "A variant of the basic ASR QE task is to consider it as a QE-based ranking problem #TARGET_REF , in which each utterance is captured by multiple microphones or transcribed by multiple ASR systems.",
                "In this case, the capability to rank transcriptions from the best to the worst can be evaluated in terms of normalized discounted cumulative gain (NDCG) or similar metrics."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: According to the type of training labels, the problem can be approached either as a regression or as a classification task.\n sent1: As a consequence, also the evaluation metrics will change.\n sent2: Precision/recall/F1 (or other metrics, such as balanced accuracy, in case of very unbalanced distributions) will be used for classification while, similar to MT QE, the mean absolute error (MAE) or similar metrics will be used for regression.\n sent3: A variant of the basic ASR QE task is to consider it as a QE-based ranking problem #TARGET_REF , in which each utterance is captured by multiple microphones or transcribed by multiple ASR systems.\n sent4: In this case, the capability to rank transcriptions from the best to the worst can be evaluated in terms of normalized discounted cumulative gain (NDCG) or similar metrics.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "For regression-based tasks (WER prediction), TranscRater includes an interface to the Scikitlearn package (#REF) , a Python machine learning library that contains a large set of classification and regression algorithms.",
                "Based on the empirical results reported in C. de #REF; #TARGET_REF , which indicate that Extremely Randomized Trees (XRT (#REF) ) is a very competitive algorithm in several WER prediction tasks, the current version of the tool exploits XRT.",
                "However, adapting the interface to apply other algorithms is an easy task and one of the future extension directions.",
                "The main hyper-parameters of the model, such as the number of tree bags, the number of trees per bag, the number of features per tree and the number of instances in the leaves, are tuned using grid search with k-fold cross-validation on the training set to minimize the mean absolute error (MAE) between the true WERs and the predicted ones.",
                "As mentioned before, TranscRater provides the possibility to evaluate multiple transcriptions (e.g. obtained from different microphones or ASR systems) and rank them based on their quality."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For regression-based tasks (WER prediction), TranscRater includes an interface to the Scikitlearn package (#REF) , a Python machine learning library that contains a large set of classification and regression algorithms.\n sent1: Based on the empirical results reported in C. de #REF; #TARGET_REF , which indicate that Extremely Randomized Trees (XRT (#REF) ) is a very competitive algorithm in several WER prediction tasks, the current version of the tool exploits XRT.\n sent2: However, adapting the interface to apply other algorithms is an easy task and one of the future extension directions.\n sent3: The main hyper-parameters of the model, such as the number of tree bags, the number of trees per bag, the number of features per tree and the number of instances in the leaves, are tuned using grid search with k-fold cross-validation on the training set to minimize the mean absolute error (MAE) between the true WERs and the predicted ones.\n sent4: As mentioned before, TranscRater provides the possibility to evaluate multiple transcriptions (e.g. obtained from different microphones or ASR systems) and rank them based on their quality.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The features and algorithms contained in TranscRater have been successfully used in previous works C. de #REF; #TARGET_REF; #REFa) .",
                "To further investigate their effectiveness, in this section we provide new results, both in WER prediction (MAE) and transcription ranking (NDCG), together with some efficiency analysis (Time in seconds 9 ).",
                "To this aim, we use data from the 3 rd CHiME challenge, 10 which were collected for multiple distant microphone speech recognition in noisy environments (#REF) .",
                "CHiME-3 data consists of sentences of the Wall Street Journal corpus, uttered by four speakers in four noisy environments, and recorded by five microphones placed on the frame of a tablet PC (a sixth one, placed on the back, mainly records background noise).",
                "Training and test respectively contain 1,640 and 1,320 sentences."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The features and algorithms contained in TranscRater have been successfully used in previous works C. de #REF; #TARGET_REF; #REFa) .\n sent1: To further investigate their effectiveness, in this section we provide new results, both in WER prediction (MAE) and transcription ranking (NDCG), together with some efficiency analysis (Time in seconds 9 ).\n sent2: To this aim, we use data from the 3 rd CHiME challenge, 10 which were collected for multiple distant microphone speech recognition in noisy environments (#REF) .\n sent3: CHiME-3 data consists of sentences of the Wall Street Journal corpus, uttered by four speakers in four noisy environments, and recorded by five microphones placed on the frame of a tablet PC (a sixth one, placed on the back, mainly records background noise).\n sent4: Training and test respectively contain 1,640 and 1,320 sentences.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "As mentioned before, TranscRater provides the possibility to evaluate multiple transcriptions (e.g. obtained from different microphones or ASR systems) and rank them based on their quality.",
                "This can be done either indirectly, by exploiting the predicted WER labels in a \"ranking by regression\" approach (RR) or directly, by exploiting machinelearned ranking methods (MLR).",
                "To train and test MLR models, TranscRater exploits RankLib 8 , a library of learning-to-rank algorithms.",
                "The current version of the tool includes an interface to the Random Forest algorithm (RF (#REF) ), the same used in #TARGET_REF .",
                "MLR predicts ranks through pairwise comparison between the transcriptions."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: As mentioned before, TranscRater provides the possibility to evaluate multiple transcriptions (e.g. obtained from different microphones or ASR systems) and rank them based on their quality.\n sent1: This can be done either indirectly, by exploiting the predicted WER labels in a \"ranking by regression\" approach (RR) or directly, by exploiting machinelearned ranking methods (MLR).\n sent2: To train and test MLR models, TranscRater exploits RankLib 8 , a library of learning-to-rank algorithms.\n sent3: The current version of the tool includes an interface to the Random Forest algorithm (RF (#REF) ), the same used in #TARGET_REF .\n sent4: MLR predicts ranks through pairwise comparison between the transcriptions.\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "One important reason seems to be that dependency parsing offers a good compromise between the conflicting demands of analysis depth, on the one hand, and robustness and efficiency, on the other.",
                "Thus, whereas a complete dependency structure provides a fully disambiguated analysis of a sentence, this analysis is typically less complex than in frameworks based on constituent analysis and can therefore often be computed deterministically with reasonable accuracy.",
                "Deterministic methods for dependency parsing have now been applied to a variety of languages, including Japanese (#REF) , English #TARGET_REF , Turkish (#REF) , and Swedish (#REF) .",
                "For English, the interest in dependency parsing has been weaker than for other languages.",
                "To some extent, this can probably be explained by the strong tradition of constituent analysis in Anglo-American linguistics, but this trend has been reinforced by the fact that the major treebank of American English, the Penn Treebank (#REF) , is annotated primarily with constituent analysis."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: One important reason seems to be that dependency parsing offers a good compromise between the conflicting demands of analysis depth, on the one hand, and robustness and efficiency, on the other.\n sent1: Thus, whereas a complete dependency structure provides a fully disambiguated analysis of a sentence, this analysis is typically less complex than in frameworks based on constituent analysis and can therefore often be computed deterministically with reasonable accuracy.\n sent2: Deterministic methods for dependency parsing have now been applied to a variety of languages, including Japanese (#REF) , English #TARGET_REF , Turkish (#REF) , and Swedish (#REF) .\n sent3: For English, the interest in dependency parsing has been weaker than for other languages.\n sent4: To some extent, this can probably be explained by the strong tradition of constituent analysis in Anglo-American linguistics, but this trend has been reinforced by the fact that the major treebank of American English, the Penn Treebank (#REF) , is annotated primarily with constituent analysis.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Moreover, the deterministic dependency parser of #TARGET_REF , when trained on the Penn Treebank, gives a dependency accuracy that is almost as good as that of #REF and #REF .",
                "The parser described in this paper is similar to that of #REF in that it uses a deterministic parsing algorithm in combination with a classifier induced from a treebank.",
                "However, there are also important differences between the two approaches.",
                "First of all, whereas Yamada and Matsumoto employs a strict bottom-up algorithm (essentially shift-reduce parsing) with multiple passes over the input, the present parser uses the algorithm proposed in #REF , which combines bottomup and top-down processing in a single pass in order to achieve incrementality.",
                "This also means that the time complexity of the algorithm used here is linear in the size of the input, while the algorithm of Yamada and Matsumoto is quadratic in the worst case."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Moreover, the deterministic dependency parser of #TARGET_REF , when trained on the Penn Treebank, gives a dependency accuracy that is almost as good as that of #REF and #REF .\n sent1: The parser described in this paper is similar to that of #REF in that it uses a deterministic parsing algorithm in combination with a classifier induced from a treebank.\n sent2: However, there are also important differences between the two approaches.\n sent3: First of all, whereas Yamada and Matsumoto employs a strict bottom-up algorithm (essentially shift-reduce parsing) with multiple passes over the input, the present parser uses the algorithm proposed in #REF , which combines bottomup and top-down processing in a single pass in order to achieve incrementality.\n sent4: This also means that the time complexity of the algorithm used here is linear in the size of the input, while the algorithm of Yamada and Matsumoto is quadratic in the worst case.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "The parser described in this paper is similar to that of #TARGET_REF in that it uses a deterministic parsing algorithm in combination with a classifier induced from a treebank.",
                "However, there are also important differences between the two approaches.",
                "First of all, whereas Yamada and Matsumoto employs a strict bottom-up algorithm (essentially shift-reduce parsing) with multiple passes over the input, the present parser uses the algorithm proposed in #REF , which combines bottomup and top-down processing in a single pass in order to achieve incrementality.",
                "This also means that the time complexity of the algorithm used here is linear in the size of the input, while the algorithm of Yamada and Matsumoto is quadratic in the worst case.",
                "Another difference is that Yamada and Matsumoto use support vector machines (#REF) , while we instead rely on memory-based learning (#REF) ."
            ],
            "label": [
                "DIFFERENCES",
                "EXTENDS"
            ]
        },
        "input": "sent0: The parser described in this paper is similar to that of #TARGET_REF in that it uses a deterministic parsing algorithm in combination with a classifier induced from a treebank.\n sent1: However, there are also important differences between the two approaches.\n sent2: First of all, whereas Yamada and Matsumoto employs a strict bottom-up algorithm (essentially shift-reduce parsing) with multiple passes over the input, the present parser uses the algorithm proposed in #REF , which combines bottomup and top-down processing in a single pass in order to achieve incrementality.\n sent3: This also means that the time complexity of the algorithm used here is linear in the size of the input, while the algorithm of Yamada and Matsumoto is quadratic in the worst case.\n sent4: Another difference is that Yamada and Matsumoto use support vector machines (#REF) , while we instead rely on memory-based learning (#REF) .\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"EXTENDS\"], \"context\": [\"sent0\", \"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Another difference is that Yamada and Matsumoto use support vector machines (#REF) , while we instead rely on memory-based learning (#REF) .",
                "Most importantly, however, the parser presented in this paper constructs labeled dependency graphs, i.e. dependency graphs where arcs are labeled with dependency types.",
                "As far as we know, this makes it different from all previous systems for dependency parsing applied to the Penn Treebank (#REF; #TARGET_REF , although there are systems that extract labeled grammatical relations based on shallow parsing, e.g. #REF .",
                "The fact that we are working with labeled dependency graphs is also one of the motivations for choosing memory-based learning over support vector machines, since we require a multi-class classifier.",
                "Even though it is possible to use SVM for multi-class classification, this can get cumbersome when the number of classes is large."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Another difference is that Yamada and Matsumoto use support vector machines (#REF) , while we instead rely on memory-based learning (#REF) .\n sent1: Most importantly, however, the parser presented in this paper constructs labeled dependency graphs, i.e. dependency graphs where arcs are labeled with dependency types.\n sent2: As far as we know, this makes it different from all previous systems for dependency parsing applied to the Penn Treebank (#REF; #TARGET_REF , although there are systems that extract labeled grammatical relations based on shallow parsing, e.g. #REF .\n sent3: The fact that we are working with labeled dependency graphs is also one of the motivations for choosing memory-based learning over support vector machines, since we require a multi-class classifier.\n sent4: Even though it is possible to use SVM for multi-class classification, this can get cumbersome when the number of classes is large.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We can also see that the unlabeled attachment score improves, for both models, when the set of dependency labels is extended.",
                "On the other hand, the labeled attachment score drops, but it must be remembered that these scores are not really comparable, since the number of classes in the classification problem increases from 7 to 50 as we move from the G set to the B set.",
                "Therefore, we have also included the labeled attachment score restricted to the G set for the parser using the B set (BG), and we see then that the attachment score improves, especially for Model 2. (All differences are significant beyond the .01 level; McNemar's test.) Table 2 shows the dependency accuracy, root accuracy and complete match scores for our best parser (Model 2 with label set B) in comparison with #REF (Model 3) , #REF , and #REF .",
                "5 It is clear that, with respect to unlabeled accuracy, our parser does not quite reach state-of-the-art performance, even if we limit the competition to deterministic methods such as that of #TARGET_REF .",
                "We believe that there are mainly three reasons for this."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: We can also see that the unlabeled attachment score improves, for both models, when the set of dependency labels is extended.\n sent1: On the other hand, the labeled attachment score drops, but it must be remembered that these scores are not really comparable, since the number of classes in the classification problem increases from 7 to 50 as we move from the G set to the B set.\n sent2: Therefore, we have also included the labeled attachment score restricted to the G set for the parser using the B set (BG), and we see then that the attachment score improves, especially for Model 2. (All differences are significant beyond the .01 level; McNemar's test.) Table 2 shows the dependency accuracy, root accuracy and complete match scores for our best parser (Model 2 with label set B) in comparison with #REF (Model 3) , #REF , and #REF .\n sent3: 5 It is clear that, with respect to unlabeled accuracy, our parser does not quite reach state-of-the-art performance, even if we limit the competition to deterministic methods such as that of #TARGET_REF .\n sent4: We believe that there are mainly three reasons for this.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Therefore, we have also included the labeled attachment score restricted to the G set for the parser using the B set (BG), and we see then that the attachment score improves, especially for Model 2. (All differences are significant beyond the .01 level; McNemar's test.) Table 2 shows the dependency accuracy, root accuracy and complete match scores for our best parser (Model 2 with label set B) in comparison with #REF (Model 3) , #REF , and #REF .",
                "5 It is clear that, with respect to unlabeled accuracy, our parser does not quite reach state-of-the-art performance, even if we limit the competition to deterministic methods such as that of #REF .",
                "We believe that there are mainly three reasons for this.",
                "First of all, the part-of-speech tagger used for preprocessing in our experiments has a lower accuracy than the one used by #TARGET_REF (96.1% vs. 97.1%) .",
                "Although this is not a very interesting explanation, it undoubtedly accounts for part of the difference."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Therefore, we have also included the labeled attachment score restricted to the G set for the parser using the B set (BG), and we see then that the attachment score improves, especially for Model 2. (All differences are significant beyond the .01 level; McNemar's test.) Table 2 shows the dependency accuracy, root accuracy and complete match scores for our best parser (Model 2 with label set B) in comparison with #REF (Model 3) , #REF , and #REF .\n sent1: 5 It is clear that, with respect to unlabeled accuracy, our parser does not quite reach state-of-the-art performance, even if we limit the competition to deterministic methods such as that of #REF .\n sent2: We believe that there are mainly three reasons for this.\n sent3: First of all, the part-of-speech tagger used for preprocessing in our experiments has a lower accuracy than the one used by #TARGET_REF (96.1% vs. 97.1%) .\n sent4: Although this is not a very interesting explanation, it undoubtedly accounts for part of the difference.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "In another study, #REF report an F-measure of 98.9% for the assignment of Penn Treebank grammatical role labels (our G set) to phrases that were correctly parsed by the parser described in #REF .",
                "If null labels (corresponding to our DEP labels) are excluded, the F-score drops to 95.7%.",
                "The corresponding F-measures for our best parser (Model 2, BG) are 99.0% and 94.7%.",
                "For the larger B set, our best parser achieves an F-measure of 96.9% (DEP labels included), which can be compared with 97.0% for a similar (but larger) set of labels in #REF .",
                "6 Although none of the previous results on labeling accuracy is strictly comparable to ours, it nevertheless seems fair to conclude that the #TARGET_REF labeling accuracy of the present parser is close to the state of the art, even if its capacity to derive correct structures is not."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: In another study, #REF report an F-measure of 98.9% for the assignment of Penn Treebank grammatical role labels (our G set) to phrases that were correctly parsed by the parser described in #REF .\n sent1: If null labels (corresponding to our DEP labels) are excluded, the F-score drops to 95.7%.\n sent2: The corresponding F-measures for our best parser (Model 2, BG) are 99.0% and 94.7%.\n sent3: For the larger B set, our best parser achieves an F-measure of 96.9% (DEP labels included), which can be compared with 97.0% for a similar (but larger) set of labels in #REF .\n sent4: 6 Although none of the previous results on labeling accuracy is strictly comparable to ours, it nevertheless seems fair to conclude that the #TARGET_REF labeling accuracy of the present parser is close to the state of the art, even if its capacity to derive correct structures is not.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "This task, which we refer to as selective generation, is often formulated as two subproblems: content selection, which involves choosing a subset of relevant records to talk about from the exhaustive database, and surface realization, which is concerned with generating natural language descriptions for this subset.",
                "Learning to perform these tasks jointly is challenging due to the ambiguity in deciding which records are relevant, the complex dependencies between selected records, and the multiple ways in which these records can be described.",
                "Previous work has made significant progress on this task (#REF; #TARGET_REF; #REF) .",
                "However, most approaches solve the two content selection and surface realization subtasks separately, use manual domain-dependent resources (e.g., semantic parsers) and features, or employ template-based generation.",
                "This limits domain adaptability and reduces coherence."
            ],
            "label": [
                "BACKGROUND",
                "MOTIVATION"
            ]
        },
        "input": "sent0: This task, which we refer to as selective generation, is often formulated as two subproblems: content selection, which involves choosing a subset of relevant records to talk about from the exhaustive database, and surface realization, which is concerned with generating natural language descriptions for this subset.\n sent1: Learning to perform these tasks jointly is challenging due to the ambiguity in deciding which records are relevant, the complex dependencies between selected records, and the multiple ways in which these records can be described.\n sent2: Previous work has made significant progress on this task (#REF; #TARGET_REF; #REF) .\n sent3: However, most approaches solve the two content selection and surface realization subtasks separately, use manual domain-dependent resources (e.g., semantic parsers) and features, or employ template-based generation.\n sent4: This limits domain adaptability and reduces coherence.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"MOTIVATION\"], \"context\": [\"sent1\", \"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Similarly, #REF employs probabilistic context-free grammars to perform surface realization.",
                "Other effective approaches include the use of tree conditional random fields (#REF) and template extraction within a log-linear framework #TARGET_REF .",
                "Recent work seeks to solve the full selective generation problem through a single framework.",
                "#REF and #REF learn alignments between comments and their corresponding event records using a translation model for parsing and generation.",
                "implement a two-stage framework that decides what to discuss using a combination of the methods of #REF and #REF , and then produces the text based on the generation system of #REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Similarly, #REF employs probabilistic context-free grammars to perform surface realization.\n sent1: Other effective approaches include the use of tree conditional random fields (#REF) and template extraction within a log-linear framework #TARGET_REF .\n sent2: Recent work seeks to solve the full selective generation problem through a single framework.\n sent3: #REF and #REF learn alignments between comments and their corresponding event records using a translation model for parsing and generation.\n sent4: implement a two-stage framework that decides what to discuss using a combination of the methods of #REF and #REF , and then produces the text based on the generation system of #REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Recent work seeks to solve the full selective generation problem through a single framework.",
                "#REF and #REF learn alignments between comments and their corresponding event records using a translation model for parsing and generation.",
                "implement a two-stage framework that decides what to discuss using a combination of the methods of #REF and #REF , and then produces the text based on the generation system of #REF .",
                "#TARGET_REF propose a unified conceptto-text model that treats joint content selection and surface realization as a sequence of local decisions represented by a log-linear model.",
                "Similar to other work, they train their model using external alignments from #REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Recent work seeks to solve the full selective generation problem through a single framework.\n sent1: #REF and #REF learn alignments between comments and their corresponding event records using a translation model for parsing and generation.\n sent2: implement a two-stage framework that decides what to discuss using a combination of the methods of #REF and #REF , and then produces the text based on the generation system of #REF .\n sent3: #TARGET_REF propose a unified conceptto-text model that treats joint content selection and surface realization as a sequence of local decisions represented by a log-linear model.\n sent4: Similar to other work, they train their model using external alignments from #REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Aligner Ablation First, we evaluate the contribution of our proposed coarse-to-fine aligner by comparing our model with the basic encoder-alignerdecoder model introduced by #REF .",
                "Table 3 reports the results demonstrating that our aligner yields superior F-1 and BLEU scores relative to a standard aligner.",
                "Encoder Ablation Next, we consider the effectiveness of the encoder.",
                "Table 4 compares the results with and without the encoder on the development set, and demonstrates that there is a significant gain from encoding the event records using the LSTM-RNN.",
                "We attribute this improvement to the LSTM-RNN's ability to capture the relationships that exist among the records, which is known to be essential to selective generation (#REF; #TARGET_REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Aligner Ablation First, we evaluate the contribution of our proposed coarse-to-fine aligner by comparing our model with the basic encoder-alignerdecoder model introduced by #REF .\n sent1: Table 3 reports the results demonstrating that our aligner yields superior F-1 and BLEU scores relative to a standard aligner.\n sent2: Encoder Ablation Next, we consider the effectiveness of the encoder.\n sent3: Table 4 compares the results with and without the encoder on the development set, and demonstrates that there is a significant gain from encoding the event records using the LSTM-RNN.\n sent4: We attribute this improvement to the LSTM-RNN's ability to capture the relationships that exist among the records, which is known to be essential to selective generation (#REF; #TARGET_REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "This limits domain adaptability and reduces coherence.",
                "We take an alternative, neural encoder-aligner-decoder approach to free-form selective generation that jointly performs content selection and surface realization, without using any specialized features, resources, or generation templates.",
                "This enables our approach to generalize to new domains.",
                "Further, our memorybased model captures the long-range contextual dependencies among records and descriptions, which are integral to this task #TARGET_REF .",
                "We formulate our model as an encoder-alignerdecoder framework that uses recurrent neural networks with long short-term memory units (LSTMRNNs) (#REF) together with a coarse-to-fine aligner to select and \"translate\" the rich world state into a natural language description."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: This limits domain adaptability and reduces coherence.\n sent1: We take an alternative, neural encoder-aligner-decoder approach to free-form selective generation that jointly performs content selection and surface realization, without using any specialized features, resources, or generation templates.\n sent2: This enables our approach to generalize to new domains.\n sent3: Further, our memorybased model captures the long-range contextual dependencies among records and descriptions, which are integral to this task #TARGET_REF .\n sent4: We formulate our model as an encoder-alignerdecoder framework that uses recurrent neural networks with long short-term memory units (LSTMRNNs) (#REF) together with a coarse-to-fine aligner to select and \"translate\" the rich world state into a natural language description.\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Together with the negative loglikelihood of the ground-truth description x * 1:T , our loss function becomes",
                "Having trained the model, we generate the natural language description by finding the maximum a posteriori words under the learned model (Eqn. 1).",
                "For inference, we perform greedy search starting with the first word x 1 .",
                "Beam search offers a way to perform approximate joint inference -however, we empirically found that beam search does not perform any better than greedy search on the datasets that we consider, an observation that is shared with previous work #TARGET_REF .",
                "We later discuss an alternative k-nearest neighbor-based beam filter (see Sec 6.2)."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: Together with the negative loglikelihood of the ground-truth description x * 1:T , our loss function becomes\n sent1: Having trained the model, we generate the natural language description by finding the maximum a posteriori words under the learned model (Eqn. 1).\n sent2: For inference, we perform greedy search starting with the first word x 1 .\n sent3: Beam search offers a way to perform approximate joint inference -however, we empirically found that beam search does not perform any better than greedy search on the datasets that we consider, an observation that is shared with previous work #TARGET_REF .\n sent4: We later discuss an alternative k-nearest neighbor-based beam filter (see Sec 6.2).\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Datasets We analyze our model on the benchmark WEATHERGOV dataset, and use the data-starved ROBOCUP dataset to demonstrate the model's generalizability.",
                "Following #TARGET_REF , we use WEATHERGOV training, development, and test splits of size 25000, 1000, and 3528, respectively.",
                "For ROBOCUP, we follow the evaluation methodology of previous work (#REF) , performing three-fold cross-validation whereby we train on three games (approximately 1000 scenarios) and test on the fourth.",
                "Within each split, we hold out 10% of the training data as the development set to tune the early-stopping criterion and γ.",
                "We then report the standard average performance (weighted by the number of scenarios) over these four splits."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Datasets We analyze our model on the benchmark WEATHERGOV dataset, and use the data-starved ROBOCUP dataset to demonstrate the model's generalizability.\n sent1: Following #TARGET_REF , we use WEATHERGOV training, development, and test splits of size 25000, 1000, and 3528, respectively.\n sent2: For ROBOCUP, we follow the evaluation methodology of previous work (#REF) , performing three-fold cross-validation whereby we train on three games (approximately 1000 scenarios) and test on the fourth.\n sent3: Within each split, we hold out 10% of the training data as the development set to tune the early-stopping criterion and γ.\n sent4: We then report the standard average performance (weighted by the number of scenarios) over these four splits.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We report the performance of content selection and surface realization using F-1 and two BLEU scores (standard sBLEU and the customized cBLEU of #TARGET_REF), respectively (Sec. 5).",
                "Table 1 compares our test results against previous methods that include KL12 (#REF) , KL13 (#REF) , and ALK10 (#REF) .",
                "Our method achieves the best results reported to-date on all three metrics, with relative improvements of 11.94% (F-1), 58.88% (sBLEU), and 36.68% (cBLEU) over the previous state-of-the-art."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We report the performance of content selection and surface realization using F-1 and two BLEU scores (standard sBLEU and the customized cBLEU of #TARGET_REF), respectively (Sec. 5).\n sent1: Table 1 compares our test results against previous methods that include KL12 (#REF) , KL13 (#REF) , and ALK10 (#REF) .\n sent2: Our method achieves the best results reported to-date on all three metrics, with relative improvements of 11.94% (F-1), 58.88% (sBLEU), and 36.68% (cBLEU) over the previous state-of-the-art.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "We report the performance of content selection and surface realization using F-1 and two BLEU scores (standard sBLEU and the customized cBLEU of #REF), respectively (Sec. 5).",
                "Table 1 compares our test results against previous methods that include KL12 (#REF) , KL13 (#REF) , and ALK10 #TARGET_REF .",
                "Our method achieves the best results reported to-date on all three metrics, with relative improvements of 11.94% (F-1), 58.88% (sBLEU), and 36.68% (cBLEU) over the previous state-of-the-art."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: We report the performance of content selection and surface realization using F-1 and two BLEU scores (standard sBLEU and the customized cBLEU of #REF), respectively (Sec. 5).\n sent1: Table 1 compares our test results against previous methods that include KL12 (#REF) , KL13 (#REF) , and ALK10 #TARGET_REF .\n sent2: Our method achieves the best results reported to-date on all three metrics, with relative improvements of 11.94% (F-1), 58.88% (sBLEU), and 36.68% (cBLEU) over the previous state-of-the-art.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We considered beam search as an alternative to greedy search in our primary setup (Eqn. 1), but this performs worse, similar to what previous work found on this dataset #TARGET_REF .",
                "As an alternative, we consider a beam filter based on a knearest neighborhood.",
                "See Supplementary Material for details.",
                "Table 9 shows that this k-NN beam filter improves results over the primary greedy results.",
                "Aligner Ablation First, we evaluate the contribution of our proposed coarse-to-fine aligner by comparing our model with the basic encoder-alignerdecoder model introduced by #REF ."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: We considered beam search as an alternative to greedy search in our primary setup (Eqn. 1), but this performs worse, similar to what previous work found on this dataset #TARGET_REF .\n sent1: As an alternative, we consider a beam filter based on a knearest neighborhood.\n sent2: See Supplementary Material for details.\n sent3: Table 9 shows that this k-NN beam filter improves results over the primary greedy results.\n sent4: Aligner Ablation First, we evaluate the contribution of our proposed coarse-to-fine aligner by comparing our model with the basic encoder-alignerdecoder model introduced by #REF .\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "The learning algorithms applied including: decision tree, decisionlist [15] , neural networks [7] , naïve Bayesian learning ( [5] , #TARGET_REF ) and maximum entropy [10] .",
                "Among these leaning methods, the most important issue is what features will be used to construct the classifier.",
                "It is common in WSD to use contextual information that can be found in the neighborhood of the ambiguous word in training data ( [6] , [16] [17] [18] ).",
                "It is generally true that when words are used in the same sense, they have similar context and co-occurrence information [13] .",
                "It is also generally true that the nearby context words of an ambiguous word give more effective patterns and features values than those far from it [12] ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The learning algorithms applied including: decision tree, decisionlist [15] , neural networks [7] , naïve Bayesian learning ( [5] , #TARGET_REF ) and maximum entropy [10] .\n sent1: Among these leaning methods, the most important issue is what features will be used to construct the classifier.\n sent2: It is common in WSD to use contextual information that can be found in the neighborhood of the ambiguous word in training data ( [6] , [16] [17] [18] ).\n sent3: It is generally true that when words are used in the same sense, they have similar context and co-occurrence information [13] .\n sent4: It is also generally true that the nearby context words of an ambiguous word give more effective patterns and features values than those far from it [12] .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Niu #TARGET_REF proved in his experiments that Naïve Bayes classifier achieved best disambiguation accuracy with small topical context window size (< 10 words).",
                "We follow their method and set the contextual window size as 10 in our system.",
                "Each of the Chinese words except the stop words inside the window range will be considered as one topical feature.",
                "Their frequencies are calculated over the entire corpus with respect to each sense of an ambiguous word w. The sense definitions are obtained from HowNet."
            ],
            "label": [
                "USE",
                "BACKGROUND"
            ]
        },
        "input": "sent0: Niu #TARGET_REF proved in his experiments that Naïve Bayes classifier achieved best disambiguation accuracy with small topical context window size (< 10 words).\n sent1: We follow their method and set the contextual window size as 10 in our system.\n sent2: Each of the Chinese words except the stop words inside the window range will be considered as one topical feature.\n sent3: Their frequencies are calculated over the entire corpus with respect to each sense of an ambiguous word w. The sense definitions are obtained from HowNet.\n",
        "output": "{\"label\": [\"USE\", \"BACKGROUND\"], \"context\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "In both Niu #TARGET_REF and Dang's [10] work, topical features as well as the so called collocational features were used.",
                "However, as discussed in Section 2, they both used bi-gram cooccurrences as the additional local features.",
                "However, bi-gram co-occurrences only indicate statistical significance which may not actually satisfy the conceptual definition of collocations.",
                "Thus instead of using co-occurrences of bigrams, we take the true bi-gram collocations extracted from our system and use this data to compare with bi-gram co-occurrences to test the usefulness of collocation for WSD.",
                "The local features in our system make use of the collocations using the template (w i , w) within a window size of ten (where i = ± 5)."
            ],
            "label": [
                "BACKGROUND",
                "DIFFERENCES"
            ]
        },
        "input": "sent0: In both Niu #TARGET_REF and Dang's [10] work, topical features as well as the so called collocational features were used.\n sent1: However, as discussed in Section 2, they both used bi-gram cooccurrences as the additional local features.\n sent2: However, bi-gram co-occurrences only indicate statistical significance which may not actually satisfy the conceptual definition of collocations.\n sent3: Thus instead of using co-occurrences of bigrams, we take the true bi-gram collocations extracted from our system and use this data to compare with bi-gram co-occurrences to test the usefulness of collocation for WSD.\n sent4: The local features in our system make use of the collocations using the template (w i , w) within a window size of ten (where i = ± 5).\n",
        "output": "{\"label\": [\"BACKGROUND\", \"DIFFERENCES\"], \"context\": [\"sent0\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "In fact, methods based on string kernels have demonstrated impressive results in various text classification tasks ranging from native language identification [22] [23] [24] 36] and authorship identification [34] to dialect identification [4, 18, 21] , sentiment analysis [13, 35] and automatic essay scoring [7] .",
                "As long as a labeled training set is available, string kernels can reach state-of-the-art results in various languages including English [7, #TARGET_REF 23] , Arabic [4, 17, 18, 24] , #REF and #REF .",
                "Different from all these recent approaches, we use unlabeled data from the test set in a transductive setting in order to significantly increase the performance of string kernels.",
                "In our recent work [19] , we proposed two transductive learning approaches combined into a unified framework that improves the results of string kernels in two different tasks.",
                "In this paper, we provide a formal and detailed description of our transductive algorithm and present results in cross-domain English polarity classification."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: In fact, methods based on string kernels have demonstrated impressive results in various text classification tasks ranging from native language identification [22] [23] [24] 36] and authorship identification [34] to dialect identification [4, 18, 21] , sentiment analysis [13, 35] and automatic essay scoring [7] .\n sent1: As long as a labeled training set is available, string kernels can reach state-of-the-art results in various languages including English [7, #TARGET_REF 23] , Arabic [4, 17, 18, 24] , #REF and #REF .\n sent2: Different from all these recent approaches, we use unlabeled data from the test set in a transductive setting in order to significantly increase the performance of string kernels.\n sent3: In our recent work [19] , we proposed two transductive learning approaches combined into a unified framework that improves the results of string kernels in two different tasks.\n sent4: In this paper, we provide a formal and detailed description of our transductive algorithm and present results in cross-domain English polarity classification.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Giménez-Pérez et al. [13] have used string kernels for single-source and multi-source polarity classification.",
                "Remarkably, they obtain state-of-the-art performance without using knowledge from the target domain, which indicates that string kernels provide robust results in the cross-domain setting without any domain adaptation.",
                "#REF obtained the best performance in the Arabic Dialect Identification Shared Task of the 2017 VarDial Evaluation #REF , with an improvement of 4.6% over the second-best method.",
                "It is important to note that the training and the test speech samples prepared for the shared task were recorded in different setups [41] , or in other words, the training and the test sets are drawn from different distributions.",
                "Different from all these recent approaches #TARGET_REF 18, 23] , we use unlabeled data from the target domain to significantly increase the performance of string kernels in cross-domain text classification, particularly in English polarity classification."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Giménez-Pérez et al. [13] have used string kernels for single-source and multi-source polarity classification.\n sent1: Remarkably, they obtain state-of-the-art performance without using knowledge from the target domain, which indicates that string kernels provide robust results in the cross-domain setting without any domain adaptation.\n sent2: #REF obtained the best performance in the Arabic Dialect Identification Shared Task of the 2017 VarDial Evaluation #REF , with an improvement of 4.6% over the second-best method.\n sent3: It is important to note that the training and the test speech samples prepared for the shared task were recorded in different setups [41] , or in other words, the training and the test sets are drawn from different distributions.\n sent4: Different from all these recent approaches #TARGET_REF 18, 23] , we use unlabeled data from the target domain to significantly increase the performance of string kernels in cross-domain text classification, particularly in English polarity classification.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "We compare our approach with several methods [3, 12, 13, 15, 32, 40] in two cross-domain settings.",
                "Using string kernels, Giménez-Pérez et al. #TARGET_REF reported better performance than SST [3] and KE-#REF in the multi-source domain setting.",
                "In addition, we compare our approach with SFA [32] , CORAL [40] and TR-#REF in the single-source setting.",
                "Method DEK→B BEK→D BDK→E BDE→K SST [3] 76.3 78.3 83.9 85.2 KE-#REF 77.9 80.4 78.9 82.5 K 0/1 [13] 82 Table 1 .",
                "Multi-source cross-domain polarity classification accuracy rates (in %) of our transductive approaches versus a state-of-the-art baseline based on string kernels [13] , as well as SST [3] and KE-#REF ."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: We compare our approach with several methods [3, 12, 13, 15, 32, 40] in two cross-domain settings.\n sent1: Using string kernels, Giménez-Pérez et al. #TARGET_REF reported better performance than SST [3] and KE-#REF in the multi-source domain setting.\n sent2: In addition, we compare our approach with SFA [32] , CORAL [40] and TR-#REF in the single-source setting.\n sent3: Method DEK→B BEK→D BDK→E BDE→K SST [3] 76.3 78.3 83.9 85.2 KE-#REF 77.9 80.4 78.9 82.5 K 0/1 [13] 82 Table 1 .\n sent4: Multi-source cross-domain polarity classification accuracy rates (in %) of our transductive approaches versus a state-of-the-art baseline based on string kernels [13] , as well as SST [3] and KE-#REF .\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Furthermore, we use the same kernels, namely the presence bits string kernel (K 0/1 ) and the intersection string kernel (K ∩ ), and the same range of character n-grams (5) (6) (7) (8) . To compute the string kernels, we used the open-source code provided by Ionescu et al. [20, 23] .",
                "For the transductive kernel classifier, we select r = 1000 unlabeled test samples to be included in the training set for the second round of training.",
                "We choose Kernel Ridge #REF as classifier and set its regularization parameter to 10 −5 in all our experiments.",
                "Although Giménez-Pérez et al. #TARGET_REF used a different classifier, namely Kernel Discriminant Analysis, we observed that Kernel Ridge Regression produces similar results (±0.1%) when we employ the same string kernels.",
                "As Giménez-Pérez et al. [13] , we evaluate our approach in two cross-domain settings."
            ],
            "label": [
                "DIFFERENCES",
                "SIMILARITY"
            ]
        },
        "input": "sent0: Furthermore, we use the same kernels, namely the presence bits string kernel (K 0/1 ) and the intersection string kernel (K ∩ ), and the same range of character n-grams (5) (6) (7) (8) . To compute the string kernels, we used the open-source code provided by Ionescu et al. [20, 23] .\n sent1: For the transductive kernel classifier, we select r = 1000 unlabeled test samples to be included in the training set for the second round of training.\n sent2: We choose Kernel Ridge #REF as classifier and set its regularization parameter to 10 −5 in all our experiments.\n sent3: Although Giménez-Pérez et al. #TARGET_REF used a different classifier, namely Kernel Discriminant Analysis, we observed that Kernel Ridge Regression produces similar results (±0.1%) when we employ the same string kernels.\n sent4: As Giménez-Pérez et al. [13] , we evaluate our approach in two cross-domain settings.\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"SIMILARITY\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "ably, the improvements brought by our transductive string kernel approach are statistically significant in all domains.",
                "Results in single-source setting.",
                "The results for the single-source crossdomain polarity classification setting are presented in Table 2 .",
                "We considered all possible combinations of source and target domains in this experiment, and we improve the results in each and every case.",
                "Without exception, the accuracy rates reached by the transductive string kernels are significantly better than the best baseline string kernel #TARGET_REF , according to the McNemar's test performed at a confidence level of 0.01."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: ably, the improvements brought by our transductive string kernel approach are statistically significant in all domains.\n sent1: Results in single-source setting.\n sent2: The results for the single-source crossdomain polarity classification setting are presented in Table 2 .\n sent3: We considered all possible combinations of source and target domains in this experiment, and we improve the results in each and every case.\n sent4: Without exception, the accuracy rates reached by the transductive string kernels are significantly better than the best baseline string kernel #TARGET_REF , according to the McNemar's test performed at a confidence level of 0.01.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "In this paper, we presented two domain adaptation approaches that can be used together to improve the results of string kernels in cross-domain settings.",
                "We provided empirical evidence indicating that our framework can be successfully applied in cross-domain text classification, particularly in cross-domain English polarity classification.",
                "Indeed, the polarity classification experiments demonstrate that our framework achieves better accuracy rates than other state-ofthe-art methods [3, 12, #TARGET_REF 15, 32, 40] .",
                "By using the same parameters across all the experiments, we showed that our transductive transfer learning framework can bring significant improvements without having to fine-tune the parameters for each individual setting.",
                "Although the framework described in this paper can be generally applied to any kernel method, we focused our work only on string kernel approaches used in text classification."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: In this paper, we presented two domain adaptation approaches that can be used together to improve the results of string kernels in cross-domain settings.\n sent1: We provided empirical evidence indicating that our framework can be successfully applied in cross-domain text classification, particularly in cross-domain English polarity classification.\n sent2: Indeed, the polarity classification experiments demonstrate that our framework achieves better accuracy rates than other state-ofthe-art methods [3, 12, #TARGET_REF 15, 32, 40] .\n sent3: By using the same parameters across all the experiments, we showed that our transductive transfer learning framework can bring significant improvements without having to fine-tune the parameters for each individual setting.\n sent4: Although the framework described in this paper can be generally applied to any kernel method, we focused our work only on string kernel approaches used in text classification.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Reviews contain star ratings (from 1 to 5) which are converted into binary labels as follows: reviews rated with more than 3 stars are labeled as positive, and those with less than 3 stars as negative.",
                "In each domain, there are 1000 positive and 1000 negative reviews.",
                "Baselines.",
                "We compare our approach with several methods [3, 12, #TARGET_REF 15, 32, 40] in two cross-domain settings.",
                "Using string kernels, Giménez-Pérez et al. [13] reported better performance than SST [3] and KE-#REF in the multi-source domain setting."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Reviews contain star ratings (from 1 to 5) which are converted into binary labels as follows: reviews rated with more than 3 stars are labeled as positive, and those with less than 3 stars as negative.\n sent1: In each domain, there are 1000 positive and 1000 negative reviews.\n sent2: Baselines.\n sent3: We compare our approach with several methods [3, 12, #TARGET_REF 15, 32, 40] in two cross-domain settings.\n sent4: Using string kernels, Giménez-Pérez et al. [13] reported better performance than SST [3] and KE-#REF in the multi-source domain setting.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Method DEK→B BEK→D BDK→E BDE→K SST [3] 76.3 78.3 83.9 85.2 KE-#REF 77.9 80.4 78.9 82.5 K 0/1 [13] 82 Table 1 .",
                "Multi-source cross-domain polarity classification accuracy rates (in %) of our transductive approaches versus a state-of-the-art baseline based on string kernels #TARGET_REF , as well as SST [3] and KE-#REF .",
                "The best accuracy rates are highlighted in bold.",
                "The marker * indicates that the performance is significantly better than the best baseline string kernel according to a paired McNemar's test performed at a significance level of 0.01.",
                "Evaluation procedure and parameters."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Method DEK→B BEK→D BDK→E BDE→K SST [3] 76.3 78.3 83.9 85.2 KE-#REF 77.9 80.4 78.9 82.5 K 0/1 [13] 82 Table 1 .\n sent1: Multi-source cross-domain polarity classification accuracy rates (in %) of our transductive approaches versus a state-of-the-art baseline based on string kernels #TARGET_REF , as well as SST [3] and KE-#REF .\n sent2: The best accuracy rates are highlighted in bold.\n sent3: The marker * indicates that the performance is significantly better than the best baseline string kernel according to a paired McNemar's test performed at a significance level of 0.01.\n sent4: Evaluation procedure and parameters.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The marker * indicates that the performance is significantly better than the best baseline string kernel according to a paired McNemar's test performed at a significance level of 0.01.",
                "Evaluation procedure and parameters.",
                "We follow the same evaluation methodology of Giménez-Pérez et al. #TARGET_REF , to ensure a fair comparison.",
                "Furthermore, we use the same kernels, namely the presence bits string kernel (K 0/1 ) and the intersection string kernel (K ∩ ), and the same range of character n-grams (5) (6) (7) (8) . To compute the string kernels, we used the open-source code provided by Ionescu et al. [20, 23] .",
                "For the transductive kernel classifier, we select r = 1000 unlabeled test samples to be included in the training set for the second round of training."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The marker * indicates that the performance is significantly better than the best baseline string kernel according to a paired McNemar's test performed at a significance level of 0.01.\n sent1: Evaluation procedure and parameters.\n sent2: We follow the same evaluation methodology of Giménez-Pérez et al. #TARGET_REF , to ensure a fair comparison.\n sent3: Furthermore, we use the same kernels, namely the presence bits string kernel (K 0/1 ) and the intersection string kernel (K ∩ ), and the same range of character n-grams (5) (6) (7) (8) . To compute the string kernels, we used the open-source code provided by Ionescu et al. [20, 23] .\n sent4: For the transductive kernel classifier, we select r = 1000 unlabeled test samples to be included in the training set for the second round of training.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "For the transductive kernel classifier, we select r = 1000 unlabeled test samples to be included in the training set for the second round of training.",
                "We choose Kernel Ridge #REF as classifier and set its regularization parameter to 10 −5 in all our experiments.",
                "Although Giménez-Pérez et al. [13] used a different classifier, namely Kernel Discriminant Analysis, we observed that Kernel Ridge Regression produces similar results (±0.1%) when we employ the same string kernels.",
                "As Giménez-Pérez et al. #TARGET_REF , we evaluate our approach in two cross-domain settings.",
                "In the multi-source setting, we train the models on all domains, except the one used for testing."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: For the transductive kernel classifier, we select r = 1000 unlabeled test samples to be included in the training set for the second round of training.\n sent1: We choose Kernel Ridge #REF as classifier and set its regularization parameter to 10 −5 in all our experiments.\n sent2: Although Giménez-Pérez et al. [13] used a different classifier, namely Kernel Discriminant Analysis, we observed that Kernel Ridge Regression produces similar results (±0.1%) when we employ the same string kernels.\n sent3: As Giménez-Pérez et al. #TARGET_REF , we evaluate our approach in two cross-domain settings.\n sent4: In the multi-source setting, we train the models on all domains, except the one used for testing.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "For example, on the Books domain the accuracy of the transductive classifier based on the presence bits kernel (84.1%) is 2.1% above the best baseline (82.0%) represented by the intersection string kernel.",
                "Remark- Table 2 .",
                "Single-source cross-domain polarity classification accuracy rates (in %) of our transductive approaches versus a state-of-the-art baseline based on string kernels #TARGET_REF , as well as SFA [32] , CORAL [40] and TR-#REF .",
                "The best accuracy rates are highlighted in bold.",
                "The marker * indicates that the performance is significantly better than the best baseline string kernel according to a paired McNemar's test performed at a significance level of 0.01."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: For example, on the Books domain the accuracy of the transductive classifier based on the presence bits kernel (84.1%) is 2.1% above the best baseline (82.0%) represented by the intersection string kernel.\n sent1: Remark- Table 2 .\n sent2: Single-source cross-domain polarity classification accuracy rates (in %) of our transductive approaches versus a state-of-the-art baseline based on string kernels #TARGET_REF , as well as SFA [32] , CORAL [40] and TR-#REF .\n sent3: The best accuracy rates are highlighted in bold.\n sent4: The marker * indicates that the performance is significantly better than the best baseline string kernel according to a paired McNemar's test performed at a significance level of 0.01.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Traditionally, \"deep learning\" is taken to be a learning process where the inference or optimization is based on the real-valued deterministic model.",
                "The \"semantic structure\" in words, sentences, entities, actions and documents drawn from a large vocabulary may not be well expressed or correctly optimized in mathematical logic or computer programs.",
                "The \"distribution function\" in discrete or continuous latent variable model for natural language may not be properly decomposed or estimated in model inference.",
                "This tutorial addresses the fundamentals of statistical models and neural networks, and focus on a series of advanced Bayesian models and deep models including hierarchical Dirichlet process , Chinese restaurant process (#REF) , hierarchical Pitman-Yor process (#REF) , Indian buffet process (#REF) , recurrent neural network (#REF; Van Den #REF) , long short-term memory (#REF; , sequence-to-sequence model (#REF), variational auto-encoder (#REF) , generative adversarial network (#REF) , attention mechanism (#REF; #REF) , memory-augmented neural network (#REF; #REF) , stochastic neural network #TARGET_REF , predictive state neural network (#REF) , policy gradient (#REF) and reinforcement learning (#REF) .",
                "We present how these models are connected and why they work for a variety of applications on symbolic and complex patterns in natural language."
            ],
            "label": [
                "USE",
                "BACKGROUND"
            ]
        },
        "input": "sent0: Traditionally, \"deep learning\" is taken to be a learning process where the inference or optimization is based on the real-valued deterministic model.\n sent1: The \"semantic structure\" in words, sentences, entities, actions and documents drawn from a large vocabulary may not be well expressed or correctly optimized in mathematical logic or computer programs.\n sent2: The \"distribution function\" in discrete or continuous latent variable model for natural language may not be properly decomposed or estimated in model inference.\n sent3: This tutorial addresses the fundamentals of statistical models and neural networks, and focus on a series of advanced Bayesian models and deep models including hierarchical Dirichlet process , Chinese restaurant process (#REF) , hierarchical Pitman-Yor process (#REF) , Indian buffet process (#REF) , recurrent neural network (#REF; Van Den #REF) , long short-term memory (#REF; , sequence-to-sequence model (#REF), variational auto-encoder (#REF) , generative adversarial network (#REF) , attention mechanism (#REF; #REF) , memory-augmented neural network (#REF; #REF) , stochastic neural network #TARGET_REF , predictive state neural network (#REF) , policy gradient (#REF) and reinforcement learning (#REF) .\n sent4: We present how these models are connected and why they work for a variety of applications on symbolic and complex patterns in natural language.\n",
        "output": "{\"label\": [\"USE\", \"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Table 1 Depicted above are distributions for each domain and language, detailing the probability (y-axis) of specific parts of speech at increasing degrees of polysemy (x-axis).",
                "These distributions were produced from the gold keys (or synsets) of the test documents by querying BabelNet for the polysemy of each word.",
                "Each distribution was normalised with one sense per discourse assumed, therefore duplicate synsets were ignored.",
                "Lastly the difference in F-Score between the conventional Run1 and the iterative Run2 and Run3 is listed beside each distribution.",
                "Firstly WSD tasks before 2013 generally relied on only a lexicon, such as WordNet (#REF) or an alternative equivalent, whereas #REF Task 12 WSD and this task #TARGET_REF included Entity Linking (EL) using the encyclopaedia Wikipedia via BabelNet (#REF) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Table 1 Depicted above are distributions for each domain and language, detailing the probability (y-axis) of specific parts of speech at increasing degrees of polysemy (x-axis).\n sent1: These distributions were produced from the gold keys (or synsets) of the test documents by querying BabelNet for the polysemy of each word.\n sent2: Each distribution was normalised with one sense per discourse assumed, therefore duplicate synsets were ignored.\n sent3: Lastly the difference in F-Score between the conventional Run1 and the iterative Run2 and Run3 is listed beside each distribution.\n sent4: Firstly WSD tasks before 2013 generally relied on only a lexicon, such as WordNet (#REF) or an alternative equivalent, whereas #REF Task 12 WSD and this task #TARGET_REF included Entity Linking (EL) using the encyclopaedia Wikipedia via BabelNet (#REF) .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Equally so, Run2 and Run3 were outperformed by Run1 for the less technical Social Issues All Domains domain in which many of the named entities are polysemous rather than monosemous.",
                "While the iterative approach achieved reasonably competitive results in English, this success did not translate as well to Spanish and Italian.",
                "The Italian Biomedical domain had the highest document monosemy, observable in Figure 1 (g ), yet this did not help the iterative Run2 and Run3.",
                "Yet it is worth noting the results of the task paper #TARGET_REF report that SUDOKU Run2 and Run3 achieved very low F-Scores for named entity disambiguation (<28.6) in Spanish and Italian.",
                "Given that more than half of the named entities were monosemous in Figure 1 (d) and (g), the WSD system either did not capture them in text or filtered them out during subgraph construction (see BabelNet API)."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Equally so, Run2 and Run3 were outperformed by Run1 for the less technical Social Issues All Domains domain in which many of the named entities are polysemous rather than monosemous.\n sent1: While the iterative approach achieved reasonably competitive results in English, this success did not translate as well to Spanish and Italian.\n sent2: The Italian Biomedical domain had the highest document monosemy, observable in Figure 1 (g ), yet this did not help the iterative Run2 and Run3.\n sent3: Yet it is worth noting the results of the task paper #TARGET_REF report that SUDOKU Run2 and Run3 achieved very low F-Scores for named entity disambiguation (<28.6) in Spanish and Italian.\n sent4: Given that more than half of the named entities were monosemous in Figure 1 (d) and (g), the WSD system either did not capture them in text or filtered them out during subgraph construction (see BabelNet API).\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "In hindsight, the iterative approach could be restricted to the parts of speech it is known to improve, while remaining with the conventional approach on others.",
                "To the right in Table 3 the author's SUDOKU runs are compared against the team with the most competitive results -LIMSI.",
                "The author could not improve on their superior results achieved in English, however for Spanish and Italian the BabelNet First Sense (BFS) baseline was much lower since it often resorted to lexicographic sorting in the absence of WordNet synsets -see (#REF) .",
                "The author's baseline-independent submissions were unaffected by this, which on reviewing results in #TARGET_REF appears to have helped SUDOKU do best for these languages.",
                "Table 3 : F1 scores for each domain/language for SUDOKU and LIMSI."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In hindsight, the iterative approach could be restricted to the parts of speech it is known to improve, while remaining with the conventional approach on others.\n sent1: To the right in Table 3 the author's SUDOKU runs are compared against the team with the most competitive results -LIMSI.\n sent2: The author could not improve on their superior results achieved in English, however for Spanish and Italian the BabelNet First Sense (BFS) baseline was much lower since it often resorted to lexicographic sorting in the absence of WordNet synsets -see (#REF) .\n sent3: The author's baseline-independent submissions were unaffected by this, which on reviewing results in #TARGET_REF appears to have helped SUDOKU do best for these languages.\n sent4: Table 3 : F1 scores for each domain/language for SUDOKU and LIMSI.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Datasets can vary by domain (e.g. product), type (social media, review), or medium (written or spoken), and to date there has been no comparative evaluation of methods from these multiple classes.",
                "Our primary and secondary contributions therefore, are to carry out the first study that reports results across all three different dataset classes, and to release a open source code framework implementing three complementary groups of TDSA methods.",
                "In terms of reproducibility via code release, recent TDSA papers have generally been very good with regards to publishing code alongside their papers (#REF; #REF; #REF; #TARGET_REF but other papers have not released code (#REF; #REF) .",
                "In some cases, the code was initially made available, then removed, and is now back online (#REFa) .",
                "Unfortunately, in some cases even when code has been published, different results have been obtained relative to the original paper."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Datasets can vary by domain (e.g. product), type (social media, review), or medium (written or spoken), and to date there has been no comparative evaluation of methods from these multiple classes.\n sent1: Our primary and secondary contributions therefore, are to carry out the first study that reports results across all three different dataset classes, and to release a open source code framework implementing three complementary groups of TDSA methods.\n sent2: In terms of reproducibility via code release, recent TDSA papers have generally been very good with regards to publishing code alongside their papers (#REF; #REF; #REF; #TARGET_REF but other papers have not released code (#REF; #REF) .\n sent3: In some cases, the code was initially made available, then removed, and is now back online (#REFa) .\n sent4: Unfortunately, in some cases even when code has been published, different results have been obtained relative to the original paper.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "In IR, specific reproducible research tracks have been created 3 and we are pleased to see the same happening at COLING 2018 4 .",
                "Turning now to the focus of our investigations, Target Dependent sentiment analysis (TDSA) research (#REF) arose as an extension to the coarse grained analysis of document level sentiment analysis (#REF; #REF) .",
                "Since its inception, papers have applied different methods such as feature based (#REF) , Recursive Neural Networks (RecNN) (#REF) , Recurrent Neural Networks (RNN) (#REFa) , attention applied to RNN (#REF; #REF; #REF) , Neural Pooling (NP) #TARGET_REF , RNN combined with NP (#REF) , and attention based neural networks (#REFb) .",
                "Others have tackled TDSA as a joint task with target extraction, thus treating it as a sequence labelling problem.",
                "#REF carried out this task using Conditional Random Fields (CRF), and this work was then extended using a neural CRF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In IR, specific reproducible research tracks have been created 3 and we are pleased to see the same happening at COLING 2018 4 .\n sent1: Turning now to the focus of our investigations, Target Dependent sentiment analysis (TDSA) research (#REF) arose as an extension to the coarse grained analysis of document level sentiment analysis (#REF; #REF) .\n sent2: Since its inception, papers have applied different methods such as feature based (#REF) , Recursive Neural Networks (RecNN) (#REF) , Recurrent Neural Networks (RNN) (#REFa) , attention applied to RNN (#REF; #REF; #REF) , Neural Pooling (NP) #TARGET_REF , RNN combined with NP (#REF) , and attention based neural networks (#REFb) .\n sent3: Others have tackled TDSA as a joint task with target extraction, thus treating it as a sequence labelling problem.\n sent4: #REF carried out this task using Conditional Random Fields (CRF), and this work was then extended using a neural CRF .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "They inputted the features into a linear SVM, and showed the importance of using the left and right context for the first time.",
                "They found in their study that using a combination of Word2Vec embeddings and sentiment embeddings performed best alongside using sentiment lexicons to filter the embedding space.",
                "Other studies have adopted more linguistic approaches.",
                "#TARGET_REF extended the work of by using the dependency linked words from the target.",
                "#REF used the dependency tree to create a Recursive Neural Network (RecNN) inspired by #REF but compared to #REF they also utilised the dependency tags to create an Adaptive RecNN (ARecNN)."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: They inputted the features into a linear SVM, and showed the importance of using the left and right context for the first time.\n sent1: They found in their study that using a combination of Word2Vec embeddings and sentiment embeddings performed best alongside using sentiment lexicons to filter the embedding space.\n sent2: Other studies have adopted more linguistic approaches.\n sent3: #TARGET_REF extended the work of by using the dependency linked words from the target.\n sent4: #REF used the dependency tree to create a Recursive Neural Network (RecNN) inspired by #REF but compared to #REF they also utilised the dependency tags to create an Adaptive RecNN (ARecNN).\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "For each of the experiments below we used the following configurations unless otherwise stated: we performed 5 fold stratified cross validation, features are scaled using Max Min scaling before inputting into the SVM, and used the respective C-Values for the SVM stated in the paper for each of the models.",
                "One major difficulty with the description of the method in the paper and re-implementation is handling the same target multiple appearances issue as originally raised by #TARGET_REF .",
                "As the method requires context with regards to the target word, if there is more than one appearance of the target word then the method does not specify which to use.",
                "We therefore took the approach of #REF and found all of the features for each appearance and performed median pooling over features.",
                "This change could explain the subtle differences between the results we report and those of the original paper."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For each of the experiments below we used the following configurations unless otherwise stated: we performed 5 fold stratified cross validation, features are scaled using Max Min scaling before inputting into the SVM, and used the respective C-Values for the SVM stated in the paper for each of the models.\n sent1: One major difficulty with the description of the method in the paper and re-implementation is handling the same target multiple appearances issue as originally raised by #TARGET_REF .\n sent2: As the method requires context with regards to the target word, if there is more than one appearance of the target word then the method does not specify which to use.\n sent3: We therefore took the approach of #REF and found all of the features for each appearance and performed median pooling over features.\n sent4: This change could explain the subtle differences between the results we report and those of the original paper.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF extended the NP work of and instead of using the full tweet/sentence/text contexts they used the full dependency graph of the target word.",
                "Thus, they created three different methods: 1. TDParse-uses only the full dependency graph context, 2.",
                "TDParse the feature of TDParse-and the left and right contexts, and 3.",
                "TDParse+ the features of TDParse and LS and RS contexts.",
                "The experiments are performed on the #REF and #REF Twitter datasets where we train and test on the previously specified train and test splits."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF extended the NP work of and instead of using the full tweet/sentence/text contexts they used the full dependency graph of the target word.\n sent1: Thus, they created three different methods: 1. TDParse-uses only the full dependency graph context, 2.\n sent2: TDParse the feature of TDParse-and the left and right contexts, and 3.\n sent3: TDParse+ the features of TDParse and LS and RS contexts.\n sent4: The experiments are performed on the #REF and #REF Twitter datasets where we train and test on the previously specified train and test splits.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Our observations within this one sub-field motivates the need to investigate further and understand how such problems can be avoided in the future.",
                "In some cases, when code has been released, it is difficult to use which could explain why the results were not reproduced.",
                "Of course, we would not expect researchers to produce industrial strength code, or provide continuing free ongoing support for multiple years after publication, but the situation is clearly problematic for the development of the new field in general.",
                "In this paper, we therefore reproduce three papers chosen as they employ widely differing methods: Neural Pooling (NP) , NP with dependency parsing #TARGET_REF , and RNN (#REFa) , as well as having been applied largely to different datasets.",
                "At the end of the paper, we reflect on bringing together elements of repeatability and generalisability which we find are crucial to NLP and data science based disciplines more widely to enable others to make use of the science created."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Our observations within this one sub-field motivates the need to investigate further and understand how such problems can be avoided in the future.\n sent1: In some cases, when code has been released, it is difficult to use which could explain why the results were not reproduced.\n sent2: Of course, we would not expect researchers to produce industrial strength code, or provide continuing free ongoing support for multiple years after publication, but the situation is clearly problematic for the development of the new field in general.\n sent3: In this paper, we therefore reproduce three papers chosen as they employ widely differing methods: Neural Pooling (NP) , NP with dependency parsing #TARGET_REF , and RNN (#REFa) , as well as having been applied largely to different datasets.\n sent4: At the end of the paper, we reflect on bringing together elements of repeatability and generalisability which we find are crucial to NLP and data science based disciplines more widely to enable others to make use of the science created.\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Second, we only used datasets that contain three distinct sentiments (#REF only has two).",
                "From the datasets we have used, we have only had issue with parsing #TARGET_REF where the annotations for the first set of the data contains the target span but the second set does not.",
                "Thus making it impossible to use the second set of annotation and forcing us to only use a subset of the dataset.",
                "An as example of this: \"Got rid of bureaucrats 'and we put that money, into 9000 more doctors and nurses'... to turn the doctors into bureaucrats#BattleForNumber10\" in that Tweet 'bureaucrats' was annotated as negative but it does not state if it was the first or second instance of 'bureaucrats' since it does not use target spans.",
                "As we can see from table 2, generally the social media datasets (Twitter and YouTube) contain more targets per sentence with the exception of #REF and #REF ."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: Second, we only used datasets that contain three distinct sentiments (#REF only has two).\n sent1: From the datasets we have used, we have only had issue with parsing #TARGET_REF where the annotations for the first set of the data contains the target span but the second set does not.\n sent2: Thus making it impossible to use the second set of annotation and forcing us to only use a subset of the dataset.\n sent3: An as example of this: \"Got rid of bureaucrats 'and we put that money, into 9000 more doctors and nurses'... to turn the doctors into bureaucrats#BattleForNumber10\" in that Tweet 'bureaucrats' was annotated as negative but it does not state if it was the first or second instance of 'bureaucrats' since it does not use target spans.\n sent4: As we can see from table 2, generally the social media datasets (Twitter and YouTube) contain more targets per sentence with the exception of #REF and #REF .\n",
        "output": "{\"label\": [null], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "From the datasets we have used, we have only had issue with parsing #REF where the annotations for the first set of the data contains the target span but the second set does not.",
                "Thus making it impossible to use the second set of annotation and forcing us to only use a subset of the dataset.",
                "An as example of this: \"Got rid of bureaucrats 'and we put that money, into 9000 more doctors and nurses'... to turn the doctors into bureaucrats#BattleForNumber10\" in that Tweet 'bureaucrats' was annotated as negative but it does not state if it was the first or second instance of 'bureaucrats' since it does not use target spans.",
                "As we can see from table 2, generally the social media datasets (Twitter and YouTube) contain more targets per sentence with the exception of #REF and #REF .",
                "The only dataset that has a small difference between the number of unique sentiments per sentence is the #TARGET_REF"
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: From the datasets we have used, we have only had issue with parsing #REF where the annotations for the first set of the data contains the target span but the second set does not.\n sent1: Thus making it impossible to use the second set of annotation and forcing us to only use a subset of the dataset.\n sent2: An as example of this: \"Got rid of bureaucrats 'and we put that money, into 9000 more doctors and nurses'... to turn the doctors into bureaucrats#BattleForNumber10\" in that Tweet 'bureaucrats' was annotated as negative but it does not state if it was the first or second instance of 'bureaucrats' since it does not use target spans.\n sent3: As we can see from table 2, generally the social media datasets (Twitter and YouTube) contain more targets per sentence with the exception of #REF and #REF .\n sent4: The only dataset that has a small difference between the number of unique sentiments per sentence is the #TARGET_REF\n",
        "output": "{\"label\": [null], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "W2V and SSWE combined.",
                "Neither of these word embeddings are available from the original authors as never released the embeddings and the link to embeddings no longer works 8 .",
                "However, the embeddings were released through #TARGET_REF code base 9 following requesting of the code from .",
                "Figure 1 shows the results of the different word embeddings across the different methods.",
                "The main finding we see is that SSWE by themselves are not as informative as W2V vectors which is different to the findings of ."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: W2V and SSWE combined.\n sent1: Neither of these word embeddings are available from the original authors as never released the embeddings and the link to embeddings no longer works 8 .\n sent2: However, the embeddings were released through #TARGET_REF code base 9 following requesting of the code from .\n sent3: Figure 1 shows the results of the different word embeddings across the different methods.\n sent4: The main finding we see is that SSWE by themselves are not as informative as W2V vectors which is different to the findings of .\n",
        "output": "{\"label\": [null], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "For each of the experiments below we used the following configurations unless otherwise stated: we performed 5 fold stratified cross validation, features are scaled using Max Min scaling before inputting into the SVM, and used the respective C-Values for the SVM stated in the paper for each of the models.",
                "One major difficulty with the description of the method in the paper and re-implementation is handling the same target multiple appearances issue as originally raised by #REF .",
                "As the method requires context with regards to the target word, if there is more than one appearance of the target word then the method does not specify which to use.",
                "We therefore took the approach of #TARGET_REF and found all of the features for each appearance and performed median pooling over features.",
                "This change could explain the subtle differences between the results we report and those of the original paper."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: For each of the experiments below we used the following configurations unless otherwise stated: we performed 5 fold stratified cross validation, features are scaled using Max Min scaling before inputting into the SVM, and used the respective C-Values for the SVM stated in the paper for each of the models.\n sent1: One major difficulty with the description of the method in the paper and re-implementation is handling the same target multiple appearances issue as originally raised by #REF .\n sent2: As the method requires context with regards to the target word, if there is more than one appearance of the target word then the method does not specify which to use.\n sent3: We therefore took the approach of #TARGET_REF and found all of the features for each appearance and performed median pooling over features.\n sent4: This change could explain the subtle differences between the results we report and those of the original paper.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "TDParse the feature of TDParse-and the left and right contexts, and 3.",
                "TDParse+ the features of TDParse and LS and RS contexts.",
                "The experiments are performed on the #REF and #TARGET_REF Twitter datasets where we train and test on the previously specified train and test splits.",
                "We also scale our features using Max Min scaling before inputting into the SVM.",
                "We used all three sentiment lexicons as in the original paper, and we found the C-Value by performing 5 fold stratified cross validation on the training datasets."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: TDParse the feature of TDParse-and the left and right contexts, and 3.\n sent1: TDParse+ the features of TDParse and LS and RS contexts.\n sent2: The experiments are performed on the #REF and #TARGET_REF Twitter datasets where we train and test on the previously specified train and test splits.\n sent3: We also scale our features using Max Min scaling before inputting into the SVM.\n sent4: We used all three sentiment lexicons as in the original paper, and we found the C-Value by performing 5 fold stratified cross validation on the training datasets.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We choose the Charniak and Johnson reranking parser because it is freely available and achieves state-of-the-art accuracy (a Parseval f-score of 91.3%) on the WSJ domain (#REF) .",
                "It is, however, affected by domain variation - #TARGET_REF report that its f-score drops by approximately 8 percentage points when applied to the BNC domain.",
                "Our training size is 500,000 sentences.",
                "We conduct two experiments: the first, in which 500,000 sentences are extracted randomly from the BNC (minus the test set sentences), and the second in which only shorter sentences, of length ≤ 20 words, are chosen as training material.",
                "The rationale behind the second experiment is that shorter sentences are less likely to contain parser errors."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: We choose the Charniak and Johnson reranking parser because it is freely available and achieves state-of-the-art accuracy (a Parseval f-score of 91.3%) on the WSJ domain (#REF) .\n sent1: It is, however, affected by domain variation - #TARGET_REF report that its f-score drops by approximately 8 percentage points when applied to the BNC domain.\n sent2: Our training size is 500,000 sentences.\n sent3: We conduct two experiments: the first, in which 500,000 sentences are extracted randomly from the BNC (minus the test set sentences), and the second in which only shorter sentences, of length ≤ 20 words, are chosen as training material.\n sent4: The rationale behind the second experiment is that shorter sentences are less likely to contain parser errors.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The BLEU score jumps from 0.5358 to 0.6135.",
                "There are at least two possible reasons why a BLEU score of 0.66 is not obtained: The first is that the quality of the f-structure-annotated trees upon which the generator has been trained has degraded.",
                "For the baseline system, the generator is trained on f-structure-annotated trees derived from gold trees.",
                "The new system is trained on f-structureannotated parser output trees, and the performance of Charniak and Johnson's parser degrades when applied to BNC data #TARGET_REF .",
                "The second reason has been suggested by #REF : WSJ data is easier to learn than the more varied data in the Brown Corpus or BNC."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The BLEU score jumps from 0.5358 to 0.6135.\n sent1: There are at least two possible reasons why a BLEU score of 0.66 is not obtained: The first is that the quality of the f-structure-annotated trees upon which the generator has been trained has degraded.\n sent2: For the baseline system, the generator is trained on f-structure-annotated trees derived from gold trees.\n sent3: The new system is trained on f-structureannotated parser output trees, and the performance of Charniak and Johnson's parser degrades when applied to BNC data #TARGET_REF .\n sent4: The second reason has been suggested by #REF : WSJ data is easier to learn than the more varied data in the Brown Corpus or BNC.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Performance drops from a BLEU score of 0.66 on WSJ test data to 0.54 on the BNC test set.",
                "Encouragingly, we have also shown that domain-specific training material produced by a parser can be used to claw back a significant portion of this performance degradation.",
                "Our method is general and could be applied to other WSJ-trained generators (e.g. (Nakanishi et , 2007) ).",
                "We intend to continue this research by training our generator on parse trees produced by a BNC-self-trained version of the Charniak and Johnson reranking parser #TARGET_REF .",
                "We also hope to extend the evaluation beyond the BLEU metric by carrying out a human judgement evaluation."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: Performance drops from a BLEU score of 0.66 on WSJ test data to 0.54 on the BNC test set.\n sent1: Encouragingly, we have also shown that domain-specific training material produced by a parser can be used to claw back a significant portion of this performance degradation.\n sent2: Our method is general and could be applied to other WSJ-trained generators (e.g. (Nakanishi et , 2007) ).\n sent3: We intend to continue this research by training our generator on parse trees produced by a BNC-self-trained version of the Charniak and Johnson reranking parser #TARGET_REF .\n sent4: We also hope to extend the evaluation beyond the BLEU metric by carrying out a human judgement evaluation.\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Traditionally, supervised approaches have attained the best results in the area, but they are expensive to build because of the need of large amounts of manually annotated examples.",
                "Alternatively, knowledge based approaches rely on lexical resources such as WordNet, which are nowadays widely available in many languages (#REF) 1 .",
                "In particular, graph-based approaches represent the knowledge base as a graph, and apply several well-known graph analysis algorithms to perform WSD.",
                "UKB is a collection of programs which was first released for performing graph-based Word Sense Disambiguation using a preexisting knowledge base such as WordNet, and attained state-of-the-art results among knowledge-based systems when evaluated on standard benchmarks #TARGET_REF .",
                "In addition, UKB has been extended to perform disambiguation of medical entities (#REF) , named-entities (#REF; , word similarity and to create knowledge-based word embeddings (#REF) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Traditionally, supervised approaches have attained the best results in the area, but they are expensive to build because of the need of large amounts of manually annotated examples.\n sent1: Alternatively, knowledge based approaches rely on lexical resources such as WordNet, which are nowadays widely available in many languages (#REF) 1 .\n sent2: In particular, graph-based approaches represent the knowledge base as a graph, and apply several well-known graph analysis algorithms to perform WSD.\n sent3: UKB is a collection of programs which was first released for performing graph-based Word Sense Disambiguation using a preexisting knowledge base such as WordNet, and attained state-of-the-art results among knowledge-based systems when evaluated on standard benchmarks #TARGET_REF .\n sent4: In addition, UKB has been extended to perform disambiguation of medical entities (#REF) , named-entities (#REF; , word similarity and to create knowledge-based word embeddings (#REF) .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The software is quite popular, with 60 stars and 26 forks in github, as well as more than eight thousand direct downloads from the website since 2011.",
                "The software is coded in C++ and released under the GPL v3.0 license.",
                "When UKB was released, the papers specified the optimal parameters for WSD #TARGET_REF , as well as other key issues like the underlying knowledge-base version, specific set of relations to be used, and method to pre-process the input text.",
                "At the time, we assumed that future researchers would use the optimal parameters and settings specified in the papers, and that they would contact the authors if in doubt.",
                "The default parameters of the software were not optimal, and the other issues were left under the users responsibility."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The software is quite popular, with 60 stars and 26 forks in github, as well as more than eight thousand direct downloads from the website since 2011.\n sent1: The software is coded in C++ and released under the GPL v3.0 license.\n sent2: When UKB was released, the papers specified the optimal parameters for WSD #TARGET_REF , as well as other key issues like the underlying knowledge-base version, specific set of relations to be used, and method to pre-process the input text.\n sent3: At the time, we assumed that future researchers would use the optimal parameters and settings specified in the papers, and that they would contact the authors if in doubt.\n sent4: The default parameters of the software were not optimal, and the other issues were left under the users responsibility.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The frequencies are often derived from manually sense annotated corpora, such as Semcor (#REF) .",
                "We use the sense frequency accompanying Wordnet, which, according to the documentation, \"represents the decimal number of times the sense is tagged in various semantic concordance texts\".",
                "The frequencies are smoothed adding one to all counts (dict weight smooth).",
                "The sense frequency is used when initializing context words, and is also used to produce the final sense weights as a linear combination of sense frequencies and graph-based sense probabilities.",
                "The use of sense frequencies with UKB was introduced in #TARGET_REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The frequencies are often derived from manually sense annotated corpora, such as Semcor (#REF) .\n sent1: We use the sense frequency accompanying Wordnet, which, according to the documentation, \"represents the decimal number of times the sense is tagged in various semantic concordance texts\".\n sent2: The frequencies are smoothed adding one to all counts (dict weight smooth).\n sent3: The sense frequency is used when initializing context words, and is also used to produce the final sense weights as a linear combination of sense frequencies and graph-based sense probabilities.\n sent4: The use of sense frequencies with UKB was introduced in #TARGET_REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "When using UKB for WSD, the main parameters and settings can be classified in five main categories.",
                "For each of those we mention the best options and the associated UKB parameter when relevant (in italics), as taken from #TARGET_REF ):",
                "• Pre-processing of input text.",
                "When running UKB for WSD, one needs to define which 2 http://ixa2.si.ehu.eus/ukb 3 https://github.com/asoroa/ukb window of words is to be used as context to initialize the random walks.",
                "One option is to take just the sentence, but given that in some cases the sentences are very short, better results are obtained when considering previous and following sentences."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: When using UKB for WSD, the main parameters and settings can be classified in five main categories.\n sent1: For each of those we mention the best options and the associated UKB parameter when relevant (in italics), as taken from #TARGET_REF ):\n sent2: • Pre-processing of input text.\n sent3: When running UKB for WSD, one needs to define which 2 http://ixa2.si.ehu.eus/ukb 3 https://github.com/asoroa/ukb window of words is to be used as context to initialize the random walks.\n sent4: One option is to take just the sentence, but given that in some cases the sentences are very short, better results are obtained when considering previous and following sentences.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The difference is of nearly 10 absolute F1 points overall.",
                "5 This decrease could be caused by the fact that Raganato et al. (2017a) did not use sense frequencies.",
                "In addition to UKB, the table also reports the 5 Note that the UKB results for S2, S3 and S07 (62.6, 63.0 and 48.6 respectively) are different from those in #TARGET_REF , which is to be expected, as the new datasets have been converted to WordNet 3.0 (we confirmed experimentally that this is the sole difference between the two experiments).",
                "best performing knowledge-based systems on this dataset.",
                "Raganato et al. (2017a) run several wellknown algorithms when presenting their datasets."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: The difference is of nearly 10 absolute F1 points overall.\n sent1: 5 This decrease could be caused by the fact that Raganato et al. (2017a) did not use sense frequencies.\n sent2: In addition to UKB, the table also reports the 5 Note that the UKB results for S2, S3 and S07 (62.6, 63.0 and 48.6 respectively) are different from those in #TARGET_REF , which is to be expected, as the new datasets have been converted to WordNet 3.0 (we confirmed experimentally that this is the sole difference between the two experiments).\n sent3: best performing knowledge-based systems on this dataset.\n sent4: Raganato et al. (2017a) run several wellknown algorithms when presenting their datasets.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "In addition to the results of UKB using the setting in #TARGET_REF as specified in Section 3, we checked whether some reasonable settings would obtain better results.",
                "Table 3 shows the results when applying the three algorithms described in Section 3, both with and without sense frequencies, as well as using a single sentence for context or extended context.",
                "The table shows that the key factor is the use of sense frequencies, and systems that do not use them (those with a nf subscript) suffer a loss between 7 and 8 percentage points in F1.",
                "This would explain part of the decrease in performance reported in (#REFa) , as they explicitly mention that they did not activate the use of sense frequencies in UKB.",
                "The table also shows that extending the context is mildly effective."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: In addition to the results of UKB using the setting in #TARGET_REF as specified in Section 3, we checked whether some reasonable settings would obtain better results.\n sent1: Table 3 shows the results when applying the three algorithms described in Section 3, both with and without sense frequencies, as well as using a single sentence for context or extended context.\n sent2: The table shows that the key factor is the use of sense frequencies, and systems that do not use them (those with a nf subscript) suffer a loss between 7 and 8 percentage points in F1.\n sent3: This would explain part of the decrease in performance reported in (#REFa) , as they explicitly mention that they did not activate the use of sense frequencies in UKB.\n sent4: The table also shows that extending the context is mildly effective.\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Our method is able to process the load of the average Twitter Firehose 3 stream on a single core of modest hardware while retaining effectiveness on par with one of the most accurate FSD systems.",
                "During the TDT program, FSD was applied to news wire documents and solely focused on effectiveness, neglecting efficiency and scalability.",
                "The traditional approach to FSD #TARGET_REF computes the distance of each incoming document 1 e.g. a natural disaster or a scandal 2 TDT by NIST -1998 NIST - -2004 .",
                "http://www.itl.nist.gov/ iad/mig/tests/tdt/resources.html (Last Update: 2008) 3 5,700 tweets per second https://about.twitter .com/company (last updated: March 31, 2015) to all previously seen documents and the minimum distance determines the novelty score.",
                "Documents, whose minimum distance falls above a certain threshold are considered to talk about a new event and declared as first stories."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Our method is able to process the load of the average Twitter Firehose 3 stream on a single core of modest hardware while retaining effectiveness on par with one of the most accurate FSD systems.\n sent1: During the TDT program, FSD was applied to news wire documents and solely focused on effectiveness, neglecting efficiency and scalability.\n sent2: The traditional approach to FSD #TARGET_REF computes the distance of each incoming document 1 e.g. a natural disaster or a scandal 2 TDT by NIST -1998 NIST - -2004 .\n sent3: http://www.itl.nist.gov/ iad/mig/tests/tdt/resources.html (Last Update: 2008) 3 5,700 tweets per second https://about.twitter .com/company (last updated: March 31, 2015) to all previously seen documents and the minimum distance determines the novelty score.\n sent4: Documents, whose minimum distance falls above a certain threshold are considered to talk about a new event and declared as first stories.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Each tweet was hashed, placing it into buckets that contain other similar tweets, which are subsequently compared.",
                "Operation in constant space was ensured by keeping the number of tweets per bucket constant.",
                "Because LSH alone performed ineffectively, #TARGET_REF additionally compared each incoming tweet with the k most recent tweets.",
                "#REF analysed scoring functions for novelty detection while focusing on their effectiveness.",
                "They presented a language-model (LM) based novelty measure using the KL divergence between the LM of a document and a single LM built on all previously scored documents, which they referred to as an aggregate measure language model."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Each tweet was hashed, placing it into buckets that contain other similar tweets, which are subsequently compared.\n sent1: Operation in constant space was ensured by keeping the number of tweets per bucket constant.\n sent2: Because LSH alone performed ineffectively, #TARGET_REF additionally compared each incoming tweet with the k most recent tweets.\n sent3: #REF analysed scoring functions for novelty detection while focusing on their effectiveness.\n sent4: They presented a language-model (LM) based novelty measure using the KL divergence between the LM of a document and a single LM built on all previously scored documents, which they referred to as an aggregate measure language model.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We compare our system (k-term) against 3 baselines.",
                "UMass is a state-of-the-art FSD system, developed by #REF .",
                "It is known for its high effectiveness in the TDT2 and TDT3 competitions (#REF) and widely used as a benchmark for FSD systems #TARGET_REF; #REF; #REF; ) .",
                "UMass makes use of an inverted index and k-nearest-neighbour clustering, which optimize the system for speed by ensuring a minimal number of comparisons.",
                "To maximise efficiency, we set-up UMass to operate in-memory by turning off its default memory mapping to disk."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: We compare our system (k-term) against 3 baselines.\n sent1: UMass is a state-of-the-art FSD system, developed by #REF .\n sent2: It is known for its high effectiveness in the TDT2 and TDT3 competitions (#REF) and widely used as a benchmark for FSD systems #TARGET_REF; #REF; #REF; ) .\n sent3: UMass makes use of an inverted index and k-nearest-neighbour clustering, which optimize the system for speed by ensuring a minimal number of comparisons.\n sent4: To maximise efficiency, we set-up UMass to operate in-memory by turning off its default memory mapping to disk.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "UMass makes use of an inverted index and k-nearest-neighbour clustering, which optimize the system for speed by ensuring a minimal number of comparisons.",
                "To maximise efficiency, we set-up UMass to operate in-memory by turning off its default memory mapping to disk.",
                "This ensures fair comparisons, as all algorithms operate in memory.",
                "LSH-FSD is a highly-scalable system by #TARGET_REF .",
                "It is based on Locality Sensitive Hashing (LSH) and claims to operate in constant time and space while performing on a comparable level of accuracy as UMass."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: UMass makes use of an inverted index and k-nearest-neighbour clustering, which optimize the system for speed by ensuring a minimal number of comparisons.\n sent1: To maximise efficiency, we set-up UMass to operate in-memory by turning off its default memory mapping to disk.\n sent2: This ensures fair comparisons, as all algorithms operate in memory.\n sent3: LSH-FSD is a highly-scalable system by #TARGET_REF .\n sent4: It is based on Locality Sensitive Hashing (LSH) and claims to operate in constant time and space while performing on a comparable level of accuracy as UMass.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "To maximise efficiency, we set-up UMass to operate in-memory by turning off its default memory mapping to disk.",
                "This ensures fair comparisons, as all algorithms operate in memory.",
                "LSH-FSD is a highly-scalable system by #REF .",
                "It is based on Locality Sensitive Hashing (LSH) and claims to operate in constant time and space while performing on a comparable level of accuracy as UMass.",
                "We configure their system using the default parameters #TARGET_REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: To maximise efficiency, we set-up UMass to operate in-memory by turning off its default memory mapping to disk.\n sent1: This ensures fair comparisons, as all algorithms operate in memory.\n sent2: LSH-FSD is a highly-scalable system by #REF .\n sent3: It is based on Locality Sensitive Hashing (LSH) and claims to operate in constant time and space while performing on a comparable level of accuracy as UMass.\n sent4: We configure their system using the default parameters #TARGET_REF .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "The pair-wise comparison of UMass causes it's throughput to decrease drastically with every new document.",
                "In Figure 2 we compare the memory requirements of k-term and LSH-FSD at different points in the stream.",
                "Although #TARGET_REF designed their system (LSH-FSD) to operate in constant space, we found that the memory requirement gradually increases with the number of documents processed, as seen in Figure 3 .",
                "We hypothesise that this increase results from new terms added to the vocabulary.",
                "Our system has a strictly constant memory footprint."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: The pair-wise comparison of UMass causes it's throughput to decrease drastically with every new document.\n sent1: In Figure 2 we compare the memory requirements of k-term and LSH-FSD at different points in the stream.\n sent2: Although #TARGET_REF designed their system (LSH-FSD) to operate in constant space, we found that the memory requirement gradually increases with the number of documents processed, as seen in Figure 3 .\n sent3: We hypothesise that this increase results from new terms added to the vocabulary.\n sent4: Our system has a strictly constant memory footprint.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "However, in practice, it is expensive and laborious to collect labeled data for all possible testing conditions.",
                "In contrast, collecting large amount of unlabeled indomain data and labeled out-of-domain data can be fast and economical.",
                "Hence, an important question arises for this scenario: how can we do unsupervised adaptation for acoustic models by utilizing labeled out-of-domain data and unlabeled in-domain data, in order to achieve good performance on in-domain data?",
                "Research on unsupervised adaptation for acoustic models can be roughly divided into three categories: (1) constrained model adaptation [5, 6, 7] , (2) domain-invariant feature extraction [8, 9, 10] , and (3) labeled in-domain data augmentation by synthesis [11, 12, #TARGET_REF .",
                "Among these approaches, data augmentation-based adaptation is favorable, because it does not require extra hyperparameter tuning for acoustic model training, and can utilize full model capacity by training a model with as much and as diverse a dataset as possible."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: However, in practice, it is expensive and laborious to collect labeled data for all possible testing conditions.\n sent1: In contrast, collecting large amount of unlabeled indomain data and labeled out-of-domain data can be fast and economical.\n sent2: Hence, an important question arises for this scenario: how can we do unsupervised adaptation for acoustic models by utilizing labeled out-of-domain data and unlabeled in-domain data, in order to achieve good performance on in-domain data?\n sent3: Research on unsupervised adaptation for acoustic models can be roughly divided into three categories: (1) constrained model adaptation [5, 6, 7] , (2) domain-invariant feature extraction [8, 9, 10] , and (3) labeled in-domain data augmentation by synthesis [11, 12, #TARGET_REF .\n sent4: Among these approaches, data augmentation-based adaptation is favorable, because it does not require extra hyperparameter tuning for acoustic model training, and can utilize full model capacity by training a model with as much and as diverse a dataset as possible.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Another benefit of this approach is that data in their original domain are more intuitive to humans.",
                "In other words, it is easier for us to inspect and manipulate the data.",
                "Furthermore, with the recent progress on domain translation #TARGET_REF 14, 15] , conditional synthesis of indomain data without parallel data has become achievable, which makes data augmentation-based adaptation a more promising direction to investigate.",
                "Variational autoencoder-based data augmentation (VAE-DA) is a domain adaptation method proposed in [13] , which pools in-domain and out-domain to train a VAE that learns factorized latent representations of speech segments.",
                "To disentangle linguistic factors from nuisance ones in the latent space, statistics of the latent representations for each utterance are computed."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Another benefit of this approach is that data in their original domain are more intuitive to humans.\n sent1: In other words, it is easier for us to inspect and manipulate the data.\n sent2: Furthermore, with the recent progress on domain translation #TARGET_REF 14, 15] , conditional synthesis of indomain data without parallel data has become achievable, which makes data augmentation-based adaptation a more promising direction to investigate.\n sent3: Variational autoencoder-based data augmentation (VAE-DA) is a domain adaptation method proposed in [13] , which pools in-domain and out-domain to train a VAE that learns factorized latent representations of speech segments.\n sent4: To disentangle linguistic factors from nuisance ones in the latent space, statistics of the latent representations for each utterance are computed.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "A key observation made in #TARGET_REF is that nuisance factors, such as speaker identity and room acoustics, are generally constant over segments within an utterance, while linguistic content changes from segment to segment.",
                "In other words, latent nuisance vectors zn are relatively consistent within an utterance, while the distribution of z conditioned on an utterance can be assumed to have the same distribution as the prior.",
                "Therefore, suppose the prior is a diagonal Gaussian with zero mean.",
                "Given an utterance",
                "of N segments, we have:"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: A key observation made in #TARGET_REF is that nuisance factors, such as speaker identity and room acoustics, are generally constant over segments within an utterance, while linguistic content changes from segment to segment.\n sent1: In other words, latent nuisance vectors zn are relatively consistent within an utterance, while the distribution of z conditioned on an utterance can be assumed to have the same distribution as the prior.\n sent2: Therefore, suppose the prior is a diagonal Gaussian with zero mean.\n sent3: Given an utterance\n sent4: of N segments, we have:\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "With a trained FHVAE, we are able to infer disentangled latent representations that capture linguistic factors z1 and nuisance factors z2.",
                "To transform nuisance factors of an utterance X without changing the corresponding transcript, one only needs to perturb Z2.",
                "Furthermore, since each z2 within an utterance is generated conditioned on a Gaussian whose mean is µ2, we can regard µ2 as the representation of nuisance factors of an utterance.",
                "We now derive two data augmentation methods similar to those proposed in #TARGET_REF , named nuisance factor replacement and nuisance factor perturbation."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: With a trained FHVAE, we are able to infer disentangled latent representations that capture linguistic factors z1 and nuisance factors z2.\n sent1: To transform nuisance factors of an utterance X without changing the corresponding transcript, one only needs to perturb Z2.\n sent2: Furthermore, since each z2 within an utterance is generated conditioned on a Gaussian whose mean is µ2, we can regard µ2 as the representation of nuisance factors of an utterance.\n sent3: We now derive two data augmentation methods similar to those proposed in #TARGET_REF , named nuisance factor replacement and nuisance factor perturbation.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Naively, we may want to sample p from a centered isotropic Gaussian.",
                "However, in practice, VAE-type of models suffer from an over-pruning issue [21] in that some latent variables become inactive, which we do not want to perturb.",
                "Instead, we only want to perturb the linear subspace which models the variation of nuisance factors between utterances.",
                "Therefore, we adopt a similar soft perturbation scheme as in #TARGET_REF .",
                "First, {µ2} M i=1 for all M utterances are estimated with the approximated MAP."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: Naively, we may want to sample p from a centered isotropic Gaussian.\n sent1: However, in practice, VAE-type of models suffer from an over-pruning issue [21] in that some latent variables become inactive, which we do not want to perturb.\n sent2: Instead, we only want to perturb the linear subspace which models the variation of nuisance factors between utterances.\n sent3: Therefore, we adopt a similar soft perturbation scheme as in #TARGET_REF .\n sent4: First, {µ2} M i=1 for all M utterances are estimated with the approximated MAP.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Both augmentation methods outperform their VAE counterparts and the domain invariant feature baseline using the same FHVAE model.",
                "We attribute the improvement to the better quality of the transformed IHM data, which covers the nuisance factors of the SDM data, without altering the original linguistic content.",
                "To verify the superiority of the proposed method of drawing random perturbation vectors, we compare two alternative sampling methods: rev-p and uni-p, similar to #TARGET_REF , with the same expected squared Euclidean norm as the proposed method.",
                "Table 2 confirm that the proposed sampling method is more effective under the same perturbation scale γ = 1.0 compared to the alternative methods as expected.",
                "Due to imperfect reconstruction using FHVAE models, some linguistic information may be lost in this process."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: Both augmentation methods outperform their VAE counterparts and the domain invariant feature baseline using the same FHVAE model.\n sent1: We attribute the improvement to the better quality of the transformed IHM data, which covers the nuisance factors of the SDM data, without altering the original linguistic content.\n sent2: To verify the superiority of the proposed method of drawing random perturbation vectors, we compare two alternative sampling methods: rev-p and uni-p, similar to #TARGET_REF , with the same expected squared Euclidean norm as the proposed method.\n sent3: Table 2 confirm that the proposed sampling method is more effective under the same perturbation scale γ = 1.0 compared to the alternative methods as expected.\n sent4: Due to imperfect reconstruction using FHVAE models, some linguistic information may be lost in this process.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The first two rows of results show that the WER gap between the unadapted model and the model trained on in-domain data is 24%.",
                "The third row reports the results of training with domain invariant feature, z1, extracted with a FHVAE as is done in [10] .",
                "It improves over the baseline by 6% absolute.",
                "VAE-DA #TARGET_REF results with nuisance factor replacement (repl) and latent nuisance perturbation (p) are shown in the last three rows.",
                "We then examine the effectiveness of our proposed method and show the results in the second, third, and fourth rows in Table 2 ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The first two rows of results show that the WER gap between the unadapted model and the model trained on in-domain data is 24%.\n sent1: The third row reports the results of training with domain invariant feature, z1, extracted with a FHVAE as is done in [10] .\n sent2: It improves over the baseline by 6% absolute.\n sent3: VAE-DA #TARGET_REF results with nuisance factor replacement (repl) and latent nuisance perturbation (p) are shown in the last three rows.\n sent4: We then examine the effectiveness of our proposed method and show the results in the second, third, and fourth rows in Table 2 .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Ultimately, we may need to read through the entire document or even search the web to find other occurances of the expression (global context) so that we can guess its meaning.",
                "Can machines help us do this work?",
                "#REF have proposed a task of generating a definition for a phrase given its local context.",
                "However, they follow the strict assumption that the target phrase is newly emerged and there is only a single local context available for the phrase, which makes the task of generating an accurate and coherent definition difficult (perhaps as difficult as a human comprehending the phrase itself).",
                "On the other hand, #TARGET_REF attempted to generate a definition of a word from an embedding induced from massive text (which can be seen as global context)."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Ultimately, we may need to read through the entire document or even search the web to find other occurances of the expression (global context) so that we can guess its meaning.\n sent1: Can machines help us do this work?\n sent2: #REF have proposed a task of generating a definition for a phrase given its local context.\n sent3: However, they follow the strict assumption that the target phrase is newly emerged and there is only a single local context available for the phrase, which makes the task of generating an accurate and coherent definition difficult (perhaps as difficult as a human comprehending the phrase itself).\n sent4: On the other hand, #TARGET_REF attempted to generate a definition of a word from an embedding induced from massive text (which can be seen as global context).\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Paraphrasing (#REF; #REF) (or text simplification (#REF) ) can be used to rephrase words with unknown senses.",
                "However, the target of paraphrase acquisition are words/phrases with no specified context.",
                "Although a few studies (#REF; #REF; #REF) consider subsentential (context-sensitive) paraphrases, they do not intend to obtain a definition-like description as a paraphrase of a word.",
                "Recently, #TARGET_REF introduced a task of generating a definition sentence of a word from its pre-trained embedding.",
                "Since their task does not take local contexts of words as inputs, their method cannot generate an appropriate definition for a polysemous word for a specific context."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Paraphrasing (#REF; #REF) (or text simplification (#REF) ) can be used to rephrase words with unknown senses.\n sent1: However, the target of paraphrase acquisition are words/phrases with no specified context.\n sent2: Although a few studies (#REF; #REF; #REF) consider subsentential (context-sensitive) paraphrases, they do not intend to obtain a definition-like description as a paraphrase of a word.\n sent3: Recently, #TARGET_REF introduced a task of generating a definition sentence of a word from its pre-trained embedding.\n sent4: Since their task does not take local contexts of words as inputs, their method cannot generate an appropriate definition for a polysemous word for a specific context.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The model therefore combines both pieces of information to generate a natural language description.",
                "Considering various applications where we need definitions of expressions, we evaluated our method with four datasets including WordNet #TARGET_REF for general words, the Oxford dictionary (#REF) for polysemous words, Urban Dictionary (#REF) for rare idioms or slang, and a newlycreated Wikipedia dataset for entities.",
                "Our contributions are as follows:",
                "• We propose a general task of defining unknown phrases given their contexts.",
                "This task is a generalization of three related tasks (#REF; #REF; #REF) and involves various situations where we need definitions of unknown phrases ( § 2)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The model therefore combines both pieces of information to generate a natural language description.\n sent1: Considering various applications where we need definitions of expressions, we evaluated our method with four datasets including WordNet #TARGET_REF for general words, the Oxford dictionary (#REF) for polysemous words, Urban Dictionary (#REF) for rare idioms or slang, and a newlycreated Wikipedia dataset for entities.\n sent2: Our contributions are as follows:\n sent3: • We propose a general task of defining unknown phrases given their contexts.\n sent4: This task is a generalization of three related tasks (#REF; #REF; #REF) and involves various situations where we need definitions of unknown phrases ( § 2).\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "In order to capture the surface information of X trg , we construct character-level CNNs (Eq. (6)) following #TARGET_REF .",
                "Note that the input to the CNNs is a sequence of words in X trg , which are concatenated with special character \" ,\" such as \"sonic boom.\" Following #REF, we set the CNN kernels of length 2-6 and size 10, 30, 40, 40, 40 respectively with a stride of 1 to obtain a 160-dimensional vector c trg ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In order to capture the surface information of X trg , we construct character-level CNNs (Eq. (6)) following #TARGET_REF .\n sent1: Note that the input to the CNNs is a sequence of words in X trg , which are concatenated with special character \" ,\" such as \"sonic boom.\" Following #REF, we set the CNN kernels of length 2-6 and size 10, 30, 40, 40, 40 respectively with a stride of 1 to obtain a 160-dimensional vector c trg .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "In order to capture the surface information of X trg , we construct character-level CNNs (Eq. (6)) following (#REF) .",
                "Note that the input to the CNNs is a sequence of words in X trg , which are concatenated with special character \" ,\" such as \"sonic boom.\" Following #TARGET_REF, we set the CNN kernels of length 2-6 and size 10, 30, 40, 40, 40 respectively with a stride of 1 to obtain a 160-dimensional vector c trg ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In order to capture the surface information of X trg , we construct character-level CNNs (Eq. (6)) following (#REF) .\n sent1: Note that the input to the CNNs is a sequence of words in X trg , which are concatenated with special character \" ,\" such as \"sonic boom.\" Following #TARGET_REF, we set the CNN kernels of length 2-6 and size 10, 30, 40, 40, 40 respectively with a stride of 1 to obtain a 160-dimensional vector c trg .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "These definitions are regarded as groundtruth descriptions.",
                "Datasets To evaluate our model on the word description task on WordNet, we followed #TARGET_REF and extracted data from WordNet using the dict-definition 9 toolkit.",
                "Each entry in the data consists of three elements: (1) a word, (2) its definition, and (3) a usage example of the Table 2 : Domains, expressions to be described, and the coverage of pre-trained embeddings of the expressions to be described.",
                "word.",
                "We split this dataset to obtain Train, Validation, and Test sets."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: These definitions are regarded as groundtruth descriptions.\n sent1: Datasets To evaluate our model on the word description task on WordNet, we followed #TARGET_REF and extracted data from WordNet using the dict-definition 9 toolkit.\n sent2: Each entry in the data consists of three elements: (1) a word, (2) its definition, and (3) a usage example of the Table 2 : Domains, expressions to be described, and the coverage of pre-trained embeddings of the expressions to be described.\n sent3: word.\n sent4: We split this dataset to obtain Train, Validation, and Test sets.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "If a word has multiple definitions/examples, we treat them as different entries.",
                "Note that the words are mutually exclusive across the three sets.",
                "The only difference between our dataset and theirs is that we extract the tuples only if the words have their usage examples in WordNet.",
                "Since not all entries in WordNet have usage examples, our dataset is a small subset of #TARGET_REF .",
                "In addition to WordNet, we use the Oxford Dictionary following #REF , the Urban Dictionary following #REF and our Wikipedia dataset described in the previous section."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: If a word has multiple definitions/examples, we treat them as different entries.\n sent1: Note that the words are mutually exclusive across the three sets.\n sent2: The only difference between our dataset and theirs is that we extract the tuples only if the words have their usage examples in WordNet.\n sent3: Since not all entries in WordNet have usage examples, our dataset is a small subset of #TARGET_REF .\n sent4: In addition to WordNet, we use the Oxford Dictionary following #REF , the Urban Dictionary following #REF and our Wikipedia dataset described in the previous section.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Global #TARGET_REF , (2) Local (#REF) with CNN, (3) I-Attention (#REF) , and our proposed model, (4) LOGCaD. The Global model is our reimplementation of the best model (S + G + CH) in #TARGET_REF .",
                "It can access the global context of a phrase to be described, but has no ability to read the local context.",
                "The Local model is the reimplementation of the best model (dual encoder) in #REF .",
                "In order to make a fair comparison of the effectiveness of local and global contexts, we slightly modify the original implementation by #REF; as the character-level encoder in the Local model, we adopt CNNs that are exactly the same as the other two models instead of the original LSTMs.",
                "The I-Attention is our reimplementation of the best model (S + I-Attention) in #REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Global #TARGET_REF , (2) Local (#REF) with CNN, (3) I-Attention (#REF) , and our proposed model, (4) LOGCaD. The Global model is our reimplementation of the best model (S + G + CH) in #TARGET_REF .\n sent1: It can access the global context of a phrase to be described, but has no ability to read the local context.\n sent2: The Local model is the reimplementation of the best model (dual encoder) in #REF .\n sent3: In order to make a fair comparison of the effectiveness of local and global contexts, we slightly modify the original implementation by #REF; as the character-level encoder in the Local model, we adopt CNNs that are exactly the same as the other two models instead of the original LSTMs.\n sent4: The I-Attention is our reimplementation of the best model (S + I-Attention) in #REF .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "To verify this idea, we propose to incorporate both local and global contexts to describe an unknown phrase.",
                "Figure 1 shows an illustration of our LOG-CaD model.",
                "Similarly to the standard encoder-decoder model with attention (#REF; #REF) , it has a context encoder and a description decoder.",
                "The challenge here is that the decoder needs to be conditioned not only on the local context, but also on its global context.",
                "To incorporate the different types of contexts, we propose to use a gate function similar to #TARGET_REF to dynamically control how the global and local contexts influence the description."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: To verify this idea, we propose to incorporate both local and global contexts to describe an unknown phrase.\n sent1: Figure 1 shows an illustration of our LOG-CaD model.\n sent2: Similarly to the standard encoder-decoder model with attention (#REF; #REF) , it has a context encoder and a description decoder.\n sent3: The challenge here is that the decoder needs to be conditioned not only on the local context, but also on its global context.\n sent4: To incorporate the different types of contexts, we propose to use a gate function similar to #TARGET_REF to dynamically control how the global and local contexts influence the description.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "In order to capture the interaction between the local and global contexts, we adopt a GATE(·) function (Eq. (7)) which is similar to #TARGET_REF .",
                "The GATE(·) function updates the LSTM output s t to s t depending on the global context x trg , local context d t , and character-level information c trg as",
                "where σ(·), and ; denote the sigmoid function, element-wise multiplication, and vector concatenation, respectively.",
                "W * and b * are weight matrices and bias terms, respectively.",
                "Here, the update gate z t controls how much the original hidden state s t is to be changed, and the reset gate r t controls how much the information from f t contributes to word generation at each time step."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: In order to capture the interaction between the local and global contexts, we adopt a GATE(·) function (Eq. (7)) which is similar to #TARGET_REF .\n sent1: The GATE(·) function updates the LSTM output s t to s t depending on the global context x trg , local context d t , and character-level information c trg as\n sent2: where σ(·), and ; denote the sigmoid function, element-wise multiplication, and vector concatenation, respectively.\n sent3: W * and b * are weight matrices and bias terms, respectively.\n sent4: Here, the update gate z t controls how much the original hidden state s t is to be changed, and the reset gate r t controls how much the information from f t contributes to word generation at each time step.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "First of all, we share the current status of researches on natural language understanding, statistical modeling and deep neural network and explain the key issues in deep Bayesian learning for discrete-valued observation data and latent semantics.",
                "A new paradigm called the symbolic neural learning is introduced to extend how data analysis is performed from language processing to semantic learning and memory networking.",
                "Secondly, we address a number of Bayesian models ranging from latent variable model to VB inference (#REF; #REF; #REFb) , MCMC sampling (#REF) and BNP learning (#REF; #REFa; #REF) for hierarchical, thematic and sparse topics from natural language.",
                "In the third part, a series of deep models including deep unfolding (#REF) , Bayesian RNN (#REF; #REF) , sequence-to-sequence learning (#REF; #REF) , CNN ( #TARGET_REF; #REF; , GAN (#REF) and VAE are introduced.",
                "The coffee break is arranged within this part."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: First of all, we share the current status of researches on natural language understanding, statistical modeling and deep neural network and explain the key issues in deep Bayesian learning for discrete-valued observation data and latent semantics.\n sent1: A new paradigm called the symbolic neural learning is introduced to extend how data analysis is performed from language processing to semantic learning and memory networking.\n sent2: Secondly, we address a number of Bayesian models ranging from latent variable model to VB inference (#REF; #REF; #REFb) , MCMC sampling (#REF) and BNP learning (#REF; #REFa; #REF) for hierarchical, thematic and sparse topics from natural language.\n sent3: In the third part, a series of deep models including deep unfolding (#REF) , Bayesian RNN (#REF; #REF) , sequence-to-sequence learning (#REF; #REF) , CNN ( #TARGET_REF; #REF; , GAN (#REF) and VAE are introduced.\n sent4: The coffee break is arranged within this part.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "This paper presents a novel approach to improve reordering in phrase-based machine translation by using richer, syntactic representations of units of bilingual language models (BiLMs).",
                "Our method to include syntactic information is simple in implementation and requires minimal changes in the decoding algorithm.",
                "The approach is evaluated in a series of ArabicEnglish and Chinese-English translation experiments.",
                "The best models demonstrate significant improvements in BLEU and TER over the phrase-based baseline, as well as over the lexicalized BiLM by #TARGET_REF .",
                "Further improvements of up to 0.45 BLEU for ArabicEnglish and up to 0.59 BLEU for ChineseEnglish are obtained by combining our dependency BiLM with a lexicalized BiLM."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This paper presents a novel approach to improve reordering in phrase-based machine translation by using richer, syntactic representations of units of bilingual language models (BiLMs).\n sent1: Our method to include syntactic information is simple in implementation and requires minimal changes in the decoding algorithm.\n sent2: The approach is evaluated in a series of ArabicEnglish and Chinese-English translation experiments.\n sent3: The best models demonstrate significant improvements in BLEU and TER over the phrase-based baseline, as well as over the lexicalized BiLM by #TARGET_REF .\n sent4: Further improvements of up to 0.45 BLEU for ArabicEnglish and up to 0.59 BLEU for ChineseEnglish are obtained by combining our dependency BiLM with a lexicalized BiLM.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Lexical distortion modeling (#REF) conditions reordering probabilities on the phrase pairs translated at the current and previous steps.",
                "Unlike linear distortion, it characterizes reordering not in terms of distance but type: monotone, swap, or discontinuous.",
                "In this paper, we base our approach to reordering on bilingual language models (#REF; #TARGET_REF .",
                "Instead of directly characterizing reordering, they model sequences of elementary translation events as a Markov process.",
                "1 Originally, #REF used this kind of model as the translation model, while more recently it has been used as an additional model in PBSMT systems (#REF) ."
            ],
            "label": [
                "BACKGROUND",
                "SIMILARITY"
            ]
        },
        "input": "sent0: Lexical distortion modeling (#REF) conditions reordering probabilities on the phrase pairs translated at the current and previous steps.\n sent1: Unlike linear distortion, it characterizes reordering not in terms of distance but type: monotone, swap, or discontinuous.\n sent2: In this paper, we base our approach to reordering on bilingual language models (#REF; #TARGET_REF .\n sent3: Instead of directly characterizing reordering, they model sequences of elementary translation events as a Markov process.\n sent4: 1 Originally, #REF used this kind of model as the translation model, while more recently it has been used as an additional model in PBSMT systems (#REF) .\n",
        "output": "{\"label\": [\"BACKGROUND\", \"SIMILARITY\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Unlike linear distortion, it characterizes reordering not in terms of distance but type: monotone, swap, or discontinuous.",
                "In this paper, we base our approach to reordering on bilingual language models (#REF; #REF) .",
                "Instead of directly characterizing reordering, they model sequences of elementary translation events as a Markov process.",
                "1 Originally, #REF used this kind of model as the translation model, while more recently it has been used as an additional model in PBSMT systems #TARGET_REF .",
                "We adopt and generalize the approach of #REF to investigate several variations of bilingual language models."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Unlike linear distortion, it characterizes reordering not in terms of distance but type: monotone, swap, or discontinuous.\n sent1: In this paper, we base our approach to reordering on bilingual language models (#REF; #REF) .\n sent2: Instead of directly characterizing reordering, they model sequences of elementary translation events as a Markov process.\n sent3: 1 Originally, #REF used this kind of model as the translation model, while more recently it has been used as an additional model in PBSMT systems #TARGET_REF .\n sent4: We adopt and generalize the approach of #REF to investigate several variations of bilingual language models.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Our method consists of labeling elementary translation events (tokens of bilingual LMs) with their different contextual properties.",
                "What kind of contextual information should be incorporated in a reordering model?",
                "Lexical information has been used by #REF but is known to suffer from data sparsity (#REF) .",
                "Also previous contributions to bilingual language modeling (#REF; #TARGET_REF have mostly used lexical information, although Crego and Yvon (2010a) and Crego and Yvon (2010b) label bilingual to-kens with a rich set of POS tags.",
                "But in general, reordering is considered to be a syntactic phenomenon and thus the relevant features are syntactic (#REF; #REF) ."
            ],
            "label": [
                "BACKGROUND",
                "MOTIVATION"
            ]
        },
        "input": "sent0: Our method consists of labeling elementary translation events (tokens of bilingual LMs) with their different contextual properties.\n sent1: What kind of contextual information should be incorporated in a reordering model?\n sent2: Lexical information has been used by #REF but is known to suffer from data sparsity (#REF) .\n sent3: Also previous contributions to bilingual language modeling (#REF; #TARGET_REF have mostly used lexical information, although Crego and Yvon (2010a) and Crego and Yvon (2010b) label bilingual to-kens with a rich set of POS tags.\n sent4: But in general, reordering is considered to be a syntactic phenomenon and thus the relevant features are syntactic (#REF; #REF) .\n",
        "output": "{\"label\": [\"BACKGROUND\", \"MOTIVATION\"], \"context\": [\"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "We investigate the following research questions: How well can we capture reordering regularities of a language pair by incorporating source syntactic parameters into the units of a bilingual language model? What kind of source syntactic parameters are necessary and sufficient?",
                "Our contributions can be summarized as follows: We argue that the contextual information used in the original bilingual models #TARGET_REF is insufficient and introduce a simple model that exploits source-side syntax to improve reordering (Sections 2 and 3).",
                "We perform a thorough comparison between different variants of our general model and compare them to the original approach.",
                "We carry out translation experiments on multiple test sets, two language pairs (ArabicEnglish and Chinese-English), and with respect to two metrics (BLEU and TER).",
                "Finally, we present a preliminary analysis of the reorderings resulting from the proposed models (Section 4)."
            ],
            "label": [
                "BACKGROUND",
                "MOTIVATION"
            ]
        },
        "input": "sent0: We investigate the following research questions: How well can we capture reordering regularities of a language pair by incorporating source syntactic parameters into the units of a bilingual language model? What kind of source syntactic parameters are necessary and sufficient?\n sent1: Our contributions can be summarized as follows: We argue that the contextual information used in the original bilingual models #TARGET_REF is insufficient and introduce a simple model that exploits source-side syntax to improve reordering (Sections 2 and 3).\n sent2: We perform a thorough comparison between different variants of our general model and compare them to the original approach.\n sent3: We carry out translation experiments on multiple test sets, two language pairs (ArabicEnglish and Chinese-English), and with respect to two metrics (BLEU and TER).\n sent4: Finally, we present a preliminary analysis of the reorderings resulting from the proposed models (Section 4).\n",
        "output": "{\"label\": [\"BACKGROUND\", \"MOTIVATION\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "where e i is the i-th target word and A : E → P(F ) is an alignment function, E and F referring to target and source sentences, and P(·) is the powerset function.",
                "In other words, the i-th translation event consists of the i-th target word and all source words aligned to it.",
                "#TARGET_REF refer to the defined translation events t i as bilingual tokens and we adopt this terminology.",
                "There are alternative definitions of bilingual language models.",
                "Our choice of the above definition is supported by the fact that it produces an unambiguous segmentation of a parallel sentence into tokens."
            ],
            "label": [
                "BACKGROUND",
                "SIMILARITY"
            ]
        },
        "input": "sent0: where e i is the i-th target word and A : E → P(F ) is an alignment function, E and F referring to target and source sentences, and P(·) is the powerset function.\n sent1: In other words, the i-th translation event consists of the i-th target word and all source words aligned to it.\n sent2: #TARGET_REF refer to the defined translation events t i as bilingual tokens and we adopt this terminology.\n sent3: There are alternative definitions of bilingual language models.\n sent4: Our choice of the above definition is supported by the fact that it produces an unambiguous segmentation of a parallel sentence into tokens.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"SIMILARITY\"], \"context\": [\"sent2\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "By using unnecessarily fine-grained categories we risk running into sparsity issues.",
                "#TARGET_REF also described an alternative variant of the original BiLM, where words are substituted by their POS tags (Figure 2 .a, shaded part).",
                "Also, however, POS information by itself may be insufficiently expressive to separate cor- , it still is a likely sequence.",
                "Indeed, the log-probabilities of the two sequences with respect to a 4-gram BiLM model 5 result in a higher probability of −10.25 for the incorrect reordering than for the correct one (−10.39).",
                "Since fully lexicalized bilingual tokens suffer from data sparsity and POS-based bilingual tokens are insufficiently expressive, the question is which level of syntactic information strikes the right balance between expressiveness and generality."
            ],
            "label": [
                "BACKGROUND",
                "MOTIVATION"
            ]
        },
        "input": "sent0: By using unnecessarily fine-grained categories we risk running into sparsity issues.\n sent1: #TARGET_REF also described an alternative variant of the original BiLM, where words are substituted by their POS tags (Figure 2 .a, shaded part).\n sent2: Also, however, POS information by itself may be insufficiently expressive to separate cor- , it still is a likely sequence.\n sent3: Indeed, the log-probabilities of the two sequences with respect to a 4-gram BiLM model 5 result in a higher probability of −10.25 for the incorrect reordering than for the correct one (−10.39).\n sent4: Since fully lexicalized bilingual tokens suffer from data sparsity and POS-based bilingual tokens are insufficiently expressive, the question is which level of syntactic information strikes the right balance between expressiveness and generality.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"MOTIVATION\"], \"context\": [\"sent1\", \"sent2\", \"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "In this section, we introduce our model which combines the BiLM from #TARGET_REF with source dependency information.",
                "We further give details on how the proposed models are trained and integrated into a phrase-based decoder."
            ],
            "label": [
                "BACKGROUND",
                "SIMILARITY"
            ]
        },
        "input": "sent0: In this section, we introduce our model which combines the BiLM from #TARGET_REF with source dependency information.\n sent1: We further give details on how the proposed models are trained and integrated into a phrase-based decoder.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"SIMILARITY\"], \"context\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We are interested in how a translation system with an integrated dependency-based BiLM feaand several gale corpora.",
                "ture performs as compared to the standard PB-SMT baseline and, more importantly, to the original BiLM model.",
                "We consider two variants of BiLM discussed by #TARGET_REF : the standard one, Lex•Lex, and the simplest syntactic one, Pos•Pos.",
                "Results for the experiments can be found in Table 2 .",
                "In the discussion below we mostly focus on the experimental results for the large, combined test set MT08+MT09."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: We are interested in how a translation system with an integrated dependency-based BiLM feaand several gale corpora.\n sent1: ture performs as compared to the standard PB-SMT baseline and, more importantly, to the original BiLM model.\n sent2: We consider two variants of BiLM discussed by #TARGET_REF : the standard one, Lex•Lex, and the simplest syntactic one, Pos•Pos.\n sent3: Results for the experiments can be found in Table 2 .\n sent4: In the discussion below we mostly focus on the experimental results for the large, combined test set MT08+MT09.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "In this paper, we have introduced a simple, yet effective way to include syntactic information into phrase-based SMT.",
                "Our method consists of enriching the representation of units of a bilingual language model (BiLM).",
                "We argued that the very limited contextual information used in the original bilingual models #TARGET_REF can capture reorderings only to a limited degree and proposed a method to incorporate information from a source dependency tree in bilingual units.",
                "In a series of translation experiments we performed a thorough comparison between various syntacticallyenriched BiLMs and competing models.",
                "The results demonstrated that adding syntactic information from a source dependency tree to the representations of bilingual tokens in an n-gram model can yield statistically significant improvements over the competing systems."
            ],
            "label": [
                "BACKGROUND",
                "MOTIVATION"
            ]
        },
        "input": "sent0: In this paper, we have introduced a simple, yet effective way to include syntactic information into phrase-based SMT.\n sent1: Our method consists of enriching the representation of units of a bilingual language model (BiLM).\n sent2: We argued that the very limited contextual information used in the original bilingual models #TARGET_REF can capture reorderings only to a limited degree and proposed a method to incorporate information from a source dependency tree in bilingual units.\n sent3: In a series of translation experiments we performed a thorough comparison between various syntacticallyenriched BiLMs and competing models.\n sent4: The results demonstrated that adding syntactic information from a source dependency tree to the representations of bilingual tokens in an n-gram model can yield statistically significant improvements over the competing systems.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"MOTIVATION\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "1 Originally, #REF used this kind of model as the translation model, while more recently it has been used as an additional model in PBSMT systems (#REF) .",
                "We adopt and generalize the approach of #TARGET_REF to investigate several variations of bilingual language models.",
                "Our method consists of labeling elementary translation events (tokens of bilingual LMs) with their different contextual properties.",
                "What kind of contextual information should be incorporated in a reordering model?",
                "Lexical information has been used by #REF but is known to suffer from data sparsity (#REF) ."
            ],
            "label": [
                "USE",
                "EXTENDS"
            ]
        },
        "input": "sent0: 1 Originally, #REF used this kind of model as the translation model, while more recently it has been used as an additional model in PBSMT systems (#REF) .\n sent1: We adopt and generalize the approach of #TARGET_REF to investigate several variations of bilingual language models.\n sent2: Our method consists of labeling elementary translation events (tokens of bilingual LMs) with their different contextual properties.\n sent3: What kind of contextual information should be incorporated in a reordering model?\n sent4: Lexical information has been used by #REF but is known to suffer from data sparsity (#REF) .\n",
        "output": "{\"label\": [\"USE\", \"EXTENDS\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF introduce an alternative method for unambiguous bilingual segmentation where tokens are defined as minimal phrases, called minimal translation units (MTUs).",
                "Figure 1 compares the BiLM and MTU tokenization for a specific example.",
                "Since #TARGET_REF have shown their model to work successfully as an additional feature in combination with commonly used standard phrase-based features, we use their approach as the main point of reference and base our approach on their segmentation method.",
                "In the rest of the text we refer to #REF as the original BiLM.",
                "4 At the same time, we do not see any specific obstacles for combining our work with MTUs."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: #REF introduce an alternative method for unambiguous bilingual segmentation where tokens are defined as minimal phrases, called minimal translation units (MTUs).\n sent1: Figure 1 compares the BiLM and MTU tokenization for a specific example.\n sent2: Since #TARGET_REF have shown their model to work successfully as an additional feature in combination with commonly used standard phrase-based features, we use their approach as the main point of reference and base our approach on their segmentation method.\n sent3: In the rest of the text we refer to #REF as the original BiLM.\n sent4: 4 At the same time, we do not see any specific obstacles for combining our work with MTUs.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "In Figure 1 one can produce a segmentation of (AsEAr Albtrwl, oil prices) into (Albtrwl, oil) and (AsEAr, prices) or leave it as is. If we allow for both segmentations, the learnt probability parameters may be different for the sum of (Albtrwl, oil) and (AsEAr, prices) and for the unsegmented phrase.",
                "#REF introduce an alternative method for unambiguous bilingual segmentation where tokens are defined as minimal phrases, called minimal translation units (MTUs).",
                "Figure 1 compares the BiLM and MTU tokenization for a specific example.",
                "Since #REF have shown their model to work successfully as an additional feature in combination with commonly used standard phrase-based features, we use their approach as the main point of reference and base our approach on their segmentation method.",
                "In the rest of the text we refer to #TARGET_REF as the original BiLM."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: In Figure 1 one can produce a segmentation of (AsEAr Albtrwl, oil prices) into (Albtrwl, oil) and (AsEAr, prices) or leave it as is. If we allow for both segmentations, the learnt probability parameters may be different for the sum of (Albtrwl, oil) and (AsEAr, prices) and for the unsegmented phrase.\n sent1: #REF introduce an alternative method for unambiguous bilingual segmentation where tokens are defined as minimal phrases, called minimal translation units (MTUs).\n sent2: Figure 1 compares the BiLM and MTU tokenization for a specific example.\n sent3: Since #REF have shown their model to work successfully as an additional feature in combination with commonly used standard phrase-based features, we use their approach as the main point of reference and base our approach on their segmentation method.\n sent4: In the rest of the text we refer to #TARGET_REF as the original BiLM.\n",
        "output": "{\"label\": [null], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "We conduct translation experiments with a baseline PBSMT system with additionally one of the dependency-based BiLM feature functions specified in Section 3.",
                "We compare the translation performance to a baseline PBSMT system and to a baseline augmented with the original BiLMs from #TARGET_REF .",
                "Word-alignment is produced with GIZA++ (#REF) .",
                "We use an in-house implementation of a PBSMT system similar to Moses (#REF) .",
                "Our baseline contains all standard PBSMT features including language model, lexical weighting, and lexicalized reordering."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: We conduct translation experiments with a baseline PBSMT system with additionally one of the dependency-based BiLM feature functions specified in Section 3.\n sent1: We compare the translation performance to a baseline PBSMT system and to a baseline augmented with the original BiLMs from #TARGET_REF .\n sent2: Word-alignment is produced with GIZA++ (#REF) .\n sent3: We use an in-house implementation of a PBSMT system similar to Moses (#REF) .\n sent4: Our baseline contains all standard PBSMT features including language model, lexical weighting, and lexicalized reordering.\n",
        "output": "{\"label\": [null], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Situational irony, instead refers to a contradictory or unexpected outcome of events (#REF) .",
                "In Twitter we can find many examples both of verbal irony and of posts where users describe aspects of an ironic situation.",
                "Most of the proposed approaches to the automatic detection of irony in social media (#REF; #REF; Ptáček et al., 2014 )take advantage of lexical factors such as n-grams, punctuation marks, among others.",
                "Information related to affect has been also exploited (#REF; #TARGET_REF; Hernández Farías et al., 2015) .",
                "Other scholars proposed methods exploiting the context surrounding an ironic utterance (#REF; #REF) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Situational irony, instead refers to a contradictory or unexpected outcome of events (#REF) .\n sent1: In Twitter we can find many examples both of verbal irony and of posts where users describe aspects of an ironic situation.\n sent2: Most of the proposed approaches to the automatic detection of irony in social media (#REF; #REF; Ptáček et al., 2014 )take advantage of lexical factors such as n-grams, punctuation marks, among others.\n sent3: Information related to affect has been also exploited (#REF; #TARGET_REF; Hernández Farías et al., 2015) .\n sent4: Other scholars proposed methods exploiting the context surrounding an ironic utterance (#REF; #REF) .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Irony is a very subjective language device that involves the expression of affective contents such as emotions, attitudes, or evaluations towards a particular target.",
                "Attempting to take advantage of the emotionally-laden characteristics of ironic expressions, we relied on emotIDM, an irony detection model that, taking advantage of several affective resources available for English (#REF) , exploits various facets of affective information from sentiment to finer-grained emotions for characterizing the presence of irony in Twitter .",
                "In ) the robustness of emotIDM was assessed over different Twitter state-of-the-art corpora for irony detection (#REF; #TARGET_REF; #REF; Ptáček et al., 2014; #REF) .",
                "The obtained results outperform those in the previous works confirming the significance of affective features for irony detection.",
                "An additional aspect to be mentioned about emotIDM is that it was designed to identify ironic content in a general sense, i.e. considering irony as a broad term covering different types of irony in tweets."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Irony is a very subjective language device that involves the expression of affective contents such as emotions, attitudes, or evaluations towards a particular target.\n sent1: Attempting to take advantage of the emotionally-laden characteristics of ironic expressions, we relied on emotIDM, an irony detection model that, taking advantage of several affective resources available for English (#REF) , exploits various facets of affective information from sentiment to finer-grained emotions for characterizing the presence of irony in Twitter .\n sent2: In ) the robustness of emotIDM was assessed over different Twitter state-of-the-art corpora for irony detection (#REF; #TARGET_REF; #REF; Ptáček et al., 2014; #REF) .\n sent3: The obtained results outperform those in the previous works confirming the significance of affective features for irony detection.\n sent4: An additional aspect to be mentioned about emotIDM is that it was designed to identify ironic content in a general sense, i.e. considering irony as a broad term covering different types of irony in tweets.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "From these tweets, 265 were belonging to the ironic class, while 592 were labeled as non-ironic.",
                "Notice that, in (Hernández-#REF) , the authors found a similar behavior regarding URL information in the dataset provided by the organizers of SentiPOLC-2014 (#REF) .",
                "Furthermore, #TARGET_REF exploited a feature for alerting the existence of an URL in a tweet; such feature was ranked among the most discriminative ones according to an information gain analysis.",
                "Since, information regarding to the presence of URL in a tweet has proven to be useful for detecting irony in Twitter, we decided to enrich emotIDM by adding a binary feature for reflecting the presence of URL in a tweet.",
                "Below, we describe our participation in the task."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: From these tweets, 265 were belonging to the ironic class, while 592 were labeled as non-ironic.\n sent1: Notice that, in (Hernández-#REF) , the authors found a similar behavior regarding URL information in the dataset provided by the organizers of SentiPOLC-2014 (#REF) .\n sent2: Furthermore, #TARGET_REF exploited a feature for alerting the existence of an URL in a tweet; such feature was ranked among the most discriminative ones according to an information gain analysis.\n sent3: Since, information regarding to the presence of URL in a tweet has proven to be useful for detecting irony in Twitter, we decided to enrich emotIDM by adding a binary feature for reflecting the presence of URL in a tweet.\n sent4: Below, we describe our participation in the task.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Distinguishing between different kinds of ironic devices is still a controversial issue.",
                "In computational linguistics, only few research works have attempted to address such a difficult task (#REF; #TARGET_REF; #REF; Van #REF) .",
                "We are interested in assessing the performance of emotIDM when it deals with different types of irony, in order to test if a wide variety of affective features can help in discriminating also in the finer-grained classification task here proposed.",
                "This could give some insights on the role of affective content among ironic devices having different communication purposes.",
                "emotIDM+URL was trained with the dataset provided for Task B (constrained setting) to test the effectiveness of affective features in such finergrained task."
            ],
            "label": [
                "BACKGROUND",
                "MOTIVATION"
            ]
        },
        "input": "sent0: Distinguishing between different kinds of ironic devices is still a controversial issue.\n sent1: In computational linguistics, only few research works have attempted to address such a difficult task (#REF; #TARGET_REF; #REF; Van #REF) .\n sent2: We are interested in assessing the performance of emotIDM when it deals with different types of irony, in order to test if a wide variety of affective features can help in discriminating also in the finer-grained classification task here proposed.\n sent3: This could give some insights on the role of affective content among ironic devices having different communication purposes.\n sent4: emotIDM+URL was trained with the dataset provided for Task B (constrained setting) to test the effectiveness of affective features in such finergrained task.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"MOTIVATION\"], \"context\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We exploited data from a set of corpora collected exploiting different approaches: self-tagging or manual annotation or crowd-sourcing 3 .",
                "We exploited the corpora developed by (#REF) , #TARGET_REF , (#REF) , (Ptáček et al., 2014) , (#REF) , (#REF) , (#REF) , and (#REF) .",
                "Besides, we also take advantage of an in-house collection of tweets containing the hashtags #irony and #sarcasm 4 .",
                "Table 1 shows the obtained results during the developing phase for Task A. We experimented with different sets of features and classifiers considering a five fold-cross validation setting.",
                "SVM emerges as the classifier with the best performance in both C and U scenarios."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We exploited data from a set of corpora collected exploiting different approaches: self-tagging or manual annotation or crowd-sourcing 3 .\n sent1: We exploited the corpora developed by (#REF) , #TARGET_REF , (#REF) , (Ptáček et al., 2014) , (#REF) , (#REF) , (#REF) , and (#REF) .\n sent2: Besides, we also take advantage of an in-house collection of tweets containing the hashtags #irony and #sarcasm 4 .\n sent3: Table 1 shows the obtained results during the developing phase for Task A. We experimented with different sets of features and classifiers considering a five fold-cross validation setting.\n sent4: SVM emerges as the classifier with the best performance in both C and U scenarios.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Cross-lingual systems rely on training data from one language to train a model that can be applied to other languages (de #REF) , alleviating the training bottleneck issues for low-resource languages.",
                "This is facilitated by recent advances in learning joint multilingual representations (#REF; #REF; #REF) .",
                "In our work, we propose a self-learning framework to incorporate the predictions of the multilingual BERT model #TARGET_REF on non-English data into an English training procedure.",
                "The initial multilingual BERT model was simultaneously pretrained on 104 languages, and has shown to perform well for cross-lingual transfer of natural language tasks (#REF) .",
                "Our model begins by learning just from available English samples, but then makes predictions on unlabeled non-English samples and a part of those samples with high confidence prediction scores are repurposed to serve as labeled examples for a next iteration of fine-tuning until the model converges."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: Cross-lingual systems rely on training data from one language to train a model that can be applied to other languages (de #REF) , alleviating the training bottleneck issues for low-resource languages.\n sent1: This is facilitated by recent advances in learning joint multilingual representations (#REF; #REF; #REF) .\n sent2: In our work, we propose a self-learning framework to incorporate the predictions of the multilingual BERT model #TARGET_REF on non-English data into an English training procedure.\n sent3: The initial multilingual BERT model was simultaneously pretrained on 104 languages, and has shown to perform well for cross-lingual transfer of natural language tasks (#REF) .\n sent4: Our model begins by learning just from available English samples, but then makes predictions on unlabeled non-English samples and a part of those samples with high confidence prediction scores are repurposed to serve as labeled examples for a next iteration of fine-tuning until the model converges.\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Both classification tasks are evaluated in terms of classification accuracy (ACC).",
                "Model Details.",
                "We tune the hyper-parameters for our neural network architecture based on each non-English validation set.",
                "For the encoder, we invoke the multilingual BERT model #TARGET_REF , which supports 104 languages 1 .",
                "It relies on a shared 110k WordPiece vocabulary across all languages and yields sentence representations in a common multilingual space."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Both classification tasks are evaluated in terms of classification accuracy (ACC).\n sent1: Model Details.\n sent2: We tune the hyper-parameters for our neural network architecture based on each non-English validation set.\n sent3: For the encoder, we invoke the multilingual BERT model #TARGET_REF , which supports 104 languages 1 .\n sent4: It relies on a shared 110k WordPiece vocabulary across all languages and yields sentence representations in a common multilingual space.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The authors released sentiment polarity dataset, a collection of 2,000 movie reviews categorized as positive or negative.",
                "Deep learning techniques and distributed word representations appeared on recent studies like [17] where the role of RNNs (Recurrent Neural Networks), and CNNs (Convolutional Neural Networks) is explored.",
                "The author reports that CNNs perform best.",
                "An important work that has relevance here is #TARGET_REF where authors present an even larger movie review dataset of 50,000 movie reviews from IMBD.",
                "This dataset has been used in various works such as [5] , [16] etc."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The authors released sentiment polarity dataset, a collection of 2,000 movie reviews categorized as positive or negative.\n sent1: Deep learning techniques and distributed word representations appeared on recent studies like [17] where the role of RNNs (Recurrent Neural Networks), and CNNs (Convolutional Neural Networks) is explored.\n sent2: The author reports that CNNs perform best.\n sent3: An important work that has relevance here is #TARGET_REF where authors present an even larger movie review dataset of 50,000 movie reviews from IMBD.\n sent4: This dataset has been used in various works such as [5] , [16] etc.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "On MR50K it has a discrete margin of more than 0.03 from the 2nd position.",
                "Again wikigiga models are positioned in the middle of the list and the worst performing models are MoodyCorpus and Text8Corpus.",
                "Our scores on this task are somehow lower than those reported from various studies that explore advanced deep learning constructs on same dataset.",
                "In #TARGET_REF for example, authors who created movie review dataset try on it their probabilistic model that is able to capture semantic similarities between words.",
                "They report a maximal accuracy of 0.88."
            ],
            "label": [
                "BACKGROUND",
                "DIFFERENCES"
            ]
        },
        "input": "sent0: On MR50K it has a discrete margin of more than 0.03 from the 2nd position.\n sent1: Again wikigiga models are positioned in the middle of the list and the worst performing models are MoodyCorpus and Text8Corpus.\n sent2: Our scores on this task are somehow lower than those reported from various studies that explore advanced deep learning constructs on same dataset.\n sent3: In #TARGET_REF for example, authors who created movie review dataset try on it their probabilistic model that is able to capture semantic similarities between words.\n sent4: They report a maximal accuracy of 0.88.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"DIFFERENCES\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Learning System KRISP (Kernel-based Robust Interpretation for Semantic Parsing) #TARGET_REF ) is a supervised learning system for semantic parsing which takes NL sentences paired with their MRs as training data.",
                "The productions of the formal MRL grammar are treated like semantic concepts.",
                "For each of these productions, a Support-Vector Machine (SVM) (#REF) classifier is trained using string similarity as the kernel (#REF) .",
                "Each classifier can then estimate the probability of any NL substring representing the semantic concept for its production.",
                "During semantic parsing, the classifiers are called to estimate probabilities on different substrings of the sentence to compositionally build the most probable meaning representation (MR) of the sentence."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Learning System KRISP (Kernel-based Robust Interpretation for Semantic Parsing) #TARGET_REF ) is a supervised learning system for semantic parsing which takes NL sentences paired with their MRs as training data.\n sent1: The productions of the formal MRL grammar are treated like semantic concepts.\n sent2: For each of these productions, a Support-Vector Machine (SVM) (#REF) classifier is trained using string similarity as the kernel (#REF) .\n sent3: Each classifier can then estimate the probability of any NL substring representing the semantic concept for its production.\n sent4: During semantic parsing, the classifiers are called to estimate probabilities on different substrings of the sentence to compositionally build the most probable meaning representation (MR) of the sentence.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "The set of negative examples includes all of the other training sentences.",
                "Using these positive and negative examples, an SVM classifier is trained for each production using a string kernel.",
                "In subsequent iterations, the parser learned from the previous iteration is applied to the training examples and more refined positive and negative examples, which are more specific substrings within the sentences, are collected for training.",
                "Iterations are continued until the classifiers converge, analogous to iterations in EM (#REF) .",
                "Experimentally, KRISP compares favorably to other existing semantic parsing systems and is particularly robust to noisy training data #TARGET_REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The set of negative examples includes all of the other training sentences.\n sent1: Using these positive and negative examples, an SVM classifier is trained for each production using a string kernel.\n sent2: In subsequent iterations, the parser learned from the previous iteration is applied to the training examples and more refined positive and negative examples, which are more specific substrings within the sentences, are collected for training.\n sent3: Iterations are continued until the classifiers converge, analogous to iterations in EM (#REF) .\n sent4: Experimentally, KRISP compares favorably to other existing semantic parsing systems and is particularly robust to noisy training data #TARGET_REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "In contrast, unannotated NL sentences are usually easily available.",
                "Semi-supervised learning methods utilize cheaply available unannotated data during training along with annotated data and often perform better than purely supervised learning methods trained on the same amount of annotated data (#REF) .",
                "In this paper we present, to our knowledge, the first semi-supervised learning system for semantic parsing.",
                "We modify KRISP, a supervised learning system for semantic parsing presented in #TARGET_REF , to make a semi-supervised system we call SEMISUP-KRISP.",
                "Experiments on a realworld dataset show the improvements SEMISUP-KRISP obtains over KRISP by utilizing unannotated sentences."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: In contrast, unannotated NL sentences are usually easily available.\n sent1: Semi-supervised learning methods utilize cheaply available unannotated data during training along with annotated data and often perform better than purely supervised learning methods trained on the same amount of annotated data (#REF) .\n sent2: In this paper we present, to our knowledge, the first semi-supervised learning system for semantic parsing.\n sent3: We modify KRISP, a supervised learning system for semantic parsing presented in #TARGET_REF , to make a semi-supervised system we call SEMISUP-KRISP.\n sent4: Experiments on a realworld dataset show the improvements SEMISUP-KRISP obtains over KRISP by utilizing unannotated sentences.\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The SINNET system is the result of several years of research #TARGET_REF; #REF) .",
                "In , we introduced the notion of social events.",
                "A social event is a happening between two people, at least one of whom is cognizant of the other and of the event taking place.",
                "At a broad level, there are two types of social events: interaction (INR) and observation (OBS).",
                "INR is a bi-directional event in which both parties are mutually aware of each other."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The SINNET system is the result of several years of research #TARGET_REF; #REF) .\n sent1: In , we introduced the notion of social events.\n sent2: A social event is a happening between two people, at least one of whom is cognizant of the other and of the event taking place.\n sent3: At a broad level, there are two types of social events: interaction (INR) and observation (OBS).\n sent4: INR is a bi-directional event in which both parties are mutually aware of each other.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "OBS is a one-directional event in which only one party is aware of the other.",
                "Examples of OBS are thinking about someone, or missing someone.",
                "In , we presented a preliminary system that uses tree kernels and Support Vector Machines (SVMs) to extract social events from news articles.",
                "In #TARGET_REF , we presented a case study on a manually extracted network from Alice in Wonderland, showing that analyzing networks based on these social events gives us insight into the roles of characters in the story.",
                "Also, static network analysis has limitations which become apparent from our analysis."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: OBS is a one-directional event in which only one party is aware of the other.\n sent1: Examples of OBS are thinking about someone, or missing someone.\n sent2: In , we presented a preliminary system that uses tree kernels and Support Vector Machines (SVMs) to extract social events from news articles.\n sent3: In #TARGET_REF , we presented a case study on a manually extracted network from Alice in Wonderland, showing that analyzing networks based on these social events gives us insight into the roles of characters in the story.\n sent4: Also, static network analysis has limitations which become apparent from our analysis.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Figure 2 shows the network extracted from an abridged version of Alice in Wonderland #TARGET_REF .",
                "Figure 3 shows the output of running the Hubs and Authority algorithm (#REF ) on the network.",
                "In information retrieval, an authority is a webpage that many hubs point to and a hub is a webpage that points to many authorities.",
                "In our network, webpages are synonymous to characters.",
                "Figure 3a shows the hubs in decreasing order of hub weights."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Figure 2 shows the network extracted from an abridged version of Alice in Wonderland #TARGET_REF .\n sent1: Figure 3 shows the output of running the Hubs and Authority algorithm (#REF ) on the network.\n sent2: In information retrieval, an authority is a webpage that many hubs point to and a hub is a webpage that points to many authorities.\n sent3: In our network, webpages are synonymous to characters.\n sent4: Figure 3a shows the hubs in decreasing order of hub weights.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Figure 3b shows the authorities in decreasing order of authority weights.",
                "We see that the main character of the story, Alice, is the main authority but not the main hub.",
                "This network may be used for other In #TARGET_REF , we argued that a static network does not bring out the true nature of a network.",
                "For example, even though the centrality of the Mouse in a static network is high, a dynamic network analysis shows that the mouse is central only in one chapter of the novel (Chapter 3 -The drying ceremony).",
                "Figure 4 shows the the network at the end of chapter 1 and chapter 3."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Figure 3b shows the authorities in decreasing order of authority weights.\n sent1: We see that the main character of the story, Alice, is the main authority but not the main hub.\n sent2: This network may be used for other In #TARGET_REF , we argued that a static network does not bring out the true nature of a network.\n sent3: For example, even though the centrality of the Mouse in a static network is high, a dynamic network analysis shows that the mouse is central only in one chapter of the novel (Chapter 3 -The drying ceremony).\n sent4: Figure 4 shows the the network at the end of chapter 1 and chapter 3.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Given the success of statistical parsing models on the Wall Street Journal (WSJ) section of the Penn Treebank (PTB) (#REF; #REF, for example) , there has been a change in focus in recent years towards the problem of replicating this success on genres other than American financial news stories.",
                "The main challenge in solving the parser adaptation problem are the resources required to construct reliable annotated training examples.",
                "A breakthrough has come in the form of research by McClosky et al. (2006a; #TARGET_REF ) who show that self-training can be used to improve parser performance when combined with a two-stage reranking parser model (#REF) .",
                "Selftraining is the process of training a parser on its own output, and earlier self-training experiments using generative statistical parsers did not yield encouraging results (#REF) .",
                "McClosky et al. (2006a; 2006b ) proceed as follows: sentences * Now affiliated to Lalic, Université Paris 4 La Sorbonne."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Given the success of statistical parsing models on the Wall Street Journal (WSJ) section of the Penn Treebank (PTB) (#REF; #REF, for example) , there has been a change in focus in recent years towards the problem of replicating this success on genres other than American financial news stories.\n sent1: The main challenge in solving the parser adaptation problem are the resources required to construct reliable annotated training examples.\n sent2: A breakthrough has come in the form of research by McClosky et al. (2006a; #TARGET_REF ) who show that self-training can be used to improve parser performance when combined with a two-stage reranking parser model (#REF) .\n sent3: Selftraining is the process of training a parser on its own output, and earlier self-training experiments using generative statistical parsers did not yield encouraging results (#REF) .\n sent4: McClosky et al. (2006a; 2006b ) proceed as follows: sentences * Now affiliated to Lalic, Université Paris 4 La Sorbonne.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Selftraining is the process of training a parser on its own output, and earlier self-training experiments using generative statistical parsers did not yield encouraging results (#REF) .",
                "McClosky et al. (2006a; #TARGET_REF ) proceed as follows: sentences * Now affiliated to Lalic, Université Paris 4 La Sorbonne.",
                "from the LA Times newspaper are parsed by a firststage generative statistical parser trained on some seed training data (WSJ Sections 2-21) and the nbest parse trees produced by this parser are reranked by a discriminative reranker.",
                "The highest ranked parse trees are added to the training set of the parser and the parser is retrained.",
                "This self-training method gives improved performance, not only on Section 23 of the WSJ (an absolute f-score improvement of 0.8%), but also on test sentences from the Brown corpus (Francis and Kučera, 1979 ) (an absolute fscore improvement of 2.6%)."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Selftraining is the process of training a parser on its own output, and earlier self-training experiments using generative statistical parsers did not yield encouraging results (#REF) .\n sent1: McClosky et al. (2006a; #TARGET_REF ) proceed as follows: sentences * Now affiliated to Lalic, Université Paris 4 La Sorbonne.\n sent2: from the LA Times newspaper are parsed by a firststage generative statistical parser trained on some seed training data (WSJ Sections 2-21) and the nbest parse trees produced by this parser are reranked by a discriminative reranker.\n sent3: The highest ranked parse trees are added to the training set of the parser and the parser is retrained.\n sent4: This self-training method gives improved performance, not only on Section 23 of the WSJ (an absolute f-score improvement of 0.8%), but also on test sentences from the Brown corpus (Francis and Kučera, 1979 ) (an absolute fscore improvement of 2.6%).\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The highest ranked parse trees are added to the training set of the parser and the parser is retrained.",
                "This self-training method gives improved performance, not only on Section 23 of the WSJ (an absolute f-score improvement of 0.8%), but also on test sentences from the Brown corpus (Francis and Kučera, 1979 ) (an absolute fscore improvement of 2.6%).",
                "In the experiments of McClosky et al. (2006a; #TARGET_REF , the parse trees used for self-training come from the same domain (American newspaper text) as the parser's original seed training material.",
                "#REF find that self-training is effective when the parse trees used for self-training (WSJ parse trees) come from a different domain to the seed training data and from the same domain as the test data (WSJ sentences).",
                "They report a performance boost of 4.2% on WSJ Section 23 for a generative statistical parser trained on Brown seed data when it is self-trained using 200,000 WSJ parse trees."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The highest ranked parse trees are added to the training set of the parser and the parser is retrained.\n sent1: This self-training method gives improved performance, not only on Section 23 of the WSJ (an absolute f-score improvement of 0.8%), but also on test sentences from the Brown corpus (Francis and Kučera, 1979 ) (an absolute fscore improvement of 2.6%).\n sent2: In the experiments of McClosky et al. (2006a; #TARGET_REF , the parse trees used for self-training come from the same domain (American newspaper text) as the parser's original seed training material.\n sent3: #REF find that self-training is effective when the parse trees used for self-training (WSJ parse trees) come from a different domain to the seed training data and from the same domain as the test data (WSJ sentences).\n sent4: They report a performance boost of 4.2% on WSJ Section 23 for a generative statistical parser trained on Brown seed data when it is self-trained using 200,000 WSJ parse trees.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "This self-training method gives improved performance, not only on Section 23 of the WSJ (an absolute f-score improvement of 0.8%), but also on test sentences from the Brown corpus (Francis and Kučera, 1979 ) (an absolute fscore improvement of 2.6%).",
                "In the experiments of McClosky et al. (2006a; 2006b) , the parse trees used for self-training come from the same domain (American newspaper text) as the parser's original seed training material.",
                "#REF find that self-training is effective when the parse trees used for self-training (WSJ parse trees) come from a different domain to the seed training data and from the same domain as the test data (WSJ sentences).",
                "They report a performance boost of 4.2% on WSJ Section 23 for a generative statistical parser trained on Brown seed data when it is self-trained using 200,000 WSJ parse trees.",
                "However, McCloskey et al. ( #TARGET_REF report a drop in performance for their reranking parser when the experiment is repeated in the opposite direction, i.e. with Brown data for self-training and testing, and WSJ data for seed training."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This self-training method gives improved performance, not only on Section 23 of the WSJ (an absolute f-score improvement of 0.8%), but also on test sentences from the Brown corpus (Francis and Kučera, 1979 ) (an absolute fscore improvement of 2.6%).\n sent1: In the experiments of McClosky et al. (2006a; 2006b) , the parse trees used for self-training come from the same domain (American newspaper text) as the parser's original seed training material.\n sent2: #REF find that self-training is effective when the parse trees used for self-training (WSJ parse trees) come from a different domain to the seed training data and from the same domain as the test data (WSJ sentences).\n sent3: They report a performance boost of 4.2% on WSJ Section 23 for a generative statistical parser trained on Brown seed data when it is self-trained using 200,000 WSJ parse trees.\n sent4: However, McCloskey et al. ( #TARGET_REF report a drop in performance for their reranking parser when the experiment is repeated in the opposite direction, i.e. with Brown data for self-training and testing, and WSJ data for seed training.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF find that self-training is effective when the parse trees used for self-training (WSJ parse trees) come from a different domain to the seed training data and from the same domain as the test data (WSJ sentences).",
                "They report a performance boost of 4.2% on WSJ Section 23 for a generative statistical parser trained on Brown seed data when it is self-trained using 200,000 WSJ parse trees.",
                "However, McCloskey et al. (2006b) report a drop in performance for their reranking parser when the experiment is repeated in the opposite direction, i.e. with Brown data for self-training and testing, and WSJ data for seed training.",
                "In contrast, we report successful in-domain 1 self-training experiments with the BNC data as self-training and test material, and with the WSJ-trained reranking parser used by McCloskey et al. (2006a; #TARGET_REF .",
                "We parse the BNC (#REF) in its entirety using the reranking parser of #REF ."
            ],
            "label": [
                "USE",
                "DIFFERENCES"
            ]
        },
        "input": "sent0: #REF find that self-training is effective when the parse trees used for self-training (WSJ parse trees) come from a different domain to the seed training data and from the same domain as the test data (WSJ sentences).\n sent1: They report a performance boost of 4.2% on WSJ Section 23 for a generative statistical parser trained on Brown seed data when it is self-trained using 200,000 WSJ parse trees.\n sent2: However, McCloskey et al. (2006b) report a drop in performance for their reranking parser when the experiment is repeated in the opposite direction, i.e. with Brown data for self-training and testing, and WSJ data for seed training.\n sent3: In contrast, we report successful in-domain 1 self-training experiments with the BNC data as self-training and test material, and with the WSJ-trained reranking parser used by McCloskey et al. (2006a; #TARGET_REF .\n sent4: We parse the BNC (#REF) in its entirety using the reranking parser of #REF .\n",
        "output": "{\"label\": [\"USE\", \"DIFFERENCES\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Charniak and Johnson's reranking parser (#REF version) is evaluated against the BNC gold standard development set.",
                "Labelled precision (LP), recall (LR) and f-score measures 2 for this parser are shown in the first row of Table 1 .",
                "The f-score of 83.7% is lower than the f-score of 85.2% reported by #TARGET_REF for the same parser on Brown corpus data.",
                "This difference is reasonable since there is greater domain variation between the WSJ and the BNC than between the WSJ and the Brown corpus, and all BNC gold standard sentences contain verbs not attested in WSJ Sections 2-21.",
                "We retrain the first-stage generative statistical parser of Charniak and Johnson using combinations of BNC trees (parsed using the reranking parser) and WSJ treebank trees."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Charniak and Johnson's reranking parser (#REF version) is evaluated against the BNC gold standard development set.\n sent1: Labelled precision (LP), recall (LR) and f-score measures 2 for this parser are shown in the first row of Table 1 .\n sent2: The f-score of 83.7% is lower than the f-score of 85.2% reported by #TARGET_REF for the same parser on Brown corpus data.\n sent3: This difference is reasonable since there is greater domain variation between the WSJ and the BNC than between the WSJ and the Brown corpus, and all BNC gold standard sentences contain verbs not attested in WSJ Sections 2-21.\n sent4: We retrain the first-stage generative statistical parser of Charniak and Johnson using combinations of BNC trees (parsed using the reranking parser) and WSJ treebank trees.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "These sentiments can be used to provided targeted advertising, automatically generate reviews, and make various predictions, such as political outcomes.",
                "In this paper we develop a sentiment detection algorithm for social media that classifies the polarity of sentence phrases as positive, negative, or neutral and test its performance in Twitter through the participation in the expression level task (subtask A) of the #REF Task 2: Sentiment Analysis in Twitter (#REF) which the authors helped organize.",
                "To do so, we build on previous work on sentiment detection algorithms for the more formal news genre, notably the work of #TARGET_REF , but adapt it for the language of social media, in particular Twitter.",
                "We show that exploiting lexical-stylistic features and dictionaries geared toward social media are useful in detecting sentiment.",
                "In this rest of this paper, we discuss related work, including the state of the art sentiment system (#REF) our method is based on, the lexicons we used, our method, and experiments and results."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: These sentiments can be used to provided targeted advertising, automatically generate reviews, and make various predictions, such as political outcomes.\n sent1: In this paper we develop a sentiment detection algorithm for social media that classifies the polarity of sentence phrases as positive, negative, or neutral and test its performance in Twitter through the participation in the expression level task (subtask A) of the #REF Task 2: Sentiment Analysis in Twitter (#REF) which the authors helped organize.\n sent2: To do so, we build on previous work on sentiment detection algorithms for the more formal news genre, notably the work of #TARGET_REF , but adapt it for the language of social media, in particular Twitter.\n sent3: We show that exploiting lexical-stylistic features and dictionaries geared toward social media are useful in detecting sentiment.\n sent4: In this rest of this paper, we discuss related work, including the state of the art sentiment system (#REF) our method is based on, the lexicons we used, our method, and experiments and results.\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "These sentiments can be used to provided targeted advertising, automatically generate reviews, and make various predictions, such as political outcomes.",
                "In this paper we develop a sentiment detection algorithm for social media that classifies the polarity of sentence phrases as positive, negative, or neutral and test its performance in Twitter through the participation in the expression level task (subtask A) of the #REF Task 2: Sentiment Analysis in Twitter (#REF) which the authors helped organize.",
                "To do so, we build on previous work on sentiment detection algorithms for the more formal news genre, notably the work of Agarwal et al (2009) , but adapt it for the language of social media, in particular Twitter.",
                "We show that exploiting lexical-stylistic features and dictionaries geared toward social media are useful in detecting sentiment.",
                "In this rest of this paper, we discuss related work, including the state of the art sentiment system #TARGET_REF our method is based on, the lexicons we used, our method, and experiments and results."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: These sentiments can be used to provided targeted advertising, automatically generate reviews, and make various predictions, such as political outcomes.\n sent1: In this paper we develop a sentiment detection algorithm for social media that classifies the polarity of sentence phrases as positive, negative, or neutral and test its performance in Twitter through the participation in the expression level task (subtask A) of the #REF Task 2: Sentiment Analysis in Twitter (#REF) which the authors helped organize.\n sent2: To do so, we build on previous work on sentiment detection algorithms for the more formal news genre, notably the work of Agarwal et al (2009) , but adapt it for the language of social media, in particular Twitter.\n sent3: We show that exploiting lexical-stylistic features and dictionaries geared toward social media are useful in detecting sentiment.\n sent4: In this rest of this paper, we discuss related work, including the state of the art sentiment system #TARGET_REF our method is based on, the lexicons we used, our method, and experiments and results.\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Several lexicons are used in our system.",
                "We use the DAL and expand it with WordNet, as it was used in the original work #TARGET_REF , and expand it further to use Wiktionary and an emoticon lexicon.",
                "We consider proper nouns that are not in the DAL to be objective.",
                "We also shorten words that are lengthened to see if we can find the shortened version in the lexicons (e.g. sweeeet → sweet).",
                "The coverage of the lexicons for each corpus is shown in Table 1 ."
            ],
            "label": [
                "EXTENDS",
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: Several lexicons are used in our system.\n sent1: We use the DAL and expand it with WordNet, as it was used in the original work #TARGET_REF , and expand it further to use Wiktionary and an emoticon lexicon.\n sent2: We consider proper nouns that are not in the DAL to be objective.\n sent3: We also shorten words that are lengthened to see if we can find the shortened version in the lexicons (e.g. sweeeet → sweet).\n sent4: The coverage of the lexicons for each corpus is shown in Table 1 .\n",
        "output": "{\"label\": [\"EXTENDS\", \"SIMILARITY\", \"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The Dictionary of Affect and Language (DAL) (#REF ) is an English language dictionary of 8742 words built to measure the emotional meaning of texts.",
                "In addition to using newswire, it was also built from individual sources such as interviews on abuse, students' retelling of a story, and adolescent's descriptions of emotions.",
                "It therefore covers a broad set of words.",
                "Each word is given three scores (pleasantness -also called evaluation (ee), activeness (aa), and imagery (ii)) on a scale of 1 (low) to 3 (high).",
                "We compute the polarity of a chunk in the same manner as the original work #TARGET_REF , using the sum of the AE Space Score's (| √ ee 2 + aa 2 |) of each word within the chunk."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: The Dictionary of Affect and Language (DAL) (#REF ) is an English language dictionary of 8742 words built to measure the emotional meaning of texts.\n sent1: In addition to using newswire, it was also built from individual sources such as interviews on abuse, students' retelling of a story, and adolescent's descriptions of emotions.\n sent2: It therefore covers a broad set of words.\n sent3: Each word is given three scores (pleasantness -also called evaluation (ee), activeness (aa), and imagery (ii)) on a scale of 1 (low) to 3 (high).\n sent4: We compute the polarity of a chunk in the same manner as the original work #TARGET_REF , using the sum of the AE Space Score's (| √ ee 2 + aa 2 |) of each word within the chunk.\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "We include POS tags and the top 500 n-gram features (#REF) .",
                "We experimented with different amounts of n-grams and found that more than 500 n-grams reduced performance.",
                "The DAL and other dictionaries are used along with a negation state machine (#REF) to determine the polarity for each word in the sentence.",
                "We include all the features described in the original system #TARGET_REF )."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: We include POS tags and the top 500 n-gram features (#REF) .\n sent1: We experimented with different amounts of n-grams and found that more than 500 n-grams reduced performance.\n sent2: The DAL and other dictionaries are used along with a negation state machine (#REF) to determine the polarity for each word in the sentence.\n sent3: We include all the features described in the original system #TARGET_REF ).\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "We include POS tags and the top 500 n-gram features #TARGET_REF .",
                "We experimented with different amounts of n-grams and found that more than 500 n-grams reduced performance.",
                "The DAL and other dictionaries are used along with a negation state machine (#REF) to determine the polarity for each word in the sentence.",
                "We include all the features described in the original system (#REF )."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We include POS tags and the top 500 n-gram features #TARGET_REF .\n sent1: We experimented with different amounts of n-grams and found that more than 500 n-grams reduced performance.\n sent2: The DAL and other dictionaries are used along with a negation state machine (#REF) to determine the polarity for each word in the sentence.\n sent3: We include all the features described in the original system (#REF ).\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "We include POS tags and the top 500 n-gram features (#REF) .",
                "We experimented with different amounts of n-grams and found that more than 500 n-grams reduced performance.",
                "The DAL and other dictionaries are used along with a negation state machine #TARGET_REF to determine the polarity for each word in the sentence.",
                "We include all the features described in the original system (#REF )."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We include POS tags and the top 500 n-gram features (#REF) .\n sent1: We experimented with different amounts of n-grams and found that more than 500 n-grams reduced performance.\n sent2: The DAL and other dictionaries are used along with a negation state machine #TARGET_REF to determine the polarity for each word in the sentence.\n sent3: We include all the features described in the original system (#REF ).\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Although several word association datasets exist, such as the Edinburgh Associative Thesaurus (EAT, #REF) , the University of South Florida Free Association Norms (USF, #REF ), or WordNet Evocation (Evocation, #REF , their reliance on human annotations mean they all suffer from coverage issues relating to limited vocabularies or sparse connectivity (#REF; De #REFb) .",
                "Although these issues would be somewhat alleviated by the creation of larger datasets, collecting human judgments for all possible word pairs is impractical.",
                "Therefore, the ability to predict association strengths between arbitrary word pairs represents the best solution to these coverage issues (#REF) .",
                "Although the prediction of Evocation ratings has attracted some attention (#REF; #TARGET_REF) , to the best of our knowledge this is the first work to focus on the prediction of USF or EAT strengths.",
                "As described in Sec-tion 2, USF and EAT have several advantages over Evocation, such as the ability to work with ambiguous words instead of WordNet synsets."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Although several word association datasets exist, such as the Edinburgh Associative Thesaurus (EAT, #REF) , the University of South Florida Free Association Norms (USF, #REF ), or WordNet Evocation (Evocation, #REF , their reliance on human annotations mean they all suffer from coverage issues relating to limited vocabularies or sparse connectivity (#REF; De #REFb) .\n sent1: Although these issues would be somewhat alleviated by the creation of larger datasets, collecting human judgments for all possible word pairs is impractical.\n sent2: Therefore, the ability to predict association strengths between arbitrary word pairs represents the best solution to these coverage issues (#REF) .\n sent3: Although the prediction of Evocation ratings has attracted some attention (#REF; #TARGET_REF) , to the best of our knowledge this is the first work to focus on the prediction of USF or EAT strengths.\n sent4: As described in Sec-tion 2, USF and EAT have several advantages over Evocation, such as the ability to work with ambiguous words instead of WordNet synsets.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "However, this approach is somewhat limited in that it frames Evocation prediction as a classification task, considering only five Evocation levels.",
                "The main drawback of Evocation prediction as a classification task is that it is too coarse-grained to deal with very weak associations, such as those in remote triads (De #REFa) , or very slight variations in association strength, such as those useful for computational humour (#REF) .",
                "To this end, #TARGET_REF framed Evocation prediction as a supervised regression task.",
                "They employed a combination of WordNet structure-based features, word embedding-based features, and lexical features and found that vector offsets, i.e. the mathematical difference between vectors, were a strong indicator of Evocation ratings.",
                "While Evocation's use of unambiguous synsets is useful for many applications, it is not without its own drawbacks."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: However, this approach is somewhat limited in that it frames Evocation prediction as a classification task, considering only five Evocation levels.\n sent1: The main drawback of Evocation prediction as a classification task is that it is too coarse-grained to deal with very weak associations, such as those in remote triads (De #REFa) , or very slight variations in association strength, such as those useful for computational humour (#REF) .\n sent2: To this end, #TARGET_REF framed Evocation prediction as a supervised regression task.\n sent3: They employed a combination of WordNet structure-based features, word embedding-based features, and lexical features and found that vector offsets, i.e. the mathematical difference between vectors, were a strong indicator of Evocation ratings.\n sent4: While Evocation's use of unambiguous synsets is useful for many applications, it is not without its own drawbacks.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "To this end, #REF framed Evocation prediction as a supervised regression task.",
                "#TARGET_REF employed a combination of WordNet structure-based features, word embedding-based features, and lexical features and found that vector offsets, i.e. the mathematical difference between vectors, were a strong indicator of Evocation ratings.",
                "While Evocation's use of unambiguous synsets is useful for many applications, it is not without its own drawbacks.",
                "First, it requires texts to be word sense disambiguated; a non-trivial task.",
                "Second, since humans do not conceptualize words as a discrete set of independent word senses, Evocation is unable to capture natural associations owing to homography, homophony, or polysemy (#REF)."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: To this end, #REF framed Evocation prediction as a supervised regression task.\n sent1: #TARGET_REF employed a combination of WordNet structure-based features, word embedding-based features, and lexical features and found that vector offsets, i.e. the mathematical difference between vectors, were a strong indicator of Evocation ratings.\n sent2: While Evocation's use of unambiguous synsets is useful for many applications, it is not without its own drawbacks.\n sent3: First, it requires texts to be word sense disambiguated; a non-trivial task.\n sent4: Second, since humans do not conceptualize words as a discrete set of independent word senses, Evocation is unable to capture natural associations owing to homography, homophony, or polysemy (#REF).\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "First, we modify #REF's lexVector.",
                "#TARGET_REF represent each word's part-ofspeech (POS) using a one-hot encoded five dimensional vector (one of each POS in WordNet).",
                "Similarly, they represent each word's lexical category using a one-hot encoded 45 dimensional vector (one for each WordNet lexicographer file).",
                "This results in a 100 dimensional vector representing the POS and lexical categories of both the cue and the response.",
                "Since words in USF and EAT can be associated with multiple synsets and we want to be able to capture associations related to polysemy, instead using a one-hot encoding we employ count vectors specifying the number of synsets from each POS/lexical category each word belongs to."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: First, we modify #REF's lexVector.\n sent1: #TARGET_REF represent each word's part-ofspeech (POS) using a one-hot encoded five dimensional vector (one of each POS in WordNet).\n sent2: Similarly, they represent each word's lexical category using a one-hot encoded 45 dimensional vector (one for each WordNet lexicographer file).\n sent3: This results in a 100 dimensional vector representing the POS and lexical categories of both the cue and the response.\n sent4: Since words in USF and EAT can be associated with multiple synsets and we want to be able to capture associations related to polysemy, instead using a one-hot encoding we employ count vectors specifying the number of synsets from each POS/lexical category each word belongs to.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Our word association prediction system extends the method in #REF with several modifications to make it better suited to the USF and EAT datasets.",
                "First, we modify #REF's lexVector.",
                "#REF represent each word's part-ofspeech (POS) using a one-hot encoded five dimensional vector (one of each POS in WordNet).",
                "Similarly, #TARGET_REF represent each word's lexical category using a one-hot encoded 45 dimensional vector (one for each WordNet lexicographer file).",
                "This results in a 100 dimensional vector representing the POS and lexical categories of both the cue and the response."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Our word association prediction system extends the method in #REF with several modifications to make it better suited to the USF and EAT datasets.\n sent1: First, we modify #REF's lexVector.\n sent2: #REF represent each word's part-ofspeech (POS) using a one-hot encoded five dimensional vector (one of each POS in WordNet).\n sent3: Similarly, #TARGET_REF represent each word's lexical category using a one-hot encoded 45 dimensional vector (one for each WordNet lexicographer file).\n sent4: This results in a 100 dimensional vector representing the POS and lexical categories of both the cue and the response.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Following #REF and #REF, we also explored the use of path and Leacock-Chodorow (#REF) similarities but found they did not add any advantage over WUP alone.",
                "We take a similar approach for adapting load and betweenness centralities (#REF) as well as AutoExtend (AutoEx, Rothe and Schütze, 2015) similarity.",
                "Third, we extend the notion of dirRel, introduced in #REF to leverage the semantic network structure of WordNet.",
                "Given a graph where nodes represent synsets and arcs represent WordNet relations such as hypernym/hyponym and holonym/meronym, dirRel(s,t,k) is the proportion of k-step neighbours of s that are also k-step neighbours of t. #TARGET_REF, s and t are nodes representing a single synset.",
                "We instead consider a set of nodes S and a set of nodes T representing the set of synsets associated with the cue and response words, respectively, as shown in Equation 1."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Following #REF and #REF, we also explored the use of path and Leacock-Chodorow (#REF) similarities but found they did not add any advantage over WUP alone.\n sent1: We take a similar approach for adapting load and betweenness centralities (#REF) as well as AutoExtend (AutoEx, Rothe and Schütze, 2015) similarity.\n sent2: Third, we extend the notion of dirRel, introduced in #REF to leverage the semantic network structure of WordNet.\n sent3: Given a graph where nodes represent synsets and arcs represent WordNet relations such as hypernym/hyponym and holonym/meronym, dirRel(s,t,k) is the proportion of k-step neighbours of s that are also k-step neighbours of t. #TARGET_REF, s and t are nodes representing a single synset.\n sent4: We instead consider a set of nodes S and a set of nodes T representing the set of synsets associated with the cue and response words, respectively, as shown in Equation 1.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Although these issues would be somewhat alleviated by the creation of larger datasets, collecting human judgments for all possible word pairs is impractical.",
                "Therefore, the ability to predict association strengths between arbitrary word pairs represents the best solution to these coverage issues (#REF) .",
                "Although the prediction of Evocation ratings has attracted some attention (#REF; #REF) , to the best of our knowledge this is the first work to focus on the prediction of USF or EAT strengths.",
                "As described in Sec-tion 2, USF and EAT have several advantages over Evocation, such as the ability to work with ambiguous words instead of WordNet synsets.",
                "Following #TARGET_REF on Evocation prediction, we frame word association prediction as a supervised regression task and introduce several new and modified features, including the first use of Gaussian embeddings (#REF) to better capture the asymmetric nature of word associations."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: Although these issues would be somewhat alleviated by the creation of larger datasets, collecting human judgments for all possible word pairs is impractical.\n sent1: Therefore, the ability to predict association strengths between arbitrary word pairs represents the best solution to these coverage issues (#REF) .\n sent2: Although the prediction of Evocation ratings has attracted some attention (#REF; #REF) , to the best of our knowledge this is the first work to focus on the prediction of USF or EAT strengths.\n sent3: As described in Sec-tion 2, USF and EAT have several advantages over Evocation, such as the ability to work with ambiguous words instead of WordNet synsets.\n sent4: Following #TARGET_REF on Evocation prediction, we frame word association prediction as a supervised regression task and introduce several new and modified features, including the first use of Gaussian embeddings (#REF) to better capture the asymmetric nature of word associations.\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Our word association prediction system extends the method in #TARGET_REF with several modifications to make it better suited to the USF and EAT datasets.",
                "First, we modify #REF's lexVector.",
                "#REF represent each word's part-ofspeech (POS) using a one-hot encoded five dimensional vector (one of each POS in WordNet).",
                "Similarly, they represent each word's lexical category using a one-hot encoded 45 dimensional vector (one for each WordNet lexicographer file).",
                "This results in a 100 dimensional vector representing the POS and lexical categories of both the cue and the response."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: Our word association prediction system extends the method in #TARGET_REF with several modifications to make it better suited to the USF and EAT datasets.\n sent1: First, we modify #REF's lexVector.\n sent2: #REF represent each word's part-ofspeech (POS) using a one-hot encoded five dimensional vector (one of each POS in WordNet).\n sent3: Similarly, they represent each word's lexical category using a one-hot encoded 45 dimensional vector (one for each WordNet lexicographer file).\n sent4: This results in a 100 dimensional vector representing the POS and lexical categories of both the cue and the response.\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Our word association prediction system extends the method in #REF with several modifications to make it better suited to the USF and EAT datasets.",
                "First, we modify #TARGET_REF lexVector.",
                "#REF represent each word's part-ofspeech (POS) using a one-hot encoded five dimensional vector (one of each POS in WordNet).",
                "Similarly, they represent each word's lexical category using a one-hot encoded 45 dimensional vector (one for each WordNet lexicographer file).",
                "This results in a 100 dimensional vector representing the POS and lexical categories of both the cue and the response."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: Our word association prediction system extends the method in #REF with several modifications to make it better suited to the USF and EAT datasets.\n sent1: First, we modify #TARGET_REF lexVector.\n sent2: #REF represent each word's part-ofspeech (POS) using a one-hot encoded five dimensional vector (one of each POS in WordNet).\n sent3: Similarly, they represent each word's lexical category using a one-hot encoded 45 dimensional vector (one for each WordNet lexicographer file).\n sent4: This results in a 100 dimensional vector representing the POS and lexical categories of both the cue and the response.\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Second, instead of computing Wu-Palmer similarity (WUP, #REF ) between a single synset pair, we compute it for all cue synset/response synset pairs and record the maximum and average values.",
                "Following #REF and #REF, we also explored the use of path and Leacock-Chodorow (#REF) similarities but found they did not add any advantage over WUP alone.",
                "We take a similar approach for adapting load and betweenness centralities (#REF) as well as AutoExtend (AutoEx, Rothe and Schütze, 2015) similarity.",
                "Third, we extend the notion of dirRel, introduced in #TARGET_REF to leverage the semantic network structure of WordNet.",
                "Given a graph where nodes represent synsets and arcs represent WordNet relations such as hypernym/hyponym and holonym/meronym, dirRel(s,t,k) is the proportion of k-step neighbours of s that are also k-step neighbours of t. In the original formula, s and t are nodes representing a single synset."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: Second, instead of computing Wu-Palmer similarity (WUP, #REF ) between a single synset pair, we compute it for all cue synset/response synset pairs and record the maximum and average values.\n sent1: Following #REF and #REF, we also explored the use of path and Leacock-Chodorow (#REF) similarities but found they did not add any advantage over WUP alone.\n sent2: We take a similar approach for adapting load and betweenness centralities (#REF) as well as AutoExtend (AutoEx, Rothe and Schütze, 2015) similarity.\n sent3: Third, we extend the notion of dirRel, introduced in #TARGET_REF to leverage the semantic network structure of WordNet.\n sent4: Given a graph where nodes represent synsets and arcs represent WordNet relations such as hypernym/hyponym and holonym/meronym, dirRel(s,t,k) is the proportion of k-step neighbours of s that are also k-step neighbours of t. In the original formula, s and t are nodes representing a single synset.\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "This results in a 100 dimensional vector representing the POS and lexical categories of both the cue and the response.",
                "Since words in USF and EAT can be associated with multiple synsets and we want to be able to capture associations related to polysemy, #TARGET_REF we employ count vectors specifying the number of synsets from each POS/lexical category each word belongs to.",
                "Second, instead of computing Wu-Palmer similarity (WUP, #REF ) between a single synset pair, we compute it for all cue synset/response synset pairs and record the maximum and average values.",
                "Following #REF and #REF, we also explored the use of path and Leacock-Chodorow (#REF) similarities but found they did not add any advantage over WUP alone.",
                "We take a similar approach for adapting load and betweenness centralities (#REF) as well as AutoExtend (AutoEx, Rothe and Schütze, 2015) similarity."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: This results in a 100 dimensional vector representing the POS and lexical categories of both the cue and the response.\n sent1: Since words in USF and EAT can be associated with multiple synsets and we want to be able to capture associations related to polysemy, #TARGET_REF we employ count vectors specifying the number of synsets from each POS/lexical category each word belongs to.\n sent2: Second, instead of computing Wu-Palmer similarity (WUP, #REF ) between a single synset pair, we compute it for all cue synset/response synset pairs and record the maximum and average values.\n sent3: Following #REF and #REF, we also explored the use of path and Leacock-Chodorow (#REF) similarities but found they did not add any advantage over WUP alone.\n sent4: We take a similar approach for adapting load and betweenness centralities (#REF) as well as AutoExtend (AutoEx, Rothe and Schütze, 2015) similarity.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Similarly, they represent each word's lexical category using a one-hot encoded 45 dimensional vector (one for each WordNet lexicographer file).",
                "This results in a 100 dimensional vector representing the POS and lexical categories of both the cue and the response.",
                "Since words in USF and EAT can be associated with multiple synsets and we want to be able to capture associations related to polysemy, instead using a one-hot encoding we employ count vectors specifying the number of synsets from each POS/lexical category each word belongs to.",
                "Second, #TARGET_REF (WUP, #REF ) between a single synset pair, we compute it for all cue synset/response synset pairs and record the maximum and average values.",
                "Following #REF and #REF, we also explored the use of path and Leacock-Chodorow (#REF) similarities but found they did not add any advantage over WUP alone."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Similarly, they represent each word's lexical category using a one-hot encoded 45 dimensional vector (one for each WordNet lexicographer file).\n sent1: This results in a 100 dimensional vector representing the POS and lexical categories of both the cue and the response.\n sent2: Since words in USF and EAT can be associated with multiple synsets and we want to be able to capture associations related to polysemy, instead using a one-hot encoding we employ count vectors specifying the number of synsets from each POS/lexical category each word belongs to.\n sent3: Second, #TARGET_REF (WUP, #REF ) between a single synset pair, we compute it for all cue synset/response synset pairs and record the maximum and average values.\n sent4: Following #REF and #REF, we also explored the use of path and Leacock-Chodorow (#REF) similarities but found they did not add any advantage over WUP alone.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Third, we extend the notion of dirRel, introduced in #REF to leverage the semantic network structure of WordNet.",
                "Given a graph where nodes represent synsets and arcs represent WordNet relations such as hypernym/hyponym and holonym/meronym, dirRel(s,t,k) is the proportion of k-step neighbours of s that are also k-step neighbours of t. In the original formula, s and t are nodes representing a single synset.",
                "We #TARGET_REF a set of nodes S and a set of nodes T representing the set of synsets associated with the cue and response words, respectively, as shown in Equation 1.",
                "This may increase the probability that |nb(S, k)∩nb(T, k)| > 0, a shortcoming of the original dirRel due to WordNet's \"relatively sparse connective structure\" (#REF) .",
                "Fourth, in addition to the Word2Vec (w2v) cosine similarity between cue/response pairs calculated using Google's pre-trained 300 dimension Word2Vec embeddings 2 ."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Third, we extend the notion of dirRel, introduced in #REF to leverage the semantic network structure of WordNet.\n sent1: Given a graph where nodes represent synsets and arcs represent WordNet relations such as hypernym/hyponym and holonym/meronym, dirRel(s,t,k) is the proportion of k-step neighbours of s that are also k-step neighbours of t. In the original formula, s and t are nodes representing a single synset.\n sent2: We #TARGET_REF a set of nodes S and a set of nodes T representing the set of synsets associated with the cue and response words, respectively, as shown in Equation 1.\n sent3: This may increase the probability that |nb(S, k)∩nb(T, k)| > 0, a shortcoming of the original dirRel due to WordNet's \"relatively sparse connective structure\" (#REF) .\n sent4: Fourth, in addition to the Word2Vec (w2v) cosine similarity between cue/response pairs calculated using Google's pre-trained 300 dimension Word2Vec embeddings 2 .\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Given a graph where nodes represent synsets and arcs represent WordNet relations such as hypernym/hyponym and holonym/meronym, dirRel(s,t,k) is the proportion of k-step neighbours of s that are also k-step neighbours of t. In the original formula, s and t are nodes representing a single synset.",
                "We instead consider a set of nodes S and a set of nodes T representing the set of synsets associated with the cue and response words, respectively, as shown in Equation 1.",
                "This may increase the probability that |nb(S, k)∩nb(T, k)| > 0, a #TARGET_REF due to WordNet's \"relatively sparse connective structure\" ( #TARGET_REF) .",
                "Fourth, in addition to the Word2Vec (w2v) cosine similarity between cue/response pairs calculated using Google's pre-trained 300 dimension Word2Vec embeddings 2 .",
                "We also examine the effectiveness of Stanford's pre-trained 300 dimension GloVe embeddings 3 ."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Given a graph where nodes represent synsets and arcs represent WordNet relations such as hypernym/hyponym and holonym/meronym, dirRel(s,t,k) is the proportion of k-step neighbours of s that are also k-step neighbours of t. In the original formula, s and t are nodes representing a single synset.\n sent1: We instead consider a set of nodes S and a set of nodes T representing the set of synsets associated with the cue and response words, respectively, as shown in Equation 1.\n sent2: This may increase the probability that |nb(S, k)∩nb(T, k)| > 0, a #TARGET_REF due to WordNet's \"relatively sparse connective structure\" ( #TARGET_REF) .\n sent3: Fourth, in addition to the Word2Vec (w2v) cosine similarity between cue/response pairs calculated using Google's pre-trained 300 dimension Word2Vec embeddings 2 .\n sent4: We also examine the effectiveness of Stanford's pre-trained 300 dimension GloVe embeddings 3 .\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Gaussian embeddings (#REF) represent words not as a fixed point in vector space but as \"potential functions\", continuous densities in latent space; therefore, they are more suitable for capturing asymmetric relationships.",
                "More specifically, for each cue/response pair, we calculate both the KL-divergence and cosine similarities of their Gaussian embeddings.",
                "The embeddings have a dimensionality of 300 and are trained on English Wikipedia using the Word2Gauss 4 (w2g) and the hyperparameters reported by the developer 5 Sixth, since cue and response words are not associated with a single synset, the AutoEx embeddings employed in #TARGET_REF to compute vector offsets are not well suited for our task.",
                "Instead, we experiment with offsets calculated using w2v, GloVe, and w2g embeddings.",
                "Finally, our 300 topic LDA model (#REF) was trained using Gensim 6 on full English Wikipedia instead of the subset of English Wikipedia used in #REF ."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Gaussian embeddings (#REF) represent words not as a fixed point in vector space but as \"potential functions\", continuous densities in latent space; therefore, they are more suitable for capturing asymmetric relationships.\n sent1: More specifically, for each cue/response pair, we calculate both the KL-divergence and cosine similarities of their Gaussian embeddings.\n sent2: The embeddings have a dimensionality of 300 and are trained on English Wikipedia using the Word2Gauss 4 (w2g) and the hyperparameters reported by the developer 5 Sixth, since cue and response words are not associated with a single synset, the AutoEx embeddings employed in #TARGET_REF to compute vector offsets are not well suited for our task.\n sent3: Instead, we experiment with offsets calculated using w2v, GloVe, and w2g embeddings.\n sent4: Finally, our 300 topic LDA model (#REF) was trained using Gensim 6 on full English Wikipedia instead of the subset of English Wikipedia used in #REF .\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The embeddings have a dimensionality of 300 and are trained on English Wikipedia using the Word2Gauss 4 (w2g) and the hyperparameters reported by the developer 5 Sixth, since cue and response words are not associated with a single synset, the AutoEx embeddings employed in #REF to compute vector offsets are not well suited for our task.",
                "#TARGET_REF experiment with offsets calculated using w2v, GloVe, and w2g embeddings.",
                "Finally, our 300 topic LDA model (#REF) was trained using Gensim 6 on full English Wikipedia instead of the subset of English Wikipedia used in #REF .",
                "Using the above features, we trained a multilayer perceptron for each of our three datasets; Evocation, USF, and EAT.",
                "In the case of Evocation, we discarded any synset information and Table 1 : Individual feature performance after 50 epochs simply use each synset's headword (e.g. given the sysnet entity.n.01, we only considered the word entity)."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: The embeddings have a dimensionality of 300 and are trained on English Wikipedia using the Word2Gauss 4 (w2g) and the hyperparameters reported by the developer 5 Sixth, since cue and response words are not associated with a single synset, the AutoEx embeddings employed in #REF to compute vector offsets are not well suited for our task.\n sent1: #TARGET_REF experiment with offsets calculated using w2v, GloVe, and w2g embeddings.\n sent2: Finally, our 300 topic LDA model (#REF) was trained using Gensim 6 on full English Wikipedia instead of the subset of English Wikipedia used in #REF .\n sent3: Using the above features, we trained a multilayer perceptron for each of our three datasets; Evocation, USF, and EAT.\n sent4: In the case of Evocation, we discarded any synset information and Table 1 : Individual feature performance after 50 epochs simply use each synset's headword (e.g. given the sysnet entity.n.01, we only considered the word entity).\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "More specifically, for each cue/response pair, we calculate both the KL-divergence and cosine similarities of their Gaussian embeddings.",
                "The embeddings have a dimensionality of 300 and are trained on English Wikipedia using the Word2Gauss 4 (w2g) and the hyperparameters reported by the developer 5 Sixth, since cue and response words are not associated with a single synset, the AutoEx embeddings employed in #REF to compute vector offsets are not well suited for our task.",
                "Instead, we experiment with offsets calculated using w2v, GloVe, and w2g embeddings.",
                "Finally, our 300 topic LDA model (#REF) was trained using Gensim 6 on full English Wikipedia instead of the subset of English Wikipedia used in #TARGET_REF) .",
                "Using the above features, we trained a multilayer perceptron for each of our three datasets; Evocation, USF, and EAT."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: More specifically, for each cue/response pair, we calculate both the KL-divergence and cosine similarities of their Gaussian embeddings.\n sent1: The embeddings have a dimensionality of 300 and are trained on English Wikipedia using the Word2Gauss 4 (w2g) and the hyperparameters reported by the developer 5 Sixth, since cue and response words are not associated with a single synset, the AutoEx embeddings employed in #REF to compute vector offsets are not well suited for our task.\n sent2: Instead, we experiment with offsets calculated using w2v, GloVe, and w2g embeddings.\n sent3: Finally, our 300 topic LDA model (#REF) was trained using Gensim 6 on full English Wikipedia instead of the subset of English Wikipedia used in #TARGET_REF) .\n sent4: Using the above features, we trained a multilayer perceptron for each of our three datasets; Evocation, USF, and EAT.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "On Evocation, our system does not perform as well as #TARGET_REF .",
                "This is expected as we explicitly ignore any synset information and instead attempt to predict association strengths between word-sense ambiguous words.",
                "Despite this, our performance is not appreciably lower, indicating the fitness of our system.",
                "The fact that we perform better on Evocation than either USF or EAT is quite interesting considering our system was designed with USF and EAT in mind.",
                "There are several possible explanations for this."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: On Evocation, our system does not perform as well as #TARGET_REF .\n sent1: This is expected as we explicitly ignore any synset information and instead attempt to predict association strengths between word-sense ambiguous words.\n sent2: Despite this, our performance is not appreciably lower, indicating the fitness of our system.\n sent3: The fact that we perform better on Evocation than either USF or EAT is quite interesting considering our system was designed with USF and EAT in mind.\n sent4: There are several possible explanations for this.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "In this paper we explored the effectiveness of various features for predicting Evocation, USF, and EAT association strengths, finding GloVe and Word2Vec cosine similarities as well as vector offsets to be the most useful features.",
                "We also examined the effectiveness of Gaussian embeddings for capturing the asymmetric nature of word embeddings but found it to be less effective than traditional word embeddings.",
                "Although we report a lower performance than that in #TARGET_REF , potentially indicating that predicting association strengths in word-sense ambiguous contexts is a harder task, we believe our results are a promising start.",
                "Training Gaussian embeddings on a larger corpus may lead to improved effectiveness.",
                "Future works should also consider incorporating word-sense frequencies or developing word-sense agnostic features, with a particular focus on asymmetric features."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: In this paper we explored the effectiveness of various features for predicting Evocation, USF, and EAT association strengths, finding GloVe and Word2Vec cosine similarities as well as vector offsets to be the most useful features.\n sent1: We also examined the effectiveness of Gaussian embeddings for capturing the asymmetric nature of word embeddings but found it to be less effective than traditional word embeddings.\n sent2: Although we report a lower performance than that in #TARGET_REF , potentially indicating that predicting association strengths in word-sense ambiguous contexts is a harder task, we believe our results are a promising start.\n sent3: Training Gaussian embeddings on a larger corpus may lead to improved effectiveness.\n sent4: Future works should also consider incorporating word-sense frequencies or developing word-sense agnostic features, with a particular focus on asymmetric features.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "In the case of Evocation, we discarded any synset information and Table 1 : Individual feature performance after 50 epochs simply use each synset's headword (e.g. given the sysnet entity.n.01, we only considered the word entity).",
                "Following the setup used in #TARGET_REF , all neural networks are trained using the Chainer 7 Python library with rectified linear units, dropout, and two hidden layers, each with 50% of the number of units in the input layer.",
                "All were trained on 80% of their respective dataset, with 20% held out for testing.",
                "Mean squared error was used as a loss function and optimization was performed using Adam algorithm (#REF) .",
                "To act as a baseline, we also reimplemented the system described in #REF and trained it on the same 80/20 split of Evocation as our system."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In the case of Evocation, we discarded any synset information and Table 1 : Individual feature performance after 50 epochs simply use each synset's headword (e.g. given the sysnet entity.n.01, we only considered the word entity).\n sent1: Following the setup used in #TARGET_REF , all neural networks are trained using the Chainer 7 Python library with rectified linear units, dropout, and two hidden layers, each with 50% of the number of units in the input layer.\n sent2: All were trained on 80% of their respective dataset, with 20% held out for testing.\n sent3: Mean squared error was used as a loss function and optimization was performed using Adam algorithm (#REF) .\n sent4: To act as a baseline, we also reimplemented the system described in #REF and trained it on the same 80/20 split of Evocation as our system.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Following the setup used in #REF , all neural networks are trained using the Chainer 7 Python library with rectified linear units, dropout, and two hidden layers, each with 50% of the number of units in the input layer.",
                "All were trained on 80% of their respective dataset, with 20% held out for testing.",
                "Mean squared error was used as a loss function and optimization was performed using Adam algorithm (#REF) .",
                "To act as a baseline, we also reimplemented the system described in #TARGET_REF and trained #TARGET_REF on the same 80/20 split of Evocation as our system.",
                "In addition to the reported results, we also performed feature selection experiments using 20% of the training sets as validation."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Following the setup used in #REF , all neural networks are trained using the Chainer 7 Python library with rectified linear units, dropout, and two hidden layers, each with 50% of the number of units in the input layer.\n sent1: All were trained on 80% of their respective dataset, with 20% held out for testing.\n sent2: Mean squared error was used as a loss function and optimization was performed using Adam algorithm (#REF) .\n sent3: To act as a baseline, we also reimplemented the system described in #TARGET_REF and trained #TARGET_REF on the same 80/20 split of Evocation as our system.\n sent4: In addition to the reported results, we also performed feature selection experiments using 20% of the training sets as validation.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The results of our #TARGET_REF implementation are roughly comparable to those reported in the #TARGET_REF (r = 0.374, ρ = 0.401 compared to r = 0.439, ρ = 0.400).",
                "Our slightly lower Pearson's R may be due to differences in way we split our training and test data as well as due to randomness in the training process itself.",
                "On Evocation, our system does not perform as well as #REF .",
                "This is expected as we explicitly ignore any synset information and instead attempt to predict association strengths between word-sense ambiguous words.",
                "Despite this, our performance is not appreciably lower, indicating the fitness of our system."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: The results of our #TARGET_REF implementation are roughly comparable to those reported in the #TARGET_REF (r = 0.374, ρ = 0.401 compared to r = 0.439, ρ = 0.400).\n sent1: Our slightly lower Pearson's R may be due to differences in way we split our training and test data as well as due to randomness in the training process itself.\n sent2: On Evocation, our system does not perform as well as #REF .\n sent3: This is expected as we explicitly ignore any synset information and instead attempt to predict association strengths between word-sense ambiguous words.\n sent4: Despite this, our performance is not appreciably lower, indicating the fitness of our system.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "To automate this time-consuming task, there has been much effort in developing systems for lexical simplification (#REF; #REF) and syntactic simplification (#REF; #REF) .",
                "The performance of the state-of-the-art systems has improved significantly #TARGET_REF; #REF) .",
                "Nonetheless, one cannot expect any single system, trained on a particular dataset, to simplify arbitrary texts in a way that would suit all readers -for example, the kinds of English words and structures suitable for a native speaker in Grade 6 are unlikely to be suitable for a non-native speaker in Grade 4.",
                "Hence, human effort is generally needed for modifying the system output.",
                "To support human post-editing, a number of researchers have developed specialized editors for text simplification."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: To automate this time-consuming task, there has been much effort in developing systems for lexical simplification (#REF; #REF) and syntactic simplification (#REF; #REF) .\n sent1: The performance of the state-of-the-art systems has improved significantly #TARGET_REF; #REF) .\n sent2: Nonetheless, one cannot expect any single system, trained on a particular dataset, to simplify arbitrary texts in a way that would suit all readers -for example, the kinds of English words and structures suitable for a native speaker in Grade 6 are unlikely to be suitable for a non-native speaker in Grade 4.\n sent3: Hence, human effort is generally needed for modifying the system output.\n sent4: To support human post-editing, a number of researchers have developed specialized editors for text simplification.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Given an input sentence, we first identify the target words, namely those words that do not appear in the vocabulary list.",
                "Following #TARGET_REF , our system simplifies neither proper nouns, as identified by the Natural Language Toolkit (#REF) , nor words in our stoplist, which are already simple.",
                "In terms of the three-step framework described above, we use the word2vec model 1 to retrieve candidates for substitution in the first step.",
                "We trained the model with all sentences from Wikipedia.",
                "For each target word, the model returns a list of the most similar words; we extract the top 20 in this list that are included in the user-supplied vocabulary list."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Given an input sentence, we first identify the target words, namely those words that do not appear in the vocabulary list.\n sent1: Following #TARGET_REF , our system simplifies neither proper nouns, as identified by the Natural Language Toolkit (#REF) , nor words in our stoplist, which are already simple.\n sent2: In terms of the three-step framework described above, we use the word2vec model 1 to retrieve candidates for substitution in the first step.\n sent3: We trained the model with all sentences from Wikipedia.\n sent4: For each target word, the model returns a list of the most similar words; we extract the top 20 in this list that are included in the user-supplied vocabulary list.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We evaluated the performance of our algorithm on the Mechanical Turk Lexical Simplification Data Set #TARGET_REF .",
                "This dataset contains 500 manually annotated sentences; the target word in each sentence was annotated by 50 independent annotators.",
                "To simulate a teacher adapting an English text for Hong Kong pupils, we used the vocabulary list from the Hong Kong Education Bureau (EDB, 2012) .",
                "To enable automatic evaluation, we considered only the 249 sentences in the dataset whose target word is not in our vocabulary list, but whose human annotations contain at least one word in the list.",
                "Precision is at 31% for the top candidate; it is at 57% for the top ten candidates."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We evaluated the performance of our algorithm on the Mechanical Turk Lexical Simplification Data Set #TARGET_REF .\n sent1: This dataset contains 500 manually annotated sentences; the target word in each sentence was annotated by 50 independent annotators.\n sent2: To simulate a teacher adapting an English text for Hong Kong pupils, we used the vocabulary list from the Hong Kong Education Bureau (EDB, 2012) .\n sent3: To enable automatic evaluation, we considered only the 249 sentences in the dataset whose target word is not in our vocabulary list, but whose human annotations contain at least one word in the list.\n sent4: Precision is at 31% for the top candidate; it is at 57% for the top ten candidates.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "We evaluated the quality of syntactic simplification on the first 300 sentences in the Mechanical Turk Lexical Simplification Data Set #TARGET_REF .",
                "For each sentence, we asked a professor of linguistics to mark the types of syntactic simplification ("
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We evaluated the quality of syntactic simplification on the first 300 sentences in the Mechanical Turk Lexical Simplification Data Set #TARGET_REF .\n sent1: For each sentence, we asked a professor of linguistics to mark the types of syntactic simplification (\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "In previous work, explanations have been categorised as either explaining 1) machine learning as in [11] who showed that they can increase trust; 2) explaining plans [2, 13] ; 3) verbalising robot [12] or agent rationalisation #TARGET_REF .",
                "However, humans do not present a constant verbalisation of their actions but they do need to be able to provide information on-demand about what they are doing and why during a live mission.",
                "We present here, MIRIAM, (Multimodal Intelligent inteRactIon for Autonomous systeMs), as seen in Figure 1 .",
                "MIRIAM allows for these 'on-demand' queries for status and explanations of behaviour.",
                "MIRIAM interfaces with the Neptune autonomy software provided by SeeByte Ltd and runs alongside their SeeTrack interface."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In previous work, explanations have been categorised as either explaining 1) machine learning as in [11] who showed that they can increase trust; 2) explaining plans [2, 13] ; 3) verbalising robot [12] or agent rationalisation #TARGET_REF .\n sent1: However, humans do not present a constant verbalisation of their actions but they do need to be able to provide information on-demand about what they are doing and why during a live mission.\n sent2: We present here, MIRIAM, (Multimodal Intelligent inteRactIon for Autonomous systeMs), as seen in Figure 1 .\n sent3: MIRIAM allows for these 'on-demand' queries for status and explanations of behaviour.\n sent4: MIRIAM interfaces with the Neptune autonomy software provided by SeeByte Ltd and runs alongside their SeeTrack interface.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "This has the advantage of being agnostic to the method of autonomy and could be used to describe rule-based autonomous behaviours but also complex deep models.",
                "Similar human-provided rationalisation has been used to generate explanations of deep neural models for game play #TARGET_REF .",
                "An interpretable model of autonomy was then derived from the expert, as partially shown in Figure 2 .",
                "If a why request is made, the decision tree is checked against the current mission status and history and the possible reasons are determined, along with a probability.",
                "As we can see from example outputs in Figure 3A , there may be multiple reasons with varying levels of certainty depending on the information available at a given point in the mission."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This has the advantage of being agnostic to the method of autonomy and could be used to describe rule-based autonomous behaviours but also complex deep models.\n sent1: Similar human-provided rationalisation has been used to generate explanations of deep neural models for game play #TARGET_REF .\n sent2: An interpretable model of autonomy was then derived from the expert, as partially shown in Figure 2 .\n sent3: If a why request is made, the decision tree is checked against the current mission status and history and the possible reasons are determined, along with a probability.\n sent4: As we can see from example outputs in Figure 3A , there may be multiple reasons with varying levels of certainty depending on the information available at a given point in the mission.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Our work is similar in comparing different neural architectures under unified settings, but differs in four main aspects: 1) Their experiments are based on a BiLSTM with handcrafted word features, while our experiments are based on end-to-end neural models without human knowledge.",
                "2) Their system gives relatively low performances on standard benchmarks 2 , while ours can give comparable or better results with state-of-the-art models, rendering our observations more informative for practitioners.",
                "3) Our findings are more consistent with #TARGET_REF on configurations such as usefulness of character information (#REF; #REF) , optimizer ( #TARGET_REF; #REF; #REF) and tag scheme (#REF; #REF) .",
                "In contrast, many results of Reimers and Gurevych (2017b) contradict existing reports.",
                "4) We conduct a wider range of comparison for word sequence representations, including all combinations of character CNN/LSTM and word CNN/LSTM structures, while Reimers and Gurevych (2017b) studied the word LSTM models only."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: Our work is similar in comparing different neural architectures under unified settings, but differs in four main aspects: 1) Their experiments are based on a BiLSTM with handcrafted word features, while our experiments are based on end-to-end neural models without human knowledge.\n sent1: 2) Their system gives relatively low performances on standard benchmarks 2 , while ours can give comparable or better results with state-of-the-art models, rendering our observations more informative for practitioners.\n sent2: 3) Our findings are more consistent with #TARGET_REF on configurations such as usefulness of character information (#REF; #REF) , optimizer ( #TARGET_REF; #REF; #REF) and tag scheme (#REF; #REF) .\n sent3: In contrast, many results of Reimers and Gurevych (2017b) contradict existing reports.\n sent4: 4) We conduct a wider range of comparison for word sequence representations, including all combinations of character CNN/LSTM and word CNN/LSTM structures, while Reimers and Gurevych (2017b) studied the word LSTM models only.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Optimizer.",
                "We compare different optimizers including SGD, Adagrad (#REF ), Adadelta (#REF , RMSProp (#REF) and Adam (#REF) .",
                "The results are shown in Figure 5 5 .",
                "In contrast to Reimers and Gurevych (2017b) , who reported that SGD is the worst optimizer, our results show that SGD outperforms all other optimizers significantly (p < 0.01), with a slower convergence process during training.",
                "Our observation is consistent with most literature ( #TARGET_REF; #REF; #REF) ."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: Optimizer.\n sent1: We compare different optimizers including SGD, Adagrad (#REF ), Adadelta (#REF , RMSProp (#REF) and Adam (#REF) .\n sent2: The results are shown in Figure 5 5 .\n sent3: In contrast to Reimers and Gurevych (2017b) , who reported that SGD is the worst optimizer, our results show that SGD outperforms all other optimizers significantly (p < 0.01), with a slower convergence process during training.\n sent4: Our observation is consistent with most literature ( #TARGET_REF; #REF; #REF) .\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Similarly, \"WLSTM\" and \"WCNN\" represent word LSTM and word CNN structure, respectively.",
                "As shown in Table 4 , most NER work focuses on WLSTM+CRF structures with different character sequence representations.",
                "We re-implement the structure of several reports ( #TARGET_REF; #REF; #REF) , which take the CCNN+WLSTM+CRF architecture.",
                "Our reproduced models give slightly better performances.",
                "The results of #REF can be reproduced by our CLSTM+WLSTM+CRF."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Similarly, \"WLSTM\" and \"WCNN\" represent word LSTM and word CNN structure, respectively.\n sent1: As shown in Table 4 , most NER work focuses on WLSTM+CRF structures with different character sequence representations.\n sent2: We re-implement the structure of several reports ( #TARGET_REF; #REF; #REF) , which take the CCNN+WLSTM+CRF architecture.\n sent3: Our reproduced models give slightly better performances.\n sent4: The results of #REF can be reproduced by our CLSTM+WLSTM+CRF.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Recently, #TARGET_REF proposed a novel reconstruction-based approach to alleviating dropped pronoun (DP) translation problems for neural machine translation models.",
                "In this work, we improve the original model from two perspectives.",
                "First, we employ a shared reconstructor to better exploit encoder and decoder representations.",
                "Second, we jointly learn to translate and predict DPs in an end-to-end manner, to avoid the errors propagated from an external DP prediction model.",
                "Experimental results show that our approach significantly improves both translation performance and DP prediction accuracy."
            ],
            "label": [
                "BACKGROUND",
                "USE",
                "EXTENDS"
            ]
        },
        "input": "sent0: Recently, #TARGET_REF proposed a novel reconstruction-based approach to alleviating dropped pronoun (DP) translation problems for neural machine translation models.\n sent1: In this work, we improve the original model from two perspectives.\n sent2: First, we employ a shared reconstructor to better exploit encoder and decoder representations.\n sent3: Second, we jointly learn to translate and predict DPs in an end-to-end manner, to avoid the errors propagated from an external DP prediction model.\n sent4: Experimental results show that our approach significantly improves both translation performance and DP prediction accuracy.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"USE\", \"EXTENDS\"], \"context\": [\"sent0\", \"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "This phenomenon leads to various translation problems in terms of completeness, syntax and even semantics of translations.",
                "A number of approaches have been investigated for DP translation (Le #REF; #REF; #REF #TARGET_REF .",
                "#REF is a pioneering work to model DP translation for neural machine trans- * Zhaopeng Tu is the corresponding author of the paper.",
                "This work was conducted when Longyue Wang was studying and Qun Liu was working at the ADAPT Centre in the School of Computing at Dublin City University.",
                "lation (NMT) models."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This phenomenon leads to various translation problems in terms of completeness, syntax and even semantics of translations.\n sent1: A number of approaches have been investigated for DP translation (Le #REF; #REF; #REF #TARGET_REF .\n sent2: #REF is a pioneering work to model DP translation for neural machine trans- * Zhaopeng Tu is the corresponding author of the paper.\n sent3: This work was conducted when Longyue Wang was studying and Qun Liu was working at the ADAPT Centre in the School of Computing at Dublin City University.\n sent4: lation (NMT) models.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "When translating sentences from a pro-drop language into a non-pro-drop language (e.g. Chinese-to-English), translation models generally fail to translate invisible dropped pronouns (DPs).",
                "This phenomenon leads to various translation problems in terms of completeness, syntax and even semantics of translations.",
                "A number of approaches have been investigated for DP translation (Le #REF; #REF; #REF #REF .",
                "#TARGET_REF is a pioneering work to model DP translation for neural machine trans- * Zhaopeng Tu is the corresponding author of the paper.",
                "This work was conducted when Longyue Wang was studying and Qun Liu was working at the ADAPT Centre in the School of Computing at Dublin City University."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: When translating sentences from a pro-drop language into a non-pro-drop language (e.g. Chinese-to-English), translation models generally fail to translate invisible dropped pronouns (DPs).\n sent1: This phenomenon leads to various translation problems in terms of completeness, syntax and even semantics of translations.\n sent2: A number of approaches have been investigated for DP translation (Le #REF; #REF; #REF #REF .\n sent3: #TARGET_REF is a pioneering work to model DP translation for neural machine trans- * Zhaopeng Tu is the corresponding author of the paper.\n sent4: This work was conducted when Longyue Wang was studying and Qun Liu was working at the ADAPT Centre in the School of Computing at Dublin City University.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "As shown in Figure 1 , #TARGET_REF introduced two independent reconstructors with their own parameters, which reconstruct the DPannotated source sentence from the encoder and decoder hidden states, respectively.",
                "The central Table 1 : Evaluation of external models on predicting the positions of DPs (\"DP Position\") and the exact words of DP (\"DP Words\").",
                "idea underpinning their approach is to guide the corresponding hidden states to embed the recalled source-side DP information and subsequently to help the NMT model generate the missing pronouns with these enhanced hidden representations.",
                "The DPs can be automatically annotated for training and test data using two different strategies (#REF) .",
                "In the training phase, where the target sentence is available, we annotate DPs for the source sentence using alignment information."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: As shown in Figure 1 , #TARGET_REF introduced two independent reconstructors with their own parameters, which reconstruct the DPannotated source sentence from the encoder and decoder hidden states, respectively.\n sent1: The central Table 1 : Evaluation of external models on predicting the positions of DPs (\"DP Position\") and the exact words of DP (\"DP Words\").\n sent2: idea underpinning their approach is to guide the corresponding hidden states to embed the recalled source-side DP information and subsequently to help the NMT model generate the missing pronouns with these enhanced hidden representations.\n sent3: The DPs can be automatically annotated for training and test data using two different strategies (#REF) .\n sent4: In the training phase, where the target sentence is available, we annotate DPs for the source sentence using alignment information.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "The DPs can be automatically annotated for training and test data using two different strategies (#REF) .",
                "In the training phase, where the target sentence is available, we annotate DPs for the source sentence using alignment information.",
                "These annotated source sentences can be used to build a neural-based DP predictor, which can be used to annotate test sentences since the target sentence is not available during the testing phase.",
                "As shown in Table 1 , Wang et al. (2016 #TARGET_REF explored to predict the exact DP words 1 , the accuracy of which is only 66% in F1-score.",
                "By analyzing the translation outputs, we found that 16.2% of errors are newly introduced and caused by errors from the DP predictor."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The DPs can be automatically annotated for training and test data using two different strategies (#REF) .\n sent1: In the training phase, where the target sentence is available, we annotate DPs for the source sentence using alignment information.\n sent2: These annotated source sentences can be used to build a neural-based DP predictor, which can be used to annotate test sentences since the target sentence is not available during the testing phase.\n sent3: As shown in Table 1 , Wang et al. (2016 #TARGET_REF explored to predict the exact DP words 1 , the accuracy of which is only 66% in F1-score.\n sent4: By analyzing the translation outputs, we found that 16.2% of errors are newly introduced and caused by errors from the DP predictor.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "To compare our work with the results reported by previous work #TARGET_REF , we conducted experiments on their released Chinese⇒English TV Subtitle corpus.",
                "2 The training, validation, and test sets contain 2.15M, 1.09K, and 1.15K sentence pairs, respectively.",
                "We used case-insensitive 4-gram NIST BLEU metrics (#REF) for evaluation, and sign-test (#REF) to test for statistical significance.",
                "We implemented our models on the code repository released by #REF .",
                "3 We used the same configurations (e.g. vocabulary size = 30K, hidden size = 1000) and reproduced their reported results."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: To compare our work with the results reported by previous work #TARGET_REF , we conducted experiments on their released Chinese⇒English TV Subtitle corpus.\n sent1: 2 The training, validation, and test sets contain 2.15M, 1.09K, and 1.15K sentence pairs, respectively.\n sent2: We used case-insensitive 4-gram NIST BLEU metrics (#REF) for evaluation, and sign-test (#REF) to test for statistical significance.\n sent3: We implemented our models on the code repository released by #REF .\n sent4: 3 We used the same configurations (e.g. vocabulary size = 30K, hidden size = 1000) and reproduced their reported results.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "2 The training, validation, and test sets contain 2.15M, 1.09K, and 1.15K sentence pairs, respectively.",
                "We used case-insensitive 4-gram NIST BLEU metrics (#REF) for evaluation, and sign-test (#REF) to test for statistical significance.",
                "We implemented our models on the code repository released by #TARGET_REF .",
                "3 We used the same configurations (e.g. vocabulary size = 30K, hidden size = 1000) and reproduced their reported results.",
                "It should be emphasized that we did not use the pre-train strategy as done in #REF , since we found training from scratch achieved a better performance in the shared reconstructor setting."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: 2 The training, validation, and test sets contain 2.15M, 1.09K, and 1.15K sentence pairs, respectively.\n sent1: We used case-insensitive 4-gram NIST BLEU metrics (#REF) for evaluation, and sign-test (#REF) to test for statistical significance.\n sent2: We implemented our models on the code repository released by #TARGET_REF .\n sent3: 3 We used the same configurations (e.g. vocabulary size = 30K, hidden size = 1000) and reproduced their reported results.\n sent4: It should be emphasized that we did not use the pre-train strategy as done in #REF , since we found training from scratch achieved a better performance in the shared reconstructor setting.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Baselines (Rows 1-4): The three baselines (Rows 1, 2, and 4) differ regarding the training data used.",
                "\"Separate-Recs⇒(+DPs)\" (Row 3) is the best model reported in #TARGET_REF , which we employed as another strong baseline.",
                "The baseline trained on the DPP-annotated data (\"Baseline (+DPPs)\", Row 4) outperforms the other two counterparts, indicating that the error propagation problem does affect the performance of translating DPs.",
                "It suggests the necessity of jointly learning to translate and predict DPs.",
                "Our Models (Rows 5-8): Using our shared reconstructor (Row 5) not only outperforms the corresponding baseline (Row 4), but also surpasses its separate reconstructor counterpart (Row 3)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Baselines (Rows 1-4): The three baselines (Rows 1, 2, and 4) differ regarding the training data used.\n sent1: \"Separate-Recs⇒(+DPs)\" (Row 3) is the best model reported in #TARGET_REF , which we employed as another strong baseline.\n sent2: The baseline trained on the DPP-annotated data (\"Baseline (+DPPs)\", Row 4) outperforms the other two counterparts, indicating that the error propagation problem does affect the performance of translating DPs.\n sent3: It suggests the necessity of jointly learning to translate and predict DPs.\n sent4: Our Models (Rows 5-8): Using our shared reconstructor (Row 5) not only outperforms the corresponding baseline (Row 4), but also surpasses its separate reconstructor counterpart (Row 3).\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "In this work, we propose to improve the original model from two perspectives.",
                "First, we use a shared reconstructor to read hidden states from both encoder and decoder.",
                "Second, we integrate a DP predictor into NMT to jointly learn to translate and predict DPs.",
                "Incorporating these as two auxiliary loss terms can guide both the encoder and decoder states to learn critical information relevant to DPs.",
                "Experimental results on a largescale Chinese-English subtitle corpus show that the two modifications can accumulatively improve translation performance, and the best result is +1.5 BLEU points better than that reported by #TARGET_REF ."
            ],
            "label": [
                "EXTENDS",
                "DIFFERENCES"
            ]
        },
        "input": "sent0: In this work, we propose to improve the original model from two perspectives.\n sent1: First, we use a shared reconstructor to read hidden states from both encoder and decoder.\n sent2: Second, we integrate a DP predictor into NMT to jointly learn to translate and predict DPs.\n sent3: Incorporating these as two auxiliary loss terms can guide both the encoder and decoder states to learn critical information relevant to DPs.\n sent4: Experimental results on a largescale Chinese-English subtitle corpus show that the two modifications can accumulatively improve translation performance, and the best result is +1.5 BLEU points better than that reported by #TARGET_REF .\n",
        "output": "{\"label\": [\"EXTENDS\", \"DIFFERENCES\"], \"context\": [\"sent0\", \"sent1\", \"sent2\", \"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Following Table 2 : Evaluation of translation performance for Chinese-English. \"Baseline\" is trained and evaluated on the original data, while \"Baseline (+DPs)\" and \"Baseline (+DPPs)\" are trained on the data annotated with DPs and DPPs, respectively.",
                "Training and decoding (beam size is 10) speeds are measured in words/second.",
                "\" †\" and \" ‡\" indicate statistically significant difference (p < 0.01) from \"Baseline (+DDPs)\" and \"Separate-Recs⇒(+DPs)\", respectively.",
                "as a reranking technique to select the best translation candidate from the generated n-best list at testing time.",
                "Different from #TARGET_REF, we reconstruct DPP-annotated source sentence, which is predicted by an external model."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Following Table 2 : Evaluation of translation performance for Chinese-English. \"Baseline\" is trained and evaluated on the original data, while \"Baseline (+DPs)\" and \"Baseline (+DPPs)\" are trained on the data annotated with DPs and DPPs, respectively.\n sent1: Training and decoding (beam size is 10) speeds are measured in words/second.\n sent2: \" †\" and \" ‡\" indicate statistically significant difference (p < 0.01) from \"Baseline (+DDPs)\" and \"Separate-Recs⇒(+DPs)\", respectively.\n sent3: as a reranking technique to select the best translation candidate from the generated n-best list at testing time.\n sent4: Different from #TARGET_REF, we reconstruct DPP-annotated source sentence, which is predicted by an external model.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "We used case-insensitive 4-gram NIST BLEU metrics (#REF) for evaluation, and sign-test (#REF) to test for statistical significance.",
                "We implemented our models on the code repository released by #REF .",
                "3 We used the same configurations (e.g. vocabulary size = 30K, hidden size = 1000) and reproduced their reported results.",
                "It should be emphasized that we did not use the pre-train strategy as done in #TARGET_REF , since we found training from scratch achieved a better performance in the shared reconstructor setting.",
                "2 https://github.com/longyuewangdcu/ tvsub 3 https://github.com/tuzhaopeng/nmt Table 2 shows the translation results."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: We used case-insensitive 4-gram NIST BLEU metrics (#REF) for evaluation, and sign-test (#REF) to test for statistical significance.\n sent1: We implemented our models on the code repository released by #REF .\n sent2: 3 We used the same configurations (e.g. vocabulary size = 30K, hidden size = 1000) and reproduced their reported results.\n sent3: It should be emphasized that we did not use the pre-train strategy as done in #TARGET_REF , since we found training from scratch achieved a better performance in the shared reconstructor setting.\n sent4: 2 https://github.com/longyuewangdcu/ tvsub 3 https://github.com/tuzhaopeng/nmt Table 2 shows the translation results.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Our Models (Rows 5-8): Using our shared reconstructor (Row 5) not only outperforms the corresponding baseline (Row 4), but also surpasses its separate reconstructor counterpart (Row 3).",
                "Introducing a joint prediction objective (Row 6) can achieve a further improvement of +0.61 BLEU points.",
                "These results verify that shared reconstructor and jointly predicting DPs can accumulatively improve translation performance.",
                "Among the variations of shared reconstructors (Rows 6-8), we found that an interaction attention from encoder to decoder (Row 7) achieves the best performance, which is +3.45 BLEU points better than our baseline (Row 4) and +1.45 BLEU points better than the best result reported by #TARGET_REF (Row 3) .",
                "We attribute the superior performance of \"Shared-Rec enc→dec \" to the fact that the attention context over encoder representations embeds useful DP information, which can help to better attend to the representations of the corresponding pronouns in the decoder side."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Our Models (Rows 5-8): Using our shared reconstructor (Row 5) not only outperforms the corresponding baseline (Row 4), but also surpasses its separate reconstructor counterpart (Row 3).\n sent1: Introducing a joint prediction objective (Row 6) can achieve a further improvement of +0.61 BLEU points.\n sent2: These results verify that shared reconstructor and jointly predicting DPs can accumulatively improve translation performance.\n sent3: Among the variations of shared reconstructors (Rows 6-8), we found that an interaction attention from encoder to decoder (Row 7) achieves the best performance, which is +3.45 BLEU points better than our baseline (Row 4) and +1.45 BLEU points better than the best result reported by #TARGET_REF (Row 3) .\n sent4: We attribute the superior performance of \"Shared-Rec enc→dec \" to the fact that the attention context over encoder representations embeds useful DP information, which can help to better attend to the representations of the corresponding pronouns in the decoder side.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "We attribute this to the useful contextual information embedded in the reconstructor representations, which are used to generate the exact DP words.",
                "Table 4 : Translation results when reconstruction is used in training only while not used in testing.",
                "Table 4 lists translation results when the reconstruction model is used in training only.",
                "We can see that the proposed model outperforms both the strong baseline and the best model reported in #TARGET_REF .",
                "This is encouraging since no extra resources and computation are introduced to online decoding, which makes the approach highly practical, for example for translation in industry applications."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: We attribute this to the useful contextual information embedded in the reconstructor representations, which are used to generate the exact DP words.\n sent1: Table 4 : Translation results when reconstruction is used in training only while not used in testing.\n sent2: Table 4 lists translation results when the reconstruction model is used in training only.\n sent3: We can see that the proposed model outperforms both the strong baseline and the best model reported in #TARGET_REF .\n sent4: This is encouraging since no extra resources and computation are introduced to online decoding, which makes the approach highly practical, for example for translation in industry applications.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "These results verify that shared reconstructor and jointly predicting DPs can accumulatively improve translation performance.",
                "Among the variations of shared reconstructors (Rows 6-8), we found that an interaction attention from encoder to decoder (Row 7) achieves the best performance, which is +3.45 BLEU points better than our baseline (Row 4) and +1.45 BLEU points better than the best result reported by #REF (Row 3) .",
                "We attribute the superior performance of \"Shared-Rec enc→dec \" to the fact that the attention context over encoder representations embeds useful DP information, which can help to better attend to the representations of the corresponding pronouns in the decoder side.",
                "Similar to #TARGET_REF , the proposed approach improves BLEU scores at the cost of decreased training and decoding speed, which is due to the large number of newly introduced parameters resulting from the incorporation of reconstructors into the NMT model.",
                "Table 3 : Evaluation of DP prediction accuracy. \"External\" model is separately trained on DP-annotated data with external neural methods (#REF) , while \"Joint\" model is jointly trained with the NMT model (Section 3.2)."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: These results verify that shared reconstructor and jointly predicting DPs can accumulatively improve translation performance.\n sent1: Among the variations of shared reconstructors (Rows 6-8), we found that an interaction attention from encoder to decoder (Row 7) achieves the best performance, which is +3.45 BLEU points better than our baseline (Row 4) and +1.45 BLEU points better than the best result reported by #REF (Row 3) .\n sent2: We attribute the superior performance of \"Shared-Rec enc→dec \" to the fact that the attention context over encoder representations embeds useful DP information, which can help to better attend to the representations of the corresponding pronouns in the decoder side.\n sent3: Similar to #TARGET_REF , the proposed approach improves BLEU scores at the cost of decreased training and decoding speed, which is due to the large number of newly introduced parameters resulting from the incorporation of reconstructors into the NMT model.\n sent4: Table 3 : Evaluation of DP prediction accuracy. \"External\" model is separately trained on DP-annotated data with external neural methods (#REF) , while \"Joint\" model is jointly trained with the NMT model (Section 3.2).\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "To address this drawback, new VQA datasets [44, 8, 37] have been recently proposed with questions that explicitly require understanding and reasoning about text in the image, which is referred to as the TextVQA task.",
                "Figure 1 .",
                "Compared to previous work (e.g. #TARGET_REF ) on the TextVQA task, our model, accompanied by rich features for image text, handles all modalities with a multimodal transformer over a joint embedding space instead of pairwise fusion mechanisms between modalities.",
                "Furthermore, answers are predicted through iterative decoding with pointers instead of one-step classification over a fixed vocabulary or copying single text token from the image.",
                "The TextVQA task distinctively requires models to see, read and reason over three modalities: the input question, the visual contents in the image such as visual objects, and the text in the image."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: To address this drawback, new VQA datasets [44, 8, 37] have been recently proposed with questions that explicitly require understanding and reasoning about text in the image, which is referred to as the TextVQA task.\n sent1: Figure 1 .\n sent2: Compared to previous work (e.g. #TARGET_REF ) on the TextVQA task, our model, accompanied by rich features for image text, handles all modalities with a multimodal transformer over a joint embedding space instead of pairwise fusion mechanisms between modalities.\n sent3: Furthermore, answers are predicted through iterative decoding with pointers instead of one-step classification over a fixed vocabulary or copying single text token from the image.\n sent4: The TextVQA task distinctively requires models to see, read and reason over three modalities: the input question, the visual contents in the image such as visual objects, and the text in the image.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Finally, we introduce a rich representation for text tokens in the images based on multiple cues, including its word embedding, appearance, location, and character-level information.",
                "Our contributions in this paper are as follows: 1) We show that multiple (more than two) input modalities can be naturally fused and jointly modeled through our multimodal transformer architecture.",
                "2) Unlike previous work on TextVQA, our model reasons about the answer beyond a single classification step and predicts it through our pointeraugmented multi-step decoder.",
                "3) We adopt a rich feature representation for text tokens in images and show that it is better than features based only on word embedding in previous work.",
                "4) Our model significantly outperforms previous work on three challenging datasets for the TextVQA task: TextVQA #TARGET_REF (+25% relative), ST-VQA [8] (+65% relative), and OCR-VQA [37] (+32% relative)."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Finally, we introduce a rich representation for text tokens in the images based on multiple cues, including its word embedding, appearance, location, and character-level information.\n sent1: Our contributions in this paper are as follows: 1) We show that multiple (more than two) input modalities can be naturally fused and jointly modeled through our multimodal transformer architecture.\n sent2: 2) Unlike previous work on TextVQA, our model reasons about the answer beyond a single classification step and predicts it through our pointeraugmented multi-step decoder.\n sent3: 3) We adopt a rich feature representation for text tokens in images and show that it is better than features based only on word embedding in previous work.\n sent4: 4) Our model significantly outperforms previous work on three challenging datasets for the TextVQA task: TextVQA #TARGET_REF (+25% relative), ST-VQA [8] (+65% relative), and OCR-VQA [37] (+32% relative).\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "We fine-tune the last layer of the Faster R-CNN detector during training.",
                "Embedding of OCR tokens with rich representations.",
                "Intuitively, to represent text in images, one needs to encode not only its characters, but also its appearance (e.g. color, font, and background) and spatial location in the image (e.g. words appearing on the top of a book cover are more likely to be book titles).",
                "We follow this intuition in our model and use a rich OCR representation consisting of four types of features, which is shown in our experiments to be significantly better than word embedding (such as FastText) alone in prior work #TARGET_REF .",
                "After obtaining a set of N OCR tokens in an image through external OCR systems, from the n-th token (where n = 1, · · · , N ) we extract 1) a 300-dimensional #REF vector x ft n , which is a word embedding with sub-word information, 2) an appearance feature x fr n from the same Faster R-CNN detector in the object detection above, extracted via RoI-Pooling on the OCR token's bounding box, 3) a 604-dimensional Pyramidal Histogram of Characters (PHOC) [2] vector x p n , capturing what characters are present in the token -this is more robust to OCR errors and can be seen as a coarse character model, and 4) a 4-dimensional location feature x b n based on the OCR token's relative bounding box coordinates [x min /W im , y min /H im , x max /W im , y max /H im ]."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: We fine-tune the last layer of the Faster R-CNN detector during training.\n sent1: Embedding of OCR tokens with rich representations.\n sent2: Intuitively, to represent text in images, one needs to encode not only its characters, but also its appearance (e.g. color, font, and background) and spatial location in the image (e.g. words appearing on the top of a book cover are more likely to be book titles).\n sent3: We follow this intuition in our model and use a rich OCR representation consisting of four types of features, which is shown in our experiments to be significantly better than word embedding (such as FastText) alone in prior work #TARGET_REF .\n sent4: After obtaining a set of N OCR tokens in an image through external OCR systems, from the n-th token (where n = 1, · · · , N ) we extract 1) a 300-dimensional #REF vector x ft n , which is a word embedding with sub-word information, 2) an appearance feature x fr n from the same Faster R-CNN detector in the object detection above, extracted via RoI-Pooling on the OCR token's bounding box, 3) a 604-dimensional Pyramidal Histogram of Characters (PHOC) [2] vector x p n , capturing what characters are present in the token -this is more robust to OCR errors and can be seen as a coarse character model, and 4) a 4-dimensional location feature x b n based on the OCR token's relative bounding box coordinates [x min /W im , y min /H im , x max /W im , y max /H im ].\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Then, the fc6 feature vector is extracted from each detected object.",
                "We apply the Faster R-CNN fc7 weights on the extracted fc6 features to output 2048-dimensional fc7 appearance features and finetune fc7 weights during training.",
                "However, we do not use the ResNet-152 convolutional features [19] as in LoRRA.",
                "Finally, we extract text tokens on each image using the Rosetta OCR system [10] .",
                "Unlike the prior work LoRRA #TARGET_REF that uses a multilingual Rosetta version, in our model we use an English-only version of Rosetta that we find has higher recall."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Then, the fc6 feature vector is extracted from each detected object.\n sent1: We apply the Faster R-CNN fc7 weights on the extracted fc6 features to output 2048-dimensional fc7 appearance features and finetune fc7 weights during training.\n sent2: However, we do not use the ResNet-152 convolutional features [19] as in LoRRA.\n sent3: Finally, we extract text tokens on each image using the Rosetta OCR system [10] .\n sent4: Unlike the prior work LoRRA #TARGET_REF that uses a multilingual Rosetta version, in our model we use an English-only version of Rosetta that we find has higher recall.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "4 We compare are from fixed answer vocabulary).",
                "Compared to the previous work LoRRA #TARGET_REF which selects one answer from training set or copies only a single OCR token, our model can copy multiple OCR tokens and combine them with its fixed vocabulary through iterative decoding.",
                "our method to DCD [32] (the challenge winner, based on ensemble) and MSFT VTI [46] (the top entry after the challenge), both relying on one-step prediction.",
                "We show that our single model (line 10) significantly outperforms these challenge winning entries on the TextVQA test set by a large margin.",
                "We also experiment with using the ST-VQA dataset [8] as additional training data (a practice used by some of the previous challenge participants), which gives another 1% improvement and 40.46% final test accuracya new state-of-the-art on the TextVQA dataset."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: 4 We compare are from fixed answer vocabulary).\n sent1: Compared to the previous work LoRRA #TARGET_REF which selects one answer from training set or copies only a single OCR token, our model can copy multiple OCR tokens and combine them with its fixed vocabulary through iterative decoding.\n sent2: our method to DCD [32] (the challenge winner, based on ensemble) and MSFT VTI [46] (the top entry after the challenge), both relying on one-step prediction.\n sent3: We show that our single model (line 10) significantly outperforms these challenge winning entries on the TextVQA test set by a large margin.\n sent4: We also experiment with using the ST-VQA dataset [8] as additional training data (a practice used by some of the previous challenge participants), which gives another 1% improvement and 40.46% final test accuracya new state-of-the-art on the TextVQA dataset.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "During the iterative answer decoding process, at each step our M4C model can decode an answer word either from the model's fixed vocabulary, or from the OCR tokens extracted from the image.",
                "We find in our experiments that it is necessary to have both the fixed vocabulary space and the OCR tokens.",
                "Table 5 shows our ablation study where we remove the fixed answer vocabulary or the dynamic pointer network for OCR copying from our M4C.",
                "Both these two ablated versions have a large accuracy drop compared to our full model.",
                "However, we note that even without fixed answer vocabulary, our restricted model (M4C w/o fixed vocabulary in Table 5 ) still outperforms the previous work LoRRA #TARGET_REF , suggesting that it is particularly important to learn to copy multiple OCR tokens to form an answer (a key feature in our model but not in LoRRA)."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: During the iterative answer decoding process, at each step our M4C model can decode an answer word either from the model's fixed vocabulary, or from the OCR tokens extracted from the image.\n sent1: We find in our experiments that it is necessary to have both the fixed vocabulary space and the OCR tokens.\n sent2: Table 5 shows our ablation study where we remove the fixed answer vocabulary or the dynamic pointer network for OCR copying from our M4C.\n sent3: Both these two ablated versions have a large accuracy drop compared to our full model.\n sent4: However, we note that even without fixed answer vocabulary, our restricted model (M4C w/o fixed vocabulary in Table 5 ) still outperforms the previous work LoRRA #TARGET_REF , suggesting that it is particularly important to learn to copy multiple OCR tokens to form an answer (a key feature in our model but not in LoRRA).\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "As it is intractable to have every possible text token in the answer vocabulary, copying text from the image would often be an easier option for answer prediction.",
                "Prior work has explored dynamically copying the inputs in different tasks such as text summarization [42] , knowledge retrieval [52] , and image captioning [35] based on Pointer #REF and its variants.",
                "For the TextVQA task, recent works #TARGET_REF 37] have proposed to copy OCR tokens by adding their indices to classifier outputs.",
                "However, apart from their limitation of copying only a single token (or block), one drawback of these approaches is that they require a pre-defined number of OCR tokens (since the classifier has a fixed output dimension) and their output is dependent on the ordering of the tokens.",
                "In this work, we overcome this drawback using a permutation-invariant pointer network together with our multimodal transformer."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: As it is intractable to have every possible text token in the answer vocabulary, copying text from the image would often be an easier option for answer prediction.\n sent1: Prior work has explored dynamically copying the inputs in different tasks such as text summarization [42] , knowledge retrieval [52] , and image captioning [35] based on Pointer #REF and its variants.\n sent2: For the TextVQA task, recent works #TARGET_REF 37] have proposed to copy OCR tokens by adding their indices to classifier outputs.\n sent3: However, apart from their limitation of copying only a single token (or block), one drawback of these approaches is that they require a pre-defined number of OCR tokens (since the classifier has a fixed output dimension) and their output is dependent on the ordering of the tokens.\n sent4: In this work, we overcome this drawback using a permutation-invariant pointer network together with our multimodal transformer.\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent2\", \"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "1 During training, the BERT parameters are fine-tuned using the question answering loss.",
                "Embedding of detected objects.",
                "Given an image, we obtain a set of M visual objects through a pretrained detector (Faster R-CNN [41] in our case).",
                "Following prior work [3, 43, #TARGET_REF , we extract appearance feature x fr m using the detector's output from the m-th object (where m = 1, · · · , M ).",
                "To capture its location in the image, we introduce a 4-dimensional location fea-"
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: 1 During training, the BERT parameters are fine-tuned using the question answering loss.\n sent1: Embedding of detected objects.\n sent2: Given an image, we obtain a set of M visual objects through a pretrained detector (Faster R-CNN [41] in our case).\n sent3: Following prior work [3, 43, #TARGET_REF , we extract appearance feature x fr m using the detector's output from the m-th object (where m = 1, · · · , M ).\n sent4: To capture its location in the image, we introduce a 4-dimensional location fea-\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "We evaluate our model on three challenging datasets for the TextVQA task, including the TextVQA dataset #TARGET_REF , the ST-VQA dataset [8] , and the OCR-VQA dataset [37] .",
                "Our model outperforms previous work by a significant margin on all the three datasets."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We evaluate our model on three challenging datasets for the TextVQA task, including the TextVQA dataset #TARGET_REF , the ST-VQA dataset [8] , and the OCR-VQA dataset [37] .\n sent1: Our model outperforms previous work by a significant margin on all the three datasets.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "The TextVQA dataset [44] contains 28,408 images from the Open Images dataset [27] , with human-written questions asking to reason about text in the image.",
                "Similar to VQAv2 [17] , each question in the TextVQA dataset has 10 human annotated answers, and the final accuracy is measured via soft voting of the 10 answers.",
                "2 We use d = 768 as the dimensionality of the joint embedding space and extract question word features with BERT-BASE using the 768-dimensional outputs from its first three layers, which are fine-tuned during training.",
                "For visual objects, following #REF and LoRRA #TARGET_REF , we detect objects with a Faster R-CNN detector [41] pretrained on the Visual Genome dataset [26] , and keeps 100 top-scoring objects per image.",
                "Then, the fc6 feature vector is extracted from each detected object."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The TextVQA dataset [44] contains 28,408 images from the Open Images dataset [27] , with human-written questions asking to reason about text in the image.\n sent1: Similar to VQAv2 [17] , each question in the TextVQA dataset has 10 human annotated answers, and the final accuracy is measured via soft voting of the 10 answers.\n sent2: 2 We use d = 768 as the dimensionality of the joint embedding space and extract question word features with BERT-BASE using the 768-dimensional outputs from its first three layers, which are fine-tuned during training.\n sent3: For visual objects, following #REF and LoRRA #TARGET_REF , we detect objects with a Faster R-CNN detector [41] pretrained on the Visual Genome dataset [26] , and keeps 100 top-scoring objects per image.\n sent4: Then, the fc6 feature vector is extracted from each detected object.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The best snapshot is selected using the validation set accuracy.",
                "The entire training takes approximately 10 hours on 4 Nvidia Tesla V100 GPUs.",
                "As a notable prior work on this dataset, we show a stepby-step comparison with the LoRRA model #TARGET_REF .",
                "Ablations on pretrained question encoding and OCR systems.",
                "We first experiment with a restricted version of our model using the multimodal transformer architecture but without iterative decoding in answer prediction, i.e. M4C (w/o dec.) in Table 1 ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The best snapshot is selected using the validation set accuracy.\n sent1: The entire training takes approximately 10 hours on 4 Nvidia Tesla V100 GPUs.\n sent2: As a notable prior work on this dataset, we show a stepby-step comparison with the LoRRA model #TARGET_REF .\n sent3: Ablations on pretrained question encoding and OCR systems.\n sent4: We first experiment with a restricted version of our model using the multimodal transformer architecture but without iterative decoding in answer prediction, i.e. M4C (w/o dec.) in Table 1 .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Here, we see that our model in line 8 still outperforms LoRRA (line 1) by as much as 9.5% (absolute) when using the same OCR system as LoRRA and even fewer pretrained components.",
                "We also analyze the performance of our model with respect to the maximum decoding steps, shown in Figure 3 , where decoding for multiple steps greatly improves the performance compared with a single step.",
                "Figure 4 shows qualitative examples (more examples in appendix) of our M4C model on the TextVQA dataset in comparison to LoRRA #TARGET_REF , where our model is capable of selecting multiple OCR tokens and combining them with its fixed vocabulary in predicted answers.",
                "Qualitative insights.",
                "When inspecting the errors, we find that a major source of errors is OCR failure (e.g. in the last example in Figure 4 , we find that the digits on the watch are not detected)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Here, we see that our model in line 8 still outperforms LoRRA (line 1) by as much as 9.5% (absolute) when using the same OCR system as LoRRA and even fewer pretrained components.\n sent1: We also analyze the performance of our model with respect to the maximum decoding steps, shown in Figure 3 , where decoding for multiple steps greatly improves the performance compared with a single step.\n sent2: Figure 4 shows qualitative examples (more examples in appendix) of our M4C model on the TextVQA dataset in comparison to LoRRA #TARGET_REF , where our model is capable of selecting multiple OCR tokens and combining them with its fixed vocabulary in predicted answers.\n sent3: Qualitative insights.\n sent4: When inspecting the errors, we find that a major source of errors is OCR failure (e.g. in the last example in Figure 4 , we find that the digits on the watch are not detected).\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Lexical simplification is the task to find and substitute a complex word or phrase in a sentence with its simpler synonymous expression.",
                "We define complex word as a word that has lexical and subjective difficulty in a sentence.",
                "It can help in reading comprehension for children and language learners (De #REF) .",
                "This task is a rather easier task which prepare a pair of complex and simple representations than a challenging task which changes the substitute pair in a given context #TARGET_REF; #REF) .",
                "Construction of a benchmark dataset is important to ensure the reliability and reproducibility of evaluation."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Lexical simplification is the task to find and substitute a complex word or phrase in a sentence with its simpler synonymous expression.\n sent1: We define complex word as a word that has lexical and subjective difficulty in a sentence.\n sent2: It can help in reading comprehension for children and language learners (De #REF) .\n sent3: This task is a rather easier task which prepare a pair of complex and simple representations than a challenging task which changes the substitute pair in a given context #TARGET_REF; #REF) .\n sent4: Construction of a benchmark dataset is important to ensure the reliability and reproducibility of evaluation.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The evaluation dataset for the English Lexical Simplification task #TARGET_REF Figure 1: A part of the dataset of #REF .",
                "notated on top of the evaluation dataset for English lexical substitution (#REF) .",
                "They asked university students to rerank substitutes according to simplification ranking.",
                "Sentences in their dataset do not always contain complex words, and it is not appropriate to evaluate simplification systems if a test sentence does not include any complex words.",
                "In addition, De #REF built an evaluation dataset for English lexical simplification based on that developed by #REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The evaluation dataset for the English Lexical Simplification task #TARGET_REF Figure 1: A part of the dataset of #REF .\n sent1: notated on top of the evaluation dataset for English lexical substitution (#REF) .\n sent2: They asked university students to rerank substitutes according to simplification ranking.\n sent3: Sentences in their dataset do not always contain complex words, and it is not appropriate to evaluate simplification systems if a test sentence does not include any complex words.\n sent4: In addition, De #REF built an evaluation dataset for English lexical simplification based on that developed by #REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Sentences in their dataset do not always contain complex words, and it is not appropriate to evaluate simplification systems if a test sentence does not include any complex words.",
                "In addition, De #REF built an evaluation dataset for English lexical simplification based on that developed by #REF .",
                "They used Amazon's Mechanical Turk to rank substitutes and employed the reliability of annotators to remove outlier annotators and/or downweight unreliable annotators.",
                "The reliability was calculated on penalty based agreement (#REF) and Fleiss' Kappa.",
                "Unlike the dataset of #TARGET_REF , sentences in their dataset contain at least one complex word, but they might contain more than one complex word."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Sentences in their dataset do not always contain complex words, and it is not appropriate to evaluate simplification systems if a test sentence does not include any complex words.\n sent1: In addition, De #REF built an evaluation dataset for English lexical simplification based on that developed by #REF .\n sent2: They used Amazon's Mechanical Turk to rank substitutes and employed the reliability of annotators to remove outlier annotators and/or downweight unreliable annotators.\n sent3: The reliability was calculated on penalty based agreement (#REF) and Fleiss' Kappa.\n sent4: Unlike the dataset of #TARGET_REF , sentences in their dataset contain at least one complex word, but they might contain more than one complex word.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Again, it is not adequate for the automatic evaluation of lexical simplification because the human ranking of the resulting simplification might be affected by the context containing complex words.",
                "Furthermore, De Belder and Moens' (2012) dataset is too small to be used for achieving a reliable evaluation of lexical simplification systems.",
                "3 Problems in previous datasets for Japanese lexical simplification #REF followed #TARGET_REF to construct an evaluation dataset for Japanese lexical simplification.",
                "Namely, they split the data creation process into two steps: substitute extraction and simplification ranking.",
                "During the substitute extraction task, they collected substitutes of each target word in 10 different contexts."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Again, it is not adequate for the automatic evaluation of lexical simplification because the human ranking of the resulting simplification might be affected by the context containing complex words.\n sent1: Furthermore, De Belder and Moens' (2012) dataset is too small to be used for achieving a reliable evaluation of lexical simplification systems.\n sent2: 3 Problems in previous datasets for Japanese lexical simplification #REF followed #TARGET_REF to construct an evaluation dataset for Japanese lexical simplification.\n sent3: Namely, they split the data creation process into two steps: substitute extraction and simplification ranking.\n sent4: During the substitute extraction task, they collected substitutes of each target word in 10 different contexts.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Our discussion in this paper is based on this example.",
                "Domain of the dataset is limited.",
                "Because #REF extracted sentences from a newswire corpus, their dataset has a poor variety of expression.",
                "English lexical simplification datasets #TARGET_REF; De #REF) do not have this problem because both of them use a balanced corpus of English (#REF) .",
                "Complex words might exist in context."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Our discussion in this paper is based on this example.\n sent1: Domain of the dataset is limited.\n sent2: Because #REF extracted sentences from a newswire corpus, their dataset has a poor variety of expression.\n sent3: English lexical simplification datasets #TARGET_REF; De #REF) do not have this problem because both of them use a balanced corpus of English (#REF) .\n sent4: Complex words might exist in context.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "This deteriorates ranking consistency if some substitutes have a similar simplicity.",
                "De #REF allow ties in simplification ranking and report considerably higher agreement among annotators than #TARGET_REF .",
                "The method of ranking integration is naïve.",
                "#REF and #REF use an average score to integrate rankings, but it might be biased by outliers.",
                "De #REF report a slight increase in agreement by greedily removing annotators to maximize the agreement score."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This deteriorates ranking consistency if some substitutes have a similar simplicity.\n sent1: De #REF allow ties in simplification ranking and report considerably higher agreement among annotators than #TARGET_REF .\n sent2: The method of ranking integration is naïve.\n sent3: #REF and #REF use an average score to integrate rankings, but it might be biased by outliers.\n sent4: De #REF report a slight increase in agreement by greedily removing annotators to maximize the agreement score.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "This deteriorates ranking consistency if some substitutes have a similar simplicity.",
                "De #REF allow ties in simplification ranking and report considerably higher agreement among annotators than #REF .",
                "The method of ranking integration is naïve.",
                "#REF and #TARGET_REF use an average score to integrate rankings, but it might be biased by outliers.",
                "De #REF report a slight increase in agreement by greedily removing annotators to maximize the agreement score."
            ],
            "label": [
                "BACKGROUND",
                "MOTIVATION"
            ]
        },
        "input": "sent0: This deteriorates ranking consistency if some substitutes have a similar simplicity.\n sent1: De #REF allow ties in simplification ranking and report considerably higher agreement among annotators than #REF .\n sent2: The method of ranking integration is naïve.\n sent3: #REF and #TARGET_REF use an average score to integrate rankings, but it might be biased by outliers.\n sent4: De #REF report a slight increase in agreement by greedily removing annotators to maximize the agreement score.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"MOTIVATION\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "This clearly shows the importance of allowing ties during the substitute ranking task.",
                "Table 2 shows the results of the ranking integration.",
                "Our method achieved better accuracy in ranking integration than previous methods #TARGET_REF; #REF) and is similar to the results from De #REF .",
                "This shows that the reliability score can be used for improving the quality.",
                "Table 3 shows the number of sentences and average substitutes in each genre."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: This clearly shows the importance of allowing ties during the substitute ranking task.\n sent1: Table 2 shows the results of the ranking integration.\n sent2: Our method achieved better accuracy in ranking integration than previous methods #TARGET_REF; #REF) and is similar to the results from De #REF .\n sent3: This shows that the reliability score can be used for improving the quality.\n sent4: Table 3 shows the number of sentences and average substitutes in each genre.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Therefore, in this work, we extract sentences containing only one complex word.",
                "Ties are not permitted in simplification ranking.",
                "When each annotator assigns a simplification ranking to a substitution list, a tie cannot be assigned in previous datasets #TARGET_REF; #REF) .",
                "This deteriorates ranking consistency if some substitutes have a similar simplicity.",
                "De #REF allow ties in simplification ranking and report considerably higher agreement among annotators than #REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Therefore, in this work, we extract sentences containing only one complex word.\n sent1: Ties are not permitted in simplification ranking.\n sent2: When each annotator assigns a simplification ranking to a substitution list, a tie cannot be assigned in previous datasets #TARGET_REF; #REF) .\n sent3: This deteriorates ranking consistency if some substitutes have a similar simplicity.\n sent4: De #REF allow ties in simplification ranking and report considerably higher agreement among annotators than #REF .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\", \"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "This method estimates the reliability of annotators in addition to determining the true order of rankings.",
                "We applied the reliability score to exclude extraordinary annotators.",
                "Table 1 shows the characteristics of our dataset.",
                "It is about the same size as previous work #TARGET_REF; #REF) .",
                "Our dataset has two advantages: (1) improved correlation with human judgment by making a controlled and balanced dataset, and (2) enhanced consistency by allowing ties in ranking and removing outlier annotators."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: This method estimates the reliability of annotators in addition to determining the true order of rankings.\n sent1: We applied the reliability score to exclude extraordinary annotators.\n sent2: Table 1 shows the characteristics of our dataset.\n sent3: It is about the same size as previous work #TARGET_REF; #REF) .\n sent4: Our dataset has two advantages: (1) improved correlation with human judgment by making a controlled and balanced dataset, and (2) enhanced consistency by allowing ties in ranking and removing outlier annotators.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Latent Dirichlet Allocation (LDA) is a topic modeling technique for textual data [5] that is widely applied in software engineering #TARGET_REF-4, 6, 10, 11, 14-16, 19, 24, 25] for different tasks such as requirements engineering [15] , software architecture [10] , source code analysis [9] , defect reports [16] , testing [14] and to bibliometric analysis of software engineering literature [11, 22] .",
                "A survey on topic modelling in software engineering has been conducted [24] and a book titled \"The art and science of analyzing software data\" [4] devoted a chapter for LDA analysis [6] .",
                "Many sources give methodological guidance on how to apply LDA topic modeling in software engineering [1, 3, 19] .",
                "Given all this, we think it is fair to say that LDA topic modelling is a relevant data analysis technique in empirical software engineering research.",
                "The quality of the resulting topic model can be evaluated with multiple metrics some inspired by mathematics such as the posterior probability of the topic model given the data [13] , perplexity of measure in the test data [13] , or Silhouette coefficient of resulting topics [19] ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Latent Dirichlet Allocation (LDA) is a topic modeling technique for textual data [5] that is widely applied in software engineering #TARGET_REF-4, 6, 10, 11, 14-16, 19, 24, 25] for different tasks such as requirements engineering [15] , software architecture [10] , source code analysis [9] , defect reports [16] , testing [14] and to bibliometric analysis of software engineering literature [11, 22] .\n sent1: A survey on topic modelling in software engineering has been conducted [24] and a book titled \"The art and science of analyzing software data\" [4] devoted a chapter for LDA analysis [6] .\n sent2: Many sources give methodological guidance on how to apply LDA topic modeling in software engineering [1, 3, 19] .\n sent3: Given all this, we think it is fair to say that LDA topic modelling is a relevant data analysis technique in empirical software engineering research.\n sent4: The quality of the resulting topic model can be evaluated with multiple metrics some inspired by mathematics such as the posterior probability of the topic model given the data [13] , perplexity of measure in the test data [13] , or Silhouette coefficient of resulting topics [19] .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "A survey on topic modelling in software engineering has been conducted [24] and a book titled \"The art and science of analyzing software data\" [4] devoted a chapter for LDA analysis [6] .",
                "Many sources give methodological guidance on how to apply LDA topic modeling in software engineering #TARGET_REF, 3, 19] .",
                "Given all this, we think it is fair to say that LDA topic modelling is a relevant data analysis technique in empirical software engineering research.",
                "The quality of the resulting topic model can be evaluated with multiple metrics some inspired by mathematics such as the posterior probability of the topic model given the data [13] , perplexity of measure in the test data [13] , or Silhouette coefficient of resulting topics [19] .",
                "Other target metrics are based on empirical observations such as coherence, which measures topic model quality using word co-occurrences in publicly available texts [23] , or stability which investigates similarity of topics between different runs [1] ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: A survey on topic modelling in software engineering has been conducted [24] and a book titled \"The art and science of analyzing software data\" [4] devoted a chapter for LDA analysis [6] .\n sent1: Many sources give methodological guidance on how to apply LDA topic modeling in software engineering #TARGET_REF, 3, 19] .\n sent2: Given all this, we think it is fair to say that LDA topic modelling is a relevant data analysis technique in empirical software engineering research.\n sent3: The quality of the resulting topic model can be evaluated with multiple metrics some inspired by mathematics such as the posterior probability of the topic model given the data [13] , perplexity of measure in the test data [13] , or Silhouette coefficient of resulting topics [19] .\n sent4: Other target metrics are based on empirical observations such as coherence, which measures topic model quality using word co-occurrences in publicly available texts [23] , or stability which investigates similarity of topics between different runs [1] .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Given all this, we think it is fair to say that LDA topic modelling is a relevant data analysis technique in empirical software engineering research.",
                "The quality of the resulting topic model can be evaluated with multiple metrics some inspired by mathematics such as the posterior probability of the topic model given the data [13] , perplexity of measure in the test data [13] , or Silhouette coefficient of resulting topics [19] .",
                "Other target metrics are based on empirical observations such as coherence, which measures topic model quality using word co-occurrences in publicly available texts [23] , or stability which investigates similarity of topics between different runs #TARGET_REF .",
                "Recently, #REF published a paper titled \"What is wrong with topic modeling? And how to fix it using search-based software engineering\", where they claimed that the instability of topics is one major shortcoming of this technique.",
                "Indeed, studies could result in wrong conclusions if the results are based on instable topics."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Given all this, we think it is fair to say that LDA topic modelling is a relevant data analysis technique in empirical software engineering research.\n sent1: The quality of the resulting topic model can be evaluated with multiple metrics some inspired by mathematics such as the posterior probability of the topic model given the data [13] , perplexity of measure in the test data [13] , or Silhouette coefficient of resulting topics [19] .\n sent2: Other target metrics are based on empirical observations such as coherence, which measures topic model quality using word co-occurrences in publicly available texts [23] , or stability which investigates similarity of topics between different runs #TARGET_REF .\n sent3: Recently, #REF published a paper titled \"What is wrong with topic modeling? And how to fix it using search-based software engineering\", where they claimed that the instability of topics is one major shortcoming of this technique.\n sent4: Indeed, studies could result in wrong conclusions if the results are based on instable topics.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Other target metrics are based on empirical observations such as coherence, which measures topic model quality using word co-occurrences in publicly available texts [23] , or stability which investigates similarity of topics between different runs [1] .",
                "Recently, #TARGET_REF published a paper titled \"What is wrong with topic modeling? And how to fix it using search-based software engineering\", where #TARGET_REF claimed that the instability of topics is one major shortcoming of this technique.",
                "Indeed, studies could result in wrong conclusions if the results are based on instable topics.",
                "They proposed using a differential evolution search algorithm to find the input parameters which maximize the topic model stability measured as the similarity of topics between multiple runs.",
                "This method reduces instability by finding optimal input parameter settings, but only uses the result of one LDA run which can still have some instable topics."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Other target metrics are based on empirical observations such as coherence, which measures topic model quality using word co-occurrences in publicly available texts [23] , or stability which investigates similarity of topics between different runs [1] .\n sent1: Recently, #TARGET_REF published a paper titled \"What is wrong with topic modeling? And how to fix it using search-based software engineering\", where #TARGET_REF claimed that the instability of topics is one major shortcoming of this technique.\n sent2: Indeed, studies could result in wrong conclusions if the results are based on instable topics.\n sent3: They proposed using a differential evolution search algorithm to find the input parameters which maximize the topic model stability measured as the similarity of topics between multiple runs.\n sent4: This method reduces instability by finding optimal input parameter settings, but only uses the result of one LDA run which can still have some instable topics.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Other target metrics are based on empirical observations such as coherence, which measures topic model quality using word co-occurrences in publicly available texts [23] , or stability which investigates similarity of topics between different runs [1] .",
                "Recently, #REF published a paper titled \"What is wrong with topic modeling? And how to fix it using search-based software engineering\", where they claimed that the instability of topics is one major shortcoming of this technique.",
                "Indeed, studies could result in wrong conclusions if the results are based on instable topics.",
                "#TARGET_REF proposed using a differential evolution search algorithm to find the input parameters which maximize the topic model stability measured as the similarity of topics between multiple runs.",
                "This method reduces instability by finding optimal input parameter settings, but only uses the result of one LDA run which can still have some instable topics."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Other target metrics are based on empirical observations such as coherence, which measures topic model quality using word co-occurrences in publicly available texts [23] , or stability which investigates similarity of topics between different runs [1] .\n sent1: Recently, #REF published a paper titled \"What is wrong with topic modeling? And how to fix it using search-based software engineering\", where they claimed that the instability of topics is one major shortcoming of this technique.\n sent2: Indeed, studies could result in wrong conclusions if the results are based on instable topics.\n sent3: #TARGET_REF proposed using a differential evolution search algorithm to find the input parameters which maximize the topic model stability measured as the similarity of topics between multiple runs.\n sent4: This method reduces instability by finding optimal input parameter settings, but only uses the result of one LDA run which can still have some instable topics.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "• For all topics k there is a vector ϕ which is the word distribution for that topic.",
                "Before topic generation, LDA requires that we set the input parameters such as the number of topics k, and hyper priors α and β.",
                "Past work in software engineering has used different techniques to find optimal input parameters such as genetic algorithms [19] or differential evolution #TARGET_REF .",
                "As pointed out in Section 1, what is optimal can be measured with many metrics such as perplexity [13] , stability [1] , or coherence [23] .",
                "The stability of a topic model can be defined as the model's ability to replicate its solutions [8] ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: • For all topics k there is a vector ϕ which is the word distribution for that topic.\n sent1: Before topic generation, LDA requires that we set the input parameters such as the number of topics k, and hyper priors α and β.\n sent2: Past work in software engineering has used different techniques to find optimal input parameters such as genetic algorithms [19] or differential evolution #TARGET_REF .\n sent3: As pointed out in Section 1, what is optimal can be measured with many metrics such as perplexity [13] , stability [1] , or coherence [23] .\n sent4: The stability of a topic model can be defined as the model's ability to replicate its solutions [8] .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Past work in software engineering has used different techniques to find optimal input parameters such as genetic algorithms [19] or differential evolution [1] .",
                "As pointed out in Section 1, what is optimal can be measured with many metrics such as perplexity [13] , stability #TARGET_REF , or coherence [23] .",
                "The stability of a topic model can be defined as the model's ability to replicate its solutions [8] .",
                "Instability (the lack of stability) is caused by the non-deterministic nature of Monte-Carlo simulation that is part of the LDA algorithm [1] .",
                "Past work has shown different stability measures and how to optimize the input parameters to provide a stable topic model [1, 8, 12] ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Past work in software engineering has used different techniques to find optimal input parameters such as genetic algorithms [19] or differential evolution [1] .\n sent1: As pointed out in Section 1, what is optimal can be measured with many metrics such as perplexity [13] , stability #TARGET_REF , or coherence [23] .\n sent2: The stability of a topic model can be defined as the model's ability to replicate its solutions [8] .\n sent3: Instability (the lack of stability) is caused by the non-deterministic nature of Monte-Carlo simulation that is part of the LDA algorithm [1] .\n sent4: Past work has shown different stability measures and how to optimize the input parameters to provide a stable topic model [1, 8, 12] .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Past work in software engineering has used different techniques to find optimal input parameters such as genetic algorithms [19] or differential evolution [1] .",
                "As pointed out in Section 1, what is optimal can be measured with many metrics such as perplexity [13] , stability [1] , or coherence [23] .",
                "The stability of a topic model can be defined as the model's ability to replicate its solutions [8] .",
                "Instability (the lack of stability) is caused by the non-deterministic nature of Monte-Carlo simulation that is part of the LDA algorithm #TARGET_REF .",
                "Past work has shown different stability measures and how to optimize the input parameters to provide a stable topic model [1, 8, 12] ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Past work in software engineering has used different techniques to find optimal input parameters such as genetic algorithms [19] or differential evolution [1] .\n sent1: As pointed out in Section 1, what is optimal can be measured with many metrics such as perplexity [13] , stability [1] , or coherence [23] .\n sent2: The stability of a topic model can be defined as the model's ability to replicate its solutions [8] .\n sent3: Instability (the lack of stability) is caused by the non-deterministic nature of Monte-Carlo simulation that is part of the LDA algorithm #TARGET_REF .\n sent4: Past work has shown different stability measures and how to optimize the input parameters to provide a stable topic model [1, 8, 12] .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The stability of a topic model can be defined as the model's ability to replicate its solutions [8] .",
                "Instability (the lack of stability) is caused by the non-deterministic nature of Monte-Carlo simulation that is part of the LDA algorithm [1] .",
                "Past work has shown different stability measures and how to optimize the input parameters to provide a stable topic model #TARGET_REF, 8, 12] .",
                "We think using the results of a single LDA run, whether optimized for stability or not, is dangerous as perfect stability is impossible to reach.",
                "The next section shows a method that can be used to make more informed decisions."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The stability of a topic model can be defined as the model's ability to replicate its solutions [8] .\n sent1: Instability (the lack of stability) is caused by the non-deterministic nature of Monte-Carlo simulation that is part of the LDA algorithm [1] .\n sent2: Past work has shown different stability measures and how to optimize the input parameters to provide a stable topic model #TARGET_REF, 8, 12] .\n sent3: We think using the results of a single LDA run, whether optimized for stability or not, is dangerous as perfect stability is impossible to reach.\n sent4: The next section shows a method that can be used to make more informed decisions.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Another anomaly is that for two topics with no intersecting top ten words, we would get a better Spearman correlation value than -1 (-0.86).",
                "Third, we can measure Jaccard similarity between the top words of any two topics.",
                "Extended Jaccard measures have been used in LDA stability task optimization #TARGET_REF 12] .",
                "When two topics have all the same top words, the Jaccard similarity would be 1.",
                "On the other hand, the worst case (when all the top words are different) would result in a Jaccard similarity of 0."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Another anomaly is that for two topics with no intersecting top ten words, we would get a better Spearman correlation value than -1 (-0.86).\n sent1: Third, we can measure Jaccard similarity between the top words of any two topics.\n sent2: Extended Jaccard measures have been used in LDA stability task optimization #TARGET_REF 12] .\n sent3: When two topics have all the same top words, the Jaccard similarity would be 1.\n sent4: On the other hand, the worst case (when all the top words are different) would result in a Jaccard similarity of 0.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Past work in software engineering #TARGET_REF and machine learning [12] point out that LDA instability may lead to incorrect conclusions and proposes input parameter optimization to alleviate the problem.",
                "This paper suggests performing replicated runs, clustering the results and measuring the topic stability.",
                "These approach are not alternative but additive.",
                "Our approach can be combined with any LDA optimization technique that relies on input parameter optimization.",
                "Finally, our approach shows topic stability by providing a metric of topic stability and allowing further investigation of the clusters when desired."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Past work in software engineering #TARGET_REF and machine learning [12] point out that LDA instability may lead to incorrect conclusions and proposes input parameter optimization to alleviate the problem.\n sent1: This paper suggests performing replicated runs, clustering the results and measuring the topic stability.\n sent2: These approach are not alternative but additive.\n sent3: Our approach can be combined with any LDA optimization technique that relies on input parameter optimization.\n sent4: Finally, our approach shows topic stability by providing a metric of topic stability and allowing further investigation of the clusters when desired.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "This method reduces instability by finding optimal input parameter settings, but only uses the result of one LDA run which can still have some instable topics.",
                "In this paper, we address the stability of topic models, but rather than optimizing input parameters we propose making stability (or instability) transparent to the user.",
                "We achieve this by performing replicated runs of LDA topic modeling and clustering the results.",
                "Subsequently, we present the clustering results as any topic modeling results by adding an additional metric of stability.",
                "Our method is not an alternative to the ones presented by #TARGET_REF but additive."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: This method reduces instability by finding optimal input parameter settings, but only uses the result of one LDA run which can still have some instable topics.\n sent1: In this paper, we address the stability of topic models, but rather than optimizing input parameters we propose making stability (or instability) transparent to the user.\n sent2: We achieve this by performing replicated runs of LDA topic modeling and clustering the results.\n sent3: Subsequently, we present the clustering results as any topic modeling results by adding an additional metric of stability.\n sent4: Our method is not an alternative to the ones presented by #TARGET_REF but additive.\n",
        "output": "{\"label\": [null], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "We calculate automatic measures for each image-retrieved sentence pair against the five reference descriptions for the original image.",
                "The test data of #REF contains 101 images paired with three reference descriptions.",
                "The images were taken from the PAS-CAL VOC Action Recognition Task, the reference descriptions were collected from Mechanical Turk, and the judgements were also collected from Mechanical Turk.",
                "#TARGET_REF generated two-sentence descriptions for each of the test images using four variants of a slot-filling model, and collected five human judgements of the semantic correctness and grammatical correctness of the description on a scale of 1-5 for each imagedescription pair, resulting in a total of 2,042 human judgement-description pairings.",
                "In this analysis, we use only the first sentence of the description, which describes the event depicted in the image."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: We calculate automatic measures for each image-retrieved sentence pair against the five reference descriptions for the original image.\n sent1: The test data of #REF contains 101 images paired with three reference descriptions.\n sent2: The images were taken from the PAS-CAL VOC Action Recognition Task, the reference descriptions were collected from Mechanical Turk, and the judgements were also collected from Mechanical Turk.\n sent3: #TARGET_REF generated two-sentence descriptions for each of the test images using four variants of a slot-filling model, and collected five human judgements of the semantic correctness and grammatical correctness of the description on a scale of 1-5 for each imagedescription pair, resulting in a total of 2,042 human judgement-description pairings.\n sent4: In this analysis, we use only the first sentence of the description, which describes the event depicted in the image.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "An analysis of the distribution of TER scores in Figure 2 (a) shows that differences in candidate and reference length are prevalent in the image description task.",
                "Unigram BLEU is also only weakly correlated against human judgements, even though it has been reported extensively for this task.",
                "Finally, Meteor is most strongly correlated measure against human judgements.",
                "A similar pattern is observed in the #TARGET_REF data set, though the correlations are lower across all measures.",
                "This could be caused by the smaller sample size or because the descriptions were generated by a computer, and not retrieved from a collection of human-written descriptions containing the goldstandard text, as in the Flickr8K data set."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: An analysis of the distribution of TER scores in Figure 2 (a) shows that differences in candidate and reference length are prevalent in the image description task.\n sent1: Unigram BLEU is also only weakly correlated against human judgements, even though it has been reported extensively for this task.\n sent2: Finally, Meteor is most strongly correlated measure against human judgements.\n sent3: A similar pattern is observed in the #TARGET_REF data set, though the correlations are lower across all measures.\n sent4: This could be caused by the smaller sample size or because the descriptions were generated by a computer, and not retrieved from a collection of human-written descriptions containing the goldstandard text, as in the Flickr8K data set.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Similar inspiration is found in distributed embeddings for state-of-the-art (sota) deep neural networks.",
                "However, wrong combination of hyper-parameters can produce poor quality vectors.",
                "The objective of this work is to show optimal combination of hyper-parameters exists and evaluate various combinations.",
                "We compare them with the original model released by #TARGET_REF.",
                "Both intrinsic and extrinsic (downstream) evaluations, including Named Entity Recognition (NER) and Sentiment Analysis (SA) were carried out."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Similar inspiration is found in distributed embeddings for state-of-the-art (sota) deep neural networks.\n sent1: However, wrong combination of hyper-parameters can produce poor quality vectors.\n sent2: The objective of this work is to show optimal combination of hyper-parameters exists and evaluate various combinations.\n sent3: We compare them with the original model released by #TARGET_REF.\n sent4: Both intrinsic and extrinsic (downstream) evaluations, including Named Entity Recognition (NER) and Sentiment Analysis (SA) were carried out.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Hyper-parameter details of the two networks for the downstream tasks are given in table 2.",
                "The metrics for extrinsic evaluation include F1, precision, recall and accuracy scores.",
                "In both tasks, the default pytorch embedding was tested before being replaced by pre-trained embeddings released by #TARGET_REF and ours.",
                "In each case, the dataset was shuffled before training and split in the ratio 70:15:15 for training, validation (dev) and test sets.",
                "Batch size of 64 was used."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Hyper-parameter details of the two networks for the downstream tasks are given in table 2.\n sent1: The metrics for extrinsic evaluation include F1, precision, recall and accuracy scores.\n sent2: In both tasks, the default pytorch embedding was tested before being replaced by pre-trained embeddings released by #TARGET_REF and ours.\n sent3: In each case, the dataset was shuffled before training and split in the ratio 70:15:15 for training, validation (dev) and test sets.\n sent4: Batch size of 64 was used.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Both intrinsic and extrinsic (downstream) evaluations, including Named Entity Recognition (NER) and Sentiment Analysis (SA) were carried out.",
                "The downstream tasks reveal that the best model is task-specific, high analogy scores don't necessarily correlate positively with F1 scores and the same applies for more data.",
                "Increasing vector dimension size after a point leads to poor quality or performance.",
                "If ethical considerations to save time, energy and the environment are made, then reasonably smaller corpora may do just as well or even better in some cases.",
                "Besides, using a small corpus, we obtain better human-assigned WordSim scores, corresponding Spearman correlation and better downstream (NER & SA) performance compared to #TARGET_REFs model, trained on 100 billion word corpus."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Both intrinsic and extrinsic (downstream) evaluations, including Named Entity Recognition (NER) and Sentiment Analysis (SA) were carried out.\n sent1: The downstream tasks reveal that the best model is task-specific, high analogy scores don't necessarily correlate positively with F1 scores and the same applies for more data.\n sent2: Increasing vector dimension size after a point leads to poor quality or performance.\n sent3: If ethical considerations to save time, energy and the environment are made, then reasonably smaller corpora may do just as well or even better in some cases.\n sent4: Besides, using a small corpus, we obtain better human-assigned WordSim scores, corresponding Spearman correlation and better downstream (NER & SA) performance compared to #TARGET_REFs model, trained on 100 billion word corpus.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Best combination changes when corpus size increases, as will be noticed from table 3.",
                "In terms of analogy score, for 10 epochs, w8s0h0 performs best while w8s1h0 performs best in terms of WordSim and corresponding Spearman correlation.",
                "Meanwhile, increasing the corpus size to BW, w4s1h0 performs best in terms of analogy score while w8s1h0 maintains its position as the best in terms of WordSim and Spearman correlation.",
                "Besides considering quality metrics, it can be observed from table 4 that comparative ratio of values between the models is not commensurate with the results in intrinsic or extrinsic values, especially when we consider the amount of time and energy spent, since more training time results in more energy consumption Information on the length of training time for the released #TARGET_REF model is not readily available.",
                "However, it's interesting to note that their presumed best model, which was released is also s1h0."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Best combination changes when corpus size increases, as will be noticed from table 3.\n sent1: In terms of analogy score, for 10 epochs, w8s0h0 performs best while w8s1h0 performs best in terms of WordSim and corresponding Spearman correlation.\n sent2: Meanwhile, increasing the corpus size to BW, w4s1h0 performs best in terms of analogy score while w8s1h0 maintains its position as the best in terms of WordSim and Spearman correlation.\n sent3: Besides considering quality metrics, it can be observed from table 4 that comparative ratio of values between the models is not commensurate with the results in intrinsic or extrinsic values, especially when we consider the amount of time and energy spent, since more training time results in more energy consumption Information on the length of training time for the released #TARGET_REF model is not readily available.\n sent4: However, it's interesting to note that their presumed best model, which was released is also s1h0.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "This trend is true for all combinations for all tests.",
                "Polynomial interpolation may be used to determine the optimal dimension in both corpora.",
                "Our models are available for confirmation and source codes are available on github.",
                "2 With regards to NER, most pretrained embeddings outperformed the default pytorch embedding, with our BW w4s1h0 model (which is best in BW analogy score) performing best in F1 score and closely followed by #TARGET_REF model.",
                "On the other hand, with regards to SA, pytorch embedding outperformed the pretrained embeddings but was closely followed by our SW w8s0h0 model (which also had the best SW analogy score)."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: This trend is true for all combinations for all tests.\n sent1: Polynomial interpolation may be used to determine the optimal dimension in both corpora.\n sent2: Our models are available for confirmation and source codes are available on github.\n sent3: 2 With regards to NER, most pretrained embeddings outperformed the default pytorch embedding, with our BW w4s1h0 model (which is best in BW analogy score) performing best in F1 score and closely followed by #TARGET_REF model.\n sent4: On the other hand, with regards to SA, pytorch embedding outperformed the pretrained embeddings but was closely followed by our SW w8s0h0 model (which also had the best SW analogy score).\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Our models are available for confirmation and source codes are available on github.",
                "2 With regards to NER, most pretrained embeddings outperformed the default pytorch embedding, with our BW w4s1h0 model (which is best in BW analogy score) performing best in F1 score and closely followed by Mikolov et al. (2013a) model.",
                "On the other hand, with regards to SA, pytorch embedding outperformed the pretrained embeddings but was closely followed by our SW w8s0h0 model (which also had the best SW analogy score).",
                "#TARGET_REF performed second worst of all, despite originating from a very huge corpus.",
                "The combinations w8s0h0 & w4s0h0 of SW performed reasonably well in both extrinsic tasks, just as the default pytorch embedding did."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Our models are available for confirmation and source codes are available on github.\n sent1: 2 With regards to NER, most pretrained embeddings outperformed the default pytorch embedding, with our BW w4s1h0 model (which is best in BW analogy score) performing best in F1 score and closely followed by Mikolov et al. (2013a) model.\n sent2: On the other hand, with regards to SA, pytorch embedding outperformed the pretrained embeddings but was closely followed by our SW w8s0h0 model (which also had the best SW analogy score).\n sent3: #TARGET_REF performed second worst of all, despite originating from a very huge corpus.\n sent4: The combinations w8s0h0 & w4s0h0 of SW performed reasonably well in both extrinsic tasks, just as the default pytorch embedding did.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "There have been many implementations of the word2vec model in either of the two architectures it provides: continuous skipgram and continuous bag of words (CBoW) ( #TARGET_REF ).",
                "Similar distributed models of word or subword embeddings (or vector representations) find usage in sota, deep neural networks like Bidirectional Encoder Representations from Transformers (BERT) and its successors (#REF ; ; #REF ).",
                "These deep networks generate contextual representations of words after been trained for extended periods on large corpora, unsupervised, using the attention mechanisms (#REF ).",
                "It has been observed that various hyper-parameter combinations have been used in different research involving word2vec with the possibility of many of them being sub-optimal (#REF ; #REF ; #REF ).",
                "Therefore, the authors seek to address the research question: what is the optimal combination of word2vec hyperparameters for intrinsic and extrinsic NLP purposes?"
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: There have been many implementations of the word2vec model in either of the two architectures it provides: continuous skipgram and continuous bag of words (CBoW) ( #TARGET_REF ).\n sent1: Similar distributed models of word or subword embeddings (or vector representations) find usage in sota, deep neural networks like Bidirectional Encoder Representations from Transformers (BERT) and its successors (#REF ; ; #REF ).\n sent2: These deep networks generate contextual representations of words after been trained for extended periods on large corpora, unsupervised, using the attention mechanisms (#REF ).\n sent3: It has been observed that various hyper-parameter combinations have been used in different research involving word2vec with the possibility of many of them being sub-optimal (#REF ; #REF ; #REF ).\n sent4: Therefore, the authors seek to address the research question: what is the optimal combination of word2vec hyperparameters for intrinsic and extrinsic NLP purposes?\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Breaking away from the non-distributed (high-dimensional, sparse) representations of words, typical of traditional bag-of-words or one-hot-encoding (#REF ), #TARGET_REF created word2vec.",
                "Word2Vec consists of two shallow neural network architectures: continuous skipgram and CBoW. It uses distributed (low-dimensional, dense) representations of words that group similar words.",
                "This new model traded the complexity of deep neural network architectures, by other researchers, for more efficient training over large corpora.",
                "Its architectures have two training algorithms: negative sampling and hierarchical softmax (Mikolov et al. (2013b) ).",
                "The released model was trained on Google news dataset of 100 billion words."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Breaking away from the non-distributed (high-dimensional, sparse) representations of words, typical of traditional bag-of-words or one-hot-encoding (#REF ), #TARGET_REF created word2vec.\n sent1: Word2Vec consists of two shallow neural network architectures: continuous skipgram and CBoW. It uses distributed (low-dimensional, dense) representations of words that group similar words.\n sent2: This new model traded the complexity of deep neural network architectures, by other researchers, for more efficient training over large corpora.\n sent3: Its architectures have two training algorithms: negative sampling and hierarchical softmax (Mikolov et al. (2013b) ).\n sent4: The released model was trained on Google news dataset of 100 billion words.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Hence, the order of words in the history or future does not influence the averaged vector.",
                "This is similar to the traditional bag-of-words, which is oblivious of the order of words in its sequence.",
                "A loglinear classifier is used in both architectures ( #TARGET_REF ).",
                "In further work, they extended the model to be able to do phrase representations and subsample frequent words (Mikolov et al. (2013b) ).",
                "Being a Neural Network Language Model (NNLM), word2vec assigns probabilities to words in a sequence, like other NNLMs such as feedforward networks or recurrent neural networks (#REF )."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Hence, the order of words in the history or future does not influence the averaged vector.\n sent1: This is similar to the traditional bag-of-words, which is oblivious of the order of words in its sequence.\n sent2: A loglinear classifier is used in both architectures ( #TARGET_REF ).\n sent3: In further work, they extended the model to be able to do phrase representations and subsample frequent words (Mikolov et al. (2013b) ).\n sent4: Being a Neural Network Language Model (NNLM), word2vec assigns probabilities to words in a sequence, like other NNLMs such as feedforward networks or recurrent neural networks (#REF ).\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "It's been shown that word vectors are beneficial for NLP tasks (#REF ), such as sentiment analysis and named entity recognition.",
                "Besides, #TARGET_REF showed with vector space algebra that relationships among words can be evaluated, expressing the quality of vectors produced from the model.",
                "The famous, semantic example: vector(\"King\") -vector(\"Man\") + vector(\"Woman\") ≈ vector(\"Queen\") can be verified using cosine distance.",
                "Another type of semantic meaning is the relationship between a capital city and its corresponding country.",
                "Syntactic relationship examples include plural verbs and past tense, among others."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: It's been shown that word vectors are beneficial for NLP tasks (#REF ), such as sentiment analysis and named entity recognition.\n sent1: Besides, #TARGET_REF showed with vector space algebra that relationships among words can be evaluated, expressing the quality of vectors produced from the model.\n sent2: The famous, semantic example: vector(\"King\") -vector(\"Man\") + vector(\"Woman\") ≈ vector(\"Queen\") can be verified using cosine distance.\n sent3: Another type of semantic meaning is the relationship between a capital city and its corresponding country.\n sent4: Syntactic relationship examples include plural verbs and past tense, among others.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Another type of semantic meaning is the relationship between a capital city and its corresponding country.",
                "Syntactic relationship examples include plural verbs and past tense, among others.",
                "Combination of both syntactic and semantic analyses is possible and provided (totaling over 19,000 questions) as Google analogy test set by #TARGET_REF .",
                "WordSimilarity-353 test set is another analysis tool for word vectors (#REF ).",
                "Unlike Google analogy score, which is based on vector space algebra, WordSimilarity is based on human expert-assigned semantic similarity on two sets of English word pairs."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Another type of semantic meaning is the relationship between a capital city and its corresponding country.\n sent1: Syntactic relationship examples include plural verbs and past tense, among others.\n sent2: Combination of both syntactic and semantic analyses is possible and provided (totaling over 19,000 questions) as Google analogy test set by #TARGET_REF .\n sent3: WordSimilarity-353 test set is another analysis tool for word vectors (#REF ).\n sent4: Unlike Google analogy score, which is based on vector space algebra, WordSimilarity is based on human expert-assigned semantic similarity on two sets of English word pairs.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Hyper-parameters are values which may be manually adjusted and include vector dimension size, type of algorithm and learning rate (#REF ).",
                "#TARGET_REF tried various hyper-parameters with both architectures of #TARGET_REF model, ranging from 50 to 1,000 dimensions, 30,000 to 3,000,000 vocabulary sizes, 1 to 3 epochs, among others.",
                "In our work, we extended research to 3,000 dimensions.",
                "Different observations were noted from the many trials.",
                "They observed diminishing returns after a certain point, despite additional dimensions or larger, unstructured training data."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Hyper-parameters are values which may be manually adjusted and include vector dimension size, type of algorithm and learning rate (#REF ).\n sent1: #TARGET_REF tried various hyper-parameters with both architectures of #TARGET_REF model, ranging from 50 to 1,000 dimensions, 30,000 to 3,000,000 vocabulary sizes, 1 to 3 epochs, among others.\n sent2: In our work, we extended research to 3,000 dimensions.\n sent3: Different observations were noted from the many trials.\n sent4: They observed diminishing returns after a certain point, despite additional dimensions or larger, unstructured training data.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "If some token does not appear in your language model (LM), then it becomes harder for the SMT engine to find its correct position in the translation.",
                "The importance of ParFDA increases with the proliferation of training material available for building SMT systems.",
                "Table 1 presents the statistics of the available training and LM corpora for the constrained (C) systems in WMT15 #TARGET_REF as well as the statistics of the ParFDA selected training and LM data.",
                "ParFDA (Biçici, 2013; Biçici et al., 2014) runs separate FDA5 (Biçici and #REF) models on randomized subsets of the training data and combines the selections afterwards.",
                "FDA5 is available at http://github.com/bicici/FDA."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: If some token does not appear in your language model (LM), then it becomes harder for the SMT engine to find its correct position in the translation.\n sent1: The importance of ParFDA increases with the proliferation of training material available for building SMT systems.\n sent2: Table 1 presents the statistics of the available training and LM corpora for the constrained (C) systems in WMT15 #TARGET_REF as well as the statistics of the ParFDA selected training and LM data.\n sent3: ParFDA (Biçici, 2013; Biçici et al., 2014) runs separate FDA5 (Biçici and #REF) models on randomized subsets of the training data and combines the selections afterwards.\n sent4: FDA5 is available at http://github.com/bicici/FDA.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We run ParFDA SMT experiments using Moses (#REF) in all language pairs in WMT15 #TARGET_REF and obtain SMT performance close to the top constrained Moses systems.",
                "ParFDA allows rapid prototyping of SMT systems for a given target domain or task.",
                "We use ParFDA for selecting parallel training data and LM data for building SMT systems.",
                "We select the LM training data with ParFDA based on the following observation (Biçici, 2013):",
                "No word not appearing in the training set can appear in the translation."
            ],
            "label": [
                "USE",
                "SIMILARITY"
            ]
        },
        "input": "sent0: We run ParFDA SMT experiments using Moses (#REF) in all language pairs in WMT15 #TARGET_REF and obtain SMT performance close to the top constrained Moses systems.\n sent1: ParFDA allows rapid prototyping of SMT systems for a given target domain or task.\n sent2: We use ParFDA for selecting parallel training data and LM data for building SMT systems.\n sent3: We select the LM training data with ParFDA based on the following observation (Biçici, 2013):\n sent4: No word not appearing in the training set can appear in the translation.\n",
        "output": "{\"label\": [\"USE\", \"SIMILARITY\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "No word not appearing in the training set can appear in the translation.",
                "Thus we are only interested in correctly ordering the words appearing in the training corpus and collecting the sentences that contain them for building the LM.",
                "At the same time, a compact and more relevant LM corpus is also useful for modeling longer range dependencies with higher order ngram models.",
                "We use 3-grams for selecting training data and 2-grams for LM corpus selection.",
                "We run ParFDA SMT experiments for all language pairs in both directions in the WMT15 translation task #TARGET_REF , which include English-Czech (en-cs), English-German (en-de), English-Finnish (en-fi), English-French (en-fr), and English-Russian (en-ru)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: No word not appearing in the training set can appear in the translation.\n sent1: Thus we are only interested in correctly ordering the words appearing in the training corpus and collecting the sentences that contain them for building the LM.\n sent2: At the same time, a compact and more relevant LM corpus is also useful for modeling longer range dependencies with higher order ngram models.\n sent3: We use 3-grams for selecting training data and 2-grams for LM corpus selection.\n sent4: We run ParFDA SMT experiments for all language pairs in both directions in the WMT15 translation task #TARGET_REF , which include English-Czech (en-cs), English-German (en-de), English-Finnish (en-fi), English-French (en-fr), and English-Russian (en-ru).\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "If some token does not appear in your language model (LM), then it becomes harder for the SMT engine to find its correct position in the translation.",
                "The importance of ParFDA increases with the proliferation of training material available for building SMT systems.",
                "Table 1 presents the statistics of the available training and LM corpora for the constrained (C) systems in WMT15 #TARGET_REF as well as the statistics of the ParFDA selected training and LM data.",
                "ParFDA (Biçici, 2013; Biçici et al., 2014) runs separate FDA5 (Biçici and #REF ) models on randomized subsets of the training data and combines the selections afterwards.",
                "FDA5 is available at http://github.com/bicici/FDA."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: If some token does not appear in your language model (LM), then it becomes harder for the SMT engine to find its correct position in the translation.\n sent1: The importance of ParFDA increases with the proliferation of training material available for building SMT systems.\n sent2: Table 1 presents the statistics of the available training and LM corpora for the constrained (C) systems in WMT15 #TARGET_REF as well as the statistics of the ParFDA selected training and LM data.\n sent3: ParFDA (Biçici, 2013; Biçici et al., 2014) runs separate FDA5 (Biçici and #REF ) models on randomized subsets of the training data and combines the selections afterwards.\n sent4: FDA5 is available at http://github.com/bicici/FDA.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "ParFDA (Biçici, 2013; Biçici et al., 2014) runs separate FDA5 (Biçici and #REF ) models on randomized subsets of the training data and combines the selections afterwards.",
                "FDA5 is available at http://github.com/bicici/FDA.",
                "We run ParFDA SMT experiments using Moses (#REF) in all language pairs in WMT15 #TARGET_REF and obtain SMT performance close to the top constrained Moses systems.",
                "ParFDA allows rapid prototyping of SMT systems for a given target domain or task.",
                "We use ParFDA for selecting parallel training data and LM data for building SMT systems."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: ParFDA (Biçici, 2013; Biçici et al., 2014) runs separate FDA5 (Biçici and #REF ) models on randomized subsets of the training data and combines the selections afterwards.\n sent1: FDA5 is available at http://github.com/bicici/FDA.\n sent2: We run ParFDA SMT experiments using Moses (#REF) in all language pairs in WMT15 #TARGET_REF and obtain SMT performance close to the top constrained Moses systems.\n sent3: ParFDA allows rapid prototyping of SMT systems for a given target domain or task.\n sent4: We use ParFDA for selecting parallel training data and LM data for building SMT systems.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We run ParFDA SMT experiments for all language pairs in both directions in the WMT15 translation task #TARGET_REF , which include English-Czech (en-cs), English-German (en-de), English-Finnish (en-fi), English-French (en-fr), and English-Russian (en-ru).",
                "We truecase all of the corpora, set the maximum sentence length to 126, use 150-best lists during tuning, set the LM order to a value in [7, 10] for all language pairs, and train the LM using SRILM (#REF) with -unk option.",
                "For GIZA++ (#REF) , max-fertility is set to 10, with the number of iterations set to 7,3,5,5,7 for IBM models 1,2,3,4, and the HMM model, and 70 word Table 1 : Data statistics for the available training and LM corpora in the constrained (C) setting compared with the ParFDA selected training and LM data.",
                "#words is in millions (M) and #sents in thousands (K).",
                "classes are learned over 3 iterations with the mkcls tool during training."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We run ParFDA SMT experiments for all language pairs in both directions in the WMT15 translation task #TARGET_REF , which include English-Czech (en-cs), English-German (en-de), English-Finnish (en-fi), English-French (en-fr), and English-Russian (en-ru).\n sent1: We truecase all of the corpora, set the maximum sentence length to 126, use 150-best lists during tuning, set the LM order to a value in [7, 10] for all language pairs, and train the LM using SRILM (#REF) with -unk option.\n sent2: For GIZA++ (#REF) , max-fertility is set to 10, with the number of iterations set to 7,3,5,5,7 for IBM models 1,2,3,4, and the HMM model, and 70 word Table 1 : Data statistics for the available training and LM corpora in the constrained (C) setting compared with the ParFDA selected training and LM data.\n sent3: #words is in millions (M) and #sents in thousands (K).\n sent4: classes are learned over 3 iterations with the mkcls tool during training.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Natural language processing algorithms, on the other hand, are traditionally constructed in ways that exploit structural sparsity.",
                "Recently, #TARGET_REF presented an approach to GPU parsing that sacrifices traditional sparsity in exchange for raw computational power, obtaining a system that can compute Viterbi parses for a high-quality grammar at about 164 sentences per second on a mid-range GPU.",
                "In this work, we reintroduce sparsity to GPU parsing by adapting a coarse-to-fine pruning approach to the constraints of a GPU.",
                "The resulting system is capable of computing over 404 Viterbi parses per second-more than a 2x speedup-on the same hardware.",
                "Moreover, our approach allows us to efficiently implement less GPU-friendly minimum Bayes risk inference, improving throughput for this more accurate algorithm from only 32 sentences per second unpruned to over 190 sentences per second using pruning-nearly a 6x speedup."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Natural language processing algorithms, on the other hand, are traditionally constructed in ways that exploit structural sparsity.\n sent1: Recently, #TARGET_REF presented an approach to GPU parsing that sacrifices traditional sparsity in exchange for raw computational power, obtaining a system that can compute Viterbi parses for a high-quality grammar at about 164 sentences per second on a mid-range GPU.\n sent2: In this work, we reintroduce sparsity to GPU parsing by adapting a coarse-to-fine pruning approach to the constraints of a GPU.\n sent3: The resulting system is capable of computing over 404 Viterbi parses per second-more than a 2x speedup-on the same hardware.\n sent4: Moreover, our approach allows us to efficiently implement less GPU-friendly minimum Bayes risk inference, improving throughput for this more accurate algorithm from only 32 sentences per second unpruned to over 190 sentences per second using pruning-nearly a 6x speedup.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "First, classic single-core processors and main memory architectures are no longer getting substantially faster over time, so speed gains must now come from parallelism within a single machine.",
                "Second, compared to CPUs, GPUs devote a much larger fraction of their computational power to actual arithmetic.",
                "Since tasks like parsing boil down to repeated read-multiply-write loops, GPUs should be many times more efficient in time, power, or cost.",
                "The challenge is that GPUs are not a good fit for the kinds of sparse computations that most current CPU-based NLP algorithms rely on.",
                "Recently, #TARGET_REF proposed a GPU implementation of a constituency parser that sacrifices all sparsity in exchange for the sheer horsepower that GPUs can provide."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: First, classic single-core processors and main memory architectures are no longer getting substantially faster over time, so speed gains must now come from parallelism within a single machine.\n sent1: Second, compared to CPUs, GPUs devote a much larger fraction of their computational power to actual arithmetic.\n sent2: Since tasks like parsing boil down to repeated read-multiply-write loops, GPUs should be many times more efficient in time, power, or cost.\n sent3: The challenge is that GPUs are not a good fit for the kinds of sparse computations that most current CPU-based NLP algorithms rely on.\n sent4: Recently, #TARGET_REF proposed a GPU implementation of a constituency parser that sacrifices all sparsity in exchange for the sheer horsepower that GPUs can provide.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Using this approach, we see a gain of more than 2x over the dense GPU implementation, resulting in overall speeds of up to 404 sentences per second.",
                "For comparison, the publicly available CPU implementation of #REF parses approximately 7 sentences per second per core on a modern CPU.",
                "A further drawback of the dense approach in #TARGET_REF is that it only computes Viterbi parses.",
                "As with other grammars with a parse/derivation distinction, the grammars of #REF only achieve their full accuracy using minimum-Bayes-risk parsing, with improvements of over 1.5 F1 over best-derivation Viterbi parsing on the Penn Treebank (#REF) .",
                "To that end, we extend our coarse-tofine GPU approach to computing marginals, along the way proposing a new way to exploit the coarse pass to avoid expensive log-domain computations in the fine pass."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Using this approach, we see a gain of more than 2x over the dense GPU implementation, resulting in overall speeds of up to 404 sentences per second.\n sent1: For comparison, the publicly available CPU implementation of #REF parses approximately 7 sentences per second per core on a modern CPU.\n sent2: A further drawback of the dense approach in #TARGET_REF is that it only computes Viterbi parses.\n sent3: As with other grammars with a parse/derivation distinction, the grammars of #REF only achieve their full accuracy using minimum-Bayes-risk parsing, with improvements of over 1.5 F1 over best-derivation Viterbi parsing on the Penn Treebank (#REF) .\n sent4: To that end, we extend our coarse-tofine GPU approach to computing marginals, along the way proposing a new way to exploit the coarse pass to avoid expensive log-domain computations in the fine pass.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "This architecture environment puts very different constraints on parsing algorithms from a CPU environment.",
                "#TARGET_REF proposed an implementation of a PCFG parser that sacrifices standard sparse methods like coarse-to-fine pruning, focusing instead on maximizing the instruction and memory throughput of the parser.",
                "They assume that they are parsing many sentences at once, with throughput being more important than latency.",
                "In this section, we describe their dense algorithm, which we take as the baseline for our work; we present it in a way that sets up the changes to follow.",
                "At the top level, the CPU and GPU communicate via a work queue of parse items of the form (s, i, k, j), where s is an identifier of a sentence, i is the start of a span, k is the split point, and j Table 1 : Performance numbers for computing Viterbi inside charts on 20,000 sentences of length ≤40 from the Penn Treebank."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This architecture environment puts very different constraints on parsing algorithms from a CPU environment.\n sent1: #TARGET_REF proposed an implementation of a PCFG parser that sacrifices standard sparse methods like coarse-to-fine pruning, focusing instead on maximizing the instruction and memory throughput of the parser.\n sent2: They assume that they are parsing many sentences at once, with throughput being more important than latency.\n sent3: In this section, we describe their dense algorithm, which we take as the baseline for our work; we present it in a way that sets up the changes to follow.\n sent4: At the top level, the CPU and GPU communicate via a work queue of parse items of the form (s, i, k, j), where s is an identifier of a sentence, i is the start of a span, k is the split point, and j Table 1 : Performance numbers for computing Viterbi inside charts on 20,000 sentences of length ≤40 from the Penn Treebank.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Speedups are measured in reference to this reimplementation.",
                "See Section 7 for discussion of the clustering algorithms and Section 6 for a description of the pruning methods.",
                "The #TARGET_REF system is benchmarked on a batch size of 1200 sentences, the others on 20,000.",
                "is the end point.",
                "The GPU takes large numbers of parse items and applies the entire grammar to them in parallel."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Speedups are measured in reference to this reimplementation.\n sent1: See Section 7 for discussion of the clustering algorithms and Section 6 for a description of the pruning methods.\n sent2: The #TARGET_REF system is benchmarked on a batch size of 1200 sentences, the others on 20,000.\n sent3: is the end point.\n sent4: The GPU takes large numbers of parse items and applies the entire grammar to them in parallel.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "One important feature of #TARGET_REF 's system is grammar compilation.",
                "Because registers are so much faster than thread-local memory, it is critical to keep as many variables in registers as possible.",
                "One way to accomplish this is to unroll loops at compilation time.",
                "Therefore, they inlined the iteration over the grammar directly into the GPU kernels (i.e. the code itself), which allows the compiler to more effectively use all of its registers.",
                "However, register space is limited on GPUs."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: One important feature of #TARGET_REF 's system is grammar compilation.\n sent1: Because registers are so much faster than thread-local memory, it is critical to keep as many variables in registers as possible.\n sent2: One way to accomplish this is to unroll loops at compilation time.\n sent3: Therefore, they inlined the iteration over the grammar directly into the GPU kernels (i.e. the code itself), which allows the compiler to more effectively use all of its registers.\n sent4: However, register space is limited on GPUs.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "However, register space is limited on GPUs.",
                "Because the Berkeley grammar is so large, the compiler is not able to efficiently schedule all of the operations in the grammar, resulting in register spills.",
                "#TARGET_REF found they had to partition the grammar into multiple different kernels.",
                "We discuss this partitioning in more detail in Section 7.",
                "However, in short, the entire grammar G is broken into multiple clusters G i where each rule belongs to exactly one cluster."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: However, register space is limited on GPUs.\n sent1: Because the Berkeley grammar is so large, the compiler is not able to efficiently schedule all of the operations in the grammar, resulting in register spills.\n sent2: #TARGET_REF found they had to partition the grammar into multiple different kernels.\n sent3: We discuss this partitioning in more detail in Section 7.\n sent4: However, in short, the entire grammar G is broken into multiple clusters G i where each rule belongs to exactly one cluster.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF clustered symbols of the grammar using a sophisticated spectral clustering algorithm to obtain a permutation of the symbols.",
                "Then the rules of the grammar were laid out in a (sparse) three-dimensional tensor, with one dimension representing the parent of the rule, one representing the left child, and one representing the right child.",
                "They then split the cube into 6x2x2 contiguous \"major cubes,\" giving a partition of the rules into 24 clusters.",
                "They then further subdivided these cubes into 2x2x2 minor cubes, giving 8 subclusters that executed in parallel.",
                "Note that the clusters induced by these major and minor cubes need not be of similar sizes; indeed, they often are not."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF clustered symbols of the grammar using a sophisticated spectral clustering algorithm to obtain a permutation of the symbols.\n sent1: Then the rules of the grammar were laid out in a (sparse) three-dimensional tensor, with one dimension representing the parent of the rule, one representing the left child, and one representing the right child.\n sent2: They then split the cube into 6x2x2 contiguous \"major cubes,\" giving a partition of the rules into 24 clusters.\n sent3: They then further subdivided these cubes into 2x2x2 minor cubes, giving 8 subclusters that executed in parallel.\n sent4: Note that the clusters induced by these major and minor cubes need not be of similar sizes; indeed, they often are not.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Apart from the model of #TARGET_REF , there have been a few attempts at using GPUs in NLP contexts before.",
                "#REF and #REF both had early attempts at porting parsing algorithms to the GPU.",
                "However, they did not demonstrate significantly increased speed over a CPU implementation.",
                "In machine translation, #REF adapted algorithms designed for GPUs in the computational biology literature to speed up on-demand phrase table extraction."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Apart from the model of #TARGET_REF , there have been a few attempts at using GPUs in NLP contexts before.\n sent1: #REF and #REF both had early attempts at porting parsing algorithms to the GPU.\n sent2: However, they did not demonstrate significantly increased speed over a CPU implementation.\n sent3: In machine translation, #REF adapted algorithms designed for GPUs in the computational biology literature to speed up on-demand phrase table extraction.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Many of the ideas described here apply to other GPUs (such as those from AMD), but some specifics will differ.",
                "All experiments are run with an NVIDIA GeForce GTX 680, a mid-range GPU that costs around $500 at time of writing.",
                "Unless otherwise noted, all experiments are conducted on sentences of length ≤ 40 words, and we estimate times based on batches of 20K sentences.",
                "1 We should note that our experimental condition differs from that of #TARGET_REF : they evaluate on sentences of length ≤ 30.",
                "Furthermore, they 1 The implementation of #REF cannot handle batches so large, and so we tested it on batches of 1200 sentences."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Many of the ideas described here apply to other GPUs (such as those from AMD), but some specifics will differ.\n sent1: All experiments are run with an NVIDIA GeForce GTX 680, a mid-range GPU that costs around $500 at time of writing.\n sent2: Unless otherwise noted, all experiments are conducted on sentences of length ≤ 40 words, and we estimate times based on batches of 20K sentences.\n sent3: 1 We should note that our experimental condition differs from that of #TARGET_REF : they evaluate on sentences of length ≤ 30.\n sent4: Furthermore, they 1 The implementation of #REF cannot handle batches so large, and so we tested it on batches of 1200 sentences.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Unless otherwise noted, all experiments are conducted on sentences of length ≤ 40 words, and we estimate times based on batches of 20K sentences.",
                "1 We should note that our experimental condition differs from that of #REF : they evaluate on sentences of length ≤ 30.",
                "Furthermore, they 1 The implementation of #TARGET_REF cannot handle batches so large, and so we tested it on batches of 1200 sentences.",
                "Our reimplementation is approximately the same speed for the same batch sizes.",
                "For batches of 20K sentences, we used sentences from the training set."
            ],
            "label": [
                "DIFFERENCES",
                "USE"
            ]
        },
        "input": "sent0: Unless otherwise noted, all experiments are conducted on sentences of length ≤ 40 words, and we estimate times based on batches of 20K sentences.\n sent1: 1 We should note that our experimental condition differs from that of #REF : they evaluate on sentences of length ≤ 30.\n sent2: Furthermore, they 1 The implementation of #TARGET_REF cannot handle batches so large, and so we tested it on batches of 1200 sentences.\n sent3: Our reimplementation is approximately the same speed for the same batch sizes.\n sent4: For batches of 20K sentences, we used sentences from the training set.\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Our system uses a coarse-to-fine approach, where the coarse pass computes a pruning mask that is used by the CPU when deciding which items to queue during the fine pass.",
                "The original system of #TARGET_REF only used the fine pass, with no pruning.",
                "instructions in lockstep, differing only in their input data.",
                "Thus sparsely skipping rules and symbols will not save any work.",
                "Indeed, it may actually slow the system down."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Our system uses a coarse-to-fine approach, where the coarse pass computes a pruning mask that is used by the CPU when deciding which items to queue during the fine pass.\n sent1: The original system of #TARGET_REF only used the fine pass, with no pruning.\n sent2: instructions in lockstep, differing only in their input data.\n sent3: Thus sparsely skipping rules and symbols will not save any work.\n sent4: Indeed, it may actually slow the system down.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Here, the rules of the grammar are clustered by their coarse parent symbol.",
                "We then have multiple work queues, with parse items only being enqueued if the span (i, j) allows that symbol in its pruning mask.",
                "All in all, #TARGET_REF 's system is able to compute Viterbi charts at 164 sentences per second, for sentences up to length 40.",
                "On larger batch sizes, our reimplementation of their approach is able to achieve 193 sentences per second on the same hardware.",
                "(See Table 1 .) 6 Pruning on a GPU Now we turn to the algorithmic and architectural changes in our approach."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Here, the rules of the grammar are clustered by their coarse parent symbol.\n sent1: We then have multiple work queues, with parse items only being enqueued if the span (i, j) allows that symbol in its pruning mask.\n sent2: All in all, #TARGET_REF 's system is able to compute Viterbi charts at 164 sentences per second, for sentences up to length 40.\n sent3: On larger batch sizes, our reimplementation of their approach is able to achieve 193 sentences per second on the same hardware.\n sent4: (See Table 1 .) 6 Pruning on a GPU Now we turn to the algorithmic and architectural changes in our approach.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "We use a very simple method: we cluster the rules in the grammar by coarse parent symbol.",
                "When coarse symbols are extremely unlikely (and therefore have few corresponding rules), we merge their clusters to avoid the overhead of beginning work on clusters where little work has to be done.",
                "4 In order to subcluster, we divide up rules among subclusters so that each subcluster has the same number of active parent symbols.",
                "We found this approach to subclustering worked well in practice.",
                "Clustering using this method is labeled 'Parent' in Table 1 . Now, when we use a coarse pruning pass, we are able to parse nearly 280 sentences per second, a 70% increase in parsing performance relative to #TARGET_REF 's system, and nearly 50% over our reimplemented baseline."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: We use a very simple method: we cluster the rules in the grammar by coarse parent symbol.\n sent1: When coarse symbols are extremely unlikely (and therefore have few corresponding rules), we merge their clusters to avoid the overhead of beginning work on clusters where little work has to be done.\n sent2: 4 In order to subcluster, we divide up rules among subclusters so that each subcluster has the same number of active parent symbols.\n sent3: We found this approach to subclustering worked well in practice.\n sent4: Clustering using this method is labeled 'Parent' in Table 1 . Now, when we use a coarse pruning pass, we are able to parse nearly 280 sentences per second, a 70% increase in parsing performance relative to #TARGET_REF 's system, and nearly 50% over our reimplemented baseline.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "We found this approach to subclustering worked well in practice.",
                "Clustering using this method is labeled 'Parent' in Table 1 . Now, when we use a coarse pruning pass, we are able to parse nearly 280 sentences per second, a 70% increase in parsing performance relative to #REF 's system, and nearly 50% over our reimplemented baseline.",
                "It turns out that this simple clustering algorithm produces relatively efficient kernels even in the unpruned case.",
                "The unpruned Viterbi computations in a fine grammar using the clustering method of #TARGET_REF yields a speed of 193 sentences per second, whereas the same computation using coarse parent clustering has a speed of 159 sentences per second.",
                "(See Table 1 .) This is not as efficient as #REF 's highly tuned method, but it is still fairly fast, and much simpler to implement."
            ],
            "label": [
                "DIFFERENCES",
                "USE"
            ]
        },
        "input": "sent0: We found this approach to subclustering worked well in practice.\n sent1: Clustering using this method is labeled 'Parent' in Table 1 . Now, when we use a coarse pruning pass, we are able to parse nearly 280 sentences per second, a 70% increase in parsing performance relative to #REF 's system, and nearly 50% over our reimplemented baseline.\n sent2: It turns out that this simple clustering algorithm produces relatively efficient kernels even in the unpruned case.\n sent3: The unpruned Viterbi computations in a fine grammar using the clustering method of #TARGET_REF yields a speed of 193 sentences per second, whereas the same computation using coarse parent clustering has a speed of 159 sentences per second.\n sent4: (See Table 1 .) This is not as efficient as #REF 's highly tuned method, but it is still fairly fast, and much simpler to implement.\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "We found this approach to subclustering worked well in practice.",
                "Clustering using this method is labeled 'Parent' in Table 1 . Now, when we use a coarse pruning pass, we are able to parse nearly 280 sentences per second, a 70% increase in parsing performance relative to #REF 's system, and nearly 50% over our reimplemented baseline.",
                "It turns out that this simple clustering algorithm produces relatively efficient kernels even in the unpruned case.",
                "The unpruned Viterbi computations in a fine grammar using the clustering method of #REF yields a speed of 193 sentences per second, whereas the same computation using coarse parent clustering has a speed of 159 sentences per second.",
                "(See Table 1 .) This is not as efficient as #TARGET_REF 's highly tuned method, but it is still fairly fast, and much simpler to implement."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: We found this approach to subclustering worked well in practice.\n sent1: Clustering using this method is labeled 'Parent' in Table 1 . Now, when we use a coarse pruning pass, we are able to parse nearly 280 sentences per second, a 70% increase in parsing performance relative to #REF 's system, and nearly 50% over our reimplemented baseline.\n sent2: It turns out that this simple clustering algorithm produces relatively efficient kernels even in the unpruned case.\n sent3: The unpruned Viterbi computations in a fine grammar using the clustering method of #REF yields a speed of 193 sentences per second, whereas the same computation using coarse parent clustering has a speed of 159 sentences per second.\n sent4: (See Table 1 .) This is not as efficient as #TARGET_REF 's highly tuned method, but it is still fairly fast, and much simpler to implement.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Second, the pruning mask for the children (i, k) and (k, j) must be non-empty.",
                "Once on the GPU, parse items are processed using the same style of compiled kernel as in #TARGET_REF .",
                "Because the entire partition (though not necessarily the entire grammar) is applied to each item in the queue, we still do not need to worry about warp divergence.",
                "At the top level, our system first computes pruning masks with a coarse grammar.",
                "Then it processes the same sentences with the fine grammar."
            ],
            "label": [
                "USE",
                "SIMILARITY"
            ]
        },
        "input": "sent0: Second, the pruning mask for the children (i, k) and (k, j) must be non-empty.\n sent1: Once on the GPU, parse items are processed using the same style of compiled kernel as in #TARGET_REF .\n sent2: Because the entire partition (though not necessarily the entire grammar) is applied to each item in the queue, we still do not need to worry about warp divergence.\n sent3: At the top level, our system first computes pruning masks with a coarse grammar.\n sent4: Then it processes the same sentences with the fine grammar.\n",
        "output": "{\"label\": [\"USE\", \"SIMILARITY\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Beyond linear mappings, #REF apply deep canonical correlation analysis to learn a nonlinear transformation for each language.",
                "Finally, additional techniques have been used to address the hubness problem in Mikolov et al. (2013b) , both through the neighbor retrieval method and the training itself .",
                "We leave the study of non-linear transformation and other additions for further work.",
                "In this paper, we propose a general framework to learn bilingual word embeddings.",
                "We start with a basic optimization objective (#REFb) and introduce several meaningful and intuitive constraints that are equivalent or closely related to previously proposed methods (#REF; #TARGET_REF ."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: Beyond linear mappings, #REF apply deep canonical correlation analysis to learn a nonlinear transformation for each language.\n sent1: Finally, additional techniques have been used to address the hubness problem in Mikolov et al. (2013b) , both through the neighbor retrieval method and the training itself .\n sent2: We leave the study of non-linear transformation and other additions for further work.\n sent3: In this paper, we propose a general framework to learn bilingual word embeddings.\n sent4: We start with a basic optimization objective (#REFb) and introduce several meaningful and intuitive constraints that are equivalent or closely related to previously proposed methods (#REF; #TARGET_REF .\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "In all the cases, length normalization and mean centering were applied to all embeddings, even if missing from the dictionary.",
                "The results show that the orthogonality constraint is key to preserve monolingual performance, and it also improves bilingual performance by enforcing a relevant property (monolingual invariance) that the transformation to learn should intuitively have.",
                "The contribution of length normalization alone is marginal, but when followed by mean centering we obtain further improvements in bilingual performance without hurting monolingual performance.",
                "Table 2 shows the results for our best performing configuration in comparison to previous work.",
                "As discussed before, (#REFb) and #TARGET_REF were implemented as part of our framework, so they correspond to our uncostrained mapping with no preprocessing and orthogonal mapping with length normalization, respectively."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: In all the cases, length normalization and mean centering were applied to all embeddings, even if missing from the dictionary.\n sent1: The results show that the orthogonality constraint is key to preserve monolingual performance, and it also improves bilingual performance by enforcing a relevant property (monolingual invariance) that the transformation to learn should intuitively have.\n sent2: The contribution of length normalization alone is marginal, but when followed by mean centering we obtain further improvements in bilingual performance without hurting monolingual performance.\n sent3: Table 2 shows the results for our best performing configuration in comparison to previous work.\n sent4: As discussed before, (#REFb) and #TARGET_REF were implemented as part of our framework, so they correspond to our uncostrained mapping with no preprocessing and orthogonal mapping with length normalization, respectively.\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Since the original embeddings are the same in all the cases and it is only the transformation that is applied to them that changes, this affects all the methods in the exact same way, so the results are perfectly comparable among themselves.",
                "With these settings, we obtain a coverage of 64.98%.",
                "We implemented the proposed method in Python using NumPy, and make it available as an open source project 5 .",
                "The code for Mikolov et al. (2013b) and #TARGET_REF is not publicly available, so we implemented and tested them as part of the proposed framework, which only differs from the original systems in the optimization method (exact solution instead of gradient descent) and the length normalization approach in the case of #TARGET_REF (postprocessing instead of constrained training).",
                "As for the method by #REF , we used their original implementation in Python and MAT-LAB 6 , which we extended to cover cases where the dictionary contains more than one entry for the same word."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Since the original embeddings are the same in all the cases and it is only the transformation that is applied to them that changes, this affects all the methods in the exact same way, so the results are perfectly comparable among themselves.\n sent1: With these settings, we obtain a coverage of 64.98%.\n sent2: We implemented the proposed method in Python using NumPy, and make it available as an open source project 5 .\n sent3: The code for Mikolov et al. (2013b) and #TARGET_REF is not publicly available, so we implemented and tested them as part of the proposed framework, which only differs from the original systems in the optimization method (exact solution instead of gradient descent) and the length normalization approach in the case of #TARGET_REF (postprocessing instead of constrained training).\n sent4: As for the method by #REF , we used their original implementation in Python and MAT-LAB 6 , which we extended to cover cases where the dictionary contains more than one entry for the same word.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "This paper develops a new framework to learn bilingual word embedding mappings, generalizing previous work and providing an efficient exact method to learn the optimal transformation.",
                "Our experiments show the effectiveness of the proposed model and give strong empirical evidence in favor of our reinterpretation of #TARGET_REF and #REF .",
                "It is the proposed method with the orthogonality constraint and a global preprocessing with length normalization and dimension-wise mean centering that achieves the best overall results both in monolingual and bilingual terms, surpassing those previous methods.",
                "In the future, we would like to study non-linear mappings (#REF) and the additional techniques in .",
                "ish Ministry of Economy and Competitiveness (TADEEP TIN2015-70214-P)."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: This paper develops a new framework to learn bilingual word embedding mappings, generalizing previous work and providing an efficient exact method to learn the optimal transformation.\n sent1: Our experiments show the effectiveness of the proposed model and give strong empirical evidence in favor of our reinterpretation of #TARGET_REF and #REF .\n sent2: It is the proposed method with the orthogonality constraint and a global preprocessing with length normalization and dimension-wise mean centering that achieves the best overall results both in monolingual and bilingual terms, surpassing those previous methods.\n sent3: In the future, we would like to study non-linear mappings (#REF) and the additional techniques in .\n sent4: ish Ministry of Economy and Competitiveness (TADEEP TIN2015-70214-P).\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Visual conversational agents (#REFa; #TARGET_REF; ) are AI agents Figure 1 : A human and an AI (a visual conversation agent called ALICE) play the proposed GuessWhich game.",
                "At the start of the game (top), ALICE is provided an image (shown above ALICE) which is unknown to the human.",
                "Both ALICE and the human are then provided a brief description of the image.",
                "The human then attempts to identify the secret image.",
                "In each subsequent round of dialog, the human asks a question about the unknown image, receives an answer from ALICE, and makes a best guess of the secret image from a fixed pool of images."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Visual conversational agents (#REFa; #TARGET_REF; ) are AI agents Figure 1 : A human and an AI (a visual conversation agent called ALICE) play the proposed GuessWhich game.\n sent1: At the start of the game (top), ALICE is provided an image (shown above ALICE) which is unknown to the human.\n sent2: Both ALICE and the human are then provided a brief description of the image.\n sent3: The human then attempts to identify the secret image.\n sent4: In each subsequent round of dialog, the human asks a question about the unknown image, receives an answer from ALICE, and makes a best guess of the secret image from a fixed pool of images.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "These agents are typically trained to mimic large corpora of human-human dialogs and are evaluated automatically on how well they retrieve actual human responses (ground truth) in novel dialogs.",
                "Recent work has evaluated these models more pragmatically by evaluating how well pairs of visual conversational agents perform on goal-based conversational tasks rather than response retrieval from fixed dialogs.",
                "Specifically, #TARGET_REF ) train two visual conversational agents -a questioning bot QBOT, and an answering bot ABOT -for an image-guessing task.",
                "Starting from a description of the scene, QBOT and ABOT converse over multiple rounds of questions (QBOT) and answers (ABOT) in order to improve QBOT's understanding of a secret image known only to ABOT.",
                "After a fixed number of rounds, QBOT must guess the secret image from a large pool and both QBOT and ABOT are evaluated based on this guess."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: These agents are typically trained to mimic large corpora of human-human dialogs and are evaluated automatically on how well they retrieve actual human responses (ground truth) in novel dialogs.\n sent1: Recent work has evaluated these models more pragmatically by evaluating how well pairs of visual conversational agents perform on goal-based conversational tasks rather than response retrieval from fixed dialogs.\n sent2: Specifically, #TARGET_REF ) train two visual conversational agents -a questioning bot QBOT, and an answering bot ABOT -for an image-guessing task.\n sent3: Starting from a description of the scene, QBOT and ABOT converse over multiple rounds of questions (QBOT) and answers (ABOT) in order to improve QBOT's understanding of a secret image known only to ABOT.\n sent4: After a fixed number of rounds, QBOT must guess the secret image from a large pool and both QBOT and ABOT are evaluated based on this guess.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Starting from a description of the scene, QBOT and ABOT converse over multiple rounds of questions (QBOT) and answers (ABOT) in order to improve QBOT's understanding of a secret image known only to ABOT.",
                "After a fixed number of rounds, QBOT must guess the secret image from a large pool and both QBOT and ABOT are evaluated based on this guess.",
                "#TARGET_REF ) compare supervised baseline models with QBOT-ABOT teams trained through reinforcement learning based self-talk on this image-guessing task.",
                "They find that the AI-AI teams improve significantly at guessing the correct image after self-talk updates compared to the supervised pretraining.",
                "While these results indicate that the self-talk fine-tuned agents are better visual conversational agents, crucially, it remains unclear if these agents are indeed better at this task when interacting with humans."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Starting from a description of the scene, QBOT and ABOT converse over multiple rounds of questions (QBOT) and answers (ABOT) in order to improve QBOT's understanding of a secret image known only to ABOT.\n sent1: After a fixed number of rounds, QBOT must guess the secret image from a large pool and both QBOT and ABOT are evaluated based on this guess.\n sent2: #TARGET_REF ) compare supervised baseline models with QBOT-ABOT teams trained through reinforcement learning based self-talk on this image-guessing task.\n sent3: They find that the AI-AI teams improve significantly at guessing the correct image after self-talk updates compared to the supervised pretraining.\n sent4: While these results indicate that the self-talk fine-tuned agents are better visual conversational agents, crucially, it remains unclear if these agents are indeed better at this task when interacting with humans.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Given that our goal is to evaluate visual conversational agents through a human computation game, we draw connections to relevant work on visual conversational agents, human computation games, and dialog evaluation below.",
                "Visual Conversational Agents.",
                "Our AI agents are visual conversational models, which have recently emerged as a popular research area in visually-grounded language modeling (#REFa; #TARGET_REF; . (#REFa ) introduced the task of Visual Dialog and collected the VisDial dataset by pairing subjects on Amazon Mechanical Turk (AMT) to chat about an image (with assigned roles of questioner and answerer).",
                "(#REFb ) pre-trained questioner and answerer agents on this VisDial dataset via supervised learning and fine-tuned them via self-talk (reinforcement learning), observing that RL-fine-tuned QBOT-ABOT are better at image-guessing after interacting with each other.",
                "However, (#REF; Chamberlain, Poesio, and #REF) , movies (#REF) etc."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Given that our goal is to evaluate visual conversational agents through a human computation game, we draw connections to relevant work on visual conversational agents, human computation games, and dialog evaluation below.\n sent1: Visual Conversational Agents.\n sent2: Our AI agents are visual conversational models, which have recently emerged as a popular research area in visually-grounded language modeling (#REFa; #TARGET_REF; . (#REFa ) introduced the task of Visual Dialog and collected the VisDial dataset by pairing subjects on Amazon Mechanical Turk (AMT) to chat about an image (with assigned roles of questioner and answerer).\n sent3: (#REFb ) pre-trained questioner and answerer agents on this VisDial dataset via supervised learning and fine-tuned them via self-talk (reinforcement learning), observing that RL-fine-tuned QBOT-ABOT are better at image-guessing after interacting with each other.\n sent4: However, (#REF; Chamberlain, Poesio, and #REF) , movies (#REF) etc.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Our AI agents are visual conversational models, which have recently emerged as a popular research area in visually-grounded language modeling (#REFa; #REFb; . (#REFa ) introduced the task of Visual Dialog and collected the VisDial dataset by pairing subjects on Amazon Mechanical Turk (AMT) to chat about an image (with assigned roles of questioner and answerer).",
                "#TARGET_REF ) pre-trained questioner and answerer agents on this VisDial dataset via supervised learning and fine-tuned them via self-talk (reinforcement learning), observing that RL-fine-tuned QBOT-ABOT are better at image-guessing after interacting with each other.",
                "However, (#REF; Chamberlain, Poesio, and #REF) , movies (#REF) etc.",
                "While such games have traditionally focused on human-human collaboration, we extend these ideas to human-AI teams.",
                "Rather than collecting labeled data, our game is designed to measure the effectiveness of the AI in the context of human-AI teams."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Our AI agents are visual conversational models, which have recently emerged as a popular research area in visually-grounded language modeling (#REFa; #REFb; . (#REFa ) introduced the task of Visual Dialog and collected the VisDial dataset by pairing subjects on Amazon Mechanical Turk (AMT) to chat about an image (with assigned roles of questioner and answerer).\n sent1: #TARGET_REF ) pre-trained questioner and answerer agents on this VisDial dataset via supervised learning and fine-tuned them via self-talk (reinforcement learning), observing that RL-fine-tuned QBOT-ABOT are better at image-guessing after interacting with each other.\n sent2: However, (#REF; Chamberlain, Poesio, and #REF) , movies (#REF) etc.\n sent3: While such games have traditionally focused on human-human collaboration, we extend these ideas to human-AI teams.\n sent4: Rather than collecting labeled data, our game is designed to measure the effectiveness of the AI in the context of human-AI teams.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "At the other end of the spectrum, free-form conversation models are often evaluated by metrics that rely on n-gram overlaps, such as BLEU, METEOR, ROUGE, but these have been shown to correlate poorly with human judgment (#REF) .",
                "Human evaluation of conversations is typically in the format where humans rate the quality of machine utterances given context, without actually taking part in the conversation, as in #TARGET_REF ) and (#REF) .",
                "To the best of our knowledge, we are the first to evaluate conversational models via team performance where humans are continuously interacting with agents to succeed at a downstream task.",
                "Turing Test.",
                "Finally, our GuessWhich game is in line with ideas in (#REF), re-imagining the traditional Turing Test for state-of-the-art AI systems, taking the pragmatic view that an effective AI teammate need not appear humanlike, act or be mistaken for one, provided its behavior does not feel jarring or baffle teammates, leaving them wondering not about what it is thinking but whether it is."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: At the other end of the spectrum, free-form conversation models are often evaluated by metrics that rely on n-gram overlaps, such as BLEU, METEOR, ROUGE, but these have been shown to correlate poorly with human judgment (#REF) .\n sent1: Human evaluation of conversations is typically in the format where humans rate the quality of machine utterances given context, without actually taking part in the conversation, as in #TARGET_REF ) and (#REF) .\n sent2: To the best of our knowledge, we are the first to evaluate conversational models via team performance where humans are continuously interacting with agents to succeed at a downstream task.\n sent3: Turing Test.\n sent4: Finally, our GuessWhich game is in line with ideas in (#REF), re-imagining the traditional Turing Test for state-of-the-art AI systems, taking the pragmatic view that an effective AI teammate need not appear humanlike, act or be mistaken for one, provided its behavior does not feel jarring or baffle teammates, leaving them wondering not about what it is thinking but whether it is.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "For completeness, we will review this work in this section.",
                "#TARGET_REF ) formulate a self-supervised imageguessing task between a questioner bot (QBOT) and an answerer bot (ABOT) which plays out over multiple rounds of dialog.",
                "At the start of the task, QBOT and ABOT are shown a one sentence description (i.e. a caption) of an image (unknown to QBOT).",
                "The pair can then engage in question and answer based dialog for a fixed number of iterations after which QBOT must try to select the secret image from a pool.",
                "The goal of the QBOT-ABOT team is two-fold, QBOT should: 1) build a mental model of the unseen image purely from the dialog and 2) be able to retrieve that image from a line-up of images."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For completeness, we will review this work in this section.\n sent1: #TARGET_REF ) formulate a self-supervised imageguessing task between a questioner bot (QBOT) and an answerer bot (ABOT) which plays out over multiple rounds of dialog.\n sent2: At the start of the task, QBOT and ABOT are shown a one sentence description (i.e. a caption) of an image (unknown to QBOT).\n sent3: The pair can then engage in question and answer based dialog for a fixed number of iterations after which QBOT must try to select the secret image from a pool.\n sent4: The goal of the QBOT-ABOT team is two-fold, QBOT should: 1) build a mental model of the unseen image purely from the dialog and 2) be able to retrieve that image from a line-up of images.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We compare the performance of the two agents ALICE SL and ALICE RL in the GuessWhich game.",
                "These bots are state-ofthe-art visual dialog agents with respect to emulating human responses and generating visually discriminative responses in AI-AI dialog.",
                "#TARGET_REF ) evaluate these agents against strong baselines and report AI-AI team results that are significantly better than chance on a pool of ∼10k images (rank ∼1000 for SL, rank ∼500 for RL).",
                "In addition to evaluating them in the context of human-AI teams we also report QBOT-ALICE team performances for reference.",
                "In Table 1 , we compare the performances of human-ALICE SL and human-ALICE RL teams according to Mean Rank (MR) and Mean Reciprocal Rank (MRR) of the secret image based on the guesses H makes at the end of dialog."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: We compare the performance of the two agents ALICE SL and ALICE RL in the GuessWhich game.\n sent1: These bots are state-ofthe-art visual dialog agents with respect to emulating human responses and generating visually discriminative responses in AI-AI dialog.\n sent2: #TARGET_REF ) evaluate these agents against strong baselines and report AI-AI team results that are significantly better than chance on a pool of ∼10k images (rank ∼1000 for SL, rank ∼500 for RL).\n sent3: In addition to evaluating them in the context of human-AI teams we also report QBOT-ALICE team performances for reference.\n sent4: In Table 1 , we compare the performances of human-ALICE SL and human-ALICE RL teams according to Mean Rank (MR) and Mean Reciprocal Rank (MRR) of the secret image based on the guesses H makes at the end of dialog.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "While these results indicate that the self-talk fine-tuned agents are better visual conversational agents, crucially, it remains unclear if these agents are indeed better at this task when interacting with humans.",
                "GuessWhich.",
                "In this work, we propose to evaluate if and how this progress in AI-AI evaluation translates to the performance of human-AI teams.",
                "Inspired by the popular GuessWhat or 20-Questions game, we design a human computation game -GuessWhich -which requires collaboration between human and visual conversational AI agents.",
                "Mirroring the setting of #TARGET_REF , GuessWhich is an image-guessing game that consists of 2 participants -questioner and answerer."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: While these results indicate that the self-talk fine-tuned agents are better visual conversational agents, crucially, it remains unclear if these agents are indeed better at this task when interacting with humans.\n sent1: GuessWhich.\n sent2: In this work, we propose to evaluate if and how this progress in AI-AI evaluation translates to the performance of human-AI teams.\n sent3: Inspired by the popular GuessWhat or 20-Questions game, we design a human computation game -GuessWhich -which requires collaboration between human and visual conversational AI agents.\n sent4: Mirroring the setting of #TARGET_REF , GuessWhich is an image-guessing game that consists of 2 participants -questioner and answerer.\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "The questioner interacts with the answerer for a fixed number of rounds of question-answer (dialog) to identify the secret image from a fixed pool of images (see Fig. 1 ).",
                "We evaluate human-AI team performance in GuessWhich, for the setting where the questioner is a human and the answerer is an AI (that we denote ALICE).",
                "Specifically, we evaluate two versions of ALICE for GuessWhich:",
                "1. ALICE SL which is trained in a supervised manner on the Visual Dialog dataset (#REFa ) to mimic the answers given by humans when engaged in a conversation with other humans about an image, and 2.",
                "ALICE RL which is pre-trained with supervised learning and fine-tuned via reinforcement learning for an imageguessing task as in #TARGET_REF ."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: The questioner interacts with the answerer for a fixed number of rounds of question-answer (dialog) to identify the secret image from a fixed pool of images (see Fig. 1 ).\n sent1: We evaluate human-AI team performance in GuessWhich, for the setting where the questioner is a human and the answerer is an AI (that we denote ALICE).\n sent2: Specifically, we evaluate two versions of ALICE for GuessWhich:\n sent3: 1. ALICE SL which is trained in a supervised manner on the Visual Dialog dataset (#REFa ) to mimic the answers given by humans when engaged in a conversation with other humans about an image, and 2.\n sent4: ALICE RL which is pre-trained with supervised learning and fine-tuned via reinforcement learning for an imageguessing task as in #TARGET_REF .\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent1\", \"sent2\", \"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "ALICE is assigned a secret image and answers questions asked about that image from a human for 9 rounds to help them identify the secret image (Sec. 4).",
                "• We evaluate human-AI team performance on this game for both supervised learning (SL) and reinforcement learning (RL) versions of ALICE.",
                "Our main experimental finding is that despite significant differences between SL and RL agents reported in previous work #TARGET_REF , we find no significant difference in performance between ALICE SL or ALICE RL when paired with human partners (Sec. 6.1).",
                "This suggests that while self-talk and RL are interesting directions to pursue for building better visual conversational agents, there appears to be a disconnect between AI-AI and human-AI evaluations -progress on former does not seem predictive of progress on latter.",
                "This is an important finding to guide future research."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: ALICE is assigned a secret image and answers questions asked about that image from a human for 9 rounds to help them identify the secret image (Sec. 4).\n sent1: • We evaluate human-AI team performance on this game for both supervised learning (SL) and reinforcement learning (RL) versions of ALICE.\n sent2: Our main experimental finding is that despite significant differences between SL and RL agents reported in previous work #TARGET_REF , we find no significant difference in performance between ALICE SL or ALICE RL when paired with human partners (Sec. 6.1).\n sent3: This suggests that while self-talk and RL are interesting directions to pursue for building better visual conversational agents, there appears to be a disconnect between AI-AI and human-AI evaluations -progress on former does not seem predictive of progress on latter.\n sent4: This is an important finding to guide future research.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We also observe that ALICE RL outperforms ALICE SL on the MRR metric.",
                "On both metrics, however, the differences are within the standard error margins (reported in the table) and not statisti- Table 1 : Performance of Human-ALICE teams with AL-ICE SL and ALICE RL measured by MR (lower is better) and MRR (higher is better).",
                "Error bars are 95% CIs from 1000 bootstrap samples.",
                "Unlike #TARGET_REF , we find no significant difference between ALICE SL and ALICE RL .",
                "cally significant."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: We also observe that ALICE RL outperforms ALICE SL on the MRR metric.\n sent1: On both metrics, however, the differences are within the standard error margins (reported in the table) and not statisti- Table 1 : Performance of Human-ALICE teams with AL-ICE SL and ALICE RL measured by MR (lower is better) and MRR (higher is better).\n sent2: Error bars are 95% CIs from 1000 bootstrap samples.\n sent3: Unlike #TARGET_REF , we find no significant difference between ALICE SL and ALICE RL .\n sent4: cally significant.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "cally significant.",
                "As we collected additional data, the error margins became smaller but the means also became closer.",
                "This interesting finding stands in stark contrast to the results reported by #TARGET_REF , where ALICE RL was found to be significantly more accurate than ALICE SL when evaluated in an AI-AI team.",
                "Our results suggest that the improvements of RL over SL (in AI-AI teams) do not seem to translate to when the agents are paired with a human in a similar setting.",
                "MR with varying number of games."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: cally significant.\n sent1: As we collected additional data, the error margins became smaller but the means also became closer.\n sent2: This interesting finding stands in stark contrast to the results reported by #TARGET_REF , where ALICE RL was found to be significantly more accurate than ALICE SL when evaluated in an AI-AI team.\n sent3: Our results suggest that the improvements of RL over SL (in AI-AI teams) do not seem to translate to when the agents are paired with a human in a similar setting.\n sent4: MR with varying number of games.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Specifically, we are considering the question-answering agent ABOT from #TARGET_REF as ABOT is the agent more likely to be deployed with a human partner in real applications (e.g. to answer questions about visual content to aid a visually impaired user).",
                "For completeness, we will review this work in this section.",
                "(#REFb ) formulate a self-supervised imageguessing task between a questioner bot (QBOT) and an answerer bot (ABOT) which plays out over multiple rounds of dialog.",
                "At the start of the task, QBOT and ABOT are shown a one sentence description (i.e. a caption) of an image (unknown to QBOT).",
                "The pair can then engage in question and answer based dialog for a fixed number of iterations after which QBOT must try to select the secret image from a pool."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: Specifically, we are considering the question-answering agent ABOT from #TARGET_REF as ABOT is the agent more likely to be deployed with a human partner in real applications (e.g. to answer questions about visual content to aid a visually impaired user).\n sent1: For completeness, we will review this work in this section.\n sent2: (#REFb ) formulate a self-supervised imageguessing task between a questioner bot (QBOT) and an answerer bot (ABOT) which plays out over multiple rounds of dialog.\n sent3: At the start of the task, QBOT and ABOT are shown a one sentence description (i.e. a caption) of an image (unknown to QBOT).\n sent4: The pair can then engage in question and answer based dialog for a fixed number of iterations after which QBOT must try to select the secret image from a pool.\n",
        "output": "{\"label\": [null], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Both QBOT and ABOT are modeled as Hierarchical Recurrent Encoder-Decoder neural networks (#REFa; ) which encode each round of dialog independently via a recurrent neural network (RNN) before accumulating this information through time with an additional RNN (resulting in hierarchical encoding).",
                "This representation (and a convolutional neural network based image encoding in ABOT's case) are used as input to a decoder RNN which produces an agent's utterance (question for QBOT and answer for ABOT) based on the dialog (and image for ABOT).",
                "In addition, QBOT includes an image feature regression network that predicts a representation of the secret image based on dialog history.",
                "We refer to #TARGET_REF for complete model details.",
                "These agents are pre-trained with supervised dialog data from the VisDial dataset (#REFa ) with a Maximum Likelihood Estimation objective."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: Both QBOT and ABOT are modeled as Hierarchical Recurrent Encoder-Decoder neural networks (#REFa; ) which encode each round of dialog independently via a recurrent neural network (RNN) before accumulating this information through time with an additional RNN (resulting in hierarchical encoding).\n sent1: This representation (and a convolutional neural network based image encoding in ABOT's case) are used as input to a decoder RNN which produces an agent's utterance (question for QBOT and answer for ABOT) based on the dialog (and image for ABOT).\n sent2: In addition, QBOT includes an image feature regression network that predicts a representation of the secret image based on dialog history.\n sent3: We refer to #TARGET_REF for complete model details.\n sent4: These agents are pre-trained with supervised dialog data from the VisDial dataset (#REFa ) with a Maximum Likelihood Estimation objective.\n",
        "output": "{\"label\": [null], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Recent efforts have shown machine translation to be an effective tool for porting dialogue system resources from French into Italian (#REF; #REF; #REF) ; this system used concept-based language understanding, and the findings were that machine translation of the target language inputs yielded better results than using translation to train an understanding component directly for the target language.",
                "Here we report similar findings on more challenging data, by exploring a dialogue system with a less structured understanding component, using off-the-shelf rather than domainadapted machine translation, and with languages that are not as closely related.",
                "Question-answering characters are designed to sustain a conversation driven primarily by the user asking questions.",
                "#TARGET_REF developed algorithms for training such characters using linked questions and responses in the form of unstructured natural language text.",
                "Given a novel user question, the character finds an appropriate response from a list of available responses, and when a direct answer is not available, the character selects an \"off-topic\" response according to a set policy, ensuring that the conversation remains coherent even with a finite number of responses."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Recent efforts have shown machine translation to be an effective tool for porting dialogue system resources from French into Italian (#REF; #REF; #REF) ; this system used concept-based language understanding, and the findings were that machine translation of the target language inputs yielded better results than using translation to train an understanding component directly for the target language.\n sent1: Here we report similar findings on more challenging data, by exploring a dialogue system with a less structured understanding component, using off-the-shelf rather than domainadapted machine translation, and with languages that are not as closely related.\n sent2: Question-answering characters are designed to sustain a conversation driven primarily by the user asking questions.\n sent3: #TARGET_REF developed algorithms for training such characters using linked questions and responses in the form of unstructured natural language text.\n sent4: Given a novel user question, the character finds an appropriate response from a list of available responses, and when a direct answer is not available, the character selects an \"off-topic\" response according to a set policy, ensuring that the conversation remains coherent even with a finite number of responses.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The response selection algorithms are languageindependent, also allowing the questions and responses to be in separate languages.",
                "These algorithms have been incorporated into a tool which has been used to create characters for a variety of applications (e.g. #TARGET_REF; #REF; #REF) .",
                "To date, most characters created using these principles understood and spoke only English; one fairly limited character spoke Pashto, a language of Afghanistan (#REF) .",
                "To test language portability we chose Tamil, a Dravidian language spoken primarily in southern India.",
                "Tamil has close to 70 million speakers worldwide (#REF) , is the official language of Tamil Nadu and Puducherry in India (#REF) , and an official language in Sri Lanka and Singapore."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The response selection algorithms are languageindependent, also allowing the questions and responses to be in separate languages.\n sent1: These algorithms have been incorporated into a tool which has been used to create characters for a variety of applications (e.g. #TARGET_REF; #REF; #REF) .\n sent2: To date, most characters created using these principles understood and spoke only English; one fairly limited character spoke Pashto, a language of Afghanistan (#REF) .\n sent3: To test language portability we chose Tamil, a Dravidian language spoken primarily in southern India.\n sent4: Tamil has close to 70 million speakers worldwide (#REF) , is the official language of Tamil Nadu and Puducherry in India (#REF) , and an official language in Sri Lanka and Singapore.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We use accuracy as our success measure: the top ranked response to a test question is considered correct if it is identified as a correct response in the linked test data (there are up to 4 correct responses per question).",
                "This measure does not take into account non-understanding, that is the classifier's determination that the best response is not good enough #TARGET_REF , since this capability was not implemented; however, since all of our test questions are known to have at least one appropriate response, any non-understanding of a question would necessarily count against accuracy anyway."
            ],
            "label": [
                "BACKGROUND",
                "DIFFERENCES"
            ]
        },
        "input": "sent0: We use accuracy as our success measure: the top ranked response to a test question is considered correct if it is identified as a correct response in the linked test data (there are up to 4 correct responses per question).\n sent1: This measure does not take into account non-understanding, that is the classifier's determination that the best response is not good enough #TARGET_REF , since this capability was not implemented; however, since all of our test questions are known to have at least one appropriate response, any non-understanding of a question would necessarily count against accuracy anyway.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"DIFFERENCES\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We reimplemented parts of the response ranking algorithms of #TARGET_REF , including both the language modeling (LM) and cross-language modeling (CLM) approaches.",
                "The LM approach constructs language models for both questions and responses using the question vocabulary.",
                "For each training question S, a language model is estimated as the frequency distribution of tokens in S, smoothed by the distribution of tokens in the entire question corpus (eq. 1).",
                "The language model of a novel question Q is estimated as the probability of each token in the vocabulary coinciding with Q (eq. 2).",
                "Each available response R is associated with a pseudo-question Q R made up by the concatenation of all the questions linked to R in the training data."
            ],
            "label": [
                "DIFFERENCES",
                "EXTENDS"
            ]
        },
        "input": "sent0: We reimplemented parts of the response ranking algorithms of #TARGET_REF , including both the language modeling (LM) and cross-language modeling (CLM) approaches.\n sent1: The LM approach constructs language models for both questions and responses using the question vocabulary.\n sent2: For each training question S, a language model is estimated as the frequency distribution of tokens in S, smoothed by the distribution of tokens in the entire question corpus (eq. 1).\n sent3: The language model of a novel question Q is estimated as the probability of each token in the vocabulary coinciding with Q (eq. 2).\n sent4: Each available response R is associated with a pseudo-question Q R made up by the concatenation of all the questions linked to R in the training data.\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"EXTENDS\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "The sum in eq. (5) is over all linked questionresponse pairs {S j , R j } in the training data, and the product is an estimate the probability of the question Q given the training question S j .",
                "In eq. (6), V R is the entire response vocabulary.",
                "We did not implement the parameter learning of #TARGET_REF ; instead we use a constant smoothing parameter λ π = λ φ = 0.1.",
                "We also do not use the response threshold parameter, which #REF use to determine whether the top-ranked response is good enough.",
                "Instead, we just check for the correctness of the top-ranked response."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: The sum in eq. (5) is over all linked questionresponse pairs {S j , R j } in the training data, and the product is an estimate the probability of the question Q given the training question S j .\n sent1: In eq. (6), V R is the entire response vocabulary.\n sent2: We did not implement the parameter learning of #TARGET_REF ; instead we use a constant smoothing parameter λ π = λ φ = 0.1.\n sent3: We also do not use the response threshold parameter, which #REF use to determine whether the top-ranked response is good enough.\n sent4: Instead, we just check for the correctness of the top-ranked response.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The sum in eq. (5) is over all linked questionresponse pairs {S j , R j } in the training data, and the product is an estimate the probability of the question Q given the training question S j .",
                "In eq. (6), V R is the entire response vocabulary.",
                "We did not implement the parameter learning of #REF ; instead we use a constant smoothing parameter λ π = λ φ = 0.1.",
                "We also do not use the response threshold parameter, which #TARGET_REF use to determine whether the top-ranked response is good enough.",
                "Instead, we just check for the correctness of the top-ranked response."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: The sum in eq. (5) is over all linked questionresponse pairs {S j , R j } in the training data, and the product is an estimate the probability of the question Q given the training question S j .\n sent1: In eq. (6), V R is the entire response vocabulary.\n sent2: We did not implement the parameter learning of #REF ; instead we use a constant smoothing parameter λ π = λ φ = 0.1.\n sent3: We also do not use the response threshold parameter, which #TARGET_REF use to determine whether the top-ranked response is good enough.\n sent4: Instead, we just check for the correctness of the top-ranked response.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The results of the experiments with matched question and response languages are reported in Table 1.",
                "The LM approach almost invariably produced better results than the CLM approach; this is the opposite of the findings of #TARGET_REF , where CLM fared consistently better.",
                "In most cases, the errors produced by the CLM approach were a superset of those of the LM approach; the only exceptions were Tamil with stemming.",
                "Accuracy of response selection on the Tamil data is about 10% lower than that of English, or twice the errors (6 errors rather than 3).",
                "The errors of automatically translated Tamil are a superset of the English errors; however, manually translated Tamil did get right some of the errors of English."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: The results of the experiments with matched question and response languages are reported in Table 1.\n sent1: The LM approach almost invariably produced better results than the CLM approach; this is the opposite of the findings of #TARGET_REF , where CLM fared consistently better.\n sent2: In most cases, the errors produced by the CLM approach were a superset of those of the LM approach; the only exceptions were Tamil with stemming.\n sent3: Accuracy of response selection on the Tamil data is about 10% lower than that of English, or twice the errors (6 errors rather than 3).\n sent4: The errors of automatically translated Tamil are a superset of the English errors; however, manually translated Tamil did get right some of the errors of English.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Another alternative is to use both languages together for classification; the fact that the manual Tamil translation identified some responses missed by the English classifier suggests that there may be benefit to this approach.",
                "Another direction for future work is identifying bad responses by using the distance between question and response to plot the tradeoff curve between errors and return rates (#REF) .",
                "In our experiments the LM approach consistently outperforms the CLM approach, contra #TARGET_REF .",
                "Our data may not be quite natural: while the English data are well tested, our sampling method may introduce biases that affect the results. But even if we achieved full English-like performance using machine translation, the questions that Tamil speakers want to ask will likely be somewhat different than those of English speakers.",
                "A translated dialogue system is therefore only an initial step towards tailoring a system to a new population."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Another alternative is to use both languages together for classification; the fact that the manual Tamil translation identified some responses missed by the English classifier suggests that there may be benefit to this approach.\n sent1: Another direction for future work is identifying bad responses by using the distance between question and response to plot the tradeoff curve between errors and return rates (#REF) .\n sent2: In our experiments the LM approach consistently outperforms the CLM approach, contra #TARGET_REF .\n sent3: Our data may not be quite natural: while the English data are well tested, our sampling method may introduce biases that affect the results. But even if we achieved full English-like performance using machine translation, the questions that Tamil speakers want to ask will likely be somewhat different than those of English speakers.\n sent4: A translated dialogue system is therefore only an initial step towards tailoring a system to a new population.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We argue that groups of unannotated texts with overlapping and non-contradictory semantics represent a valuable source of information for learning semantic representations.",
                "A simple and efficient inference method recursively induces joint semantic representations for each group and discovers correspondence between lexical entries and latent semantic concepts.",
                "We consider the generative semantics-text correspondence model #TARGET_REF and demonstrate that exploiting the noncontradiction relation between texts leads to substantial improvements over natural baselines on a problem of analyzing human-written weather forecasts."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: We argue that groups of unannotated texts with overlapping and non-contradictory semantics represent a valuable source of information for learning semantic representations.\n sent1: A simple and efficient inference method recursively induces joint semantic representations for each group and discovers correspondence between lexical entries and latent semantic concepts.\n sent2: We consider the generative semantics-text correspondence model #TARGET_REF and demonstrate that exploiting the noncontradiction relation between texts leads to substantial improvements over natural baselines on a problem of analyzing human-written weather forecasts.\n",
        "output": "{\"label\": [null], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Unsupervised learning with shared latent semantic representations presents its own challenges, as exact inference requires marginalization over possible assignments of the latent semantic state, consequently, introducing non-local statistical dependencies between the decisions about the semantic structure of each text.",
                "We propose a simple and fairly general approximate inference algorithm for probabilistic models of semantics which is efficient for the considered model, and achieves favorable results in our experiments.",
                "In this paper, we do not consider models which aim to produce complete formal meaning of text (#REF; #REF; #REF) , instead focusing on a simpler problem studied in #TARGET_REF .",
                "They investigate grounded language acquisition set-up and assume that semantics (world state) can be represented as a set of records each consisting of a set of fields.",
                "Their model segments text into utterances and identifies records, fields and field values discussed in each utterance."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: Unsupervised learning with shared latent semantic representations presents its own challenges, as exact inference requires marginalization over possible assignments of the latent semantic state, consequently, introducing non-local statistical dependencies between the decisions about the semantic structure of each text.\n sent1: We propose a simple and fairly general approximate inference algorithm for probabilistic models of semantics which is efficient for the considered model, and achieves favorable results in our experiments.\n sent2: In this paper, we do not consider models which aim to produce complete formal meaning of text (#REF; #REF; #REF) , instead focusing on a simpler problem studied in #TARGET_REF .\n sent3: They investigate grounded language acquisition set-up and assume that semantics (world state) can be represented as a set of records each consisting of a set of fields.\n sent4: Their model segments text into utterances and identifies records, fields and field values discussed in each utterance.\n",
        "output": "{\"label\": [null], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The parameters corresponding to each form of generation are estimated during learning.",
                "For details on these emission models, as well as for details on modeling record and field transitions, we refer the reader to the original publication #TARGET_REF ).",
                "In our experiments, when choosing a world state s, we generate the field values independently.",
                "This is clearly a suboptimal regime as often there are very strong dependencies between field values: e.g., in the weather domain many record types contain groups of related fields defining minimal, maximal and average values of some parameter.",
                "Extending the method to model, e.g., pairwise dependencies between field values is relatively straightforward."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: The parameters corresponding to each form of generation are estimated during learning.\n sent1: For details on these emission models, as well as for details on modeling record and field transitions, we refer the reader to the original publication #TARGET_REF ).\n sent2: In our experiments, when choosing a world state s, we generate the field values independently.\n sent3: This is clearly a suboptimal regime as often there are very strong dependencies between field values: e.g., in the weather domain many record types contain groups of related fields defining minimal, maximal and average values of some parameter.\n sent4: Extending the method to model, e.g., pairwise dependencies between field values is relatively straightforward.\n",
        "output": "{\"label\": [null], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "However, most of this research has focused on supervised methods requiring large amounts of labeled data.",
                "The supervision was either given in the form of meaning representations aligned with sentences (#REF; #REF; #REF) or in a somewhat more relaxed form, such as lists of candidate meanings for each sentence (#REF; #REF) or formal representations of the described world state for each text #TARGET_REF .",
                "Such annotated resources are scarce and expensive to create, motivating the need for unsupervised or semi-supervised techniques (#REF ).",
                "However, unsupervised methods have their own challenges: they are not always able to discover semantic equivalences of lexical entries or logical forms or, on the contrary, cluster semantically different or even opposite expressions (#REF ).",
                "Unsupervised approaches can only rely on distributional similarity of contexts (#REF) to decide on semantic relatedness of terms, but this information may be sparse and not reliable (#REF) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: However, most of this research has focused on supervised methods requiring large amounts of labeled data.\n sent1: The supervision was either given in the form of meaning representations aligned with sentences (#REF; #REF; #REF) or in a somewhat more relaxed form, such as lists of candidate meanings for each sentence (#REF; #REF) or formal representations of the described world state for each text #TARGET_REF .\n sent2: Such annotated resources are scarce and expensive to create, motivating the need for unsupervised or semi-supervised techniques (#REF ).\n sent3: However, unsupervised methods have their own challenges: they are not always able to discover semantic equivalences of lexical entries or logical forms or, on the contrary, cluster semantically different or even opposite expressions (#REF ).\n sent4: Unsupervised approaches can only rely on distributional similarity of contexts (#REF) to decide on semantic relatedness of terms, but this information may be sparse and not reliable (#REF) .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The rest of the paper is structured as follows.",
                "In section 2 we describe our inference algorithm for groups of non-contradictory documents.",
                "Section 3 redescribes the semantics-text correspondence model #TARGET_REF in the context of our learning scenario.",
                "In section 4 we provide an empirical evaluation of the proposed method.",
                "We conclude in section 5 with an examination of additional related work."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The rest of the paper is structured as follows.\n sent1: In section 2 we describe our inference algorithm for groups of non-contradictory documents.\n sent2: Section 3 redescribes the semantics-text correspondence model #TARGET_REF in the context of our learning scenario.\n sent3: In section 4 we provide an empirical evaluation of the proposed method.\n sent4: We conclude in section 5 with an examination of additional related work.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "An instantiation of the algorithm for the semantics-text correspondence model is given in section 3.2.",
                "Statistical models of parsing can often be regarded as defining the probability distribution of meaning m and its alignment a with the given text w, P (m, a, w) = P (a, w|m)P (m).",
                "The semantics m can be represented either as a logical formula (see, e.g., (#REF )) or as a set of field values if database records are used as a meaning representation #TARGET_REF ).",
                "The alignment a defines how semantics is verbalized in the text w, and it can be represented by a meaning derivation tree in case of full semantic parsing (#REF) or, e.g., by a hierarchical segmentation into utterances along with an utterance-field alignment in a more shallow variation of the problem.",
                "In semantic parsing, we aim to find the most likely underlying semantics and alignment given the text:"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: An instantiation of the algorithm for the semantics-text correspondence model is given in section 3.2.\n sent1: Statistical models of parsing can often be regarded as defining the probability distribution of meaning m and its alignment a with the given text w, P (m, a, w) = P (a, w|m)P (m).\n sent2: The semantics m can be represented either as a logical formula (see, e.g., (#REF )) or as a set of field values if database records are used as a meaning representation #TARGET_REF ).\n sent3: The alignment a defines how semantics is verbalized in the text w, and it can be represented by a meaning derivation tree in case of full semantic parsing (#REF) or, e.g., by a hierarchical segmentation into utterances along with an utterance-field alignment in a more shallow variation of the problem.\n sent4: In semantic parsing, we aim to find the most likely underlying semantics and alignment given the text:\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We select the model parameters θ by maximizing the marginal likelihood of the data, where the data D is given in the form of groups w = {w 1 ,..., w K } sharing the same latent state: 5 max θ w∈D s",
                "To estimate the parameters, we use the Expectation-Maximization algorithm (#REF) .",
                "When the world state is observable, learning does not require any approximations, as dynamic programming (a form of the forward-backward algorithm) can be used to infer the posterior distribution on the E-step #TARGET_REF .",
                "However, when the state is latent, dependencies are not local anymore, and approximate inference is required.",
                "We use the algorithm described in section 2 (figure 2) to infer the state."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: We select the model parameters θ by maximizing the marginal likelihood of the data, where the data D is given in the form of groups w = {w 1 ,..., w K } sharing the same latent state: 5 max θ w∈D s\n sent1: To estimate the parameters, we use the Expectation-Maximization algorithm (#REF) .\n sent2: When the world state is observable, learning does not require any approximations, as dynamic programming (a form of the forward-backward algorithm) can be used to infer the posterior distribution on the E-step #TARGET_REF .\n sent3: However, when the state is latent, dependencies are not local anymore, and approximate inference is required.\n sent4: We use the algorithm described in section 2 (figure 2) to infer the state.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We follow their set-up, but assume that instead of having access to the full semantic state for every training example, we have a very small amount of data annotated with semantic states and a larger number of unannotated texts with noncontradictory semantics.",
                "We study our set-up on the weather forecast data #TARGET_REF where the original textual weather forecasts were complemented by additional forecasts describing the same weather states (see figure 1 for an example).",
                "The average overlap between the verbalized fields in each group of noncontradictory forecasts was below 35%, and more than 60% of fields are mentioned only in a single forecast from a group.",
                "Our model, learned from 100 labeled forecasts and 259 groups of unannotated non-contradictory forecasts (750 texts in total), achieved 73.9% F 1 .",
                "This compares favorably with 69.1% shown by a semi-supervised learning approach, though, as expected, does not reach the score of the model which, in training, observed semantics states for all the 750 documents (77.7% F 1 )."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We follow their set-up, but assume that instead of having access to the full semantic state for every training example, we have a very small amount of data annotated with semantic states and a larger number of unannotated texts with noncontradictory semantics.\n sent1: We study our set-up on the weather forecast data #TARGET_REF where the original textual weather forecasts were complemented by additional forecasts describing the same weather states (see figure 1 for an example).\n sent2: The average overlap between the verbalized fields in each group of noncontradictory forecasts was below 35%, and more than 60% of fields are mentioned only in a single forecast from a group.\n sent3: Our model, learned from 100 labeled forecasts and 259 groups of unannotated non-contradictory forecasts (750 texts in total), achieved 73.9% F 1 .\n sent4: This compares favorably with 69.1% shown by a semi-supervised learning approach, though, as expected, does not reach the score of the model which, in training, observed semantics states for all the 750 documents (77.7% F 1 ).\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Though the semantics m k (k / ∈ n∪{j}) used in the estimates (line 6) can be inconsistent with each other, the final list of meanings m is guaranteed to be consistent.",
                "It holds because on each iteration we add a single meaningm n i to m (line 7), and m n i is guaranteed to be consistent with m , as the semanticsm n i was conditioned on the meaning m during inference (line 4).",
                "An important aspect of this algorithm is that unlike usual greedy inference, the remaining ('future') texts do affect the choice of meaning representations made on the earlier stages.",
                "As soon as semantics m k are inferred for every k, we find ourselves in the set-up of learning with unaligned semantic states considered in #TARGET_REF .",
                "The induced alignments a 1 ,..., a K of semantics m to texts w 1 ,..., w K at the same time induce alignments between the texts."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Though the semantics m k (k / ∈ n∪{j}) used in the estimates (line 6) can be inconsistent with each other, the final list of meanings m is guaranteed to be consistent.\n sent1: It holds because on each iteration we add a single meaningm n i to m (line 7), and m n i is guaranteed to be consistent with m , as the semanticsm n i was conditioned on the meaning m during inference (line 4).\n sent2: An important aspect of this algorithm is that unlike usual greedy inference, the remaining ('future') texts do affect the choice of meaning representations made on the earlier stages.\n sent3: As soon as semantics m k are inferred for every k, we find ourselves in the set-up of learning with unaligned semantic states considered in #TARGET_REF .\n sent4: The induced alignments a 1 ,..., a K of semantics m to texts w 1 ,..., w K at the same time induce alignments between the texts.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "As soon as the meaning representations m are inferred, we find ourselves in the set-up studied in #TARGET_REF ): the state s is no longer latent and we can run efficient inference on the E-step.",
                "Though some fields of the state s may still not be specified by m , we prohibit utterances from aligning to these non-specified fields.",
                "On the M-step of EM the parameters are estimated as proportional to the expected marginal counts computed on the E-step.",
                "We smooth the distributions of values for numerical fields with convolution smoothing equivalent to the assumption that the fields are affected by distortion in the form of a two-sided geometric distribution with the success rate parameter equal to 0.67.",
                "We use add-0.1 smoothing for all the remaining multinomial distributions."
            ],
            "label": [
                "USE",
                "SIMILARITY"
            ]
        },
        "input": "sent0: As soon as the meaning representations m are inferred, we find ourselves in the set-up studied in #TARGET_REF ): the state s is no longer latent and we can run efficient inference on the E-step.\n sent1: Though some fields of the state s may still not be specified by m , we prohibit utterances from aligning to these non-specified fields.\n sent2: On the M-step of EM the parameters are estimated as proportional to the expected marginal counts computed on the E-step.\n sent3: We smooth the distributions of values for numerical fields with convolution smoothing equivalent to the assumption that the fields are affected by distortion in the form of a two-sided geometric distribution with the success rate parameter equal to 0.67.\n sent4: We use add-0.1 smoothing for all the remaining multinomial distributions.\n",
        "output": "{\"label\": [\"USE\", \"SIMILARITY\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "To perform the experiments we used a subset of the weather dataset introduced in #TARGET_REF ).",
                "The original dataset contains 22,146 texts of 28.7 words on average, there are 12 types of records (predicates) and 36.0 records per forecast on average.",
                "We randomly chose 100 texts along with their world states to be used as the labeled data.",
                "6 To produce groups of noncontradictory texts we have randomly selected a subset of weather states, represented them in a visual form (icons accompanied by numerical and symbolic parameters) and then manually annotated these illustrations.",
                "These newly-produced forecasts, when combined with the original texts, resulted in 259 groups of non-contradictory texts (650 texts, 2.5 texts per group)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: To perform the experiments we used a subset of the weather dataset introduced in #TARGET_REF ).\n sent1: The original dataset contains 22,146 texts of 28.7 words on average, there are 12 types of records (predicates) and 36.0 records per forecast on average.\n sent2: We randomly chose 100 texts along with their world states to be used as the labeled data.\n sent3: 6 To produce groups of noncontradictory texts we have randomly selected a subset of weather states, represented them in a visual form (icons accompanied by numerical and symbolic parameters) and then manually annotated these illustrations.\n sent4: These newly-produced forecasts, when combined with the original texts, resulted in 259 groups of non-contradictory texts (650 texts, 2.5 texts per group).\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "We aimed to preserve approximately the same proportion of new and original examples as we had in the training set, therefore, we combined 50 texts originally present in the weather dataset with additional 100 newly-produced texts.",
                "We annotated these 100 texts by aligning each line to one or more records, 7 whereas for the original texts the alignments were already present.",
                "Following #TARGET_REF we evaluate the models on how well they predict these alignments.",
                "When estimating the model parameters, we followed the training regime prescribed in (#REF) .",
                "Namely, 5 iterations of EM with a basic model (with no segmentation or coherence modeling), followed by 5 iterations of EM with the model which generates fields independently and, at last, 5 iterations with the full model."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We aimed to preserve approximately the same proportion of new and original examples as we had in the training set, therefore, we combined 50 texts originally present in the weather dataset with additional 100 newly-produced texts.\n sent1: We annotated these 100 texts by aligning each line to one or more records, 7 whereas for the original texts the alignments were already present.\n sent2: Following #TARGET_REF we evaluate the models on how well they predict these alignments.\n sent3: When estimating the model parameters, we followed the training regime prescribed in (#REF) .\n sent4: Namely, 5 iterations of EM with a basic model (with no segmentation or coherence modeling), followed by 5 iterations of EM with the model which generates fields independently and, at last, 5 iterations with the full model.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We aimed to preserve approximately the same proportion of new and original examples as we had in the training set, therefore, we combined 50 texts originally present in the weather dataset with additional 100 newly-produced texts.",
                "We annotated these 100 texts by aligning each line to one or more records, 7 whereas for the original texts the alignments were already present.",
                "Following #REF we evaluate the models on how well they predict these alignments.",
                "When estimating the model parameters, we followed the training regime prescribed in #TARGET_REF .",
                "Namely, 5 iterations of EM with a basic model (with no segmentation or coherence modeling), followed by 5 iterations of EM with the model which generates fields independently and, at last, 5 iterations with the full model."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We aimed to preserve approximately the same proportion of new and original examples as we had in the training set, therefore, we combined 50 texts originally present in the weather dataset with additional 100 newly-produced texts.\n sent1: We annotated these 100 texts by aligning each line to one or more records, 7 whereas for the original texts the alignments were already present.\n sent2: Following #REF we evaluate the models on how well they predict these alignments.\n sent3: When estimating the model parameters, we followed the training regime prescribed in #TARGET_REF .\n sent4: Namely, 5 iterations of EM with a basic model (with no segmentation or coherence modeling), followed by 5 iterations of EM with the model which generates fields independently and, at last, 5 iterations with the full model.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "However, exact inference for groups of documents with overlapping semantic representation is generally prohibitively expensive, as the shared latent semantics introduces nonlocal dependences between semantic representations of individual documents.",
                "To combat it, we proposed a simple iterative inference algorithm.",
                "We showed how it can be instantiated for the semantics-text correspondence model #TARGET_REF ) and evaluated it on a dataset of weather forecasts.",
                "Our approach resulted in an improvement over the scores of both the supervised baseline and of the traditional semi-supervised learning.",
                "There are many directions we plan on investigating in the future for the problem of learning semantics with non-contradictory relations."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: However, exact inference for groups of documents with overlapping semantic representation is generally prohibitively expensive, as the shared latent semantics introduces nonlocal dependences between semantic representations of individual documents.\n sent1: To combat it, we proposed a simple iterative inference algorithm.\n sent2: We showed how it can be instantiated for the semantics-text correspondence model #TARGET_REF ) and evaluated it on a dataset of weather forecasts.\n sent3: Our approach resulted in an improvement over the scores of both the supervised baseline and of the traditional semi-supervised learning.\n sent4: There are many directions we plan on investigating in the future for the problem of learning semantics with non-contradictory relations.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The problem of predicting text-meaning alignments is interesting in itself, as the extracted alignments can be used in training of a statistical generation system or information extractors, but we also believe that evaluation on this problem is an appropriate test for the relative comparison of the semantic analyzers' performance.",
                "Additionally, note that the success of our weaklysupervised scenario indirectly suggests that the model is sufficiently accurate in predicting semantics of an unlabeled text, as otherwise there would be no useful information passed in between semantically overlapping documents during learning and, consequently, no improvement from sharing the state.",
                "8 To confirm that the model trained by our approach indeed assigns new words to correct fields and records, we visualize top words for the field characterizing sky cover (table 2).",
                "Note that the words \"sun\", \"cloudiness\" or \"gaps\" were not appearing in the labeled part of the data, but seem to be assigned to correct categories.",
                "However, correlation between rain and overcast, as also noted in #TARGET_REF , results in the wrong assignment of the rain-related words to the field value corresponding to very cloudy weather."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: The problem of predicting text-meaning alignments is interesting in itself, as the extracted alignments can be used in training of a statistical generation system or information extractors, but we also believe that evaluation on this problem is an appropriate test for the relative comparison of the semantic analyzers' performance.\n sent1: Additionally, note that the success of our weaklysupervised scenario indirectly suggests that the model is sufficiently accurate in predicting semantics of an unlabeled text, as otherwise there would be no useful information passed in between semantically overlapping documents during learning and, consequently, no improvement from sharing the state.\n sent2: 8 To confirm that the model trained by our approach indeed assigns new words to correct fields and records, we visualize top words for the field characterizing sky cover (table 2).\n sent3: Note that the words \"sun\", \"cloudiness\" or \"gaps\" were not appearing in the labeled part of the data, but seem to be assigned to correct categories.\n sent4: However, correlation between rain and overcast, as also noted in #TARGET_REF , results in the wrong assignment of the rain-related words to the field value corresponding to very cloudy weather.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Namely, 5 iterations of EM with a basic model (with no segmentation or coherence modeling), followed by 5 iterations of EM with the model which generates fields independently and, at last, 5 iterations with the full model.",
                "Only then, in the semi-supervised learning scenarios, we added unlabeled data and ran 5 additional iterations of EM.",
                "Instead of prohibiting records from crossing punctuation, as suggested by #TARGET_REF , in our implementation we disregard the words not attached to specific fields (attached to the nullfield, see section 3.1) when computing spans of records.",
                "To speed-up training, only a single record of each type is allowed to be generated when running inference for unlabeled examples on the E- Table 1 : Results (precision, recall and F 1 ) on the weather forecast dataset.",
                "step of the EM algorithm, as it significantly reduces the search space."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Namely, 5 iterations of EM with a basic model (with no segmentation or coherence modeling), followed by 5 iterations of EM with the model which generates fields independently and, at last, 5 iterations with the full model.\n sent1: Only then, in the semi-supervised learning scenarios, we added unlabeled data and ran 5 additional iterations of EM.\n sent2: Instead of prohibiting records from crossing punctuation, as suggested by #TARGET_REF , in our implementation we disregard the words not attached to specific fields (attached to the nullfield, see section 3.1) when computing spans of records.\n sent3: To speed-up training, only a single record of each type is allowed to be generated when running inference for unlabeled examples on the E- Table 1 : Results (precision, recall and F 1 ) on the weather forecast dataset.\n sent4: step of the EM algorithm, as it significantly reduces the search space.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Distributed representations of multimodal embeddings (Feng & #REF) are receiving increasing attention recently in the machine learning literature, and techniques developed have found a wide spectrum of applications in the real world.",
                "These types of vector representations are particularly desirable for the way in which they better model the grounding of perceptual or semantic concepts in human vocabulary #TARGET_REF; Glenberg & #REF; ).",
                "As such, there has been development towards so-called multimodal distributional semantic models (Silberer & #REF; #REF; #REF; #REF; #REF) , which leverage textual co-occurance and visual features to form multimodal representations of words or concepts.",
                "The work introduced in #REF sought to address many of the drawbacks of these models.",
                "In particular, by incorporating visual information into the training objective, they address the biological inaccuracy of the existing models, in that word representations grounded in visual information have been shown to more closely approximate the way humans learn language."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Distributed representations of multimodal embeddings (Feng & #REF) are receiving increasing attention recently in the machine learning literature, and techniques developed have found a wide spectrum of applications in the real world.\n sent1: These types of vector representations are particularly desirable for the way in which they better model the grounding of perceptual or semantic concepts in human vocabulary #TARGET_REF; Glenberg & #REF; ).\n sent2: As such, there has been development towards so-called multimodal distributional semantic models (Silberer & #REF; #REF; #REF; #REF; #REF) , which leverage textual co-occurance and visual features to form multimodal representations of words or concepts.\n sent3: The work introduced in #REF sought to address many of the drawbacks of these models.\n sent4: In particular, by incorporating visual information into the training objective, they address the biological inaccuracy of the existing models, in that word representations grounded in visual information have been shown to more closely approximate the way humans learn language.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Distributed representations of multimodal embeddings (Feng & #REF) are receiving increasing attention recently in the machine learning literature, and techniques developed have found a wide spectrum of applications in the real world.",
                "These types of vector representations are particularly desirable for the way in which they better model the grounding of perceptual or semantic concepts in human vocabulary (#REF; Glenberg & #REF; ).",
                "As such, there has been development towards so-called multimodal distributional semantic models (Silberer & #REF; #TARGET_REF; #REF; #REF; #REF) , which leverage textual co-occurance and visual features to form multimodal representations of words or concepts.",
                "The work introduced in #REF sought to address many of the drawbacks of these models.",
                "In particular, by incorporating visual information into the training objective, they address the biological inaccuracy of the existing models, in that word representations grounded in visual information have been shown to more closely approximate the way humans learn language."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Distributed representations of multimodal embeddings (Feng & #REF) are receiving increasing attention recently in the machine learning literature, and techniques developed have found a wide spectrum of applications in the real world.\n sent1: These types of vector representations are particularly desirable for the way in which they better model the grounding of perceptual or semantic concepts in human vocabulary (#REF; Glenberg & #REF; ).\n sent2: As such, there has been development towards so-called multimodal distributional semantic models (Silberer & #REF; #TARGET_REF; #REF; #REF; #REF) , which leverage textual co-occurance and visual features to form multimodal representations of words or concepts.\n sent3: The work introduced in #REF sought to address many of the drawbacks of these models.\n sent4: In particular, by incorporating visual information into the training objective, they address the biological inaccuracy of the existing models, in that word representations grounded in visual information have been shown to more closely approximate the way humans learn language.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "As such, there has been development towards so-called multimodal distributional semantic models (Silberer & #REF; #REF; #REF; #REF; #REF) , which leverage textual co-occurance and visual features to form multimodal representations of words or concepts.",
                "The work introduced in #TARGET_REF sought to address many of the drawbacks of these models.",
                "In particular, by incorporating visual information into the training objective, they address the biological inaccuracy of the existing models, in that word representations grounded in visual information have been shown to more closely approximate the way humans learn language.",
                "Furthermore, incorporating visual information alongside the text corpus allows the training set to consist of both visual and non-visual words.",
                "As a result, the induced multimodal representations and multimodal mapping no longer rely on the assumption of full visual coverage of the vocabulary, so the results are able to generalize beyond the initial training set and to be applied to various representation-related tasks, such as image annotation or retrieval."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: As such, there has been development towards so-called multimodal distributional semantic models (Silberer & #REF; #REF; #REF; #REF; #REF) , which leverage textual co-occurance and visual features to form multimodal representations of words or concepts.\n sent1: The work introduced in #TARGET_REF sought to address many of the drawbacks of these models.\n sent2: In particular, by incorporating visual information into the training objective, they address the biological inaccuracy of the existing models, in that word representations grounded in visual information have been shown to more closely approximate the way humans learn language.\n sent3: Furthermore, incorporating visual information alongside the text corpus allows the training set to consist of both visual and non-visual words.\n sent4: As a result, the induced multimodal representations and multimodal mapping no longer rely on the assumption of full visual coverage of the vocabulary, so the results are able to generalize beyond the initial training set and to be applied to various representation-related tasks, such as image annotation or retrieval.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "In the last few years, there has been a wealth of literature on multimodal representational models.",
                "As explained in #TARGET_REF , the majority of this literature focuses on constructing textual and visual representations independently and then combining them under some metrics.",
                "#REF utilize a direct approach to \"mixing\" the vector representations by concatenating the text and image vectors and applying Singular Value Decomposition.",
                "The image vectors used here, though, are constructed using the bag-of-visual-words method.",
                "In Kiela & #REF , the authors utilize a more sophisticated approach to the concatenation method by extracting visual features using state-of-the-art convolutional neural networks and the skip-gram architecture for the text."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In the last few years, there has been a wealth of literature on multimodal representational models.\n sent1: As explained in #TARGET_REF , the majority of this literature focuses on constructing textual and visual representations independently and then combining them under some metrics.\n sent2: #REF utilize a direct approach to \"mixing\" the vector representations by concatenating the text and image vectors and applying Singular Value Decomposition.\n sent3: The image vectors used here, though, are constructed using the bag-of-visual-words method.\n sent4: In Kiela & #REF , the authors utilize a more sophisticated approach to the concatenation method by extracting visual features using state-of-the-art convolutional neural networks and the skip-gram architecture for the text.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "In Kiela & #REF , the authors utilize a more sophisticated approach to the concatenation method by extracting visual features using state-of-the-art convolutional neural networks and the skip-gram architecture for the text.",
                "Similarly, #REF also utilizes the skip-gram architecture and convolutional features; however the two modalities are then combined using a natural similarity metric.",
                "Other recent work has presented several methods for directly incorporating visual context in neural language models.",
                "In #REF , word context is enhanced by global visual context; i.e., a single image is used as the context for the whole sentence (conversely, the sentence acts as a caption for the image).",
                "The multimodal skip-gram architecture proposed by #TARGET_REF takes a more fine-grained approach by incorporating word-level visual context and concurrently training words to predict other text words in the window as well as their visual representation."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In Kiela & #REF , the authors utilize a more sophisticated approach to the concatenation method by extracting visual features using state-of-the-art convolutional neural networks and the skip-gram architecture for the text.\n sent1: Similarly, #REF also utilizes the skip-gram architecture and convolutional features; however the two modalities are then combined using a natural similarity metric.\n sent2: Other recent work has presented several methods for directly incorporating visual context in neural language models.\n sent3: In #REF , word context is enhanced by global visual context; i.e., a single image is used as the context for the whole sentence (conversely, the sentence acts as a caption for the image).\n sent4: The multimodal skip-gram architecture proposed by #TARGET_REF takes a more fine-grained approach by incorporating word-level visual context and concurrently training words to predict other text words in the window as well as their visual representation.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "In particular, by incorporating visual information into the training objective, they address the biological inaccuracy of the existing models, in that word representations grounded in visual information have been shown to more closely approximate the way humans learn language.",
                "Furthermore, incorporating visual information alongside the text corpus allows the training set to consist of both visual and non-visual words.",
                "As a result, the induced multimodal representations and multimodal mapping no longer rely on the assumption of full visual coverage of the vocabulary, so the results are able to generalize beyond the initial training set and to be applied to various representation-related tasks, such as image annotation or retrieval.",
                "In this work, we introduce a further refinement on the multimodal skip-gram architecture, building upon the approaches of Mikolov et al. (2013a; b) , , and #TARGET_REF .",
                "Rather than adding a visual term to the linguistic training objective, we directly situate terms in a visual context by replacing relevant words with multimodal pseudowords, derived by composing the textual representations with convolutional features projected into the multimodal space."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: In particular, by incorporating visual information into the training objective, they address the biological inaccuracy of the existing models, in that word representations grounded in visual information have been shown to more closely approximate the way humans learn language.\n sent1: Furthermore, incorporating visual information alongside the text corpus allows the training set to consist of both visual and non-visual words.\n sent2: As a result, the induced multimodal representations and multimodal mapping no longer rely on the assumption of full visual coverage of the vocabulary, so the results are able to generalize beyond the initial training set and to be applied to various representation-related tasks, such as image annotation or retrieval.\n sent3: In this work, we introduce a further refinement on the multimodal skip-gram architecture, building upon the approaches of Mikolov et al. (2013a; b) , , and #TARGET_REF .\n sent4: Rather than adding a visual term to the linguistic training objective, we directly situate terms in a visual context by replacing relevant words with multimodal pseudowords, derived by composing the textual representations with convolutional features projected into the multimodal space.\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "For our text corpus, keeping with the existing literature, we use a preprocessed dump of Wikipedia 3 containing approximately 800M tokens.",
                "For the visual data, we use the image data from ILSVRC 2012 (#REF) and the corresponding Wordnet hierarchy (#REF) to represent a word visually if the word or any of its hyponyms has an entry in Imagenet and occurs more than 500 times in the text corpus.",
                "This yields approximately 5,100 \"visual\" words.",
                "To construct the vectors for the visual representations, we follow a similar experimental set-up as that used by #TARGET_REF .",
                "In each of the cases described above-centroid and hypersphere-, we randomly sample 100 images from the corresponding synsets of Imagenet for each visual word and use a pre-trained convolutional neural network as described in #REF via the Caffe toolkit (#REF) to extract a 4096-dimensional vector representation of each image."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: For our text corpus, keeping with the existing literature, we use a preprocessed dump of Wikipedia 3 containing approximately 800M tokens.\n sent1: For the visual data, we use the image data from ILSVRC 2012 (#REF) and the corresponding Wordnet hierarchy (#REF) to represent a word visually if the word or any of its hyponyms has an entry in Imagenet and occurs more than 500 times in the text corpus.\n sent2: This yields approximately 5,100 \"visual\" words.\n sent3: To construct the vectors for the visual representations, we follow a similar experimental set-up as that used by #TARGET_REF .\n sent4: In each of the cases described above-centroid and hypersphere-, we randomly sample 100 images from the corresponding synsets of Imagenet for each visual word and use a pre-trained convolutional neural network as described in #REF via the Caffe toolkit (#REF) to extract a 4096-dimensional vector representation of each image.\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "To judge our model, we calculate the cosine similarity of our embeddings for the word pairs and then calculate Spearman's ρ between our list of ratings and those of the human judges.",
                "We evaluate three versions of our model on these benchmarks: pseudowords using the centroid method (PSUEDOWORDS-C), pseudowords using the hypersphere method (PSEUDOWORDS-H), and the centroid method with a randomly initialized mapping (PSEUDOWORDS-RAN), as explained below.",
                "Existing Multimodal Models We compare our results on these benchmarks against previously published results for other multimodal word embeddings .",
                "Using the results published by #TARGET_REF and a target word embedding of 300, we compare our results to their MMSKIP-GRAM-A and MMSKIP-GRAM-B, which maximize the similarity of the textual and visual representations Table 1 : Spearman correlation between the generated multimodal similarities and the benchmark human judgments.",
                "In all cases, results are reported on the full set of word similarity pairs."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: To judge our model, we calculate the cosine similarity of our embeddings for the word pairs and then calculate Spearman's ρ between our list of ratings and those of the human judges.\n sent1: We evaluate three versions of our model on these benchmarks: pseudowords using the centroid method (PSUEDOWORDS-C), pseudowords using the hypersphere method (PSEUDOWORDS-H), and the centroid method with a randomly initialized mapping (PSEUDOWORDS-RAN), as explained below.\n sent2: Existing Multimodal Models We compare our results on these benchmarks against previously published results for other multimodal word embeddings .\n sent3: Using the results published by #TARGET_REF and a target word embedding of 300, we compare our results to their MMSKIP-GRAM-A and MMSKIP-GRAM-B, which maximize the similarity of the textual and visual representations Table 1 : Spearman correlation between the generated multimodal similarities and the benchmark human judgments.\n sent4: In all cases, results are reported on the full set of word similarity pairs.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "On the other hand, when the mapping is quickly pretrained on existing distributed word representations, the results are greatly improved.",
                "In the cases of capturing general relatedness and pure visual similarity, the multimodal model of #TARGET_REF performs better.",
                "However, in the case of capturing semantic word similarity, our model performs signficantly better than MMSKIP-GRAM-B (although it should be noted that these results are roughly on par with the benchmark authors (Silberer & #REF) and a point below the non-mapping MMSKIP-GRAM-A).",
                "Although further work is needed to examine this result, the performance of the model in this case can be visualized through an example.",
                "Table 2 provides some insights on the changes made to the word embeddings as a result of the inclusion of visual information in the learning process."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: On the other hand, when the mapping is quickly pretrained on existing distributed word representations, the results are greatly improved.\n sent1: In the cases of capturing general relatedness and pure visual similarity, the multimodal model of #TARGET_REF performs better.\n sent2: However, in the case of capturing semantic word similarity, our model performs signficantly better than MMSKIP-GRAM-B (although it should be noted that these results are roughly on par with the benchmark authors (Silberer & #REF) and a point below the non-mapping MMSKIP-GRAM-A).\n sent3: Although further work is needed to examine this result, the performance of the model in this case can be visualized through an example.\n sent4: Table 2 provides some insights on the changes made to the word embeddings as a result of the inclusion of visual information in the learning process.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Beyond interpretability, this often results in improved decision making by the network.",
                "Recently, #TARGET_REF proposed the Transformer architecture for machine translation.",
                "It relies only on attention mechanisms, instead of making use of either recurrent or convolutional * Authors contributed equally to this work.",
                "neural networks.",
                "This architecture contains layers called self-attention (or intra-attention) which allow each word in the sequence to pay attention to other words in the sequence, independently of their positions."
            ],
            "label": [
                "BACKGROUND",
                "MOTIVATION"
            ]
        },
        "input": "sent0: Beyond interpretability, this often results in improved decision making by the network.\n sent1: Recently, #TARGET_REF proposed the Transformer architecture for machine translation.\n sent2: It relies only on attention mechanisms, instead of making use of either recurrent or convolutional * Authors contributed equally to this work.\n sent3: neural networks.\n sent4: This architecture contains layers called self-attention (or intra-attention) which allow each word in the sequence to pay attention to other words in the sequence, independently of their positions.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"MOTIVATION\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                ". . . ; x T n ] be the concatenation of a sequence of n vectors giving a matrix X ∈ R n×d such that x i ∈ R d .",
                "#TARGET_REF defined attention as a function with as input a triplet containing queries Q, keys K with associated values V .",
                "In the case of self-attention, Q, K and V are linear projections of X. Thus, we define the dot-product (#REF) .",
                "The self-attention block is repeated N times.",
                "self-attention mechanism as follows."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: . . . ; x T n ] be the concatenation of a sequence of n vectors giving a matrix X ∈ R n×d such that x i ∈ R d .\n sent1: #TARGET_REF defined attention as a function with as input a triplet containing queries Q, keys K with associated values V .\n sent2: In the case of self-attention, Q, K and V are linear projections of X. Thus, we define the dot-product (#REF) .\n sent3: The self-attention block is repeated N times.\n sent4: self-attention mechanism as follows.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "One key difference between our approach and #REF 's is that we only perform input-input attention with self-attention, as we do not have sequences as output but a text classification.",
                "Moreover, we employ global max pooling at the top, which enables our architecture to process input sequences of arbitrary length.",
                ". . . ; x T n ] be the concatenation of a sequence of n vectors giving a matrix X ∈ R n×d such that x i ∈ R d .",
                "#REF defined attention as a function with as input a triplet containing queries Q, keys K with associated values V .",
                "In the case of self-attention, Q, K and V are linear projections of X. Thus, we define the dot-product #TARGET_REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: One key difference between our approach and #REF 's is that we only perform input-input attention with self-attention, as we do not have sequences as output but a text classification.\n sent1: Moreover, we employ global max pooling at the top, which enables our architecture to process input sequences of arbitrary length.\n sent2: . . . ; x T n ] be the concatenation of a sequence of n vectors giving a matrix X ∈ R n×d such that x i ∈ R d .\n sent3: #REF defined attention as a function with as input a triplet containing queries Q, keys K with associated values V .\n sent4: In the case of self-attention, Q, K and V are linear projections of X. Thus, we define the dot-product #TARGET_REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Attention mechanisms are a way to add interpretability in neural networks.",
                "They were introduced by #REF , where they achieved state-of-the-art in machine translation.",
                "Since then, attention mechanisms have been used in other language modeling tasks such as image captioning (#REF) , question answer-ing (#REF; #REF) , and text classification (#REF) .",
                "The concept of self-attention (#REF; #REF) , central to our proposed approach, has shown great promises in natural language processing; It produced state-of-the-art results for machine translation #TARGET_REF .",
                "In text classification, the focus on interpretability has thus far been limited."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: Attention mechanisms are a way to add interpretability in neural networks.\n sent1: They were introduced by #REF , where they achieved state-of-the-art in machine translation.\n sent2: Since then, attention mechanisms have been used in other language modeling tasks such as image captioning (#REF) , question answer-ing (#REF; #REF) , and text classification (#REF) .\n sent3: The concept of self-attention (#REF; #REF) , central to our proposed approach, has shown great promises in natural language processing; It produced state-of-the-art results for machine translation #TARGET_REF .\n sent4: In text classification, the focus on interpretability has thus far been limited.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Our network (depicted in Figure 1 ) first encodes each word to its embedding.",
                "Pre-trained embeddings, like GloVe (#REF) , may be used and fine-tuned during the learning process.",
                "Next, to inject information about the order of the words, the positional encoding layer adds location information to each word.",
                "We use the positional encoding vectors that were defined by #TARGET_REF as follows.",
                "A linear layer then performs dimensionality reduction/augmentation of the embedding space to a vector space of dimension d, which is kept constant throughout the network."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: Our network (depicted in Figure 1 ) first encodes each word to its embedding.\n sent1: Pre-trained embeddings, like GloVe (#REF) , may be used and fine-tuned during the learning process.\n sent2: Next, to inject information about the order of the words, the positional encoding layer adds location information to each word.\n sent3: We use the positional encoding vectors that were defined by #TARGET_REF as follows.\n sent4: A linear layer then performs dimensionality reduction/augmentation of the embedding space to a vector space of dimension d, which is kept constant throughout the network.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF used a densely connected CNN (#REF) to apply attention to n-grams.",
                "However, their approach limits the range and acuteness of the interactions between the words in the text. and #REF both combined an attention mechanism with a recurrent neural network.",
                "The main difference with our work is, while being interpretable, these approaches do not perform true word-on-word attention across a whole sequence such as our self-attention layer.",
                "3 SANet: Self-Attention Network Inspired by the Transformer architecture #TARGET_REF which performed machine translation without recurrent or convolutional layers, we propose the Self-Attention Network (SANet) architecture targeting instead text classification.",
                "One key difference between our approach and #REF 's is that we only perform input-input attention with self-attention, as we do not have sequences as output but a text classification."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: #REF used a densely connected CNN (#REF) to apply attention to n-grams.\n sent1: However, their approach limits the range and acuteness of the interactions between the words in the text. and #REF both combined an attention mechanism with a recurrent neural network.\n sent2: The main difference with our work is, while being interpretable, these approaches do not perform true word-on-word attention across a whole sequence such as our self-attention layer.\n sent3: 3 SANet: Self-Attention Network Inspired by the Transformer architecture #TARGET_REF which performed machine translation without recurrent or convolutional layers, we propose the Self-Attention Network (SANet) architecture targeting instead text classification.\n sent4: One key difference between our approach and #REF 's is that we only perform input-input attention with self-attention, as we do not have sequences as output but a text classification.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "3 SANet: Self-Attention Network Inspired by the Transformer architecture (#REF) which performed machine translation without recurrent or convolutional layers, we propose the Self-Attention Network (SANet) architecture targeting instead text classification.",
                "One key difference between our approach and #TARGET_REF 's is that we only perform input-input attention with self-attention, as we do not have sequences as output but a text classification.",
                "Moreover, we employ global max pooling at the top, which enables our architecture to process input sequences of arbitrary length.",
                ". . . ; x T n ] be the concatenation of a sequence of n vectors giving a matrix X ∈ R n×d such that x i ∈ R d .",
                "#REF defined attention as a function with as input a triplet containing queries Q, keys K with associated values V ."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: 3 SANet: Self-Attention Network Inspired by the Transformer architecture (#REF) which performed machine translation without recurrent or convolutional layers, we propose the Self-Attention Network (SANet) architecture targeting instead text classification.\n sent1: One key difference between our approach and #TARGET_REF 's is that we only perform input-input attention with self-attention, as we do not have sequences as output but a text classification.\n sent2: Moreover, we employ global max pooling at the top, which enables our architecture to process input sequences of arbitrary length.\n sent3: . . . ; x T n ] be the concatenation of a sequence of n vectors giving a matrix X ∈ R n×d such that x i ∈ R d .\n sent4: #REF defined attention as a function with as input a triplet containing queries Q, keys K with associated values V .\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "It is followed by one or several \"self-attention blocks\" stacked one onto another.",
                "These blocks are comprised of a selfattention layer followed by a feed-forward network, both with residual connections.",
                "Contrary to #TARGET_REF , we only use a single attention head, with attention performed on the complete sequence with constant d-dimensional inputs.",
                "The feed-forward network consists of a single hidden layer with a ReLU.",
                "Where W 1 , W 2 ∈ R d×d are learned parameters."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: It is followed by one or several \"self-attention blocks\" stacked one onto another.\n sent1: These blocks are comprised of a selfattention layer followed by a feed-forward network, both with residual connections.\n sent2: Contrary to #TARGET_REF , we only use a single attention head, with attention performed on the complete sequence with constant d-dimensional inputs.\n sent3: The feed-forward network consists of a single hidden layer with a ReLU.\n sent4: Where W 1 , W 2 ∈ R d×d are learned parameters.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "In these methods, a user is often represented by the concatenation of their tweets, and the geolocation model is trained on a very small percentage of explicitly geotagged tweets, noting the potential biases implicit in geotagged tweets (#REF) .",
                "Lexical dialectology is (in part) the converse of user geolocation (#REF) : given text associated with a variety of regions, the task is to identify terms that are distinctive of particular regions.",
                "The complexity of the task is two-fold: (1) localised named entities (e.g. sporting team names) are not of interest; and (2) without semantic knowledge it is difficult to detect terms that are in general use but have a special meaning in a region.",
                "In this paper we propose a text-based geolocation method based on neural networks.",
                "Our contributions are as follows: (1) we achieve state-of-the-art results on benchmark Twitter geolocation datasets; (2) we show that the model is less sensitive to the specific location discretisation method; (3) we release the first broad-coverage dataset for evaluation of lexical dialectology models; (4) we incorporate our text-based model into a network-based model #TARGET_REF and improve the performance utilising both network and text; and (5) we use the model's embeddings for extraction of local terms and show that it outperforms two baselines."
            ],
            "label": [
                "BACKGROUND",
                "EXTENDS"
            ]
        },
        "input": "sent0: In these methods, a user is often represented by the concatenation of their tweets, and the geolocation model is trained on a very small percentage of explicitly geotagged tweets, noting the potential biases implicit in geotagged tweets (#REF) .\n sent1: Lexical dialectology is (in part) the converse of user geolocation (#REF) : given text associated with a variety of regions, the task is to identify terms that are distinctive of particular regions.\n sent2: The complexity of the task is two-fold: (1) localised named entities (e.g. sporting team names) are not of interest; and (2) without semantic knowledge it is difficult to detect terms that are in general use but have a special meaning in a region.\n sent3: In this paper we propose a text-based geolocation method based on neural networks.\n sent4: Our contributions are as follows: (1) we achieve state-of-the-art results on benchmark Twitter geolocation datasets; (2) we show that the model is less sensitive to the specific location discretisation method; (3) we release the first broad-coverage dataset for evaluation of lexical dialectology models; (4) we incorporate our text-based model into a network-based model #TARGET_REF and improve the performance utilising both network and text; and (5) we use the model's embeddings for extraction of local terms and show that it outperforms two baselines.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"EXTENDS\"], \"context\": [\"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Three main text-based approaches are: (1) the use of gazetteers #REF) ; (2) unsupervised text clustering based on topic models or similar (#REF; #REF; #REF) ; and (3) supervised classification (#REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF) , which unlike gazetteers can be applied to informal text and compared to topic models, scales better.",
                "The classification models often rely on less than 1% of geotagged tweets for supervision and discretise real-valued coordinates into equalsized grids (#REF ), administrative regions (#REF; #REF; #REF; #REF , or flat (#REF) or hierarchical k-d tree clusters (#REF) .",
                "Network-based methods also use either real-valued coordinates (#REF) or discretised regions #TARGET_REF as labels, and use label propagation over the interaction graph (e.g. @-mentions).",
                "More recent methods have focused on representation learning by using sparse coding (#REF) or neural networks (#REF) , utilising both text and network information (#REFa) .",
                "Dialect is a variety of language shared by a group of speakers (#REF) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Three main text-based approaches are: (1) the use of gazetteers #REF) ; (2) unsupervised text clustering based on topic models or similar (#REF; #REF; #REF) ; and (3) supervised classification (#REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF) , which unlike gazetteers can be applied to informal text and compared to topic models, scales better.\n sent1: The classification models often rely on less than 1% of geotagged tweets for supervision and discretise real-valued coordinates into equalsized grids (#REF ), administrative regions (#REF; #REF; #REF; #REF , or flat (#REF) or hierarchical k-d tree clusters (#REF) .\n sent2: Network-based methods also use either real-valued coordinates (#REF) or discretised regions #TARGET_REF as labels, and use label propagation over the interaction graph (e.g. @-mentions).\n sent3: More recent methods have focused on representation learning by using sparse coding (#REF) or neural networks (#REF) , utilising both text and network information (#REFa) .\n sent4: Dialect is a variety of language shared by a group of speakers (#REF) .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The classification models often rely on less than 1% of geotagged tweets for supervision and discretise real-valued coordinates into equalsized grids (#REF ), administrative regions (#REF; #REF; #REF; #REF , or flat (#REF) or hierarchical k-d tree clusters (#REF) .",
                "Network-based methods also use either real-valued coordinates (#REF) or discretised regions (#REFa) as labels, and use label propagation over the interaction graph (e.g. @-mentions).",
                "More recent methods have focused on representation learning by using sparse coding (#REF) or neural networks (#REF) , utilising both text and network information #TARGET_REF .",
                "Dialect is a variety of language shared by a group of speakers (#REF) .",
                "Our focus here is on geographical dialects which are spoken (and written in social media) by people from particular areas."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The classification models often rely on less than 1% of geotagged tweets for supervision and discretise real-valued coordinates into equalsized grids (#REF ), administrative regions (#REF; #REF; #REF; #REF , or flat (#REF) or hierarchical k-d tree clusters (#REF) .\n sent1: Network-based methods also use either real-valued coordinates (#REF) or discretised regions (#REFa) as labels, and use label propagation over the interaction graph (e.g. @-mentions).\n sent2: More recent methods have focused on representation learning by using sparse coding (#REF) or neural networks (#REF) , utilising both text and network information #TARGET_REF .\n sent3: Dialect is a variety of language shared by a group of speakers (#REF) .\n sent4: Our focus here is on geographical dialects which are spoken (and written in social media) by people from particular areas.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Comparing the two discretisation strategies, k-means outperforms k-d tree by a reasonable margin.",
                "We also incorporated the MLP predictions into a network-based model based on the method of #TARGET_REF , and improved upon their work.",
                "We analysed the Table 2 : Nearest neighbours of place names.",
                "in Figure 3 .",
                "The error is highest in states with lower training coverage (e.g. Maine, Montana, Wisconsin, Iowa and Kansas)."
            ],
            "label": [
                "BACKGROUND",
                "EXTENDS"
            ]
        },
        "input": "sent0: Comparing the two discretisation strategies, k-means outperforms k-d tree by a reasonable margin.\n sent1: We also incorporated the MLP predictions into a network-based model based on the method of #TARGET_REF , and improved upon their work.\n sent2: We analysed the Table 2 : Nearest neighbours of place names.\n sent3: in Figure 3 .\n sent4: The error is highest in states with lower training coverage (e.g. Maine, Montana, Wisconsin, Iowa and Kansas).\n",
        "output": "{\"label\": [\"BACKGROUND\", \"EXTENDS\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Note that lower numbers are better for Mean and Median, and higher numbers better for Acc@161.",
                "4 The results reported in Rahimi et al. (2015b; 2015a) for TWITTER-WORLD were over a superset of the dataset; the results reported here are based on the actual dataset.",
                "While the focus of this paper is text-based user geolocation, state-of-the-art results for the three datasets have been achieved with hybrid text+network-based models, where the predictions of the text-based model are fed into a mention network as \"dongle\" nodes to each user node, providing a personalised geolocation prior for each user #TARGET_REF .",
                "Note that it would, of course, be possible to combine text and network information in a joint deep learning model (#REF; #REF) , which we leave to future work (noting that scalability will potentially be a major issue for the larger datasets).",
                "To test the applicability of the model's embeddings in dialectology, we created DAREDS."
            ],
            "label": [
                "EXTENDS",
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Note that lower numbers are better for Mean and Median, and higher numbers better for Acc@161.\n sent1: 4 The results reported in Rahimi et al. (2015b; 2015a) for TWITTER-WORLD were over a superset of the dataset; the results reported here are based on the actual dataset.\n sent2: While the focus of this paper is text-based user geolocation, state-of-the-art results for the three datasets have been achieved with hybrid text+network-based models, where the predictions of the text-based model are fed into a mention network as \"dongle\" nodes to each user node, providing a personalised geolocation prior for each user #TARGET_REF .\n sent3: Note that it would, of course, be possible to combine text and network information in a joint deep learning model (#REF; #REF) , which we leave to future work (noting that scalability will potentially be a major issue for the larger datasets).\n sent4: To test the applicability of the model's embeddings in dialectology, we created DAREDS.\n",
        "output": "{\"label\": [\"EXTENDS\", \"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Following #REF and #REF , we evaluated the geolocation model using mean and median error in km (\"Mean\" and \"Median\" resp.) and accuracy within 161km of the actual location (\"Acc@161\").",
                "Note that lower numbers are better for Mean and Median, and higher numbers better for Acc@161.",
                "4 The results reported in Rahimi et al. (2015b; #TARGET_REF for TWITTER-WORLD were over a superset of the dataset; the results reported here are based on the actual dataset.",
                "While the focus of this paper is text-based user geolocation, state-of-the-art results for the three datasets have been achieved with hybrid text+network-based models, where the predictions of the text-based model are fed into a mention network as \"dongle\" nodes to each user node, providing a personalised geolocation prior for each user (#REFa) .",
                "Note that it would, of course, be possible to combine text and network information in a joint deep learning model (#REF; #REF) , which we leave to future work (noting that scalability will potentially be a major issue for the larger datasets)."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Following #REF and #REF , we evaluated the geolocation model using mean and median error in km (\"Mean\" and \"Median\" resp.) and accuracy within 161km of the actual location (\"Acc@161\").\n sent1: Note that lower numbers are better for Mean and Median, and higher numbers better for Acc@161.\n sent2: 4 The results reported in Rahimi et al. (2015b; #TARGET_REF for TWITTER-WORLD were over a superset of the dataset; the results reported here are based on the actual dataset.\n sent3: While the focus of this paper is text-based user geolocation, state-of-the-art results for the three datasets have been achieved with hybrid text+network-based models, where the predictions of the text-based model are fed into a mention network as \"dongle\" nodes to each user node, providing a personalised geolocation prior for each user (#REFa) .\n sent4: Note that it would, of course, be possible to combine text and network information in a joint deep learning model (#REF; #REF) , which we leave to future work (noting that scalability will potentially be a major issue for the larger datasets).\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Recently, however, there have been efforts to apply neural networks to QE and these neural approaches have shown potential for QE.",
                "#REF use continuous space language model features for sentence-level QE and word embedding features for word-level QE, in combination with other features produced by QuEst++ .",
                "#REF apply neural networks using pre-trained alignments and word lookup-table to word-level QE, which achieve the excellent performance by using the combination of baseline features at word level.",
                "However, these are not 'pure' neural approaches for QE.",
                "#TARGET_REF apply neural machine translation (NMT) models, based on recurrent neural network, to sentence-level QE."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Recently, however, there have been efforts to apply neural networks to QE and these neural approaches have shown potential for QE.\n sent1: #REF use continuous space language model features for sentence-level QE and word embedding features for word-level QE, in combination with other features produced by QuEst++ .\n sent2: #REF apply neural networks using pre-trained alignments and word lookup-table to word-level QE, which achieve the excellent performance by using the combination of baseline features at word level.\n sent3: However, these are not 'pure' neural approaches for QE.\n sent4: #TARGET_REF apply neural machine translation (NMT) models, based on recurrent neural network, to sentence-level QE.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Each quality vector is generated by decomposing the probability of each target word from the modified NMT model.",
                "1 #TARGET_REF modify the NMT model to 1) use source and target A Recurrent Neural Networks Approach for Estimating the Quality of Machine Translation Output",
                "Figure 1: First part of recurrent neural network based quality estimation model for generating quality vectors (#REF) sentences as inputs, 2 2) apply bidirectional RNNs both on source and target sentences, which enable to fully utilize the bidirectional quality information, and 3) generate quality vectors for target words as outputs.",
                "In the second part ( Figure 2 , 3 and 4), the final quality of translation at various level (sentencelevel/word-level/phrase-level) is predicted by using the quality vectors as inputs.",
                "#REF apply RNN based model to sentence-level QE and we extend this model to word and phraselevel QE."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Each quality vector is generated by decomposing the probability of each target word from the modified NMT model.\n sent1: 1 #TARGET_REF modify the NMT model to 1) use source and target A Recurrent Neural Networks Approach for Estimating the Quality of Machine Translation Output\n sent2: Figure 1: First part of recurrent neural network based quality estimation model for generating quality vectors (#REF) sentences as inputs, 2 2) apply bidirectional RNNs both on source and target sentences, which enable to fully utilize the bidirectional quality information, and 3) generate quality vectors for target words as outputs.\n sent3: In the second part ( Figure 2 , 3 and 4), the final quality of translation at various level (sentencelevel/word-level/phrase-level) is predicted by using the quality vectors as inputs.\n sent4: #REF apply RNN based model to sentence-level QE and we extend this model to word and phraselevel QE.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "1 #REF modify the NMT model to 1) use source and target A Recurrent Neural Networks Approach for Estimating the Quality of Machine Translation Output",
                "Figure 1: First part of recurrent neural network based quality estimation model for generating quality vectors #TARGET_REF sentences as inputs, 2 2) apply bidirectional RNNs both on source and target sentences, which enable to fully utilize the bidirectional quality information, and 3) generate quality vectors for target words as outputs.",
                "In the second part ( Figure 2 , 3 and 4), the final quality of translation at various level (sentencelevel/word-level/phrase-level) is predicted by using the quality vectors as inputs.",
                "#REF apply RNN based model to sentence-level QE and we extend this model to word and phraselevel QE.",
                "In subsection 2.1, 2.2 and 2.3, we describe the RNN based 3 (second part) sentence, word and phrase-level QE models."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: 1 #REF modify the NMT model to 1) use source and target A Recurrent Neural Networks Approach for Estimating the Quality of Machine Translation Output\n sent1: Figure 1: First part of recurrent neural network based quality estimation model for generating quality vectors #TARGET_REF sentences as inputs, 2 2) apply bidirectional RNNs both on source and target sentences, which enable to fully utilize the bidirectional quality information, and 3) generate quality vectors for target words as outputs.\n sent2: In the second part ( Figure 2 , 3 and 4), the final quality of translation at various level (sentencelevel/word-level/phrase-level) is predicted by using the quality vectors as inputs.\n sent3: #REF apply RNN based model to sentence-level QE and we extend this model to word and phraselevel QE.\n sent4: In subsection 2.1, 2.2 and 2.3, we describe the RNN based 3 (second part) sentence, word and phrase-level QE models.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "In RNN based sentence-level QE model (Figure 2) , HTER (human-targeted translation edit rate) (#REF) in [0,1] for target sentence is predicted by using a logistic sigmoid func-A Recurrent Neural Networks Approach for Estimating the Quality of Machine Translation Output #TARGET_REF tion such that",
                "(1)",
                "W s is the weight matrix of sigmoid function 5 at sentence-level QE.",
                "s is a summary unit of the sequential quality vectors and is fixed to the last hidden state 6 h T y of RNN.",
                "The hidden state h j is computed by"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In RNN based sentence-level QE model (Figure 2) , HTER (human-targeted translation edit rate) (#REF) in [0,1] for target sentence is predicted by using a logistic sigmoid func-A Recurrent Neural Networks Approach for Estimating the Quality of Machine Translation Output #TARGET_REF tion such that\n sent1: (1)\n sent2: W s is the weight matrix of sigmoid function 5 at sentence-level QE.\n sent3: s is a summary unit of the sequential quality vectors and is fixed to the last hidden state 6 h T y of RNN.\n sent4: The hidden state h j is computed by\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "(1)",
                "W s is the weight matrix of sigmoid function 5 at sentence-level QE.",
                "s is a summary unit of the sequential quality vectors and is fixed to the last hidden state 6 h T y of RNN.",
                "The hidden state h j is computed by",
                "where f is the activation function of RNN #TARGET_REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: (1)\n sent1: W s is the weight matrix of sigmoid function 5 at sentence-level QE.\n sent2: s is a summary unit of the sequential quality vectors and is fixed to the last hidden state 6 h T y of RNN.\n sent3: The hidden state h j is computed by\n sent4: where f is the activation function of RNN #TARGET_REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Figure 1: First part of recurrent neural network based quality estimation model for generating quality vectors (#REF) sentences as inputs, 2 2) apply bidirectional RNNs both on source and target sentences, which enable to fully utilize the bidirectional quality information, and 3) generate quality vectors for target words as outputs.",
                "In the second part ( Figure 2 , 3 and 4), the final quality of translation at various level (sentencelevel/word-level/phrase-level) is predicted by using the quality vectors as inputs.",
                "#TARGET_REF apply RNN based model to sentence-level QE and we extend this model to word and phraselevel QE.",
                "In subsection 2.1, 2.2 and 2.3, we describe the RNN based 3 (second part) sentence, word and phrase-level QE models.",
                "4 The cause of these separated parts of the QE model comes from the insufficiency of QE datasets to train the whole QE model."
            ],
            "label": [
                "USE",
                "EXTENDS"
            ]
        },
        "input": "sent0: Figure 1: First part of recurrent neural network based quality estimation model for generating quality vectors (#REF) sentences as inputs, 2 2) apply bidirectional RNNs both on source and target sentences, which enable to fully utilize the bidirectional quality information, and 3) generate quality vectors for target words as outputs.\n sent1: In the second part ( Figure 2 , 3 and 4), the final quality of translation at various level (sentencelevel/word-level/phrase-level) is predicted by using the quality vectors as inputs.\n sent2: #TARGET_REF apply RNN based model to sentence-level QE and we extend this model to word and phraselevel QE.\n sent3: In subsection 2.1, 2.2 and 2.3, we describe the RNN based 3 (second part) sentence, word and phrase-level QE models.\n sent4: 4 The cause of these separated parts of the QE model comes from the insufficiency of QE datasets to train the whole QE model.\n",
        "output": "{\"label\": [\"USE\", \"EXTENDS\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "For example, POS tags can be projected via word alignments, and the projected POS is then used to train a model in the lowresource language #REF; #TARGET_REF .",
                "These methods overall have limited effectiveness due to errors in the alignment and fundamental differences between the languages.",
                "They also assume a large parallel corpus, which may not be available for many low-resource languages.",
                "To address these limitations, we propose a new technique for low resource tagging, with more modest resource requirements: 1) a bilingual dictionary; 2) monolingual corpora in the high and low resource languages; and 3) a small annotated corpus of around 1, 000 tokens in the low-resource language.",
                "The first two resources are used as a form of distant supervision through learning crosslingual word embeddings over the monolingual corpora and bilingual dictionary (#REF) ."
            ],
            "label": [
                "BACKGROUND",
                "DIFFERENCES",
                "MOTIVATION"
            ]
        },
        "input": "sent0: For example, POS tags can be projected via word alignments, and the projected POS is then used to train a model in the lowresource language #REF; #TARGET_REF .\n sent1: These methods overall have limited effectiveness due to errors in the alignment and fundamental differences between the languages.\n sent2: They also assume a large parallel corpus, which may not be available for many low-resource languages.\n sent3: To address these limitations, we propose a new technique for low resource tagging, with more modest resource requirements: 1) a bilingual dictionary; 2) monolingual corpora in the high and low resource languages; and 3) a small annotated corpus of around 1, 000 tokens in the low-resource language.\n sent4: The first two resources are used as a form of distant supervision through learning crosslingual word embeddings over the monolingual corpora and bilingual dictionary (#REF) .\n",
        "output": "{\"label\": [\"BACKGROUND\", \"DIFFERENCES\", \"MOTIVATION\"], \"context\": [\"sent0\", \"sent1\", \"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Recently, neural network models have been developed for POS tagging and achieved good performance, such as RNN and bidirectional long short-term memory (BiLSTM) and CRF-BiLSTM models (#REF; #REF) .",
                "For example, the CRFBiLSTM POS tagger obtained the state-of-theart performance on Penn Treebank WSJ corpus (#REF) .",
                "However, in low-resource languages, these models are seldom used because of limited labelled data.",
                "Parallel data therefore appears to be the most realistic additional source of information for developing NLP systems in low-resource languages (#REF; Täckström et al., 2013; #TARGET_REF; #REF) .",
                "#REF pioneered the use of parallel data for projecting POS tag information from one language to another language."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Recently, neural network models have been developed for POS tagging and achieved good performance, such as RNN and bidirectional long short-term memory (BiLSTM) and CRF-BiLSTM models (#REF; #REF) .\n sent1: For example, the CRFBiLSTM POS tagger obtained the state-of-theart performance on Penn Treebank WSJ corpus (#REF) .\n sent2: However, in low-resource languages, these models are seldom used because of limited labelled data.\n sent3: Parallel data therefore appears to be the most realistic additional source of information for developing NLP systems in low-resource languages (#REF; Täckström et al., 2013; #TARGET_REF; #REF) .\n sent4: #REF pioneered the use of parallel data for projecting POS tag information from one language to another language.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "To model this data we employ the same model structure as above but augmented with a second perceptron output layer, as illustrated in Figure 1 (right) .",
                "Formally,ỹ t ∼ Categorial(õ t ) whereõ t = MLP(o t ) is a single hidden layer perceptron with tanh activation and softmax output transformation.",
                "This component allows for a more expressive label mapping than #TARGET_REF's linear matrix translation.",
                "Joint multi-task learning To combine the two sources of information, we use a joint objective,",
                "where N and M index the token positions in the distant and ground truth corpora, respectively, and γ is a constant balancing the two components which we set for uniform weighting, γ = |M| |N | ."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: To model this data we employ the same model structure as above but augmented with a second perceptron output layer, as illustrated in Figure 1 (right) .\n sent1: Formally,ỹ t ∼ Categorial(õ t ) whereõ t = MLP(o t ) is a single hidden layer perceptron with tanh activation and softmax output transformation.\n sent2: This component allows for a more expressive label mapping than #TARGET_REF's linear matrix translation.\n sent3: Joint multi-task learning To combine the two sources of information, we use a joint objective,\n sent4: where N and M index the token positions in the distant and ground truth corpora, respectively, and γ is a constant balancing the two components which we set for uniform weighting, γ = |M| |N | .\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "4 The BiLSTM layer uses 128 hidden units, and 32 hidden units for the transformation step.",
                "We used SGD with momentum to train models, with early stopping based on development performance.",
                "For benchmarks, we compare the proposed model against various state-of-the-art supervised learning methods, namely: a BILSTM tagger, BILSTM-CRF tagger (#REF) , and a state-of-the-art semi-supervised POS tagging algorithm, MINITAGGER (#REF) , which is also focusing on minimising the amount of labelled data.",
                "Note these methods do not use cross-lingual supervision.",
                "For a more direct comparison, we include BILSTM-DEBIAS #TARGET_REF , applied using our proposed cross-lingual supervision based on dictionaries, instead of parallel corpora; accordingly the key difference is their linear transformation for the distant data, versus our non-linear transformation to the gold data."
            ],
            "label": [
                "DIFFERENCES",
                "USE"
            ]
        },
        "input": "sent0: 4 The BiLSTM layer uses 128 hidden units, and 32 hidden units for the transformation step.\n sent1: We used SGD with momentum to train models, with early stopping based on development performance.\n sent2: For benchmarks, we compare the proposed model against various state-of-the-art supervised learning methods, namely: a BILSTM tagger, BILSTM-CRF tagger (#REF) , and a state-of-the-art semi-supervised POS tagging algorithm, MINITAGGER (#REF) , which is also focusing on minimising the amount of labelled data.\n sent3: Note these methods do not use cross-lingual supervision.\n sent4: For a more direct comparison, we include BILSTM-DEBIAS #TARGET_REF , applied using our proposed cross-lingual supervision based on dictionaries, instead of parallel corpora; accordingly the key difference is their linear transformation for the distant data, versus our non-linear transformation to the gold data.\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "The poor performance of the supervised methods suggests they are overfitting the small training set, however this is much less of a problem for our approach (labelled Joint).",
                "Note that distant supervision alone gives reasonable performance (labelled DISTANT) however the joint modelling of the ground truth and distant data yields significant improvements in almost all cases.",
                "BILSTM-DEBIAS #TARGET_REF performs worse than our proposed method, indicating that a linear transformation is insufficient for modelling distant supervision.",
                "The accuracies are higher overall for the European cf.",
                "Turkic languages, presumably because these languages are Table 1 : POS tagging accuracy on over the ten target languages, showing first approaches using only the gold data; next methods using only distant cross-lingual supervision, and lastly joint multi-task learning."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: The poor performance of the supervised methods suggests they are overfitting the small training set, however this is much less of a problem for our approach (labelled Joint).\n sent1: Note that distant supervision alone gives reasonable performance (labelled DISTANT) however the joint modelling of the ground truth and distant data yields significant improvements in almost all cases.\n sent2: BILSTM-DEBIAS #TARGET_REF performs worse than our proposed method, indicating that a linear transformation is insufficient for modelling distant supervision.\n sent3: The accuracies are higher overall for the European cf.\n sent4: Turkic languages, presumably because these languages are Table 1 : POS tagging accuracy on over the ten target languages, showing first approaches using only the gold data; next methods using only distant cross-lingual supervision, and lastly joint multi-task learning.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We now describe the modelling framework for POS tagging in a low-resource language, based on very limited linguistic resources.",
                "Our approach extends the work of #TARGET_REF , who present a model based on distant supervision in the form of cross-lingual projection and use projected tags generated from parallel corpora as distant annotations.",
                "There are three main differences between their work and ours: 1) We do not use parallel corpora, but instead use a bilingual dictionary for knowledge transfer.",
                "2) Our model uses a more expressive multi-layer perceptron when generating the gold standard tags.",
                "The multi-layer perceptron can capture both language-specific infor- mation and consistent tagging errors arising from this method of supervision."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: We now describe the modelling framework for POS tagging in a low-resource language, based on very limited linguistic resources.\n sent1: Our approach extends the work of #TARGET_REF , who present a model based on distant supervision in the form of cross-lingual projection and use projected tags generated from parallel corpora as distant annotations.\n sent2: There are three main differences between their work and ours: 1) We do not use parallel corpora, but instead use a bilingual dictionary for knowledge transfer.\n sent3: 2) Our model uses a more expressive multi-layer perceptron when generating the gold standard tags.\n sent4: The multi-layer perceptron can capture both language-specific infor- mation and consistent tagging errors arising from this method of supervision.\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "In particular, we start with the gentle introduction to the fundamentals of reinforcement learning (#REF; #REF) .",
                "We further discuss We further discuss several critical issues in DRL solutions for NLP tasks, including (1) The efficient and practical design of the action space, state space, and reward functions; (2) The trade-off between exploration and exploitation; and (3) The goal of incorporating linguistic structures in DRL.",
                "To address the model design issue, we discuss several recent solutions (#REFb; #REF; #TARGET_REF .",
                "We then focus on a new case study of hierarchical deep reinforcement learning for video captioning (#REFb) , discussing the techniques of leveraging hierarchies in DRL for NLP generation problems.",
                "This tutorial aims at introducing deep reinforcement learning methods to researchers in the NLP community."
            ],
            "label": [
                "BACKGROUND",
                "DIFFERENCES"
            ]
        },
        "input": "sent0: In particular, we start with the gentle introduction to the fundamentals of reinforcement learning (#REF; #REF) .\n sent1: We further discuss We further discuss several critical issues in DRL solutions for NLP tasks, including (1) The efficient and practical design of the action space, state space, and reward functions; (2) The trade-off between exploration and exploitation; and (3) The goal of incorporating linguistic structures in DRL.\n sent2: To address the model design issue, we discuss several recent solutions (#REFb; #REF; #TARGET_REF .\n sent3: We then focus on a new case study of hierarchical deep reinforcement learning for video captioning (#REFb) , discussing the techniques of leveraging hierarchies in DRL for NLP generation problems.\n sent4: This tutorial aims at introducing deep reinforcement learning methods to researchers in the NLP community.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"DIFFERENCES\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "This tutorial draws connections from theories of deep reinforcement learning to practical applications in NLP.",
                "In particular, we start with the gentle introduction to the fundamentals of reinforcement learning (#REF; #REF) .",
                "We further discuss their modern deep learning extensions such as Deep QNetworks (#REF) , Policy Networks (#REF) , and Deep Hierarchical Reinforcement Learning (#REF) .",
                "We outline the applications of deep reinforcement learning in NLP, including dialog (#REF) , semi-supervised text classification (#REF) , coreference (#REF; #REF) , knowledge graph reasoning #TARGET_REF ), text games (#REF; #REFa) , social media (#REFb; #REF) , information extraction (#REF; #REF) , language and vision (#REF; #REF; #REFa,b,c; #REF) , etc.",
                "We further discuss several critical issues in DRL solutions for NLP tasks, including (1) The efficient and practical design of the action space, state space, and reward functions; (2) The trade-off between exploration and exploitation; and (3) The goal of incorporating linguistic structures in DRL."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This tutorial draws connections from theories of deep reinforcement learning to practical applications in NLP.\n sent1: In particular, we start with the gentle introduction to the fundamentals of reinforcement learning (#REF; #REF) .\n sent2: We further discuss their modern deep learning extensions such as Deep QNetworks (#REF) , Policy Networks (#REF) , and Deep Hierarchical Reinforcement Learning (#REF) .\n sent3: We outline the applications of deep reinforcement learning in NLP, including dialog (#REF) , semi-supervised text classification (#REF) , coreference (#REF; #REF) , knowledge graph reasoning #TARGET_REF ), text games (#REF; #REFa) , social media (#REFb; #REF) , information extraction (#REF; #REF) , language and vision (#REF; #REF; #REFa,b,c; #REF) , etc.\n sent4: We further discuss several critical issues in DRL solutions for NLP tasks, including (1) The efficient and practical design of the action space, state space, and reward functions; (2) The trade-off between exploration and exploitation; and (3) The goal of incorporating linguistic structures in DRL.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "We further discuss several critical issues in DRL solutions for NLP tasks, including (1) The efficient and practical design of the action space, state space, and reward functions; (2) The trade-off between exploration and exploitation; and (3) The goal of incorporating linguistic structures in DRL.",
                "To address the model design issue, we discuss several recent solutions (#REFb; #REF; #TARGET_REF .",
                "We then focus on a new case study of hierarchical deep reinforcement learning for video captioning (#REFb) , discussing the techniques of leveraging hierarchies in DRL for NLP generation problems.",
                "This tutorial aims at introducing deep reinforcement learning methods to researchers in the NLP community.",
                "We do not assume any particular prior knowledge in reinforcement learning."
            ],
            "label": [
                "BACKGROUND",
                "DIFFERENCES"
            ]
        },
        "input": "sent0: We further discuss several critical issues in DRL solutions for NLP tasks, including (1) The efficient and practical design of the action space, state space, and reward functions; (2) The trade-off between exploration and exploitation; and (3) The goal of incorporating linguistic structures in DRL.\n sent1: To address the model design issue, we discuss several recent solutions (#REFb; #REF; #TARGET_REF .\n sent2: We then focus on a new case study of hierarchical deep reinforcement learning for video captioning (#REFb) , discussing the techniques of leveraging hierarchies in DRL for NLP generation problems.\n sent3: This tutorial aims at introducing deep reinforcement learning methods to researchers in the NLP community.\n sent4: We do not assume any particular prior knowledge in reinforcement learning.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"DIFFERENCES\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The added paths consisted of unlexicalized dependency labels obtained from a dependency parsed external corpus.",
                "To improve the expressivity of the added paths, instead of the unlexicalized labels, #TARGET_REF augmented the KB graph with verbs (surface relations) from a corpus containing over 600 million Subject-Verb-Object (SVO) triples.",
                "These verbs act as edges that connect previously unconnected entities thereby increasing the connectivity of the KB graph which can potentially improve PRA performance.",
                "However, naïvely adding these edges increases the feature sparsity which degrades the discriminative ability of the logistic regression classifier used in PRA.",
                "This can be addressed by adding latent relations obtained by clustering the surface relations, instead of directly adding the surface relations."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The added paths consisted of unlexicalized dependency labels obtained from a dependency parsed external corpus.\n sent1: To improve the expressivity of the added paths, instead of the unlexicalized labels, #TARGET_REF augmented the KB graph with verbs (surface relations) from a corpus containing over 600 million Subject-Verb-Object (SVO) triples.\n sent2: These verbs act as edges that connect previously unconnected entities thereby increasing the connectivity of the KB graph which can potentially improve PRA performance.\n sent3: However, naïvely adding these edges increases the feature sparsity which degrades the discriminative ability of the logistic regression classifier used in PRA.\n sent4: This can be addressed by adding latent relations obtained by clustering the surface relations, instead of directly adding the surface relations.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "This can be addressed by adding latent relations obtained by clustering the surface relations, instead of directly adding the surface relations.",
                "This reduces feature sparsity and has been shown to improve PRA inference #TARGET_REF , (#REF) .",
                "In this article we propose a scheme for augmenting the KB using paths obtained by mining noun phrases that connect two SVO triples from an external corpus.",
                "We term these noun phrases as bridging entities since they bridge two KB relations to form a path.",
                "This is different from the scheme in (#REF) and (#REF) , which adds edges between KB nodes by mining surface relations from an external corpus."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This can be addressed by adding latent relations obtained by clustering the surface relations, instead of directly adding the surface relations.\n sent1: This reduces feature sparsity and has been shown to improve PRA inference #TARGET_REF , (#REF) .\n sent2: In this article we propose a scheme for augmenting the KB using paths obtained by mining noun phrases that connect two SVO triples from an external corpus.\n sent3: We term these noun phrases as bridging entities since they bridge two KB relations to form a path.\n sent4: This is different from the scheme in (#REF) and (#REF) , which adds edges between KB nodes by mining surface relations from an external corpus.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "It was extended by (#REF) , to improve the inference by augmenting the KB with syntactic information obtained from a dependency parsed corpus.",
                "Augmenting the KB for improving PRA inference using surface relations mined from an external corpus and using latent edge labels obtained by performing PCA on the surface relations was explored in #TARGET_REF .",
                "Instead of hard mapping of surface relations to latent embeddings, (#REF ) perform a 'soft' mapping using vector space random walks.",
                "This allows the random walker to traverse an edge semantically similar to the current edge type more frequently than other edges.",
                "Although, like others, we too use an external corpus to augment the KB, the crucial difference in our approach is that apart from adding surface relations, we also add bridging entities that enable us to create new paths in the KB."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: It was extended by (#REF) , to improve the inference by augmenting the KB with syntactic information obtained from a dependency parsed corpus.\n sent1: Augmenting the KB for improving PRA inference using surface relations mined from an external corpus and using latent edge labels obtained by performing PCA on the surface relations was explored in #TARGET_REF .\n sent2: Instead of hard mapping of surface relations to latent embeddings, (#REF ) perform a 'soft' mapping using vector space random walks.\n sent3: This allows the random walker to traverse an edge semantically similar to the current edge type more frequently than other edges.\n sent4: Although, like others, we too use an external corpus to augment the KB, the crucial difference in our approach is that apart from adding surface relations, we also add bridging entities that enable us to create new paths in the KB.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF and PRA-VS are the systems proposed in #TARGET_REF and (#REF) respectively, where the KB graph is augmented with edges mined from a large subject-verb-object (SVO) triple corpus.",
                "In these two systems, only new edges are added over the fixed set of nodes, and the augmentation happens in a batch, offline setting.",
                "In contrast, PRA-ODA, the method proposed in the paper, also expands the set of nodes through bridging entities, and performs the augmentation in an on-demand manner."
            ],
            "label": [
                "BACKGROUND",
                "MOTIVATION"
            ]
        },
        "input": "sent0: #TARGET_REF and PRA-VS are the systems proposed in #TARGET_REF and (#REF) respectively, where the KB graph is augmented with edges mined from a large subject-verb-object (SVO) triple corpus.\n sent1: In these two systems, only new edges are added over the fixed set of nodes, and the augmentation happens in a batch, offline setting.\n sent2: In contrast, PRA-ODA, the method proposed in the paper, also expands the set of nodes through bridging entities, and performs the augmentation in an on-demand manner.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"MOTIVATION\"], \"context\": [\"sent0\", \"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "and journalistwritesforpublication).",
                "The hyperparameter values d max = 2, K = 10 reported the highest MRR and were used for the rest of the relations.",
                "For the L 1 and L 2 regularization parameters in the logistic regression classifier, we used the same values as used in ( #TARGET_REF; #REF) , viz., L 1 = 0.005, and L 2 = 1.0.",
                "This is because the parameters were reported to be robust, and seemed to work well even when the knowledge base was augmented.",
                "We compare the results (PRA-ODA) with the PRA algorithm executed on the NELL KB, NELL KB augmented with surface relations (PRA-SVO) (#REF) and vector space random walk PRA (PRA-VS) (#REF) ."
            ],
            "label": [
                "USE",
                "SIMILARITY"
            ]
        },
        "input": "sent0: and journalistwritesforpublication).\n sent1: The hyperparameter values d max = 2, K = 10 reported the highest MRR and were used for the rest of the relations.\n sent2: For the L 1 and L 2 regularization parameters in the logistic regression classifier, we used the same values as used in ( #TARGET_REF; #REF) , viz., L 1 = 0.005, and L 2 = 1.0.\n sent3: This is because the parameters were reported to be robust, and seemed to work well even when the knowledge base was augmented.\n sent4: We compare the results (PRA-ODA) with the PRA algorithm executed on the NELL KB, NELL KB augmented with surface relations (PRA-SVO) (#REF) and vector space random walk PRA (PRA-VS) (#REF) .\n",
        "output": "{\"label\": [\"USE\", \"SIMILARITY\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "In addition to the time taken for the full SVO augmentation, PRA-VS takes additional time to generate embeddings (13 minutes) from the added verbs.",
                "We note that the batch augmentation in case of PRA-SVO and PRA-VS, and embedding computation in case of PRA-VS are all specific to the relations in the evaluation set, and hence can't be ignored as a one-time offline cost.",
                "In other words, these costs are likely to increase as more relations (and their instances) are included during training and testing.",
                "Runtime gains with PRA-ODA are likely to be even more pronounced in such settings.",
                "An additional advantage of the proposed algorithm is that it can also be run on the top of any PRA based algorithm such as the #TARGET_REF and PRA-VS."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: In addition to the time taken for the full SVO augmentation, PRA-VS takes additional time to generate embeddings (13 minutes) from the added verbs.\n sent1: We note that the batch augmentation in case of PRA-SVO and PRA-VS, and embedding computation in case of PRA-VS are all specific to the relations in the evaluation set, and hence can't be ignored as a one-time offline cost.\n sent2: In other words, these costs are likely to increase as more relations (and their instances) are included during training and testing.\n sent3: Runtime gains with PRA-ODA are likely to be even more pronounced in such settings.\n sent4: An additional advantage of the proposed algorithm is that it can also be run on the top of any PRA based algorithm such as the #TARGET_REF and PRA-VS.\n",
        "output": "{\"label\": [null], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Systems, such as treebank-based parsers #TARGET_REF; #REF) and semantic role labelers (#REF; #REF) , are trained and tested on hand-annotated data.",
                "Evaluation is based on differences between system output and test data.",
                "Other systems use these programs to perform tasks unrelated to the original annotation.",
                "For example, participating systems in CONLL (#REF; Hajič et al., 2009 ), ACE and GALE tasks merged the results of several processors (parsers, named entity recognizers, etc.) not initially designed for the task at hand.",
                "This paper discusses differences between handannotated data and automatically generated data with respect to our GLARFers, systems for generating Grammatical and Logical Representation Framework (GLARF) for English, Chinese and Japanese sentences."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Systems, such as treebank-based parsers #TARGET_REF; #REF) and semantic role labelers (#REF; #REF) , are trained and tested on hand-annotated data.\n sent1: Evaluation is based on differences between system output and test data.\n sent2: Other systems use these programs to perform tasks unrelated to the original annotation.\n sent3: For example, participating systems in CONLL (#REF; Hajič et al., 2009 ), ACE and GALE tasks merged the results of several processors (parsers, named entity recognizers, etc.) not initially designed for the task at hand.\n sent4: This paper discusses differences between handannotated data and automatically generated data with respect to our GLARFers, systems for generating Grammatical and Logical Representation Framework (GLARF) for English, Chinese and Japanese sentences.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "While annotation guidelines may direct a human annotator to prefer, for example, high attachment, systems output may have other preferences, e.g., the probability that land is modified by a PP (headed by in) versus the probability that acres can be so modified.",
                "Even if the manual annotation for a particular corpus is consistent when it comes to other factors such as tokenization or part of speech, developers of parsers sometimes change these guidelines to suit their needs.",
                "For example, users of the Charniak parser #TARGET_REF should add the AUX category to the PTB parts of speech and adjust their systems to account for the conversion of the word ain't into the tokens IS and n't.",
                "Similarly, tokenization decisions with respect to hyphens vary among different versions of the Penn Treebank, as well as different parsers based on these treebanks.",
                "Thus if a system uses multiple parsers, such differences must be accounted for."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: While annotation guidelines may direct a human annotator to prefer, for example, high attachment, systems output may have other preferences, e.g., the probability that land is modified by a PP (headed by in) versus the probability that acres can be so modified.\n sent1: Even if the manual annotation for a particular corpus is consistent when it comes to other factors such as tokenization or part of speech, developers of parsers sometimes change these guidelines to suit their needs.\n sent2: For example, users of the Charniak parser #TARGET_REF should add the AUX category to the PTB parts of speech and adjust their systems to account for the conversion of the word ain't into the tokens IS and n't.\n sent3: Similarly, tokenization decisions with respect to hyphens vary among different versions of the Penn Treebank, as well as different parsers based on these treebanks.\n sent4: Thus if a system uses multiple parsers, such differences must be accounted for.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We use Charniak, UMD and KNP parsers #TARGET_REF; #REF; #REF) , JET Named Entity tagger (#REF; #REF) and other resources in conjunction with languagespecific GLARFers that incorporate hand-written rules to convert output of these processors into a final representation, including logic1 structure, the focus of this paper.",
                "English GLARFer rules use Comlex (#REFa) and the various NomBank lexicons (http:// nlp.cs.nyu.edu/meyers/nombank/) for lexical lookup.",
                "The GLARF rules implemented vary by language as follows.",
                "English: correcting/standardizing phrase boundaries and part of speech (POS); recognizing multiword expressions; marking subconstituents; labeling relations; incorporating NEs; regularizing infinitival, passives, relatives, VP deletion, predicative and numerous other constructions.",
                "Chinese: correcting/standardizing phrase boundaries and POS, marking subconstituents, labeling relations; regularizing copula constructions; incorporating NEs; recognizing dates and number expressions."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We use Charniak, UMD and KNP parsers #TARGET_REF; #REF; #REF) , JET Named Entity tagger (#REF; #REF) and other resources in conjunction with languagespecific GLARFers that incorporate hand-written rules to convert output of these processors into a final representation, including logic1 structure, the focus of this paper.\n sent1: English GLARFer rules use Comlex (#REFa) and the various NomBank lexicons (http:// nlp.cs.nyu.edu/meyers/nombank/) for lexical lookup.\n sent2: The GLARF rules implemented vary by language as follows.\n sent3: English: correcting/standardizing phrase boundaries and part of speech (POS); recognizing multiword expressions; marking subconstituents; labeling relations; incorporating NEs; regularizing infinitival, passives, relatives, VP deletion, predicative and numerous other constructions.\n sent4: Chinese: correcting/standardizing phrase boundaries and POS, marking subconstituents, labeling relations; regularizing copula constructions; incorporating NEs; recognizing dates and number expressions.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "So far, most researchers interested in co-occurrence of mutual translations have relied on bitexts where sentence boundaries (or other text unit boundaries) were easy to find (e.g. Gale & #REF; Kumano & #REF; #REF; #TARGET_REF .",
                "Aligned text segments suggest a boundary-based model of cooccurrence, illustrated in Figure 2 .",
                "For bitexts involving languages with similar word order, a more accurate combined model of co-occurrence can be built using both segment boundary information and the map-distance threshold.",
                "As shown in Figure 3 , each of these constraints eliminates the noise from a characteristic region of the bitext space.",
                "00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: So far, most researchers interested in co-occurrence of mutual translations have relied on bitexts where sentence boundaries (or other text unit boundaries) were easy to find (e.g. Gale & #REF; Kumano & #REF; #REF; #TARGET_REF .\n sent1: Aligned text segments suggest a boundary-based model of cooccurrence, illustrated in Figure 2 .\n sent2: For bitexts involving languages with similar word order, a more accurate combined model of co-occurrence can be built using both segment boundary information and the map-distance threshold.\n sent3: As shown in Figure 3 , each of these constraints eliminates the noise from a characteristic region of the bitext space.\n sent4: 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000 00000000000000000000000000000000\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Yet, these constraints do not answer the question of how to count co-occurrences within the restricted regions.",
                "It is somewhat surprising that this is a question at all, and most authors ignore it.",
                "However, when authors specify their algorithms in sufficient detail to answer this question, the most common answer (given, e.g., by #REF; #REF; #REF; #TARGET_REF turns out to be unsound.",
                "The problem is easiest to illustrate under the boundary-based model of co-occurrence.",
                "Given two aligned text segments, the naive way to count co-occurrences is"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Yet, these constraints do not answer the question of how to count co-occurrences within the restricted regions.\n sent1: It is somewhat surprising that this is a question at all, and most authors ignore it.\n sent2: However, when authors specify their algorithms in sufficient detail to answer this question, the most common answer (given, e.g., by #REF; #REF; #REF; #TARGET_REF turns out to be unsound.\n sent3: The problem is easiest to illustrate under the boundary-based model of co-occurrence.\n sent4: Given two aligned text segments, the naive way to count co-occurrences is\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Co-occurrence is a universal precondition for translational equivalence among word tokens in bitexts.",
                "Other preconditions may be imposed if certain language-specific resources are available #TARGET_REF .",
                "For example, parts of speech tend to be preserved in translation (#REF) .",
                "If part-of-speech taggers are available for both languages in a bitext, and if cases where one part of speech is translated to another are not important for the intended application, then we can rule out the possibility of translational equivalence for all token pairs involving different parts of speech.",
                "A more obvious source of language-specific information is a machine-readable bilingual dictionary (MRBD)."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Co-occurrence is a universal precondition for translational equivalence among word tokens in bitexts.\n sent1: Other preconditions may be imposed if certain language-specific resources are available #TARGET_REF .\n sent2: For example, parts of speech tend to be preserved in translation (#REF) .\n sent3: If part-of-speech taggers are available for both languages in a bitext, and if cases where one part of speech is translated to another are not important for the intended application, then we can rule out the possibility of translational equivalence for all token pairs involving different parts of speech.\n sent4: A more obvious source of language-specific information is a machine-readable bilingual dictionary (MRBD).\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Similarly exclusive candidacy can be granted to cognate token pairs (#REF) .",
                "Most published translation models treat co-occurrence counts as counts of potential link tokens (#REF) .",
                "More accurate models may result if the co-occurrence counts are biased with language-specific knowledge.",
                "Without loss of generality, whenever translation models refer to cooccurrence counts, they can refer to co-occurrence counts that have been filtered using whatever language-specific resources happen to be available.",
                "It does not matter if there are dependencies among the different knowledge sources, as long as each is used as a simple filter on the co-occurrence relation #TARGET_REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Similarly exclusive candidacy can be granted to cognate token pairs (#REF) .\n sent1: Most published translation models treat co-occurrence counts as counts of potential link tokens (#REF) .\n sent2: More accurate models may result if the co-occurrence counts are biased with language-specific knowledge.\n sent3: Without loss of generality, whenever translation models refer to cooccurrence counts, they can refer to co-occurrence counts that have been filtered using whatever language-specific resources happen to be available.\n sent4: It does not matter if there are dependencies among the different knowledge sources, as long as each is used as a simple filter on the co-occurrence relation #TARGET_REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "The last years have witnessed a continuous progress in statistical multilingual models for syntax, thanks to shared tasks such as CoNLL 2006-7 (#REF; #REF) and, more recently, SPMRL 2013-14 (#REF; #REF) .",
                "As a global trend, we observe that models that incorporate rich global features are typically more accurate, even if pruning is necessary or decoding needs to be approximate #REF; #REF; #REF #TARGET_REF .",
                "The same rationale applies to semantic dependency parsing, also a structured prediction problem, but where the output variable is a semantic graph, rather than a syntactic tree.",
                "Indeed, the best performing systems in last year shared task on broad-coverage semantic dependency parsing follow this principle (#REF) .",
                "This year, a new challenge was put forth: how to handle multiple languages and out-ofdomain data?"
            ],
            "label": [
                "BACKGROUND",
                "MOTIVATION"
            ]
        },
        "input": "sent0: The last years have witnessed a continuous progress in statistical multilingual models for syntax, thanks to shared tasks such as CoNLL 2006-7 (#REF; #REF) and, more recently, SPMRL 2013-14 (#REF; #REF) .\n sent1: As a global trend, we observe that models that incorporate rich global features are typically more accurate, even if pruning is necessary or decoding needs to be approximate #REF; #REF; #REF #TARGET_REF .\n sent2: The same rationale applies to semantic dependency parsing, also a structured prediction problem, but where the output variable is a semantic graph, rather than a syntactic tree.\n sent3: Indeed, the best performing systems in last year shared task on broad-coverage semantic dependency parsing follow this principle (#REF) .\n sent4: This year, a new challenge was put forth: how to handle multiple languages and out-ofdomain data?\n",
        "output": "{\"label\": [\"BACKGROUND\", \"MOTIVATION\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF for further details.",
                "The parser was built as an extension of a recent dependency parser, TurboParser (#REF #TARGET_REF , with the goal of performing semantic parsing using any of the three formalisms considered in the shared task (DM, PAS, and PSD).",
                "We have followed prior work in semantic role labeling (#REF; #REF; #REF; #REF) , by adding constraints and modeling interactions among arguments within the same frame; however, we went beyond such sibling interactions to consider more complex grandparent and co-parent structures, effectively correlating different predicates.",
                "The overall set of parts used by our parser is illustrated in Figure 1 ; note that by using only a subset of the parts (predicate, arc, labeled arc, and sibling parts), the semantic parser decodes each predicate frame independently from other predicates; it is the co-parent and grandparent parts that have the effect of creating inter-dependence among predicates; we will analyze the effect of these dependencies in the experimental section ( §3).",
                "For each part in our model (shown in Figure 1 ), we computed binary features based on various combination of lexical forms, lemmas, POS tags and syntactic dependency relations of words related to the corresponding predicates and arguments."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: #REF for further details.\n sent1: The parser was built as an extension of a recent dependency parser, TurboParser (#REF #TARGET_REF , with the goal of performing semantic parsing using any of the three formalisms considered in the shared task (DM, PAS, and PSD).\n sent2: We have followed prior work in semantic role labeling (#REF; #REF; #REF; #REF) , by adding constraints and modeling interactions among arguments within the same frame; however, we went beyond such sibling interactions to consider more complex grandparent and co-parent structures, effectively correlating different predicates.\n sent3: The overall set of parts used by our parser is illustrated in Figure 1 ; note that by using only a subset of the parts (predicate, arc, labeled arc, and sibling parts), the semantic parser decodes each predicate frame independently from other predicates; it is the co-parent and grandparent parts that have the effect of creating inter-dependence among predicates; we will analyze the effect of these dependencies in the experimental section ( §3).\n sent4: For each part in our model (shown in Figure 1 ), we computed binary features based on various combination of lexical forms, lemmas, POS tags and syntactic dependency relations of words related to the corresponding predicates and arguments.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The parser was built as an extension of a recent dependency parser, TurboParser (#REF (#REF , with the goal of performing semantic parsing using any of the three formalisms considered in the shared task (DM, PAS, and PSD).",
                "We have followed prior work in semantic role labeling (#REF; #REF; #REF; #REF) , by adding constraints and modeling interactions among arguments within the same frame; however, we went beyond such sibling interactions to consider more complex grandparent and co-parent structures, effectively correlating different predicates.",
                "The overall set of parts used by our parser is illustrated in Figure 1 ; note that by using only a subset of the parts (predicate, arc, labeled arc, and sibling parts), the semantic parser decodes each predicate frame independently from other predicates; it is the co-parent and grandparent parts that have the effect of creating inter-dependence among predicates; we will analyze the effect of these dependencies in the experimental section ( §3).",
                "For each part in our model (shown in Figure 1 ), we computed binary features based on various combination of lexical forms, lemmas, POS tags and syntactic dependency relations of words related to the corresponding predicates and arguments.",
                "Most of these features were taken from TurboParser #TARGET_REF , and others were inspired by the semantic parser of #REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The parser was built as an extension of a recent dependency parser, TurboParser (#REF (#REF , with the goal of performing semantic parsing using any of the three formalisms considered in the shared task (DM, PAS, and PSD).\n sent1: We have followed prior work in semantic role labeling (#REF; #REF; #REF; #REF) , by adding constraints and modeling interactions among arguments within the same frame; however, we went beyond such sibling interactions to consider more complex grandparent and co-parent structures, effectively correlating different predicates.\n sent2: The overall set of parts used by our parser is illustrated in Figure 1 ; note that by using only a subset of the parts (predicate, arc, labeled arc, and sibling parts), the semantic parser decodes each predicate frame independently from other predicates; it is the co-parent and grandparent parts that have the effect of creating inter-dependence among predicates; we will analyze the effect of these dependencies in the experimental section ( §3).\n sent3: For each part in our model (shown in Figure 1 ), we computed binary features based on various combination of lexical forms, lemmas, POS tags and syntactic dependency relations of words related to the corresponding predicates and arguments.\n sent4: Most of these features were taken from TurboParser #TARGET_REF , and others were inspired by the semantic parser of #REF .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "To circumvent this problem, #TARGET_REF proposed a WSD method that can be applied to a language even when no sense tagged corpus for that language is available.",
                "This is achieved by projecting Wordnet and corpus parameters from another language to the language in question.",
                "The approach is centered on a novel synset based multilingual dictionary (#REF) where the synsets of different languages are aligned and thereafter the words within the synsets are manually cross-linked.",
                "For example, the word W L 1 belonging to synset S of language L 1 will be manually cross-linked to the word W L 2 of the corresponding synset in language L 2 to indicate that W L 2 is the best substitute for W L 1 according to an experienced bilingual speaker's intuition.",
                "We extend their work by addressing the following question on the economics of annotation, lexicon building and performance:"
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: To circumvent this problem, #TARGET_REF proposed a WSD method that can be applied to a language even when no sense tagged corpus for that language is available.\n sent1: This is achieved by projecting Wordnet and corpus parameters from another language to the language in question.\n sent2: The approach is centered on a novel synset based multilingual dictionary (#REF) where the synsets of different languages are aligned and thereafter the words within the synsets are manually cross-linked.\n sent3: For example, the word W L 1 belonging to synset S of language L 1 will be manually cross-linked to the word W L 2 of the corresponding synset in language L 2 to indicate that W L 2 is the best substitute for W L 1 according to an experienced bilingual speaker's intuition.\n sent4: We extend their work by addressing the following question on the economics of annotation, lexicon building and performance:\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "In section 4 we discuss the work of #TARGET_REF on parameter projection for multilingual WSD.",
                "Section 5 is on the economics of multilingual WSD.",
                "In section 6 we propose a probabilistic model for representing the cross-linkage of words within synsets.",
                "In section 7 we present a strategy for injecting hard-to-disambiguate cases from the target language using selective sampling.",
                "In section 8 we introduce a measure for cost-benefit analysis for calculating the value for money in terms of accuracy, annotation effort and lexicon building effort."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In section 4 we discuss the work of #TARGET_REF on parameter projection for multilingual WSD.\n sent1: Section 5 is on the economics of multilingual WSD.\n sent2: In section 6 we propose a probabilistic model for representing the cross-linkage of words within synsets.\n sent3: In section 7 we present a strategy for injecting hard-to-disambiguate cases from the target language using selective sampling.\n sent4: In section 8 we introduce a measure for cost-benefit analysis for calculating the value for money in terms of accuracy, annotation effort and lexicon building effort.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Knowledge based approaches to WSD such as Lesk's algorithm (#REF ), Walker's algorithm (#REF) , Conceptual Density (#REF) and PageRank (#REF) are less demanding in terms of resources but fail to deliver good results.",
                "Supervised approaches like SVM (#REF) and k-NN (#REF) , on the other hand, give better accuracies, but the requirement of large annotated corpora renders them unsuitable for resource scarce languages.",
                "Recent work by #TARGET_REF has shown that it is possible to project the parameters learnt from the annotation work of one language to another language provided aligned Wordnets for two languages are available.",
                "However, their work does not address the question of further improving the accuracy of WSD by using a small amount of training data from the target language.",
                "Some similar work has been done in the area of domain adaptation where #REF showed that adding just 30% of the target data to the source data achieved the same performance as that obtained by taking the entire source and target data."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Knowledge based approaches to WSD such as Lesk's algorithm (#REF ), Walker's algorithm (#REF) , Conceptual Density (#REF) and PageRank (#REF) are less demanding in terms of resources but fail to deliver good results.\n sent1: Supervised approaches like SVM (#REF) and k-NN (#REF) , on the other hand, give better accuracies, but the requirement of large annotated corpora renders them unsuitable for resource scarce languages.\n sent2: Recent work by #TARGET_REF has shown that it is possible to project the parameters learnt from the annotation work of one language to another language provided aligned Wordnets for two languages are available.\n sent3: However, their work does not address the question of further improving the accuracy of WSD by using a small amount of training data from the target language.\n sent4: Some similar work has been done in the area of domain adaptation where #REF showed that adding just 30% of the target data to the source data achieved the same performance as that obtained by taking the entire source and target data.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The second factor is the cost of sense annotated data from the target language.",
                "The third factor is the accuracy of WSD The first two factors in some sense relate to the cost of purchasing a commodity and the third factor relates to the commodity itself.",
                "The work of #TARGET_REF as described above does not attempt to reach an optimal costbenefit point in this economic system.",
                "They place their bets on manual cross-linking only and settle for the accuracy achieved thereof.",
                "Specifically, they do not explore the inclusion of small amount of annotated data from the target language to boost the accuracy (as mentioned earlier, supervised systems which use annotated data from the target language are known to perform better)."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The second factor is the cost of sense annotated data from the target language.\n sent1: The third factor is the accuracy of WSD The first two factors in some sense relate to the cost of purchasing a commodity and the third factor relates to the commodity itself.\n sent2: The work of #TARGET_REF as described above does not attempt to reach an optimal costbenefit point in this economic system.\n sent3: They place their bets on manual cross-linking only and settle for the accuracy achieved thereof.\n sent4: Specifically, they do not explore the inclusion of small amount of annotated data from the target language to boost the accuracy (as mentioned earlier, supervised systems which use annotated data from the target language are known to perform better).\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "In the following paragraphs, we explain our probabilistic cross linking model.",
                "The model proposed by #TARGET_REF ) is a deterministic model where the expected count for (Sense S, Marathi Word W ), i.e., the number of times the word W appears in sense S is approximated by the count for the corresponding cross linked Hindi word.",
                "Such a model assumes that each Marathi word links to appropriate Hindi word(s) as identified manually by a lexicographer.",
                "Instead, we propose a probabilistic model where a Marathi word can link to every word in the corresponding Hindi synset with some probability.",
                "The expected count for (S, W ) can then be estimated as:"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In the following paragraphs, we explain our probabilistic cross linking model.\n sent1: The model proposed by #TARGET_REF ) is a deterministic model where the expected count for (Sense S, Marathi Word W ), i.e., the number of times the word W appears in sense S is approximated by the count for the corresponding cross linked Hindi word.\n sent2: Such a model assumes that each Marathi word links to appropriate Hindi word(s) as identified manually by a lexicographer.\n sent3: Instead, we propose a probabilistic model where a Marathi word can link to every word in the corresponding Hindi synset with some probability.\n sent4: The expected count for (S, W ) can then be estimated as:\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "There it makes sense to place fewer bets on manual crosslinking and more on collecting annotated corpora.",
                "On the other hand if manual cross-linking is cheap then a very small amount of annotated corpora can be used in conjunction with full manual crosslinking to boost the accuracy.",
                "Based on the above discussion, if k a is the cost of sense annotating one word, k c is the cost of manually cross-linking a word and A is the accuracy desired then the problem of multilingual WSD can be cast as an optimization problem:",
                "Accuracy ≥ A where, w c and w a are the number of words to be manually cross linked and annotated respectively.",
                "Ours is thus a 3-factor economic model (crosslinking, annotation and accuracy) as opposed to the 2-factor model (cross-linking, accuracy) proposed by #TARGET_REF ."
            ],
            "label": [
                "DIFFERENCES",
                "EXTENDS"
            ]
        },
        "input": "sent0: There it makes sense to place fewer bets on manual crosslinking and more on collecting annotated corpora.\n sent1: On the other hand if manual cross-linking is cheap then a very small amount of annotated corpora can be used in conjunction with full manual crosslinking to boost the accuracy.\n sent2: Based on the above discussion, if k a is the cost of sense annotating one word, k c is the cost of manually cross-linking a word and A is the accuracy desired then the problem of multilingual WSD can be cast as an optimization problem:\n sent3: Accuracy ≥ A where, w c and w a are the number of words to be manually cross linked and annotated respectively.\n sent4: Ours is thus a 3-factor economic model (crosslinking, annotation and accuracy) as opposed to the 2-factor model (cross-linking, accuracy) proposed by #TARGET_REF .\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"EXTENDS\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "We used Hindi as the source language (S L ) and trained a WSD engine using Hindi sense tagged corpus.",
                "The parameters thus learnt were then projected using the MultiDict (refer section 3 and 4) to build a resource conscious Marathi (T L ) WSD engine.",
                "We used the same dataset as described in #TARGET_REF for all our experiments.",
                "The data was collected from two domains, viz., Tourism and Health.",
                "The data for Tourism domain was collected by manually translating English documents downloaded from Indian Tourism websites into Hindi and Marathi."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: We used Hindi as the source language (S L ) and trained a WSD engine using Hindi sense tagged corpus.\n sent1: The parameters thus learnt were then projected using the MultiDict (refer section 3 and 4) to build a resource conscious Marathi (T L ) WSD engine.\n sent2: We used the same dataset as described in #TARGET_REF for all our experiments.\n sent3: The data was collected from two domains, viz., Tourism and Health.\n sent4: The data for Tourism domain was collected by manually translating English documents downloaded from Indian Tourism websites into Hindi and Marathi.\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "A common approach to Information Extraction (IE) is to (manually or automatically) create a set of patterns which match against text to identify information of interest.",
                "#REF reviewed the approaches which were used at the time and found that the most common techniques relied on lexicosyntactic patterns being applied to text which has undergone relatively shallow linguistic processing.",
                "For example, the extraction rules used by #REF and #REF match text in which syntactic chunks have been identified.",
                "More recently researchers have begun to employ deeper syntactic analysis, such as dependency parsing (#REF; #REF; #TARGET_REF; #REF) .",
                "In these approaches extraction patterns are essentially parts of the dependency tree."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: A common approach to Information Extraction (IE) is to (manually or automatically) create a set of patterns which match against text to identify information of interest.\n sent1: #REF reviewed the approaches which were used at the time and found that the most common techniques relied on lexicosyntactic patterns being applied to text which has undergone relatively shallow linguistic processing.\n sent2: For example, the extraction rules used by #REF and #REF match text in which syntactic chunks have been identified.\n sent3: More recently researchers have begun to employ deeper syntactic analysis, such as dependency parsing (#REF; #REF; #TARGET_REF; #REF) .\n sent4: In these approaches extraction patterns are essentially parts of the dependency tree.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "For example the patterns used by #REF are the subject-verb-object tuples from the dependency tree (the remainder of the dependency parse is discarded) while #TARGET_REF allow any subtree within the dependency parse to act as an extraction pattern.",
                "#REF showed that the choice of pattern model has important implications for IE algorithms including significant differences between the various models in terms of their ability to identify information of interest in text.",
                "However, there has been little comparison between the various pattern models.",
                "Those which have been carried out have been limited by the fact that they used indirect tasks to evaluate the various models and did not compare them in an IE scenario.",
                "We address this limitation here by presenting a direct comparison of four previously described pattern models using an unsupervised learning method applied to a commonly used IE scenario."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For example the patterns used by #REF are the subject-verb-object tuples from the dependency tree (the remainder of the dependency parse is discarded) while #TARGET_REF allow any subtree within the dependency parse to act as an extraction pattern.\n sent1: #REF showed that the choice of pattern model has important implications for IE algorithms including significant differences between the various models in terms of their ability to identify information of interest in text.\n sent2: However, there has been little comparison between the various pattern models.\n sent3: Those which have been carried out have been limited by the fact that they used indirect tasks to evaluate the various models and did not compare them in an IE scenario.\n sent4: We address this limitation here by presenting a direct comparison of four previously described pattern models using an unsupervised learning method applied to a commonly used IE scenario.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Predicate-Argument Model (SVO): A simple approach, used by #REF , #REF and , is to use subject-verb-object tuples from the dependency parse as extraction patterns.",
                "These consist of a verb and its subject and/or direct object.",
                "Figure  2 shows the two SVO patterns 1 which are produced for the dependency tree shown in Figure 1 .",
                "This model can identify information which is expressed using simple predicate-argument constructions such as the relation between Acme and Smith 1 The formalism used for representing dependency patterns is similar to the one introduced by #TARGET_REF .",
                "Each node in the tree is represented in the format a[b/c] (e.g. subj[N/Acme]) where c is the lexical item (Acme), b its grammatical tag (N) and a the dependency relation between this node and its parent (subj)."
            ],
            "label": [
                "BACKGROUND",
                "USE"
            ]
        },
        "input": "sent0: Predicate-Argument Model (SVO): A simple approach, used by #REF , #REF and , is to use subject-verb-object tuples from the dependency parse as extraction patterns.\n sent1: These consist of a verb and its subject and/or direct object.\n sent2: Figure  2 shows the two SVO patterns 1 which are produced for the dependency tree shown in Figure 1 .\n sent3: This model can identify information which is expressed using simple predicate-argument constructions such as the relation between Acme and Smith 1 The formalism used for representing dependency patterns is similar to the one introduced by #TARGET_REF .\n sent4: Each node in the tree is represented in the format a[b/c] (e.g. subj[N/Acme]) where c is the lexical item (Acme), b its grammatical tag (N) and a the dependency relation between this node and its parent (subj).\n",
        "output": "{\"label\": [\"BACKGROUND\", \"USE\"], \"context\": [\"sent0\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Example linked chains are shown in Figure 2 .",
                "This pattern representation encodes most of the information in the sentence with the advantage of being able to link together event participants which neither of the SVO or chain model can, for example the relation between \"Smith\" and \"Bloggs\" in Figure 1 .",
                "Subtrees: The final model to be considered is the subtree model #TARGET_REF .",
                "In this model any subtree of a dependency tree can be used as an extraction pattern, where a subtree is any set of nodes in the tree which are connected to one another.",
                "Single nodes are not considered to be subtrees."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Example linked chains are shown in Figure 2 .\n sent1: This pattern representation encodes most of the information in the sentence with the advantage of being able to link together event participants which neither of the SVO or chain model can, for example the relation between \"Smith\" and \"Bloggs\" in Figure 1 .\n sent2: Subtrees: The final model to be considered is the subtree model #TARGET_REF .\n sent3: In this model any subtree of a dependency tree can be used as an extraction pattern, where a subtree is any set of nodes in the tree which are connected to one another.\n sent4: Single nodes are not considered to be subtrees.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "There have been few direct comparisons of the various pattern models.",
                "#TARGET_REF compared three models (SVO, chains and subtrees) on two IE scenarios using a entity extraction task.",
                "Models were evaluated in terms of their ability to identify entities taking part in events and distinguish them from those which did not.",
                "They found the SVO model performed poorly in comparison with the other two models and that the performance of the subtree model was generally the same as, or better than, the chain model.",
                "However, they did not attempt to determine whether the models could identify the relations between these entities, simply whether they could identify the entities participating in relevant events."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: There have been few direct comparisons of the various pattern models.\n sent1: #TARGET_REF compared three models (SVO, chains and subtrees) on two IE scenarios using a entity extraction task.\n sent2: Models were evaluated in terms of their ability to identify entities taking part in events and distinguish them from those which did not.\n sent3: They found the SVO model performed poorly in comparison with the other two models and that the performance of the subtree model was generally the same as, or better than, the chain model.\n sent4: However, they did not attempt to determine whether the models could identify the relations between these entities, simply whether they could identify the entities participating in relevant events.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The subtree model could represent more of the relations than any other model but that there was no statistical difference between those relations and the ones covered by the linked chain model.",
                "They concluded that the linked chain model was optional since it is expressive enough to represent the information of interest without introducing a potentially unwieldy number of patterns.",
                "There is some agreement between these two studies, for example that the SVO model performs poorly in comparison with other models.",
                "However, #REF also found that the coverage of the chain model was significantly worse than the subtree model, although #TARGET_REF found that in some cases their performance could not be distinguished.",
                "In addition to these disagreements, these studies are also limited by the fact that they are indirect; they do not evaluate the various pattern models on an IE task."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The subtree model could represent more of the relations than any other model but that there was no statistical difference between those relations and the ones covered by the linked chain model.\n sent1: They concluded that the linked chain model was optional since it is expressive enough to represent the information of interest without introducing a potentially unwieldy number of patterns.\n sent2: There is some agreement between these two studies, for example that the SVO model performs poorly in comparison with other models.\n sent3: However, #REF also found that the coverage of the chain model was significantly worse than the subtree model, although #TARGET_REF found that in some cases their performance could not be distinguished.\n sent4: In addition to these disagreements, these studies are also limited by the fact that they are indirect; they do not evaluate the various pattern models on an IE task.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Equation 1 combines two factors: the term frequency (in relevant documents) and inverse document frequency (across the corpus).",
                "Patterns which occur frequently in relevant documents without being too prevalent in the corpus are preferred.",
                "#TARGET_REF found that it was important to find the appropriate balance between these two factors.",
                "They introduced the β parameter as a way of controlling the relative contribution of the inverse document frequency.",
                "β is tuned for each extraction task and pattern model combination."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Equation 1 combines two factors: the term frequency (in relevant documents) and inverse document frequency (across the corpus).\n sent1: Patterns which occur frequently in relevant documents without being too prevalent in the corpus are preferred.\n sent2: #TARGET_REF found that it was important to find the appropriate balance between these two factors.\n sent3: They introduced the β parameter as a way of controlling the relative contribution of the inverse document frequency.\n sent4: β is tuned for each extraction task and pattern model combination.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "For example, #REF used subtrees for parse ranking but could only generate subtrees which appear at least ten times in a 40,000 sentence corpus.",
                "They comment that the size of their data set meant that it would have been difficult to complete the experiments with less restrictive parameters.",
                "In addition, #TARGET_REF only generated subtrees which appeared in at least three documents.",
                "#REF and #REF both used the rightmost extension algorithm to generate subtrees.",
                "To provide a direct comparison of the pattern models we also produced versions of the sets of patterns extracted for the SVO, chain and linked chain models in which patterns which occurred fewer than four times were removed."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For example, #REF used subtrees for parse ranking but could only generate subtrees which appear at least ten times in a 40,000 sentence corpus.\n sent1: They comment that the size of their data set meant that it would have been difficult to complete the experiments with less restrictive parameters.\n sent2: In addition, #TARGET_REF only generated subtrees which appeared in at least three documents.\n sent3: #REF and #REF both used the rightmost extension algorithm to generate subtrees.\n sent4: To provide a direct comparison of the pattern models we also produced versions of the sets of patterns extracted for the SVO, chain and linked chain models in which patterns which occurred fewer than four times were removed.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "For example, #REF used subtrees for parse ranking but could only generate subtrees which appear at least ten times in a 40,000 sentence corpus.",
                "They comment that the size of their data set meant that it would have been difficult to complete the experiments with less restrictive parameters.",
                "In addition, #REF only generated subtrees which appeared in at least three documents.",
                "#REF and #TARGET_REF both used the rightmost extension algorithm to generate subtrees.",
                "To provide a direct comparison of the pattern models we also produced versions of the sets of patterns extracted for the SVO, chain and linked chain models in which patterns which occurred fewer than four times were removed."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For example, #REF used subtrees for parse ranking but could only generate subtrees which appear at least ten times in a 40,000 sentence corpus.\n sent1: They comment that the size of their data set meant that it would have been difficult to complete the experiments with less restrictive parameters.\n sent2: In addition, #REF only generated subtrees which appeared in at least three documents.\n sent3: #REF and #TARGET_REF both used the rightmost extension algorithm to generate subtrees.\n sent4: To provide a direct comparison of the pattern models we also produced versions of the sets of patterns extracted for the SVO, chain and linked chain models in which patterns which occurred fewer than four times were removed.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "We compared each of the patterns models described in Section 2 using an unsupervised IE experiment similar to one described by #TARGET_REF .",
                "Let D be a corpus of documents and R a set of documents which are relevant to a particular extraction task.",
                "In this context \"relevant\" means that the document contains the information we are interested in identifying.",
                "D and R are such that D = R ∪R and R∩R = ∅. As assumption behind this approach is that useful patterns will be far more likely to occur in R than D overall."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We compared each of the patterns models described in Section 2 using an unsupervised IE experiment similar to one described by #TARGET_REF .\n sent1: Let D be a corpus of documents and R a set of documents which are relevant to a particular extraction task.\n sent2: In this context \"relevant\" means that the document contains the information we are interested in identifying.\n sent3: D and R are such that D = R ∪R and R∩R = ∅. As assumption behind this approach is that useful patterns will be far more likely to occur in R than D overall.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "The value of β in equation 1 was set using a separate corpus from which the patterns were generated, a methodology suggested by #TARGET_REF .",
                "To generate this additional text we used the Reuters Corpus (#REF ) which consists of a year's worth of newswire output.",
                "Each document in the Reuters corpus has been manually annotated with topic codes indicating its general subject area(s).",
                "One of these topic codes (C411) refers to management succession events and was used to identify documents which are relevant to the MUC6 IE scenario.",
                "A corpus consisting of 348 documents annotated with code C411 and 250 documents without that code, representing irrelevant documents, were taken from the Reuters corpus to create a corpus with the same distribution of relevant and irrelevant documents as found in the MUC-6 corpus."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The value of β in equation 1 was set using a separate corpus from which the patterns were generated, a methodology suggested by #TARGET_REF .\n sent1: To generate this additional text we used the Reuters Corpus (#REF ) which consists of a year's worth of newswire output.\n sent2: Each document in the Reuters corpus has been manually annotated with topic codes indicating its general subject area(s).\n sent3: One of these topic codes (C411) refers to management succession events and was used to identify documents which are relevant to the MUC6 IE scenario.\n sent4: A corpus consisting of 348 documents annotated with code C411 and 250 documents without that code, representing irrelevant documents, were taken from the Reuters corpus to create a corpus with the same distribution of relevant and irrelevant documents as found in the MUC-6 corpus.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Relevant information describes an executive entering or leaving a position within a company, for example \"Last month Smith resigned as CEO of Rooter Ltd.\".",
                "This sentence described as event involving three items: a person (Smith), position (CEO) and company (Rooter Ltd).",
                "We made use of a version of the MUC-6 corpus described by #REF which consists of 598 documents.",
                "For these experiments relevant documents were identified using annotations in the corpus.",
                "However, this is not necessary since #TARGET_REF showed that adequate knowledge about document relevance could be obtained automatically using an IR system."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Relevant information describes an executive entering or leaving a position within a company, for example \"Last month Smith resigned as CEO of Rooter Ltd.\".\n sent1: This sentence described as event involving three items: a person (Smith), position (CEO) and company (Rooter Ltd).\n sent2: We made use of a version of the MUC-6 corpus described by #REF which consists of 598 documents.\n sent3: For these experiments relevant documents were identified using annotations in the corpus.\n sent4: However, this is not necessary since #TARGET_REF showed that adequate knowledge about document relevance could be obtained automatically using an IR system.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Our intuitive explanation for this result is that lazy learning techniques keep all training items, whereas greedy approaches lose useful information by forgetting low-frequency or exceptional instances of the task, not covered by the extracted rules or models (#REF) .",
                "Apart from the empirical work in Tilburg and Antwerp, a number of recent studies on statistical natural language processing (e.g. Dagan & #REF; #TARGET_REF also suggest that, contrary to common wisdom, forgetting specific training items, even when they represent extremely low-frequency events, is harmful to generalization accuracy.",
                "After reviewing this empirical work briefly, I will report on new results (work in progress in collaboration with van den Bosch and Zavrel), systematically comparing greedy and lazy learning techniques on a number of benehrnark natural language processing tasks: tagging, grapheme-to-phoneme conversion, and pp-attachment.",
                "The results show that forgetting individual training items, however \"improbable' they may be, is indeed harmful.",
                "Furthermore, they show that combining lazy learning with training set editing techniques (based on typicality and other regularity criteria) also leads to worse generalization results."
            ],
            "label": [
                "BACKGROUND",
                "SIMILARITY"
            ]
        },
        "input": "sent0: Our intuitive explanation for this result is that lazy learning techniques keep all training items, whereas greedy approaches lose useful information by forgetting low-frequency or exceptional instances of the task, not covered by the extracted rules or models (#REF) .\n sent1: Apart from the empirical work in Tilburg and Antwerp, a number of recent studies on statistical natural language processing (e.g. Dagan & #REF; #TARGET_REF also suggest that, contrary to common wisdom, forgetting specific training items, even when they represent extremely low-frequency events, is harmful to generalization accuracy.\n sent2: After reviewing this empirical work briefly, I will report on new results (work in progress in collaboration with van den Bosch and Zavrel), systematically comparing greedy and lazy learning techniques on a number of benehrnark natural language processing tasks: tagging, grapheme-to-phoneme conversion, and pp-attachment.\n sent3: The results show that forgetting individual training items, however \"improbable' they may be, is indeed harmful.\n sent4: Furthermore, they show that combining lazy learning with training set editing techniques (based on typicality and other regularity criteria) also leads to worse generalization results.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"SIMILARITY\"], \"context\": [\"sent1\", \"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Our intuitive explanation for this result is that lazy learning techniques keep all training items, whereas greedy approaches lose useful information by forgetting low-frequency or exceptional instances of the task, not covered by the extracted rules or models (#REF) .",
                "Apart from the empirical work in Tilburg and Antwerp, a number of recent studies on statistical natural language processing (e.g. Dagan & #REF; #TARGET_REF also suggest that, contrary to common wisdom, forgetting specific training items, even when they represent extremely low-frequency events, is harmful to generalization accuracy.",
                "After reviewing this empirical work briefly, I will report on new results (work in progress in collaboration with van den Bosch and Zavrel), systematically comparing greedy and lazy learning techniques on a number of benehrnark natural language processing tasks: tagging, grapheme-to-phoneme conversion, and pp-attachment.",
                "The results show that forgetting individual training items, however \"improbable' they may be, is indeed harmful.",
                "Furthermore, they show that combining lazy learning with training set editing techniques (based on typicality and other regularity criteria) also leads to worse generalization results."
            ],
            "label": [
                "BACKGROUND",
                "EXTENDS"
            ]
        },
        "input": "sent0: Our intuitive explanation for this result is that lazy learning techniques keep all training items, whereas greedy approaches lose useful information by forgetting low-frequency or exceptional instances of the task, not covered by the extracted rules or models (#REF) .\n sent1: Apart from the empirical work in Tilburg and Antwerp, a number of recent studies on statistical natural language processing (e.g. Dagan & #REF; #TARGET_REF also suggest that, contrary to common wisdom, forgetting specific training items, even when they represent extremely low-frequency events, is harmful to generalization accuracy.\n sent2: After reviewing this empirical work briefly, I will report on new results (work in progress in collaboration with van den Bosch and Zavrel), systematically comparing greedy and lazy learning techniques on a number of benehrnark natural language processing tasks: tagging, grapheme-to-phoneme conversion, and pp-attachment.\n sent3: The results show that forgetting individual training items, however \"improbable' they may be, is indeed harmful.\n sent4: Furthermore, they show that combining lazy learning with training set editing techniques (based on typicality and other regularity criteria) also leads to worse generalization results.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"EXTENDS\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "LSTM-based language models have been shown effective in Word Sense Disambiguation (WSD).",
                "In particular, the technique proposed by #TARGET_REF returned state-of-the-art performance in several benchmarks, but neither the training data nor the source code was released.",
                "This paper presents the results of a reproduction study and analysis of this technique using only openly available datasets (GigaWord, SemCor, OMSTI) and software (TensorFlow).",
                "Our study showed that similar results can be obtained with much less data than hinted at by #REF .",
                "Detailed analyses shed light on the strengths and weaknesses of this method."
            ],
            "label": [
                "BACKGROUND",
                "USE"
            ]
        },
        "input": "sent0: LSTM-based language models have been shown effective in Word Sense Disambiguation (WSD).\n sent1: In particular, the technique proposed by #TARGET_REF returned state-of-the-art performance in several benchmarks, but neither the training data nor the source code was released.\n sent2: This paper presents the results of a reproduction study and analysis of this technique using only openly available datasets (GigaWord, SemCor, OMSTI) and software (TensorFlow).\n sent3: Our study showed that similar results can be obtained with much less data than hinted at by #REF .\n sent4: Detailed analyses shed light on the strengths and weaknesses of this method.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"USE\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Many approaches have been proposed -the more popular ones include the usage of Support Vector Machine (SVM) (#REF) , SVM combined with unsupervised trained embeddings (#REF; Rothe and Schütze, 2017) , and graph-based approaches (#REF; #REF) .",
                "In recent years, there has been a surge in interest in using Long short-term memory (LSTM) (#REF) to perform WSD (#REFb; #REF) .",
                "These approaches are characterized by their high performance, simplicity and their ability to extract a lot of information from raw text.",
                "Among the best-performing ones is the approach by #TARGET_REF , in which an LSTM language model trained on a corpus with 100 billion tokens was coupled with small sense-annotated datasets to achieve state-of-the-art performance in all-words WSD.",
                "Even though the results obtained by #REF outperform the previous state-of-the-art, neither the used datasets nor the constructed models are available to the community."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Many approaches have been proposed -the more popular ones include the usage of Support Vector Machine (SVM) (#REF) , SVM combined with unsupervised trained embeddings (#REF; Rothe and Schütze, 2017) , and graph-based approaches (#REF; #REF) .\n sent1: In recent years, there has been a surge in interest in using Long short-term memory (LSTM) (#REF) to perform WSD (#REFb; #REF) .\n sent2: These approaches are characterized by their high performance, simplicity and their ability to extract a lot of information from raw text.\n sent3: Among the best-performing ones is the approach by #TARGET_REF , in which an LSTM language model trained on a corpus with 100 billion tokens was coupled with small sense-annotated datasets to achieve state-of-the-art performance in all-words WSD.\n sent4: Even though the results obtained by #REF outperform the previous state-of-the-art, neither the used datasets nor the constructed models are available to the community.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The method proposed by #TARGET_REF performs WSD by annotating each lemma in a text with one WordNet synset that is associated with its meaning.",
                "Broadly speaking, the disambiguation is done by: 1) constructing a language model from a large unannotated dataset; 2) extracting sense embeddings from this model using a much smaller annotated dataset; 3) relying on the sense embeddings to make predictions on the lemmas in unseen sentences.",
                "Each operation is described below.",
                "Constructing Language Models.",
                "Long Short-Term Memory (LSTM) (#REF ) is a celebrated recurrent neural network architecture that has proven to be effective in many natural language processing tasks (#REF; #REF; #REF, among others) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The method proposed by #TARGET_REF performs WSD by annotating each lemma in a text with one WordNet synset that is associated with its meaning.\n sent1: Broadly speaking, the disambiguation is done by: 1) constructing a language model from a large unannotated dataset; 2) extracting sense embeddings from this model using a much smaller annotated dataset; 3) relying on the sense embeddings to make predictions on the lemmas in unseen sentences.\n sent2: Each operation is described below.\n sent3: Constructing Language Models.\n sent4: Long Short-Term Memory (LSTM) (#REF ) is a celebrated recurrent neural network architecture that has proven to be effective in many natural language processing tasks (#REF; #REF; #REF, among others) .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Constructing Language Models.",
                "Long Short-Term Memory (LSTM) (#REF ) is a celebrated recurrent neural network architecture that has proven to be effective in many natural language processing tasks (#REF; #REF; #REF, among others) .",
                "Different from previous architectures, LSTM is equipped with trainable gates that control the flow of information, allowing the neural networks to learn both short-and long-range dependencies.",
                "In #TARGET_REF , the first operation consists of constructing an LSTM language model to capture the meaning of words in context.",
                "They use an LSTM network with a single hidden layer of h nodes."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Constructing Language Models.\n sent1: Long Short-Term Memory (LSTM) (#REF ) is a celebrated recurrent neural network architecture that has proven to be effective in many natural language processing tasks (#REF; #REF; #REF, among others) .\n sent2: Different from previous architectures, LSTM is equipped with trainable gates that control the flow of information, allowing the neural networks to learn both short-and long-range dependencies.\n sent3: In #TARGET_REF , the first operation consists of constructing an LSTM language model to capture the meaning of words in context.\n sent4: They use an LSTM network with a single hidden layer of h nodes.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "3. The procedure invokes a subroutine to choose one of the n senses for the context vector c t .",
                "It selects the sense whose vector is closest to c t using cosine as the similarity function.",
                "Label Propagation.",
                "#TARGET_REF argue that the averaging procedure is suboptimal because of two reasons.",
                "First, the distribution of occurrences of senses is unknown whereas averaging is only suitable for spherical clusters."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: 3. The procedure invokes a subroutine to choose one of the n senses for the context vector c t .\n sent1: It selects the sense whose vector is closest to c t using cosine as the similarity function.\n sent2: Label Propagation.\n sent3: #TARGET_REF argue that the averaging procedure is suboptimal because of two reasons.\n sent4: First, the distribution of occurrences of senses is unknown whereas averaging is only suitable for spherical clusters.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Notice that the lack of available models has been explicitly mentioned, in a recent work, as the cause for the missing comparison of this technique with other competitors (#REFb, footnote 10) .",
                "On the other hand, we present other experiments to shed more light on the value of this and similar methods.",
                "We anticipate some conclusions.",
                "First, a positive result is that we were able to reproduce the method from #TARGET_REF and obtain similar results to the ones originally published.",
                "However, to our surprise, these results were obtained using a much smaller corpus of 1.8 billion tokens (Gigaword), which is less than 2% of the data used in the original study."
            ],
            "label": [
                "USE",
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Notice that the lack of available models has been explicitly mentioned, in a recent work, as the cause for the missing comparison of this technique with other competitors (#REFb, footnote 10) .\n sent1: On the other hand, we present other experiments to shed more light on the value of this and similar methods.\n sent2: We anticipate some conclusions.\n sent3: First, a positive result is that we were able to reproduce the method from #TARGET_REF and obtain similar results to the ones originally published.\n sent4: However, to our surprise, these results were obtained using a much smaller corpus of 1.8 billion tokens (Gigaword), which is less than 2% of the data used in the original study.\n",
        "output": "{\"label\": [\"USE\", \"DIFFERENCES\"], \"context\": [\"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "The features are then added to the default feature set of IMS (#REF) .",
                "Moreover, Raganato et al. (2017b) present a number of end-to-end neural WSD architectures.",
                "The best performing one is based on a bidirectional Long Short-Term Memory (BLSTM) with attention and two auxiliary loss functions (part-of-speech and the WordNet coarse-grained semantic labels).",
                "#REF also make use of unannotated data to train a BLSTM.",
                "The work by #TARGET_REF , which we consider in this paper, belongs to this last category."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The features are then added to the default feature set of IMS (#REF) .\n sent1: Moreover, Raganato et al. (2017b) present a number of end-to-end neural WSD architectures.\n sent2: The best performing one is based on a bidirectional Long Short-Term Memory (BLSTM) with attention and two auxiliary loss functions (part-of-speech and the WordNet coarse-grained semantic labels).\n sent3: #REF also make use of unannotated data to train a BLSTM.\n sent4: The work by #TARGET_REF , which we consider in this paper, belongs to this last category.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "For the training of the sense embeddings, we use the same two corpora used by #TARGET_REF: 1. SemCor (#REF ) is a corpus containing approximately 240,000 sense annotated words.",
                "The tagged documents originate from the Brown corpus (#REF) and cover various genres.",
                "2. OMSTI (#REF) contains one million sense annotations automatically tagged by exploiting the English-Chinese part of the parallel MultiUN corpus (#REF) .",
                "A list of English translations were manually created for each WordNet sense.",
                "If the Chinese translation of an English word matches one of the manually curated translations for a WordNet sense, that sense is selected."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: For the training of the sense embeddings, we use the same two corpora used by #TARGET_REF: 1. SemCor (#REF ) is a corpus containing approximately 240,000 sense annotated words.\n sent1: The tagged documents originate from the Brown corpus (#REF) and cover various genres.\n sent2: 2. OMSTI (#REF) contains one million sense annotations automatically tagged by exploiting the English-Chinese part of the parallel MultiUN corpus (#REF) .\n sent3: A list of English translations were manually created for each WordNet sense.\n sent4: If the Chinese translation of an English word matches one of the manually curated translations for a WordNet sense, that sense is selected.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "In the naming of our models, LSTM indicates that the averaging technique was used for the sense assignment, while LSTMLP refers to the results obtained using label propagation (see Section 3).",
                "The datasets following T: indicate the annotated corpus used to represent the senses while U:OMSTI stands for using OMSTI as unlabeled sentences in case label propagation is used.",
                "P: SemCor indicates that sense distributions from SemCor are used in the system architecture.",
                "Three scorers are used: \"framework\" refers to the WSD evaluation framework from Raganato et al. (2017a) ; \"mapping to WN3.0\" refers to the evaluation used by #TARGET_REF while \"competition\" refers to the scorer provided by the competition itself (e.g., semeval2013).",
                "1,644 test instances in total, which are all nouns."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In the naming of our models, LSTM indicates that the averaging technique was used for the sense assignment, while LSTMLP refers to the results obtained using label propagation (see Section 3).\n sent1: The datasets following T: indicate the annotated corpus used to represent the senses while U:OMSTI stands for using OMSTI as unlabeled sentences in case label propagation is used.\n sent2: P: SemCor indicates that sense distributions from SemCor are used in the system architecture.\n sent3: Three scorers are used: \"framework\" refers to the WSD evaluation framework from Raganato et al. (2017a) ; \"mapping to WN3.0\" refers to the evaluation used by #TARGET_REF while \"competition\" refers to the scorer provided by the competition itself (e.g., semeval2013).\n sent4: 1,644 test instances in total, which are all nouns.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "In this section, we report our reproduction of the results of #TARGET_REF and additional experiments to gain a deeper insight into the strengths and weaknesses of the approach.",
                "These experiments focus on the performance on the most-and less-frequent senses, coverage of the annotated dataset and the consequent impact on the overall predictions, the granularity of the sense representation, and the impact of the unannotated data and model complexity on the accuracy of WSD.",
                "Reproduction results.",
                "We trained the LSTM model with the best reported settings in #REF (hidden layer size h = 2048, embedding dimensionality p = 512) using a machine equipped with an Intel Xeon E5-2650, 256GB of RAM, 8TB of disk space, and two nVIDIA GeForce GTX 1080 Ti GPUs.",
                "During our training, one epoch took about one day to finish with TensorFlow fully utilizing one GPU."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In this section, we report our reproduction of the results of #TARGET_REF and additional experiments to gain a deeper insight into the strengths and weaknesses of the approach.\n sent1: These experiments focus on the performance on the most-and less-frequent senses, coverage of the annotated dataset and the consequent impact on the overall predictions, the granularity of the sense representation, and the impact of the unannotated data and model complexity on the accuracy of WSD.\n sent2: Reproduction results.\n sent3: We trained the LSTM model with the best reported settings in #REF (hidden layer size h = 2048, embedding dimensionality p = 512) using a machine equipped with an Intel Xeon E5-2650, 256GB of RAM, 8TB of disk space, and two nVIDIA GeForce GTX 1080 Ti GPUs.\n sent4: During our training, one epoch took about one day to finish with TensorFlow fully utilizing one GPU.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Reproduction results.",
                "We trained the LSTM model with the best reported settings in #TARGET_REF (hidden layer size h = 2048, embedding dimensionality p = 512) using a machine equipped with an Intel Xeon E5-2650, 256GB of RAM, 8TB of disk space, and two nVIDIA GeForce GTX 1080 Ti GPUs.",
                "During our training, one epoch took about one day to finish with TensorFlow fully utilizing one GPU.",
                "The whole training process took four months.",
                "We tested the performance of the downstream WSD task three times during the training and observed that the best performance is obtained at the 65 th epoch, despite a later model producing a lower negative log-likelihood."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Reproduction results.\n sent1: We trained the LSTM model with the best reported settings in #TARGET_REF (hidden layer size h = 2048, embedding dimensionality p = 512) using a machine equipped with an Intel Xeon E5-2650, 256GB of RAM, 8TB of disk space, and two nVIDIA GeForce GTX 1080 Ti GPUs.\n sent2: During our training, one epoch took about one day to finish with TensorFlow fully utilizing one GPU.\n sent3: The whole training process took four months.\n sent4: We tested the performance of the downstream WSD task three times during the training and observed that the best performance is obtained at the 65 th epoch, despite a later model producing a lower negative log-likelihood.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Table 1 presents the results using the test sets senseval2 and semeval2013, respectively.",
                "The top part of the table presents our reproduction results, the middle part reports the results from #TARGET_REF , while the bottom part reports a representative sample of the other state-of-the-art approaches.",
                "It should be noted that with the test set semeval2013, all scorers use WordNet 3.0, therefore the performance of the various methods can be directly compared.",
                "However, not all answers in senseval2 can be mapped to WN3.0 and we do not know how #REF handled these cases.",
                "In the WSD evaluation framework (#REF ) that we selected for evaluation, these cases were either re-annotated or removed."
            ],
            "label": [
                "USE",
                "SIMILARITY"
            ]
        },
        "input": "sent0: Table 1 presents the results using the test sets senseval2 and semeval2013, respectively.\n sent1: The top part of the table presents our reproduction results, the middle part reports the results from #TARGET_REF , while the bottom part reports a representative sample of the other state-of-the-art approaches.\n sent2: It should be noted that with the test set semeval2013, all scorers use WordNet 3.0, therefore the performance of the various methods can be directly compared.\n sent3: However, not all answers in senseval2 can be mapped to WN3.0 and we do not know how #REF handled these cases.\n sent4: In the WSD evaluation framework (#REF ) that we selected for evaluation, these cases were either re-annotated or removed.\n",
        "output": "{\"label\": [\"USE\", \"SIMILARITY\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "This analysis is important because it is well-known that the simple strategy of always choosing the MFS is a strong baseline in WSD, thus there is a tendency for WSD systems to overfit towards the MFS (#REF) .",
                "Table 2 shows that the method by #TARGET_REF does not overfit towards the MFS to the same extent as other supervised systems since the recall on LFS instances is still quite high 0.41 (a lower recall on LFS instances than on MFS ones is expected due to the reduced training data for them).",
                "On semeval13, the recall on LFS is already relatively high using only SemCor (0.33), and reaches 0.38 when using both SemCor and OMSTI.",
                "For comparison, the default system IMS (#REF) trained on SemCor only obtains an R lfs of 0.15 on semeval13 (#REF) and only reaches 0.33 with a large amount of annotated data.",
                "Finally, our implementation of the label propagation does seem to slightly overfit towards the MFS."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This analysis is important because it is well-known that the simple strategy of always choosing the MFS is a strong baseline in WSD, thus there is a tendency for WSD systems to overfit towards the MFS (#REF) .\n sent1: Table 2 shows that the method by #TARGET_REF does not overfit towards the MFS to the same extent as other supervised systems since the recall on LFS instances is still quite high 0.41 (a lower recall on LFS instances than on MFS ones is expected due to the reduced training data for them).\n sent2: On semeval13, the recall on LFS is already relatively high using only SemCor (0.33), and reaches 0.38 when using both SemCor and OMSTI.\n sent3: For comparison, the default system IMS (#REF) trained on SemCor only obtains an R lfs of 0.15 on semeval13 (#REF) and only reaches 0.33 with a large amount of annotated data.\n sent4: Finally, our implementation of the label propagation does seem to slightly overfit towards the MFS.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "More in particular, the size of the training data was set at 1%, 10%, 25%, and 100% of the GigaWord corpus (which contains 1.8 × 10 7 , 1.8 × 10 8 , 4.5 × 10 8 and 1.8 × 10 9 words, respectively).",
                "Figure 2a shows the effect of unannotated data volume on WSD performance.",
                "The data points at 100 billion (10 11 ) tokens correspond to #TARGET_REF 's reported results.",
                "As might be expected, a bigger corpus leads to more meaningful context vectors and therefore higher performance on WSD.",
                "However, the amount of data needed for 1% of improvement in F 1 grows exponentially fast (notice that the horizontal axis is in log scale)."
            ],
            "label": [
                "USE",
                "SIMILARITY"
            ]
        },
        "input": "sent0: More in particular, the size of the training data was set at 1%, 10%, 25%, and 100% of the GigaWord corpus (which contains 1.8 × 10 7 , 1.8 × 10 8 , 4.5 × 10 8 and 1.8 × 10 9 words, respectively).\n sent1: Figure 2a shows the effect of unannotated data volume on WSD performance.\n sent2: The data points at 100 billion (10 11 ) tokens correspond to #TARGET_REF 's reported results.\n sent3: As might be expected, a bigger corpus leads to more meaningful context vectors and therefore higher performance on WSD.\n sent4: However, the amount of data needed for 1% of improvement in F 1 grows exponentially fast (notice that the horizontal axis is in log scale).\n",
        "output": "{\"label\": [\"USE\", \"SIMILARITY\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "This paper reports the results of a reproduction study of the model proposed by #TARGET_REF and an additional analysis to gain a deeper understanding of the impact of various factors on its performance.",
                "A number of interesting conclusions can be drawn from our results.",
                "First, we observed that we do not need a very large unannotated dataset to achieve state-of-the-art all-words WSD performance since we used the Gigaword corpus, which is two orders of magnitude smaller than #REF 's proprietary corpus, and got similar performance on senseval2 and semeval2013.",
                "A more detailed analysis hints that adding more unannotated data and increasing model capacity are subject to diminishing returns.",
                "Moreover, we observed that this approach has a more balanced sense assignment than other techniques, as shown by the relatively good performance on less-frequent-sense instances."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This paper reports the results of a reproduction study of the model proposed by #TARGET_REF and an additional analysis to gain a deeper understanding of the impact of various factors on its performance.\n sent1: A number of interesting conclusions can be drawn from our results.\n sent2: First, we observed that we do not need a very large unannotated dataset to achieve state-of-the-art all-words WSD performance since we used the Gigaword corpus, which is two orders of magnitude smaller than #REF 's proprietary corpus, and got similar performance on senseval2 and semeval2013.\n sent3: A more detailed analysis hints that adding more unannotated data and increasing model capacity are subject to diminishing returns.\n sent4: Moreover, we observed that this approach has a more balanced sense assignment than other techniques, as shown by the relatively good performance on less-frequent-sense instances.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "This paper presents the results of a reproduction study and analysis of this technique using only openly available datasets (GigaWord, SemCor, OMSTI) and software (TensorFlow).",
                "Our study showed that similar results can be obtained with much less data than hinted at by #TARGET_REF .",
                "Detailed analyses shed light on the strengths and weaknesses of this method.",
                "First, adding more unannotated training data is useful, but is subject to diminishing returns.",
                "Second, the model can correctly identify both popular and unpopular meanings."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: This paper presents the results of a reproduction study and analysis of this technique using only openly available datasets (GigaWord, SemCor, OMSTI) and software (TensorFlow).\n sent1: Our study showed that similar results can be obtained with much less data than hinted at by #TARGET_REF .\n sent2: Detailed analyses shed light on the strengths and weaknesses of this method.\n sent3: First, adding more unannotated training data is useful, but is subject to diminishing returns.\n sent4: Second, the model can correctly identify both popular and unpopular meanings.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "However, the gap with the graph-based approach of #REF is still significant.",
                "When we use both SemCor and OMSTI for the annotated data, our results drop 0.02 point for senseval2, whereas they increase by almost 0.01 for semeval2013.",
                "Different from #TARGET_REF, we did not observe improvement by using label propagation (comparing T: SemCor, U: OMSTI against T:SemCor without propagation).",
                "However, the performance of the label propagation strategy is still competitive on both test sets.",
                "Most-vs."
            ],
            "label": [
                "DIFFERENCES",
                "SIMILARITY"
            ]
        },
        "input": "sent0: However, the gap with the graph-based approach of #REF is still significant.\n sent1: When we use both SemCor and OMSTI for the annotated data, our results drop 0.02 point for senseval2, whereas they increase by almost 0.01 for semeval2013.\n sent2: Different from #TARGET_REF, we did not observe improvement by using label propagation (comparing T: SemCor, U: OMSTI against T:SemCor without propagation).\n sent3: However, the performance of the label propagation strategy is still competitive on both test sets.\n sent4: Most-vs.\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"SIMILARITY\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "This paper reports the results of a reproduction study of the model proposed by #REF and an additional analysis to gain a deeper understanding of the impact of various factors on its performance.",
                "A number of interesting conclusions can be drawn from our results.",
                "First, we observed that we do not need a very large unannotated dataset to achieve state-of-the-art all-words WSD performance since we used the Gigaword corpus, which is two orders of magnitude smaller than #TARGET_REF 's proprietary corpus, and got similar performance on senseval2 and semeval2013.",
                "A more detailed analysis hints that adding more unannotated data and increasing model capacity are subject to diminishing returns.",
                "Moreover, we observed that this approach has a more balanced sense assignment than other techniques, as shown by the relatively good performance on less-frequent-sense instances."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: This paper reports the results of a reproduction study of the model proposed by #REF and an additional analysis to gain a deeper understanding of the impact of various factors on its performance.\n sent1: A number of interesting conclusions can be drawn from our results.\n sent2: First, we observed that we do not need a very large unannotated dataset to achieve state-of-the-art all-words WSD performance since we used the Gigaword corpus, which is two orders of magnitude smaller than #TARGET_REF 's proprietary corpus, and got similar performance on senseval2 and semeval2013.\n sent3: A more detailed analysis hints that adding more unannotated data and increasing model capacity are subject to diminishing returns.\n sent4: Moreover, we observed that this approach has a more balanced sense assignment than other techniques, as shown by the relatively good performance on less-frequent-sense instances.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "This is unfortunate because this makes the re-application of this technique a non-trivial process, and it hinders further studies for understanding which limitations prevent even higher accuracies.",
                "These could be, for instance, of algorithmic nature or relate to the input (either size or quality), and a deeper understanding is crucial for enabling further improvements.",
                "In addition, some details are not reported, and this could prevent other attempts from replicating the results.",
                "To address these issues, we reimplemented #TARGET_REF 's method with the goal of: 1) reproducing and making available the code, trained models, and results and 2) understanding which are the main factors that constitute the strengths and weaknesses of this method.",
                "While a full replication is not possible due to the unavailability of the original data, we nevertheless managed to reproduce their approach with other public text corpora, and this allowed us to perform a deeper investigation on the performance of this technique."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: This is unfortunate because this makes the re-application of this technique a non-trivial process, and it hinders further studies for understanding which limitations prevent even higher accuracies.\n sent1: These could be, for instance, of algorithmic nature or relate to the input (either size or quality), and a deeper understanding is crucial for enabling further improvements.\n sent2: In addition, some details are not reported, and this could prevent other attempts from replicating the results.\n sent3: To address these issues, we reimplemented #TARGET_REF 's method with the goal of: 1) reproducing and making available the code, trained models, and results and 2) understanding which are the main factors that constitute the strengths and weaknesses of this method.\n sent4: While a full replication is not possible due to the unavailability of the original data, we nevertheless managed to reproduce their approach with other public text corpora, and this allowed us to perform a deeper investigation on the performance of this technique.\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "It should be noted that with the test set semeval2013, all scorers use WordNet 3.0, therefore the performance of the various methods can be directly compared.",
                "However, not all answers in senseval2 can be mapped to WN3.0 and we do not know how #TARGET_REF handled these cases.",
                "In the WSD evaluation framework (#REF ) that we selected for evaluation, these cases were either re-annotated or removed.",
                "Thus, our F 1 on senseval2 cannot be directly compared with the F 1 in the original paper.",
                "From a first glance at Table 1 , we observe that if we use SemCor to train the synset embeddings, then our results come close to the state-of-the-art on senseval2 (0.720 vs. 0.733)."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: It should be noted that with the test set semeval2013, all scorers use WordNet 3.0, therefore the performance of the various methods can be directly compared.\n sent1: However, not all answers in senseval2 can be mapped to WN3.0 and we do not know how #TARGET_REF handled these cases.\n sent2: In the WSD evaluation framework (#REF ) that we selected for evaluation, these cases were either re-annotated or removed.\n sent3: Thus, our F 1 on senseval2 cannot be directly compared with the F 1 in the original paper.\n sent4: From a first glance at Table 1 , we observe that if we use SemCor to train the synset embeddings, then our results come close to the state-of-the-art on senseval2 (0.720 vs. 0.733).\n",
        "output": "{\"label\": [null], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Since unannotated data is abundant, it is tempting to use more and more data to train language models, hoping that better word embeddings would translate into improved WSD performance.",
                "The fact that #TARGET_REF used a 100-billion-token corpus only reinforces this intuition.",
                "We empirically evaluate the effectiveness of unlabeled data by varying the size of the corpus used to train LSTM models and measure the corresponding WSD performance.",
                "More in particular, the size of the training data was set at 1%, 10%, 25%, and 100% of the GigaWord corpus (which contains 1.8 × 10 7 , 1.8 × 10 8 , 4.5 × 10 8 and 1.8 × 10 9 words, respectively).",
                "Figure 2a shows the effect of unannotated data volume on WSD performance."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: Since unannotated data is abundant, it is tempting to use more and more data to train language models, hoping that better word embeddings would translate into improved WSD performance.\n sent1: The fact that #TARGET_REF used a 100-billion-token corpus only reinforces this intuition.\n sent2: We empirically evaluate the effectiveness of unlabeled data by varying the size of the corpus used to train LSTM models and measure the corresponding WSD performance.\n sent3: More in particular, the size of the training data was set at 1%, 10%, 25%, and 100% of the GigaWord corpus (which contains 1.8 × 10 7 , 1.8 × 10 8 , 4.5 × 10 8 and 1.8 × 10 9 words, respectively).\n sent4: Figure 2a shows the effect of unannotated data volume on WSD performance.\n",
        "output": "{\"label\": [null], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "These human attention maps can be used both for evaluating machine-generated attention maps and for explicitly training attention-based models.",
                "Contributions.",
                "First, we design and test multiple game-inspired novel interfaces for collecting human attention maps of where humans choose to look to answer questions from the large-scale VQA dataset (#REF) ; this VQA-HAT (Human ATtention) dataset will be released publicly.",
                "Second, we perform qualitative and quantitative comparison of the maps generated by state-of-the-art attention-based VQA models (#REF; #TARGET_REF ) and a task-independent saliency baseline (#REF ) against our human attention maps through visualizations and rank-order correlation.",
                "We find that machine-generated attention maps from the most accurate VQA model have a mean rank-correlation of 0.26 with human attention maps, which is worse than task-independent saliency maps that have a mean rank-correlation of 0.49."
            ],
            "label": [
                "BACKGROUND",
                "MOTIVATION"
            ]
        },
        "input": "sent0: These human attention maps can be used both for evaluating machine-generated attention maps and for explicitly training attention-based models.\n sent1: Contributions.\n sent2: First, we design and test multiple game-inspired novel interfaces for collecting human attention maps of where humans choose to look to answer questions from the large-scale VQA dataset (#REF) ; this VQA-HAT (Human ATtention) dataset will be released publicly.\n sent3: Second, we perform qualitative and quantitative comparison of the maps generated by state-of-the-art attention-based VQA models (#REF; #TARGET_REF ) and a task-independent saliency baseline (#REF ) against our human attention maps through visualizations and rank-order correlation.\n sent4: We find that machine-generated attention maps from the most accurate VQA model have a mean rank-correlation of 0.26 with human attention maps, which is worse than task-independent saliency maps that have a mean rank-correlation of 0.49.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"MOTIVATION\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Attention-based models for VQA typically use convolutional neural networks to highlight relevant regions of image given a question.",
                "Stacked Attention Networks (SAN) proposed in (#REF) use LSTM encodings of question words to produce a spatial attention distribution over the convolutional layer features of the image.",
                "Hierarchical Co-Attention Network #TARGET_REF generates multiple levels of image attention based on words, phrases and complete questions, and is the top entry on the VQA Challenge 1 as of the time of this submission.",
                "Another interesting approach uses question parsing to compose the neural network from modules, attention being one of the sub-tasks addressed by these modules (#REF) .",
                "Note that all these works are unsupervised attention models, where \"attention\" is simply an intermediate variable (a spatial distribution) that is produced by the model to optimize downstream loss (VQA cross-entropy)."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Attention-based models for VQA typically use convolutional neural networks to highlight relevant regions of image given a question.\n sent1: Stacked Attention Networks (SAN) proposed in (#REF) use LSTM encodings of question words to produce a spatial attention distribution over the convolutional layer features of the image.\n sent2: Hierarchical Co-Attention Network #TARGET_REF generates multiple levels of image attention based on words, phrases and complete questions, and is the top entry on the VQA Challenge 1 as of the time of this submission.\n sent3: Another interesting approach uses question parsing to compose the neural network from modules, attention being one of the sub-tasks addressed by these modules (#REF) .\n sent4: Note that all these works are unsupervised attention models, where \"attention\" is simply an intermediate variable (a spatial distribution) that is produced by the model to optimize downstream loss (VQA cross-entropy).\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We evaluate maps generated by the following unsupervised models:",
                "Figure 6: Random samples of human attention (column 2) v/s machine-generated attention (columns 3-5).",
                "• Stacked Attention Network (SAN) (#REF) with two attention layers (SAN-2) 2 .",
                "• Hierarchical Co-Attention Network (HieCoAtt) #TARGET_REF with word-level (HieCoAtt-W), phrase-level (HieCoAtt-P) and question-level (HieCoAtt-Q) attention maps; we evaluate all three maps 3 .",
                "Comparison Metric: Rank Correlation."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: We evaluate maps generated by the following unsupervised models:\n sent1: Figure 6: Random samples of human attention (column 2) v/s machine-generated attention (columns 3-5).\n sent2: • Stacked Attention Network (SAN) (#REF) with two attention layers (SAN-2) 2 .\n sent3: • Hierarchical Co-Attention Network (HieCoAtt) #TARGET_REF with word-level (HieCoAtt-W), phrase-level (HieCoAtt-P) and question-level (HieCoAtt-Q) attention maps; we evaluate all three maps 3 .\n sent4: Comparison Metric: Rank Correlation.\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "In this direction, there is a huge amount of research in the statistical approach (Costa-jussà, 2015) and also starting in the neural approach (#REF) .",
                "Finally, there is am emerging line of research in the topic of unsupervised neural MT (#REF; #REF) .",
                "This study designs and details an experiment for testing the standard cascade pivot architecture which has been employed in standard statistical machine translation (Costajussà et al., 2012) .",
                "The system that we propose builds on top of one of the latest neural MT architectures called the Transformer #TARGET_REF .",
                "This architecture is an encoderdecoder structure which uses attention-based mechanisms as an alternative to recurrent neural networks proposed in initial architectures (#REF; #REF) ."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: In this direction, there is a huge amount of research in the statistical approach (Costa-jussà, 2015) and also starting in the neural approach (#REF) .\n sent1: Finally, there is am emerging line of research in the topic of unsupervised neural MT (#REF; #REF) .\n sent2: This study designs and details an experiment for testing the standard cascade pivot architecture which has been employed in standard statistical machine translation (Costajussà et al., 2012) .\n sent3: The system that we propose builds on top of one of the latest neural MT architectures called the Transformer #TARGET_REF .\n sent4: This architecture is an encoderdecoder structure which uses attention-based mechanisms as an alternative to recurrent neural networks proposed in initial architectures (#REF; #REF) .\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "This section provides a brief high-level explanation of the neural MT approach that we are using as a baseline system, which is one of the strongest systems presented recently #TARGET_REF , as well as a glance of its differences with other popular neural machine translation architectures.",
                "Sequence-to-sequence recurrent models (#REF; #REF) have been the standard approach for neural machine translation, especially since the incorporation of attention mechanisms #REF) , which enables the system to learn to identify the information which is relevant for producing each word in the translation.",
                "Convolutional networks (#REF) were the second paradigm to effectively approach sequence transduction tasks like machine translation.",
                "In this paper we make use of the third paradigm for neural machine translation, proposed in (#REF) , namely the Transformer architecture, which is based on a feed-forward encoder-decoder scheme with attention mechanisms.",
                "The type of attention mechanism used in the system, referred to as multi-head attention, allows to train several attention modules in parallel, combining also self-attention with standard attention."
            ],
            "label": [
                "BACKGROUND",
                "MOTIVATION"
            ]
        },
        "input": "sent0: This section provides a brief high-level explanation of the neural MT approach that we are using as a baseline system, which is one of the strongest systems presented recently #TARGET_REF , as well as a glance of its differences with other popular neural machine translation architectures.\n sent1: Sequence-to-sequence recurrent models (#REF; #REF) have been the standard approach for neural machine translation, especially since the incorporation of attention mechanisms #REF) , which enables the system to learn to identify the information which is relevant for producing each word in the translation.\n sent2: Convolutional networks (#REF) were the second paradigm to effectively approach sequence transduction tasks like machine translation.\n sent3: In this paper we make use of the third paradigm for neural machine translation, proposed in (#REF) , namely the Transformer architecture, which is based on a feed-forward encoder-decoder scheme with attention mechanisms.\n sent4: The type of attention mechanism used in the system, referred to as multi-head attention, allows to train several attention modules in parallel, combining also self-attention with standard attention.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"MOTIVATION\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "In this paper we make use of the third paradigm for neural machine translation, proposed in (#REF) , namely the Transformer architecture, which is based on a feed-forward encoder-decoder scheme with attention mechanisms.",
                "The type of attention mechanism used in the system, referred to as multi-head attention, allows to train several attention modules in parallel, combining also self-attention with standard attention.",
                "Self-attention differs from standard attention in the use of the same sentence as input and trains over it allowing to solve issues as coreference resolution.",
                "Equations and details about the transformer system can be found in the original paper #TARGET_REF and are out of the scope of this paper.",
                "For the definition of the vocabulary to be used as input for the neural network, we used the sub-word mechanism from tensor2tensor package, which is similar to BytePair Encoding (BPE) from (#REF) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In this paper we make use of the third paradigm for neural machine translation, proposed in (#REF) , namely the Transformer architecture, which is based on a feed-forward encoder-decoder scheme with attention mechanisms.\n sent1: The type of attention mechanism used in the system, referred to as multi-head attention, allows to train several attention modules in parallel, combining also self-attention with standard attention.\n sent2: Self-attention differs from standard attention in the use of the same sentence as input and trains over it allowing to solve issues as coreference resolution.\n sent3: Equations and details about the transformer system can be found in the original paper #TARGET_REF and are out of the scope of this paper.\n sent4: For the definition of the vocabulary to be used as input for the neural network, we used the sub-word mechanism from tensor2tensor package, which is similar to BytePair Encoding (BPE) from (#REF) .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Convolutional networks (#REF) were the second paradigm to effectively approach sequence transduction tasks like machine translation.",
                "In this paper we make use of the third paradigm for neural machine translation, proposed in #TARGET_REF , namely the Transformer architecture, which is based on a feed-forward encoder-decoder scheme with attention mechanisms.",
                "The type of attention mechanism used in the system, referred to as multi-head attention, allows to train several attention modules in parallel, combining also self-attention with standard attention.",
                "Self-attention differs from standard attention in the use of the same sentence as input and trains over it allowing to solve issues as coreference resolution.",
                "Equations and details about the transformer system can be found in the original paper (#REF) and are out of the scope of this paper."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Convolutional networks (#REF) were the second paradigm to effectively approach sequence transduction tasks like machine translation.\n sent1: In this paper we make use of the third paradigm for neural machine translation, proposed in #TARGET_REF , namely the Transformer architecture, which is based on a feed-forward encoder-decoder scheme with attention mechanisms.\n sent2: The type of attention mechanism used in the system, referred to as multi-head attention, allows to train several attention modules in parallel, combining also self-attention with standard attention.\n sent3: Self-attention differs from standard attention in the use of the same sentence as input and trains over it allowing to solve issues as coreference resolution.\n sent4: Equations and details about the transformer system can be found in the original paper (#REF) and are out of the scope of this paper.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "The results of each of the pivotal translation systems as well as the combined cascaded translation are summarized in table 4, which shows the high quality of the translations of the attentional architecture from #TARGET_REF .",
                "The English-to-Spanish translation obtains a BLEU score of 46.55 in the test set of the WMT Biomedical test set while the Spanish-to-Catalan translation obtains a BLEU score of 86.89 in the El Periódico test set.",
                "The cascaded translation achives a BLEU score of 41.38 in the translated WMT Biometical test set.",
                "All BLEU scores are case-sensitive and where obtained with script t2t-bleu from the tensor2tensor framework, whose results are equivalent to those from mteval-v14.pl from the Moses package.",
                "In order to illustrate the quality of the cascaded translations quality, some sample translations are shown in table 3."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: The results of each of the pivotal translation systems as well as the combined cascaded translation are summarized in table 4, which shows the high quality of the translations of the attentional architecture from #TARGET_REF .\n sent1: The English-to-Spanish translation obtains a BLEU score of 46.55 in the test set of the WMT Biomedical test set while the Spanish-to-Catalan translation obtains a BLEU score of 86.89 in the El Periódico test set.\n sent2: The cascaded translation achives a BLEU score of 41.38 in the translated WMT Biometical test set.\n sent3: All BLEU scores are case-sensitive and where obtained with script t2t-bleu from the tensor2tensor framework, whose results are equivalent to those from mteval-v14.pl from the Moses package.\n sent4: In order to illustrate the quality of the cascaded translations quality, some sample translations are shown in table 3.\n",
        "output": "{\"label\": [null], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "The measure, tested on a window-based DSM, outperformed Vector Cosine on the ESL and on the TOEFL datasets.",
                "In the present work, we perform a systematic evaluation of APSyn, testing it on the most popular test sets for similarity estimation -namely WordSim-353 (#REF) , MEN (#REF) and #TARGET_REF ( #TARGET_REF) .",
                "For comparison, Vector Cosine is also calculated on several countbased DSMs.",
                "We implement a total of twenty-eight models with different parameters settings, each of which differs according to corpus size, context window width, weighting scheme and SVD application.",
                "The new metric is shown to outperform Vector Cosine in most settings, except when the latter metric is applied on a PPMI-SVD reduced matrix (#REF) , against which APSyn still obtains competitive performances."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The measure, tested on a window-based DSM, outperformed Vector Cosine on the ESL and on the TOEFL datasets.\n sent1: In the present work, we perform a systematic evaluation of APSyn, testing it on the most popular test sets for similarity estimation -namely WordSim-353 (#REF) , MEN (#REF) and #TARGET_REF ( #TARGET_REF) .\n sent2: For comparison, Vector Cosine is also calculated on several countbased DSMs.\n sent3: We implement a total of twenty-eight models with different parameters settings, each of which differs according to corpus size, context window width, weighting scheme and SVD application.\n sent4: The new metric is shown to outperform Vector Cosine in most settings, except when the latter metric is applied on a PPMI-SVD reduced matrix (#REF) , against which APSyn still obtains competitive performances.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We implement a total of twenty-eight models with different parameters settings, each of which differs according to corpus size, context window width, weighting scheme and SVD application.",
                "The new metric is shown to outperform Vector Cosine in most settings, except when the latter metric is applied on a PPMI-SVD reduced matrix (#REF) , against which APSyn still obtains competitive performances.",
                "The results are also discussed in relation to the state-of-the-art DSMs, as reported in #TARGET_REF .",
                "In such comparison, the best settings of our models outperform the word embeddings in almost all datasets.",
                "A pilot study was also carried out to investigate whether APSyn is scalable."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We implement a total of twenty-eight models with different parameters settings, each of which differs according to corpus size, context window width, weighting scheme and SVD application.\n sent1: The new metric is shown to outperform Vector Cosine in most settings, except when the latter metric is applied on a PPMI-SVD reduced matrix (#REF) , against which APSyn still obtains competitive performances.\n sent2: The results are also discussed in relation to the state-of-the-art DSMs, as reported in #TARGET_REF .\n sent3: In such comparison, the best settings of our models outperform the word embeddings in almost all datasets.\n sent4: A pilot study was also carried out to investigate whether APSyn is scalable.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "For our evaluation, we used three widely popular datasets: WordSim-353 (#REF) , MEN (#REF) , #TARGET_REF ( #TARGET_REF) .",
                "These datasets have a different history, but all of them consist in word pairs with an associated score, that should either represent word association or word similarity.",
                "WordSim-353 (#REF ) was proposed as a word similarity dataset containing 353 pairs annotated with scores between 0 and 10.",
                "However, #REF claimed that the instructions to the annotators were ambiguous with respect to similarity and association, so that the subjects assigned high similarity scores to entities that are only related by virtue of frequent association (e.g. coffee and cup; movie and theater).",
                "On top of it, WordSim-353 does not provide the POS-tags for the 439 words that it contains, forcing the users to decide which POS to assign to the ambiguous words (e.g. [white, rabbit] and [run, marathon] )."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: For our evaluation, we used three widely popular datasets: WordSim-353 (#REF) , MEN (#REF) , #TARGET_REF ( #TARGET_REF) .\n sent1: These datasets have a different history, but all of them consist in word pairs with an associated score, that should either represent word association or word similarity.\n sent2: WordSim-353 (#REF ) was proposed as a word similarity dataset containing 353 pairs annotated with scores between 0 and 10.\n sent3: However, #REF claimed that the instructions to the annotators were ambiguous with respect to similarity and association, so that the subjects assigned high similarity scores to entities that are only related by virtue of frequent association (e.g. coffee and cup; movie and theater).\n sent4: On top of it, WordSim-353 does not provide the POS-tags for the 439 words that it contains, forcing the users to decide which POS to assign to the ambiguous words (e.g. [white, rabbit] and [run, marathon] ).\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "In order to compare our results with state-of-the-art DSMs, we report the scores for the Vector Cosines calculated on the neural language models (NLM) by #TARGET_REF , who used the code (or directly the embeddings) shared by the original authors.",
                "As we trained our models on almost the same corpora used by Hill and colleagues, the results are perfectly comparable.",
                "The three models we compare our results to are: i) the convolutional neural network of #REF , which was trained on 852 million words of Wikipedia; ii) the neural network of #REF , which was trained on 990 million words of Wikipedia; and iii) the word2vec of #REF , which was trained on 1000 million words of Wikipedia and on the RCV Vol.",
                "1 Corpus (#REF #REF , as reported in #REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In order to compare our results with state-of-the-art DSMs, we report the scores for the Vector Cosines calculated on the neural language models (NLM) by #TARGET_REF , who used the code (or directly the embeddings) shared by the original authors.\n sent1: As we trained our models on almost the same corpora used by Hill and colleagues, the results are perfectly comparable.\n sent2: The three models we compare our results to are: i) the convolutional neural network of #REF , which was trained on 852 million words of Wikipedia; ii) the neural network of #REF , which was trained on 990 million words of Wikipedia; and iii) the word2vec of #REF , which was trained on 1000 million words of Wikipedia and on the RCV Vol.\n sent3: 1 Corpus (#REF #REF , as reported in #REF .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "For our experiments, we implemented twenty-eight DSMs, but for reasons of space only sixteen of them are reported in the tables.",
                "All of them include the pos-tagged target words used in the three datasets (i.e. MEN, WordSim-353 and #TARGET_REF) and the pos-tagged contexts having frequency above 100 in the two corpora.",
                "We considered as contexts the content words (i.e. nouns, verbs and adjectives) within a window of 2, 3 and 5, even though the latter was given up for its poor performances.",
                "As for SVD factorization, we found out that the best results were always achieved when the number of latent dimensions was between 300 and 500.",
                "We report here only the scores for k = 300, since 300 is one of the most common choices for the dimensionality of SVD-reduced spaces and it is always close to be an optimal value for the parameter."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: For our experiments, we implemented twenty-eight DSMs, but for reasons of space only sixteen of them are reported in the tables.\n sent1: All of them include the pos-tagged target words used in the three datasets (i.e. MEN, WordSim-353 and #TARGET_REF) and the pos-tagged contexts having frequency above 100 in the two corpora.\n sent2: We considered as contexts the content words (i.e. nouns, verbs and adjectives) within a window of 2, 3 and 5, even though the latter was given up for its poor performances.\n sent3: As for SVD factorization, we found out that the best results were always achieved when the number of latent dimensions was between 300 and 500.\n sent4: We report here only the scores for k = 300, since 300 is one of the most common choices for the dimensionality of SVD-reduced spaces and it is always close to be an optimal value for the parameter.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Given the twenty-eight DSMs, for each dataset we have measured the Vector Cosine and APSyn between the words in the test pairs.",
                "Table 2 : Spearman correlation scores for our eight models trained on Wikipedia, in the three datasets #TARGET_REF, WordSim-353 and MEN.",
                "In the bottom the performance of the state-of-the-art models of #REF , #REF, #REF , as reported in #REF .",
                "The Spearman correlation between our scores and the gold standard was then computed for every model and it is reported in Table 1 and Table 2 .",
                "In particular, Table 1 describes the performances on #REF9, WordSim-353 and MEN for the measures applied on RCV Vol."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Given the twenty-eight DSMs, for each dataset we have measured the Vector Cosine and APSyn between the words in the test pairs.\n sent1: Table 2 : Spearman correlation scores for our eight models trained on Wikipedia, in the three datasets #TARGET_REF, WordSim-353 and MEN.\n sent2: In the bottom the performance of the state-of-the-art models of #REF , #REF, #REF , as reported in #REF .\n sent3: The Spearman correlation between our scores and the gold standard was then computed for every model and it is reported in Table 1 and Table 2 .\n sent4: In particular, Table 1 describes the performances on #REF9, WordSim-353 and MEN for the measures applied on RCV Vol.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Table 2 : Spearman correlation scores for our eight models trained on Wikipedia, in the three datasets #REF9, WordSim-353 and MEN.",
                "In the bottom the performance of the state-of-the-art models of #REF , #REF, #REF , as reported in #TARGET_REF .",
                "The Spearman correlation between our scores and the gold standard was then computed for every model and it is reported in Table 1 and Table 2 .",
                "In particular, Table 1 describes the performances on #REF9, WordSim-353 and MEN for the measures applied on RCV Vol.",
                "1 models."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Table 2 : Spearman correlation scores for our eight models trained on Wikipedia, in the three datasets #REF9, WordSim-353 and MEN.\n sent1: In the bottom the performance of the state-of-the-art models of #REF , #REF, #REF , as reported in #TARGET_REF .\n sent2: The Spearman correlation between our scores and the gold standard was then computed for every model and it is reported in Table 1 and Table 2 .\n sent3: In particular, Table 1 describes the performances on #REF9, WordSim-353 and MEN for the measures applied on RCV Vol.\n sent4: 1 models.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "In the bottom the performance of the state-of-the-art models of #REF , #REF, #REF , as reported in #REF .",
                "The Spearman correlation between our scores and the gold standard was then computed for every model and it is reported in Table 1 and Table 2 .",
                "In particular, Table 1 describes the performances on #TARGET_REF, WordSim-353 and MEN for the measures applied on RCV Vol.",
                "1 models.",
                "Table 2 , instead, describes the performances of the measures on the three datasets for the Wikipedia models."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In the bottom the performance of the state-of-the-art models of #REF , #REF, #REF , as reported in #REF .\n sent1: The Spearman correlation between our scores and the gold standard was then computed for every model and it is reported in Table 1 and Table 2 .\n sent2: In particular, Table 1 describes the performances on #TARGET_REF, WordSim-353 and MEN for the measures applied on RCV Vol.\n sent3: 1 models.\n sent4: Table 2 , instead, describes the performances of the measures on the three datasets for the Wikipedia models.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Table 2 , instead, describes the performances of the measures on the three datasets for the Wikipedia models.",
                "Concurrently, Table 3 and Table 4 describe the performances of the measures respectively on the RCV Vol.",
                "1 and Wikipedia models, tested on the subsets of WordSim-353 extracted by #REF .",
                "Table 1 shows the Spearman correlation scores for Vector Cosine and APSyn on the three datasets for the eight most representative DSMs built using RCV Vol. 1. Table 2 does the same for the DSMs built using Wikipedia.",
                "For the sake of comparison, we also report the results of the state-of-the-art DSMs mentioned in #TARGET_REF (see Section 2.5)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Table 2 , instead, describes the performances of the measures on the three datasets for the Wikipedia models.\n sent1: Concurrently, Table 3 and Table 4 describe the performances of the measures respectively on the RCV Vol.\n sent2: 1 and Wikipedia models, tested on the subsets of WordSim-353 extracted by #REF .\n sent3: Table 1 shows the Spearman correlation scores for Vector Cosine and APSyn on the three datasets for the eight most representative DSMs built using RCV Vol. 1. Table 2 does the same for the DSMs built using Wikipedia.\n sent4: For the sake of comparison, we also report the results of the state-of-the-art DSMs mentioned in #TARGET_REF (see Section 2.5).\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "The former appears to perform better on #REF9, while the latter seems to have some advantages on the other datasets.",
                "This Table 3 : Spearman correlation scores for our eight models trained on RCV1, in the two subsets of might depend on the different type of similarity encoded in #TARGET_REF (i.e. genuine similarity).",
                "On top of it, despite #REF 's claim that no evidence supports the hypothesis that smaller context windows improve the ability of models to capture similarity (#REF; #REF) , we need to mention that window 5 was abandoned because of its low performance.",
                "With reference to the hubness effect, we have conducted a pilot study inspired to the one carried out by #REF , using the words of the #REF9 dataset as query words and collecting for each of them the top 1000 nearest neighbors.",
                "Given all the neighbors at rank r, we have checked their rank in the frequency list extracted from our corpora."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The former appears to perform better on #REF9, while the latter seems to have some advantages on the other datasets.\n sent1: This Table 3 : Spearman correlation scores for our eight models trained on RCV1, in the two subsets of might depend on the different type of similarity encoded in #TARGET_REF (i.e. genuine similarity).\n sent2: On top of it, despite #REF 's claim that no evidence supports the hypothesis that smaller context windows improve the ability of models to capture similarity (#REF; #REF) , we need to mention that window 5 was abandoned because of its low performance.\n sent3: With reference to the hubness effect, we have conducted a pilot study inspired to the one carried out by #REF , using the words of the #REF9 dataset as query words and collecting for each of them the top 1000 nearest neighbors.\n sent4: Given all the neighbors at rank r, we have checked their rank in the frequency list extracted from our corpora.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "On top of it, despite #REF 's claim that no evidence supports the hypothesis that smaller context windows improve the ability of models to capture similarity (#REF; #REF) , we need to mention that window 5 was abandoned because of its low performance.",
                "With reference to the hubness effect, we have conducted a pilot study inspired to the one carried out by #REF , using the words of the #TARGET_REF dataset as query words and collecting for each of them the top 1000 nearest neighbors.",
                "Given all the neighbors at rank r, we have checked their rank in the frequency list extracted from our corpora.",
                "Figure 1 shows the relation between the rank in the nearest neighbor list and the rank in the frequency list.",
                "It can be easily noticed that the highest ranked nearest neighbors tend to have higher rank also in the frequency list, supporting the idea that frequent words are more likely to be nearest neighbors."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: On top of it, despite #REF 's claim that no evidence supports the hypothesis that smaller context windows improve the ability of models to capture similarity (#REF; #REF) , we need to mention that window 5 was abandoned because of its low performance.\n sent1: With reference to the hubness effect, we have conducted a pilot study inspired to the one carried out by #REF , using the words of the #TARGET_REF dataset as query words and collecting for each of them the top 1000 nearest neighbors.\n sent2: Given all the neighbors at rank r, we have checked their rank in the frequency list extracted from our corpora.\n sent3: Figure 1 shows the relation between the rank in the nearest neighbor list and the rank in the frequency list.\n sent4: It can be easily noticed that the highest ranked nearest neighbors tend to have higher rank also in the frequency list, supporting the idea that frequent words are more likely to be nearest neighbors.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "APSyn does not seem to be able to overcome such bias, which seems to be in fact an inherent property of the DSMs (#REF) .",
                "Further investigation is needed to see whether variations of APSyn can tackle this problem.",
                "Finally, few words need to be spent with regard to the ability of calculating genuine similarity, as distinguished from word relatedness (#REF; #REF; #TARGET_REF) .",
                "Table 3 and  Table 4 show the Spearman correlation scores for the two measures calculated on the models respectively trained on RCV1 and Wikipedia, tested on the subsets of WordSim-353 extracted by #REF .",
                "It can be easily noticed that our best models work better on the similarity subset."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: APSyn does not seem to be able to overcome such bias, which seems to be in fact an inherent property of the DSMs (#REF) .\n sent1: Further investigation is needed to see whether variations of APSyn can tackle this problem.\n sent2: Finally, few words need to be spent with regard to the ability of calculating genuine similarity, as distinguished from word relatedness (#REF; #REF; #TARGET_REF) .\n sent3: Table 3 and  Table 4 show the Spearman correlation scores for the two measures calculated on the models respectively trained on RCV1 and Wikipedia, tested on the subsets of WordSim-353 extracted by #REF .\n sent4: It can be easily noticed that our best models work better on the similarity subset.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "On top of it, APSyn does not suffer from some of the problems reported for the Vector Cosine, such as the inability of identifying the number of shared features.",
                "It still however seems to be affected by the hubness issue, and more research should be carried out to tackle it.",
                "Concerning the discrimination between similarity and association, the good performance of APSyn on #TARGET_REF (which was built with a specific attention to genuine similarity) and the large difference in performance between the two subsets of WordSim-353 described in Table 3 and Table 4 make us conclude that APSyn is indeed efficient in quantifying genuine similarity.",
                "To conclude, being a linguistically and cognitively grounded metric, APSyn offers the possibility for further improvements, by simply combining it to other properties that were not yet considered in its definition.",
                "A natural extension would be to verify whether APSyn hypothesis and implementation holds on SVD reduced matrices and word embeddings."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: On top of it, APSyn does not suffer from some of the problems reported for the Vector Cosine, such as the inability of identifying the number of shared features.\n sent1: It still however seems to be affected by the hubness issue, and more research should be carried out to tackle it.\n sent2: Concerning the discrimination between similarity and association, the good performance of APSyn on #TARGET_REF (which was built with a specific attention to genuine similarity) and the large difference in performance between the two subsets of WordSim-353 described in Table 3 and Table 4 make us conclude that APSyn is indeed efficient in quantifying genuine similarity.\n sent3: To conclude, being a linguistically and cognitively grounded metric, APSyn offers the possibility for further improvements, by simply combining it to other properties that were not yet considered in its definition.\n sent4: A natural extension would be to verify whether APSyn hypothesis and implementation holds on SVD reduced matrices and word embeddings.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "In such comparison, the best settings of our models outperform the word embeddings in almost all datasets.",
                "A pilot study was also carried out to investigate whether APSyn is scalable.",
                "Results prove its high performance also when calculated on large corpora, such as those used by .",
                "On top of the performance, APSyn seems not to be subject to some of the biases that affect Vector Cosine.",
                "Finally, considering the debate about the ability of DSMs to calculate genuine similarity as opposed to word relatedness (#REF; #REF; #TARGET_REF) , we test the ability of the models to quantify genuine semantic similarity."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In such comparison, the best settings of our models outperform the word embeddings in almost all datasets.\n sent1: A pilot study was also carried out to investigate whether APSyn is scalable.\n sent2: Results prove its high performance also when calculated on large corpora, such as those used by .\n sent3: On top of the performance, APSyn seems not to be subject to some of the biases that affect Vector Cosine.\n sent4: Finally, considering the debate about the ability of DSMs to calculate genuine similarity as opposed to word relatedness (#REF; #REF; #TARGET_REF) , we test the ability of the models to quantify genuine semantic similarity.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "WordSim-353 (#REF ) was proposed as a word similarity dataset containing 353 pairs annotated with scores between 0 and 10.",
                "However, #TARGET_REF claimed that the instructions to the annotators were ambiguous with respect to similarity and association, so that the subjects assigned high similarity scores to entities that are only related by virtue of frequent association (e.g. coffee and cup; movie and theater).",
                "On top of it, WordSim-353 does not provide the POS-tags for the 439 words that it contains, forcing the users to decide which POS to assign to the ambiguous words (e.g. [white, rabbit] and [run, marathon] ).",
                "An extension of this dataset resulted from the subclassification carried out by #REF , which discriminated between similar and associated word pairs.",
                "Such discrimination was done by asking annotators to classify all pairs according to the semantic relation they hold (i.e. identical, synonymy, antonymy, hypernymy, meronymy and none-of-the-above)."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: WordSim-353 (#REF ) was proposed as a word similarity dataset containing 353 pairs annotated with scores between 0 and 10.\n sent1: However, #TARGET_REF claimed that the instructions to the annotators were ambiguous with respect to similarity and association, so that the subjects assigned high similarity scores to entities that are only related by virtue of frequent association (e.g. coffee and cup; movie and theater).\n sent2: On top of it, WordSim-353 does not provide the POS-tags for the 439 words that it contains, forcing the users to decide which POS to assign to the ambiguous words (e.g. [white, rabbit] and [run, marathon] ).\n sent3: An extension of this dataset resulted from the subclassification carried out by #REF , which discriminated between similar and associated word pairs.\n sent4: Such discrimination was done by asking annotators to classify all pairs according to the semantic relation they hold (i.e. identical, synonymy, antonymy, hypernymy, meronymy and none-of-the-above).\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Such discrimination was done by asking annotators to classify all pairs according to the semantic relation they hold (i.e. identical, synonymy, antonymy, hypernymy, meronymy and none-of-the-above).",
                "The annotation was then used to group the pairs in three categories: similar pairs (those classified as identical, synonyms, antonyms and hypernyms), associated pairs (those classified as meronyms and none-of-the-above, with an average similarity greater than 5), and non-associated pairs (those classified as none-of-the-above, with an average similarity below or equal to 5).",
                "Two gold standard were finally produced: i) one for similarity, containing 203 word pairs resulting from the union of similar and non-associated pairs; ii) one for relatedness, containing 252 word pairs resulting from the union of associated and non-associated pairs.",
                "Even though such a classification made a clear distinction between the two types of relations (i.e. similarity and association), #TARGET_REF argue that these gold standards still carry the scores they had in WordSim-353, which are known to be ambiguous in this regard.",
                "The MEN Test Collection (#REF) includes 3,000 word pairs divided in two sets (one for training and one for testing) together with human judgments, obtained through Amazon Mechanical Turk."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Such discrimination was done by asking annotators to classify all pairs according to the semantic relation they hold (i.e. identical, synonymy, antonymy, hypernymy, meronymy and none-of-the-above).\n sent1: The annotation was then used to group the pairs in three categories: similar pairs (those classified as identical, synonyms, antonyms and hypernyms), associated pairs (those classified as meronyms and none-of-the-above, with an average similarity greater than 5), and non-associated pairs (those classified as none-of-the-above, with an average similarity below or equal to 5).\n sent2: Two gold standard were finally produced: i) one for similarity, containing 203 word pairs resulting from the union of similar and non-associated pairs; ii) one for relatedness, containing 252 word pairs resulting from the union of associated and non-associated pairs.\n sent3: Even though such a classification made a clear distinction between the two types of relations (i.e. similarity and association), #TARGET_REF argue that these gold standards still carry the scores they had in WordSim-353, which are known to be ambiguous in this regard.\n sent4: The MEN Test Collection (#REF) includes 3,000 word pairs divided in two sets (one for training and one for testing) together with human judgments, obtained through Amazon Mechanical Turk.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Every pairs-couple was proposed only once, and a final score out of 50 was attributed to each pair, according to how many times it was rated as the most related.",
                "According to #TARGET_REF , the major weakness of this dataset is that it does not encode word similarity, but a more general notion of association.",
                "#REF9 is the dataset introduced by #REF to address the above mentioned criticisms of confusion between similarity and association.",
                "The dataset consists of 999 pairs containing 1,028 words, which were also evaluated in terms of POS-tags and concreteness.",
                "The pairs were annotated with a score between 0 and 10, and the instructions were strictly requiring the identification of word similarity, rather than word association."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Every pairs-couple was proposed only once, and a final score out of 50 was attributed to each pair, according to how many times it was rated as the most related.\n sent1: According to #TARGET_REF , the major weakness of this dataset is that it does not encode word similarity, but a more general notion of association.\n sent2: #REF9 is the dataset introduced by #REF to address the above mentioned criticisms of confusion between similarity and association.\n sent3: The dataset consists of 999 pairs containing 1,028 words, which were also evaluated in terms of POS-tags and concreteness.\n sent4: The pairs were annotated with a score between 0 and 10, and the instructions were strictly requiring the identification of word similarity, rather than word association.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Every pairs-couple was proposed only once, and a final score out of 50 was attributed to each pair, according to how many times it was rated as the most related.",
                "According to #REF , the major weakness of this dataset is that it does not encode word similarity, but a more general notion of association.",
                "#TARGET_REF is the dataset introduced by #TARGET_REF to address the above mentioned criticisms of confusion between similarity and association.",
                "The dataset consists of 999 pairs containing 1,028 words, which were also evaluated in terms of POS-tags and concreteness.",
                "The pairs were annotated with a score between 0 and 10, and the instructions were strictly requiring the identification of word similarity, rather than word association."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Every pairs-couple was proposed only once, and a final score out of 50 was attributed to each pair, according to how many times it was rated as the most related.\n sent1: According to #REF , the major weakness of this dataset is that it does not encode word similarity, but a more general notion of association.\n sent2: #TARGET_REF is the dataset introduced by #TARGET_REF to address the above mentioned criticisms of confusion between similarity and association.\n sent3: The dataset consists of 999 pairs containing 1,028 words, which were also evaluated in terms of POS-tags and concreteness.\n sent4: The pairs were annotated with a score between 0 and 10, and the instructions were strictly requiring the identification of word similarity, rather than word association.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "According to #REF , the major weakness of this dataset is that it does not encode word similarity, but a more general notion of association.",
                "#REF9 is the dataset introduced by #REF to address the above mentioned criticisms of confusion between similarity and association.",
                "The dataset consists of 999 pairs containing 1,028 words, which were also evaluated in terms of POS-tags and concreteness.",
                "The pairs were annotated with a score between 0 and 10, and the instructions were strictly requiring the identification of word similarity, rather than word association.",
                "#TARGET_REF claim that differently from other datasets, #TARGET_REF interannotator agreement has not been surpassed by any automatic approach."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: According to #REF , the major weakness of this dataset is that it does not encode word similarity, but a more general notion of association.\n sent1: #REF9 is the dataset introduced by #REF to address the above mentioned criticisms of confusion between similarity and association.\n sent2: The dataset consists of 999 pairs containing 1,028 words, which were also evaluated in terms of POS-tags and concreteness.\n sent3: The pairs were annotated with a score between 0 and 10, and the instructions were strictly requiring the identification of word similarity, rather than word association.\n sent4: #TARGET_REF claim that differently from other datasets, #TARGET_REF interannotator agreement has not been surpassed by any automatic approach.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "In order to compare our results with state-of-the-art DSMs, we report the scores for the Vector Cosines calculated on the neural language models (NLM) by #REF , who used the code (or directly the embeddings) shared by the original authors.",
                "As we trained our models on almost the same corpora used by #TARGET_REF, the results are perfectly comparable.",
                "The three models we compare our results to are: i) the convolutional neural network of #REF , which was trained on 852 million words of Wikipedia; ii) the neural network of #REF , which was trained on 990 million words of Wikipedia; and iii) the word2vec of #REF , which was trained on 1000 million words of Wikipedia and on the RCV Vol.",
                "1 Corpus (#REF #REF , as reported in #REF ."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: In order to compare our results with state-of-the-art DSMs, we report the scores for the Vector Cosines calculated on the neural language models (NLM) by #REF , who used the code (or directly the embeddings) shared by the original authors.\n sent1: As we trained our models on almost the same corpora used by #TARGET_REF, the results are perfectly comparable.\n sent2: The three models we compare our results to are: i) the convolutional neural network of #REF , which was trained on 852 million words of Wikipedia; ii) the neural network of #REF , which was trained on 990 million words of Wikipedia; and iii) the word2vec of #REF , which was trained on 1000 million words of Wikipedia and on the RCV Vol.\n sent3: 1 Corpus (#REF #REF , as reported in #REF .\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Interestingly, our best models achieve results that are comparable to -or even better than -those reported by #TARGET_REF for the stateof-the-art word embeddings models.",
                "In Section 3.5 we show that APSyn is scalable, outperforming the state-of-the-art count-based models reported in .",
                "On top of it, APSyn does not suffer from some of the problems reported for the Vector Cosine, such as the inability of identifying the number of shared features.",
                "It still however seems to be affected by the hubness issue, and more research should be carried out to tackle it.",
                "Concerning the discrimination between similarity and association, the good performance of APSyn on #REF9 (which was built with a specific attention to genuine similarity) and the large difference in performance between the two subsets of WordSim-353 described in Table  3 and Table 4 make us conclude that APSyn is indeed efficient in quantifying genuine similarity."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: Interestingly, our best models achieve results that are comparable to -or even better than -those reported by #TARGET_REF for the stateof-the-art word embeddings models.\n sent1: In Section 3.5 we show that APSyn is scalable, outperforming the state-of-the-art count-based models reported in .\n sent2: On top of it, APSyn does not suffer from some of the problems reported for the Vector Cosine, such as the inability of identifying the number of shared features.\n sent3: It still however seems to be affected by the hubness issue, and more research should be carried out to tackle it.\n sent4: Concerning the discrimination between similarity and association, the good performance of APSyn on #REF9 (which was built with a specific attention to genuine similarity) and the large difference in performance between the two subsets of WordSim-353 described in Table  3 and Table 4 make us conclude that APSyn is indeed efficient in quantifying genuine similarity.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "These two models perform consistently and in a comparable way across the datasets, generally outperforming the state-of-the-art DSMs, with an exception for the Wikipedia-trained models in WordSim-353.",
                "Some further observations are: i) corpus size strongly affects the results; ii) PPMI strongly outperforms LMI for both Vector Cosine and APSyn; iii) SVD boosts the Vector Cosine, especially when it is combined with PPMI; iv) N has some impact on the performance of APSyn, which generally achieves the best results for N=500.",
                "As a note about iii), the results of using SVD jointly with LMI spaces are less predictable than when combining it with PPMI.",
                "Also, we can notice that the smaller window (i.e. 2) does not always perform better than the larger one (i.e. 3).",
                "The former appears to perform better on #TARGET_REF, while the latter seems to have some advantages on the other datasets."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: These two models perform consistently and in a comparable way across the datasets, generally outperforming the state-of-the-art DSMs, with an exception for the Wikipedia-trained models in WordSim-353.\n sent1: Some further observations are: i) corpus size strongly affects the results; ii) PPMI strongly outperforms LMI for both Vector Cosine and APSyn; iii) SVD boosts the Vector Cosine, especially when it is combined with PPMI; iv) N has some impact on the performance of APSyn, which generally achieves the best results for N=500.\n sent2: As a note about iii), the results of using SVD jointly with LMI spaces are less predictable than when combining it with PPMI.\n sent3: Also, we can notice that the smaller window (i.e. 2) does not always perform better than the larger one (i.e. 3).\n sent4: The former appears to perform better on #TARGET_REF, while the latter seems to have some advantages on the other datasets.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\", \"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "On top of it, despite #TARGET_REF 's claim that no evidence supports the hypothesis that smaller context windows improve the ability of models to capture similarity (#REF; #REF) , we need to mention that window 5 was abandoned because of its low performance.",
                "With reference to the hubness effect, we have conducted a pilot study inspired to the one carried out by #REF , using the words of the #REF9 dataset as query words and collecting for each of them the top 1000 nearest neighbors.",
                "Given all the neighbors at rank r, we have checked their rank in the frequency list extracted from our corpora.",
                "Figure 1 shows the relation between the rank in the nearest neighbor list and the rank in the frequency list.",
                "It can be easily noticed that the highest ranked nearest neighbors tend to have higher rank also in the frequency list, supporting the idea that frequent words are more likely to be nearest neighbors."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: On top of it, despite #TARGET_REF 's claim that no evidence supports the hypothesis that smaller context windows improve the ability of models to capture similarity (#REF; #REF) , we need to mention that window 5 was abandoned because of its low performance.\n sent1: With reference to the hubness effect, we have conducted a pilot study inspired to the one carried out by #REF , using the words of the #REF9 dataset as query words and collecting for each of them the top 1000 nearest neighbors.\n sent2: Given all the neighbors at rank r, we have checked their rank in the frequency list extracted from our corpora.\n sent3: Figure 1 shows the relation between the rank in the nearest neighbor list and the rank in the frequency list.\n sent4: It can be easily noticed that the highest ranked nearest neighbors tend to have higher rank also in the frequency list, supporting the idea that frequent words are more likely to be nearest neighbors.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Our generator is based on the sequence-tosequence (seq2seq) generation technique (#REF; #REF) , combined with beam search and an n-best list reranker to suppress irrelevant information in the outputs.",
                "Unlike most previous NLG systems for SDS (e.g., (#REF; #REF; ), it is trainable from unaligned pairs of MR and sentences alone.",
                "We experiment with using much less training data than recent systems based on recurrent neural networks (RNN) (#REFb; #REF) , and we find that our generator learns successfully to produce both strings and deep syntax trees on the BAGEL restaurant information dataset .",
                "It is able to surpass n-gram-based scores achieved previously by #TARGET_REF , offering a simpler setup and more relevant outputs.",
                "We introduce the generation setting in Section 2 and describe our generator architecture in Section 3."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Our generator is based on the sequence-tosequence (seq2seq) generation technique (#REF; #REF) , combined with beam search and an n-best list reranker to suppress irrelevant information in the outputs.\n sent1: Unlike most previous NLG systems for SDS (e.g., (#REF; #REF; ), it is trainable from unaligned pairs of MR and sentences alone.\n sent2: We experiment with using much less training data than recent systems based on recurrent neural networks (RNN) (#REFb; #REF) , and we find that our generator learns successfully to produce both strings and deep syntax trees on the BAGEL restaurant information dataset .\n sent3: It is able to surpass n-gram-based scores achieved previously by #TARGET_REF , offering a simpler setup and more relevant outputs.\n sent4: We introduce the generation setting in Section 2 and describe our generator architecture in Section 3.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "A larger beam leads to a small BLEU decrease even though the sentences contain less errors; here, NIST reflects the situation more accurately.",
                "A comparison of the two approaches goes in favor of the joint setup: Without the reranker, models generating trees produce less semantic errors and gain higher BLEU/NIST scores.",
                "However, with the reranker, the string-based model is able to reduce the number of semantic errors while producing outputs significantly better in terms of BLEU/NIST.",
                "11 In addition, the joint setup does not need an external surface realizer.",
                "The best results of both setups surpass the best results on this dataset using training data without manual alignments #TARGET_REF in both automatic metrics 12 and the number of semantic errors."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: A larger beam leads to a small BLEU decrease even though the sentences contain less errors; here, NIST reflects the situation more accurately.\n sent1: A comparison of the two approaches goes in favor of the joint setup: Without the reranker, models generating trees produce less semantic errors and gain higher BLEU/NIST scores.\n sent2: However, with the reranker, the string-based model is able to reduce the number of semantic errors while producing outputs significantly better in terms of BLEU/NIST.\n sent3: 11 In addition, the joint setup does not need an external surface realizer.\n sent4: The best results of both setups surpass the best results on this dataset using training data without manual alignments #TARGET_REF in both automatic metrics 12 and the number of semantic errors.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "The results show the direct approach as more favorable, with significantly higher n-gram based scores and a similar number of semantic errors in the output.",
                "We also showed that our generator can learn to produce meaningful utterances using a much smaller amount of training data than what is typically used for RNN-based approaches.",
                "The resulting models had virtually no problems with produc-ing fluent, coherent sentences or with generating valid structure of bracketed deep syntax trees.",
                "Our generator was able to surpass the best BLEU/NIST scores on the same dataset previously achieved by a perceptron-based generator of #TARGET_REF while reducing the amount of irrelevant information on the output.",
                "Our generator is released on GitHub at the following URL:"
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: The results show the direct approach as more favorable, with significantly higher n-gram based scores and a similar number of semantic errors in the output.\n sent1: We also showed that our generator can learn to produce meaningful utterances using a much smaller amount of training data than what is typically used for RNN-based approaches.\n sent2: The resulting models had virtually no problems with produc-ing fluent, coherent sentences or with generating valid structure of bracketed deep syntax trees.\n sent3: Our generator was able to surpass the best BLEU/NIST scores on the same dataset previously achieved by a perceptron-based generator of #TARGET_REF while reducing the amount of irrelevant information on the output.\n sent4: Our generator is released on GitHub at the following URL:\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Our generator operates in two modes, producing either deep syntax trees (Dušek et al., 2012) or natural language strings (see Fig. 1 ).",
                "The first mode corresponds to the sentence planning NLG stage as it decides the syntactic shape of the output sentence; the resulting deep syntax tree involves content words (lemmas) and their syntactic form (formemes, purple in Fig. 1 ).",
                "The trees are linearized to strings using a surface realizer from the TectoMT translation system .",
                "The second generator mode joins sentence planning and surface realization into one step, producing natural language sentences directly.",
                "Both modes offer their advantages: The twostep mode simplifies generation by abstracting away from complex surface syntax and morphology, which can be handled by a handcrafted, domain-independent module to ensure grammatical correctness at all times #TARGET_REF , and the joint mode does not need to model structure explicitly and avoids accumulating errors along the pipeline (#REF) ."
            ],
            "label": [
                "BACKGROUND",
                "USE"
            ]
        },
        "input": "sent0: Our generator operates in two modes, producing either deep syntax trees (Dušek et al., 2012) or natural language strings (see Fig. 1 ).\n sent1: The first mode corresponds to the sentence planning NLG stage as it decides the syntactic shape of the output sentence; the resulting deep syntax tree involves content words (lemmas) and their syntactic form (formemes, purple in Fig. 1 ).\n sent2: The trees are linearized to strings using a surface realizer from the TectoMT translation system .\n sent3: The second generator mode joins sentence planning and surface realization into one step, producing natural language sentences directly.\n sent4: Both modes offer their advantages: The twostep mode simplifies generation by abstracting away from complex surface syntax and morphology, which can be handled by a handcrafted, domain-independent module to ensure grammatical correctness at all times #TARGET_REF , and the joint mode does not need to model structure explicitly and avoids accumulating errors along the pipeline (#REF) .\n",
        "output": "{\"label\": [\"BACKGROUND\", \"USE\"], \"context\": [\"sent0\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "The reranker is able to reduce the number of semantic errors while increasing automatic scores considerably.",
                "Using a larger beam increases the effect of the reranker as expected, resulting in slightly improved outputs.",
                "Models generating deep syntax trees are also able to learn the domain style, and they have virtually no problems producing valid trees.",
                "10 The surface realizer works almost flawlessly on this lim-ited domain #TARGET_REF , leaving the seq2seq generator as the major error source.",
                "The syntax-generating models tend to make different kinds of errors than the string-based models: Some outputs are valid trees but not entirely syntactically fluent; missing, incorrect, or repeated information is more frequent than a confusion of semantically similar items (see Table 2 )."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The reranker is able to reduce the number of semantic errors while increasing automatic scores considerably.\n sent1: Using a larger beam increases the effect of the reranker as expected, resulting in slightly improved outputs.\n sent2: Models generating deep syntax trees are also able to learn the domain style, and they have virtually no problems producing valid trees.\n sent3: 10 The surface realizer works almost flawlessly on this lim-ited domain #TARGET_REF , leaving the seq2seq generator as the major error source.\n sent4: The syntax-generating models tend to make different kinds of errors than the string-based models: Some outputs are valid trees but not entirely syntactically fluent; missing, incorrect, or repeated information is more frequent than a confusion of semantically similar items (see Table 2 ).\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The systems participating in the task are required to apply the labels BAD and OK, either to words or phrases.",
                "In this paper we describe the approach behind the submissions of the Universitat d'Alacant team to these sub-tasks.",
                "For our word-level submissions we have applied the approach proposed by #TARGET_REF , where we used black-box bilingual on-line resources.",
                "The new task tackles MTQE for translating English into German.",
                "For this task we have combined two on-line-available MT systems, 1 Lucy LT KWIK Translator 2 and Google Translate, 3 and the bilingual concordancer Reverso Context 4 to spot sub-segment correspondences between a sentence S in the source language (SL) and a given translation hypothesis T in the target language (TL)."
            ],
            "label": [
                "DIFFERENCES",
                "EXTENDS"
            ]
        },
        "input": "sent0: The systems participating in the task are required to apply the labels BAD and OK, either to words or phrases.\n sent1: In this paper we describe the approach behind the submissions of the Universitat d'Alacant team to these sub-tasks.\n sent2: For our word-level submissions we have applied the approach proposed by #TARGET_REF , where we used black-box bilingual on-line resources.\n sent3: The new task tackles MTQE for translating English into German.\n sent4: For this task we have combined two on-line-available MT systems, 1 Lucy LT KWIK Translator 2 and Google Translate, 3 and the bilingual concordancer Reverso Context 4 to spot sub-segment correspondences between a sentence S in the source language (SL) and a given translation hypothesis T in the target language (TL).\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"EXTENDS\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The new task tackles MTQE for translating English into German.",
                "For this task we have combined two on-line-available MT systems, 1 Lucy LT KWIK Translator 2 and Google Translate, 3 and the bilingual concordancer Reverso Context 4 to spot sub-segment correspondences between a sentence S in the source language (SL) and a given translation hypothesis T in the target language (TL).",
                "As described by #TARGET_REF , a collection of features is obtained from these correspondences and then used by a binary classifier to determine the final word-level MTQE labels.",
                "We have repeated the approach proposed in WMT 2015 for word-level sub-tasks, and have proposed a new one for phrase-level MTQE that builds upon the system trained for word-level MTQE.",
                "The rest of the paper is organised as follows."
            ],
            "label": [
                "DIFFERENCES",
                "EXTENDS"
            ]
        },
        "input": "sent0: The new task tackles MTQE for translating English into German.\n sent1: For this task we have combined two on-line-available MT systems, 1 Lucy LT KWIK Translator 2 and Google Translate, 3 and the bilingual concordancer Reverso Context 4 to spot sub-segment correspondences between a sentence S in the source language (SL) and a given translation hypothesis T in the target language (TL).\n sent2: As described by #TARGET_REF , a collection of features is obtained from these correspondences and then used by a binary classifier to determine the final word-level MTQE labels.\n sent3: We have repeated the approach proposed in WMT 2015 for word-level sub-tasks, and have proposed a new one for phrase-level MTQE that builds upon the system trained for word-level MTQE.\n sent4: The rest of the paper is organised as follows.\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"EXTENDS\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "A multilayer perceptron (#REF , Section 6) was used for classification, as implemented in Weka 3.7 (#REF) .",
                "Following the approach by #TARGET_REF , the perceptron was built with a single hidden layer containing the same number of nodes as the number of features; this was the best performing architecture in the preliminary experiments.",
                "7 The training sets 5 The list of features can be found in the file features list in the package http://www.quest.",
                "dcs.shef.ac.uk/wmt16_files_qe/task2_ en-de_test.tar.gz 6 The list of features can be found in the file features list in the package http://www.quest.",
                "dcs.shef.ac.uk/wmt16_files_qe/task2p_ en-de_test.tar.gz 7 The rest of parameters of the classifiers were also kept as in the approach by Esplà-#REF ."
            ],
            "label": [
                "DIFFERENCES",
                "EXTENDS"
            ]
        },
        "input": "sent0: A multilayer perceptron (#REF , Section 6) was used for classification, as implemented in Weka 3.7 (#REF) .\n sent1: Following the approach by #TARGET_REF , the perceptron was built with a single hidden layer containing the same number of nodes as the number of features; this was the best performing architecture in the preliminary experiments.\n sent2: 7 The training sets 5 The list of features can be found in the file features list in the package http://www.quest.\n sent3: dcs.shef.ac.uk/wmt16_files_qe/task2_ en-de_test.tar.gz 6 The list of features can be found in the file features list in the package http://www.quest.\n sent4: dcs.shef.ac.uk/wmt16_files_qe/task2p_ en-de_test.tar.gz 7 The rest of parameters of the classifiers were also kept as in the approach by Esplà-#REF .\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"EXTENDS\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "In fact, one of the sources of bilingual information used in the previous edition of the shared task, Apertium, has been replaced by a new one: Lucy LT.",
                "The results obtained confirm the conclusion by #TARGET_REF that combining the baseline features with those obtained from external sources of bilingual information provide a noticeable improvement, in this case, not only for word-level MTQE, but also for phrase-level MTQE.",
                "Some future work may be interesting, specially as regards the approach to phrase-level MTQE.",
                "As already mentioned, it would be interesting to use binary classifiers that support sparse features, in order to be able to directly train a single binary classifier capable to deal with phrases of any length.",
                "This would make it possible to put together all the data available, avoiding splitting it into smaller training sets for different classifiers, and therefore allowing to have larger training data set."
            ],
            "label": [
                "DIFFERENCES",
                "EXTENDS"
            ]
        },
        "input": "sent0: In fact, one of the sources of bilingual information used in the previous edition of the shared task, Apertium, has been replaced by a new one: Lucy LT.\n sent1: The results obtained confirm the conclusion by #TARGET_REF that combining the baseline features with those obtained from external sources of bilingual information provide a noticeable improvement, in this case, not only for word-level MTQE, but also for phrase-level MTQE.\n sent2: Some future work may be interesting, specially as regards the approach to phrase-level MTQE.\n sent3: As already mentioned, it would be interesting to use binary classifiers that support sparse features, in order to be able to directly train a single binary classifier capable to deal with phrases of any length.\n sent4: This would make it possible to put together all the data available, avoiding splitting it into smaller training sets for different classifiers, and therefore allowing to have larger training data set.\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"EXTENDS\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "7 The training sets 5 The list of features can be found in the file features list in the package http://www.quest.",
                "dcs.shef.ac.uk/wmt16_files_qe/task2_ en-de_test.tar.gz 6 The list of features can be found in the file features list in the package http://www.quest.",
                "dcs.shef.ac.uk/wmt16_files_qe/task2p_ en-de_test.tar.gz 7 The rest of parameters of the classifiers were also kept as in the approach by #TARGET_REF .",
                "provided by the organisation were used to train the binary classifiers, both for word and phrase levels, while the development sets were used as validation sets on which the training error was computed, in order to minimise the risk of overfitting.",
                "The binary classifiers for the sub-task on phrase-level MTQE was trained to optimise the main comparison metric: F BAD 1 ·F OK 1 , while the classifier for word-level MTQE was trained to optimise the F BAD 1 metric, which was the main comparison metric in WMT 2015."
            ],
            "label": [
                "USE",
                "SIMILARITY"
            ]
        },
        "input": "sent0: 7 The training sets 5 The list of features can be found in the file features list in the package http://www.quest.\n sent1: dcs.shef.ac.uk/wmt16_files_qe/task2_ en-de_test.tar.gz 6 The list of features can be found in the file features list in the package http://www.quest.\n sent2: dcs.shef.ac.uk/wmt16_files_qe/task2p_ en-de_test.tar.gz 7 The rest of parameters of the classifiers were also kept as in the approach by #TARGET_REF .\n sent3: provided by the organisation were used to train the binary classifiers, both for word and phrase levels, while the development sets were used as validation sets on which the training error was computed, in order to minimise the risk of overfitting.\n sent4: The binary classifiers for the sub-task on phrase-level MTQE was trained to optimise the main comparison metric: F BAD 1 ·F OK 1 , while the classifier for word-level MTQE was trained to optimise the F BAD 1 metric, which was the main comparison metric in WMT 2015.\n",
        "output": "{\"label\": [\"USE\", \"SIMILARITY\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "It is worth mentioning that the binary classifiers for phrase-level MTQE use the probabilities provided by the best performing system for word-level MTQE: the one that combines the features obtained from on-line sources of bilingual information with the baseline features.",
                "However, the phrase-level baseline features are only used in one of the systems submitted.",
                "Table 1 shows the results obtained by the systems submitted to the shared task on MTQE, both at the level of words and at the level of phrases.",
                "The table also includes the results obtained with a binary classifier trained only on the baseline features (baseline), in order to estimate the contribution of the features described in this work on the performance of the system.",
                "Incidentally, and in spite of the changes in languages and machine translation systems, the results obtained for word-level MTQE are very similar to those obtained by #TARGET_REF for the translation from English into Spanish."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: It is worth mentioning that the binary classifiers for phrase-level MTQE use the probabilities provided by the best performing system for word-level MTQE: the one that combines the features obtained from on-line sources of bilingual information with the baseline features.\n sent1: However, the phrase-level baseline features are only used in one of the systems submitted.\n sent2: Table 1 shows the results obtained by the systems submitted to the shared task on MTQE, both at the level of words and at the level of phrases.\n sent3: The table also includes the results obtained with a binary classifier trained only on the baseline features (baseline), in order to estimate the contribution of the features described in this work on the performance of the system.\n sent4: Incidentally, and in spite of the changes in languages and machine translation systems, the results obtained for word-level MTQE are very similar to those obtained by #TARGET_REF for the translation from English into Spanish.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Both NLP and vision communities have proposed various multi-modal learning methods to bridge the gap between language and vision.",
                "However, how general sentence representations can be benefited from visual grounding has not been fully explored yet.",
                "Very recently, #TARGET_REF proposed a multi-modal #TARGET_REF that, given an image caption, jointly predicts another caption and the features of associated image.",
                "The work showed promising results for further improving general sentence representations by grounding them visually.",
                "However, according to the model, visual association only occurs at the final hidden state of the encoder, potentially limiting the effect of visual grounding."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Both NLP and vision communities have proposed various multi-modal learning methods to bridge the gap between language and vision.\n sent1: However, how general sentence representations can be benefited from visual grounding has not been fully explored yet.\n sent2: Very recently, #TARGET_REF proposed a multi-modal #TARGET_REF that, given an image caption, jointly predicts another caption and the features of associated image.\n sent3: The work showed promising results for further improving general sentence representations by grounding them visually.\n sent4: However, according to the model, visual association only occurs at the final hidden state of the encoder, potentially limiting the effect of visual grounding.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Image captioning [11] [12] [13] [14] and image synthesis [15] are two common tasks.",
                "There have been significant studies focusing on improving word embeddings [16, 17] , phrase embeddings [18] , sentence embeddings #TARGET_REF, 19] , language models [20] through multi-modal learning of vision and language.",
                "Among all studies, [8] is the first to apply skip-gram-like intuition (predicting multiple modalities from langauge) to joint learning of language and vision in the perspective of general sentence representations.",
                "Attention Mechanism in Multi-Modal Semantics.",
                "Attention mechanism was first introduced in [21] for neural machine translation."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Image captioning [11] [12] [13] [14] and image synthesis [15] are two common tasks.\n sent1: There have been significant studies focusing on improving word embeddings [16, 17] , phrase embeddings [18] , sentence embeddings #TARGET_REF, 19] , language models [20] through multi-modal learning of vision and language.\n sent2: Among all studies, [8] is the first to apply skip-gram-like intuition (predicting multiple modalities from langauge) to joint learning of language and vision in the perspective of general sentence representations.\n sent3: Attention Mechanism in Multi-Modal Semantics.\n sent4: Attention mechanism was first introduced in [21] for neural machine translation.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We base our model on the #TARGET_REF introduced in #TARGET_REF .",
                "A bidirectional Long Short-Term Memory (LSTM) [24] encodes an input sentence and produces a sentence representation for the input.",
                "A pair of LSTM cells encodes the input sequence in both directions and produce two final hidden states: h t and h t .",
                "The hidden representation of the entire sequence is produced by selecting maximum elements between the two hidden states: h S = max( h t , h t ).",
                "The decoder calculates the probability of a target word y t at each time step t, conditional to the sentence representation h S and all target words before t. P (y t | y <t , h S )."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We base our model on the #TARGET_REF introduced in #TARGET_REF .\n sent1: A bidirectional Long Short-Term Memory (LSTM) [24] encodes an input sentence and produces a sentence representation for the input.\n sent2: A pair of LSTM cells encodes the input sequence in both directions and produce two final hidden states: h t and h t .\n sent3: The hidden representation of the entire sequence is produced by selecting maximum elements between the two hidden states: h S = max( h t , h t ).\n sent4: The decoder calculates the probability of a target word y t at each time step t, conditional to the sentence representation h S and all target words before t. P (y t | y <t , h S ).\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Following the experimental design of #TARGET_REF , we conduct experiments on three different learning objectives: CAP2ALL, CAP2CAP, CAP2IMG.",
                "Under CAP2ALL, the model is trained to predict both the target caption and the associated image: L = L C + L V G .",
                "Under CAP2CAP, the model is trained to predict only the target caption (L = L C ) and, under CAP2IMG, only the associated image (L = L V G )."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Following the experimental design of #TARGET_REF , we conduct experiments on three different learning objectives: CAP2ALL, CAP2CAP, CAP2IMG.\n sent1: Under CAP2ALL, the model is trained to predict both the target caption and the associated image: L = L C + L V G .\n sent2: Under CAP2CAP, the model is trained to predict only the target caption (L = L C ) and, under CAP2IMG, only the associated image (L = L V G ).\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "We employ orthogonal initialization [30] for recurrent weights and xavier initialization [31] for all others.",
                "For the datasets, we use Karpathy and Fei-Fei's split for MS-COCO dataset [13] .",
                "Image features are prepared by extracting hidden representations at the final layer of ResNet-101 [32] .",
                "We evaluate sentence representation quality using SentEval 2 #TARGET_REF, 10] scripts.",
                "Mini-batch size is 128 and negative samples are prepared from remaining data samples in the same mini-batch."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We employ orthogonal initialization [30] for recurrent weights and xavier initialization [31] for all others.\n sent1: For the datasets, we use Karpathy and Fei-Fei's split for MS-COCO dataset [13] .\n sent2: Image features are prepared by extracting hidden representations at the final layer of ResNet-101 [32] .\n sent3: We evaluate sentence representation quality using SentEval 2 #TARGET_REF, 10] scripts.\n sent4: Mini-batch size is 128 and negative samples are prepared from remaining data samples in the same mini-batch.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Adhering to the experimental settings of #TARGET_REF , we concatenate sentence representations produced from our model with those obtained from the state-of-the-art unsupervised learning model (Layer Normalized Skip-Thoughts, ST-LN) [33] .",
                "We evaluate the quality of sentence representations produced from different variants of our encoders on well-known transfer tasks: movie review sentiment (MR) [34] , customer reviews (CR) [35] , subjectivity (SUBJ) [36] , opinion polarity (MPQA) [37] , paraphrase identification (MSRP) [38] , binary sentiment classification (SST) [39] , SICK entailment and SICK relatedness [40] ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Adhering to the experimental settings of #TARGET_REF , we concatenate sentence representations produced from our model with those obtained from the state-of-the-art unsupervised learning model (Layer Normalized Skip-Thoughts, ST-LN) [33] .\n sent1: We evaluate the quality of sentence representations produced from different variants of our encoders on well-known transfer tasks: movie review sentiment (MR) [34] , customer reviews (CR) [35] , subjectivity (SUBJ) [36] , opinion polarity (MPQA) [37] , paraphrase identification (MSRP) [38] , binary sentiment classification (SST) [39] , SICK entailment and SICK relatedness [40] .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Given the source caption representation h S and the relevant image representation h I , we associate the two representations by projecting h S into image feature space.",
                "We train the model to rank the similarity between predicted image featuresh I and the target image features h I higher than other pairs, which is achieved by ranking loss functions.",
                "Although margin ranking loss has been the dominant choice for training cross-modal feature matching #TARGET_REF, 20, 25] , we find that log-exp-sum pairwise ranking [26] yields better results in terms of evaluation performance and efficiency.",
                "Thus, the objective for ranking",
                "where N is the set of negative examples and sim is cosine similarity."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Given the source caption representation h S and the relevant image representation h I , we associate the two representations by projecting h S into image feature space.\n sent1: We train the model to rank the similarity between predicted image featuresh I and the target image features h I higher than other pairs, which is achieved by ranking loss functions.\n sent2: Although margin ranking loss has been the dominant choice for training cross-modal feature matching #TARGET_REF, 20, 25] , we find that log-exp-sum pairwise ranking [26] yields better results in terms of evaluation performance and efficiency.\n sent3: Thus, the objective for ranking\n sent4: where N is the set of negative examples and sim is cosine similarity.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "In order to study the effects of incorporating self-attention mechanism in joint prediction of image and language features, we examine attention vectors for selected samples from MS-COCO dataset and compare them to associated images ( Figure 1 ).",
                "For example, given the sentence \"man in black shirt is playing guitar\", our model identifies words that have association with strong visual imagery, such as \"man\", \"black\" and \"guitar\".",
                "Given the second sentence, our model learned to attend to visually significant words such as \"cat\" and \"bowl\".",
                "These findings show that visually grounding self-attended sentence representations helps to expose word-level visual features onto sentence representations #TARGET_REF .",
                "Figure 1: Activated attention weights on two samples from MS-COCO dataset."
            ],
            "label": [
                "DIFFERENCES",
                "EXTENDS"
            ]
        },
        "input": "sent0: In order to study the effects of incorporating self-attention mechanism in joint prediction of image and language features, we examine attention vectors for selected samples from MS-COCO dataset and compare them to associated images ( Figure 1 ).\n sent1: For example, given the sentence \"man in black shirt is playing guitar\", our model identifies words that have association with strong visual imagery, such as \"man\", \"black\" and \"guitar\".\n sent2: Given the second sentence, our model learned to attend to visually significant words such as \"cat\" and \"bowl\".\n sent3: These findings show that visually grounding self-attended sentence representations helps to expose word-level visual features onto sentence representations #TARGET_REF .\n sent4: Figure 1: Activated attention weights on two samples from MS-COCO dataset.\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"EXTENDS\"], \"context\": [\"sent0\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "This paper proposes dynamic chunk reader (DCR), an end-toend neural reading comprehension (RC) model that is able to extract and rank a set of answer candidates from a given document to answer questions.",
                "DCR is able to predict answers of variable lengths, whereas previous neural RC models primarily focused on predicting single tokens or entities.",
                "DCR encodes a document and an input question with recurrent neural networks, and then applies a word-by-word attention mechanism to acquire question-aware representations for the document, followed by the generation of chunk representations and a ranking module to propose the top-ranked chunk as the answer.",
                "Experimental results show that DCR achieves stateof-the-art exact match and F1 scores on the SQuAD dataset #TARGET_REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This paper proposes dynamic chunk reader (DCR), an end-toend neural reading comprehension (RC) model that is able to extract and rank a set of answer candidates from a given document to answer questions.\n sent1: DCR is able to predict answers of variable lengths, whereas previous neural RC models primarily focused on predicting single tokens or entities.\n sent2: DCR encodes a document and an input question with recurrent neural networks, and then applies a word-by-word attention mechanism to acquire question-aware representations for the document, followed by the generation of chunk representations and a ranking module to propose the top-ranked chunk as the answer.\n sent3: Experimental results show that DCR achieves stateof-the-art exact match and F1 scores on the SQuAD dataset #TARGET_REF .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The contributions of this paper are three-fold.",
                "(1) We pro- We also propose several simple but effective features to strengthen the attention mechanism, which fundamentally improves candidate ranking, with the by-product of higher exact boundary match accuracy.",
                "The experiments on the Stanford Question Answering Dataset (SQuAD) #TARGET_REF , which contains a variety of human-generated factoid and non-factoid questions, have shown the effectiveness of above three contributions.",
                "Our paper is organized as follows.",
                "We formally define the RCQA problem first."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The contributions of this paper are three-fold.\n sent1: (1) We pro- We also propose several simple but effective features to strengthen the attention mechanism, which fundamentally improves candidate ranking, with the by-product of higher exact boundary match accuracy.\n sent2: The experiments on the Stanford Question Answering Dataset (SQuAD) #TARGET_REF , which contains a variety of human-generated factoid and non-factoid questions, have shown the effectiveness of above three contributions.\n sent3: Our paper is organized as follows.\n sent4: We formally define the RCQA problem first.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Remark: Categories of RC Tasks Other simpler variants of the aforementioned RC task were explored in the past.",
                "For example, quiz-style datasets (e.g., MCTest (Richardson, Burges, and #REF), MovieQA (#REF) ) have multiple-choice questions with answer options.",
                "Cloze-style datesets #REF; #REF) , usually automatically generated, have factoid \"question\"s created by replacing the answer in a sentence from the text with blank.",
                "For the answer selection task this paper focuses on, several datasets exist, e.g. TREC-QA for factoid answer extraction from multiple given passages, bAbI (Weston, Chopra, and #REF) designed for inference purpose, and the SQuAD dataset #TARGET_REF used in this paper.",
                "To the best of our knowledge, the SQuAD dataset is the only one for both factoid and nonfactoid answer extraction with a question distribution more close to real-world applications."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Remark: Categories of RC Tasks Other simpler variants of the aforementioned RC task were explored in the past.\n sent1: For example, quiz-style datasets (e.g., MCTest (Richardson, Burges, and #REF), MovieQA (#REF) ) have multiple-choice questions with answer options.\n sent2: Cloze-style datesets #REF; #REF) , usually automatically generated, have factoid \"question\"s created by replacing the answer in a sentence from the text with blank.\n sent3: For the answer selection task this paper focuses on, several datasets exist, e.g. TREC-QA for factoid answer extraction from multiple given passages, bAbI (Weston, Chopra, and #REF) designed for inference purpose, and the SQuAD dataset #TARGET_REF used in this paper.\n sent4: To the best of our knowledge, the SQuAD dataset is the only one for both factoid and nonfactoid answer extraction with a question distribution more close to real-world applications.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Answer Chunking To reduce the errors generated by the rule-based chunker in #TARGET_REF , first, we capture the part-of-speech (POS) pattern of all answer subsequences in the training dataset to form a POS pattern trie tree, and then apply the answer POS patterns to passage P i to acquire a collection of all subsequences (chunk candidates) C i whose POS patterns can be matched to the POS pattern trie.",
                "This is equivalent to putting an constraint subj(m, n, P i ) to candidate answer chunk generation process that only choose the chunk with a POS pattern seen for answers in the training data.",
                "Then the sub-sequences C i are used as answer candidates for P i .",
                "Note that overlapping chunks could be generated for a passage, and we rely on the ranker to choose the best candidate based on features from the cloze-style RC system.",
                "Experiments showed that for > 90% of the questions on the development set, the ground truth answer is included in the candidate set constructed in such manner. ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Answer Chunking To reduce the errors generated by the rule-based chunker in #TARGET_REF , first, we capture the part-of-speech (POS) pattern of all answer subsequences in the training dataset to form a POS pattern trie tree, and then apply the answer POS patterns to passage P i to acquire a collection of all subsequences (chunk candidates) C i whose POS patterns can be matched to the POS pattern trie.\n sent1: This is equivalent to putting an constraint subj(m, n, P i ) to candidate answer chunk generation process that only choose the chunk with a POS pattern seen for answers in the training data.\n sent2: Then the sub-sequences C i are used as answer candidates for P i .\n sent3: Note that overlapping chunks could be generated for a passage, and we rely on the ranker to choose the best candidate based on features from the cloze-style RC system.\n sent4: Experiments showed that for > 90% of the questions on the development set, the ground truth answer is included in the candidate set constructed in such manner. .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Dataset We used the Stanford Question Answering Dataset (SQuAD) #TARGET_REF for the experiment.",
                "SQuAD came into our sight because it is a mix of factoid and non-factoid questions, a real-world data (crowd-sourced), and of large scale (over 100K question-answer pairs collected from 536 Wikipedia articles).",
                "Answers range from single words to long, variable-length phrase/clauses.",
                "It is a relaxation of assumptions by the cloze-style and quiz-style RC datasets in the Problem Definition section.",
                "Features The input vector representation of each word w to encoder RNNs has six parts including a pre-trained 300-dimensional GloVe embedding (Pennington, Socher, and and five features (see Figure 1) : (1) a onehot encoding (46 dimensions) for the part-of-speech (POS) tag of w; (2) a one-hot encoding (14 dimensions) for named entity (NE) tag of w; (3) a binary value indicating whether w's surface form is the same to any word in the quesiton; (4) if the lemma form of w is the same to any word in the question; and (5) if w is caplitalized."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Dataset We used the Stanford Question Answering Dataset (SQuAD) #TARGET_REF for the experiment.\n sent1: SQuAD came into our sight because it is a mix of factoid and non-factoid questions, a real-world data (crowd-sourced), and of large scale (over 100K question-answer pairs collected from 536 Wikipedia articles).\n sent2: Answers range from single words to long, variable-length phrase/clauses.\n sent3: It is a relaxation of assumptions by the cloze-style and quiz-style RC datasets in the Problem Definition section.\n sent4: Features The input vector representation of each word w to encoder RNNs has six parts including a pre-trained 300-dimensional GloVe embedding (Pennington, Socher, and and five features (see Figure 1) : (1) a onehot encoding (46 dimensions) for the part-of-speech (POS) tag of w; (2) a one-hot encoding (14 dimensions) for named entity (NE) tag of w; (3) a binary value indicating whether w's surface form is the same to any word in the quesiton; (4) if the lemma form of w is the same to any word in the question; and (5) if w is caplitalized.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "This is because RCQA can exploit the textual evidences to ensure increased answer coverage, which is particularly helpful for non-factoid answers.",
                "However, it is also challenging for RCQA to identify answer in arbitrary position in the passage with arbitrary length, especially for nonfactoid answers which might be clauses or sentences.",
                "As a result, apart from a few exceptions #TARGET_REF; #REF) , this research direction has not been fully explored yet.",
                "Compared to the relatively easier RC task of predicting single tokens/entities 1 , predicting answers of arbitrary lengths and positions significantly increase the search space complexity: the number of possible candidates to consider is in the order of O(n 2 ), where n is the number of passage words.",
                "In contrast, for previous works in which answers are single tokens/entities or from candidate lists, the complexity is in O(n) or the size of candidate lists l (usually l ≤5), respectively."
            ],
            "label": [
                "BACKGROUND",
                "MOTIVATION"
            ]
        },
        "input": "sent0: This is because RCQA can exploit the textual evidences to ensure increased answer coverage, which is particularly helpful for non-factoid answers.\n sent1: However, it is also challenging for RCQA to identify answer in arbitrary position in the passage with arbitrary length, especially for nonfactoid answers which might be clauses or sentences.\n sent2: As a result, apart from a few exceptions #TARGET_REF; #REF) , this research direction has not been fully explored yet.\n sent3: Compared to the relatively easier RC task of predicting single tokens/entities 1 , predicting answers of arbitrary lengths and positions significantly increase the search space complexity: the number of possible candidates to consider is in the order of O(n 2 ), where n is the number of passage words.\n sent4: In contrast, for previous works in which answers are single tokens/entities or from candidate lists, the complexity is in O(n) or the size of candidate lists l (usually l ≤5), respectively.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"MOTIVATION\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "In contrast, for previous works in which answers are single tokens/entities or from candidate lists, the complexity is in O(n) or the size of candidate lists l (usually l ≤5), respectively.",
                "To address the above complexity, #REF used a two-step chunk-and-rank approach that employs a rule-based algorithm to extract answer candidates from a passage, followed by a ranking approach with hand-crafted features to select the best answer.",
                "The rule-based chunking approach suffered from low coverage (≈ 70% recall of answer chunks) that cannot be improved during training; and candidate ranking performance depends greatly on the quality of the hand-crafted features.",
                "More recently, #REF proposed two endto-end neural network models, one of which chunks a candidate answer by predicting the answer's two boundary indices and the other classifies each passage word into answer/notanswer.",
                "Both models improved significantly over the method proposed by #TARGET_REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In contrast, for previous works in which answers are single tokens/entities or from candidate lists, the complexity is in O(n) or the size of candidate lists l (usually l ≤5), respectively.\n sent1: To address the above complexity, #REF used a two-step chunk-and-rank approach that employs a rule-based algorithm to extract answer candidates from a passage, followed by a ranking approach with hand-crafted features to select the best answer.\n sent2: The rule-based chunking approach suffered from low coverage (≈ 70% recall of answer chunks) that cannot be improved during training; and candidate ranking performance depends greatly on the quality of the hand-crafted features.\n sent3: More recently, #REF proposed two endto-end neural network models, one of which chunks a candidate answer by predicting the answer's two boundary indices and the other classifies each passage word into answer/notanswer.\n sent4: Both models improved significantly over the method proposed by #TARGET_REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Our proposed model, called dynamic chunk reader (DCR), not only significantly differs from both the above systems in the way that answer candidates are generated and ranked, but also shares merits with both works.",
                "First, our model uses deep networks to learn better representations for candidate answer chunks, instead of using fixed feature representations as in #TARGET_REF .",
                "Second, it represents answer candidates as chunks, as in (#REF ), instead of word-level representations (#REF) , to make the model aware of the subtle differences among candidates (importantly, overlapping candidates).",
                "The contributions of this paper are three-fold.",
                "(1) We pro- We also propose several simple but effective features to strengthen the attention mechanism, which fundamentally improves candidate ranking, with the by-product of higher exact boundary match accuracy."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Our proposed model, called dynamic chunk reader (DCR), not only significantly differs from both the above systems in the way that answer candidates are generated and ranked, but also shares merits with both works.\n sent1: First, our model uses deep networks to learn better representations for candidate answer chunks, instead of using fixed feature representations as in #TARGET_REF .\n sent2: Second, it represents answer candidates as chunks, as in (#REF ), instead of word-level representations (#REF) , to make the model aware of the subtle differences among candidates (importantly, overlapping candidates).\n sent3: The contributions of this paper are three-fold.\n sent4: (1) We pro- We also propose several simple but effective features to strengthen the attention mechanism, which fundamentally improves candidate ranking, with the by-product of higher exact boundary match accuracy.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "As the first row of Table 3 shows, our baseline system improves 10% (EM) over #TARGET_REF (Table 2 , row 1), the feature-based ranking system.",
                "However when compared to our DCR model (Table 3 , row 2), the baseline (row 1) is more than 12% (EM) behind even though it is based on the state-of-the-art model for cloze-style RC tasks.",
                "This can be attributed to the advanced model structure and end-to-end manner of DCR.",
                "We also did ablation tests on our DCR model.",
                "First, replacing the word-by-word attention with Attentive Reader style attention decreases the EM score by about 4.5%, showing the strength of our proposed attention mechanism."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: As the first row of Table 3 shows, our baseline system improves 10% (EM) over #TARGET_REF (Table 2 , row 1), the feature-based ranking system.\n sent1: However when compared to our DCR model (Table 3 , row 2), the baseline (row 1) is more than 12% (EM) behind even though it is based on the state-of-the-art model for cloze-style RC tasks.\n sent2: This can be attributed to the advanced model structure and end-to-end manner of DCR.\n sent3: We also did ablation tests on our DCR model.\n sent4: First, replacing the word-by-word attention with Attentive Reader style attention decreases the EM score by about 4.5%, showing the strength of our proposed attention mechanism.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Multiple Deep Neural Network (DNN) architectures have been explored to combine the token representations into a single segment representation that captures relevant information for the task.",
                "Of the two state-of-the-art approaches on dialog act recognition, one uses a deep stack of Recurrent Neural Networks (RNNs) (#REF) to capture long distance relations between tokens (#REF) , while the other uses multiple parallel temporal Convolutional Neural Networks (CNNs) (#REF) to capture relevant functional patterns with different length #TARGET_REF .",
                "Although these approaches focus on capturing different information, both have been proved successful on the task.",
                "Thus, an approach able to capture both kinds of information is expected to outperform both of these approaches.",
                "Finally, concerning the third aspect, a dialog act is not only related to the words in a segment, but also to the whole dialog context."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Multiple Deep Neural Network (DNN) architectures have been explored to combine the token representations into a single segment representation that captures relevant information for the task.\n sent1: Of the two state-of-the-art approaches on dialog act recognition, one uses a deep stack of Recurrent Neural Networks (RNNs) (#REF) to capture long distance relations between tokens (#REF) , while the other uses multiple parallel temporal Convolutional Neural Networks (CNNs) (#REF) to capture relevant functional patterns with different length #TARGET_REF .\n sent2: Although these approaches focus on capturing different information, both have been proved successful on the task.\n sent3: Thus, an approach able to capture both kinds of information is expected to outperform both of these approaches.\n sent4: Finally, concerning the third aspect, a dialog act is not only related to the words in a segment, but also to the whole dialog context.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Furthermore, it is also influenced by the speaker's intrinsic characteristics.",
                "The latter are hard to capture and are usually not available for a dialog system.",
                "Thus, only speaker information that is directly related to the dialog, such as turn-taking #TARGET_REF , is typically considered.",
                "Concerning information from the surrounding segments, its influence, especially that of preceding segments, has been thoroughly explored in at least two studies (#REF; #REF) .",
                "However, in both cases, although it is one of its most important characteristics, that information was represented in ways that are not appropriate to capture its sequentiality."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Furthermore, it is also influenced by the speaker's intrinsic characteristics.\n sent1: The latter are hard to capture and are usually not available for a dialog system.\n sent2: Thus, only speaker information that is directly related to the dialog, such as turn-taking #TARGET_REF , is typically considered.\n sent3: Concerning information from the surrounding segments, its influence, especially that of preceding segments, has been thoroughly explored in at least two studies (#REF; #REF) .\n sent4: However, in both cases, although it is one of its most important characteristics, that information was represented in ways that are not appropriate to capture its sequentiality.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The latter are hard to capture and are usually not available for a dialog system.",
                "Thus, only speaker information that is directly related to the dialog, such as turn-taking (#REF) , is typically considered.",
                "Concerning information from the surrounding segments, its influence, especially that of preceding segments, has been thoroughly explored in at least two studies (#REF; #TARGET_REF .",
                "However, in both cases, although it is one of its most important characteristics, that information was represented in ways that are not appropriate to capture its sequentiality.",
                "In this article we use the Switchboard Dialog Act Corpus (#REF) , which is the most explored corpus for dialog act recognition, to study and compare different solutions concerning the three aspects."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The latter are hard to capture and are usually not available for a dialog system.\n sent1: Thus, only speaker information that is directly related to the dialog, such as turn-taking (#REF) , is typically considered.\n sent2: Concerning information from the surrounding segments, its influence, especially that of preceding segments, has been thoroughly explored in at least two studies (#REF; #TARGET_REF .\n sent3: However, in both cases, although it is one of its most important characteristics, that information was represented in ways that are not appropriate to capture its sequentiality.\n sent4: In this article we use the Switchboard Dialog Act Corpus (#REF) , which is the most explored corpus for dialog act recognition, to study and compare different solutions concerning the three aspects.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "This way, the model is able to capture long distance relations between tokens.",
                "On the convolutional side, #TARGET_REF generated the segment representation by combining the outputs of three parallel CNNs with different context window sizes, in order to capture different functional patterns.",
                "In both cases, pre-trained word-embeddings were used as input to the network.",
                "#REF compared the performance of embeddings with different dimensionality trained on multiple corpora using GloVe and Word2Vec (#REF) .",
                "The best results were achieved when using 150-dimensional embeddings trained on Wikipedia data using Word2Vec."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This way, the model is able to capture long distance relations between tokens.\n sent1: On the convolutional side, #TARGET_REF generated the segment representation by combining the outputs of three parallel CNNs with different context window sizes, in order to capture different functional patterns.\n sent2: In both cases, pre-trained word-embeddings were used as input to the network.\n sent3: #REF compared the performance of embeddings with different dimensionality trained on multiple corpora using GloVe and Word2Vec (#REF) .\n sent4: The best results were achieved when using 150-dimensional embeddings trained on Wikipedia data using Word2Vec.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "On the convolutional side, #REF generated the segment representation by combining the outputs of three parallel CNNs with different context window sizes, in order to capture different functional patterns.",
                "In both cases, pre-trained word-embeddings were used as input to the network.",
                "#REF compared the performance of embeddings with different dimensionality trained on multiple corpora using GloVe and Word2Vec (#REF) .",
                "The best results were achieved when using 150-dimensional embeddings trained on Wikipedia data using Word2Vec.",
                "#TARGET_REF used 200-dimensional Word2Vec embeddings trained on Facebook data."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: On the convolutional side, #REF generated the segment representation by combining the outputs of three parallel CNNs with different context window sizes, in order to capture different functional patterns.\n sent1: In both cases, pre-trained word-embeddings were used as input to the network.\n sent2: #REF compared the performance of embeddings with different dimensionality trained on multiple corpora using GloVe and Word2Vec (#REF) .\n sent3: The best results were achieved when using 150-dimensional embeddings trained on Wikipedia data using Word2Vec.\n sent4: #TARGET_REF used 200-dimensional Word2Vec embeddings trained on Facebook data.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Overall, from the reported results, it is not possible to state which is the top performing segment representation approach since the evaluation was performed on different subsets of the corpus.",
                "Still, #REF reported 73.9% accuracy on the validation set and 80.1% on the test set, while #TARGET_REF reported 74.5% and 76.9% accuracy on the two sets used to evaluate #TARGET_REF.",
                "Additionally, #REF explored the use of context information concerning speaker changes and from the surrounding segments.",
                "The first was provided as a flag and concatenated to the segment representation.",
                "Concerning the latter, they explored the use of discourse models, as well as of approaches that concatenated the context information directly to the segment representation."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Overall, from the reported results, it is not possible to state which is the top performing segment representation approach since the evaluation was performed on different subsets of the corpus.\n sent1: Still, #REF reported 73.9% accuracy on the validation set and 80.1% on the test set, while #TARGET_REF reported 74.5% and 76.9% accuracy on the two sets used to evaluate #TARGET_REF.\n sent2: Additionally, #REF explored the use of context information concerning speaker changes and from the surrounding segments.\n sent3: The first was provided as a flag and concatenated to the segment representation.\n sent4: Concerning the latter, they explored the use of discourse models, as well as of approaches that concatenated the context information directly to the segment representation.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF used 200-dimensional Word2Vec embeddings trained on Facebook data.",
                "Overall, from the reported results, it is not possible to state which is the top performing segment representation approach since the evaluation was performed on different subsets of the corpus.",
                "Still, #REF reported 73.9% accuracy on the validation set and 80.1% on the test set, while #REF reported 74.5% and 76.9% accuracy on the two sets used to evaluate their experiments.",
                "Additionally, #TARGET_REF explored the use of context information concerning speaker changes and from the surrounding segments.",
                "The first was provided as a flag and concatenated to the segment representation."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #REF used 200-dimensional Word2Vec embeddings trained on Facebook data.\n sent1: Overall, from the reported results, it is not possible to state which is the top performing segment representation approach since the evaluation was performed on different subsets of the corpus.\n sent2: Still, #REF reported 73.9% accuracy on the validation set and 80.1% on the test set, while #REF reported 74.5% and 76.9% accuracy on the two sets used to evaluate their experiments.\n sent3: Additionally, #TARGET_REF explored the use of context information concerning speaker changes and from the surrounding segments.\n sent4: The first was provided as a flag and concatenated to the segment representation.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Thus, when predicting the classification of a segment, the surrounding ones are also taken into account.",
                "However, when the discourse model is based on a CNN or a bidirectional LSTM unit, it considers information from future segments, which is not available for a dialog system.",
                "Still, even when relying on future information, the approaches based on discourse models performed worse than those that concatenated the context information directly to the segment representation.",
                "In this sense, similarly to our previous study using SVMs (#REF) , #TARGET_REF concluded that providing that information in the form of the classification of the surrounding segments leads to better results than using #TARGET_REF, even when those classifications are obtained automatically.",
                "Furthermore, both studies have shown that the first preceding segment is the most important and that the influence decays with the distance."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Thus, when predicting the classification of a segment, the surrounding ones are also taken into account.\n sent1: However, when the discourse model is based on a CNN or a bidirectional LSTM unit, it considers information from future segments, which is not available for a dialog system.\n sent2: Still, even when relying on future information, the approaches based on discourse models performed worse than those that concatenated the context information directly to the segment representation.\n sent3: In this sense, similarly to our previous study using SVMs (#REF) , #TARGET_REF concluded that providing that information in the form of the classification of the surrounding segments leads to better results than using #TARGET_REF, even when those classifications are obtained automatically.\n sent4: Furthermore, both studies have shown that the first preceding segment is the most important and that the influence decays with the distance.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "However, when the discourse model is based on a CNN or a bidirectional LSTM unit, it considers information from future segments, which is not available for a dialog system.",
                "Still, even when relying on future information, the approaches based on discourse models performed worse than those that concatenated the context information directly to the segment representation.",
                "In this sense, similarly to our previous study using SVMs (#REF) , #REF concluded that providing that information in the form of the classification of the surrounding segments leads to better results than using their words, even when those classifications are obtained automatically.",
                "Furthermore, both studies have shown that the first preceding segment is the most important and that the influence decays with the distance.",
                "Using the setup with gold standard labels from three preceding segments, #TARGET_REF achieved 79.6% and 81.8% on the two sets used to evaluate the approach."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: However, when the discourse model is based on a CNN or a bidirectional LSTM unit, it considers information from future segments, which is not available for a dialog system.\n sent1: Still, even when relying on future information, the approaches based on discourse models performed worse than those that concatenated the context information directly to the segment representation.\n sent2: In this sense, similarly to our previous study using SVMs (#REF) , #REF concluded that providing that information in the form of the classification of the surrounding segments leads to better results than using their words, even when those classifications are obtained automatically.\n sent3: Furthermore, both studies have shown that the first preceding segment is the most important and that the influence decays with the distance.\n sent4: Using the setup with gold standard labels from three preceding segments, #TARGET_REF achieved 79.6% and 81.8% on the two sets used to evaluate the approach.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Consequently, it is where the main differences between previous dialog act recognition approaches occur.",
                "Thus, we start our study by exploring different approaches for this step.",
                "As stated in Section 3, of the two state-of-the-art approaches on dialog act recognition, one uses a RNN-based approach (#REF) for segment representation, while the other uses one based on CNNs #TARGET_REF .",
                "Both have their own advantages, as while the first focuses on capturing information from relevant sequences of tokens, the latter focuses on the context surrounding each token and, thus, captures information concerning neighboring tokens.",
                "Concerning the task at hand, this is relevant since, among other aspects, while some dialog acts are distinguishable due to the order of the tokens in the segment (e.g. subject-auxiliary inversion in questions), others are distinguishable due to the presence of certain tokens or sequences of tokens independently of where they occur in the segment (e.g. greetings)."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Consequently, it is where the main differences between previous dialog act recognition approaches occur.\n sent1: Thus, we start our study by exploring different approaches for this step.\n sent2: As stated in Section 3, of the two state-of-the-art approaches on dialog act recognition, one uses a RNN-based approach (#REF) for segment representation, while the other uses one based on CNNs #TARGET_REF .\n sent3: Both have their own advantages, as while the first focuses on capturing information from relevant sequences of tokens, the latter focuses on the context surrounding each token and, thus, captures information concerning neighboring tokens.\n sent4: Concerning the task at hand, this is relevant since, among other aspects, while some dialog acts are distinguishable due to the order of the tokens in the segment (e.g. subject-auxiliary inversion in questions), others are distinguishable due to the presence of certain tokens or sequences of tokens independently of where they occur in the segment (e.g. greetings).\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "As described in Section 3, the convolutional approach by #TARGET_REF uses a set of parallel temporal CNNs with different window size, each followed by a max pooling operation.",
                "The segment representation is given by the concatenation of the results of the pooling operations.",
                "This way, the representation contains information concerning groups of tokens with different sizes.",
                "To achieve the results presented in their paper, #REF used three CNNs with 100 filters and 1, 2, and 3 as context window sizes.",
                "In a previous study using the same architecture for different tasks, #REF used 3, 4, and 5 as window sizes."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: As described in Section 3, the convolutional approach by #TARGET_REF uses a set of parallel temporal CNNs with different window size, each followed by a max pooling operation.\n sent1: The segment representation is given by the concatenation of the results of the pooling operations.\n sent2: This way, the representation contains information concerning groups of tokens with different sizes.\n sent3: To achieve the results presented in their paper, #REF used three CNNs with 100 filters and 1, 2, and 3 as context window sizes.\n sent4: In a previous study using the same architecture for different tasks, #REF used 3, 4, and 5 as window sizes.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "This way, the representation contains information concerning groups of tokens with different sizes.",
                "To achieve the results presented in #TARGET_REF, #TARGET_REF used three CNNs with 100 filters and 1, 2, and 3 as context window sizes.",
                "In a previous study using the same architecture for different tasks, #REF used 3, 4, and 5 as window sizes.",
                "Both setups performed similarly in our experiments.",
                "However, their combination, that is, five parallel CNNs with window sizes between 1 and 5, led to better results, which means that both small and large groups of tokens provide relevant information."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This way, the representation contains information concerning groups of tokens with different sizes.\n sent1: To achieve the results presented in #TARGET_REF, #TARGET_REF used three CNNs with 100 filters and 1, 2, and 3 as context window sizes.\n sent2: In a previous study using the same architecture for different tasks, #REF used 3, 4, and 5 as window sizes.\n sent3: Both setups performed similarly in our experiments.\n sent4: However, their combination, that is, five parallel CNNs with window sizes between 1 and 5, led to better results, which means that both small and large groups of tokens provide relevant information.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "However, up to a certain level, ambiguity is not necessarily harmful.",
                "As stated in Section 3, #REF explored embedding spaces with dimensionality 75, 150, and 300 together with different embedding approaches.",
                "In every case, the embedding space with dimensionality 150 led to the best results.",
                "#TARGET_REF used a different dimensionality value, 200, in #TARGET_REF.",
                "In our experiments we explored the use of the four different dimensionality values."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: However, up to a certain level, ambiguity is not necessarily harmful.\n sent1: As stated in Section 3, #REF explored embedding spaces with dimensionality 75, 150, and 300 together with different embedding approaches.\n sent2: In every case, the embedding space with dimensionality 150 led to the best results.\n sent3: #TARGET_REF used a different dimensionality value, 200, in #TARGET_REF.\n sent4: In our experiments we explored the use of the four different dimensionality values.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\", \"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "The most common approaches are the Continuous Bag of Words (CBOW) model (#REF) , commonly known as Word2Vec, and the GloVe (#REF).",
                "#REF used pre-trained embeddings using both approaches in their study and achieved their best results using Word2Vec embeddings trained on Wikipedia data.",
                "#TARGET_REF also used Word2Vec embeddings, but trained on Facebook data.",
                "Since we have access to the embeddings trained on Wikipedia data, but not to those trained on Facebook data, we used the first in our experiments.",
                "The CBOW model generates word representations based on the co-occurrence of adjacent words."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The most common approaches are the Continuous Bag of Words (CBOW) model (#REF) , commonly known as Word2Vec, and the GloVe (#REF).\n sent1: #REF used pre-trained embeddings using both approaches in their study and achieved their best results using Word2Vec embeddings trained on Wikipedia data.\n sent2: #TARGET_REF also used Word2Vec embeddings, but trained on Facebook data.\n sent3: Since we have access to the embeddings trained on Wikipedia data, but not to those trained on Facebook data, we used the first in our experiments.\n sent4: The CBOW model generates word representations based on the co-occurrence of adjacent words.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Although a dialog act represents the intention behind a set of words, that intention is not constrained to a specific segment and its context provides relevant cues.",
                "As described in Section 3, previous studies have shown that the most important source of context information Table 8 : Accuracy results using POS tags.",
                "for dialog act recognition is the dialog history, with influence decaying with distance (#REF; Lee & #REF; #TARGET_REF .",
                "However, information concerning the speakers and, more specifically, turn-taking has also been proved important (#REF) .",
                "Thus, in our study, we explore both the surrounding segments and speaker information as sources of context information."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Although a dialog act represents the intention behind a set of words, that intention is not constrained to a specific segment and its context provides relevant cues.\n sent1: As described in Section 3, previous studies have shown that the most important source of context information Table 8 : Accuracy results using POS tags.\n sent2: for dialog act recognition is the dialog history, with influence decaying with distance (#REF; Lee & #REF; #TARGET_REF .\n sent3: However, information concerning the speakers and, more specifically, turn-taking has also been proved important (#REF) .\n sent4: Thus, in our study, we explore both the surrounding segments and speaker information as sources of context information.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Although a dialog act represents the intention behind a set of words, that intention is not constrained to a specific segment and its context provides relevant cues.",
                "As described in Section 3, previous studies have shown that the most important source of context information Table 8 : Accuracy results using POS tags.",
                "for dialog act recognition is the dialog history, with influence decaying with distance (#REF; Lee & #REF; #REF) .",
                "However, information concerning the speakers and, more specifically, turn-taking has also been proved important #TARGET_REF .",
                "Thus, in our study, we explore both the surrounding segments and speaker information as sources of context information."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Although a dialog act represents the intention behind a set of words, that intention is not constrained to a specific segment and its context provides relevant cues.\n sent1: As described in Section 3, previous studies have shown that the most important source of context information Table 8 : Accuracy results using POS tags.\n sent2: for dialog act recognition is the dialog history, with influence decaying with distance (#REF; Lee & #REF; #REF) .\n sent3: However, information concerning the speakers and, more specifically, turn-taking has also been proved important #TARGET_REF .\n sent4: Thus, in our study, we explore both the surrounding segments and speaker information as sources of context information.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "As stated in Section 3, considering the preceding segments, we have shown in a previous study (#REF) that providing information in the form of segment classifications leads to better results than in the form of words.",
                "#TARGET_REF further showed that using a single label per segment is better than using the probability of each class.",
                "Furthermore, both studies showed that using automatic predictions leads to a decrease in performance around 2 percentage points in comparison to using the manual annotations.",
                "Thus, in order to simplify the experiments and obtain an upper bound for the approach, in this study we just use the manual annotations.",
                "In our previous study, we have used up to five preceding segments and showed that the gain becomes smaller as the number of preceding segments increases, which supports the claim that the closest segments are the most relevant."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: As stated in Section 3, considering the preceding segments, we have shown in a previous study (#REF) that providing information in the form of segment classifications leads to better results than in the form of words.\n sent1: #TARGET_REF further showed that using a single label per segment is better than using the probability of each class.\n sent2: Furthermore, both studies showed that using automatic predictions leads to a decrease in performance around 2 percentage points in comparison to using the manual annotations.\n sent3: Thus, in order to simplify the experiments and obtain an upper bound for the approach, in this study we just use the manual annotations.\n sent4: In our previous study, we have used up to five preceding segments and showed that the gain becomes smaller as the number of preceding segments increases, which supports the claim that the closest segments are the most relevant.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Using pre-trained embeddings typically leads to results that generalize better, since they are trained on large amounts of data and not only on a reduced set focused on a particular domain.",
                "However, this also means that their generation does not take their future use on a specific task into account.",
                "In #TARGET_REF, #TARGET_REF used pre-trained embeddings but let them adapt to the task during the training phase.",
                "However, they did not perform a comparison with the case where the embeddings are not adaptable.",
                "Thus, in our study we experimented with both fixed and adaptable embeddings."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Using pre-trained embeddings typically leads to results that generalize better, since they are trained on large amounts of data and not only on a reduced set focused on a particular domain.\n sent1: However, this also means that their generation does not take their future use on a specific task into account.\n sent2: In #TARGET_REF, #TARGET_REF used pre-trained embeddings but let them adapt to the task during the training phase.\n sent3: However, they did not perform a comparison with the case where the embeddings are not adaptable.\n sent4: Thus, in our study we experimented with both fixed and adaptable embeddings.\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent2\", \"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Thus, in order to simplify the experiments and obtain an upper bound for the approach, in this study we just use the manual annotations.",
                "In our previous study, we have used up to five preceding segments and showed that the gain becomes smaller as the number of preceding segments increases, which supports the claim that the closest segments are the most relevant.",
                "#REF stopped at three preceding segments, but noticed a similar pattern.",
                "In this study we explore up to five preceding segments, as well as the entire dialog history.",
                "Although both our previous study and that by #TARGET_REF used the classifications of preceding segments as context information, none of them took into account that those segments have a sequential nature and simply flattened the sequence before appending it to the segment representation."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Thus, in order to simplify the experiments and obtain an upper bound for the approach, in this study we just use the manual annotations.\n sent1: In our previous study, we have used up to five preceding segments and showed that the gain becomes smaller as the number of preceding segments increases, which supports the claim that the closest segments are the most relevant.\n sent2: #REF stopped at three preceding segments, but noticed a similar pattern.\n sent3: In this study we explore up to five preceding segments, as well as the entire dialog history.\n sent4: Although both our previous study and that by #TARGET_REF used the classifications of preceding segments as context information, none of them took into account that those segments have a sequential nature and simply flattened the sequence before appending it to the segment representation.\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "In this sense, intentions may vary if two sequential segments are uttered by the same or different speakers.",
                "Thus, turn-taking information is relevant for dialog act recognition.",
                "In fact, this has been confirmed in the study by #TARGET_REF .",
                "Thus, we also use turn-taking information in this study.",
                "It is provided as a flag that states whether the speaker is different from that of the preceding segment."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: In this sense, intentions may vary if two sequential segments are uttered by the same or different speakers.\n sent1: Thus, turn-taking information is relevant for dialog act recognition.\n sent2: In fact, this has been confirmed in the study by #TARGET_REF .\n sent3: Thus, we also use turn-taking information in this study.\n sent4: It is provided as a flag that states whether the speaker is different from that of the preceding segment.\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent1\", \"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Adaptable embeddings attempt to balance the trade-off between the two.",
                "However, all those kinds of information may be complementary and relevant for the task.",
                "Thus, we assessed whether it is advantageous to combine multiple embedding representations.",
                "To do so, we used the approach suggested by #REF in the paper that inspired the CNN approach used by #TARGET_REF .",
                "Considering the architecture in Figure 2 , our approach replicates all the steps up to segment representation, inclusively, for each embedding approach, and then concatenates the generated segment representations before passing them to the dimensionality reduction layer."
            ],
            "label": [
                null
            ]
        },
        "input": "sent0: Adaptable embeddings attempt to balance the trade-off between the two.\n sent1: However, all those kinds of information may be complementary and relevant for the task.\n sent2: Thus, we assessed whether it is advantageous to combine multiple embedding representations.\n sent3: To do so, we used the approach suggested by #REF in the paper that inspired the CNN approach used by #TARGET_REF .\n sent4: Considering the architecture in Figure 2 , our approach replicates all the steps up to segment representation, inclusively, for each embedding approach, and then concatenates the generated segment representations before passing them to the dimensionality reduction layer.\n",
        "output": "{\"label\": [null], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "We started with the segment representation approaches, since that is the step with higher variation among the previous studies described in Section 3 and that which introduces more changes in the overall architecture.",
                "We used adaptations of the approaches with top performance in previous studies, namely the RNN-based approach by #REF and the CNN-based approach by #TARGET_REF .",
                "However, those approaches focus on capturing different kinds of information, both of which are relevant for the task.",
                "Thus, we introduced the use of the RCNN-based approach by #REF and adapted it to capture relevant relations between distant tokens.",
                "This approach outperformed the other two, proving that both token sequences and the contexts that surround each token are relevant for the task."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: We started with the segment representation approaches, since that is the step with higher variation among the previous studies described in Section 3 and that which introduces more changes in the overall architecture.\n sent1: We used adaptations of the approaches with top performance in previous studies, namely the RNN-based approach by #REF and the CNN-based approach by #TARGET_REF .\n sent2: However, those approaches focus on capturing different kinds of information, both of which are relevant for the task.\n sent3: Thus, we introduced the use of the RCNN-based approach by #REF and adapted it to capture relevant relations between distant tokens.\n sent4: This approach outperformed the other two, proving that both token sequences and the contexts that surround each token are relevant for the task.\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "In terms of token embedding, we have explored approaches at the character, word, and functional levels.",
                "Starting with the typically used word-level, we have shown that using an embedding space with 200 dimensions as used by #TARGET_REF in #TARGET_REF leads to better results than any of the dimensionality values used by #REF .",
                "Furthermore, we have shown that, since the dialogs in the Switchboard Dialog Act Corpus have multiple domains, using fixed pre-trained word embeddings leads to better results than letting them be trained along with the network.",
                "In this sense, we have shown that dependency-based embeddings outperform those generated by Word2Vec, which is the most used embedding approach.",
                "This is in accordance with the task, since many dialog acts are related to the structure of the segment and, thus, the dependencies between tokens."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In terms of token embedding, we have explored approaches at the character, word, and functional levels.\n sent1: Starting with the typically used word-level, we have shown that using an embedding space with 200 dimensions as used by #TARGET_REF in #TARGET_REF leads to better results than any of the dimensionality values used by #REF .\n sent2: Furthermore, we have shown that, since the dialogs in the Switchboard Dialog Act Corpus have multiple domains, using fixed pre-trained word embeddings leads to better results than letting them be trained along with the network.\n sent3: In this sense, we have shown that dependency-based embeddings outperform those generated by Word2Vec, which is the most used embedding approach.\n sent4: This is in accordance with the task, since many dialog acts are related to the structure of the segment and, thus, the dependencies between tokens.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Furthermore, our study has shown that their approach can be improved in many aspects.",
                "In the case of #TARGET_REF by #TARGET_REF , direct result comparison with those reported is not possible since they were obtained on different sets.",
                "However, the result differences between overlapping steps in our experiments are consistent with those described in their paper.",
                "Thus, we can safely state that their approach can be improved by using five parallel CNNs, dependency-based word embeddings, and the summary representation of context information.",
                "Still, it does not perform as well as the RCNN-based approach in the same conditions."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Furthermore, our study has shown that their approach can be improved in many aspects.\n sent1: In the case of #TARGET_REF by #TARGET_REF , direct result comparison with those reported is not possible since they were obtained on different sets.\n sent2: However, the result differences between overlapping steps in our experiments are consistent with those described in their paper.\n sent3: Thus, we can safely state that their approach can be improved by using five parallel CNNs, dependency-based word embeddings, and the summary representation of context information.\n sent4: Still, it does not perform as well as the RCNN-based approach in the same conditions.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Fashioning a good story is an act of creativity and developing algorithms to replicate this has been a long running challenge.",
                "Adding pictures as input can provide information for guiding story construction by offering visual illustrations of the storyline.",
                "In the related task of image captioning, most methods try to generate descriptions only for individual images or for short videos depicting a single activity.",
                "Very recently, datasets have been introduced that extend this task to longer temporal sequences such as movies or photo albums (#REF; #REF; #REF; #TARGET_REF .",
                "The type of data we consider in this paper provides input illustrations for story generation in the form of photo albums, sampled over a few minutes to a few days of time."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Fashioning a good story is an act of creativity and developing algorithms to replicate this has been a long running challenge.\n sent1: Adding pictures as input can provide information for guiding story construction by offering visual illustrations of the storyline.\n sent2: In the related task of image captioning, most methods try to generate descriptions only for individual images or for short videos depicting a single activity.\n sent3: Very recently, datasets have been introduced that extend this task to longer temporal sequences such as movies or photo albums (#REF; #REF; #REF; #TARGET_REF .\n sent4: The type of data we consider in this paper provides input illustrations for story generation in the form of photo albums, sampled over a few minutes to a few days of time.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "To drive this work (Park and collected a dataset mined from Blog Posts.",
                "However, this kind of data often contains contextual information or loosely related language.",
                "A more direct dataset was recently released #TARGET_REF , where multi-sentence stories are collected describing photo albums via Amazon Mechanical Turk.",
                "In this paper, we make use of the Visual Storytelling Dataset (#REF) .",
                "While the authors provide a seq2seq baseline, they only deal with the task of generating stories given 5-representative (summary) photos hand-selected by people from an album."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: To drive this work (Park and collected a dataset mined from Blog Posts.\n sent1: However, this kind of data often contains contextual information or loosely related language.\n sent2: A more direct dataset was recently released #TARGET_REF , where multi-sentence stories are collected describing photo albums via Amazon Mechanical Turk.\n sent3: In this paper, we make use of the Visual Storytelling Dataset (#REF) .\n sent4: While the authors provide a seq2seq baseline, they only deal with the task of generating stories given 5-representative (summary) photos hand-selected by people from an album.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "While some approaches use unsupervised learning (#REF; #REF) or intuitive criteria to pick salient frames, recent models learn from human-created summaries (#REF; #REFb,a; #REF) .",
                "Recently, to better exploit semantics, (#REF) proposed textually customized summaries.",
                "Visual Storytelling: Visual storytelling tries to tell a coherent visual or textual story about an image set.",
                "Previous works include storyline graph modeling #REF), unsupervised mining (#REF) , blog-photo alignment , and language retelling #TARGET_REF; #REF) .",
                "While (Park and collects data by mining Blog Posts, (#REF) collects stories using Mechanical Turk, providing more directly relevant stories."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: While some approaches use unsupervised learning (#REF; #REF) or intuitive criteria to pick salient frames, recent models learn from human-created summaries (#REF; #REFb,a; #REF) .\n sent1: Recently, to better exploit semantics, (#REF) proposed textually customized summaries.\n sent2: Visual Storytelling: Visual storytelling tries to tell a coherent visual or textual story about an image set.\n sent3: Previous works include storyline graph modeling #REF), unsupervised mining (#REF) , blog-photo alignment , and language retelling #TARGET_REF; #REF) .\n sent4: While (Park and collects data by mining Blog Posts, (#REF) collects stories using Mechanical Turk, providing more directly relevant stories.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "While some approaches use unsupervised learning (#REF; #REF) or intuitive criteria to pick salient frames, recent models learn from human-created summaries (#REF; #REFb,a; #REF) .",
                "Recently, to better exploit semantics, (#REF) proposed textually customized summaries.",
                "Visual Storytelling: Visual storytelling tries to tell a coherent visual or textual story about an image set.",
                "Previous works include storyline graph modeling #REF), unsupervised mining (#REF) , blog-photo alignment , and language retelling (#REF; #REF) .",
                "While (Park and collects data by mining Blog Posts, #TARGET_REF collects stories using Mechanical Turk, providing more directly relevant stories."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: While some approaches use unsupervised learning (#REF; #REF) or intuitive criteria to pick salient frames, recent models learn from human-created summaries (#REF; #REFb,a; #REF) .\n sent1: Recently, to better exploit semantics, (#REF) proposed textually customized summaries.\n sent2: Visual Storytelling: Visual storytelling tries to tell a coherent visual or textual story about an image set.\n sent3: Previous works include storyline graph modeling #REF), unsupervised mining (#REF) , blog-photo alignment , and language retelling (#REF; #REF) .\n sent4: While (Park and collects data by mining Blog Posts, #TARGET_REF collects stories using Mechanical Turk, providing more directly relevant stories.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "However, this kind of data often contains contextual information or loosely related language.",
                "A more direct dataset was recently released (#REF) , where multi-sentence stories are collected describing photo albums via Amazon Mechanical Turk.",
                "In this paper, we make use of the Visual Storytelling Dataset #TARGET_REF .",
                "While the authors provide a seq2seq baseline, they only deal with the task of generating stories given 5-representative (summary) photos hand-selected by people from an album.",
                "Instead, we focus on the more challenging and realistic problem of end-toend generation of stories from entire albums."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: However, this kind of data often contains contextual information or loosely related language.\n sent1: A more direct dataset was recently released (#REF) , where multi-sentence stories are collected describing photo albums via Amazon Mechanical Turk.\n sent2: In this paper, we make use of the Visual Storytelling Dataset #TARGET_REF .\n sent3: While the authors provide a seq2seq baseline, they only deal with the task of generating stories given 5-representative (summary) photos hand-selected by people from an album.\n sent4: Instead, we focus on the more challenging and realistic problem of end-toend generation of stories from entire albums.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Given an album A = {a 1 , a 2 , ..., a n }, composed of a set of photos, we use a bi-directional RNN to encode the local album context for each photo.",
                "We first extract the 2048-dimensional visual representation f i ∈ R k for each photo using ResNet101 , then a bi-directional RNN is applied to encode the full album.",
                "Following #TARGET_REF , we choose a Gated Recurrent Unit (GRU) as the RNN unit to encode the photo sequence.",
                "The sequence output at each time step encodes the local album context for each photo (from both directions).",
                "Fused with the visual representation followed by ReLU, our final photo representation is (top module in Fig. 1 ):"
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Given an album A = {a 1 , a 2 , ..., a n }, composed of a set of photos, we use a bi-directional RNN to encode the local album context for each photo.\n sent1: We first extract the 2048-dimensional visual representation f i ∈ R k for each photo using ResNet101 , then a bi-directional RNN is applied to encode the full album.\n sent2: Following #TARGET_REF , we choose a Gated Recurrent Unit (GRU) as the RNN unit to encode the photo sequence.\n sent3: The sequence output at each time step encodes the local album context for each photo (from both directions).\n sent4: Fused with the visual representation followed by ReLU, our final photo representation is (top module in Fig. 1 ):\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "We use the Visual Storytelling Dataset #TARGET_REF , consisting of 10,000 albums with 200,000 photos.",
                "Each album contains 10-50 photos taken within a 48-hour span with two annotations: 1) 2 album summarizations, each with 5 selected representative photos, and 2) 5 stories describing the selected photos."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We use the Visual Storytelling Dataset #TARGET_REF , consisting of 10,000 albums with 200,000 photos.\n sent1: Each album contains 10-50 photos taken within a 48-hour span with two annotations: 1) 2 album summarizations, each with 5 selected representative photos, and 2) 5 stories describing the selected photos.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Following this property of finding relations or analogies, one popular example of gender bias is the word association between man to computer programmer as woman to homemaker (#REF) .",
                "Pre-trained word embeddings are used in many NLP downstream tasks, such as natural language inference (NLI), machine translation (MT) or question answering (QA).",
                "Recent progress in word embedding techniques has been achieved with contextualized word embeddings (#REF) which provide different vector representations for the same word in different contexts.",
                "While gender bias has been studied, detected and partially addressed for standard word embeddings techniques #TARGET_REF; #REFa; #REF) , it is not the case for the latest techniques of contextualized word embeddings.",
                "Only just recently, #REF present a first analysis on the topic based on the proposed methods in #REF ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Following this property of finding relations or analogies, one popular example of gender bias is the word association between man to computer programmer as woman to homemaker (#REF) .\n sent1: Pre-trained word embeddings are used in many NLP downstream tasks, such as natural language inference (NLI), machine translation (MT) or question answering (QA).\n sent2: Recent progress in word embedding techniques has been achieved with contextualized word embeddings (#REF) which provide different vector representations for the same word in different contexts.\n sent3: While gender bias has been studied, detected and partially addressed for standard word embeddings techniques #TARGET_REF; #REFa; #REF) , it is not the case for the latest techniques of contextualized word embeddings.\n sent4: Only just recently, #REF present a first analysis on the topic based on the proposed methods in #REF .\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Human-generated corpora suffer from social biases.",
                "Those biases are reflected in the cooccurrence statistics, and therefore learned into word embeddings trained in those corpora, amplifying them #TARGET_REF; #REF) .",
                "#REF studied from a geometrical point of view the presence of gender bias in word embeddings.",
                "For this, they compute the subspace where the gender information concentrates by computing the principal components of the difference of vector representations of male and female gender-defining word pairs.",
                "With the gender subspace, the authors identify direct and indirect biases in profession words."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Human-generated corpora suffer from social biases.\n sent1: Those biases are reflected in the cooccurrence statistics, and therefore learned into word embeddings trained in those corpora, amplifying them #TARGET_REF; #REF) .\n sent2: #REF studied from a geometrical point of view the presence of gender bias in word embeddings.\n sent3: For this, they compute the subspace where the gender information concentrates by computing the principal components of the difference of vector representations of male and female gender-defining word pairs.\n sent4: With the gender subspace, the authors identify direct and indirect biases in profession words.\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Zhao et al. (2018b) proposed an extension to GloVe embeddings (#REF) where the loss function used to train the embeddings is enriched with terms that confine the gender information to a specific portion of the embedded vector.",
                "The authors refer to these pieces of information as protected attributes.",
                "Once the embeddings are trained, the gender protected attribute can be simply removed from the vector representation, therefore eliminating any gender bias present in it.",
                "The transformations proposed by both #TARGET_REF and Zhao et al. (2018b) are downstream task-agnostic.",
                "This fact is used in the work of #REF to showcase that, while apparently the embedding information is removed, there is still gender information remaining in the vector representations."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Zhao et al. (2018b) proposed an extension to GloVe embeddings (#REF) where the loss function used to train the embeddings is enriched with terms that confine the gender information to a specific portion of the embedded vector.\n sent1: The authors refer to these pieces of information as protected attributes.\n sent2: Once the embeddings are trained, the gender protected attribute can be simply removed from the vector representation, therefore eliminating any gender bias present in it.\n sent3: The transformations proposed by both #TARGET_REF and Zhao et al. (2018b) are downstream task-agnostic.\n sent4: This fact is used in the work of #REF to showcase that, while apparently the embedding information is removed, there is still gender information remaining in the vector representations.\n",
        "output": "{\"label\": [\"MOTIVATION\"], \"context\": [\"sent2\", \"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Given the high impact of contextualized word embeddings in the area of NLP and the social consequences of having biases in such embeddings, in this work we analyse the presence of bias in these contextualized word embeddings.",
                "In particular, we focus on gender biases, and specifically on the following questions:",
                "• Do contextualized word embeddings exhibit gender bias and how does this bias compare to standard and debiased word embeddings?",
                "• Do different evaluation techniques identify bias similarly and what would be the best measure to use for gender bias detection in contextualized embeddings?",
                "To address these questions, we adapt and contrast with the evaluation measures proposed by #TARGET_REF and #REF ."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Given the high impact of contextualized word embeddings in the area of NLP and the social consequences of having biases in such embeddings, in this work we analyse the presence of bias in these contextualized word embeddings.\n sent1: In particular, we focus on gender biases, and specifically on the following questions:\n sent2: • Do contextualized word embeddings exhibit gender bias and how does this bias compare to standard and debiased word embeddings?\n sent3: • Do different evaluation techniques identify bias similarly and what would be the best measure to use for gender bias detection in contextualized embeddings?\n sent4: To address these questions, we adapt and contrast with the evaluation measures proposed by #TARGET_REF and #REF .\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "where N is the amount of gender neutral words, g the gender direction, and w the word vector of each profession.",
                "We got direct bias of 0.03, compared to 0.08 from standard word2vec embeddings described in #TARGET_REF .",
                "This reduction on the direct bias confirms that the substantial component along the gender direction that is present in standard word embeddings is less for the contextualized word embeddings.",
                "Probably, this reduction comes from the fact that we are using different word embeddings for the same profession depending on the sentence which is a direct consequence and advantage of using contextualized embeddings.",
                "Male and female-biased words clustering."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: where N is the amount of gender neutral words, g the gender direction, and w the word vector of each profession.\n sent1: We got direct bias of 0.03, compared to 0.08 from standard word2vec embeddings described in #TARGET_REF .\n sent2: This reduction on the direct bias confirms that the substantial component along the gender direction that is present in standard word embeddings is less for the contextualized word embeddings.\n sent3: Probably, this reduction comes from the fact that we are using different word embeddings for the same profession depending on the sentence which is a direct consequence and advantage of using contextualized embeddings.\n sent4: Male and female-biased words clustering.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent1\", \"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "We refer to the list of female and male professions 3 as 'Professional List' (e.g. accountant, surgeon).",
                "The 'Biased List' is the list used in the clustering experiment and it consists of biased male and female words (500 female biased tokens and 500 male biased token).",
                "This list is generated by taking the most biased words, where the bias of a word is computed by taking its projection on the gender direction ( − → he-−→ she) (e.g. breastfeeding, bridal and diet for female and hero, cigar and teammates for male).",
                "The 'Extended Biased List' is the list used in classification experiment , which contains 5000 male and female biased tokens, 2500 for each gender, generated in the same way of the Biased List 4 .",
                "A note to be considered, is that the lists we used in our experiments (and obtained from #TARGET_REF and #REF ) may contain words that are missing in our corpus and so we can not obtain contextualized embeddings for them."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We refer to the list of female and male professions 3 as 'Professional List' (e.g. accountant, surgeon).\n sent1: The 'Biased List' is the list used in the clustering experiment and it consists of biased male and female words (500 female biased tokens and 500 male biased token).\n sent2: This list is generated by taking the most biased words, where the bias of a word is computed by taking its projection on the gender direction ( − → he-−→ she) (e.g. breastfeeding, bridal and diet for female and hero, cigar and teammates for male).\n sent3: The 'Extended Biased List' is the list used in classification experiment , which contains 5000 male and female biased tokens, 2500 for each gender, generated in the same way of the Biased List 4 .\n sent4: A note to be considered, is that the lists we used in our experiments (and obtained from #TARGET_REF and #REF ) may contain words that are missing in our corpus and so we can not obtain contextualized embeddings for them.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "There is no standard measure for gender bias, and even less for such the recently proposed contextualized word embeddings.",
                "In this section, we adapt gender bias measures for word embedding methods from previous work #TARGET_REF and (#REF) to be applicable to contextualized word embeddings.",
                "This way, we first compute the gender subspace from the ELMo vector representations of genderdefining words, then identify the presence of direct bias in the contextualized representations.",
                "We then proceed to identify gender information by means of clustering and classifications techniques.",
                "We compare our results to previous results from debiased and non-debiased word embeddings (#REF) ."
            ],
            "label": [
                "EXTENDS"
            ]
        },
        "input": "sent0: There is no standard measure for gender bias, and even less for such the recently proposed contextualized word embeddings.\n sent1: In this section, we adapt gender bias measures for word embedding methods from previous work #TARGET_REF and (#REF) to be applicable to contextualized word embeddings.\n sent2: This way, we first compute the gender subspace from the ELMo vector representations of genderdefining words, then identify the presence of direct bias in the contextualized representations.\n sent3: We then proceed to identify gender information by means of clustering and classifications techniques.\n sent4: We compare our results to previous results from debiased and non-debiased word embeddings (#REF) .\n",
        "output": "{\"label\": [\"EXTENDS\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "On the set of difference vectors, we compute their principal components to verify the presence of bias.",
                "In order to have a reference, we computed the principal components of representation of random words.",
                "Similarly to #TARGET_REF , figure 1 shows that the first eigenvalue is significantly larger than the rest and that there is also a single direction describing the majority of variance in these vectors, still the difference between the percentage of variances is less in case of contextualized embeddings, which may refer that there is less bias in such embeddings.",
                "We can easily note the difference in the case of random, where there is a smooth and gradual decrease in eigenvalues, and hence the variance percentage.",
                "A similar conclusion was stated in the recent work (#REF) where the authors applied the same approach, but for gender swapped variants of sentences with professions."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: On the set of difference vectors, we compute their principal components to verify the presence of bias.\n sent1: In order to have a reference, we computed the principal components of representation of random words.\n sent2: Similarly to #TARGET_REF , figure 1 shows that the first eigenvalue is significantly larger than the rest and that there is also a single direction describing the majority of variance in these vectors, still the difference between the percentage of variances is less in case of contextualized embeddings, which may refer that there is less bias in such embeddings.\n sent3: We can easily note the difference in the case of random, where there is a smooth and gradual decrease in eigenvalues, and hence the variance percentage.\n sent4: A similar conclusion was stated in the recent work (#REF) where the authors applied the same approach, but for gender swapped variants of sentences with professions.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Direct Bias Direct Bias is a measure of how close a certain set of words are to the gender vector.",
                "To compute it, we extracted from the training data the sentences that contain words in the Professional List.",
                "We excluded the sentences that have both a professional token and definitional gender word to avoid the influence of the latter over the presence of bias in the former.",
                "We applied the definition of direct bias from #TARGET_REF on the ELMo representations of the professional words in these sentences.",
                "where N is the amount of gender neutral words, g the gender direction, and w the word vector of each profession."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: Direct Bias Direct Bias is a measure of how close a certain set of words are to the gender vector.\n sent1: To compute it, we extracted from the training data the sentences that contain words in the Professional List.\n sent2: We excluded the sentences that have both a professional token and definitional gender word to avoid the influence of the latter over the presence of bias in the former.\n sent3: We applied the definition of direct bias from #TARGET_REF on the ELMo representations of the professional words in these sentences.\n sent4: where N is the amount of gender neutral words, g the gender direction, and w the word vector of each profession.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "In this paper, we limit ourselves to word-by-word normalization, ignoring the syntactic differences between modern-day languages and their historic predecessors.",
                "Resources for historical text normalization are scarce.",
                "Even for major languages like English and German, we have very little training data for inducing normalization models, and the models we induce may be very specific to these datasets and not scale to writings from other historic periodsor even just writings from another monastery or by another author.",
                "Bollmann and Søgaard (2016) and #TARGET_REF recently showed that we can obtain more robust historical text normalization models by exploiting synergies across historical text normalization datasets and with related tasks.",
                "Specifically, #REF showed that multitask learning with German grapheme-to-phoneme translation as an auxiliary task improves a stateof-the-art sequence-to-sequence model for historical text normalization of medieval German manuscripts."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In this paper, we limit ourselves to word-by-word normalization, ignoring the syntactic differences between modern-day languages and their historic predecessors.\n sent1: Resources for historical text normalization are scarce.\n sent2: Even for major languages like English and German, we have very little training data for inducing normalization models, and the models we induce may be very specific to these datasets and not scale to writings from other historic periodsor even just writings from another monastery or by another author.\n sent3: Bollmann and Søgaard (2016) and #TARGET_REF recently showed that we can obtain more robust historical text normalization models by exploiting synergies across historical text normalization datasets and with related tasks.\n sent4: Specifically, #REF showed that multitask learning with German grapheme-to-phoneme translation as an auxiliary task improves a stateof-the-art sequence-to-sequence model for historical text normalization of medieval German manuscripts.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "In this paper, we limit ourselves to word-by-word normalization, ignoring the syntactic differences between modern-day languages and their historic predecessors.",
                "Resources for historical text normalization are scarce.",
                "Even for major languages like English and German, we have very little training data for inducing normalization models, and the models we induce may be very specific to these datasets and not scale to writings from other historic periodsor even just writings from another monastery or by another author.",
                "Bollmann and Søgaard (2016) and #REF recently showed that we can obtain more robust historical text normalization models by exploiting synergies across historical text normalization datasets and with related tasks.",
                "Specifically, #TARGET_REF showed that multitask learning with German grapheme-to-phoneme translation as an auxiliary task improves a stateof-the-art sequence-to-sequence model for historical text normalization of medieval German manuscripts."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In this paper, we limit ourselves to word-by-word normalization, ignoring the syntactic differences between modern-day languages and their historic predecessors.\n sent1: Resources for historical text normalization are scarce.\n sent2: Even for major languages like English and German, we have very little training data for inducing normalization models, and the models we induce may be very specific to these datasets and not scale to writings from other historic periodsor even just writings from another monastery or by another author.\n sent3: Bollmann and Søgaard (2016) and #REF recently showed that we can obtain more robust historical text normalization models by exploiting synergies across historical text normalization datasets and with related tasks.\n sent4: Specifically, #TARGET_REF showed that multitask learning with German grapheme-to-phoneme translation as an auxiliary task improves a stateof-the-art sequence-to-sequence model for historical text normalization of medieval German manuscripts.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Training is done on mini-batches of 50 samples with early stopping based on validation on the individual development sets.",
                "The hyperparameters were set on a randomly selected subset of 50,000 tokens from each of the following datasets: English, German (Anselm), Hungarian, Icelandic, and Slovene (Gaj).",
                "#TARGET_REF also describe a multi-task learning (MTL) scenario where the encoder-decoder model is trained on two datasets in parallel.",
                "We perform similar experiments on pairwise combinations of our datasets.",
                "Table 2 : Examples of input tokens (first line) and reference normalization (second line) for each of the historical datasets."
            ],
            "label": [
                "BACKGROUND",
                "SIMILARITY"
            ]
        },
        "input": "sent0: Training is done on mini-batches of 50 samples with early stopping based on validation on the individual development sets.\n sent1: The hyperparameters were set on a randomly selected subset of 50,000 tokens from each of the following datasets: English, German (Anselm), Hungarian, Icelandic, and Slovene (Gaj).\n sent2: #TARGET_REF also describe a multi-task learning (MTL) scenario where the encoder-decoder model is trained on two datasets in parallel.\n sent3: We perform similar experiments on pairwise combinations of our datasets.\n sent4: Table 2 : Examples of input tokens (first line) and reference normalization (second line) for each of the historical datasets.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"SIMILARITY\"], \"context\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "We study when multi-task learning leads to improvements in historical text normalization.",
                "Specifically, we evaluate a state-ofthe-art approach to historical text normalization #TARGET_REF with and without various auxiliary tasks, across 10 historical text normalization datasets.",
                "We also include an experiment in English historical text normalization using data from Twitter and a grammatical error correction corpus (FCE) as auxiliary datasets.",
                "Across the board, we find that, unlike what has been observed for other NLP tasks, multi-task learning only helps when target task data is scarce."
            ],
            "label": [
                "USE",
                "MOTIVATION"
            ]
        },
        "input": "sent0: We study when multi-task learning leads to improvements in historical text normalization.\n sent1: Specifically, we evaluate a state-ofthe-art approach to historical text normalization #TARGET_REF with and without various auxiliary tasks, across 10 historical text normalization datasets.\n sent2: We also include an experiment in English historical text normalization using data from Twitter and a grammatical error correction corpus (FCE) as auxiliary datasets.\n sent3: Across the board, we find that, unlike what has been observed for other NLP tasks, multi-task learning only helps when target task data is scarce.\n",
        "output": "{\"label\": [\"USE\", \"MOTIVATION\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "We consider 10 datasets from 8 different languages: German, using the #TARGET_REF (taken from #TARGET_REF) and texts from the RIDGES corpus (#REF) #TARGET_REF to obtain a single dataset.",
                "For RIDGES, we use 16 texts and randomly sample 70% of all sentences from each text for the training set, and 15% for the dev/test sets.",
                "The Spanish and Portuguese datasets consist of manually normalized subsets of the Post Scriptum corpus; here, we randomly sample 80% (train) and 10% (dev/test) of all sentences per century represented in the corpus.",
                "Dataset splits for the other languages are taken from #REF and Ljubešić et al. (2016) .",
                "We preprocessed all datasets to remove punctuation, perform Unicode normalization, replace digits that do not require normalization with a dummy symbol, and lowercase all tokens."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We consider 10 datasets from 8 different languages: German, using the #TARGET_REF (taken from #TARGET_REF) and texts from the RIDGES corpus (#REF) #TARGET_REF to obtain a single dataset.\n sent1: For RIDGES, we use 16 texts and randomly sample 70% of all sentences from each text for the training set, and 15% for the dev/test sets.\n sent2: The Spanish and Portuguese datasets consist of manually normalized subsets of the Post Scriptum corpus; here, we randomly sample 80% (train) and 10% (dev/test) of all sentences per century represented in the corpus.\n sent3: Dataset splits for the other languages are taken from #REF and Ljubešić et al. (2016) .\n sent4: We preprocessed all datasets to remove punctuation, perform Unicode normalization, replace digits that do not require normalization with a dummy symbol, and lowercase all tokens.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "Model We use the same encoder-decoder architecture with attention as described in #TARGET_REF .",
                "4 This is a fairly standard model consisting of one bidirectional LSTM unit in the encoder and one (unidirectional) LSTM unit in the decoder.",
                "The input for the encoder is a single historical word form represented as a sequence of characters and padded with word boundary symbols; i.e., we only input single tokens in isolation, not full sentences.",
                "The decoder attends over the encoder's outputs and generates the normalized output characters."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Model We use the same encoder-decoder architecture with attention as described in #TARGET_REF .\n sent1: 4 This is a fairly standard model consisting of one bidirectional LSTM unit in the encoder and one (unidirectional) LSTM unit in the decoder.\n sent2: The input for the encoder is a single historical word form represented as a sequence of characters and padded with word boundary symbols; i.e., we only input single tokens in isolation, not full sentences.\n sent3: The decoder attends over the encoder's outputs and generates the normalized output characters.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "We use the Adam optimizer (#REF) with a character-wise cross-entropy loss.",
                "Training is done on mini-batches of 50 samples with early stopping based on validation on the individual development sets.",
                "The hyperparameters were set on a randomly selected subset of 50,000 tokens from each of the following datasets: English, German ( #TARGET_REF), Hungarian, Icelandic, and Slovene (Gaj).",
                "#REF also describe a multi-task learning (MTL) scenario where the encoder-decoder model is trained on two datasets in parallel.",
                "We perform similar experiments on pairwise combinations of our datasets."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We use the Adam optimizer (#REF) with a character-wise cross-entropy loss.\n sent1: Training is done on mini-batches of 50 samples with early stopping based on validation on the individual development sets.\n sent2: The hyperparameters were set on a randomly selected subset of 50,000 tokens from each of the following datasets: English, German ( #TARGET_REF), Hungarian, Icelandic, and Slovene (Gaj).\n sent3: #REF also describe a multi-task learning (MTL) scenario where the encoder-decoder model is trained on two datasets in parallel.\n sent4: We perform similar experiments on pairwise combinations of our datasets.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "There has been considerable work on multitask sequence-to-sequence models for other tasks (#REF; #REF; Elliott and Kádár, 2017) .",
                "There is a wide range of design questions and sharing strategies that we ignore here, focusing instead on under what circumstances the approach advocated in #TARGET_REF works.",
                "Our main observation-that the size of the target dataset is most predictive of multi-task learning gains-runs counter previous findings for other NLP tasks (Martínez #REF; Bingel and Søgaard, 2017) .",
                "Martínez #REF find that the label entropy of the auxiliary dataset is more predictive; Bingel and Sø-gaard (2017) find that the relative differences in the steepness of the two single-task loss curves is more predictive.",
                "Both papers consider sequence tagging problems with a small number of labels; and it is probably not a surprise that their findings do not seem to scale to the case of historical text normalization."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: There has been considerable work on multitask sequence-to-sequence models for other tasks (#REF; #REF; Elliott and Kádár, 2017) .\n sent1: There is a wide range of design questions and sharing strategies that we ignore here, focusing instead on under what circumstances the approach advocated in #TARGET_REF works.\n sent2: Our main observation-that the size of the target dataset is most predictive of multi-task learning gains-runs counter previous findings for other NLP tasks (Martínez #REF; Bingel and Søgaard, 2017) .\n sent3: Martínez #REF find that the label entropy of the auxiliary dataset is more predictive; Bingel and Sø-gaard (2017) find that the relative differences in the steepness of the two single-task loss curves is more predictive.\n sent4: Both papers consider sequence tagging problems with a small number of labels; and it is probably not a surprise that their findings do not seem to scale to the case of historical text normalization.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Text available in the social media are typically written in a more casual style, are opinionated and speculative (#REF) .",
                "Because of this, techniques developed for formal texts, such as news articles, often do not behave as well when dealing with informal documents.",
                "In particular, news articles are more uniform in style and structure; whereas blogs often do not exhibit a stereotypical discourse structure.",
                "As a result, for blogs, it is usually more difficult to identify and rank relevant units for summarization compared to news articles.",
                "Several work have shown that discourse relations can improve the results of summarization in the case of factual texts or news articles (e.g. #TARGET_REF )."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Text available in the social media are typically written in a more casual style, are opinionated and speculative (#REF) .\n sent1: Because of this, techniques developed for formal texts, such as news articles, often do not behave as well when dealing with informal documents.\n sent2: In particular, news articles are more uniform in style and structure; whereas blogs often do not exhibit a stereotypical discourse structure.\n sent3: As a result, for blogs, it is usually more difficult to identify and rank relevant units for summarization compared to news articles.\n sent4: Several work have shown that discourse relations can improve the results of summarization in the case of factual texts or news articles (e.g. #TARGET_REF ).\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "The use of discourse relations for text summarization is not new.",
                "Most notably, (#REF) used discourse relations for single document summarization and proposed a discourse relation identification parsing algorithm.",
                "In some work (e.g. (#REF; #REF) ), discourse relations have been exploited successfully for multi-document summarization.",
                "In particular, #TARGET_REF experimentally showed that discourse relations can improve the coherence of multi-document summaries.",
                "(#REF) showed how discourse relations can be used effectively to incorporate additional contextual information for a given question in a query-based summarization."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The use of discourse relations for text summarization is not new.\n sent1: Most notably, (#REF) used discourse relations for single document summarization and proposed a discourse relation identification parsing algorithm.\n sent2: In some work (e.g. (#REF; #REF) ), discourse relations have been exploited successfully for multi-document summarization.\n sent3: In particular, #TARGET_REF experimentally showed that discourse relations can improve the coherence of multi-document summaries.\n sent4: (#REF) showed how discourse relations can be used effectively to incorporate additional contextual information for a given question in a query-based summarization.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "In building our relation taxonomy, we considered all main discourse relations listed in the taxonomy of Mann and Thompson's Rhetorical Structure Theory (RST) (#REF) .",
                "These discourse relations are also considered in Grimes' (#REF) and Williams' predicate lists.",
                "From our corpus analysis, we have identified the six most prevalent discourse relations in this blog dataset, namely comparison, contingency, illustration, attribution, topic-opinion, and attributive.",
                "The comparison, contingency, and illustration relations are also considered by most of the work in the field of discourse analysis such as the PDTB: Penn Discourse TreeBank research group #TARGET_REF and the RST Discourse Treebank research group (#REF ).",
                "We considered three additional classes of relations: attributive, attribution, and topic-opinion."
            ],
            "label": [
                "BACKGROUND",
                "SIMILARITY",
                "EXTENDS",
                "DIFFERENCES"
            ]
        },
        "input": "sent0: In building our relation taxonomy, we considered all main discourse relations listed in the taxonomy of Mann and Thompson's Rhetorical Structure Theory (RST) (#REF) .\n sent1: These discourse relations are also considered in Grimes' (#REF) and Williams' predicate lists.\n sent2: From our corpus analysis, we have identified the six most prevalent discourse relations in this blog dataset, namely comparison, contingency, illustration, attribution, topic-opinion, and attributive.\n sent3: The comparison, contingency, and illustration relations are also considered by most of the work in the field of discourse analysis such as the PDTB: Penn Discourse TreeBank research group #TARGET_REF and the RST Discourse Treebank research group (#REF ).\n sent4: We considered three additional classes of relations: attributive, attribution, and topic-opinion.\n",
        "output": "{\"label\": [\"BACKGROUND\", \"SIMILARITY\", \"EXTENDS\", \"DIFFERENCES\"], \"context\": [\"sent2\", \"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "These discourse relations are summarized in Figure 1 while a description of these relations is given below.",
                "Illustration: Is used to provide additional information or detail about a situation.",
                "For example: \"Allied Capital is a closed-end management investment company that will operate as a business development concern.\" As shown in Figure 1 , illustration relations can be sub-divided into sub-categories: joint, list, disjoint, and elaboration relations according to the RST Discourse Treebank (#REF ) and the Penn Discourse TreeBank #TARGET_REF .",
                "Contingency: Provides cause, condition, reason or evidence for a situation, result or claim.",
                "For example: \"The meat is good because they slice it right in front of you.\""
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: These discourse relations are summarized in Figure 1 while a description of these relations is given below.\n sent1: Illustration: Is used to provide additional information or detail about a situation.\n sent2: For example: \"Allied Capital is a closed-end management investment company that will operate as a business development concern.\" As shown in Figure 1 , illustration relations can be sub-divided into sub-categories: joint, list, disjoint, and elaboration relations according to the RST Discourse Treebank (#REF ) and the Penn Discourse TreeBank #TARGET_REF .\n sent3: Contingency: Provides cause, condition, reason or evidence for a situation, result or claim.\n sent4: For example: \"The meat is good because they slice it right in front of you.\"\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "For example: \"Allied Capital is a closed-end management investment company that will operate as a business development concern.\" As shown in Figure 1 , illustration relations can be sub-divided into sub-categories: joint, list, disjoint, and elaboration relations according to the RST Discourse Treebank (#REF ) and the Penn Discourse TreeBank (#REF) .",
                "Contingency: Provides cause, condition, reason or evidence for a situation, result or claim.",
                "For example: \"The meat is good because they slice it right in front of you.\"",
                "As shown in Figure 1 , the contingency relation subsumes several more specific relations: explanation, evidence, reason, cause, result, consequence, background, condition, hypothetical, enablement, and purpose relations according to the Penn Discourse TreeBank #TARGET_REF .",
                "Comparison: Gives a comparison and contrast among different situations."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For example: \"Allied Capital is a closed-end management investment company that will operate as a business development concern.\" As shown in Figure 1 , illustration relations can be sub-divided into sub-categories: joint, list, disjoint, and elaboration relations according to the RST Discourse Treebank (#REF ) and the Penn Discourse TreeBank (#REF) .\n sent1: Contingency: Provides cause, condition, reason or evidence for a situation, result or claim.\n sent2: For example: \"The meat is good because they slice it right in front of you.\"\n sent3: As shown in Figure 1 , the contingency relation subsumes several more specific relations: explanation, evidence, reason, cause, result, consequence, background, condition, hypothetical, enablement, and purpose relations according to the Penn Discourse TreeBank #TARGET_REF .\n sent4: Comparison: Gives a comparison and contrast among different situations.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "As shown in Figure 1 , the contingency relation subsumes several more specific relations: explanation, evidence, reason, cause, result, consequence, background, condition, hypothetical, enablement, and purpose relations according to the Penn Discourse TreeBank (#REF) .",
                "Comparison: Gives a comparison and contrast among different situations.",
                "For example, \"Its fastforward and rewind work much more smoothly and consistently than those of other models I've had.\"",
                "The comparison relation subsumes the contrast relation according to the Penn Discourse TreeBank #TARGET_REF and the analogy and preference relations according to the RST Discourse Treebank (#REF) .",
                "Attributive: Relation provides details about an entity or an event -e.g."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: As shown in Figure 1 , the contingency relation subsumes several more specific relations: explanation, evidence, reason, cause, result, consequence, background, condition, hypothetical, enablement, and purpose relations according to the Penn Discourse TreeBank (#REF) .\n sent1: Comparison: Gives a comparison and contrast among different situations.\n sent2: For example, \"Its fastforward and rewind work much more smoothly and consistently than those of other models I've had.\"\n sent3: The comparison relation subsumes the contrast relation according to the Penn Discourse TreeBank #TARGET_REF and the analogy and preference relations according to the RST Discourse Treebank (#REF) .\n sent4: Attributive: Relation provides details about an entity or an event -e.g.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "However, at the time this research was done, the only publicly available discourse parser was SPADE (#REF) which operates on individual sentences.",
                "To identify illustration, contingency, comparison, and attribution relations, we have used SPADE discourse parser.",
                "However, we have complemented this parser with three other approaches: (#REF )'s approach is used to identify intra-sentence comparison relations; we have designed a tagger based on (#REF) 's approach to identify topic-opinion relations; and we have proposed a new approach to tag attributive relations (#REF) .",
                "A description and evaluation of these approaches can be found in #TARGET_REF .",
                "By combining these approaches, a sentence is tagged with all possible discourse relations that it contains."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: However, at the time this research was done, the only publicly available discourse parser was SPADE (#REF) which operates on individual sentences.\n sent1: To identify illustration, contingency, comparison, and attribution relations, we have used SPADE discourse parser.\n sent2: However, we have complemented this parser with three other approaches: (#REF )'s approach is used to identify intra-sentence comparison relations; we have designed a tagger based on (#REF) 's approach to identify topic-opinion relations; and we have proposed a new approach to tag attributive relations (#REF) .\n sent3: A description and evaluation of these approaches can be found in #TARGET_REF .\n sent4: By combining these approaches, a sentence is tagged with all possible discourse relations that it contains.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Each schema is designed to give priority to its associated question type and subjective sentences as summaries for opinionated texts are generated.",
                "Each schema specifies the types of discourse relations and the order in which they should appear in the output summary for a particular question type.",
                "Figure 2 shows a sample schema that is used to answer reason questions (e.g. \"Why do people like Picasa?\").",
                "According to this schema 8 , one or more sentences containing a topic-opinion or attribution relation followed by zero or many sentences containing a contingency or comparison relation followed by zero or many sentences containing a attributive relation should be used.",
                "Finally the most appropriate schema is selected based on a given question type; and candidate sentences fill particular slots in the selected schema based on which discourse relations they contain in order to create the final summary (details of BlogSum can be found in #TARGET_REF )."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Each schema is designed to give priority to its associated question type and subjective sentences as summaries for opinionated texts are generated.\n sent1: Each schema specifies the types of discourse relations and the order in which they should appear in the output summary for a particular question type.\n sent2: Figure 2 shows a sample schema that is used to answer reason questions (e.g. \"Why do people like Picasa?\").\n sent3: According to this schema 8 , one or more sentences containing a topic-opinion or attribution relation followed by zero or many sentences containing a contingency or comparison relation followed by zero or many sentences containing a attributive relation should be used.\n sent4: Finally the most appropriate schema is selected based on a given question type; and candidate sentences fill particular slots in the selected schema based on which discourse relations they contain in order to create the final summary (details of BlogSum can be found in #TARGET_REF ).\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "However, at the time this research was done, the only publicly available discourse parser was SPADE (#REF) which operates on individual sentences.",
                "To identify illustration, contingency, comparison, and attribution relations, we have used SPADE discourse parser.",
                "However, we have complemented this parser with three other approaches: (#REF )'s approach is used to identify intra-sentence comparison relations; we have designed a tagger based on (#REF) 's approach to identify topic-opinion relations; and we have proposed a new approach to tag attributive relations #TARGET_REF .",
                "A description and evaluation of these approaches can be found in (#REF) .",
                "By combining these approaches, a sentence is tagged with all possible discourse relations that it contains."
            ],
            "label": [
                "EXTENDS",
                "USE"
            ]
        },
        "input": "sent0: However, at the time this research was done, the only publicly available discourse parser was SPADE (#REF) which operates on individual sentences.\n sent1: To identify illustration, contingency, comparison, and attribution relations, we have used SPADE discourse parser.\n sent2: However, we have complemented this parser with three other approaches: (#REF )'s approach is used to identify intra-sentence comparison relations; we have designed a tagger based on (#REF) 's approach to identify topic-opinion relations; and we have proposed a new approach to tag attributive relations #TARGET_REF .\n sent3: A description and evaluation of these approaches can be found in (#REF) .\n sent4: By combining these approaches, a sentence is tagged with all possible discourse relations that it contains.\n",
        "output": "{\"label\": [\"EXTENDS\", \"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "To measure the usefulness of discourse relations for the summarization of informal texts, we have tested the effect of each relation with four different summarizers: BlogSum #TARGET_REF , MEAD #TARGET_REF , the best scoring system at TAC 2008 5 and the best scoring system at DUC 2007 6 .",
                "We have evaluated the effect of each discourse relation on the summaries generated and compared the results.",
                "Let us first describe the BlogSum summarizer."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: To measure the usefulness of discourse relations for the summarization of informal texts, we have tested the effect of each relation with four different summarizers: BlogSum #TARGET_REF , MEAD #TARGET_REF , the best scoring system at TAC 2008 5 and the best scoring system at DUC 2007 6 .\n sent1: We have evaluated the effect of each discourse relation on the summaries generated and compared the results.\n sent2: Let us first describe the BlogSum summarizer.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                "This result indicates that the use of discourse relations as a whole helps to include more question relevant sentences and improve the summary content.",
                "To ensure that the results were not specific to our summarizer, we performed the same experiments with two other systems: the MEAD summarizer #TARGET_REF , a publicly available and a widely used summarizer, and with the output of the TAC best-scoring system.",
                "For MEAD, we first generated candidate sentences using MEAD, then these candidate sentences were tagged using discourse relation taggers used under BlogSum.",
                "Then these tagged sentences were filtered using BlogSum so that no sentence with a specific relation is used in summary generation for a particular experiment.",
                "We have calculated ROUGE scores using the original candidate sentences generated by MEAD and also using the filtered candidate sentences."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This result indicates that the use of discourse relations as a whole helps to include more question relevant sentences and improve the summary content.\n sent1: To ensure that the results were not specific to our summarizer, we performed the same experiments with two other systems: the MEAD summarizer #TARGET_REF , a publicly available and a widely used summarizer, and with the output of the TAC best-scoring system.\n sent2: For MEAD, we first generated candidate sentences using MEAD, then these candidate sentences were tagged using discourse relation taggers used under BlogSum.\n sent3: Then these tagged sentences were filtered using BlogSum so that no sentence with a specific relation is used in summary generation for a particular experiment.\n sent4: We have calculated ROUGE scores using the original candidate sentences generated by MEAD and also using the filtered candidate sentences.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "It can be applied in many areas, like word sense disambiguation, information retrieval, information extraction and others.",
                "It has long history of improvements, starting with simple models, like bag-of-words (often weighted by TF-IDF score), continuing with more complex ones, like LSA [6] , which attempts to find \"latent\" meanings of words and phrases, and even more abstract models, like NNLM [3] .",
                "Latest results are based on neural network experience, but are far more simple: various versions of Word2Vec, Skip-gram and CBOW models [8] , which currently show the State-of-the-Art results and have proven success with morphologically complex languages like #REF , #TARGET_REF .",
                "These are corpus-based approaches, where one computes or trains the model from a large corpus.",
                "They usually consider some word context, like in bag-ofwords, where model is simple count of how often can some word be seen in context of a word being described."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: It can be applied in many areas, like word sense disambiguation, information retrieval, information extraction and others.\n sent1: It has long history of improvements, starting with simple models, like bag-of-words (often weighted by TF-IDF score), continuing with more complex ones, like LSA [6] , which attempts to find \"latent\" meanings of words and phrases, and even more abstract models, like NNLM [3] .\n sent2: Latest results are based on neural network experience, but are far more simple: various versions of Word2Vec, Skip-gram and CBOW models [8] , which currently show the State-of-the-Art results and have proven success with morphologically complex languages like #REF , #TARGET_REF .\n sent3: These are corpus-based approaches, where one computes or trains the model from a large corpus.\n sent4: They usually consider some word context, like in bag-ofwords, where model is simple count of how often can some word be seen in context of a word being described.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "The primary goal of this paper is to prove usefulness of Russian Twitter stream as word semantic similarity resource.",
                "Twitter is a popular social network 1 , or also called \"microblogging service\", which enables users to share and interact with short messages instantly and publicly (although private accounts are also available).",
                "Users all over the world generate hundreds of millions of tweets per day, all over the world, in many languages, generating enormous amount of verbal data.",
                "Traditional corpora for the word semantic similarity task are News, Wikipedia, electronic libraries and others (e.g. RUSSE workshop #TARGET_REF ).",
                "It was shown that type of corpus used for training affects the resulting accuracy."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The primary goal of this paper is to prove usefulness of Russian Twitter stream as word semantic similarity resource.\n sent1: Twitter is a popular social network 1 , or also called \"microblogging service\", which enables users to share and interact with short messages instantly and publicly (although private accounts are also available).\n sent2: Users all over the world generate hundreds of millions of tweets per day, all over the world, in many languages, generating enormous amount of verbal data.\n sent3: Traditional corpora for the word semantic similarity task are News, Wikipedia, electronic libraries and others (e.g. RUSSE workshop #TARGET_REF ).\n sent4: It was shown that type of corpus used for training affects the resulting accuracy.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Until recent days there was no such dataset for Russian language.",
                "To mitigate this the \"RUSSE: The First Workshop on Russian Semantic Similarity\" #TARGET_REF was conducted, producing RUSSE Human-Judgements evaluation dataset (we will refer to it as HJ-dataset).",
                "RUSSE dataset was constructed the following way.",
                "Firstly, datasets WordSim353, MC [9] and RG-65 were combined and translated.",
                "Then human judgements were obtained by crowdsourcing (using custom implementation)."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Until recent days there was no such dataset for Russian language.\n sent1: To mitigate this the \"RUSSE: The First Workshop on Russian Semantic Similarity\" #TARGET_REF was conducted, producing RUSSE Human-Judgements evaluation dataset (we will refer to it as HJ-dataset).\n sent2: RUSSE dataset was constructed the following way.\n sent3: Firstly, datasets WordSim353, MC [9] and RG-65 were combined and translated.\n sent4: Then human judgements were obtained by crowdsourcing (using custom implementation).\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Firstly, datasets WordSim353, MC [9] and RG-65 were combined and translated.",
                "Then human judgements were obtained by crowdsourcing (using custom implementation).",
                "Final size of the dataset is 333 word pairs, it is available online 5 .",
                "The RUSSE contest was followed by paper from its organizers #TARGET_REF and several participators [1] , [5] , thus filling the gap in word semantic similarity task for Russian language.",
                "In this paper we evaluate a Word2Vec model, trained on Russian Twitter corpus against RUSSE HJ-dataset, and show results comparable to top results of other RUSSE competitors."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Firstly, datasets WordSim353, MC [9] and RG-65 were combined and translated.\n sent1: Then human judgements were obtained by crowdsourcing (using custom implementation).\n sent2: Final size of the dataset is 333 word pairs, it is available online 5 .\n sent3: The RUSSE contest was followed by paper from its organizers #TARGET_REF and several participators [1] , [5] , thus filling the gap in word semantic similarity task for Russian language.\n sent4: In this paper we evaluate a Word2Vec model, trained on Russian Twitter corpus against RUSSE HJ-dataset, and show results comparable to top results of other RUSSE competitors.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "There are several implementations of Word2Vec available, including original C utility 10 and a Python library gensim 11 .",
                "We use the latter one as we find it more convenient.",
                "Output of Tomita Parser is fed directly line-by-line to the model.",
                "It produces the set of vectors, which we then query to obtain similarity between word vectors, in order to compute the correlation with HJ-dataset.",
                "To compute correlation we use Spearman coefficient, since it was used as accuracy measure in RUSSE #TARGET_REF ."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: There are several implementations of Word2Vec available, including original C utility 10 and a Python library gensim 11 .\n sent1: We use the latter one as we find it more convenient.\n sent2: Output of Tomita Parser is fed directly line-by-line to the model.\n sent3: It produces the set of vectors, which we then query to obtain similarity between word vectors, in order to compute the correlation with HJ-dataset.\n sent4: To compute correlation we use Spearman coefficient, since it was used as accuracy measure in RUSSE #TARGET_REF .\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "Comparing our results to ones reported by RUSSE participants, we conclude that our best result of 0.598 is comparable to other results, as it (virtually) encloses the top-10 of results.",
                "However, best submission of RUSSE has huge gap in accuracy of 0.16, compared to our Twitter corpus.",
                "Having in mind that best results in RUSSE combine several corpora, it is reasonable to compare Twitter results to other single-corpus results.",
                "For convenience we replicate results for these corpora, originally presented in #TARGET_REF , alongside with our result in Table 5 .",
                "Given these considerations we conclude that with size of Twitter corpus of 500M one can achieve reasonably good results on task of word semantic similarity."
            ],
            "label": [
                "SIMILARITY",
                "USE"
            ]
        },
        "input": "sent0: Comparing our results to ones reported by RUSSE participants, we conclude that our best result of 0.598 is comparable to other results, as it (virtually) encloses the top-10 of results.\n sent1: However, best submission of RUSSE has huge gap in accuracy of 0.16, compared to our Twitter corpus.\n sent2: Having in mind that best results in RUSSE combine several corpora, it is reasonable to compare Twitter results to other single-corpus results.\n sent3: For convenience we replicate results for these corpora, originally presented in #TARGET_REF , alongside with our result in Table 5 .\n sent4: Given these considerations we conclude that with size of Twitter corpus of 500M one can achieve reasonably good results on task of word semantic similarity.\n",
        "output": "{\"label\": [\"SIMILARITY\", \"USE\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "In this paper we investigated the use of Twitter corpus for training Word2Vec model for task of word semantic similarity.",
                "We described a method to obtain stream of Twitter messages and prepare them for training.",
                "We use HJ-dataset, which was created for RUSSE contest #TARGET_REF to measure correlation between similarity of word vectors and human judgements on word pairs similarity.",
                "We achieve results comparable with results obtained while training Word2Vec on traditional corpora, like Wikipedia and Web pages [1] , [5] .",
                "This is especially important because Twitter data is highly dynamic, and traditional sources are mostly static (rarely change over time)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In this paper we investigated the use of Twitter corpus for training Word2Vec model for task of word semantic similarity.\n sent1: We described a method to obtain stream of Twitter messages and prepare them for training.\n sent2: We use HJ-dataset, which was created for RUSSE contest #TARGET_REF to measure correlation between similarity of word vectors and human judgements on word pairs similarity.\n sent3: We achieve results comparable with results obtained while training Word2Vec on traditional corpora, like Wikipedia and Web pages [1] , [5] .\n sent4: This is especially important because Twitter data is highly dynamic, and traditional sources are mostly static (rarely change over time).\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "However, since new named entities arise regularly, it becomes increasingly difficult to maintain an up-to-date dictionary and/or adapt a named entity classifier to a new domain; for example, sequence labeling techniques that use feature templates (#REF; #REF) are not robust for unknown named entities because their feature space is very sparse #TARGET_REF .",
                "This problem worsens when we attempt to use a combination of features for sparse named entity classification.",
                "Therefore, in this paper, we propose the use of matrix factorization for named entity classification to consider the relationships between sparse features.",
                "Through our experiments, we achieved competitive accuracy to models developed in previous works in terms of using fewer features and compactness using factorization machines (#REF) .",
                "The main contributions of this paper are as follows:"
            ],
            "label": [
                "BACKGROUND",
                "MOTIVATION"
            ]
        },
        "input": "sent0: However, since new named entities arise regularly, it becomes increasingly difficult to maintain an up-to-date dictionary and/or adapt a named entity classifier to a new domain; for example, sequence labeling techniques that use feature templates (#REF; #REF) are not robust for unknown named entities because their feature space is very sparse #TARGET_REF .\n sent1: This problem worsens when we attempt to use a combination of features for sparse named entity classification.\n sent2: Therefore, in this paper, we propose the use of matrix factorization for named entity classification to consider the relationships between sparse features.\n sent3: Through our experiments, we achieved competitive accuracy to models developed in previous works in terms of using fewer features and compactness using factorization machines (#REF) .\n sent4: The main contributions of this paper are as follows:\n",
        "output": "{\"label\": [\"BACKGROUND\", \"MOTIVATION\"], \"context\": [\"sent0\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "These studies heavily rely on feature templates for learning combinations of features; however, since combinations of features in conventional supervised learning are treated independently, this approach is not robust for named entities that do not appear in the training data.",
                "To address the task of unknown named entity classification, #TARGET_REF explored the use of sparse combinatorial features.",
                "They proposed a log-bilinear model that defines a score function considering interactions between features; the score function is regularized via a nuclear norm on a feature weight matrix.",
                "Further, heir method employs singular value decomposition (SVD)-based regularization to handle the combination of features.",
                "They reported that their regularization achieved higher accuracy than L1 and L2 regularization, frequently used in natural language processing (#REF )."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: These studies heavily rely on feature templates for learning combinations of features; however, since combinations of features in conventional supervised learning are treated independently, this approach is not robust for named entities that do not appear in the training data.\n sent1: To address the task of unknown named entity classification, #TARGET_REF explored the use of sparse combinatorial features.\n sent2: They proposed a log-bilinear model that defines a score function considering interactions between features; the score function is regularized via a nuclear norm on a feature weight matrix.\n sent3: Further, heir method employs singular value decomposition (SVD)-based regularization to handle the combination of features.\n sent4: They reported that their regularization achieved higher accuracy than L1 and L2 regularization, frequently used in natural language processing (#REF ).\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "These results show that factorization machines require a compact model to achieve state-of-the-art results on the test set of this corpus.",
                "It would be interesting to point out that the performance of our factorization machines approach on the development dataset was lower than that of the log-bilinear model by 4.6 points.",
                "This phenomenon may occur because the log-bilinear model overfits to sparse combinatorial features even with nuclear norm regularization; further factorization machines typically have better generalization abilities than those of nuclear norm regularization.",
                "Both our approach and the methods of #REF address the problem of incorporating sparse combinatorial features by dimension reduction (i.e., matrix factorization); however, they differ in terms of the objective function to be optimized.",
                "#TARGET_REF use maximum likelihood estimation as an objective function; whereas other objective functions such as hinge loss can be used in factorization machines."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: These results show that factorization machines require a compact model to achieve state-of-the-art results on the test set of this corpus.\n sent1: It would be interesting to point out that the performance of our factorization machines approach on the development dataset was lower than that of the log-bilinear model by 4.6 points.\n sent2: This phenomenon may occur because the log-bilinear model overfits to sparse combinatorial features even with nuclear norm regularization; further factorization machines typically have better generalization abilities than those of nuclear norm regularization.\n sent3: Both our approach and the methods of #REF address the problem of incorporating sparse combinatorial features by dimension reduction (i.e., matrix factorization); however, they differ in terms of the objective function to be optimized.\n sent4: #TARGET_REF use maximum likelihood estimation as an objective function; whereas other objective functions such as hinge loss can be used in factorization machines.\n",
        "output": "{\"label\": [\"BACKGROUND\"], \"context\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                "As described above, we aim to classify named entities that rarely appear in a given training corpus.",
                "We compared factorization machines with a log-linear model, a polynomial-kernel SVM, and a state-ofthe-art log-bilinear model using nuclear norm for regularization #TARGET_REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: As described above, we aim to classify named entities that rarely appear in a given training corpus.\n sent1: We compared factorization machines with a log-linear model, a polynomial-kernel SVM, and a state-ofthe-art log-bilinear model using nuclear norm for regularization #TARGET_REF .\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Data.",
                "We used the dataset provided by #TARGET_REF ; this dataset was created for evaluating unknown named entity classification and is context features: Right and left contexts of the candidate in a sentence (do not take the order into account).",
                "cap=1, cap=0: Whether the first letter of the candidate is uppercase, or not. all-low=1, all-low=0: Whether all letters of the candidate are lowercase, or not.",
                "all-cap1=1, all-cap1=0: Whether all letters of the candidate are uppercase, or not.",
                "all-cap2=1, all-cap2=0: Whether all letters of the candidate are uppercase and periods, or not."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Data.\n sent1: We used the dataset provided by #TARGET_REF ; this dataset was created for evaluating unknown named entity classification and is context features: Right and left contexts of the candidate in a sentence (do not take the order into account).\n sent2: cap=1, cap=0: Whether the first letter of the candidate is uppercase, or not. all-low=1, all-low=0: Whether all letters of the candidate are lowercase, or not.\n sent3: all-cap1=1, all-cap1=0: Whether all letters of the candidate are uppercase, or not.\n sent4: all-cap2=1, all-cap2=0: Whether all letters of the candidate are uppercase and periods, or not.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Features.",
                "We used a subset of features from experiments performed by #TARGET_REF .",
                "Table 3 summarizes the features used in our experiment, including context and entity features.",
                "Tools.",
                "In terms of tools, we used scikit-learn 0.17 to implement a log-linear model and polynomial kernel in an SVM."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Features.\n sent1: We used a subset of features from experiments performed by #TARGET_REF .\n sent2: Table 3 summarizes the features used in our experiment, including context and entity features.\n sent3: Tools.\n sent4: In terms of tools, we used scikit-learn 0.17 to implement a log-linear model and polynomial kernel in an SVM.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Upon investigating the confusion matrix, we found that the LOC tag was often misclassified as PER.",
                "We therefore conclude here that clustering and POS features are necessary to distinguish these tags.",
                "Figure 1 plots the F1-score of our proposed method as dimension k changes for matrix factor- ization using the same development data as that of #TARGET_REF .",
                "Our method yielded the best F1-score (i.e., 57.1) at k = 5, whereas the log-bilinear model achieved the best F1-score (i.e., 61.73) at k = 40.",
                "These results show that factorization machines require a compact model to achieve state-of-the-art results on the test set of this corpus."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Upon investigating the confusion matrix, we found that the LOC tag was often misclassified as PER.\n sent1: We therefore conclude here that clustering and POS features are necessary to distinguish these tags.\n sent2: Figure 1 plots the F1-score of our proposed method as dimension k changes for matrix factor- ization using the same development data as that of #TARGET_REF .\n sent3: Our method yielded the best F1-score (i.e., 57.1) at k = 5, whereas the log-bilinear model achieved the best F1-score (i.e., 61.73) at k = 40.\n sent4: These results show that factorization machines require a compact model to achieve state-of-the-art results on the test set of this corpus.\n",
        "output": "{\"label\": [\"USE\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "Table 2 presents results of our experiments.",
                "Note that #TARGET_REF used additional features such as Brown clustering and parts-of-speech (POS) features, which we did not use.",
                "Table 4 and Figure 2 show the performance and precision-recall curves of named entity classification for each tag, respectively.",
                "We observed here that, aside from LOC, we obtained competitive results to the state-of-the-art named entity classifier proposed by #REF with fewer features.",
                "Overall, the microaveraged F1 score improved by 1.4 points."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Table 2 presents results of our experiments.\n sent1: Note that #TARGET_REF used additional features such as Brown clustering and parts-of-speech (POS) features, which we did not use.\n sent2: Table 4 and Figure 2 show the performance and precision-recall curves of named entity classification for each tag, respectively.\n sent3: We observed here that, aside from LOC, we obtained competitive results to the state-of-the-art named entity classifier proposed by #REF with fewer features.\n sent4: Overall, the microaveraged F1 score improved by 1.4 points.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                "Experimental results show that performance on ORG was improved.",
                "For example, the term \"VicePresident\" appears in both contexts of ORG and O, and our method correctly handled this sparse combination of context and entity features.",
                "The accuracy of LOC, however, was lower than that of the log-bilinear model #TARGET_REF .",
                "Upon investigating the confusion matrix, we found that the LOC tag was often misclassified as PER.",
                "We therefore conclude here that clustering and POS features are necessary to distinguish these tags."
            ],
            "label": [
                "DIFFERENCES"
            ]
        },
        "input": "sent0: Experimental results show that performance on ORG was improved.\n sent1: For example, the term \"VicePresident\" appears in both contexts of ORG and O, and our method correctly handled this sparse combination of context and entity features.\n sent2: The accuracy of LOC, however, was lower than that of the log-bilinear model #TARGET_REF .\n sent3: Upon investigating the confusion matrix, we found that the LOC tag was often misclassified as PER.\n sent4: We therefore conclude here that clustering and POS features are necessary to distinguish these tags.\n",
        "output": "{\"label\": [\"DIFFERENCES\"], \"context\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                "These results show that factorization machines require a compact model to achieve state-of-the-art results on the test set of this corpus.",
                "It would be interesting to point out that the performance of our factorization machines approach on the development dataset was lower than that of the log-bilinear model by 4.6 points.",
                "This phenomenon may occur because the log-bilinear model overfits to sparse combinatorial features even with nuclear norm regularization; further factorization machines typically have better generalization abilities than those of nuclear norm regularization.",
                "Both our approach and the methods of #TARGET_REF address the problem of incorporating sparse combinatorial features by dimension reduction (i.e., matrix factorization); however, they differ in terms of the objective function to be optimized.",
                "#REF use maximum likelihood estimation as an objective function; whereas other objective functions such as hinge loss can be used in factorization machines."
            ],
            "label": [
                "DIFFERENCES",
                "SIMILARITY"
            ]
        },
        "input": "sent0: These results show that factorization machines require a compact model to achieve state-of-the-art results on the test set of this corpus.\n sent1: It would be interesting to point out that the performance of our factorization machines approach on the development dataset was lower than that of the log-bilinear model by 4.6 points.\n sent2: This phenomenon may occur because the log-bilinear model overfits to sparse combinatorial features even with nuclear norm regularization; further factorization machines typically have better generalization abilities than those of nuclear norm regularization.\n sent3: Both our approach and the methods of #TARGET_REF address the problem of incorporating sparse combinatorial features by dimension reduction (i.e., matrix factorization); however, they differ in terms of the objective function to be optimized.\n sent4: #REF use maximum likelihood estimation as an objective function; whereas other objective functions such as hinge loss can be used in factorization machines.\n",
        "output": "{\"label\": [\"DIFFERENCES\", \"SIMILARITY\"], \"context\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                "Table 2 presents results of our experiments.",
                "Note that #REF used additional features such as Brown clustering and parts-of-speech (POS) features, which we did not use.",
                "Table 4 and Figure 2 show the performance and precision-recall curves of named entity classification for each tag, respectively.",
                "We observed here that, aside from LOC, we obtained competitive results to the state-of-the-art named entity classifier proposed by #TARGET_REF with fewer features.",
                "Overall, the microaveraged F1 score improved by 1.4 points."
            ],
            "label": [
                "SIMILARITY"
            ]
        },
        "input": "sent0: Table 2 presents results of our experiments.\n sent1: Note that #REF used additional features such as Brown clustering and parts-of-speech (POS) features, which we did not use.\n sent2: Table 4 and Figure 2 show the performance and precision-recall curves of named entity classification for each tag, respectively.\n sent3: We observed here that, aside from LOC, we obtained competitive results to the state-of-the-art named entity classifier proposed by #TARGET_REF with fewer features.\n sent4: Overall, the microaveraged F1 score improved by 1.4 points.\n",
        "output": "{\"label\": [\"SIMILARITY\"], \"context\": [\"sent3\"]}"
    }
]