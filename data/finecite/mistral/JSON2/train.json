[
    {
        "gold": {
            "text": [
                [
                    "To",
                    "test",
                    "how",
                    "well",
                    "our",
                    "data",
                    "is",
                    "suited",
                    "for",
                    "emotion",
                    "projection,",
                    "we",
                    "projected",
                    "the",
                    "English",
                    "annotations",
                    "onto",
                    "our",
                    "Finnish",
                    "unannotated",
                    "data",
                    "using",
                    "OPUS",
                    "tools",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "We",
                    "chose",
                    "Finnish",
                    "as",
                    "our",
                    "main",
                    "test",
                    "language",
                    "as",
                    "we",
                    "also",
                    "have",
                    "some",
                    "annotated",
                    "data",
                    "for",
                    "it",
                    "to",
                    "use",
                    "as",
                    "a",
                    "test",
                    "set."
                ],
                [
                    "The",
                    "manually",
                    "annotated",
                    "Finnish",
                    "data",
                    "consists",
                    "of",
                    "nearly",
                    "20k",
                    "individual",
                    "annotations",
                    "and",
                    "almost",
                    "15k",
                    "unique",
                    "annotated",
                    "sentences",
                    "plus",
                    "an",
                    "additional",
                    "7,536",
                    "sentences",
                    "annotated",
                    "as",
                    "neutral",
                    "6",
                    "."
                ],
                [
                    "The",
                    "criteria",
                    "for",
                    "the",
                    "inclusion",
                    "of",
                    "an",
                    "annotation",
                    "was",
                    "the",
                    "same",
                    "as",
                    "for",
                    "English."
                ],
                [
                    "The",
                    "distribution",
                    "of",
                    "the",
                    "number",
                    "of",
                    "labels",
                    "and",
                    "the",
                    "labels",
                    "themselves",
                    "are",
                    "quite",
                    "similar",
                    "to",
                    "that",
                    "of",
                    "the",
                    "English",
                    "data."
                ],
                [
                    "Relatively",
                    "speaking",
                    "there",
                    "is",
                    "a",
                    "little",
                    "less",
                    "anticipation",
                    "in",
                    "the",
                    "Finnish",
                    "data,",
                    "but",
                    "anger",
                    "is",
                    "the",
                    "biggest",
                    "category",
                    "in",
                    "both",
                    "languages."
                ],
                [
                    "We",
                    "used",
                    "the",
                    "11,128",
                    "Finnish",
                    "sentences",
                    "for",
                    "which",
                    "directly",
                    "parallel",
                    "sentences",
                    "existed",
                    "and",
                    "projected",
                    "the",
                    "English",
                    "annotations",
                    "on",
                    "them",
                    "using",
                    "the",
                    "unique",
                    "alignment",
                    "IDs",
                    "for",
                    "both",
                    "languages",
                    "as",
                    "guide."
                ],
                [
                    "Some",
                    "of",
                    "those",
                    "parallel",
                    "sentences",
                    "were",
                    "part",
                    "of",
                    "our",
                    "already",
                    "annotated",
                    "data",
                    "and",
                    "were",
                    "discarded",
                    "as",
                    "training",
                    "data."
                ],
                [
                    "This",
                    "served",
                    "as",
                    "a",
                    "useful",
                    "point",
                    "of",
                    "comparison."
                ],
                [
                    "The",
                    "average",
                    "annotation",
                    "correlation",
                    "using",
                    "Cohen's",
                    "kappa",
                    "is",
                    "0.44",
                    "(although",
                    "accuracy",
                    "by",
                    "percentage",
                    "is",
                    "over",
                    "90%),",
                    "and",
                    "highest",
                    "for",
                    "joy",
                    "at",
                    "0.65,",
                    "showing",
                    "that",
                    "annotation",
                    "projection",
                    "differs",
                    "from",
                    "human",
                    "annotation",
                    "to",
                    "a",
                    "similar",
                    "degree",
                    "as",
                    "human",
                    "annotations",
                    "differ",
                    "from",
                    "each",
                    "other."
                ]
            ],
            "context": [
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: To test how well our data is suited for emotion projection, we projected the English annotations onto our Finnish unannotated data using OPUS tools #TARGET_REF .\n sent1: We chose Finnish as our main test language as we also have some annotated data for it to use as a test set.\n sent2: The manually annotated Finnish data consists of nearly 20k individual annotations and almost 15k unique annotated sentences plus an additional 7,536 sentences annotated as neutral 6 .\n sent3: The criteria for the inclusion of an annotation was the same as for English.\n sent4: The distribution of the number of labels and the labels themselves are quite similar to that of the English data.\n sent5: Relatively speaking there is a little less anticipation in the Finnish data, but anger is the biggest category in both languages.\n sent6: We used the 11,128 Finnish sentences for which directly parallel sentences existed and projected the English annotations on them using the unique alignment IDs for both languages as guide.\n sent7: Some of those parallel sentences were part of our already annotated data and were discarded as training data.\n sent8: This served as a useful point of comparison.\n sent9: The average annotation correlation using Cohen's kappa is 0.44 (although accuracy by percentage is over 90%), and highest for joy at 0.65, showing that annotation projection differs from human annotation to a similar degree as human annotations differ from each other.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "For",
                    "comparison",
                    "with",
                    "previous",
                    "research",
                    "#TARGET_REF",
                    ",",
                    "we",
                    "decoded",
                    "word",
                    "length,",
                    "frequency",
                    "and",
                    "class",
                    "with",
                    "linear",
                    "SVMs",
                    "in",
                    "a",
                    "temporally-resolved",
                    "fashion",
                    "from",
                    "0",
                    "to",
                    "700",
                    "ms",
                    "poststimulus",
                    "EEG,",
                    "recorded",
                    "during",
                    "sentence",
                    "reading."
                ]
            ],
            "context": [
                2
            ]
        },
        "input": "sent0: For comparison with previous research #TARGET_REF , we decoded word length, frequency and class with linear SVMs in a temporally-resolved fashion from 0 to 700 ms poststimulus EEG, recorded during sentence reading.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "GLOVE",
                    "#REF",
                    "71.9",
                    "ELMO",
                    "#TARGET_REF",
                    "80.2",
                    "BERT",
                    "BASE",
                    "→",
                    "LR",
                    "#REF",
                    "80.6",
                    "BERT",
                    "LARGE",
                    "→",
                    "LR",
                    "#REF",
                    "79",
                    "BERT-base",
                    "by",
                    "adding",
                    "an",
                    "internal",
                    "entity",
                    "linker."
                ]
            ],
            "context": [
                0
            ]
        },
        "input": "sent0: GLOVE #REF 71.9 ELMO #TARGET_REF 80.2 BERT BASE → LR #REF 80.6 BERT LARGE → LR #REF 79 BERT-base by adding an internal entity linker.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "correct",
                    "answer",
                    "and",
                    "at",
                    "least",
                    "50%",
                    "were",
                    "inspired",
                    "by",
                    "the",
                    "accuracy",
                    "@",
                    "x%",
                    "approach",
                    "used",
                    "by",
                    "different",
                    "authors",
                    "working",
                    "with",
                    "the",
                    "Amazon",
                    "dataset",
                    "and",
                    "performing",
                    "similar",
                    "tasks",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "In",
                    "accuracy",
                    "@",
                    "x%",
                    "the",
                    "commonly",
                    "used",
                    "measure",
                    "is",
                    "accuracy",
                    "@",
                    "50%."
                ],
                [
                    "This",
                    "approach",
                    "helps",
                    "in",
                    "identifying",
                    "the",
                    "top",
                    "answers",
                    "crossing",
                    "a",
                    "threshold",
                    "and",
                    "has",
                    "better",
                    "relationship",
                    "in",
                    "real",
                    "world",
                    "applications",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                1,
                3,
                2
            ]
        },
        "input": "sent0: The correct answer and at least 50% were inspired by the accuracy @ x% approach used by different authors working with the Amazon dataset and performing similar tasks #TARGET_REF .\n sent1: In accuracy @ x% the commonly used measure is accuracy @ 50%.\n sent2: This approach helps in identifying the top answers crossing a threshold and has better relationship in real world applications #REF .\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Another",
                    "axis",
                    "of",
                    "development",
                    "concerns",
                    "the",
                    "use",
                    "of",
                    "neural",
                    "architecture",
                    "search",
                    "#REF",
                    "which",
                    "allows",
                    "to",
                    "optimize",
                    "a",
                    "model",
                    "by",
                    "progressively",
                    "modifying",
                    "the",
                    "design",
                    "of",
                    "the",
                    "network",
                    "through",
                    "trial",
                    "and",
                    "error,",
                    "eliminating",
                    "insignificant",
                    "operations."
                ],
                [
                    "To",
                    "avoid",
                    "the",
                    "unnecessary",
                    "large",
                    "number",
                    "of",
                    "parameters,",
                    "adapters",
                    "#TARGET_REF",
                    "were",
                    "introduced",
                    "to",
                    "allow",
                    "fine-tuning",
                    "of",
                    "the",
                    "set",
                    "of",
                    "parameters",
                    "specific",
                    "to",
                    "the",
                    "task",
                    "of",
                    "interest",
                    "rather",
                    "than",
                    "the",
                    "entire",
                    "model."
                ]
            ],
            "context": [
                0,
                1
            ]
        },
        "input": "sent0: Another axis of development concerns the use of neural architecture search #REF which allows to optimize a model by progressively modifying the design of the network through trial and error, eliminating insignificant operations.\n sent1: To avoid the unnecessary large number of parameters, adapters #TARGET_REF were introduced to allow fine-tuning of the set of parameters specific to the task of interest rather than the entire model.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "A",
                    "probabilistic",
                    "LR",
                    "parser",
                    "was",
                    "trained",
                    "with",
                    "the",
                    "integrated",
                    "grammar",
                    "by",
                    "exploiting",
                    "the",
                    "Susanne",
                    "treebank",
                    "bracketing."
                ],
                [
                    "An",
                    "LR",
                    "parser",
                    "#REF",
                    "was",
                    "applied",
                    "to",
                    "unlabelled",
                    "brack",
                    "eted",
                    "sentences",
                    "from",
                    "the",
                    "Susanne",
                    "treebank,",
                    "and",
                    "a",
                    "new",
                    "treebank",
                    "of",
                    "1758",
                    "correct",
                    "and",
                    "complete",
                    "analyses",
                    "with",
                    "respect",
                    "to",
                    "the",
                    "integrated",
                    "grammar",
                    "was",
                    "constructed",
                    "semi-automatically",
                    "by",
                    "manu",
                    "ally•",
                    "resolving",
                    "the",
                    "remaining",
                    "ambiguities."
                ],
                [
                    "250",
                    "sentences",
                    "from",
                    "the",
                    "new",
                    "treebank",
                    "were",
                    "kept",
                    "back",
                    "for",
                    "testing."
                ],
                [
                    "The",
                    "remainder,",
                    "together",
                    "with",
                    "a",
                    "further",
                    "set",
                    "of",
                    "analyses",
                    "from",
                    "2285",
                    "tree",
                    "bank",
                    "sentences",
                    "that",
                    "were",
                    "not",
                    "checked",
                    "manually,",
                    "were",
                    "used",
                    "to",
                    "train",
                    "a",
                    "probabilistic",
                    "version",
                    "of",
                    "the",
                    "LR",
                    "parser,",
                    "using",
                    "Good-Turing",
                    "smoothing",
                    "to",
                    "estimate",
                    "the",
                    "probability",
                    "of",
                    "unseen",
                    "transitions",
                    "in",
                    "the",
                    "LALR(",
                    "1",
                    ")",
                    "table",
                    "#REF",
                    "."
                ],
                [
                    "The",
                    "probabilistic",
                    "parser",
                    "can",
                    "then",
                    "return",
                    "a",
                    "ranking",
                    "of",
                    "all",
                    "possible",
                    "analyses",
                    "for",
                    "a",
                    "sentence,",
                    "or",
                    "efficiently",
                    "return",
                    "just",
                    "the",
                    "n-most",
                    "probable",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "sent0: A probabilistic LR parser was trained with the integrated grammar by exploiting the Susanne treebank bracketing.\n sent1: An LR parser #REF was applied to unlabelled brack eted sentences from the Susanne treebank, and a new treebank of 1758 correct and complete analyses with respect to the integrated grammar was constructed semi-automatically by manu ally• resolving the remaining ambiguities.\n sent2: 250 sentences from the new treebank were kept back for testing.\n sent3: The remainder, together with a further set of analyses from 2285 tree bank sentences that were not checked manually, were used to train a probabilistic version of the LR parser, using Good-Turing smoothing to estimate the probability of unseen transitions in the LALR( 1 ) table #REF .\n sent4: The probabilistic parser can then return a ranking of all possible analyses for a sentence, or efficiently return just the n-most probable #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [\"sent4\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Among",
                    "the",
                    "configurations,",
                    "FastText",
                    "similarity",
                    "combined",
                    "with",
                    "the",
                    "Koyyalagunta",
                    "scoring",
                    "function",
                    "was",
                    "evaluated",
                    "previously",
                    "by",
                    "#TARGET_REF",
                    ",",
                    "where",
                    "it",
                    "was",
                    "the",
                    "best",
                    "agent",
                    "without",
                    "any",
                    "language-specific",
                    "resource,",
                    "i.e."
                ],
                [
                    "using",
                    "raw",
                    "corpora",
                    "only."
                ],
                [
                    "The",
                    "results",
                    "show",
                    "that",
                    "this",
                    "is",
                    "outperformed",
                    "by",
                    "many",
                    "of",
                    "our",
                    "new",
                    "configurations."
                ]
            ],
            "context": [
                1,
                3,
                1
            ]
        },
        "input": "sent0: Among the configurations, FastText similarity combined with the Koyyalagunta scoring function was evaluated previously by #TARGET_REF , where it was the best agent without any language-specific resource, i.e.\n sent1: using raw corpora only.\n sent2: The results show that this is outperformed by many of our new configurations.\n",
        "output": "{\"INFORMATION\": [\"sent0\", \"sent2\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "these",
                    "experiments",
                    "we",
                    "have",
                    "compared",
                    "the",
                    "conditioned",
                    "text",
                    "generation",
                    "of",
                    "CTERM-GAN",
                    "with",
                    "that",
                    "of",
                    "the",
                    "state-of-the-art",
                    "adversarial",
                    "architectures",
                    "-Se-qGAN",
                    "#REF",
                    ",",
                    "RelGAN",
                    "#TARGET_REF",
                    ",",
                    "and",
                    "TGVAE",
                    "#REF",
                    "-and",
                    "a",
                    "classic",
                    "auto-regressive",
                    "LSTM",
                    "language",
                    "model",
                    "with",
                    "an",
                    "initial",
                    "conditioning,",
                    "in",
                    "terms",
                    "of",
                    "both",
                    "syntactic",
                    "and",
                    "semantic",
                    "quality."
                ],
                [
                    "The",
                    "main",
                    "goal",
                    "is",
                    "to",
                    "ensure",
                    "a",
                    "good",
                    "quality",
                    "for",
                    "the",
                    "generation",
                    "by",
                    "introducing",
                    "a",
                    "conditioning",
                    "on",
                    "the",
                    "semantic",
                    "of",
                    "the",
                    "sentence."
                ],
                [
                    "In",
                    "this",
                    "task,",
                    "the",
                    "conditioning",
                    "consists",
                    "of",
                    "the",
                    "word",
                    "distribution",
                    "for",
                    "a",
                    "topic",
                    "extracted",
                    "from",
                    "a",
                    "sentence,",
                    "either",
                    "provided",
                    "by",
                    "the",
                    "user",
                    "or,",
                    "as",
                    "in",
                    "our",
                    "case,",
                    "sampled",
                    "from",
                    "the",
                    "dataset."
                ],
                [
                    "Any",
                    "type",
                    "of",
                    "topic",
                    "model",
                    "can",
                    "be",
                    "adopted:",
                    "in",
                    "our",
                    "case,",
                    "an",
                    "LDA",
                    "model",
                    "#REF",
                    "has",
                    "been",
                    "trained",
                    "on",
                    "a",
                    "starting",
                    "dataset",
                    "in",
                    "order",
                    "to",
                    "have",
                    "a",
                    "distribution",
                    "of",
                    "the",
                    "topics",
                    "covered",
                    "within",
                    "the",
                    "corpus."
                ],
                [
                    "The",
                    "LDA",
                    "model,",
                    "both",
                    "in",
                    "training",
                    "and",
                    "in",
                    "inference,",
                    "given",
                    "an",
                    "input",
                    "sentence,",
                    "builds",
                    "a",
                    "distribution",
                    "on",
                    "the",
                    "vocabulary."
                ],
                [
                    "In",
                    "turn,",
                    "this",
                    "distribution",
                    "influences",
                    "the",
                    "model's",
                    "sentence",
                    "generation",
                    "thanks",
                    "to",
                    "its",
                    "inclusion",
                    "in",
                    "the",
                    "generation",
                    "process."
                ],
                [
                    "Most",
                    "likely,",
                    "improving",
                    "the",
                    "quality",
                    "of",
                    "the",
                    "topic",
                    "extraction",
                    "is",
                    "likely",
                    "to",
                    "improve",
                    "the",
                    "final",
                    "results",
                    "of",
                    "the",
                    "model."
                ],
                [
                    "Eventually,",
                    "the",
                    "extracted",
                    "distribution",
                    "is",
                    "used",
                    "as",
                    "the",
                    "condition-",
                    "ing",
                    "input,",
                    "c,",
                    "for",
                    "the",
                    "relational",
                    "memory",
                    "during",
                    "the",
                    "generation,",
                    "as",
                    "described",
                    "in",
                    "Section",
                    "3."
                ]
            ],
            "context": [
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: In these experiments we have compared the conditioned text generation of CTERM-GAN with that of the state-of-the-art adversarial architectures -Se-qGAN #REF , RelGAN #TARGET_REF , and TGVAE #REF -and a classic auto-regressive LSTM language model with an initial conditioning, in terms of both syntactic and semantic quality.\n sent1: The main goal is to ensure a good quality for the generation by introducing a conditioning on the semantic of the sentence.\n sent2: In this task, the conditioning consists of the word distribution for a topic extracted from a sentence, either provided by the user or, as in our case, sampled from the dataset.\n sent3: Any type of topic model can be adopted: in our case, an LDA model #REF has been trained on a starting dataset in order to have a distribution of the topics covered within the corpus.\n sent4: The LDA model, both in training and in inference, given an input sentence, builds a distribution on the vocabulary.\n sent5: In turn, this distribution influences the model's sentence generation thanks to its inclusion in the generation process.\n sent6: Most likely, improving the quality of the topic extraction is likely to improve the final results of the model.\n sent7: Eventually, the extracted distribution is used as the condition- ing input, c, for the relational memory during the generation, as described in Section 3.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "used",
                    "Plutchik's",
                    "core",
                    "emotions",
                    "as",
                    "our",
                    "annotation",
                    "scheme",
                    "resulting",
                    "in",
                    "8",
                    "distinct",
                    "emotion",
                    "categories",
                    "plus",
                    "neutral."
                ],
                [
                    "The",
                    "Sentimentator",
                    "platform",
                    "#TARGET_REF",
                    "allows",
                    "for",
                    "the",
                    "annotation",
                    "of",
                    "intensities",
                    "resulting",
                    "in",
                    "what",
                    "is",
                    "essentially",
                    "30",
                    "emotions",
                    "and",
                    "sentiments,",
                    "however,",
                    "as",
                    "the",
                    "intensity",
                    "score",
                    "is",
                    "not",
                    "available",
                    "for",
                    "all",
                    "annotations,",
                    "the",
                    "intensity",
                    "scores",
                    "were",
                    "discarded."
                ],
                [
                    "The",
                    "granularity",
                    "of",
                    "our",
                    "annotations",
                    "roughly",
                    "correspond",
                    "to",
                    "sentence-level",
                    "annotations,",
                    "although",
                    "as",
                    "our",
                    "source",
                    "data",
                    "is",
                    "movie",
                    "subtitles,",
                    "our",
                    "shortest",
                    "subtitle",
                    "is",
                    "!"
                ],
                [
                    "and",
                    "the",
                    "longest",
                    "subtitle",
                    "consists",
                    "of",
                    "three",
                    "separate",
                    "sentences."
                ],
                [
                    "A",
                    "majority",
                    "of",
                    "the",
                    "subtitles",
                    "for",
                    "English",
                    "were",
                    "assigned",
                    "one",
                    "emotion",
                    "label",
                    "(78%),",
                    "17%",
                    "were",
                    "assigned",
                    "two,",
                    "and",
                    "roughly",
                    "5%",
                    "had",
                    "three",
                    "or",
                    "more",
                    "categories",
                    "(see",
                    "also",
                    "Table",
                    "3)."
                ]
            ],
            "context": [
                2,
                2,
                0,
                0,
                0
            ]
        },
        "input": "sent0: We used Plutchik's core emotions as our annotation scheme resulting in 8 distinct emotion categories plus neutral.\n sent1: The Sentimentator platform #TARGET_REF allows for the annotation of intensities resulting in what is essentially 30 emotions and sentiments, however, as the intensity score is not available for all annotations, the intensity scores were discarded.\n sent2: The granularity of our annotations roughly correspond to sentence-level annotations, although as our source data is movie subtitles, our shortest subtitle is !\n sent3: and the longest subtitle consists of three separate sentences.\n sent4: A majority of the subtitles for English were assigned one emotion label (78%), 17% were assigned two, and roughly 5% had three or more categories (see also Table 3).\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\", \"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "that",
                    "disentangled",
                    "representations",
                    "are",
                    "likely",
                    "to",
                    "be",
                    "easier",
                    "to",
                    "discriminate,",
                    "although",
                    "the",
                    "role",
                    "of",
                    "sparsely",
                    "learned",
                    "representations",
                    "could",
                    "contribute",
                    "to",
                    "MAT-VAE's",
                    "success",
                    "as",
                    "well",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                0
            ]
        },
        "input": "sent0: that disentangled representations are likely to be easier to discriminate, although the role of sparsely learned representations could contribute to MAT-VAE's success as well #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "For",
                    "the",
                    "downstream",
                    "NER",
                    "tasks",
                    "we",
                    "map",
                    "or",
                    "encode",
                    "the",
                    "NER",
                    "annotations",
                    "into",
                    "the",
                    "phonetic",
                    "representation."
                ],
                [
                    "We",
                    "thus",
                    "edited",
                    "the",
                    "labels",
                    "(PER,",
                    "ORG,",
                    "DATE,",
                    "and",
                    "LOC)",
                    "to",
                    "convert",
                    "them",
                    "from",
                    "word-level",
                    "labels",
                    "to",
                    "phone-level",
                    "labels",
                    "as",
                    "shown",
                    "in",
                    "Fig."
                ],
                [
                    "3."
                ],
                [
                    "Unlike",
                    "#TARGET_REF",
                    ",",
                    "we",
                    "leave",
                    "in",
                    "the",
                    "B-and",
                    "I-prefixes."
                ]
            ],
            "context": [
                0,
                3,
                3,
                0
            ]
        },
        "input": "sent0: For the downstream NER tasks we map or encode the NER annotations into the phonetic representation.\n sent1: We thus edited the labels (PER, ORG, DATE, and LOC) to convert them from word-level labels to phone-level labels as shown in Fig.\n sent2: 3.\n sent3: Unlike #TARGET_REF , we leave in the B-and I-prefixes.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "use",
                    "DRQA",
                    "#TARGET_REF",
                    "to",
                    "extract",
                    "k",
                    "sentences",
                    "S",
                    "=",
                    "{s",
                    "i",
                    "}",
                    "k",
                    "i=1",
                    "and",
                    "n",
                    "tables",
                    "T",
                    "=",
                    "{t",
                    "i",
                    "}",
                    "n",
                    "i=1",
                    "from",
                    "the",
                    "retrieved",
                    "pages,",
                    "respectively."
                ],
                [
                    "Then",
                    "we",
                    "select",
                    "cells",
                    "from",
                    "the",
                    "extracted",
                    "tables."
                ],
                [
                    "Many",
                    "instances",
                    "in",
                    "the",
                    "FEVEROUS",
                    "dataset",
                    "require",
                    "evidence",
                    "cells",
                    "from",
                    "more",
                    "than",
                    "one",
                    "table,",
                    "and",
                    "each",
                    "retrieved",
                    "table",
                    "has",
                    "different",
                    "relevance",
                    "score",
                    "to",
                    "the",
                    "claim."
                ],
                [
                    "However,",
                    "the",
                    "widely-used",
                    "cell",
                    "extractor",
                    "#REF",
                    "reserves",
                    "cells",
                    "from",
                    "only",
                    "one",
                    "table",
                    "in",
                    "their",
                    "implementation."
                ]
            ],
            "context": [
                2,
                2,
                2,
                2
            ]
        },
        "input": "sent0: We use DRQA #TARGET_REF to extract k sentences S = {s i } k i=1 and n tables T = {t i } n i=1 from the retrieved pages, respectively.\n sent1: Then we select cells from the extracted tables.\n sent2: Many instances in the FEVEROUS dataset require evidence cells from more than one table, and each retrieved table has different relevance score to the claim.\n sent3: However, the widely-used cell extractor #REF reserves cells from only one table in their implementation.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\", \"sent1\", \"sent2\", \"sent3\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "A",
                    "recently",
                    "popular",
                    "approach",
                    "in",
                    "Named-Entity",
                    "Recognition",
                    "tasks",
                    "has",
                    "been",
                    "to",
                    "use",
                    "Conditional",
                    "Random",
                    "Fields",
                    "(CRF)",
                    "with",
                    "BERT-based",
                    "models."
                ],
                [
                    "Inspired",
                    "by",
                    "the",
                    "CRF-based",
                    "approaches",
                    "#TARGET_REF",
                    ",",
                    "we",
                    "use",
                    "BERT-based",
                    "models",
                    "with",
                    "a",
                    "single",
                    "BiLSTM",
                    "layer",
                    "and",
                    "a",
                    "CRF",
                    "layer."
                ],
                [
                    "During",
                    "training,",
                    "the",
                    "CRF",
                    "loss",
                    "is",
                    "used",
                    "and",
                    "during",
                    "prediction,",
                    "Viterbi",
                    "Decoding",
                    "is",
                    "performed."
                ],
                [
                    "Though",
                    "CRF",
                    "is",
                    "generally",
                    "used",
                    "for",
                    "word-level",
                    "classification,",
                    "we",
                    "do",
                    "not",
                    "mask",
                    "inner",
                    "and",
                    "end",
                    "tokens",
                    "for",
                    "a",
                    "word",
                    "as",
                    "it",
                    "degrades",
                    "dev",
                    "set",
                    "performance",
                    "for",
                    "our",
                    "systems."
                ],
                [
                    "Hence,",
                    "all",
                    "the",
                    "tokens",
                    "of",
                    "a",
                    "word",
                    "are",
                    "considered",
                    "for",
                    "classification."
                ]
            ],
            "context": [
                3,
                2,
                0,
                0,
                0
            ]
        },
        "input": "sent0: A recently popular approach in Named-Entity Recognition tasks has been to use Conditional Random Fields (CRF) with BERT-based models.\n sent1: Inspired by the CRF-based approaches #TARGET_REF , we use BERT-based models with a single BiLSTM layer and a CRF layer.\n sent2: During training, the CRF loss is used and during prediction, Viterbi Decoding is performed.\n sent3: Though CRF is generally used for word-level classification, we do not mask inner and end tokens for a word as it degrades dev set performance for our systems.\n sent4: Hence, all the tokens of a word are considered for classification.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "As",
                    "a",
                    "baseline",
                    "experiment",
                    "we",
                    "applied",
                    "our",
                    "NIST",
                    "Arabic/English",
                    "system",
                    "#TARGET_REF",
                    "to",
                    "the",
                    "BTEC",
                    "task",
                    "of",
                    "this",
                    "evaluation."
                ],
                [
                    "It",
                    "can",
                    "be",
                    "seen",
                    "in",
                    "Table",
                    "4,",
                    "first",
                    "line,",
                    "that",
                    "a",
                    "system",
                    "optimized",
                    "on",
                    "a",
                    "news",
                    "task",
                    "does",
                    "not",
                    "perform",
                    "very",
                    "well",
                    "on",
                    "tourism",
                    "related",
                    "short",
                    "sentences",
                    "of",
                    "the",
                    "BTEC",
                    "task."
                ],
                [
                    "Note",
                    "that",
                    "both",
                    "systems",
                    "use",
                    "exactly",
                    "the",
                    "same",
                    "tokenization."
                ],
                [
                    "Using",
                    "a",
                    "language",
                    "model",
                    "optimized",
                    "on",
                    "the",
                    "BTEC",
                    "task",
                    "does",
                    "improve",
                    "the",
                    "BLEU",
                    "score",
                    "by",
                    "3.8",
                    "points",
                    "on",
                    "Dev6,",
                    "but",
                    "only",
                    "marginally",
                    "on",
                    "Dev5."
                ],
                [
                    "Also,",
                    "the",
                    "BLEU",
                    "score",
                    "obtained",
                    "by",
                    "this",
                    "generic",
                    "system",
                    "is",
                    "comparable",
                    "to",
                    "the",
                    "one",
                    "obtained",
                    "when",
                    "using",
                    "the",
                    "in-domain",
                    "BTEC",
                    "corpus",
                    "to",
                    "train",
                    "the",
                    "translation",
                    "model."
                ],
                [
                    "On",
                    "the",
                    "other",
                    "hand,",
                    "there",
                    "is",
                    "a",
                    "4:",
                    "BLEU",
                    "scores",
                    "of",
                    "a",
                    "generic",
                    "Arabic/English",
                    "translation",
                    "system",
                    "(NIST",
                    "task)."
                ]
            ],
            "context": [
                2,
                2,
                3,
                0,
                0,
                0
            ]
        },
        "input": "sent0: As a baseline experiment we applied our NIST Arabic/English system #TARGET_REF to the BTEC task of this evaluation.\n sent1: It can be seen in Table 4, first line, that a system optimized on a news task does not perform very well on tourism related short sentences of the BTEC task.\n sent2: Note that both systems use exactly the same tokenization.\n sent3: Using a language model optimized on the BTEC task does improve the BLEU score by 3.8 points on Dev6, but only marginally on Dev5.\n sent4: Also, the BLEU score obtained by this generic system is comparable to the one obtained when using the in-domain BTEC corpus to train the translation model.\n sent5: On the other hand, there is a 4: BLEU scores of a generic Arabic/English translation system (NIST task).\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\", \"sent1\"], \"BACKGROUND\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Of",
                    "particular",
                    "interest",
                    "to",
                    "us",
                    "were",
                    "the",
                    "keywords",
                    "apology",
                    "(noun)",
                    "and",
                    "regret",
                    "(verb)."
                ],
                [
                    "We",
                    "compare",
                    "the",
                    "SentiWordNet",
                    "scores",
                    "and",
                    "the",
                    "WordNet-Affect",
                    "labels",
                    "of",
                    "these",
                    "two",
                    "keywords."
                ],
                [
                    "While",
                    "emotion",
                    "is",
                    "defined",
                    "as",
                    "a",
                    "relatively",
                    "brief",
                    "episode",
                    "of",
                    "response",
                    "to",
                    "the",
                    "evaluation",
                    "of",
                    "an",
                    "external",
                    "or",
                    "internal",
                    "event",
                    "as",
                    "being",
                    "of",
                    "major",
                    "significance."
                ],
                [
                    "(such",
                    "as",
                    "angry,",
                    "sad,",
                    "joyful,",
                    "fearful,",
                    "ashamed,",
                    "proud,",
                    "elated,",
                    "desperate),",
                    "a",
                    "sentiment",
                    "is",
                    "the",
                    "positive",
                    "or",
                    "negative",
                    "orientation",
                    "that",
                    "a",
                    "person",
                    "expresses",
                    "toward",
                    "some",
                    "object",
                    "or",
                    "situation",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Thus,",
                    "we",
                    "can",
                    "posit",
                    "that",
                    "the",
                    "word",
                    "apology",
                    "which",
                    "has",
                    "no",
                    "emotion",
                    "label,",
                    "has",
                    "no",
                    "or",
                    "weak",
                    "emotional",
                    "connect,",
                    "which",
                    "also",
                    "aligns",
                    "with",
                    "our",
                    "conclusion",
                    "about",
                    "the",
                    "keyword",
                    "apologize."
                ],
                [
                    "In",
                    "contrast,",
                    "the",
                    "verb",
                    "regret",
                    "helps",
                    "to",
                    "effectively",
                    "communicate",
                    "the",
                    "emotion",
                    "of",
                    "repentance."
                ],
                [
                    "Looking",
                    "at",
                    "the",
                    "sentiment",
                    "associated",
                    "with",
                    "these",
                    "words,",
                    "we",
                    "conclude",
                    "that",
                    "the",
                    "mental",
                    "attitude",
                    "of",
                    "the",
                    "writer",
                    "is",
                    "more",
                    "objective",
                    "to",
                    "the",
                    "situation",
                    "in",
                    "using",
                    "the",
                    "verb",
                    "regret",
                    "while",
                    "it",
                    "is",
                    "highly",
                    "negative",
                    "in",
                    "the",
                    "case",
                    "of",
                    "the",
                    "usage",
                    "of",
                    "the",
                    "word",
                    "apology."
                ],
                [
                    "This",
                    "further",
                    "implies",
                    "that",
                    "a",
                    "high",
                    "negative",
                    "sentiment",
                    "score",
                    "means",
                    "that",
                    "the",
                    "writer",
                    "of",
                    "the",
                    "apology",
                    "realizes",
                    "the",
                    "gravity",
                    "of",
                    "the",
                    "transgression",
                    "and",
                    "to",
                    "some",
                    "extent",
                    "admits",
                    "to",
                    "the",
                    "wrong",
                    "done."
                ],
                [
                    "However,",
                    "a",
                    "high",
                    "objective",
                    "score",
                    "implies",
                    "the",
                    "writer",
                    "taking",
                    "a",
                    "neutral",
                    "stance",
                    "to",
                    "the",
                    "situation",
                    "and",
                    "not",
                    "necessarily",
                    "admitting",
                    "to",
                    "any",
                    "wrongdoing."
                ]
            ],
            "context": [
                0,
                0,
                2,
                1,
                2,
                3,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Of particular interest to us were the keywords apology (noun) and regret (verb).\n sent1: We compare the SentiWordNet scores and the WordNet-Affect labels of these two keywords.\n sent2: While emotion is defined as a relatively brief episode of response to the evaluation of an external or internal event as being of major significance.\n sent3: (such as angry, sad, joyful, fearful, ashamed, proud, elated, desperate), a sentiment is the positive or negative orientation that a person expresses toward some object or situation #TARGET_REF .\n sent4: Thus, we can posit that the word apology which has no emotion label, has no or weak emotional connect, which also aligns with our conclusion about the keyword apologize.\n sent5: In contrast, the verb regret helps to effectively communicate the emotion of repentance.\n sent6: Looking at the sentiment associated with these words, we conclude that the mental attitude of the writer is more objective to the situation in using the verb regret while it is highly negative in the case of the usage of the word apology.\n sent7: This further implies that a high negative sentiment score means that the writer of the apology realizes the gravity of the transgression and to some extent admits to the wrong done.\n sent8: However, a high objective score implies the writer taking a neutral stance to the situation and not necessarily admitting to any wrongdoing.\n",
        "output": "{\"INFORMATION\": [\"sent3\"], \"PERCEPTION\": [\"sent2\", \"sent4\"], \"BACKGROUND\": [\"sent5\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "investigate",
                    "this",
                    "relation",
                    "from",
                    "the",
                    "opposite",
                    "direction",
                    "by",
                    "evaluating",
                    "whether",
                    "our",
                    "model",
                    "(BRIO-Mul),",
                    "which",
                    "is",
                    "trained",
                    "to",
                    "have",
                    "better",
                    "sequencelevel",
                    "performance,",
                    "would",
                    "also",
                    "be",
                    "more",
                    "calibrated",
                    "at",
                    "the",
                    "token-level",
                    "compared",
                    "with",
                    "the",
                    "baseline",
                    "models",
                    "that",
                    "are",
                    "trained",
                    "using",
                    "MLE",
                    "and",
                    "label",
                    "smoothing."
                ],
                [
                    "We",
                    "follow",
                    "previous",
                    "work",
                    "by",
                    "using",
                    "the",
                    "Expected",
                    "Calibration",
                    "Error",
                    "#TARGET_REF",
                    "(ECE)",
                    "as",
                    "the",
                    "evaluation",
                    "metric",
                    "of",
                    "calibration:ECE",
                    "=",
                    "M",
                    "m=1",
                    "|B",
                    "m",
                    "|",
                    "n",
                    "|acc(B",
                    "m",
                    ")",
                    "−",
                    "conf(B",
                    "m",
                    ")|",
                    "(12)where",
                    "the",
                    "samples",
                    "are",
                    "grouped",
                    "into",
                    "M",
                    "equal-width",
                    "buckets",
                    "by",
                    "confidence",
                    "(conf),",
                    "B",
                    "m",
                    "denotes",
                    "the",
                    "m-th",
                    "bucket,",
                    "and",
                    "n",
                    "is",
                    "the",
                    "total",
                    "number",
                    "of",
                    "samples."
                ],
                [
                    "Following",
                    "#REF",
                    ",",
                    "we",
                    "evaluate",
                    "model",
                    "calibration",
                    "on",
                    "the",
                    "system-generated",
                    "summaries",
                    "during",
                    "inference",
                    "and",
                    "use",
                    "the",
                    "tercom",
                    "toolkit",
                    "11",
                    "to",
                    "assign",
                    "labels",
                    "(correct/incorrect)",
                    "to",
                    "the",
                    "system-generated",
                    "summaries",
                    "based",
                    "on",
                    "the",
                    "reference",
                    "summaries."
                ]
            ],
            "context": [
                0,
                1,
                0
            ]
        },
        "input": "sent0: We investigate this relation from the opposite direction by evaluating whether our model (BRIO-Mul), which is trained to have better sequencelevel performance, would also be more calibrated at the token-level compared with the baseline models that are trained using MLE and label smoothing.\n sent1: We follow previous work by using the Expected Calibration Error #TARGET_REF (ECE) as the evaluation metric of calibration:ECE = M m=1 |B m | n |acc(B m ) − conf(B m )| (12)where the samples are grouped into M equal-width buckets by confidence (conf), B m denotes the m-th bucket, and n is the total number of samples.\n sent2: Following #REF , we evaluate model calibration on the system-generated summaries during inference and use the tercom toolkit 11 to assign labels (correct/incorrect) to the system-generated summaries based on the reference summaries.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "addition,",
                    "they",
                    "introduce",
                    "another",
                    "method",
                    "to",
                    "score",
                    "clues",
                    "not",
                    "only",
                    "on",
                    "the",
                    "basis",
                    "of",
                    "word",
                    "similarities,",
                    "but",
                    "also",
                    "on",
                    "the",
                    "basis",
                    "of",
                    "their",
                    "frequency",
                    "and",
                    "the",
                    "similarity",
                    "of",
                    "Dict2vec",
                    "vectors",
                    "#TARGET_REF",
                    "-but",
                    "this",
                    "is",
                    "actually",
                    "a",
                    "modification",
                    "of",
                    "the",
                    "original",
                    "distance",
                    "matrix."
                ]
            ],
            "context": [
                2
            ]
        },
        "input": "sent0: In addition, they introduce another method to score clues not only on the basis of word similarities, but also on the basis of their frequency and the similarity of Dict2vec vectors #TARGET_REF -but this is actually a modification of the original distance matrix.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Our",
                    "training",
                    "data",
                    "include",
                    "two",
                    "public",
                    "datasets:",
                    "MNLI",
                    "#TARGET_REF"
                ]
            ],
            "context": [
                2
            ]
        },
        "input": "sent0: Our training data include two public datasets: MNLI #TARGET_REF\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "As",
                    "training",
                    "loss,",
                    "we",
                    "have",
                    "used",
                    "a",
                    "non-saturating",
                    "GAN",
                    "loss",
                    "function",
                    "#TARGET_REF",
                    ",",
                    "that,",
                    "considering",
                    "the",
                    "doublediscriminator",
                    "model,",
                    "is",
                    "extended",
                    "as:lD",
                    "=",
                    "1",
                    "m",
                    "m",
                    "X",
                    "i=1",
                    "✓",
                    "log(D",
                    "S",
                    "(xr))",
                    "+",
                    "log(1",
                    "D",
                    "S",
                    "(G(xz)))",
                    "◆",
                    "+",
                    "✓",
                    "log(D",
                    "T",
                    "(xr))",
                    "+",
                    "log(1",
                    "D",
                    "T",
                    "(G(xz)))◆",
                    "1",
                    "All",
                    "the",
                    "training",
                    "information",
                    "and",
                    "hyperparameters",
                    "are",
                    "described",
                    "in",
                    "Appendix",
                    "D.",
                    "We",
                    "will",
                    "release",
                    "all",
                    "our",
                    "code",
                    "publicly",
                    "after",
                    "the",
                    "anonymity",
                    "period."
                ],
                [
                    "1:",
                    "CTERM-GAN",
                    "architecture",
                    "lG",
                    "=",
                    "1",
                    "m",
                    "m",
                    "X",
                    "i=1",
                    "",
                    "log(D",
                    "S",
                    "(G(xz)))",
                    "log(D",
                    "T",
                    "(G(xz)))where",
                    "is",
                    "a",
                    "hyperparameter",
                    "that",
                    "assigns",
                    "a",
                    "relative",
                    "weight",
                    "to",
                    "the",
                    "topic",
                    "discriminator",
                    "with",
                    "respect",
                    "to",
                    "the",
                    "syntax",
                    "one."
                ],
                [
                    "plays",
                    "an",
                    "important",
                    "role",
                    "during",
                    "training",
                    "since,",
                    "if",
                    "it",
                    "is",
                    "too",
                    "low,",
                    "the",
                    "model",
                    "ignores",
                    "the",
                    "conditioning",
                    "due",
                    "to",
                    "the",
                    "limited",
                    "penalty."
                ],
                [
                    "Conversely,",
                    "a",
                    "too",
                    "high",
                    "a",
                    "value",
                    "would",
                    "give",
                    "too",
                    "much",
                    "importance",
                    "to",
                    "the",
                    "conditioning,",
                    "affecting",
                    "the",
                    "quality",
                    "of",
                    "the",
                    "generated",
                    "sentence."
                ]
            ],
            "context": [
                1,
                0,
                0,
                0
            ]
        },
        "input": "sent0: As training loss, we have used a non-saturating GAN loss function #TARGET_REF , that, considering the doublediscriminator model, is extended as:lD = 1 m m X i=1 ✓ log(D S (xr)) + log(1 D S (G(xz))) ◆ + ✓ log(D T (xr)) + log(1 D T (G(xz)))◆ 1 All the training information and hyperparameters are described in Appendix D. We will release all our code publicly after the anonymity period.\n sent1: 1: CTERM-GAN architecture lG = 1 m m X i=1  log(D S (G(xz))) log(D T (G(xz)))where is a hyperparameter that assigns a relative weight to the topic discriminator with respect to the syntax one.\n sent2: plays an important role during training since, if it is too low, the model ignores the conditioning due to the limited penalty.\n sent3: Conversely, a too high a value would give too much importance to the conditioning, affecting the quality of the generated sentence.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Choice",
                    "of",
                    "BERT",
                    "Output",
                    "Layer",
                    "and",
                    "Wordpiece",
                    "Embeddings",
                    "We",
                    "were",
                    "interested",
                    "in",
                    "how",
                    "the",
                    "choice",
                    "of",
                    "BERT",
                    "output",
                    "layers",
                    "and",
                    "word",
                    "piece",
                    "embeddings",
                    "impacts",
                    "performance",
                    "of",
                    "our",
                    "model."
                ],
                [
                    "Hence,",
                    "we",
                    "did",
                    "the",
                    "following",
                    "experiments",
                    "with",
                    "our",
                    "base",
                    "model,",
                    "shown",
                    "in",
                    "Table",
                    "4."
                ],
                [
                    "First,",
                    "we",
                    "use",
                    "BERT's",
                    "middle",
                    "(7",
                    "th",
                    ")",
                    "output",
                    "layer,",
                    "using",
                    "the",
                    "embedding",
                    "of",
                    "the",
                    "initial",
                    "word",
                    "piece",
                    "for",
                    "each",
                    "word",
                    "as",
                    "input",
                    "to",
                    "the",
                    "classifiers."
                ],
                [
                    "Second,",
                    "we",
                    "used",
                    "the",
                    "middle",
                    "layer,",
                    "but",
                    "with",
                    "the",
                    "mean",
                    "vector",
                    "of",
                    "all",
                    "word",
                    "pieces",
                    "(this",
                    "is",
                    "the",
                    "method",
                    "we",
                    "used",
                    "in",
                    "all",
                    "previous",
                    "experiments)."
                ],
                [
                    "Third,",
                    "we",
                    "used",
                    "the",
                    "mean",
                    "value",
                    "of",
                    "the",
                    "final",
                    "(12",
                    "th",
                    ")",
                    "BERT",
                    "output",
                    "layer,",
                    "which",
                    "helped",
                    "van",
                    "Noord",
                    "et",
                    "al."
                ],
                [
                    "(",
                    "2020)",
                    "build",
                    "their",
                    "best",
                    "model,",
                    "yet",
                    "according",
                    "to",
                    "#TARGET_REF",
                    "random",
                    "errors,",
                    "we",
                    "did",
                    "five",
                    "trials",
                    "on",
                    "each",
                    "of",
                    "these",
                    "embedding",
                    "approaches",
                    "and",
                    "averaged",
                    "the",
                    "results."
                ],
                [
                    "Although",
                    "the",
                    "differences",
                    "are",
                    "rather",
                    "small,",
                    "the",
                    "mean",
                    "vector",
                    "of",
                    "the",
                    "middle",
                    "layer",
                    "seems",
                    "to",
                    "provide",
                    "the",
                    "best",
                    "scores",
                    "across",
                    "the",
                    "board."
                ],
                [
                    "Therefore,",
                    "we",
                    "stuck",
                    "to",
                    "this",
                    "setting",
                    "for",
                    "subsequent",
                    "experiments."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                2,
                2,
                0,
                0
            ]
        },
        "input": "sent0: Choice of BERT Output Layer and Wordpiece Embeddings We were interested in how the choice of BERT output layers and word piece embeddings impacts performance of our model.\n sent1: Hence, we did the following experiments with our base model, shown in Table 4.\n sent2: First, we use BERT's middle (7 th ) output layer, using the embedding of the initial word piece for each word as input to the classifiers.\n sent3: Second, we used the middle layer, but with the mean vector of all word pieces (this is the method we used in all previous experiments).\n sent4: Third, we used the mean value of the final (12 th ) BERT output layer, which helped van Noord et al.\n sent5: ( 2020) build their best model, yet according to #TARGET_REF random errors, we did five trials on each of these embedding approaches and averaged the results.\n sent6: Although the differences are rather small, the mean vector of the middle layer seems to provide the best scores across the board.\n sent7: Therefore, we stuck to this setting for subsequent experiments.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent4\", \"sent5\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "As",
                    "#TARGET_REF",
                    "showed,",
                    "they",
                    "might",
                    "associate",
                    "words",
                    "that",
                    "are",
                    "not",
                    "in",
                    "a",
                    "strong",
                    "direct",
                    "connection,",
                    "but",
                    "are",
                    "only",
                    "indirectly",
                    "related",
                    "(e.g."
                ],
                [
                    "religion",
                    "is",
                    "not",
                    "related",
                    "to",
                    "tree,",
                    "but",
                    "both",
                    "are",
                    "related",
                    "to",
                    "Christmas,",
                    "therefore",
                    "religion",
                    "could",
                    "be",
                    "a",
                    "clue",
                    "for",
                    "tree)."
                ]
            ],
            "context": [
                2,
                3
            ]
        },
        "input": "sent0: As #TARGET_REF showed, they might associate words that are not in a strong direct connection, but are only indirectly related (e.g.\n sent1: religion is not related to tree, but both are related to Christmas, therefore religion could be a clue for tree).\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "consider",
                    "z",
                    "as",
                    "a",
                    "disentangled",
                    "representation",
                    "for",
                    "x,",
                    "if",
                    "the",
                    "changes",
                    "in",
                    "single",
                    "latent",
                    "dimensions",
                    "of",
                    "z",
                    "are",
                    "sensitive",
                    "to",
                    "changes",
                    "in",
                    "single",
                    "generative",
                    "factors",
                    "of",
                    "x",
                    "while",
                    "being",
                    "relatively",
                    "invariant",
                    "to",
                    "changes",
                    "in",
                    "other",
                    "factors",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Several",
                    "probabilistic",
                    "models",
                    "are",
                    "designed",
                    "to",
                    "reveal",
                    "this",
                    "process,",
                    "here",
                    "we",
                    "look",
                    "at",
                    "some",
                    "of",
                    "the",
                    "most",
                    "widely",
                    "used",
                    "ones."
                ]
            ],
            "context": [
                2,
                0
            ]
        },
        "input": "sent0: We consider z as a disentangled representation for x, if the changes in single latent dimensions of z are sensitive to changes in single generative factors of x while being relatively invariant to changes in other factors #TARGET_REF .\n sent1: Several probabilistic models are designed to reveal this process, here we look at some of the most widely used ones.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Finally,",
                    "to",
                    "test",
                    "whether",
                    "the",
                    "MT",
                    "engine",
                    "was",
                    "better",
                    "prepared",
                    "to",
                    "address",
                    "Text",
                    "A",
                    "or",
                    "Text",
                    "B,",
                    "we",
                    "calculated",
                    "perplexity",
                    "and",
                    "out-of-vocabulary",
                    "(OOV)",
                    "words."
                ],
                [
                    "Perplexity",
                    "is",
                    "used",
                    "as",
                    "a",
                    "mea-surement",
                    "of",
                    "how",
                    "well",
                    "the",
                    "language",
                    "model",
                    "predicts",
                    "the",
                    "reference",
                    "translations."
                ],
                [
                    "The",
                    "smaller",
                    "the",
                    "perplexity,",
                    "the",
                    "more",
                    "and",
                    "longer",
                    "overlap",
                    "exists",
                    "between",
                    "the",
                    "reference",
                    "and",
                    "the",
                    "language",
                    "model",
                    "in",
                    "the",
                    "MT",
                    "system."
                ],
                [
                    "This",
                    "measurement",
                    "shows",
                    "that",
                    "the",
                    "MT",
                    "engine",
                    "is",
                    "better",
                    "suited",
                    "to",
                    "output",
                    "a",
                    "correct",
                    "version",
                    "for",
                    "Text",
                    "A",
                    "than",
                    "for",
                    "Text",
                    "B",
                    "(see",
                    "Table",
                    "10)."
                ],
                [
                    "Note",
                    "that",
                    "the",
                    "high",
                    "perplexity",
                    "values,",
                    "calculated",
                    "per",
                    "word,",
                    "are",
                    "in",
                    "line",
                    "with",
                    "those",
                    "reported",
                    "for",
                    "morphologically",
                    "rich",
                    "languages",
                    "(see",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Table",
                    "10."
                ],
                [
                    "Perplexity",
                    "calculated",
                    "on",
                    "5-grams."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                1,
                3,
                3
            ]
        },
        "input": "sent0: Finally, to test whether the MT engine was better prepared to address Text A or Text B, we calculated perplexity and out-of-vocabulary (OOV) words.\n sent1: Perplexity is used as a mea-surement of how well the language model predicts the reference translations.\n sent2: The smaller the perplexity, the more and longer overlap exists between the reference and the language model in the MT system.\n sent3: This measurement shows that the MT engine is better suited to output a correct version for Text A than for Text B (see Table 10).\n sent4: Note that the high perplexity values, calculated per word, are in line with those reported for morphologically rich languages (see #TARGET_REF .\n sent5: Table 10.\n sent6: Perplexity calculated on 5-grams.\n",
        "output": "{\"INFORMATION\": [\"sent4\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent5\", \"sent6\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Adopting",
                    "the",
                    "idiom",
                    "principle",
                    "#REF",
                    "to",
                    "produce",
                    "a",
                    "single",
                    "token",
                    "representation",
                    "for",
                    "MWEs",
                    "has",
                    "been",
                    "used",
                    "widely",
                    "within",
                    "static",
                    "embedding",
                    "distributional",
                    "semantic",
                    "models",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Within",
                    "contextualised",
                    "representation",
                    "models,",
                    "#REF",
                    "show",
                    "that",
                    "the",
                    "contextualised",
                    "representations",
                    "produced",
                    "by",
                    "context2vec",
                    "#REF",
                    "and",
                    "BERT",
                    "#REF",
                    "models",
                    "can",
                    "be",
                    "used",
                    "to",
                    "differentiate",
                    "between",
                    "idiomatic",
                    "and",
                    "literal",
                    "uses",
                    "of",
                    "MWEs."
                ],
                [
                    "However,",
                    "the",
                    "MWEs",
                    "are",
                    "only",
                    "represented",
                    "by",
                    "one",
                    "token",
                    "in",
                    "the",
                    "input,",
                    "before",
                    "being",
                    "broken",
                    "into",
                    "many",
                    "tokens",
                    "using",
                    "BERTs",
                    "word",
                    "piece",
                    "tokenizer."
                ],
                [
                    "Tayyar",
                    "Madabushi",
                    "et",
                    "al.,",
                    "2021",
                    "add",
                    "a",
                    "token",
                    "to",
                    "the",
                    "BERT",
                    "embedding",
                    "matrix",
                    "and",
                    "shows",
                    "that",
                    "this",
                    "method",
                    "improves",
                    "representations",
                    "through",
                    "increased",
                    "performance",
                    "on",
                    "their",
                    "proposed",
                    "STS",
                    "task."
                ],
                [
                    "The",
                    "embeddings",
                    "they",
                    "add",
                    "to",
                    "BERT",
                    "are",
                    "randomly",
                    "initialised,",
                    "however,",
                    "and",
                    "only",
                    "trained",
                    "during",
                    "the",
                    "fine-tun",
                    "step",
                    "on",
                    "limited",
                    "data."
                ],
                [
                    "Form",
                    "embeddings",
                    "are",
                    "then",
                    "learnt",
                    "using",
                    "trained",
                    "ngram",
                    "character",
                    "embeddings,",
                    "before",
                    "being",
                    "passed",
                    "with",
                    "a",
                    "context",
                    "into",
                    "a",
                    "BERT",
                    "model."
                ],
                [
                    "The",
                    "output",
                    "of",
                    "the",
                    "BERT",
                    "model",
                    "forms",
                    "the",
                    "embedding",
                    "for",
                    "that",
                    "specific",
                    "context."
                ],
                [
                    "To",
                    "incorporate",
                    "knowledge",
                    "from",
                    "many",
                    "contexts",
                    "an",
                    "attention",
                    "layer",
                    "is",
                    "applied",
                    "over",
                    "the",
                    "outputs",
                    "for",
                    "each",
                    "context",
                    "to",
                    "get",
                    "the",
                    "final",
                    "embedding."
                ],
                [
                    "There",
                    "exist",
                    "other",
                    "models",
                    "to",
                    "produce",
                    "effective",
                    "embeddings",
                    "from",
                    "a",
                    "small",
                    "number",
                    "of",
                    "contexts",
                    "#REF",
                    ",",
                    "however,",
                    "BERTRAM",
                    "is",
                    "the",
                    "only",
                    "model",
                    "that",
                    "is",
                    "non-bag-ofwords",
                    "and",
                    "incorporates",
                    "both",
                    "form",
                    "and",
                    "context",
                    "information",
                    "when",
                    "creating",
                    "the",
                    "embedding."
                ]
            ],
            "context": [
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Adopting the idiom principle #REF to produce a single token representation for MWEs has been used widely within static embedding distributional semantic models #TARGET_REF .\n sent1: Within contextualised representation models, #REF show that the contextualised representations produced by context2vec #REF and BERT #REF models can be used to differentiate between idiomatic and literal uses of MWEs.\n sent2: However, the MWEs are only represented by one token in the input, before being broken into many tokens using BERTs word piece tokenizer.\n sent3: Tayyar Madabushi et al., 2021 add a token to the BERT embedding matrix and shows that this method improves representations through increased performance on their proposed STS task.\n sent4: The embeddings they add to BERT are randomly initialised, however, and only trained during the fine-tun step on limited data.\n sent5: Form embeddings are then learnt using trained ngram character embeddings, before being passed with a context into a BERT model.\n sent6: The output of the BERT model forms the embedding for that specific context.\n sent7: To incorporate knowledge from many contexts an attention layer is applied over the outputs for each context to get the final embedding.\n sent8: There exist other models to produce effective embeddings from a small number of contexts #REF , however, BERTRAM is the only model that is non-bag-ofwords and incorporates both form and context information when creating the embedding.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Our",
                    "study",
                    "is",
                    "also",
                    "related",
                    "to",
                    "attribution",
                    "approaches,",
                    "which",
                    "aims",
                    "to",
                    "find",
                    "features",
                    "or",
                    "regions",
                    "of",
                    "input",
                    "that",
                    "are",
                    "important",
                    "for",
                    "tasks."
                ],
                [
                    "Different",
                    "types",
                    "of",
                    "techniques,",
                    "including",
                    "gradient-based",
                    "#REF",
                    ",",
                    "are",
                    "applied",
                    "for",
                    "reinforcement",
                    "learning",
                    "#REF",
                    ",",
                    "computer",
                    "vision",
                    "#REF",
                    ",",
                    "and",
                    "text",
                    "classification",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "While",
                    "these",
                    "works",
                    "focus",
                    "on",
                    "interpreting",
                    "model",
                    "behaviors,",
                    "we",
                    "aim",
                    "to",
                    "find",
                    "salient",
                    "words",
                    "beyond",
                    "input",
                    "and",
                    "utilize",
                    "them",
                    "as",
                    "action",
                    "representations."
                ]
            ],
            "context": [
                3,
                2,
                2
            ]
        },
        "input": "sent0: Our study is also related to attribution approaches, which aims to find features or regions of input that are important for tasks.\n sent1: Different types of techniques, including gradient-based #REF , are applied for reinforcement learning #REF , computer vision #REF , and text classification #TARGET_REF .\n sent2: While these works focus on interpreting model behaviors, we aim to find salient words beyond input and utilize them as action representations.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\", \"sent2\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "4."
                ],
                [
                    "For",
                    "French,",
                    "we",
                    "use",
                    "the",
                    "French",
                    "TimeBank",
                    "as",
                    "it",
                    "is",
                    "the",
                    "ISO-TimeML",
                    "annotated",
                    "reference",
                    "corpus",
                    "for",
                    "event",
                    "annotation",
                    "tasks",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "The",
                    "corpus",
                    "consists",
                    "of",
                    "16,208",
                    "tokens",
                    "and",
                    "2,100",
                    "event",
                    "mentions."
                ]
            ],
            "context": [
                0,
                1,
                1
            ]
        },
        "input": "sent0: 4.\n sent1: For French, we use the French TimeBank as it is the ISO-TimeML annotated reference corpus for event annotation tasks #TARGET_REF .\n sent2: The corpus consists of 16,208 tokens and 2,100 event mentions.\n",
        "output": "{\"INFORMATION\": [\"sent1\", \"sent2\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "It",
                    "is",
                    "again",
                    "relatively",
                    "recent",
                    "that",
                    "the",
                    "notion",
                    "of",
                    "MT",
                    "with",
                    "restricted",
                    "input",
                    "has",
                    "come",
                    "to",
                    "be",
                    "associated",
                    "with",
                    "'sublanguage'",
                    "(",
                    "#REF",
                    "),",
                    "as",
                    "opposed",
                    "to",
                    "the",
                    "case",
                    "previously,",
                    "when",
                    "it",
                    "was",
                    "typically",
                    "the",
                    "MT",
                    "system",
                    "which",
                    "dictated",
                    "the",
                    "restrictions",
                    "rather",
                    "than",
                    "vice",
                    "versa",
                    "(e.g."
                ],
                [
                    "#TARGET_REF",
                    ")."
                ],
                [
                    "In",
                    "the",
                    "system",
                    "described",
                    "here,",
                    "the",
                    "sublanguage",
                    "approach",
                    "gives",
                    "us",
                    "not",
                    "only",
                    "vocabulary",
                    "and",
                    "syntax,",
                    "but",
                    "also",
                    "the",
                    "contextual",
                    "and",
                    "domain",
                    "knowledge",
                    "employed",
                    "by",
                    "the",
                    "system."
                ],
                [
                    "And,",
                    "in",
                    "keeping",
                    "with",
                    "the",
                    "strong",
                    "corpus-based",
                    "approach,",
                    "these",
                    "knowledge",
                    "sources",
                    "are",
                    "derived",
                    "directly",
                    "from",
                    "an",
                    "analysis",
                    "of",
                    "corpora",
                    "(",
                    "#REF",
                    "),",
                    "not",
                    "from",
                    "some",
                    "linguist's",
                    "introspection,",
                    "as",
                    "is",
                    "the",
                    "case",
                    "in",
                    "conventional",
                    "rule-based",
                    "MT",
                    "systems."
                ],
                [
                    "An",
                    "additional",
                    "point",
                    "of",
                    "interest",
                    "in",
                    "this",
                    "research",
                    "is",
                    "the",
                    "contrastive",
                    "aspect,",
                    "since",
                    "our",
                    "approach",
                    "requires",
                    "us",
                    "to",
                    "make",
                    "an",
                    "explicitly",
                    "comparative",
                    "analysis",
                    "of",
                    "three",
                    "corpora",
                    "which,",
                    "it",
                    "should",
                    "be",
                    "stressed,",
                    "are",
                    "not",
                    "in",
                    "fact",
                    "parallel",
                    "corpora,",
                    "but",
                    "simply",
                    "collections",
                    "of",
                    "pragmatically",
                    "similar",
                    "material."
                ]
            ],
            "context": [
                3,
                1,
                0,
                3,
                0
            ]
        },
        "input": "sent0: It is again relatively recent that the notion of MT with restricted input has come to be associated with 'sublanguage' ( #REF ), as opposed to the case previously, when it was typically the MT system which dictated the restrictions rather than vice versa (e.g.\n sent1: #TARGET_REF ).\n sent2: In the system described here, the sublanguage approach gives us not only vocabulary and syntax, but also the contextual and domain knowledge employed by the system.\n sent3: And, in keeping with the strong corpus-based approach, these knowledge sources are derived directly from an analysis of corpora ( #REF ), not from some linguist's introspection, as is the case in conventional rule-based MT systems.\n sent4: An additional point of interest in this research is the contrastive aspect, since our approach requires us to make an explicitly comparative analysis of three corpora which, it should be stressed, are not in fact parallel corpora, but simply collections of pragmatically similar material.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "There",
                    "is",
                    "an",
                    "ever",
                    "increasing",
                    "need",
                    "for",
                    "labeled",
                    "datasets",
                    "for",
                    "machine",
                    "learning."
                ],
                [
                    "This",
                    "is",
                    "true",
                    "for",
                    "English",
                    "as",
                    "well",
                    "as",
                    "other,",
                    "often",
                    "under-resourced,",
                    "languages."
                ],
                [
                    "We",
                    "provide",
                    "a",
                    "cross-lingual",
                    "fine-grained",
                    "sentence-level",
                    "emotion",
                    "and",
                    "sentiment",
                    "dataset."
                ],
                [
                    "The",
                    "dataset",
                    "consists",
                    "of",
                    "parallel",
                    "manually",
                    "annotated",
                    "data",
                    "for",
                    "English",
                    "and",
                    "Finnish,",
                    "with",
                    "additional",
                    "parallel",
                    "datasets",
                    "of",
                    "varying",
                    "sizes",
                    "for",
                    "a",
                    "total",
                    "of",
                    "32",
                    "languages",
                    "created",
                    "by",
                    "annotation",
                    "projection."
                ],
                [
                    "We",
                    "use",
                    "Plutchik's",
                    "Wheel",
                    "of",
                    "Emotions",
                    "(anger,",
                    "anticipation,",
                    "disgust,",
                    "fear,",
                    "joy,",
                    "sadness,",
                    "surprise,",
                    "trust)",
                    "#TARGET_REF",
                    "as",
                    "our",
                    "annotation",
                    "scheme",
                    "with",
                    "the",
                    "addition",
                    "of",
                    "neutral",
                    "on",
                    "movie",
                    "subtitle",
                    "data",
                    "from",
                    "OPUS",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "sent0: There is an ever increasing need for labeled datasets for machine learning.\n sent1: This is true for English as well as other, often under-resourced, languages.\n sent2: We provide a cross-lingual fine-grained sentence-level emotion and sentiment dataset.\n sent3: The dataset consists of parallel manually annotated data for English and Finnish, with additional parallel datasets of varying sizes for a total of 32 languages created by annotation projection.\n sent4: We use Plutchik's Wheel of Emotions (anger, anticipation, disgust, fear, joy, sadness, surprise, trust) #TARGET_REF as our annotation scheme with the addition of neutral on movie subtitle data from OPUS #REF .\n",
        "output": "{\"INFORMATION\": [\"sent4\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Word",
                    "co-occurrence",
                    "probabilities",
                    "are",
                    "a",
                    "key",
                    "ingredient",
                    "in",
                    "usage-based",
                    "cognitive",
                    "models",
                    "of",
                    "language."
                ],
                [
                    "By",
                    "word",
                    "co-occurrence",
                    "probabilities,",
                    "I",
                    "mean",
                    "the",
                    "probability",
                    "of",
                    "a",
                    "word",
                    "w",
                    "given",
                    "some",
                    "other",
                    "single",
                    "word",
                    "c,",
                    "p(w",
                    "|",
                    "c),",
                    "where",
                    "words",
                    "w",
                    "and",
                    "c",
                    "have",
                    "some",
                    "specific",
                    "relationship,",
                    "for",
                    "example",
                    "adjectives",
                    "that",
                    "attributively",
                    "modify",
                    "nouns",
                    "or",
                    "nouns",
                    "serving",
                    "as",
                    "direct",
                    "objects",
                    "of",
                    "verbs",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                3,
                1
            ]
        },
        "input": "sent0: Word co-occurrence probabilities are a key ingredient in usage-based cognitive models of language.\n sent1: By word co-occurrence probabilities, I mean the probability of a word w given some other single word c, p(w | c), where words w and c have some specific relationship, for example adjectives that attributively modify nouns or nouns serving as direct objects of verbs #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "SemEval-2021",
                    "Task-5,",
                    "#REF",
                    "provide",
                    "a",
                    "dataset",
                    "of",
                    "10k",
                    "English",
                    "texts",
                    "filtered",
                    "from",
                    "Civil",
                    "Comments",
                    "#REF",
                    "dataset."
                ],
                [
                    "Each",
                    "text",
                    "is",
                    "crowd-annotated",
                    "with",
                    "character",
                    "offsets",
                    "that",
                    "make",
                    "the",
                    "text",
                    "toxic."
                ],
                [
                    "The",
                    "task",
                    "is",
                    "to",
                    "predict",
                    "these",
                    "character",
                    "offsets",
                    "given",
                    "the",
                    "text."
                ],
                [
                    "The",
                    "work",
                    "presented",
                    "in",
                    "this",
                    "paper",
                    "aims",
                    "to",
                    "provide",
                    "a",
                    "comprehensive",
                    "analysis",
                    "of",
                    "simple",
                    "Token",
                    "Classification",
                    "(TC)",
                    "and",
                    "Span",
                    "Prediction",
                    "(SP)",
                    "methods",
                    "across",
                    "multiple",
                    "BERT-based",
                    "models",
                    "-BERT",
                    "#TARGET_REF",
                    ",",
                    "RoBERTa",
                    "#REF",
                    "and",
                    "SpanBERT",
                    "#REF",
                    "."
                ],
                [
                    "Additionally,",
                    "we",
                    "experiment",
                    "with",
                    "a",
                    "few",
                    "hybrid",
                    "approaches",
                    "-Multi-Span",
                    "(MSP),",
                    "where",
                    "the",
                    "model",
                    "is",
                    "trained",
                    "on",
                    "multiple",
                    "spans",
                    "simultaneously,",
                    "Span+Token",
                    "(SP-TC),",
                    "where",
                    "the",
                    "model",
                    "is",
                    "trained",
                    "on",
                    "both",
                    "kinds",
                    "of",
                    "tasks",
                    "simultaneously,",
                    "LSTM-CRF",
                    "(LC),",
                    "which",
                    "uses",
                    "a",
                    "LSTM",
                    "and",
                    "CRF",
                    "layer",
                    "on",
                    "top",
                    "of",
                    "BERT-based",
                    "models,",
                    "and",
                    "a",
                    "combination",
                    "of",
                    "predicted",
                    "offsets",
                    "for",
                    "above",
                    "techniques",
                    "using",
                    "union/intersection."
                ],
                [
                    "In",
                    "Section",
                    "2,",
                    "we",
                    "perform",
                    "a",
                    "compendious",
                    "literature",
                    "survey."
                ],
                [
                    "Section",
                    "3",
                    "elucidates",
                    "our",
                    "approach,",
                    "including",
                    "the",
                    "modelling",
                    "aspect,",
                    "the",
                    "various",
                    "variants",
                    "of",
                    "the",
                    "base",
                    "model,",
                    "and",
                    "the",
                    "different",
                    "Hybrid",
                    "Systems."
                ],
                [
                    "In",
                    "Section",
                    "4,",
                    "we",
                    "describe",
                    "our",
                    "experimental",
                    "setup",
                    "and",
                    "hyperparameters",
                    "used",
                    "for",
                    "our",
                    "methods."
                ],
                [
                    "Lastly,",
                    "in",
                    "Section",
                    "5",
                    "we",
                    "analyze",
                    "our",
                    "results",
                    "and",
                    "perform",
                    "ablative",
                    "analysis",
                    "on",
                    "our",
                    "systems."
                ]
            ],
            "context": [
                0,
                0,
                0,
                3,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: In SemEval-2021 Task-5, #REF provide a dataset of 10k English texts filtered from Civil Comments #REF dataset.\n sent1: Each text is crowd-annotated with character offsets that make the text toxic.\n sent2: The task is to predict these character offsets given the text.\n sent3: The work presented in this paper aims to provide a comprehensive analysis of simple Token Classification (TC) and Span Prediction (SP) methods across multiple BERT-based models -BERT #TARGET_REF , RoBERTa #REF and SpanBERT #REF .\n sent4: Additionally, we experiment with a few hybrid approaches -Multi-Span (MSP), where the model is trained on multiple spans simultaneously, Span+Token (SP-TC), where the model is trained on both kinds of tasks simultaneously, LSTM-CRF (LC), which uses a LSTM and CRF layer on top of BERT-based models, and a combination of predicted offsets for above techniques using union/intersection.\n sent5: In Section 2, we perform a compendious literature survey.\n sent6: Section 3 elucidates our approach, including the modelling aspect, the various variants of the base model, and the different Hybrid Systems.\n sent7: In Section 4, we describe our experimental setup and hyperparameters used for our methods.\n sent8: Lastly, in Section 5 we analyze our results and perform ablative analysis on our systems.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "There",
                    "is",
                    "a",
                    "growing",
                    "trend",
                    "in",
                    "using",
                    "adversarial",
                    "models",
                    "for",
                    "data",
                    "creation",
                    "to",
                    "make",
                    "the",
                    "dataset",
                    "more",
                    "challenging",
                    "or",
                    "discard",
                    "examples",
                    "that",
                    "can",
                    "be",
                    "solved",
                    "using",
                    "surface",
                    "cues",
                    "#REF",
                    "."
                ],
                [
                    "Quoref",
                    "is",
                    "also",
                    "created",
                    "using",
                    "an",
                    "adversarial",
                    "data",
                    "collection",
                    "method",
                    "to",
                    "discard",
                    "examples",
                    "that",
                    "can",
                    "be",
                    "solved",
                    "using",
                    "simple",
                    "lexical",
                    "cues."
                ],
                [
                    "The",
                    "assumption",
                    "is",
                    "that",
                    "it",
                    "is",
                    "hard",
                    "to",
                    "avoid",
                    "simple",
                    "lexical",
                    "cues",
                    "by",
                    "which",
                    "the",
                    "model",
                    "can",
                    "answer",
                    "questions",
                    "without",
                    "coreference",
                    "reasoning."
                ],
                [
                    "Therefore,",
                    "an",
                    "adversarial",
                    "model",
                    "(A)",
                    "is",
                    "used",
                    "to",
                    "discard",
                    "examples",
                    "that",
                    "contain",
                    "such",
                    "lexical",
                    "cues."
                ],
                [
                    "While",
                    "this",
                    "adversarial",
                    "filtering",
                    "removes",
                    "examples",
                    "that",
                    "are",
                    "easy",
                    "to",
                    "solve",
                    "by",
                    "A,",
                    "it",
                    "does",
                    "not",
                    "ensure",
                    "that",
                    "the",
                    "remaining",
                    "examples",
                    "do",
                    "not",
                    "contain",
                    "shortcuts",
                    "that",
                    "are",
                    "not",
                    "explored",
                    "by",
                    "A."
                ],
                [
                    "First,",
                    "the",
                    "adversarial",
                    "model",
                    "in",
                    "Quoref",
                    "is",
                    "trained",
                    "on",
                    "another",
                    "dataset,",
                    "i.e.,",
                    "SQuAD."
                ],
                [
                    "Thus,",
                    "the",
                    "failure",
                    "of",
                    "A",
                    "on",
                    "Quoref",
                    "examples",
                    "may",
                    "be",
                    "due",
                    "to",
                    "(1)",
                    "Quoref",
                    "having",
                    "different",
                    "lexical",
                    "cues",
                    "than",
                    "those",
                    "in",
                    "SQuAD,",
                    "or",
                    "(2)",
                    "domain",
                    "shift."
                ],
                [
                    "Second,",
                    "and",
                    "more",
                    "importantly,",
                    "as",
                    "argued",
                    "by",
                    "#TARGET_REF",
                    ",",
                    "making",
                    "the",
                    "task",
                    "challenging",
                    "by",
                    "focusing",
                    "on",
                    "examples",
                    "that",
                    "are",
                    "more",
                    "difficult",
                    "for",
                    "existing",
                    "models",
                    "is",
                    "not",
                    "a",
                    "solution",
                    "for",
                    "more",
                    "useful",
                    "reading",
                    "comprehension."
                ],
                [
                    "7",
                    "We",
                    "instead",
                    "propose",
                    "a",
                    "methodology",
                    "for",
                    "creating",
                    "question-answer",
                    "pairs",
                    "as",
                    "follows:"
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                3,
                0,
                0,
                1,
                2
            ]
        },
        "input": "sent0: There is a growing trend in using adversarial models for data creation to make the dataset more challenging or discard examples that can be solved using surface cues #REF .\n sent1: Quoref is also created using an adversarial data collection method to discard examples that can be solved using simple lexical cues.\n sent2: The assumption is that it is hard to avoid simple lexical cues by which the model can answer questions without coreference reasoning.\n sent3: Therefore, an adversarial model (A) is used to discard examples that contain such lexical cues.\n sent4: While this adversarial filtering removes examples that are easy to solve by A, it does not ensure that the remaining examples do not contain shortcuts that are not explored by A.\n sent5: First, the adversarial model in Quoref is trained on another dataset, i.e., SQuAD.\n sent6: Thus, the failure of A on Quoref examples may be due to (1) Quoref having different lexical cues than those in SQuAD, or (2) domain shift.\n sent7: Second, and more importantly, as argued by #TARGET_REF , making the task challenging by focusing on examples that are more difficult for existing models is not a solution for more useful reading comprehension.\n sent8: 7 We instead propose a methodology for creating question-answer pairs as follows:\n",
        "output": "{\"INFORMATION\": [\"sent7\"], \"PERCEPTION\": [\"sent8\"], \"BACKGROUND\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "More",
                    "recently,",
                    "researchers",
                    "have",
                    "noted",
                    "that",
                    "the",
                    "linguistic",
                    "features",
                    "of",
                    "a",
                    "comment",
                    "alone",
                    "may",
                    "not",
                    "be",
                    "sufficient",
                    "to",
                    "classify",
                    "it",
                    "as",
                    "abusive",
                    "or",
                    "not."
                ],
                [
                    "Information",
                    "of",
                    "the",
                    "user",
                    "who",
                    "posted",
                    "the",
                    "comment,",
                    "and",
                    "of",
                    "the",
                    "surrounding",
                    "social",
                    "community",
                    "of",
                    "that",
                    "user,",
                    "further",
                    "provides",
                    "valuable",
                    "insights",
                    "into",
                    "the",
                    "abusiveness",
                    "of",
                    "the",
                    "comment."
                ],
                [
                    "An",
                    "example",
                    "of",
                    "this",
                    "is",
                    "the",
                    "study",
                    "by",
                    "#TARGET_REF",
                    ",",
                    "which",
                    "mapped",
                    "the",
                    "locations",
                    "of",
                    "racist",
                    "tweets",
                    "in",
                    "response",
                    "to",
                    "President",
                    "Obama's",
                    "re-election",
                    "to",
                    "show",
                    "that",
                    "such",
                    "tweets",
                    "were",
                    "not",
                    "uniformly",
                    "distributed",
                    "across",
                    "the",
                    "United",
                    "States",
                    "but",
                    "instead",
                    "came",
                    "from",
                    "specific",
                    "geographical",
                    "communities",
                    "of",
                    "users."
                ],
                [
                    "Other",
                    "works",
                    "have",
                    "also",
                    "shown",
                    "how",
                    "users",
                    "on",
                    "online",
                    "platforms",
                    "organize",
                    "into",
                    "communities",
                    "based",
                    "on",
                    "factors",
                    "such",
                    "as",
                    "shared",
                    "beliefs,",
                    "stereotypes,",
                    "linguistic",
                    "norms,",
                    "or",
                    "geographical",
                    "propinquity",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                2,
                2,
                1,
                0
            ]
        },
        "input": "sent0: More recently, researchers have noted that the linguistic features of a comment alone may not be sufficient to classify it as abusive or not.\n sent1: Information of the user who posted the comment, and of the surrounding social community of that user, further provides valuable insights into the abusiveness of the comment.\n sent2: An example of this is the study by #TARGET_REF , which mapped the locations of racist tweets in response to President Obama's re-election to show that such tweets were not uniformly distributed across the United States but instead came from specific geographical communities of users.\n sent3: Other works have also shown how users on online platforms organize into communities based on factors such as shared beliefs, stereotypes, linguistic norms, or geographical propinquity #REF .\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [\"sent0\", \"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Training",
                    "Methods",
                    "of",
                    "Seq2Seq",
                    "Models",
                    "In",
                    "order",
                    "to",
                    "align",
                    "the",
                    "training",
                    "objective",
                    "and",
                    "evaluation",
                    "metric,",
                    "structured",
                    "losses",
                    "have",
                    "been",
                    "used",
                    "for",
                    "the",
                    "Seq2Seq",
                    "model",
                    "training."
                ],
                [
                    "Among",
                    "them,",
                    "marginbased",
                    "losses",
                    "#REF",
                    ",",
                    "which",
                    "require",
                    "the",
                    "model",
                    "to",
                    "assign",
                    "higher",
                    "probability",
                    "to",
                    "the",
                    "better",
                    "output,",
                    "are",
                    "a",
                    "major",
                    "category."
                ],
                [
                    "Many",
                    "margin-based",
                    "losses",
                    "used",
                    "in",
                    "modern",
                    "seq2seq",
                    "models",
                    "#TARGET_REF",
                    "assume",
                    "a",
                    "deterministic",
                    "(one-point)",
                    "distribution:",
                    "a",
                    "model",
                    "can",
                    "achieve",
                    "zero",
                    "loss",
                    "if",
                    "it",
                    "can",
                    "assign",
                    "a",
                    "much",
                    "higher",
                    "probability",
                    "to",
                    "the",
                    "(pseudo)-reference,",
                    "regardless",
                    "of",
                    "relative",
                    "comparisons",
                    "of",
                    "other",
                    "candidate",
                    "summaries."
                ],
                [
                    "By",
                    "contrast,",
                    "our",
                    "method",
                    "has",
                    "a",
                    "non-deterministic",
                    "assumption",
                    "(Eq."
                ],
                [
                    "7),",
                    "which",
                    "focuses",
                    "on",
                    "the",
                    "pair-wise",
                    "ranking",
                    "of",
                    "a",
                    "set",
                    "of",
                    "candidate",
                    "summaries."
                ]
            ],
            "context": [
                3,
                3,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Training Methods of Seq2Seq Models In order to align the training objective and evaluation metric, structured losses have been used for the Seq2Seq model training.\n sent1: Among them, marginbased losses #REF , which require the model to assign higher probability to the better output, are a major category.\n sent2: Many margin-based losses used in modern seq2seq models #TARGET_REF assume a deterministic (one-point) distribution: a model can achieve zero loss if it can assign a much higher probability to the (pseudo)-reference, regardless of relative comparisons of other candidate summaries.\n sent3: By contrast, our method has a non-deterministic assumption (Eq.\n sent4: 7), which focuses on the pair-wise ranking of a set of candidate summaries.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "have",
                    "also",
                    "collated",
                    "eye",
                    "tracking",
                    "measurements",
                    "such",
                    "as",
                    "total",
                    "fixation",
                    "counts,",
                    "average",
                    "fixation",
                    "duration",
                    "and",
                    "percentage",
                    "change",
                    "in",
                    "pupil",
                    "dilation,",
                    "all",
                    "of",
                    "which",
                    "are",
                    "shown",
                    "to",
                    "be",
                    "indicators",
                    "of",
                    "cognitive",
                    "load",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                3
            ]
        },
        "input": "sent0: We have also collated eye tracking measurements such as total fixation counts, average fixation duration and percentage change in pupil dilation, all of which are shown to be indicators of cognitive load #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "Symlink",
                    "track",
                    "at",
                    "SemEval-2022",
                    "received",
                    "4",
                    "system",
                    "description",
                    "paper",
                    "submissions",
                    "presented",
                    "in",
                    "Table",
                    "2."
                ],
                [
                    "Overall,",
                    "all",
                    "submitted",
                    "systems",
                    "are",
                    "based",
                    "on",
                    "BERT",
                    "architecture",
                    "#REF",
                    "."
                ],
                [
                    "Among",
                    "those,",
                    "two",
                    "out",
                    "of",
                    "four",
                    "systems",
                    "use",
                    "SciBERT",
                    "#REF",
                    ",",
                    "while",
                    "two",
                    "remaining",
                    "systems",
                    "use",
                    "other",
                    "variants",
                    "of",
                    "BERT",
                    "such",
                    "as",
                    "original",
                    "BERT",
                    "#TARGET_REF",
                    "and",
                    "mBERT",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                3,
                3,
                3
            ]
        },
        "input": "sent0: The Symlink track at SemEval-2022 received 4 system description paper submissions presented in Table 2.\n sent1: Overall, all submitted systems are based on BERT architecture #REF .\n sent2: Among those, two out of four systems use SciBERT #REF , while two remaining systems use other variants of BERT such as original BERT #TARGET_REF and mBERT #REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\", \"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Word",
                    "associations",
                    "have",
                    "been",
                    "a",
                    "subject",
                    "of",
                    "active",
                    "research",
                    "for",
                    "a",
                    "long",
                    "time",
                    "in",
                    "cognitive",
                    "science",
                    "and",
                    "psycholinguistics",
                    "for",
                    "various",
                    "reasons."
                ],
                [
                    "They",
                    "were",
                    "used",
                    "to",
                    "study",
                    "mental",
                    "functioning,",
                    "memory,",
                    "and",
                    "certain",
                    "diseases."
                ],
                [
                    "Word",
                    "associations",
                    "were",
                    "also",
                    "applied",
                    "for",
                    "modeling",
                    "the",
                    "cognitive",
                    "lexicon",
                    "and",
                    "some",
                    "linguistic",
                    "processes",
                    "(summarized",
                    "by",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                2,
                3,
                3
            ]
        },
        "input": "sent0: Word associations have been a subject of active research for a long time in cognitive science and psycholinguistics for various reasons.\n sent1: They were used to study mental functioning, memory, and certain diseases.\n sent2: Word associations were also applied for modeling the cognitive lexicon and some linguistic processes (summarized by #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Our",
                    "LTR",
                    "features",
                    "fall",
                    "into",
                    "four",
                    "categories:",
                    "term-based,",
                    "score-based,",
                    "proximity-based,",
                    "and",
                    "translation-based."
                ],
                [
                    "These",
                    "features",
                    "are",
                    "inspired",
                    "by",
                    "previous",
                    "studies",
                    "on",
                    "LTR",
                    "#TARGET_REF",
                    "and",
                    "summarized",
                    "in",
                    "Table",
                    "1."
                ],
                [
                    "An",
                    "enumeration",
                    "of",
                    "all",
                    "features",
                    "is",
                    "presented",
                    "in",
                    "Appendix",
                    "A."
                ]
            ],
            "context": [
                2,
                2,
                3
            ]
        },
        "input": "sent0: Our LTR features fall into four categories: term-based, score-based, proximity-based, and translation-based.\n sent1: These features are inspired by previous studies on LTR #TARGET_REF and summarized in Table 1.\n sent2: An enumeration of all features is presented in Appendix A.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\", \"sent1\"], \"BACKGROUND\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Pretraining",
                    "Corpus:",
                    "Following",
                    "the",
                    "E2E",
                    "pretraining",
                    "strategy",
                    "#TARGET_REF",
                    ",",
                    "we",
                    "take",
                    "indomain",
                    "datasets:",
                    "MSCOCO",
                    "#REF",
                    "and",
                    "VG",
                    "#REF",
                    "as",
                    "pretraining",
                    "datasets",
                    "since",
                    "it",
                    "is",
                    "widely",
                    "used",
                    "in",
                    "literature."
                ],
                [
                    "In",
                    "total,",
                    "two",
                    "datasets",
                    "comprise",
                    "about",
                    "200K",
                    "images",
                    "and",
                    "5.6M",
                    "image-text",
                    "pairs,",
                    "where",
                    "each",
                    "image",
                    "is",
                    "associated",
                    "with",
                    "multiple",
                    "captions."
                ],
                [
                    "VQA2.0",
                    "R@1",
                    "/",
                    "R@5/",
                    "R@10",
                    "R@1",
                    "/",
                    "R@5",
                    "/",
                    "R@10",
                    "val",
                    "/",
                    "test",
                    "dev",
                    "/",
                    "test-p",
                    "test-dev",
                    "/",
                    "-std",
                    "two-step",
                    "pretraining",
                    "ViLBert",
                    "#REF",
                    "MSCOCO-TR(5K)",
                    "VCR",
                    "R@1",
                    "/",
                    "R@5",
                    "/",
                    "R@10",
                    "R@1",
                    "/",
                    "R@5",
                    "/",
                    "R@10",
                    "R@1",
                    "/",
                    "R@5",
                    "/",
                    "R@10",
                    "R@1",
                    "/",
                    "R@5",
                    "/",
                    "R@10",
                    "Q→A",
                    "QA→R",
                    "Q→AR",
                    "two-step",
                    "pretraining",
                    "Unicoder-VL",
                    "#REF",
                    "Implementation",
                    "Details:",
                    "We",
                    "follow",
                    "BERT",
                    "to",
                    "tokenize",
                    "caption",
                    "into",
                    "word",
                    "tokens",
                    "by",
                    "using",
                    "Word-Piece,",
                    "and",
                    "resize",
                    "the",
                    "image",
                    "into",
                    "(800,",
                    "1333)",
                    "as",
                    "prior",
                    "works."
                ],
                [
                    "For",
                    "model",
                    "architecture,",
                    "a",
                    "widely-used",
                    "ResNet101",
                    "for",
                    "visual",
                    "encoding",
                    "and",
                    "12-layer",
                    "Transformer",
                    "for",
                    "multi-modal",
                    "fusion",
                    "are",
                    "adopted",
                    "for",
                    "a",
                    "fair",
                    "comparison."
                ],
                [
                    "Both",
                    "networks",
                    "are",
                    "initialized",
                    "with",
                    "Im-ageNet",
                    "and",
                    "BERT",
                    "pretrained",
                    "parameters."
                ],
                [
                    "Besides,",
                    "following",
                    "the",
                    "majority",
                    "of",
                    "two-step",
                    "methods,",
                    "we",
                    "apply",
                    "the",
                    "widely-used",
                    "object",
                    "detector",
                    "BUTD",
                    "#REF",
                    "to",
                    "generate",
                    "object",
                    "proposals",
                    "as",
                    "well",
                    "as",
                    "their",
                    "RoI",
                    "embeddings",
                    "as",
                    "our",
                    "supervision."
                ]
            ],
            "context": [
                2,
                3,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Pretraining Corpus: Following the E2E pretraining strategy #TARGET_REF , we take indomain datasets: MSCOCO #REF and VG #REF as pretraining datasets since it is widely used in literature.\n sent1: In total, two datasets comprise about 200K images and 5.6M image-text pairs, where each image is associated with multiple captions.\n sent2: VQA2.0 R@1 / R@5/ R@10 R@1 / R@5 / R@10 val / test dev / test-p test-dev / -std two-step pretraining ViLBert #REF MSCOCO-TR(5K) VCR R@1 / R@5 / R@10 R@1 / R@5 / R@10 R@1 / R@5 / R@10 R@1 / R@5 / R@10 Q→A QA→R Q→AR two-step pretraining Unicoder-VL #REF Implementation Details: We follow BERT to tokenize caption into word tokens by using Word-Piece, and resize the image into (800, 1333) as prior works.\n sent3: For model architecture, a widely-used ResNet101 for visual encoding and 12-layer Transformer for multi-modal fusion are adopted for a fair comparison.\n sent4: Both networks are initialized with Im-ageNet and BERT pretrained parameters.\n sent5: Besides, following the majority of two-step methods, we apply the widely-used object detector BUTD #REF to generate object proposals as well as their RoI embeddings as our supervision.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "EEG",
                    "signals",
                    "were",
                    "extracted",
                    "from",
                    "-100",
                    "to",
                    "700",
                    "ms",
                    "relative",
                    "to",
                    "word",
                    "onset."
                ],
                [
                    "For",
                    "baseline",
                    "correction,",
                    "we",
                    "subtracted",
                    "the",
                    "channel-wise",
                    "mean",
                    "from",
                    "-100",
                    "ms",
                    "to",
                    "0",
                    "ms",
                    "from",
                    "the",
                    "evoked",
                    "post-stimulus",
                    "EEG",
                    "response",
                    "([0",
                    "700]",
                    "ms",
                    "separately",
                    "for",
                    "each",
                    "word,",
                    "see",
                    "Figure",
                    "1)."
                ],
                [
                    "EEG",
                    "data",
                    "were",
                    "spatially",
                    "multivariate",
                    "noise",
                    "normalised",
                    "using",
                    "the",
                    "noise",
                    "covariance",
                    "matrix",
                    "estimated",
                    "separately",
                    "for",
                    "each",
                    "target",
                    "class",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Each",
                    "EEG",
                    "trial",
                    "was",
                    "annotated",
                    "with",
                    "the",
                    "gold",
                    "part",
                    "of",
                    "speech",
                    "tags",
                    "of",
                    "the",
                    "current",
                    "and",
                    "subsequent",
                    "words,",
                    "their",
                    "word",
                    "lengths,",
                    "and",
                    "Zipf-logarithmic",
                    "frequency",
                    "scores",
                    "from",
                    "the",
                    "Python",
                    "package",
                    "WordFreq",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                0,
                0,
                1,
                0
            ]
        },
        "input": "sent0: EEG signals were extracted from -100 to 700 ms relative to word onset.\n sent1: For baseline correction, we subtracted the channel-wise mean from -100 ms to 0 ms from the evoked post-stimulus EEG response ([0 700] ms separately for each word, see Figure 1).\n sent2: EEG data were spatially multivariate noise normalised using the noise covariance matrix estimated separately for each target class #TARGET_REF .\n sent3: Each EEG trial was annotated with the gold part of speech tags of the current and subsequent words, their word lengths, and Zipf-logarithmic frequency scores from the Python package WordFreq #REF .\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "This",
                    "section",
                    "describes",
                    "how",
                    "we",
                    "prepare",
                    "the",
                    "collapsed",
                    "single",
                    "sequence",
                    "composed",
                    "of",
                    "s",
                    "i",
                    "in",
                    "Eq."
                ],
                [
                    "(",
                    "8)."
                ],
                [
                    "We",
                    "explain",
                    "this",
                    "data",
                    "preparation",
                    "with",
                    "both",
                    "English",
                    "(TED-LIUM",
                    "release",
                    "2",
                    "(TEDLIUM2)",
                    "#TARGET_REF",
                    ")",
                    "and",
                    "Japanese",
                    "(corpus",
                    "of",
                    "spontaneous",
                    "Japanese",
                    "(CSJ)",
                    "#REF",
                    "))",
                    "data",
                    "as",
                    "an",
                    "example."
                ],
                [
                    "The",
                    "sequence",
                    "type",
                    "includes",
                    "the",
                    "graphemic",
                    "and",
                    "phonemic",
                    "transcripts",
                    "1",
                    ",",
                    "as",
                    "well",
                    "as",
                    "the",
                    "POS",
                    "tags."
                ]
            ],
            "context": [
                3,
                3,
                2,
                3
            ]
        },
        "input": "sent0: This section describes how we prepare the collapsed single sequence composed of s i in Eq.\n sent1: ( 8).\n sent2: We explain this data preparation with both English (TED-LIUM release 2 (TEDLIUM2) #TARGET_REF ) and Japanese (corpus of spontaneous Japanese (CSJ) #REF )) data as an example.\n sent3: The sequence type includes the graphemic and phonemic transcripts 1 , as well as the POS tags.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": [\"sent0\", \"sent1\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Base",
                    "models."
                ],
                [
                    "For",
                    "BoW,",
                    "CNN,",
                    "and",
                    "LSTM,",
                    "all",
                    "models",
                    "use",
                    "pre-trained",
                    "GloVe",
                    "embeddings",
                    "#TARGET_REF",
                    ",",
                    "and",
                    "have",
                    "one",
                    "hidden",
                    "layer",
                    "of",
                    "the",
                    "corresponding",
                    "type",
                    "with",
                    "100",
                    "hidden",
                    "size."
                ],
                [
                    "Similar",
                    "to",
                    "the",
                    "baseline",
                    "performance",
                    "reported",
                    "in",
                    "GLUE",
                    "#REF",
                    ",",
                    "our",
                    "trained",
                    "models",
                    "have",
                    "an",
                    "evaluation",
                    "accuracy",
                    "of",
                    "81.4%,",
                    "82.5%,",
                    "and",
                    "81.7%,",
                    "respectively."
                ],
                [
                    "For",
                    "attention-based",
                    "models,",
                    "we",
                    "train",
                    "a",
                    "3-layer",
                    "Transformer",
                    "(the",
                    "largest",
                    "size",
                    "in",
                    "#REF",
                    ")",
                    "and",
                    "fine-tune",
                    "a",
                    "pre-trained",
                    "bertbase-uncased",
                    "from",
                    "HuggingFace",
                    "#REF",
                    "."
                ],
                [
                    "The",
                    "Transformer",
                    "uses",
                    "4",
                    "attention",
                    "heads",
                    "and",
                    "64",
                    "hidden",
                    "size,",
                    "and",
                    "obtains",
                    "82.1%",
                    "accuracy."
                ]
            ],
            "context": [
                0,
                3,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Base models.\n sent1: For BoW, CNN, and LSTM, all models use pre-trained GloVe embeddings #TARGET_REF , and have one hidden layer of the corresponding type with 100 hidden size.\n sent2: Similar to the baseline performance reported in GLUE #REF , our trained models have an evaluation accuracy of 81.4%, 82.5%, and 81.7%, respectively.\n sent3: For attention-based models, we train a 3-layer Transformer (the largest size in #REF ) and fine-tune a pre-trained bertbase-uncased from HuggingFace #REF .\n sent4: The Transformer uses 4 attention heads and 64 hidden size, and obtains 82.1% accuracy.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "DeKo",
                    "rules",
                    "can",
                    "only",
                    "work",
                    "if",
                    "they",
                    "can",
                    "refer",
                    "to",
                    "detailed",
                    "information",
                    "on",
                    "lexical",
                    "items",
                    "-therefore",
                    "the",
                    "DeKo",
                    "team",
                    "and",
                    "other",
                    "researchers",
                    "at",
                    "the",
                    "IMS",
                    "in",
                    "Stuttgart",
                    "developed",
                    "a",
                    "highly",
                    "flexible",
                    "lexicon",
                    "concept",
                    "where",
                    "different",
                    "kinds",
                    "of",
                    "information",
                    "are",
                    "stored",
                    "together",
                    "with",
                    "morphological",
                    "elements",
                    "(see",
                    "#TARGET_REF",
                    "for",
                    "more",
                    "details)."
                ],
                [
                    "At",
                    "the",
                    "moment",
                    "the",
                    "relevant",
                    "information",
                    "is",
                    "still",
                    "being",
                    "collected",
                    "and",
                    "encoded",
                    "into",
                    "the",
                    "IMSLex."
                ],
                [
                    "Therefore,",
                    "the",
                    "DeKo",
                    "rules",
                    "as",
                    "they",
                    "stand",
                    "now",
                    "are",
                    "much",
                    "less",
                    "specific",
                    "than",
                    "they",
                    "should",
                    "be."
                ]
            ],
            "context": [
                1,
                0,
                3
            ]
        },
        "input": "sent0: The DeKo rules can only work if they can refer to detailed information on lexical items -therefore the DeKo team and other researchers at the IMS in Stuttgart developed a highly flexible lexicon concept where different kinds of information are stored together with morphological elements (see #TARGET_REF for more details).\n sent1: At the moment the relevant information is still being collected and encoded into the IMSLex.\n sent2: Therefore, the DeKo rules as they stand now are much less specific than they should be.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Simple",
                    "definition",
                    "in",
                    "OALD",
                    "a",
                    "notice",
                    "or",
                    "announcement",
                    "in",
                    "a",
                    "public",
                    "medium",
                    "promoting",
                    "a",
                    "product,",
                    "service,",
                    "or",
                    "event",
                    "or",
                    "publicizing",
                    "a",
                    "job",
                    "vacancy."
                ],
                [
                    "current",
                    "context",
                    "because",
                    "of",
                    "the",
                    "cognitively",
                    "inaccurate",
                    "nature",
                    "of",
                    "discrete",
                    "sense",
                    "boundaries",
                    "#REF",
                    "."
                ],
                [
                    "Secondly,",
                    "the",
                    "predefined",
                    "inventories",
                    "need",
                    "to",
                    "be",
                    "updated",
                    "manually",
                    "by",
                    "lexicographers,",
                    "which",
                    "is",
                    "time-consuming",
                    "and",
                    "causes",
                    "dictionaries",
                    "to",
                    "lag",
                    "behind",
                    "the",
                    "ever-changing",
                    "language",
                    "usage."
                ],
                [
                    "Different",
                    "from",
                    "previous",
                    "work",
                    "#TARGET_REF",
                    "that",
                    "focused",
                    "only",
                    "on",
                    "how",
                    "to",
                    "generate",
                    "definitions,",
                    "we",
                    "further",
                    "propose",
                    "a",
                    "novel",
                    "task",
                    "of",
                    "Simple",
                    "Definition",
                    "Generation",
                    "(SDG)."
                ],
                [
                    "Making",
                    "the",
                    "definitions",
                    "easier",
                    "to",
                    "read",
                    "and",
                    "understand",
                    "could",
                    "benefit",
                    "the",
                    "language",
                    "learners,",
                    "low",
                    "literacy",
                    "readers,",
                    "as",
                    "well",
                    "as",
                    "helping",
                    "people",
                    "with",
                    "aphasia",
                    "or",
                    "dyslexia."
                ],
                [
                    "For",
                    "example,",
                    "compared",
                    "with",
                    "the",
                    "Oxford",
                    "Dictionary",
                    "(OD),",
                    "the",
                    "Oxford",
                    "Advanced",
                    "Learner's",
                    "Dictionary",
                    "(OALD)",
                    "has",
                    "simpler",
                    "definitions,",
                    "which",
                    "are",
                    "specifically",
                    "designed",
                    "for",
                    "language",
                    "learners."
                ],
                [
                    "As",
                    "shown",
                    "in",
                    "Figure",
                    "1,",
                    "the",
                    "definition",
                    "of",
                    "the",
                    "word",
                    "advertisement",
                    "in",
                    "OALD",
                    "does",
                    "not",
                    "contain",
                    "difficult",
                    "words",
                    "or",
                    "phrases",
                    "such",
                    "as",
                    "announcement",
                    "and",
                    "public",
                    "medium."
                ]
            ],
            "context": [
                0,
                0,
                0,
                2,
                2,
                3,
                3
            ]
        },
        "input": "sent0: Simple definition in OALD a notice or announcement in a public medium promoting a product, service, or event or publicizing a job vacancy.\n sent1: current context because of the cognitively inaccurate nature of discrete sense boundaries #REF .\n sent2: Secondly, the predefined inventories need to be updated manually by lexicographers, which is time-consuming and causes dictionaries to lag behind the ever-changing language usage.\n sent3: Different from previous work #TARGET_REF that focused only on how to generate definitions, we further propose a novel task of Simple Definition Generation (SDG).\n sent4: Making the definitions easier to read and understand could benefit the language learners, low literacy readers, as well as helping people with aphasia or dyslexia.\n sent5: For example, compared with the Oxford Dictionary (OD), the Oxford Advanced Learner's Dictionary (OALD) has simpler definitions, which are specifically designed for language learners.\n sent6: As shown in Figure 1, the definition of the word advertisement in OALD does not contain difficult words or phrases such as announcement and public medium.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent3\", \"sent4\"], \"BACKGROUND\": [\"sent5\", \"sent6\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Some",
                    "research",
                    "adopted",
                    "argumentation",
                    "schemes",
                    "as",
                    "a",
                    "framework,",
                    "making",
                    "comparisons",
                    "with",
                    "discourse",
                    "relations",
                    "#REF",
                    "and",
                    "collecting",
                    "and",
                    "leveraging",
                    "data",
                    "at",
                    "varying",
                    "degrees",
                    "of",
                    "granularity."
                ],
                [
                    "At",
                    "a",
                    "coarse",
                    "level,",
                    "prior",
                    "studies",
                    "annotated",
                    "the",
                    "presence",
                    "of",
                    "particular",
                    "argumentation",
                    "schemes",
                    "in",
                    "text",
                    "#REF",
                    "and",
                    "developed",
                    "models",
                    "to",
                    "classify",
                    "different",
                    "schemes",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "However,",
                    "each",
                    "scheme",
                    "often",
                    "accommodates",
                    "both",
                    "support",
                    "and",
                    "attack",
                    "relations",
                    "between",
                    "statements,",
                    "so",
                    "classifying",
                    "those",
                    "relations",
                    "requires",
                    "semantically",
                    "richer",
                    "information",
                    "within",
                    "the",
                    "scheme",
                    "than",
                    "just",
                    "its",
                    "presence."
                ],
                [
                    "To",
                    "that",
                    "end,",
                    "#REF",
                    "annotated",
                    "individual",
                    "components",
                    "within",
                    "schemes,",
                    "particularly",
                    "emphasizing",
                    "argument",
                    "from",
                    "consequences."
                ],
                [
                    "Based",
                    "on",
                    "the",
                    "logic",
                    "behind",
                    "this",
                    "scheme,",
                    "#REF",
                    "developed",
                    "an",
                    "unsupervised",
                    "method",
                    "to",
                    "classify",
                    "the",
                    "support",
                    "and",
                    "attack",
                    "relations",
                    "using",
                    "syntactic",
                    "rules",
                    "and",
                    "lexicons."
                ],
                [
                    "Our",
                    "work",
                    "extends",
                    "these",
                    "studies",
                    "by",
                    "including",
                    "other",
                    "normative",
                    "schemes",
                    "(practical",
                    "reasoning",
                    "and",
                    "property-based",
                    "reasoning)",
                    "and",
                    "annotating",
                    "richer",
                    "information."
                ]
            ],
            "context": [
                0,
                2,
                3,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Some research adopted argumentation schemes as a framework, making comparisons with discourse relations #REF and collecting and leveraging data at varying degrees of granularity.\n sent1: At a coarse level, prior studies annotated the presence of particular argumentation schemes in text #REF and developed models to classify different schemes #TARGET_REF .\n sent2: However, each scheme often accommodates both support and attack relations between statements, so classifying those relations requires semantically richer information within the scheme than just its presence.\n sent3: To that end, #REF annotated individual components within schemes, particularly emphasizing argument from consequences.\n sent4: Based on the logic behind this scheme, #REF developed an unsupervised method to classify the support and attack relations using syntactic rules and lexicons.\n sent5: Our work extends these studies by including other normative schemes (practical reasoning and property-based reasoning) and annotating richer information.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Related",
                    "work",
                    "Our",
                    "work",
                    "follows",
                    "a",
                    "long",
                    "line",
                    "of",
                    "works",
                    "on",
                    "efficient",
                    "Transformers",
                    "(see",
                    "§1)."
                ],
                [
                    "Our",
                    "method",
                    "employs",
                    "three",
                    "main",
                    "ideas:",
                    "(a)",
                    "computing",
                    "the",
                    "top-k",
                    "attention",
                    "scores",
                    "for",
                    "each",
                    "query",
                    "(b)",
                    "grouping",
                    "the",
                    "queries",
                    "into",
                    "chunks",
                    "and",
                    "processing",
                    "these",
                    "sequentially",
                    "(c)",
                    "caching",
                    "only",
                    "a",
                    "part",
                    "of",
                    "the",
                    "activations",
                    "for",
                    "the",
                    "backward",
                    "pass."
                ],
                [
                    "Top-k",
                    "operation",
                    "was",
                    "used",
                    "at",
                    "self-attention",
                    "layers",
                    "by",
                    "#REF",
                    "to",
                    "show",
                    "improved",
                    "model",
                    "performance,",
                    "attributed",
                    "to",
                    "the",
                    "removal",
                    "of",
                    "irrelevant",
                    "information",
                    "in",
                    "the",
                    "context."
                ],
                [
                    "We",
                    "use",
                    "it",
                    "to",
                    "reduce",
                    "the",
                    "resource",
                    "usage",
                    "of",
                    "multi-head",
                    "attention",
                    "and",
                    "feed-forward",
                    "layers."
                ],
                [
                    "Processing",
                    "query",
                    "chunks",
                    "sequentially",
                    "was",
                    "also",
                    "used",
                    "in",
                    "Reformer",
                    "#TARGET_REF",
                    "as",
                    "activations",
                    "are",
                    "not",
                    "cached."
                ],
                [
                    "But",
                    "in",
                    "that",
                    "case,",
                    "by",
                    "replacing",
                    "vanilla",
                    "residual",
                    "connections",
                    "in",
                    "the",
                    "Transformer",
                    "with",
                    "reversible",
                    "connections",
                    "#REF",
                    "."
                ],
                [
                    "Similar",
                    "to",
                    "the",
                    "explanation",
                    "provided",
                    "in",
                    "§2.2,",
                    "these",
                    "require",
                    "an",
                    "extra",
                    "implicit",
                    "forward",
                    "pass",
                    "during",
                    "the",
                    "backward",
                    "pass",
                    "and",
                    "do",
                    "not",
                    "provide",
                    "the",
                    "compute",
                    "and",
                    "memory",
                    "savings",
                    "we",
                    "get",
                    "from",
                    "our",
                    "top-k",
                    "specific",
                    "backward",
                    "pass",
                    "(",
                    "§2.2)."
                ],
                [
                    "Secondly,",
                    "replacing",
                    "residual",
                    "connections",
                    "with",
                    "reversible",
                    "ones",
                    "changes",
                    "the",
                    "function",
                    "computed",
                    "by",
                    "the",
                    "model",
                    "and",
                    "would",
                    "require",
                    "corrective",
                    "pre-training",
                    "to",
                    "be",
                    "used",
                    "with",
                    "BERT,",
                    "T5,",
                    "etc",
                    "(",
                    "§4.3-",
                    "§4.5)."
                ]
            ],
            "context": [
                0,
                0,
                0,
                3,
                1,
                3,
                0,
                0
            ]
        },
        "input": "sent0: Related work Our work follows a long line of works on efficient Transformers (see §1).\n sent1: Our method employs three main ideas: (a) computing the top-k attention scores for each query (b) grouping the queries into chunks and processing these sequentially (c) caching only a part of the activations for the backward pass.\n sent2: Top-k operation was used at self-attention layers by #REF to show improved model performance, attributed to the removal of irrelevant information in the context.\n sent3: We use it to reduce the resource usage of multi-head attention and feed-forward layers.\n sent4: Processing query chunks sequentially was also used in Reformer #TARGET_REF as activations are not cached.\n sent5: But in that case, by replacing vanilla residual connections in the Transformer with reversible connections #REF .\n sent6: Similar to the explanation provided in §2.2, these require an extra implicit forward pass during the backward pass and do not provide the compute and memory savings we get from our top-k specific backward pass ( §2.2).\n sent7: Secondly, replacing residual connections with reversible ones changes the function computed by the model and would require corrective pre-training to be used with BERT, T5, etc ( §4.3- §4.5).\n",
        "output": "{\"INFORMATION\": [\"sent4\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent3\", \"sent5\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "hypothesize",
                    "that",
                    "texts",
                    "with",
                    "alignment",
                    "are",
                    "less",
                    "cognitively",
                    "demanding",
                    "to",
                    "process,",
                    "and",
                    "so",
                    "less",
                    "effortful",
                    "to",
                    "post-edit",
                    "than",
                    "texts",
                    "without",
                    "alignment."
                ],
                [
                    "If",
                    "this",
                    "is",
                    "the",
                    "case,",
                    "shorter",
                    "post-editing",
                    "times",
                    "for",
                    "texts",
                    "with",
                    "alignment",
                    "are",
                    "consistent",
                    "with",
                    "previous",
                    "findings",
                    "by",
                    "#TARGET_REF",
                    ",",
                    "who",
                    "found",
                    "that",
                    "per",
                    "word",
                    "post-editing",
                    "times",
                    "were",
                    "shorter",
                    "for",
                    "segments",
                    "that",
                    "were",
                    "less",
                    "cognitively",
                    "demanding",
                    "because",
                    "of",
                    "the",
                    "linguistic",
                    "structure."
                ],
                [
                    "Related",
                    "work",
                    "on",
                    "cognitive",
                    "effort",
                    "in",
                    "post-editing",
                    "#REF",
                    "has",
                    "also",
                    "shown",
                    "decreased",
                    "densities",
                    "of",
                    "short",
                    "pauses",
                    "when",
                    "less",
                    "cognitively",
                    "demanding",
                    "segments",
                    "are",
                    "post-edited."
                ]
            ],
            "context": [
                2,
                1,
                3
            ]
        },
        "input": "sent0: We hypothesize that texts with alignment are less cognitively demanding to process, and so less effortful to post-edit than texts without alignment.\n sent1: If this is the case, shorter post-editing times for texts with alignment are consistent with previous findings by #TARGET_REF , who found that per word post-editing times were shorter for segments that were less cognitively demanding because of the linguistic structure.\n sent2: Related work on cognitive effort in post-editing #REF has also shown decreased densities of short pauses when less cognitively demanding segments are post-edited.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Specifically,",
                    "the",
                    "triplet",
                    "loss",
                    "minimizes",
                    "the",
                    "distance",
                    "between",
                    "an",
                    "anchor",
                    "and",
                    "a",
                    "positive",
                    "example,",
                    "while",
                    "maximizing",
                    "the",
                    "distance",
                    "between",
                    "the",
                    "same",
                    "anchor",
                    "and",
                    "a",
                    "negative",
                    "example."
                ],
                [
                    "The",
                    "team",
                    "treats",
                    "question",
                    "and",
                    "correct",
                    "answer",
                    "as",
                    "the",
                    "anchor,",
                    "while",
                    "the",
                    "facts",
                    "annotated",
                    "with",
                    "high",
                    "ratings",
                    "are",
                    "adopted",
                    "as",
                    "positive",
                    "examples."
                ],
                [
                    "Different",
                    "experiments",
                    "are",
                    "conducted",
                    "with",
                    "three",
                    "negative",
                    "sampling",
                    "strategies",
                    "for",
                    "retrieval",
                    "and",
                    "re-ranking."
                ],
                [
                    "The",
                    "best",
                    "results",
                    "are",
                    "obtained",
                    "when",
                    "sampling",
                    "negative",
                    "examples",
                    "from",
                    "the",
                    "same",
                    "tables",
                    "of",
                    "highly",
                    "relevant",
                    "facts."
                ],
                [
                    "The",
                    "authors",
                    "find",
                    "that",
                    "the",
                    "best",
                    "performance",
                    "is",
                    "obtained",
                    "when",
                    "averaging",
                    "the",
                    "results",
                    "from",
                    "RoBERTa",
                    "#REF",
                    "facts",
                    "to",
                    "the",
                    "question",
                    "using",
                    "BM25",
                    "vectors",
                    "and",
                    "then",
                    "update",
                    "the",
                    "query",
                    "vector",
                    "via",
                    "a",
                    "max",
                    "operation."
                ],
                [
                    "The",
                    "iterative",
                    "retrieval",
                    "step",
                    "is",
                    "performed",
                    "until",
                    "a",
                    "list",
                    "of",
                    "K",
                    "=",
                    "200",
                    "facts",
                    "is",
                    "selected",
                    "from",
                    "the",
                    "knowledge",
                    "base."
                ],
                [
                    "Subsequently,",
                    "the",
                    "top",
                    "K",
                    "explanation",
                    "facts",
                    "are",
                    "re-ranked",
                    "using",
                    "language",
                    "models."
                ],
                [
                    "The",
                    "best",
                    "model",
                    "consists",
                    "of",
                    "an",
                    "ensemble",
                    "of",
                    "BERT",
                    "#TARGET_REF",
                    "and",
                    "SciBERT",
                    "#REF",
                    "."
                ],
                [
                    "These",
                    "models",
                    "are",
                    "fine-tuned",
                    "to",
                    "predict",
                    "the",
                    "target",
                    "explanatory",
                    "relevance",
                    "ratings",
                    "using",
                    "the",
                    "following",
                    "input:",
                    "Question",
                    "+",
                    "Answer",
                    "[SEP]",
                    "Explanation."
                ],
                [
                    "Specifically,",
                    "the",
                    "authors",
                    "frame",
                    "the",
                    "problem",
                    "as",
                    "a",
                    "regression",
                    "via",
                    "mean",
                    "squared",
                    "error",
                    "loss."
                ],
                [
                    "The",
                    "ensemble",
                    "is",
                    "achieved",
                    "by",
                    "linearly",
                    "combining",
                    "the",
                    "scores",
                    "of",
                    "the",
                    "models."
                ],
                [
                    "The",
                    "authors",
                    "reported",
                    "two",
                    "negative",
                    "results",
                    "obtained",
                    "using",
                    "a",
                    "two-stage",
                    "approach",
                    "and",
                    "different",
                    "negative",
                    "sampling",
                    "techniques."
                ],
                [
                    "In",
                    "the",
                    "two-stage",
                    "approach,",
                    "the",
                    "facts",
                    "were",
                    "firstly",
                    "categorized",
                    "using",
                    "binary",
                    "scores",
                    "to",
                    "discriminate",
                    "between",
                    "relevant",
                    "and",
                    "irrelevant",
                    "sentences,",
                    "and",
                    "then",
                    "re-ranked",
                    "predicting",
                    "the",
                    "target",
                    "explanatory",
                    "relevance",
                    "rating."
                ],
                [
                    "Regarding",
                    "the",
                    "negative",
                    "sampling",
                    "strategy,",
                    "the",
                    "authors",
                    "noticed",
                    "that",
                    "highest",
                    "percentage",
                    "of",
                    "errors",
                    "occurring",
                    "at",
                    "inference",
                    "time",
                    "was",
                    "due",
                    "to",
                    "irrelevant",
                    "facts",
                    "that",
                    "are",
                    "lexically",
                    "close",
                    "to",
                    "highly",
                    "relevant",
                    "explanation",
                    "sentences."
                ],
                [
                    "They",
                    "attempted",
                    "to",
                    "alleviate",
                    "this",
                    "problem",
                    "by",
                    "randomly",
                    "sampling",
                    "facts",
                    "from",
                    "the",
                    "knowledge",
                    "base",
                    "and",
                    "retrieving",
                    "close",
                    "negative",
                    "examples",
                    "during",
                    "training."
                ],
                [
                    "Neither",
                    "of",
                    "these",
                    "two",
                    "methods",
                    "resulted",
                    "in",
                    "significant",
                    "improvements."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Specifically, the triplet loss minimizes the distance between an anchor and a positive example, while maximizing the distance between the same anchor and a negative example.\n sent1: The team treats question and correct answer as the anchor, while the facts annotated with high ratings are adopted as positive examples.\n sent2: Different experiments are conducted with three negative sampling strategies for retrieval and re-ranking.\n sent3: The best results are obtained when sampling negative examples from the same tables of highly relevant facts.\n sent4: The authors find that the best performance is obtained when averaging the results from RoBERTa #REF facts to the question using BM25 vectors and then update the query vector via a max operation.\n sent5: The iterative retrieval step is performed until a list of K = 200 facts is selected from the knowledge base.\n sent6: Subsequently, the top K explanation facts are re-ranked using language models.\n sent7: The best model consists of an ensemble of BERT #TARGET_REF and SciBERT #REF .\n sent8: These models are fine-tuned to predict the target explanatory relevance ratings using the following input: Question + Answer [SEP] Explanation.\n sent9: Specifically, the authors frame the problem as a regression via mean squared error loss.\n sent10: The ensemble is achieved by linearly combining the scores of the models.\n sent11: The authors reported two negative results obtained using a two-stage approach and different negative sampling techniques.\n sent12: In the two-stage approach, the facts were firstly categorized using binary scores to discriminate between relevant and irrelevant sentences, and then re-ranked predicting the target explanatory relevance rating.\n sent13: Regarding the negative sampling strategy, the authors noticed that highest percentage of errors occurring at inference time was due to irrelevant facts that are lexically close to highly relevant explanation sentences.\n sent14: They attempted to alleviate this problem by randomly sampling facts from the knowledge base and retrieving close negative examples during training.\n sent15: Neither of these two methods resulted in significant improvements.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent7\", \"sent8\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "•",
                    "Capsule",
                    "NN",
                    "#TARGET_REF",
                    ")",
                    "is",
                    "a",
                    "capsule-based",
                    "neural",
                    "network",
                    "that",
                    "explicitly",
                    "captures",
                    "the",
                    "semantic",
                    "hierarchical",
                    "relationship",
                    "among",
                    "words,",
                    "slots",
                    "and",
                    "intents",
                    "via",
                    "a",
                    "dynamic",
                    "routing-by-agreement",
                    "schema."
                ]
            ],
            "context": [
                1
            ]
        },
        "input": "sent0: • Capsule NN #TARGET_REF ) is a capsule-based neural network that explicitly captures the semantic hierarchical relationship among words, slots and intents via a dynamic routing-by-agreement schema.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "work",
                    "of",
                    "#REF",
                    "showed",
                    "that",
                    "large",
                    "pre-trained",
                    "multilingual",
                    "models",
                    "are",
                    "not",
                    "enough",
                    "for",
                    "question-answering",
                    "in",
                    "underrepresented",
                    "languages",
                    "and",
                    "presented",
                    "several",
                    "novel",
                    "strategies",
                    "to",
                    "improve",
                    "the",
                    "performance",
                    "of",
                    "mBERT",
                    "with",
                    "translations."
                ],
                [
                    "This",
                    "work",
                    "achieved",
                    "languageindependent",
                    "embeddings,",
                    "which",
                    "improved",
                    "the",
                    "cross-lingual",
                    "transfer",
                    "performance",
                    "with",
                    "additional",
                    "pre-training",
                    "on",
                    "adversarial",
                    "tasks."
                ],
                [
                    "It",
                    "also",
                    "introduced",
                    "a",
                    "Language",
                    "Arbitration",
                    "Framework",
                    "(LAF),",
                    "which",
                    "consolidated",
                    "the",
                    "embedding",
                    "representations",
                    "across",
                    "languages",
                    "using",
                    "properties",
                    "of",
                    "translation."
                ],
                [
                    "Crosslingual",
                    "manifold",
                    "mixup",
                    "(X-Mixup)",
                    "#REF",
                    "achieved",
                    "better",
                    "cross-lingual",
                    "transfer",
                    "by",
                    "calibrating",
                    "the",
                    "representation",
                    "discrepancy,",
                    "which",
                    "resulted",
                    "in",
                    "a",
                    "compromised",
                    "representation",
                    "for",
                    "target",
                    "languages."
                ],
                [
                    "It",
                    "was",
                    "shown",
                    "that",
                    "the",
                    "multilingual",
                    "pretraining",
                    "process",
                    "can",
                    "be",
                    "improved",
                    "by",
                    "implementing",
                    "X-Mixup",
                    "on",
                    "parallel",
                    "data."
                ],
                [
                    "Contrastive",
                    "Language-Image",
                    "pre-training",
                    "(CLIP)",
                    "#TARGET_REF",
                    "introduced",
                    "an",
                    "efficient",
                    "way",
                    "to",
                    "learn",
                    "scalable",
                    "image",
                    "representations",
                    "with",
                    "natural",
                    "language",
                    "supervision."
                ],
                [
                    "Drawing",
                    "inspiration",
                    "from",
                    "ConVIRT",
                    "#REF",
                    ",",
                    "CLIP",
                    "used",
                    "a",
                    "contrastive",
                    "objective",
                    "that",
                    "maximizes",
                    "the",
                    "cosine",
                    "similarity",
                    "of",
                    "the",
                    "correct",
                    "pairs",
                    "of",
                    "images",
                    "and",
                    "text,",
                    "while",
                    "minimizing",
                    "the",
                    "same",
                    "for",
                    "incorrect",
                    "pairs."
                ],
                [
                    "Building",
                    "upon",
                    "the",
                    "work",
                    "of",
                    "#REF",
                    ",",
                    "we",
                    "show",
                    "that",
                    "translations",
                    "of",
                    "a",
                    "small-scale",
                    "dataset",
                    "into",
                    "cross-family",
                    "languages",
                    "could",
                    "degrade",
                    "the",
                    "QA",
                    "performance."
                ],
                [
                    "To",
                    "overcome",
                    "this",
                    "problem,",
                    "we",
                    "propose",
                    "multilingual",
                    "contrastive",
                    "training",
                    "to",
                    "encourage",
                    "cross-lingual",
                    "invariance."
                ],
                [
                    "Our",
                    "approach",
                    "is",
                    "relatively",
                    "simpler",
                    "compared",
                    "to",
                    "adversarial",
                    "training",
                    "and",
                    "LAF",
                    "used",
                    "in",
                    "#REF",
                    "."
                ],
                [
                    "Though",
                    "the",
                    "proposed",
                    "contrastive",
                    "loss",
                    "has",
                    "a",
                    "similar",
                    "objective",
                    "to",
                    "the",
                    "pretraining",
                    "loss",
                    "in",
                    "#REF",
                    ",",
                    "there",
                    "are",
                    "subtle",
                    "differences",
                    "because",
                    "we",
                    "use",
                    "it",
                    "in",
                    "multi-task",
                    "learning",
                    "setup",
                    "along",
                    "with",
                    "the",
                    "original",
                    "task",
                    "loss",
                    "for",
                    "finetuning."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                1,
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: The work of #REF showed that large pre-trained multilingual models are not enough for question-answering in underrepresented languages and presented several novel strategies to improve the performance of mBERT with translations.\n sent1: This work achieved languageindependent embeddings, which improved the cross-lingual transfer performance with additional pre-training on adversarial tasks.\n sent2: It also introduced a Language Arbitration Framework (LAF), which consolidated the embedding representations across languages using properties of translation.\n sent3: Crosslingual manifold mixup (X-Mixup) #REF achieved better cross-lingual transfer by calibrating the representation discrepancy, which resulted in a compromised representation for target languages.\n sent4: It was shown that the multilingual pretraining process can be improved by implementing X-Mixup on parallel data.\n sent5: Contrastive Language-Image pre-training (CLIP) #TARGET_REF introduced an efficient way to learn scalable image representations with natural language supervision.\n sent6: Drawing inspiration from ConVIRT #REF , CLIP used a contrastive objective that maximizes the cosine similarity of the correct pairs of images and text, while minimizing the same for incorrect pairs.\n sent7: Building upon the work of #REF , we show that translations of a small-scale dataset into cross-family languages could degrade the QA performance.\n sent8: To overcome this problem, we propose multilingual contrastive training to encourage cross-lingual invariance.\n sent9: Our approach is relatively simpler compared to adversarial training and LAF used in #REF .\n sent10: Though the proposed contrastive loss has a similar objective to the pretraining loss in #REF , there are subtle differences because we use it in multi-task learning setup along with the original task loss for finetuning.\n",
        "output": "{\"INFORMATION\": [\"sent5\", \"sent6\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "system",
                    "of",
                    "#TARGET_REF",
                    "pairs",
                    "two",
                    "dependency",
                    "trees",
                    "based",
                    "on",
                    "a",
                    "Deep",
                    "Syntactic",
                    "Structure",
                    "(DSyntS)",
                    "of",
                    "Meaning",
                    "Text",
                    "Theory",
                    "(MTT)",
                    "#REF",
                    ",",
                    "a",
                    "dependency",
                    "representation",
                    "composed",
                    "of",
                    "nodes",
                    "labeled",
                    "by",
                    "lexemes",
                    "that",
                    "correspond",
                    "to",
                    "meaning-bearing",
                    "words",
                    "(nouns,",
                    "verbs,",
                    "adjectives,",
                    "adverbs)",
                    "and",
                    "directed",
                    "arcs",
                    "with",
                    "dependency",
                    "relation",
                    "labels."
                ],
                [
                    "Transfer",
                    "rules",
                    "are",
                    "also",
                    "represented",
                    "by",
                    "DSyntS",
                    "trees,",
                    "with",
                    "variables."
                ],
                [
                    "3",
                    "The",
                    "goal",
                    "of",
                    "this",
                    "particular",
                    "dependency",
                    "representation",
                    "is",
                    "to",
                    "minimise",
                    "'spurious'",
                    "structural",
                    "divergences,",
                    "such",
                    "as",
                    "when",
                    "a",
                    "preposition",
                    "in",
                    "one",
                    "language",
                    "is",
                    "represented",
                    "by",
                    "a",
                    "verbal",
                    "inflection",
                    "in",
                    "the",
                    "other."
                ],
                [
                    "However,",
                    "some",
                    "divergences",
                    "still",
                    "occur,",
                    "as",
                    "in",
                    "(1)."
                ],
                [
                    "The",
                    "transfer",
                    "rule",
                    "then",
                    "requires",
                    "that",
                    "the",
                    "two",
                    "nodes",
                    "is",
                    "and",
                    "small",
                    "pair",
                    "with",
                    "the",
                    "single",
                    "node",
                    "cakayo:",
                    "a",
                    "transfer",
                    "rule",
                    "for",
                    "Figure",
                    "1,",
                    "treating",
                    "them",
                    "as",
                    "a",
                    "gCN,",
                    "would",
                    "be",
                    "as",
                    "in",
                    "the",
                    "righthand",
                    "side",
                    "of",
                    "that",
                    "figure."
                ],
                [
                    "4",
                    "However,",
                    "there",
                    "are",
                    "constructions",
                    "which",
                    "cannot",
                    "be",
                    "handled",
                    "in",
                    "such",
                    "a",
                    "way."
                ],
                [
                    "Consider",
                    "the",
                    "translation",
                    "pair",
                    "in",
                    "(3)."
                ]
            ],
            "context": [
                3,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: The system of #TARGET_REF pairs two dependency trees based on a Deep Syntactic Structure (DSyntS) of Meaning Text Theory (MTT) #REF , a dependency representation composed of nodes labeled by lexemes that correspond to meaning-bearing words (nouns, verbs, adjectives, adverbs) and directed arcs with dependency relation labels.\n sent1: Transfer rules are also represented by DSyntS trees, with variables.\n sent2: 3 The goal of this particular dependency representation is to minimise 'spurious' structural divergences, such as when a preposition in one language is represented by a verbal inflection in the other.\n sent3: However, some divergences still occur, as in (1).\n sent4: The transfer rule then requires that the two nodes is and small pair with the single node cakayo: a transfer rule for Figure 1, treating them as a gCN, would be as in the righthand side of that figure.\n sent5: 4 However, there are constructions which cannot be handled in such a way.\n sent6: Consider the translation pair in (3).\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Experimental",
                    "details",
                    "For",
                    "all",
                    "models,",
                    "we",
                    "benchmark",
                    "by",
                    "running",
                    "a",
                    "forward",
                    "and",
                    "backward",
                    "pass",
                    "over",
                    "random",
                    "inputs."
                ],
                [
                    "Each",
                    "measurement",
                    "is",
                    "an",
                    "average",
                    "over",
                    "3",
                    "runs",
                    "on",
                    "an",
                    "Nvidia",
                    "A100",
                    "GPU",
                    "and",
                    "is",
                    "discarded",
                    "if",
                    "memory",
                    "usage",
                    "exceeds",
                    "30GiB."
                ],
                [
                    "We",
                    "use",
                    "causal",
                    "masking",
                    "for",
                    "self-attention",
                    "layers",
                    "to",
                    "highlight",
                    "the",
                    "simplicity",
                    "of",
                    "our",
                    "approach",
                    "that",
                    "can",
                    "seamlessly",
                    "handle",
                    "arbitrary",
                    "attention",
                    "masks,",
                    "unlike",
                    "other",
                    "methods",
                    "#TARGET_REF",
                    ",",
                    "where",
                    "implementing",
                    "causal",
                    "masking",
                    "requires",
                    "customized",
                    "CUDA",
                    "implementations."
                ],
                [
                    "For",
                    "Performer,",
                    "we",
                    "use",
                    "256",
                    "random",
                    "features,",
                    "and",
                    "the",
                    "CUDA",
                    "implementation",
                    "from",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                0,
                0,
                2,
                0
            ]
        },
        "input": "sent0: Experimental details For all models, we benchmark by running a forward and backward pass over random inputs.\n sent1: Each measurement is an average over 3 runs on an Nvidia A100 GPU and is discarded if memory usage exceeds 30GiB.\n sent2: We use causal masking for self-attention layers to highlight the simplicity of our approach that can seamlessly handle arbitrary attention masks, unlike other methods #TARGET_REF , where implementing causal masking requires customized CUDA implementations.\n sent3: For Performer, we use 256 random features, and the CUDA implementation from #REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Schabes",
                    "et",
                    "al."
                ],
                [
                    "#REF",
                    "and",
                    "#TARGET_REF",
                    "report",
                    "results",
                    "using",
                    "the",
                    "GEIG",
                    "evaluation",
                    "scheme",
                    "which",
                    "are",
                    "numerically",
                    "superior",
                    "to",
                    "ours."
                ],
                [
                    "However,",
                    "their",
                    "experiments",
                    "are",
                    "not",
                    "strictly",
                    "compati",
                    "ble",
                    "because",
                    "they",
                    "both",
                    "utilise",
                    "more",
                    "homogeneous",
                    "and",
                    "probably",
                    "simpler",
                    "corpora."
                ],
                [
                    "In",
                    "addition,",
                    "Schabes",
                    "et",
                    "al."
                ],
                [
                    "do",
                    "not",
                    "recover",
                    "tree",
                    "labelling,",
                    ",",
                    "whilst",
                    "Magerman",
                    "has",
                    "developed",
                    "a",
                    "parser",
                    "designed",
                    "to",
                    "produce",
                    "identical",
                    "analyses",
                    "to",
                    "those",
                    "used",
                    "in",
                    "the",
                    "Penn",
                    "'Ireebank,",
                    "removing",
                    "the",
                    "problem",
                    "of",
                    "spurious",
                    "errors",
                    "due",
                    "to",
                    "grammatical",
                    "incompatibility."
                ],
                [
                    "Both",
                    "these",
                    "approaches",
                    "achieve",
                    "better",
                    "cov",
                    "erage",
                    "by",
                    "constructing",
                    "the",
                    "grammar",
                    "fully",
                    "automatically."
                ],
                [
                    "No",
                    "one",
                    "has",
                    "yet",
                    "shown",
                    "that",
                    "any",
                    "robust",
                    "parser",
                    "is",
                    "practical",
                    "and",
                    "useful",
                    "for",
                    "some",
                    "NLP",
                    "task."
                ],
                [
                    "However,",
                    "it",
                    "seems",
                    "likely",
                    "that",
                    "say",
                    "rule-to-rule",
                    "semantic",
                    "interpretation",
                    "will",
                    "be",
                    "easier",
                    "with",
                    "hand-constructed",
                    "grammars",
                    "with",
                    "an",
                    "explicit,",
                    "de",
                    "terminate",
                    "ruleset."
                ],
                [
                    "A",
                    "more",
                    "meaningful",
                    "comparison",
                    "will",
                    "require",
                    "application",
                    "of",
                    "different",
                    "parsers",
                    "to",
                    "an",
                    "identical",
                    "and",
                    "extended",
                    "test",
                    "suite",
                    "and",
                    "utllisation",
                    "of",
                    "a",
                    "more",
                    "stringent",
                    "standard",
                    "evaluation",
                    "procedure",
                    "sensitive",
                    "to",
                    "node",
                    "labellings."
                ]
            ],
            "context": [
                3,
                1,
                0,
                3,
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Schabes et al.\n sent1: #REF and #TARGET_REF report results using the GEIG evaluation scheme which are numerically superior to ours.\n sent2: However, their experiments are not strictly compati ble because they both utilise more homogeneous and probably simpler corpora.\n sent3: In addition, Schabes et al.\n sent4: do not recover tree labelling, , whilst Magerman has developed a parser designed to produce identical analyses to those used in the Penn 'Ireebank, removing the problem of spurious errors due to grammatical incompatibility.\n sent5: Both these approaches achieve better cov erage by constructing the grammar fully automatically.\n sent6: No one has yet shown that any robust parser is practical and useful for some NLP task.\n sent7: However, it seems likely that say rule-to-rule semantic interpretation will be easier with hand-constructed grammars with an explicit, de terminate ruleset.\n sent8: A more meaningful comparison will require application of different parsers to an identical and extended test suite and utllisation of a more stringent standard evaluation procedure sensitive to node labellings.\n",
        "output": "{\"INFORMATION\": [\"sent1\", \"sent4\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Among",
                    "the",
                    "similarity",
                    "measures",
                    "of",
                    "#TARGET_REF",
                    ",",
                    "generally",
                    "FastText",
                    "seems",
                    "to",
                    "be",
                    "the",
                    "best",
                    "model."
                ],
                [
                    "So,",
                    "following",
                    "the",
                    "cited",
                    "work,",
                    "we",
                    "create",
                    "a",
                    "relatedness",
                    "matrix",
                    "based",
                    "on",
                    "the",
                    "cosine",
                    "similarity",
                    "of",
                    "FastText",
                    "vectors."
                ],
                [
                    "That",
                    "is,",
                    "if",
                    "v",
                    "i",
                    ",",
                    "v",
                    "j",
                    "are",
                    "vectors",
                    "corresponding",
                    "to",
                    "words",
                    "w",
                    "i",
                    ",",
                    "w",
                    "j",
                    ",",
                    "thens",
                    "F",
                    "(w",
                    "i",
                    ",",
                    "w",
                    "j",
                    ")",
                    "=",
                    "cos(v",
                    "i",
                    ",",
                    "v",
                    "j",
                    ").For",
                    "comparability",
                    "with",
                    "the",
                    "other",
                    "methods,",
                    "we",
                    "train",
                    "our",
                    "FastText",
                    "models",
                    "on",
                    "the",
                    "above",
                    "corpora",
                    "for",
                    "English",
                    "and",
                    "Hungarian",
                    "in",
                    "300",
                    "dimensions,",
                    "using",
                    "window",
                    "size",
                    "10."
                ]
            ],
            "context": [
                2,
                2,
                2
            ]
        },
        "input": "sent0: Among the similarity measures of #TARGET_REF , generally FastText seems to be the best model.\n sent1: So, following the cited work, we create a relatedness matrix based on the cosine similarity of FastText vectors.\n sent2: That is, if v i , v j are vectors corresponding to words w i , w j , thens F (w i , w j ) = cos(v i , v j ).For comparability with the other methods, we train our FastText models on the above corpora for English and Hungarian in 300 dimensions, using window size 10.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\", \"sent1\", \"sent2\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "If",
                    "we",
                    "fine-tune",
                    "a",
                    "RoBERTa-large",
                    "model",
                    "on",
                    "Quoref,",
                    "it",
                    "achieves",
                    "78",
                    "F1",
                    "score",
                    "while",
                    "the",
                    "estimated",
                    "human",
                    "performance",
                    "is",
                    "around",
                    "93",
                    "F1",
                    "score",
                    "#REF",
                    "."
                ],
                [
                    "This",
                    "high",
                    "performance,",
                    "given",
                    "that",
                    "RoBERTa",
                    "can",
                    "only",
                    "predict",
                    "continuous",
                    "span",
                    "answers",
                    "while",
                    "Quoref",
                    "also",
                    "contains",
                    "discontinuous",
                    "answers,",
                    "indicates",
                    "that",
                    "either",
                    "(1)",
                    "Quoref",
                    "presents",
                    "coreference-aware",
                    "QA",
                    "very",
                    "well",
                    "so",
                    "that",
                    "the",
                    "model",
                    "can",
                    "properly",
                    "learn",
                    "coreference",
                    "reasoning",
                    "from",
                    "the",
                    "training",
                    "data,",
                    "(2)",
                    "pretrained",
                    "transformer-based",
                    "models",
                    "have",
                    "already",
                    "learned",
                    "coreference",
                    "reasoning",
                    "during",
                    "their",
                    "pre-training,",
                    "e.g.,",
                    "as",
                    "suggested",
                    "by",
                    "#TARGET_REF",
                    "and",
                    "#REF",
                    ",",
                    "or",
                    "(3)",
                    "coreference",
                    "reasoning",
                    "is",
                    "not",
                    "necessarily",
                    "required",
                    "for",
                    "solving",
                    "most",
                    "examples."
                ]
            ],
            "context": [
                0,
                3
            ]
        },
        "input": "sent0: If we fine-tune a RoBERTa-large model on Quoref, it achieves 78 F1 score while the estimated human performance is around 93 F1 score #REF .\n sent1: This high performance, given that RoBERTa can only predict continuous span answers while Quoref also contains discontinuous answers, indicates that either (1) Quoref presents coreference-aware QA very well so that the model can properly learn coreference reasoning from the training data, (2) pretrained transformer-based models have already learned coreference reasoning during their pre-training, e.g., as suggested by #TARGET_REF and #REF , or (3) coreference reasoning is not necessarily required for solving most examples.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "sample",
                    "sentences",
                    "from",
                    "a",
                    "subset",
                    "of",
                    "online",
                    "news",
                    "articles",
                    "and",
                    "label",
                    "them",
                    "with",
                    "our",
                    "distant",
                    "supervision",
                    "knowledge",
                    "base",
                    "#REF",
                    "using",
                    "a",
                    "query",
                    "streams",
                    "as",
                    "the",
                    "source",
                    "of",
                    "supervision,",
                    "as",
                    "in",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "We",
                    "sample",
                    "12.6",
                    "million",
                    "entityattribute",
                    "pairs",
                    "from",
                    "a",
                    "knowledge",
                    "base,",
                    "finding",
                    "6",
                    "million",
                    "unique",
                    "entities",
                    "and",
                    "788",
                    "thousand",
                    "unique",
                    "attributes."
                ],
                [
                    "Each",
                    "sentence",
                    "that",
                    "contains",
                    "an",
                    "entityattribute",
                    "pair",
                    "from",
                    "our",
                    "knowledge",
                    "base",
                    "is",
                    "used",
                    "as",
                    "the",
                    "support",
                    "set",
                    "for",
                    "that",
                    "pair."
                ]
            ],
            "context": [
                3,
                3,
                3
            ]
        },
        "input": "sent0: We sample sentences from a subset of online news articles and label them with our distant supervision knowledge base #REF using a query streams as the source of supervision, as in #TARGET_REF .\n sent1: We sample 12.6 million entityattribute pairs from a knowledge base, finding 6 million unique entities and 788 thousand unique attributes.\n sent2: Each sentence that contains an entityattribute pair from our knowledge base is used as the support set for that pair.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\", \"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "ALBERT",
                    "is",
                    "based",
                    "on",
                    "parameter",
                    "sharing/reduction",
                    "techniques",
                    "that",
                    "allows",
                    "to",
                    "reduce",
                    "the",
                    "computational",
                    "complexity",
                    "and",
                    "speed",
                    "up",
                    "training",
                    "and",
                    "inference",
                    "phases."
                ],
                [
                    "Compared",
                    "to",
                    "previous",
                    "compact",
                    "models",
                    "such",
                    "as",
                    "DistilBERT",
                    "#REF",
                    ",",
                    "Q-BERT",
                    "#REF",
                    "or",
                    "TernaryBERT",
                    "#TARGET_REF",
                    ",",
                    "ALBERT",
                    "is",
                    "to",
                    "the",
                    "date",
                    "the",
                    "smallest",
                    "pre-trained",
                    "models",
                    "with",
                    "12",
                    "million",
                    "parameters",
                    "and",
                    "&lt,50",
                    "megabyte",
                    "(MB)",
                    "model",
                    "size."
                ]
            ],
            "context": [
                0,
                0
            ]
        },
        "input": "sent0: ALBERT is based on parameter sharing/reduction techniques that allows to reduce the computational complexity and speed up training and inference phases.\n sent1: Compared to previous compact models such as DistilBERT #REF , Q-BERT #REF or TernaryBERT #TARGET_REF , ALBERT is to the date the smallest pre-trained models with 12 million parameters and &lt,50 megabyte (MB) model size.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Recent",
                    "work",
                    "has",
                    "identified",
                    "that",
                    "consecutive",
                    "lay-",
                    "ers",
                    "of",
                    "BERT",
                    "have",
                    "similar",
                    "functionality",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "To",
                    "study",
                    "this,",
                    "we",
                    "considered",
                    "configurations",
                    "where",
                    "six",
                    "even",
                    "and",
                    "odd",
                    "alternate",
                    "layers",
                    "are",
                    "pruned",
                    "and",
                    "compare",
                    "it",
                    "with",
                    "other",
                    "strategies",
                    "of",
                    "pruning",
                    "50%",
                    "layers",
                    "of",
                    "BERT",
                    "(Table",
                    "6)."
                ],
                [
                    "We",
                    "observe",
                    "that",
                    "the",
                    "odd",
                    "configuration",
                    "performs",
                    "better",
                    "than",
                    "the",
                    "Top",
                    "6",
                    "and",
                    "Bottom",
                    "6",
                    "configurations,",
                    "indicating",
                    "a",
                    "preference",
                    "to",
                    "avoid",
                    "pruning",
                    "of",
                    "consecutive",
                    "layers."
                ],
                [
                    "Effect",
                    "of",
                    "Fine-Tuning."
                ],
                [
                    "Recent",
                    "studies",
                    "#REF",
                    "have",
                    "reported",
                    "that",
                    "when",
                    "fine-tuning",
                    "BERT",
                    "for",
                    "specific",
                    "tasks,",
                    "the",
                    "top",
                    "layers",
                    "change",
                    "much",
                    "more",
                    "than",
                    "the",
                    "lower",
                    "layers."
                ],
                [
                    "We",
                    "now",
                    "evaluate",
                    "this",
                    "for",
                    "fine-tuning",
                    "after",
                    "pruning."
                ]
            ],
            "context": [
                1,
                2,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Recent work has identified that consecutive lay- ers of BERT have similar functionality #TARGET_REF .\n sent1: To study this, we considered configurations where six even and odd alternate layers are pruned and compare it with other strategies of pruning 50% layers of BERT (Table 6).\n sent2: We observe that the odd configuration performs better than the Top 6 and Bottom 6 configurations, indicating a preference to avoid pruning of consecutive layers.\n sent3: Effect of Fine-Tuning.\n sent4: Recent studies #REF have reported that when fine-tuning BERT for specific tasks, the top layers change much more than the lower layers.\n sent5: We now evaluate this for fine-tuning after pruning.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "All",
                    "models",
                    "use",
                    "the",
                    "SHIBA",
                    "implementation",
                    "of",
                    "CA-NINE",
                    "#REF",
                    "."
                ],
                [
                    "SHIBA",
                    "was",
                    "designed",
                    "for",
                    "use",
                    "on",
                    "the",
                    "Japanese",
                    "[jpn]",
                    "language,",
                    "which",
                    "does",
                    "not",
                    "include",
                    "spaces",
                    "between",
                    "its",
                    "characters",
                    "(similar",
                    "to",
                    "our",
                    "phonetic",
                    "representations",
                    "without",
                    "word",
                    "boundaries)."
                ],
                [
                    "We",
                    "used",
                    "the",
                    "default",
                    "hyperparameter",
                    "settings",
                    "for",
                    "SHIBA",
                    "pre-training",
                    "and",
                    "finetuning,",
                    "because",
                    "we",
                    "are",
                    "primarily",
                    "concerned",
                    "with",
                    "the",
                    "relative",
                    "impact",
                    "of",
                    "various",
                    "combinations",
                    "of",
                    "pretraining",
                    "data",
                    "on",
                    "the",
                    "downstream",
                    "NER",
                    "tasks."
                ],
                [
                    "We",
                    "use",
                    "the",
                    "Hugging",
                    "Face",
                    "transformers",
                    "library",
                    "#TARGET_REF",
                    "to",
                    "train",
                    "all",
                    "models."
                ]
            ],
            "context": [
                0,
                0,
                0,
                2
            ]
        },
        "input": "sent0: All models use the SHIBA implementation of CA-NINE #REF .\n sent1: SHIBA was designed for use on the Japanese [jpn] language, which does not include spaces between its characters (similar to our phonetic representations without word boundaries).\n sent2: We used the default hyperparameter settings for SHIBA pre-training and finetuning, because we are primarily concerned with the relative impact of various combinations of pretraining data on the downstream NER tasks.\n sent3: We use the Hugging Face transformers library #TARGET_REF to train all models.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent3\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "have",
                    "chosen",
                    "the",
                    "CYK-like",
                    "algorithm",
                    "for",
                    "LIG",
                    "described",
                    "in",
                    "#TARGET_REF",
                    "as",
                    "our",
                    "starting",
                    "point."
                ],
                [
                    "Due",
                    "to",
                    "the",
                    "intrinsic",
                    "limitations",
                    "of",
                    "this",
                    "pure",
                    "bottom-up",
                    "algorithm,",
                    "the",
                    "grammars",
                    "it",
                    "can",
                    "deal",
                    "with",
                    "are",
                    "restricted",
                    "to",
                    "those",
                    "having",
                    "two",
                    "elements,",
                    "or",
                    "one",
                    "element",
                    "which",
                    "must",
                    "be",
                    "a",
                    "terminal,",
                    "in",
                    "the",
                    "right-hand",
                    "side",
                    "of",
                    "each",
                    "production."
                ],
                [
                    "This",
                    "restriction",
                    "could",
                    "be",
                    "considered",
                    "as",
                    "the",
                    "transposition",
                    "of",
                    "the",
                    "Chomsky",
                    "normal",
                    "form",
                    "to",
                    "linear",
                    "indexed",
                    "grammars."
                ]
            ],
            "context": [
                2,
                0,
                0
            ]
        },
        "input": "sent0: We have chosen the CYK-like algorithm for LIG described in #TARGET_REF as our starting point.\n sent1: Due to the intrinsic limitations of this pure bottom-up algorithm, the grammars it can deal with are restricted to those having two elements, or one element which must be a terminal, in the right-hand side of each production.\n sent2: This restriction could be considered as the transposition of the Chomsky normal form to linear indexed grammars.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "When",
                    "processing",
                    "human",
                    "language",
                    "it",
                    "is",
                    "difficult",
                    "to",
                    "operate",
                    "only",
                    "at",
                    "the",
                    "level",
                    "of",
                    "individual",
                    "words."
                ],
                [
                    "While",
                    "for",
                    "some",
                    "tasks,",
                    "perhaps",
                    "monolingual",
                    "information",
                    "retrieval",
                    "in",
                    "particular,",
                    "this",
                    "might",
                    "seem",
                    "reasonable,",
                    "for",
                    "others",
                    "such",
                    "as",
                    "machine",
                    "translation,",
                    "cross-language",
                    "question",
                    "answering,",
                    "and",
                    "translin-gual",
                    "information",
                    "retrieval,",
                    "restriction",
                    "to",
                    "processing",
                    "single",
                    "words",
                    "is",
                    "a",
                    "significant",
                    "impediment."
                ],
                [
                    "There",
                    "has",
                    "been",
                    "much",
                    "recent",
                    "interest",
                    "in",
                    "computational",
                    "approaches",
                    "to",
                    "dealing",
                    "with",
                    "multiword",
                    "expressions",
                    "(MWEs)",
                    "as",
                    "workshops",
                    "at",
                    "#TARGET_REF",
                    "and",
                    "other",
                    "conferences",
                    "attest."
                ]
            ],
            "context": [
                0,
                0,
                1
            ]
        },
        "input": "sent0: When processing human language it is difficult to operate only at the level of individual words.\n sent1: While for some tasks, perhaps monolingual information retrieval in particular, this might seem reasonable, for others such as machine translation, cross-language question answering, and translin-gual information retrieval, restriction to processing single words is a significant impediment.\n sent2: There has been much recent interest in computational approaches to dealing with multiword expressions (MWEs) as workshops at #TARGET_REF and other conferences attest.\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Google-BERT."
                ],
                [
                    "#TARGET_REF",
                    "propose",
                    "a",
                    "framework",
                    "composed",
                    "of",
                    "three",
                    "main",
                    "steps."
                ],
                [
                    "In",
                    "the",
                    "first",
                    "step,",
                    "the",
                    "model",
                    "adopts",
                    "a",
                    "simple",
                    "tf.idf",
                    "model",
                    "with",
                    "cosine",
                    "similarity",
                    "to",
                    "retrieve",
                    "the",
                    "top-K",
                    "relevant",
                    "explanation",
                    "sentences",
                    "(K",
                    "=",
                    "50)",
                    "for",
                    "each",
                    "question",
                    "and",
                    "correct",
                    "answer",
                    "pair."
                ],
                [
                    "In",
                    "the",
                    "second",
                    "step,",
                    "the",
                    "authors",
                    "employ",
                    "an",
                    "autoregressive",
                    "model",
                    "which",
                    "selects",
                    "the",
                    "most",
                    "relevant",
                    "facts",
                    "in",
                    "a",
                    "iterative",
                    "manner."
                ],
                [
                    "Specifically,",
                    "the",
                    "authors",
                    "propose",
                    "the",
                    "adoption",
                    "of",
                    "a",
                    "BERT-based",
                    "model",
                    "#REF",
                    "that",
                    "selects",
                    "the",
                    "facts",
                    "at",
                    "iteration",
                    "n",
                    "given",
                    "the",
                    "facts",
                    "retrieved",
                    "in",
                    "the",
                    "previous",
                    "step."
                ],
                [
                    "The",
                    "model",
                    "uses",
                    "up",
                    "to",
                    "4",
                    "iterations."
                ],
                [
                    "Finally,",
                    "the",
                    "authors",
                    "employ",
                    "a",
                    "re-ranking",
                    "module",
                    "to",
                    "re-score",
                    "the",
                    "retrieved",
                    "candidate",
                    "explanations",
                    "computing",
                    "the",
                    "relevance",
                    "between",
                    "each",
                    "fact",
                    "and",
                    "the",
                    "question-answer",
                    "pairs."
                ],
                [
                    "The",
                    "re-ranking",
                    "model",
                    "is",
                    "implemented",
                    "using",
                    "a",
                    "BERT",
                    "model",
                    "for",
                    "binary",
                    "classification."
                ],
                [
                    "The",
                    "ablation",
                    "study",
                    "shows",
                    "that",
                    "the",
                    "first",
                    "two",
                    "steps",
                    "allow",
                    "achieving",
                    "a",
                    "performance",
                    "of",
                    "0.679",
                    "NDCG,",
                    "that",
                    "is",
                    "improved",
                    "up",
                    "to",
                    "0.700",
                    "NDCG",
                    "using",
                    "the",
                    "re-ranking",
                    "model."
                ],
                [
                    "Moreover,",
                    "the",
                    "experiments",
                    "show",
                    "that",
                    "the",
                    "best",
                    "performance",
                    "is",
                    "achieved",
                    "when",
                    "the",
                    "re-ranking",
                    "model",
                    "is",
                    "adopted",
                    "to",
                    "re-score",
                    "the",
                    "top",
                    "K",
                    "=",
                    "30",
                    "facts."
                ]
            ],
            "context": [
                0,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                3,
                3
            ]
        },
        "input": "sent0: Google-BERT.\n sent1: #TARGET_REF propose a framework composed of three main steps.\n sent2: In the first step, the model adopts a simple tf.idf model with cosine similarity to retrieve the top-K relevant explanation sentences (K = 50) for each question and correct answer pair.\n sent3: In the second step, the authors employ an autoregressive model which selects the most relevant facts in a iterative manner.\n sent4: Specifically, the authors propose the adoption of a BERT-based model #REF that selects the facts at iteration n given the facts retrieved in the previous step.\n sent5: The model uses up to 4 iterations.\n sent6: Finally, the authors employ a re-ranking module to re-score the retrieved candidate explanations computing the relevance between each fact and the question-answer pairs.\n sent7: The re-ranking model is implemented using a BERT model for binary classification.\n sent8: The ablation study shows that the first two steps allow achieving a performance of 0.679 NDCG, that is improved up to 0.700 NDCG using the re-ranking model.\n sent9: Moreover, the experiments show that the best performance is achieved when the re-ranking model is adopted to re-score the top K = 30 facts.\n",
        "output": "{\"INFORMATION\": [\"sent1\", \"sent2\", \"sent3\", \"sent4\", \"sent5\", \"sent6\", \"sent7\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent8\", \"sent9\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "This",
                    "challenge",
                    "aims",
                    "at",
                    "identifying",
                    "the",
                    "reliability",
                    "of",
                    "information",
                    "shared",
                    "on",
                    "social",
                    "network",
                    "sites",
                    "(SNSs)."
                ],
                [
                    "With",
                    "the",
                    "blazing-fast",
                    "spurt",
                    "of",
                    "SNSs",
                    "(e.g."
                ],
                [
                    "Facebook,",
                    "Zalo",
                    "and",
                    "Lotus),",
                    "there",
                    "are",
                    "approximately",
                    "65",
                    "million",
                    "Vietnamese",
                    "users",
                    "on",
                    "board",
                    "with",
                    "the",
                    "annual",
                    "growth",
                    "of",
                    "2.7",
                    "million",
                    "in",
                    "the",
                    "recent",
                    "year,",
                    "as",
                    "reported",
                    "by",
                    "the",
                    "Digital",
                    "2020",
                    "1",
                    "."
                ],
                [
                    "SNSs",
                    "have",
                    "become",
                    "widely",
                    "accessible",
                    "for",
                    "users",
                    "to",
                    "not",
                    "only",
                    "connect",
                    "friends",
                    "but",
                    "also",
                    "freely",
                    "create",
                    "and",
                    "share",
                    "diverse",
                    "content",
                    "#REF",
                    "."
                ],
                [
                    "A",
                    "number",
                    "of",
                    "users,",
                    "1",
                    "https://wearesocial.com/digital-2020",
                    "however,",
                    "has",
                    "exploited",
                    "these",
                    "social",
                    "platforms",
                    "to",
                    "distribute",
                    "fake",
                    "news",
                    "and",
                    "unreliable",
                    "information",
                    "to",
                    "fulfill",
                    "their",
                    "personal",
                    "or",
                    "political",
                    "purposes",
                    "(e.g."
                ],
                [
                    "US",
                    "election",
                    "2016",
                    "#REF",
                    ")."
                ],
                [
                    "It",
                    "is",
                    "not",
                    "easy",
                    "for",
                    "other",
                    "ordinary",
                    "users",
                    "to",
                    "realize",
                    "the",
                    "unreliability,",
                    "hence,",
                    "they",
                    "keep",
                    "spreading",
                    "the",
                    "fake",
                    "content",
                    "to",
                    "their",
                    "friends."
                ],
                [
                    "The",
                    "problem",
                    "becomes",
                    "more",
                    "seriously",
                    "once",
                    "the",
                    "unreliable",
                    "post",
                    "becomes",
                    "popular",
                    "and",
                    "gains",
                    "belief",
                    "among",
                    "the",
                    "community."
                ],
                [
                    "Therefore,",
                    "it",
                    "raises",
                    "an",
                    "urgent",
                    "need",
                    "for",
                    "detecting",
                    "whether",
                    "a",
                    "piece",
                    "of",
                    "news",
                    "on",
                    "SNSs",
                    "is",
                    "reliable",
                    "or",
                    "not."
                ],
                [
                    "This",
                    "task",
                    "has",
                    "gained",
                    "significant",
                    "attention",
                    "recently",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                0,
                1,
                2
            ]
        },
        "input": "sent0: This challenge aims at identifying the reliability of information shared on social network sites (SNSs).\n sent1: With the blazing-fast spurt of SNSs (e.g.\n sent2: Facebook, Zalo and Lotus), there are approximately 65 million Vietnamese users on board with the annual growth of 2.7 million in the recent year, as reported by the Digital 2020 1 .\n sent3: SNSs have become widely accessible for users to not only connect friends but also freely create and share diverse content #REF .\n sent4: A number of users, 1 https://wearesocial.com/digital-2020 however, has exploited these social platforms to distribute fake news and unreliable information to fulfill their personal or political purposes (e.g.\n sent5: US election 2016 #REF ).\n sent6: It is not easy for other ordinary users to realize the unreliability, hence, they keep spreading the fake content to their friends.\n sent7: The problem becomes more seriously once the unreliable post becomes popular and gains belief among the community.\n sent8: Therefore, it raises an urgent need for detecting whether a piece of news on SNSs is reliable or not.\n sent9: This task has gained significant attention recently #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [\"sent8\"], \"PERCEPTION\": [\"sent9\"], \"BACKGROUND\": [\"sent6\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Ten",
                    "#TARGET_REF",
                    "describes",
                    "the",
                    "opposition",
                    "between",
                    "the",
                    "coverage",
                    "of",
                    "the",
                    "lexical",
                    "component",
                    "in",
                    "the",
                    "standard",
                    "approach",
                    "and",
                    "in",
                    "WM",
                    "in",
                    "terms",
                    "of",
                    "two",
                    "orthogonal",
                    "dichotomies:"
                ]
            ],
            "context": [
                1
            ]
        },
        "input": "sent0: Ten #TARGET_REF describes the opposition between the coverage of the lexical component in the standard approach and in WM in terms of two orthogonal dichotomies:\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "While",
                    "we",
                    "do",
                    "not",
                    "have",
                    "access",
                    "to",
                    "many",
                    "coreference",
                    "annotations",
                    "for",
                    "the",
                    "task",
                    "of",
                    "coreference-aware",
                    "MRC,",
                    "there",
                    "are",
                    "various",
                    "datasets",
                    "for",
                    "the",
                    "task",
                    "of",
                    "coreference",
                    "resolution."
                ],
                [
                    "Coreference",
                    "resolution",
                    "datasets",
                    "contain",
                    "the",
                    "annotation",
                    "of",
                    "expressions",
                    "that",
                    "refer",
                    "to",
                    "the",
                    "same",
                    "entity."
                ],
                [
                    "In",
                    "this",
                    "paper,",
                    "we",
                    "hypothesize",
                    "that",
                    "we",
                    "can",
                    "directly",
                    "use",
                    "coreference",
                    "resolution",
                    "corpora",
                    "to",
                    "improve",
                    "the",
                    "coreference",
                    "reasoning",
                    "of",
                    "MRC",
                    "models."
                ],
                [
                    "We",
                    "propose",
                    "an",
                    "effective",
                    "approach",
                    "to",
                    "convert",
                    "coreference",
                    "annotations",
                    "into",
                    "QA",
                    "pairs",
                    "so",
                    "that",
                    "models",
                    "learn",
                    "to",
                    "perform",
                    "coreference",
                    "resolution",
                    "by",
                    "answering",
                    "those",
                    "questions."
                ],
                [
                    "In",
                    "our",
                    "experiments,",
                    "we",
                    "use",
                    "the",
                    "9",
                    "We",
                    "examine",
                    "50",
                    "randomly",
                    "selected",
                    "examples",
                    "from",
                    "our",
                    "challenge",
                    "set,",
                    "and",
                    "they",
                    "were",
                    "all",
                    "answerable",
                    "by",
                    "a",
                    "human."
                ],
                [
                    "#TARGET_REF",
                    ")",
                    "that",
                    "is",
                    "the",
                    "largest",
                    "annotated",
                    "dataset",
                    "with",
                    "coreference",
                    "information."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "sent0: While we do not have access to many coreference annotations for the task of coreference-aware MRC, there are various datasets for the task of coreference resolution.\n sent1: Coreference resolution datasets contain the annotation of expressions that refer to the same entity.\n sent2: In this paper, we hypothesize that we can directly use coreference resolution corpora to improve the coreference reasoning of MRC models.\n sent3: We propose an effective approach to convert coreference annotations into QA pairs so that models learn to perform coreference resolution by answering those questions.\n sent4: In our experiments, we use the 9 We examine 50 randomly selected examples from our challenge set, and they were all answerable by a human.\n sent5: #TARGET_REF ) that is the largest annotated dataset with coreference information.\n",
        "output": "{\"INFORMATION\": [\"sent5\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "For",
                    "swh",
                    "pre-training",
                    "data",
                    "we",
                    "use:",
                    "(i)",
                    "the",
                    "\"Language",
                    "Modeling",
                    "Data",
                    "for",
                    "Swahili\"",
                    "dataset",
                    "(Shikali",
                    "and",
                    "Refuoe,",
                    "2019)",
                    "hosted",
                    "on",
                    "Hugging",
                    "Face",
                    "(which",
                    "we",
                    "refer",
                    "to",
                    "as",
                    "the",
                    "\"HF",
                    "Swahili\"",
                    "data",
                    "set),",
                    "and",
                    "(ii)",
                    "the",
                    "ALFFA",
                    "speech",
                    "dataset",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "For",
                    "ALFFA",
                    "data",
                    "we",
                    "process",
                    "both",
                    "the",
                    "audio",
                    "files",
                    "(using",
                    "Allosaurus)",
                    "and",
                    "the",
                    "original",
                    "\"gold\"",
                    "text",
                    "transcriptions",
                    "(using",
                    "Epitran)."
                ]
            ],
            "context": [
                0,
                2
            ]
        },
        "input": "sent0: For swh pre-training data we use: (i) the \"Language Modeling Data for Swahili\" dataset (Shikali and Refuoe, 2019) hosted on Hugging Face (which we refer to as the \"HF Swahili\" data set), and (ii) the ALFFA speech dataset #TARGET_REF .\n sent1: For ALFFA data we process both the audio files (using Allosaurus) and the original \"gold\" text transcriptions (using Epitran).\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Natural",
                    "Questions",
                    "Test",
                    "MRR@10",
                    "R@50",
                    "R@1000",
                    "R@5",
                    "R@20",
                    "R@100",
                    "BM25",
                    "(anserini)",
                    "#REF",
                    "#REF",
                    "BERTbase",
                    "----78.4",
                    "85.4",
                    "ANCE",
                    "(single)",
                    "#REF",
                    "RoBERTabase",
                    "33.0",
                    "-95.9",
                    "-81.9",
                    "87.5",
                    "ME-BERT",
                    "#TARGET_REF",
                    "BERTlarge",
                    "33.8",
                    "-----RocketQA",
                    "ERNIEbase",
                    "37.0",
                    "85.5",
                    "97.9",
                    "74.0",
                    "82.7",
                    "88.5"
                ]
            ],
            "context": [
                0
            ]
        },
        "input": "sent0: Natural Questions Test MRR@10 R@50 R@1000 R@5 R@20 R@100 BM25 (anserini) #REF #REF BERTbase ----78.4 85.4 ANCE (single) #REF RoBERTabase 33.0 -95.9 -81.9 87.5 ME-BERT #TARGET_REF BERTlarge 33.8 -----RocketQA ERNIEbase 37.0 85.5 97.9 74.0 82.7 88.5\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Visual",
                    "Embedding",
                    "We",
                    "adopt",
                    "a",
                    "CNN",
                    "backbone",
                    "to",
                    "extract",
                    "image",
                    "features",
                    "V",
                    "=",
                    "{v",
                    "i",
                    "}",
                    "L",
                    "i=1",
                    "for",
                    "each",
                    "image",
                    "I",
                    "where",
                    "L",
                    "is",
                    "the",
                    "size",
                    "of",
                    "feature",
                    "grids",
                    "and",
                    "v",
                    "i",
                    "∈",
                    "R",
                    "dv",
                    "is",
                    "a",
                    "feature",
                    "vector",
                    "with",
                    "dimension",
                    "d",
                    "v",
                    "."
                ],
                [
                    "In",
                    "addition,",
                    "each",
                    "feature",
                    "is",
                    "further",
                    "concatenated",
                    "with",
                    "its",
                    "2-D",
                    "sine",
                    "position",
                    "embedding",
                    "#REF",
                    "."
                ],
                [
                    "Following",
                    "SOHO,",
                    "we",
                    "use",
                    "a",
                    "ResNet-101",
                    "#TARGET_REF",
                    "as",
                    "the",
                    "visual",
                    "backbone,",
                    "followed",
                    "by",
                    "additional",
                    "1x1",
                    "Conv",
                    "and",
                    "2x2",
                    "strides",
                    "Max-pooling",
                    "to",
                    "reduce",
                    "the",
                    "memory",
                    "footprint."
                ]
            ],
            "context": [
                0,
                0,
                2
            ]
        },
        "input": "sent0: Visual Embedding We adopt a CNN backbone to extract image features V = {v i } L i=1 for each image I where L is the size of feature grids and v i ∈ R dv is a feature vector with dimension d v .\n sent1: In addition, each feature is further concatenated with its 2-D sine position embedding #REF .\n sent2: Following SOHO, we use a ResNet-101 #TARGET_REF as the visual backbone, followed by additional 1x1 Conv and 2x2 strides Max-pooling to reduce the memory footprint.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "5."
                ],
                [
                    "For",
                    "Hindi,",
                    "we",
                    "use",
                    "the",
                    "gold-standard",
                    "corpus",
                    "of",
                    "#TARGET_REF",
                    ",",
                    "which",
                    "consists",
                    "of",
                    "810",
                    "event",
                    "annotated",
                    "news",
                    "articles",
                    "based",
                    "on",
                    "modified",
                    "TimeML",
                    "rules."
                ],
                [
                    "The",
                    "dataset",
                    "has",
                    "242,201",
                    "tokens",
                    "and",
                    "20,190",
                    "event",
                    "mentions."
                ]
            ],
            "context": [
                0,
                1,
                1
            ]
        },
        "input": "sent0: 5.\n sent1: For Hindi, we use the gold-standard corpus of #TARGET_REF , which consists of 810 event annotated news articles based on modified TimeML rules.\n sent2: The dataset has 242,201 tokens and 20,190 event mentions.\n",
        "output": "{\"INFORMATION\": [\"sent1\", \"sent2\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "•",
                    "and",
                    "the",
                    "WordNet",
                    "database",
                    "#TARGET_REF",
                    "with",
                    "a",
                    "number",
                    "of",
                    "different",
                    "distance",
                    "functions."
                ]
            ],
            "context": [
                1
            ]
        },
        "input": "sent0: • and the WordNet database #TARGET_REF with a number of different distance functions.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "use",
                    "pretrained",
                    "BERT",
                    "2",
                    "#REF",
                    "for",
                    "the",
                    "mention",
                    "and",
                    "context",
                    "encoder."
                ],
                [
                    "This",
                    "BERT-based",
                    "encoder",
                    "accepts",
                    "as",
                    "input",
                    "a",
                    "token",
                    "sequence",
                    "formatted",
                    "asx",
                    "=",
                    "[CLS]",
                    "m",
                    "[SEP]",
                    "s",
                    "[SEP],where",
                    "the",
                    "mention",
                    "m",
                    "and",
                    "context",
                    "s",
                    "are",
                    "chunked",
                    "into",
                    "WordPiece",
                    "tokens",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "We",
                    "encode",
                    "the",
                    "whole",
                    "sequence",
                    "using",
                    "BERT",
                    "and",
                    "use",
                    "the",
                    "hidden",
                    "vector",
                    "at",
                    "the",
                    "[CLS]",
                    "token",
                    "as",
                    "the",
                    "mention",
                    "and",
                    "context",
                    "representation:h",
                    "[CLS]",
                    "=",
                    "BERTENCODER(x).Type",
                    "Embeddings",
                    "This",
                    "output",
                    "layer",
                    "is",
                    "a",
                    "single",
                    "linear",
                    "layer",
                    "whose",
                    "parameter",
                    "matrix",
                    "can",
                    "be",
                    "viewed",
                    "as",
                    "a",
                    "matrix",
                    "of",
                    "type",
                    "embeddings",
                    "E",
                    "∈",
                    "R",
                    "|T",
                    "|×d",
                    ",",
                    "where",
                    "d",
                    "is",
                    "the",
                    "dimension",
                    "of",
                    "the",
                    "mention",
                    "and",
                    "context",
                    "representation",
                    "h",
                    "#REF",
                    "."
                ],
                [
                    "We",
                    "obtain",
                    "the",
                    "output",
                    "probabilities",
                    "t",
                    "by",
                    "multiplying",
                    "E",
                    "by",
                    "h",
                    "[CLS]",
                    ",",
                    "followed",
                    "by",
                    "an",
                    "element-wise",
                    "sigmoid",
                    "function:t",
                    "=",
                    "σ",
                    "(E",
                    "•",
                    "h",
                    "[CLS])."
                ],
                [
                    "3",
                    "Similar",
                    "to",
                    "previous",
                    "work",
                    "#REF",
                    ",",
                    "we",
                    "assume",
                    "independence",
                    "between",
                    "all",
                    "entity",
                    "type",
                    "in",
                    "T",
                    "."
                ]
            ],
            "context": [
                2,
                2,
                0,
                0,
                0
            ]
        },
        "input": "sent0: We use pretrained BERT 2 #REF for the mention and context encoder.\n sent1: This BERT-based encoder accepts as input a token sequence formatted asx = [CLS] m [SEP] s [SEP],where the mention m and context s are chunked into WordPiece tokens #TARGET_REF .\n sent2: We encode the whole sequence using BERT and use the hidden vector at the [CLS] token as the mention and context representation:h [CLS] = BERTENCODER(x).Type Embeddings This output layer is a single linear layer whose parameter matrix can be viewed as a matrix of type embeddings E ∈ R |T |×d , where d is the dimension of the mention and context representation h #REF .\n sent3: We obtain the output probabilities t by multiplying E by h [CLS] , followed by an element-wise sigmoid function:t = σ (E • h [CLS]).\n sent4: 3 Similar to previous work #REF , we assume independence between all entity type in T .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\", \"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "•",
                    "Langage",
                    "identification",
                    "(langid)",
                    "#TARGET_REF",
                    "):",
                    "We",
                    "use",
                    "fastText",
                    "5",
                    "for",
                    "language",
                    "identification",
                    "filtering,",
                    "which",
                    "removes",
                    "sentence",
                    "pairs",
                    "that",
                    "are",
                    "not",
                    "predicted",
                    "as",
                    "the",
                    "correct",
                    "language",
                    "on",
                    "either",
                    "side."
                ]
            ],
            "context": [
                2
            ]
        },
        "input": "sent0: • Langage identification (langid) #TARGET_REF ): We use fastText 5 for language identification filtering, which removes sentence pairs that are not predicted as the correct language on either side.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "It",
                    "is",
                    "not",
                    "clear",
                    "how",
                    "much",
                    "WordNet",
                    "synsets",
                    "should",
                    "be",
                    "expected",
                    "to",
                    "overlap",
                    "with",
                    "Levin",
                    "classes,",
                    "and",
                    "preliminary",
                    "indications",
                    "are",
                    "that",
                    "there",
                    "is",
                    "a",
                    "wide",
                    "discrepancy",
                    "#REF",
                    ",",
                    "#TARGET_REF",
                    ",",
                    "#REF",
                    "."
                ],
                [
                    "However,",
                    "it",
                    "would",
                    "be",
                    "useful",
                    "for",
                    "the",
                    "WordNet",
                    "synsets",
                    "to",
                    "have",
                    "access",
                    "to",
                    "the",
                    "detailed",
                    "syntactic",
                    "information",
                    "that",
                    "the",
                    "Levin",
                    "classes",
                    "contain,",
                    "and",
                    "it",
                    "would",
                    "be",
                    "equally",
                    "useful",
                    "to",
                    "have",
                    "more",
                    "guidance",
                    "as",
                    "to",
                    "when",
                    "membership",
                    "in",
                    "a",
                    "Levin",
                    "class",
                    "does",
                    "in",
                    "fact",
                    "indicate",
                    "shared",
                    "semantic",
                    "components."
                ],
                [
                    "Identification",
                    "of",
                    "these",
                    "components",
                    "is",
                    "critical",
                    "to",
                    "the",
                    "use",
                    "of",
                    "classes",
                    "and",
                    "their",
                    "semantic",
                    "features",
                    "for",
                    "translation",
                    "purposes,",
                    "whether",
                    "transfer-based",
                    "or",
                    "interlingua",
                    "based."
                ],
                [
                    "Although",
                    "Levin",
                    "classes",
                    "group",
                    "together",
                    "verbs",
                    "with",
                    "similar",
                    "argument",
                    "structures,",
                    "the",
                    "meanings",
                    "of",
                    "the",
                    "verbs",
                    "are",
                    "not",
                    "necessarily",
                    "synonymous."
                ],
                [
                    "Some",
                    "classes",
                    "such",
                    "as",
                    "break",
                    "(break,",
                    "chip,",
                    "crack,",
                    "crash,",
                    "crush,",
                    "fracture,",
                    "rip,",
                    "shatter,",
                    "smash,",
                    "snap,",
                    "splinter,",
                    "tear)",
                    "and",
                    "cut",
                    "(chip,",
                    "clip,",
                    "cut,",
                    "hack,",
                    "hew,",
                    "saw,",
                    "scrape,",
                    "scratch,",
                    "slash,",
                    "snip)",
                    "contain",
                    "verbs",
                    "that",
                    "are",
                    "quite",
                    "synonymous,",
                    "but",
                    "others,",
                    "such",
                    "as",
                    "braid",
                    "(bob,",
                    "braid,",
                    "brush,",
                    "clip,",
                    "coldcream,",
                    "comb,",
                    "condition,",
                    "crimp,",
                    "crop,",
                    "curl,",
                    "etc.)"
                ],
                [
                    "do",
                    "not,",
                    "which",
                    "at",
                    "least",
                    "partly",
                    "explains",
                    "the",
                    "lack",
                    "of",
                    "overlap",
                    "between",
                    "Levin",
                    "and",
                    "WordNet."
                ]
            ],
            "context": [
                1,
                3,
                2,
                2,
                0,
                0
            ]
        },
        "input": "sent0: It is not clear how much WordNet synsets should be expected to overlap with Levin classes, and preliminary indications are that there is a wide discrepancy #REF , #TARGET_REF , #REF .\n sent1: However, it would be useful for the WordNet synsets to have access to the detailed syntactic information that the Levin classes contain, and it would be equally useful to have more guidance as to when membership in a Levin class does in fact indicate shared semantic components.\n sent2: Identification of these components is critical to the use of classes and their semantic features for translation purposes, whether transfer-based or interlingua based.\n sent3: Although Levin classes group together verbs with similar argument structures, the meanings of the verbs are not necessarily synonymous.\n sent4: Some classes such as break (break, chip, crack, crash, crush, fracture, rip, shatter, smash, snap, splinter, tear) and cut (chip, clip, cut, hack, hew, saw, scrape, scratch, slash, snip) contain verbs that are quite synonymous, but others, such as braid (bob, braid, brush, clip, coldcream, comb, condition, crimp, crop, curl, etc.)\n sent5: do not, which at least partly explains the lack of overlap between Levin and WordNet.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [\"sent2\", \"sent3\"], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "This",
                    "task",
                    "is",
                    "highly",
                    "related",
                    "to",
                    "word",
                    "association",
                    "modeling,",
                    "which",
                    "has",
                    "been",
                    "studied",
                    "extensively",
                    "in",
                    "psycholinguistics",
                    "for",
                    "a",
                    "long",
                    "time",
                    "#TARGET_REF",
                    ",",
                    "but",
                    "is",
                    "by",
                    "no",
                    "means",
                    "equivalent",
                    "to",
                    "it."
                ],
                [
                    "In",
                    "word",
                    "association",
                    "experiments,",
                    "subjects",
                    "should",
                    "name",
                    "any",
                    "word",
                    "associated",
                    "with",
                    "a",
                    "given",
                    "word",
                    "as",
                    "quickly",
                    "as",
                    "possible,",
                    "but",
                    "in",
                    "this",
                    "case,",
                    "the",
                    "spymaster's",
                    "task",
                    "is",
                    "to",
                    "find",
                    "a",
                    "word",
                    "that",
                    "is",
                    "related",
                    "to",
                    "as",
                    "many",
                    "words",
                    "from",
                    "a",
                    "given",
                    "set",
                    "as",
                    "possible,",
                    "but",
                    "not",
                    "or",
                    "significantly",
                    "less",
                    "closely",
                    "to",
                    "a",
                    "set",
                    "of",
                    "other",
                    "words."
                ],
                [
                    "The",
                    "time",
                    "allotted",
                    "for",
                    "the",
                    "task",
                    "is",
                    "also",
                    "limited",
                    "at",
                    "most",
                    "very",
                    "loosely",
                    "(by",
                    "the",
                    "patience",
                    "of",
                    "the",
                    "other",
                    "players),",
                    "and",
                    "based",
                    "on",
                    "personal",
                    "experiences,",
                    "spymasters",
                    "often",
                    "use",
                    "several",
                    "minutes",
                    "of",
                    "thinking",
                    "time",
                    "to",
                    "come",
                    "up",
                    "with",
                    "the",
                    "right",
                    "clue."
                ],
                [
                    "For",
                    "this",
                    "reason,",
                    "connected",
                    "words",
                    "are",
                    "often",
                    "related",
                    "in",
                    "a",
                    "complex",
                    "way,",
                    "even",
                    "indirectly."
                ],
                [
                    "The",
                    "task",
                    "of",
                    "agents",
                    "-to",
                    "find",
                    "words",
                    "in",
                    "the",
                    "table",
                    "related",
                    "to",
                    "the",
                    "clue",
                    "word",
                    "-is",
                    "more",
                    "like",
                    "simple",
                    "associations,",
                    "but",
                    "time",
                    "is",
                    "not",
                    "dominant",
                    "here",
                    "either,",
                    "and",
                    "more",
                    "complex,",
                    "indirect",
                    "relations",
                    "also",
                    "matter."
                ],
                [
                    "In",
                    "a",
                    "game",
                    "between",
                    "people,",
                    "the",
                    "relationship",
                    "and",
                    "common",
                    "knowledge",
                    "between",
                    "the",
                    "players",
                    "can",
                    "also",
                    "count,",
                    "but",
                    "this",
                    "is",
                    "not",
                    "an",
                    "influencing",
                    "factor",
                    "in",
                    "a",
                    "game",
                    "with",
                    "an",
                    "agent."
                ]
            ],
            "context": [
                1,
                2,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: This task is highly related to word association modeling, which has been studied extensively in psycholinguistics for a long time #TARGET_REF , but is by no means equivalent to it.\n sent1: In word association experiments, subjects should name any word associated with a given word as quickly as possible, but in this case, the spymaster's task is to find a word that is related to as many words from a given set as possible, but not or significantly less closely to a set of other words.\n sent2: The time allotted for the task is also limited at most very loosely (by the patience of the other players), and based on personal experiences, spymasters often use several minutes of thinking time to come up with the right clue.\n sent3: For this reason, connected words are often related in a complex way, even indirectly.\n sent4: The task of agents -to find words in the table related to the clue word -is more like simple associations, but time is not dominant here either, and more complex, indirect relations also matter.\n sent5: In a game between people, the relationship and common knowledge between the players can also count, but this is not an influencing factor in a game with an agent.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "our",
                    "experiments,",
                    "we",
                    "use",
                    "ChAII",
                    "#REF",
                    "question-answering",
                    "dataset",
                    "for",
                    "fine-tuning",
                    "and",
                    "evaluation."
                ],
                [
                    "This",
                    "dataset",
                    "was",
                    "recently",
                    "released",
                    "by",
                    "Google",
                    "Research",
                    "India",
                    "and",
                    "has",
                    "1,114",
                    "records",
                    "of",
                    "context,",
                    "question,",
                    "answer,",
                    "and",
                    "its",
                    "corresponding",
                    "start",
                    "position",
                    "in",
                    "the",
                    "context",
                    "for",
                    "Tamil",
                    "and",
                    "Hindi",
                    "languages."
                ],
                [
                    "Hindi",
                    "is",
                    "represented",
                    "predominantly",
                    "in",
                    "the",
                    "dataset",
                    "with",
                    "nearly",
                    "two-thirds",
                    "of",
                    "the",
                    "records."
                ],
                [
                    "As",
                    "the",
                    "ChAII",
                    "dataset",
                    "has",
                    "been",
                    "published",
                    "as",
                    "part",
                    "of",
                    "an",
                    "ongoing",
                    "Kaggle",
                    "competition",
                    "#TARGET_REF",
                    ",",
                    "the",
                    "complete",
                    "test",
                    "dataset",
                    "has",
                    "not",
                    "been",
                    "disclosed",
                    "to",
                    "the",
                    "public."
                ],
                [
                    "Hence,",
                    "we",
                    "have",
                    "used",
                    "Scikit-learn's",
                    "train_test_split",
                    "method",
                    "with",
                    "a",
                    "test",
                    "size",
                    "of",
                    "100,",
                    "stratified",
                    "on",
                    "language",
                    "and",
                    "with",
                    "a",
                    "random",
                    "seed",
                    "of",
                    "0,",
                    "to",
                    "get",
                    "the",
                    "test",
                    "split",
                    "from",
                    "the",
                    "training",
                    "data."
                ],
                [
                    "Similarly,",
                    "we",
                    "applied",
                    "the",
                    "same",
                    "method",
                    "over",
                    "the",
                    "filtered",
                    "train",
                    "split",
                    "to",
                    "get",
                    "the",
                    "validation",
                    "split",
                    "of",
                    "100",
                    "samples."
                ],
                [
                    "We",
                    "also",
                    "use",
                    "the",
                    "translations",
                    "and",
                    "transliterations",
                    "of",
                    "this",
                    "training",
                    "split",
                    "as",
                    "augmented",
                    "samples",
                    "for",
                    "fine-tuning",
                    "the",
                    "QA",
                    "model."
                ],
                [
                    "Stanford",
                    "Question",
                    "Answering",
                    "Dataset",
                    "(SQuAD)",
                    "#REF",
                    "is",
                    "the",
                    "most",
                    "popular",
                    "question-answering",
                    "dataset",
                    "in",
                    "English."
                ],
                [
                    "This",
                    "dataset",
                    "had",
                    "been",
                    "crowdsourced",
                    "to",
                    "form",
                    "100K",
                    "records",
                    "of",
                    "answerable",
                    "question-answer",
                    "pairs",
                    "along",
                    "with",
                    "the",
                    "context."
                ],
                [
                    "This",
                    "dataset",
                    "is",
                    "used",
                    "to",
                    "pre-train",
                    "the",
                    "QA",
                    "head",
                    "added",
                    "to",
                    "the",
                    "pre-trained",
                    "mBERT",
                    "model,",
                    "which",
                    "is",
                    "subsequently",
                    "fine-tuned",
                    "using",
                    "the",
                    "ChAII",
                    "dataset."
                ]
            ],
            "context": [
                2,
                3,
                3,
                3,
                2,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: In our experiments, we use ChAII #REF question-answering dataset for fine-tuning and evaluation.\n sent1: This dataset was recently released by Google Research India and has 1,114 records of context, question, answer, and its corresponding start position in the context for Tamil and Hindi languages.\n sent2: Hindi is represented predominantly in the dataset with nearly two-thirds of the records.\n sent3: As the ChAII dataset has been published as part of an ongoing Kaggle competition #TARGET_REF , the complete test dataset has not been disclosed to the public.\n sent4: Hence, we have used Scikit-learn's train_test_split method with a test size of 100, stratified on language and with a random seed of 0, to get the test split from the training data.\n sent5: Similarly, we applied the same method over the filtered train split to get the validation split of 100 samples.\n sent6: We also use the translations and transliterations of this training split as augmented samples for fine-tuning the QA model.\n sent7: Stanford Question Answering Dataset (SQuAD) #REF is the most popular question-answering dataset in English.\n sent8: This dataset had been crowdsourced to form 100K records of answerable question-answer pairs along with the context.\n sent9: This dataset is used to pre-train the QA head added to the pre-trained mBERT model, which is subsequently fine-tuned using the ChAII dataset.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\", \"sent4\"], \"BACKGROUND\": [\"sent1\", \"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "TNT",
                    "tagger",
                    "#TARGET_REF",
                    ",",
                    "which",
                    "was",
                    "used",
                    "in",
                    "the",
                    "process,",
                    "relies",
                    "heavily",
                    "on",
                    "context",
                    "to",
                    "disambiguate",
                    "ambiguities."
                ],
                [
                    "In",
                    "a",
                    "word",
                    "list",
                    "each",
                    "word",
                    "is",
                    "treated",
                    "separately,",
                    "there",
                    "is",
                    "no",
                    "context,",
                    "so",
                    "the",
                    "word",
                    "tagging",
                    "quality",
                    "is",
                    "lower",
                    "than",
                    "the",
                    "values",
                    "on",
                    "running",
                    "text."
                ]
            ],
            "context": [
                1,
                0
            ]
        },
        "input": "sent0: The TNT tagger #TARGET_REF , which was used in the process, relies heavily on context to disambiguate ambiguities.\n sent1: In a word list each word is treated separately, there is no context, so the word tagging quality is lower than the values on running text.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Bridging",
                    "vs.",
                    "TNE",
                    "Bridging",
                    "has",
                    "been",
                    "extensively",
                    "studied",
                    "in",
                    "the",
                    "past",
                    "decades,",
                    "as",
                    "we",
                    "discuss",
                    "in",
                    "§10."
                ],
                [
                    "Here,",
                    "we",
                    "explore",
                    "how",
                    "many",
                    "of",
                    "the",
                    "relations",
                    "we",
                    "collected",
                    "correspond",
                    "to",
                    "the",
                    "definition",
                    "of",
                    "bridging."
                ],
                [
                    "We",
                    "use",
                    "the",
                    "same",
                    "three",
                    "documents",
                    "from",
                    "the",
                    "analysis",
                    "described",
                    "above,",
                    "and",
                    "follow",
                    "the",
                    "annotation",
                    "scheme",
                    "from",
                    "ISNotes1.0",
                    "#REF",
                    "16",
                    "to",
                    "annotate",
                    "them",
                    "for",
                    "bridging."
                ],
                [
                    "We",
                    "found",
                    "that",
                    "15",
                    "out",
                    "of",
                    "the",
                    "590",
                    "links",
                    "(2.5%)",
                    "in",
                    "these",
                    "documents",
                    "are",
                    "bridging",
                    "links",
                    "(i.e.,",
                    "meet",
                    "the",
                    "criteria",
                    "for",
                    "bridging",
                    "defined",
                    "in",
                    "ISNotes)."
                ],
                [
                    "These",
                    "three",
                    "documents",
                    "contain",
                    "104",
                    "NPs,",
                    "that",
                    "is,",
                    "the",
                    "ratio",
                    "of",
                    "bridging",
                    "links",
                    "per",
                    "NP",
                    "is",
                    "0.14."
                ],
                [
                    "While",
                    "the",
                    "ratio",
                    "is",
                    "small,",
                    "it",
                    "is",
                    "larger",
                    "than",
                    "the",
                    "ratio",
                    "in",
                    "ISNotes,",
                    "which",
                    "contains",
                    "663",
                    "bridging",
                    "links",
                    "out",
                    "of",
                    "11K",
                    "annotated",
                    "NPs",
                    "#TARGET_REF",
                    ",",
                    "that",
                    "is,",
                    "0.06",
                    "bridging",
                    "links",
                    "per",
                    "NP."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "sent0: Bridging vs. TNE Bridging has been extensively studied in the past decades, as we discuss in §10.\n sent1: Here, we explore how many of the relations we collected correspond to the definition of bridging.\n sent2: We use the same three documents from the analysis described above, and follow the annotation scheme from ISNotes1.0 #REF 16 to annotate them for bridging.\n sent3: We found that 15 out of the 590 links (2.5%) in these documents are bridging links (i.e., meet the criteria for bridging defined in ISNotes).\n sent4: These three documents contain 104 NPs, that is, the ratio of bridging links per NP is 0.14.\n sent5: While the ratio is small, it is larger than the ratio in ISNotes, which contains 663 bridging links out of 11K annotated NPs #TARGET_REF , that is, 0.06 bridging links per NP.\n",
        "output": "{\"INFORMATION\": [\"sent5\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "this",
                    "paper",
                    "we",
                    "describe",
                    "the",
                    "machine",
                    "translation",
                    "systems",
                    "built",
                    "for",
                    "our",
                    "participation",
                    "in",
                    "IWSLT",
                    "2011",
                    "evaluation",
                    "campaign",
                    "#TARGET_REF",
                    "for",
                    "the",
                    "Arabic-English",
                    "(Ar-En)",
                    "and",
                    "Chinese-English",
                    "(Zh-En)",
                    "MT",
                    "track",
                    "translation",
                    "tasks."
                ],
                [
                    "We",
                    "use",
                    "different",
                    "SMT",
                    "models,",
                    "ranging",
                    "from",
                    "standard",
                    "phrase-based",
                    "SMT",
                    "models",
                    "#REF",
                    "to",
                    "CCG-augmented",
                    "hierarchical",
                    "phrasebased",
                    "models",
                    "#REF",
                    "to",
                    "translate",
                    "the",
                    "test",
                    "data",
                    "provided."
                ],
                [
                    "The",
                    "open-domain",
                    "nature",
                    "of",
                    "the",
                    "data",
                    "and",
                    "the",
                    "restricted",
                    "size",
                    "of",
                    "the",
                    "in-domain",
                    "training",
                    "corpora",
                    "necessitated",
                    "the",
                    "use",
                    "of",
                    "domain",
                    "adaptation",
                    "techniques",
                    "to",
                    "improve",
                    "translation",
                    "quality."
                ]
            ],
            "context": [
                2,
                0,
                0
            ]
        },
        "input": "sent0: In this paper we describe the machine translation systems built for our participation in IWSLT 2011 evaluation campaign #TARGET_REF for the Arabic-English (Ar-En) and Chinese-English (Zh-En) MT track translation tasks.\n sent1: We use different SMT models, ranging from standard phrase-based SMT models #REF to CCG-augmented hierarchical phrasebased models #REF to translate the test data provided.\n sent2: The open-domain nature of the data and the restricted size of the in-domain training corpora necessitated the use of domain adaptation techniques to improve translation quality.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Considering",
                    "the",
                    "previous",
                    "results",
                    "on",
                    "the",
                    "relationship",
                    "between",
                    "associations",
                    "and",
                    "co-occurrences",
                    "(Spence",
                    "and",
                    "#TARGET_REF",
                    ",",
                    "we",
                    "create",
                    "our",
                    "distance",
                    "matrices",
                    "not",
                    "from",
                    "the",
                    "latest",
                    "neural",
                    "methods",
                    "of",
                    "NLP,",
                    "but",
                    "from",
                    "co-occurrences",
                    "counted",
                    "1",
                    "The",
                    "game:",
                    "http://spymasters.herokuapp.com/",
                    "Source",
                    "code",
                    "and",
                    "data:",
                    "https://github.com/xerevity/",
                    "CodeNamesAgent",
                    "in",
                    "raw",
                    "text."
                ],
                [
                    "As",
                    "English",
                    "corpora",
                    "we",
                    "use",
                    "the",
                    "concatenation",
                    "of",
                    "the",
                    "English",
                    "Wikipedia",
                    "and",
                    "the",
                    "English",
                    "OpenSubtitles",
                    "corpus,",
                    "consisting",
                    "of",
                    "5.692",
                    "billion",
                    "tokens",
                    "in",
                    "total."
                ],
                [
                    "For",
                    "Hungarian,",
                    "we",
                    "use",
                    "the",
                    "lemmatized",
                    "version",
                    "of",
                    "the",
                    "Hungarian",
                    "Webcorpus",
                    "(Nemeskey,",
                    "2020),",
                    "also",
                    "including",
                    "the",
                    "Hungarian",
                    "Wikipedia",
                    "(1.414",
                    "billion",
                    "tokens)."
                ],
                [
                    "We",
                    "work",
                    "with",
                    "vocabulary",
                    "sizes",
                    "15K",
                    "in",
                    "English",
                    "and",
                    "10K",
                    "in",
                    "Hungarian,",
                    "and",
                    "remove",
                    "stopwords."
                ]
            ],
            "context": [
                2,
                2,
                2,
                2
            ]
        },
        "input": "sent0: Considering the previous results on the relationship between associations and co-occurrences (Spence and #TARGET_REF , we create our distance matrices not from the latest neural methods of NLP, but from co-occurrences counted 1 The game: http://spymasters.herokuapp.com/ Source code and data: https://github.com/xerevity/ CodeNamesAgent in raw text.\n sent1: As English corpora we use the concatenation of the English Wikipedia and the English OpenSubtitles corpus, consisting of 5.692 billion tokens in total.\n sent2: For Hungarian, we use the lemmatized version of the Hungarian Webcorpus (Nemeskey, 2020), also including the Hungarian Wikipedia (1.414 billion tokens).\n sent3: We work with vocabulary sizes 15K in English and 10K in Hungarian, and remove stopwords.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\", \"sent1\", \"sent2\", \"sent3\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "For",
                    "our",
                    "automated",
                    "slot",
                    "labelling,",
                    "we",
                    "generated",
                    "the",
                    "game",
                    "toxicity",
                    "lexicon",
                    "by",
                    "taking",
                    "the",
                    "supplemental",
                    "materials",
                    "released",
                    "by",
                    "#TARGET_REF",
                    "and",
                    "#REF",
                    "and",
                    "the",
                    "list",
                    "of",
                    "words",
                    "banned",
                    "by",
                    "Google",
                    "6",
                    "."
                ],
                [
                    "We",
                    "then",
                    "added",
                    "variants",
                    "or",
                    "new",
                    "toxic",
                    "words",
                    "found",
                    "in",
                    "the",
                    "utterances",
                    "extracted",
                    "from",
                    "Kaggle."
                ],
                [
                    "For",
                    "intent",
                    "labelling,",
                    "all",
                    "volunteer",
                    "annotators",
                    "were",
                    "recruited",
                    "from",
                    "academia",
                    "and",
                    "research",
                    "students."
                ],
                [
                    "They",
                    "were",
                    "informed",
                    "about",
                    "toxic",
                    "behavior",
                    "in",
                    "online",
                    "games",
                    "before",
                    "handling",
                    "the",
                    "data."
                ],
                [
                    "Our",
                    "instructions",
                    "allowed",
                    "them",
                    "to",
                    "feel",
                    "free",
                    "to",
                    "leave",
                    "if",
                    "they",
                    "were",
                    "uncomfortable",
                    "with",
                    "the",
                    "content."
                ],
                [
                    "Due",
                    "to",
                    "privacy",
                    "considerations,",
                    "we",
                    "group",
                    "them",
                    "by",
                    "online",
                    "game",
                    "experiences",
                    "and",
                    "do",
                    "not",
                    "take",
                    "into",
                    "account",
                    "annotators'",
                    "demographic",
                    "information."
                ]
            ],
            "context": [
                2,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: For our automated slot labelling, we generated the game toxicity lexicon by taking the supplemental materials released by #TARGET_REF and #REF and the list of words banned by Google 6 .\n sent1: We then added variants or new toxic words found in the utterances extracted from Kaggle.\n sent2: For intent labelling, all volunteer annotators were recruited from academia and research students.\n sent3: They were informed about toxic behavior in online games before handling the data.\n sent4: Our instructions allowed them to feel free to leave if they were uncomfortable with the content.\n sent5: Due to privacy considerations, we group them by online game experiences and do not take into account annotators' demographic information.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "There",
                    "are",
                    "two",
                    "obvious",
                    "approaches",
                    "to",
                    "try",
                    "and",
                    "answer",
                    "this",
                    "question."
                ],
                [
                    "The",
                    "first",
                    "is",
                    "to",
                    "simply",
                    "consider",
                    "transformer-based",
                    "features",
                    "(e.g.,",
                    "BERT",
                    "score,",
                    "ColBERT",
                    "score,",
                    "etc.)"
                ],
                [
                    "as",
                    "yet",
                    "another",
                    "feature",
                    "within",
                    "a",
                    "learning-to-rank",
                    "framework-for",
                    "example,",
                    "with",
                    "gradient",
                    "boosted",
                    "decision",
                    "trees",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "This",
                    "is",
                    "not",
                    "the",
                    "route",
                    "that",
                    "we",
                    "take,",
                    "because",
                    "this",
                    "approach",
                    "has",
                    "less",
                    "bearing",
                    "on",
                    "our",
                    "desire",
                    "to",
                    "increase",
                    "the",
                    "efficiency",
                    "of",
                    "transformer-based",
                    "models."
                ],
                [
                    "Instead,",
                    "we",
                    "take",
                    "the",
                    "alternative",
                    "approach",
                    "of",
                    "using",
                    "learningto-rank",
                    "techniques",
                    "as",
                    "a",
                    "\"filtering\"",
                    "stage",
                    "in",
                    "a",
                    "multistage",
                    "ranking",
                    "architecture",
                    "to",
                    "reduce",
                    "the",
                    "number",
                    "of",
                    "candidates",
                    "under",
                    "consideration",
                    "by",
                    "BERT."
                ],
                [
                    "More",
                    "concretely,",
                    "we",
                    "find",
                    "that",
                    "a",
                    "design",
                    "based",
                    "on",
                    "this",
                    "idea",
                    "achieves",
                    "the",
                    "same",
                    "level",
                    "of",
                    "effectiveness",
                    "as",
                    "a",
                    "standard",
                    "retrieve-and-rerank",
                    "approach",
                    "using",
                    "BERT,",
                    "but",
                    "is",
                    "up",
                    "to",
                    "18×",
                    "faster."
                ],
                [
                    "Other",
                    "effectiveness-efficiency",
                    "tradeoffs",
                    "are",
                    "possible,",
                    "giving",
                    "developers",
                    "a",
                    "rich",
                    "design",
                    "space",
                    "to",
                    "build",
                    "systems",
                    "tailored",
                    "to",
                    "different",
                    "application",
                    "scenarios."
                ]
            ],
            "context": [
                2,
                2,
                2,
                2,
                0,
                0,
                0
            ]
        },
        "input": "sent0: There are two obvious approaches to try and answer this question.\n sent1: The first is to simply consider transformer-based features (e.g., BERT score, ColBERT score, etc.)\n sent2: as yet another feature within a learning-to-rank framework-for example, with gradient boosted decision trees #TARGET_REF .\n sent3: This is not the route that we take, because this approach has less bearing on our desire to increase the efficiency of transformer-based models.\n sent4: Instead, we take the alternative approach of using learningto-rank techniques as a \"filtering\" stage in a multistage ranking architecture to reduce the number of candidates under consideration by BERT.\n sent5: More concretely, we find that a design based on this idea achieves the same level of effectiveness as a standard retrieve-and-rerank approach using BERT, but is up to 18× faster.\n sent6: Other effectiveness-efficiency tradeoffs are possible, giving developers a rich design space to build systems tailored to different application scenarios.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\", \"sent1\", \"sent2\", \"sent3\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Lastly,",
                    "we",
                    "test",
                    "two",
                    "recent",
                    "models",
                    "from",
                    "stance",
                    "detection",
                    "and",
                    "dis/agreement",
                    "classification."
                ],
                [
                    "TGA",
                    "Net",
                    "#TARGET_REF",
                    "takes",
                    "a",
                    "statement-topic",
                    "pair",
                    "and",
                    "predicts",
                    "the",
                    "statement's",
                    "stance."
                ],
                [
                    "It",
                    "encodes",
                    "the",
                    "input",
                    "using",
                    "BERT",
                    "and",
                    "weighs",
                    "topic",
                    "tokens",
                    "based",
                    "on",
                    "similarity",
                    "to",
                    "other",
                    "topics."
                ],
                [
                    "In",
                    "our",
                    "task,",
                    "claims",
                    "serve",
                    "as",
                    "''topics''."
                ],
                [
                    "We",
                    "use",
                    "the",
                    "published",
                    "implementation,",
                    "exploring",
                    "{50,",
                    "100,",
                    "150,",
                    "200}",
                    "for",
                    "the",
                    "number",
                    "of",
                    "clusters",
                    "and",
                    "increasing",
                    "the",
                    "max",
                    "input",
                    "size",
                    "to",
                    "the",
                    "BERT",
                    "input",
                    "size."
                ],
                [
                    "Hybrid",
                    "Net",
                    "#REF",
                    "takes",
                    "a",
                    "quote-response",
                    "pair",
                    "and",
                    "predicts",
                    "whether",
                    "the",
                    "response",
                    "agrees",
                    "or",
                    "disagrees",
                    "with",
                    "the",
                    "quote."
                ],
                [
                    "It",
                    "encodes",
                    "the",
                    "input",
                    "using",
                    "BiLSTM",
                    "and",
                    "uses",
                    "selfand",
                    "cross-attention",
                    "between",
                    "tokens."
                ],
                [
                    "In",
                    "our",
                    "task,",
                    "claims",
                    "and",
                    "statements",
                    "serve",
                    "as",
                    "''quotes''",
                    "and",
                    "''responses'',",
                    "respectively."
                ]
            ],
            "context": [
                2,
                1,
                1,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Lastly, we test two recent models from stance detection and dis/agreement classification.\n sent1: TGA Net #TARGET_REF takes a statement-topic pair and predicts the statement's stance.\n sent2: It encodes the input using BERT and weighs topic tokens based on similarity to other topics.\n sent3: In our task, claims serve as ''topics''.\n sent4: We use the published implementation, exploring {50, 100, 150, 200} for the number of clusters and increasing the max input size to the BERT input size.\n sent5: Hybrid Net #REF takes a quote-response pair and predicts whether the response agrees or disagrees with the quote.\n sent6: It encodes the input using BiLSTM and uses selfand cross-attention between tokens.\n sent7: In our task, claims and statements serve as ''quotes'' and ''responses'', respectively.\n",
        "output": "{\"INFORMATION\": [\"sent1\", \"sent2\"], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "fact,",
                    "deploying",
                    "ever-larger",
                    "models",
                    "raises",
                    "questions",
                    "and",
                    "concerns",
                    "about",
                    "the",
                    "increasing",
                    "magnitude",
                    "of",
                    "the",
                    "temporal,",
                    "financial,",
                    "and",
                    "environmental",
                    "cost",
                    "of",
                    "training",
                    "and",
                    "usability",
                    "#REF",
                    "."
                ],
                [
                    "Typically,",
                    "due",
                    "to",
                    "their",
                    "resource",
                    "requirements,",
                    "these",
                    "models",
                    "are",
                    "trained",
                    "and",
                    "deployed",
                    "for",
                    "industrial",
                    "operations",
                    "on",
                    "remote",
                    "servers."
                ],
                [
                    "This",
                    "leads",
                    "to",
                    "a",
                    "high",
                    "use",
                    "of",
                    "over-the-air",
                    "communications,",
                    "which",
                    "are",
                    "particularly",
                    "resourceintensive",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "In",
                    "particular,",
                    "some",
                    "NLP",
                    "applications",
                    "(speech",
                    "recognition,",
                    "speech",
                    "to",
                    "text,",
                    "etc.)"
                ],
                [
                    "have",
                    "some",
                    "known",
                    "problems",
                    "related",
                    "to",
                    "network",
                    "latency,",
                    "transmission",
                    "path",
                    "difficulties,",
                    "or",
                    "privacy",
                    "concerns."
                ],
                [
                    "To",
                    "reduce",
                    "the",
                    "impact",
                    "of",
                    "these",
                    "communications,",
                    "there",
                    "is",
                    "a",
                    "solution",
                    "that",
                    "is",
                    "to",
                    "allow",
                    "these",
                    "models",
                    "to",
                    "run",
                    "directly",
                    "on",
                    "peripheral",
                    "or",
                    "mobile",
                    "devices,",
                    "that",
                    "is,",
                    "in",
                    "environments",
                    "with",
                    "limited",
                    "resources",
                    "that",
                    "require",
                    "lightweight,",
                    "responsive",
                    "models",
                    "and",
                    "energy",
                    "efficiency."
                ],
                [
                    "Reducing",
                    "the",
                    "size",
                    "of",
                    "the",
                    "models",
                    "is",
                    "therefore",
                    "one",
                    "of",
                    "the",
                    "increasingly",
                    "favoured",
                    "avenues,",
                    "especially",
                    "for",
                    "the",
                    "reduction",
                    "of",
                    "memory",
                    "resources",
                    "and",
                    "computation",
                    "time",
                    "involved",
                    "in",
                    "training",
                    "and",
                    "use."
                ]
            ],
            "context": [
                0,
                2,
                1,
                0,
                3,
                0,
                0
            ]
        },
        "input": "sent0: In fact, deploying ever-larger models raises questions and concerns about the increasing magnitude of the temporal, financial, and environmental cost of training and usability #REF .\n sent1: Typically, due to their resource requirements, these models are trained and deployed for industrial operations on remote servers.\n sent2: This leads to a high use of over-the-air communications, which are particularly resourceintensive #TARGET_REF .\n sent3: In particular, some NLP applications (speech recognition, speech to text, etc.)\n sent4: have some known problems related to network latency, transmission path difficulties, or privacy concerns.\n sent5: To reduce the impact of these communications, there is a solution that is to allow these models to run directly on peripheral or mobile devices, that is, in environments with limited resources that require lightweight, responsive models and energy efficiency.\n sent6: Reducing the size of the models is therefore one of the increasingly favoured avenues, especially for the reduction of memory resources and computation time involved in training and use.\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "With",
                    "the",
                    "advent",
                    "of",
                    "social",
                    "media,",
                    "anti-social",
                    "and",
                    "abusive",
                    "behavior",
                    "has",
                    "become",
                    "a",
                    "prominent",
                    "occurrence",
                    "online."
                ],
                [
                    "Undesirable",
                    "psychological",
                    "effects",
                    "of",
                    "abuse",
                    "on",
                    "individuals",
                    "make",
                    "it",
                    "an",
                    "important",
                    "societal",
                    "problem",
                    "of",
                    "our",
                    "time."
                ],
                [
                    "#TARGET_REF",
                    "studied",
                    "the",
                    "ill-effects",
                    "of",
                    "online",
                    "abuse",
                    "on",
                    "children,",
                    "concluding",
                    "that",
                    "children",
                    "may",
                    "develop",
                    "depression,",
                    "anxiety,",
                    "and",
                    "other",
                    "mental",
                    "health",
                    "problems",
                    "as",
                    "a",
                    "result",
                    "of",
                    "their",
                    "encounters",
                    "online."
                ],
                [
                    "Pew",
                    "Research",
                    "Center,",
                    "in",
                    "its",
                    "latest",
                    "report",
                    "on",
                    "online",
                    "harassment",
                    "#REF",
                    ",",
                    "revealed",
                    "that",
                    "40%",
                    "of",
                    "adults",
                    "in",
                    "the",
                    "United",
                    "States",
                    "have",
                    "experienced",
                    "abusive",
                    "behavior",
                    "online,",
                    "of",
                    "which",
                    "18%",
                    "have",
                    "faced",
                    "severe",
                    "forms",
                    "of",
                    "harassment,",
                    "e.g.,",
                    "that",
                    "of",
                    "sexual",
                    "nature."
                ],
                [
                    "These",
                    "statistics",
                    "stress",
                    "the",
                    "need",
                    "for",
                    "automated",
                    "detection",
                    "and",
                    "moderation",
                    "systems."
                ],
                [
                    "Hence,",
                    "in",
                    "recent",
                    "years,",
                    "a",
                    "new",
                    "research",
                    "effort",
                    "on",
                    "abusive",
                    "language",
                    "detection",
                    "has",
                    "sprung",
                    "up",
                    "in",
                    "NLP."
                ]
            ],
            "context": [
                3,
                0,
                1,
                0,
                2,
                0
            ]
        },
        "input": "sent0: With the advent of social media, anti-social and abusive behavior has become a prominent occurrence online.\n sent1: Undesirable psychological effects of abuse on individuals make it an important societal problem of our time.\n sent2: #TARGET_REF studied the ill-effects of online abuse on children, concluding that children may develop depression, anxiety, and other mental health problems as a result of their encounters online.\n sent3: Pew Research Center, in its latest report on online harassment #REF , revealed that 40% of adults in the United States have experienced abusive behavior online, of which 18% have faced severe forms of harassment, e.g., that of sexual nature.\n sent4: These statistics stress the need for automated detection and moderation systems.\n sent5: Hence, in recent years, a new research effort on abusive language detection has sprung up in NLP.\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [\"sent4\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "Spanish",
                    "language",
                    "news",
                    "articles",
                    "used",
                    "in",
                    "this",
                    "study",
                    "lack",
                    "corresponding",
                    "reference",
                    "translations."
                ],
                [
                    "Thus,",
                    "unlike",
                    "the",
                    "case",
                    "of",
                    "our",
                    "Russian",
                    "data,",
                    "no",
                    "matter",
                    "how",
                    "high",
                    "the",
                    "quality",
                    "of",
                    "machine",
                    "translations,",
                    "no",
                    "Spanish-English",
                    "machine",
                    "translation",
                    "segment",
                    "could",
                    "possibly",
                    "receive",
                    "a",
                    "score",
                    "of",
                    "12."
                ],
                [
                    "For",
                    "Spanish-English,",
                    "we",
                    "therefore",
                    "follow",
                    "the",
                    "10-point",
                    "adequacy",
                    "scale",
                    "of",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "This",
                    "adequacy",
                    "scale",
                    "is",
                    "shown",
                    "in",
                    "Table",
                    "1b",
                    "on",
                    "page",
                    "2,",
                    "this",
                    "scale",
                    "is",
                    "very",
                    "similar",
                    "to",
                    "the",
                    "former,",
                    "but",
                    "has",
                    "a",
                    "high",
                    "of",
                    "10",
                    "(the",
                    "meaning",
                    "of",
                    "the",
                    "source",
                    "sentence",
                    "is",
                    "fully",
                    "conveyed",
                    "in",
                    "the",
                    "English",
                    "translation)",
                    "instead",
                    "of",
                    "12."
                ]
            ],
            "context": [
                0,
                0,
                2,
                0
            ]
        },
        "input": "sent0: The Spanish language news articles used in this study lack corresponding reference translations.\n sent1: Thus, unlike the case of our Russian data, no matter how high the quality of machine translations, no Spanish-English machine translation segment could possibly receive a score of 12.\n sent2: For Spanish-English, we therefore follow the 10-point adequacy scale of #TARGET_REF .\n sent3: This adequacy scale is shown in Table 1b on page 2, this scale is very similar to the former, but has a high of 10 (the meaning of the source sentence is fully conveyed in the English translation) instead of 12.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "will",
                    "describe",
                    "parsing",
                    "algorithms",
                    "using",
                    "Parsing",
                    "Schemata,",
                    "a",
                    "framework",
                    "for",
                    "high-level",
                    "description",
                    "of",
                    "parsing",
                    "algorithms",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "An",
                    "interesting",
                    "application",
                    "of",
                    "this",
                    "framework",
                    "is",
                    "the",
                    "analysis",
                    "of",
                    "the",
                    "relations",
                    "between",
                    "different",
                    "parsing",
                    "algorithms",
                    "by",
                    "studying",
                    "the",
                    "formal",
                    "relations",
                    "between",
                    "their",
                    "underlying",
                    "parsing",
                    "schemata."
                ]
            ],
            "context": [
                1,
                3
            ]
        },
        "input": "sent0: We will describe parsing algorithms using Parsing Schemata, a framework for high-level description of parsing algorithms #TARGET_REF .\n sent1: An interesting application of this framework is the analysis of the relations between different parsing algorithms by studying the formal relations between their underlying parsing schemata.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "Champollion",
                    "system",
                    "was",
                    "developed",
                    "by",
                    "#REF",
                    "to",
                    "specifically",
                    "address",
                    "translation",
                    "of",
                    "collocations,",
                    "including",
                    "non-compositional",
                    "expressions."
                ],
                [
                    "They",
                    "used",
                    "aligned",
                    "sentences",
                    "from",
                    "the",
                    "Canadian",
                    "Hansards",
                    "corpus",
                    "(as",
                    "did",
                    "#REF",
                    "."
                ],
                [
                    "They",
                    "used",
                    "a",
                    "tool",
                    "they",
                    "had",
                    "previously",
                    "developed",
                    "(XTRACT)",
                    "to",
                    "identify",
                    "collocations",
                    "and",
                    "they",
                    "translated",
                    "approximately",
                    "900",
                    "medium",
                    "frequency",
                    "English",
                    "phrases",
                    "to",
                    "French."
                ],
                [
                    "Manual",
                    "evaluation",
                    "by",
                    "bilingual",
                    "speakers",
                    "revealed",
                    "accuracies",
                    "between",
                    "65",
                    "and",
                    "78%."
                ],
                [
                    "Champollion",
                    "works",
                    "by",
                    "iteratively",
                    "fusing",
                    "together",
                    "target",
                    "language",
                    "words",
                    "that",
                    "are",
                    "strongly",
                    "correlated",
                    "to",
                    "the",
                    "source",
                    "language",
                    "collocation",
                    "for",
                    "which",
                    "translation",
                    "is",
                    "attempted."
                ],
                [
                    "Dice",
                    "scores",
                    "are",
                    "used",
                    "to",
                    "filter",
                    "out",
                    "unlikely",
                    "word",
                    "combinations."
                ],
                [
                    "#TARGET_REF",
                    "ambitiously",
                    "produce",
                    "alignments",
                    "from",
                    "comparable",
                    "corpora,",
                    "corpora",
                    "where",
                    "exact",
                    "translations",
                    "may",
                    "not",
                    "be",
                    "available,",
                    "but",
                    "in",
                    "which",
                    "the",
                    "same",
                    "topics",
                    "or",
                    "entities",
                    "are",
                    "being",
                    "discussed",
                    "such",
                    "as",
                    "contemporaneous",
                    "newswire."
                ],
                [
                    "They",
                    "use",
                    "suffix",
                    "trees",
                    "in",
                    "both",
                    "languages",
                    "and",
                    "a",
                    "bilingual",
                    "lexicon",
                    "to",
                    "provide",
                    "points",
                    "of",
                    "correspondence",
                    "between",
                    "the",
                    "two",
                    "languages."
                ],
                [
                    "Not",
                    "only",
                    "to",
                    "they",
                    "successfully",
                    "create",
                    "alignments",
                    "in",
                    "the",
                    "comparable",
                    "data,",
                    "thus",
                    "creating",
                    "a",
                    "parallel",
                    "corpus,",
                    "they",
                    "create",
                    "phrasal",
                    "alignments",
                    "of",
                    "a",
                    "restricted",
                    "sort."
                ],
                [
                    "Namely,",
                    "their",
                    "parallel",
                    "phrases",
                    "have",
                    "the",
                    "same",
                    "number",
                    "of",
                    "tokens",
                    "(i.e.,",
                    "words)",
                    "in",
                    "each",
                    "language",
                    "and",
                    "the",
                    "word",
                    "order",
                    "of",
                    "the",
                    "source",
                    "and",
                    "target",
                    "languages",
                    "must",
                    "be",
                    "the",
                    "same."
                ],
                [
                    "Some",
                    "examples",
                    "of",
                    "English/French",
                    "alignments",
                    "that",
                    "they",
                    "identified",
                    "are:"
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                1,
                1,
                1,
                1,
                1
            ]
        },
        "input": "sent0: The Champollion system was developed by #REF to specifically address translation of collocations, including non-compositional expressions.\n sent1: They used aligned sentences from the Canadian Hansards corpus (as did #REF .\n sent2: They used a tool they had previously developed (XTRACT) to identify collocations and they translated approximately 900 medium frequency English phrases to French.\n sent3: Manual evaluation by bilingual speakers revealed accuracies between 65 and 78%.\n sent4: Champollion works by iteratively fusing together target language words that are strongly correlated to the source language collocation for which translation is attempted.\n sent5: Dice scores are used to filter out unlikely word combinations.\n sent6: #TARGET_REF ambitiously produce alignments from comparable corpora, corpora where exact translations may not be available, but in which the same topics or entities are being discussed such as contemporaneous newswire.\n sent7: They use suffix trees in both languages and a bilingual lexicon to provide points of correspondence between the two languages.\n sent8: Not only to they successfully create alignments in the comparable data, thus creating a parallel corpus, they create phrasal alignments of a restricted sort.\n sent9: Namely, their parallel phrases have the same number of tokens (i.e., words) in each language and the word order of the source and target languages must be the same.\n sent10: Some examples of English/French alignments that they identified are:\n",
        "output": "{\"INFORMATION\": [\"sent6\", \"sent7\", \"sent8\", \"sent9\", \"sent10\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Other",
                    "architectural",
                    "improvements",
                    "highlighted",
                    "with",
                    "the",
                    "introduction",
                    "of",
                    "the",
                    "ALBERT",
                    "model",
                    "#REF",
                    "such",
                    "as",
                    "the",
                    "factorization",
                    "of",
                    "the",
                    "attention",
                    "matrix",
                    "or",
                    "parameter",
                    "sharing."
                ],
                [
                    "Indeed,",
                    "the",
                    "most",
                    "time-consuming",
                    "and",
                    "memory-intensive",
                    "operations",
                    "concerns",
                    "the",
                    "forward",
                    "propagation",
                    "and",
                    "attention",
                    "computation",
                    "operations."
                ],
                [
                    "The",
                    "self-attention",
                    "layer",
                    "of",
                    "BERT",
                    "pretrained",
                    "models",
                    "grows",
                    "quadratically",
                    "in",
                    "respect",
                    "to",
                    "the",
                    "input",
                    "sequence",
                    "length."
                ],
                [
                    "One",
                    "common",
                    "approach",
                    "to",
                    "this",
                    "issue",
                    "consists",
                    "of",
                    "approximating",
                    "the",
                    "dot-product",
                    "attention",
                    "for",
                    "example",
                    "by",
                    "using",
                    "hashing",
                    "techniques",
                    "#TARGET_REF",
                    "to",
                    "accelerate",
                    "the",
                    "training",
                    "and",
                    "inference",
                    "phases",
                    "when",
                    "long",
                    "sequence",
                    "lengths",
                    "are",
                    "used."
                ],
                [
                    "However",
                    "these",
                    "solutions",
                    "have",
                    "demonstrated",
                    "they",
                    "suffer",
                    "from",
                    "important",
                    "computational",
                    "overheads",
                    "for",
                    "tasks",
                    "with",
                    "smaller",
                    "lengths,",
                    "such",
                    "as",
                    "question-answering."
                ]
            ],
            "context": [
                0,
                3,
                0,
                2,
                0
            ]
        },
        "input": "sent0: Other architectural improvements highlighted with the introduction of the ALBERT model #REF such as the factorization of the attention matrix or parameter sharing.\n sent1: Indeed, the most time-consuming and memory-intensive operations concerns the forward propagation and attention computation operations.\n sent2: The self-attention layer of BERT pretrained models grows quadratically in respect to the input sequence length.\n sent3: One common approach to this issue consists of approximating the dot-product attention for example by using hashing techniques #TARGET_REF to accelerate the training and inference phases when long sequence lengths are used.\n sent4: However these solutions have demonstrated they suffer from important computational overheads for tasks with smaller lengths, such as question-answering.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent3\"], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "main",
                    "aspect",
                    "of",
                    "an",
                    "apology",
                    "lies",
                    "in",
                    "the",
                    "verb",
                    "that",
                    "the",
                    "tenderer",
                    "chooses",
                    "to",
                    "use."
                ],
                [
                    "We",
                    "do",
                    "an",
                    "analysis",
                    "of",
                    "the",
                    "two",
                    "verbs,",
                    "apologize",
                    "and",
                    "regret,",
                    "using",
                    "WordNet,",
                    "the",
                    "former",
                    "being",
                    "an",
                    "explicit",
                    "performative",
                    "verb",
                    "#TARGET_REF",
                    ",",
                    "The",
                    "selected",
                    "sense",
                    "of",
                    "the",
                    "verb",
                    "apologize",
                    "is",
                    "defined",
                    "as",
                    "-to",
                    "acknowledge",
                    "faults",
                    "or",
                    "shortcomings",
                    "or",
                    "failing."
                ],
                [
                    "Its",
                    "semantic",
                    "relation",
                    "of",
                    "entailment",
                    "is",
                    "admit,",
                    "acknowledge,",
                    "which",
                    "means",
                    "to",
                    "declare",
                    "to",
                    "be",
                    "true",
                    "or",
                    "admit",
                    "the",
                    "existence",
                    "or",
                    "reality",
                    "or",
                    "truth",
                    "of."
                ],
                [
                    "One",
                    "of",
                    "its",
                    "troponym",
                    "is",
                    "to",
                    "concede,",
                    "profess,",
                    "confess",
                    "which",
                    "is",
                    "defined",
                    "as",
                    "to",
                    "admit",
                    "(to",
                    "a",
                    "wrongdoing)."
                ],
                [
                    "The",
                    "superordinate",
                    "concept",
                    "of",
                    "this",
                    "chain",
                    "is",
                    "the",
                    "verb",
                    "think,",
                    "cogitate,",
                    "cerebrate",
                    "which",
                    "is",
                    "defined",
                    "as-to",
                    "use",
                    "or",
                    "exercise",
                    "the",
                    "mind",
                    "or",
                    "one's",
                    "power",
                    "of",
                    "reason",
                    "in",
                    "order",
                    "to",
                    "make",
                    "inferences,",
                    "decisions,",
                    "or",
                    "arrive",
                    "at",
                    "a",
                    "solution",
                    "or",
                    "judgments."
                ],
                [
                    "Thus,",
                    "it",
                    "is",
                    "clear",
                    "from",
                    "the",
                    "semantic",
                    "hierarchy",
                    "that",
                    "to",
                    "apologize",
                    "is",
                    "to",
                    "undergo",
                    "a",
                    "logical",
                    "thought",
                    "process,",
                    "the",
                    "natural",
                    "entailment",
                    "of",
                    "which",
                    "is",
                    "to",
                    "admit",
                    "to",
                    "a",
                    "wrong."
                ],
                [
                    "Once",
                    "the",
                    "wrongdoing",
                    "is",
                    "admitted",
                    "the",
                    "natural",
                    "consequence",
                    "should",
                    "be",
                    "to",
                    "take",
                    "responsibility",
                    "and",
                    "offer",
                    "amends."
                ],
                [
                    "For",
                    "instance,",
                    "apology",
                    "number",
                    "2",
                    "says-I",
                    "sincerely",
                    "apologize",
                    "to",
                    "all",
                    "Satyamites",
                    "and",
                    "stakeholders."
                ],
                [
                    "This",
                    "is",
                    "a",
                    "clear",
                    "admission",
                    "of",
                    "wrongdoing."
                ],
                [
                    "The",
                    "selected",
                    "concept",
                    "of",
                    "the",
                    "verb",
                    "regret",
                    "is",
                    "defined",
                    "as",
                    "to",
                    "feel",
                    "remorse",
                    "for,",
                    "feel",
                    "sorry",
                    "for",
                    "or",
                    "be",
                    "contrite",
                    "about."
                ],
                [
                    "Its",
                    "inherited",
                    "hypernymy",
                    "is",
                    "to",
                    "feel,",
                    "experience,",
                    "which",
                    "is",
                    "defined",
                    "as",
                    "to",
                    "undergo",
                    "an",
                    "emotional",
                    "sensation",
                    "or",
                    "be",
                    "in",
                    "a",
                    "particular",
                    "state",
                    "of",
                    "mind."
                ],
                [
                    "Thus,",
                    "to",
                    "regret",
                    "is",
                    "to",
                    "undergo",
                    "a",
                    "feeling",
                    "by",
                    "the",
                    "offender",
                    "about",
                    "the",
                    "wrongdoing."
                ],
                [
                    "In",
                    "the",
                    "corpus",
                    "apology",
                    "number",
                    "10,",
                    "the",
                    "Amazon",
                    "India",
                    "letter",
                    "states,",
                    "To",
                    "the",
                    "extent",
                    "that",
                    "these",
                    "items",
                    "offered",
                    "by",
                    "a",
                    "third-party",
                    "seller",
                    "in",
                    "Canada",
                    "offended",
                    "Indian",
                    "sensibilities,",
                    "Amazon",
                    "regrets",
                    "the",
                    "same."
                ]
            ],
            "context": [
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: The main aspect of an apology lies in the verb that the tenderer chooses to use.\n sent1: We do an analysis of the two verbs, apologize and regret, using WordNet, the former being an explicit performative verb #TARGET_REF , The selected sense of the verb apologize is defined as -to acknowledge faults or shortcomings or failing.\n sent2: Its semantic relation of entailment is admit, acknowledge, which means to declare to be true or admit the existence or reality or truth of.\n sent3: One of its troponym is to concede, profess, confess which is defined as to admit (to a wrongdoing).\n sent4: The superordinate concept of this chain is the verb think, cogitate, cerebrate which is defined as-to use or exercise the mind or one's power of reason in order to make inferences, decisions, or arrive at a solution or judgments.\n sent5: Thus, it is clear from the semantic hierarchy that to apologize is to undergo a logical thought process, the natural entailment of which is to admit to a wrong.\n sent6: Once the wrongdoing is admitted the natural consequence should be to take responsibility and offer amends.\n sent7: For instance, apology number 2 says-I sincerely apologize to all Satyamites and stakeholders.\n sent8: This is a clear admission of wrongdoing.\n sent9: The selected concept of the verb regret is defined as to feel remorse for, feel sorry for or be contrite about.\n sent10: Its inherited hypernymy is to feel, experience, which is defined as to undergo an emotional sensation or be in a particular state of mind.\n sent11: Thus, to regret is to undergo a feeling by the offender about the wrongdoing.\n sent12: In the corpus apology number 10, the Amazon India letter states, To the extent that these items offered by a third-party seller in Canada offended Indian sensibilities, Amazon regrets the same.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Say",
                    "that",
                    "the",
                    "agent",
                    "plays",
                    "in",
                    "the",
                    "blue",
                    "team,",
                    "i.e."
                ],
                [
                    "we",
                    "want",
                    "to",
                    "generate",
                    "clues",
                    "associated",
                    "to",
                    "the",
                    "blue",
                    "words,",
                    "based",
                    "on",
                    "the",
                    "distance",
                    "functions",
                    "above."
                ],
                [
                    "The",
                    "functions",
                    "of",
                    "#TARGET_REF",
                    "(see",
                    "(",
                    "1))",
                    "determined",
                    "the",
                    "score",
                    "of",
                    "a",
                    "possible",
                    "reference",
                    "based",
                    "on",
                    "relatedness",
                    "of",
                    "the",
                    "clue",
                    "word",
                    "to",
                    "the",
                    "least",
                    "related",
                    "blue",
                    "word",
                    "targeted."
                ],
                [
                    "The",
                    "shortcoming",
                    "of",
                    "this,",
                    "however,",
                    "is",
                    "that",
                    "in",
                    "addition",
                    "to",
                    "blue",
                    "(good)",
                    "words",
                    "that",
                    "are",
                    "similar",
                    "to",
                    "the",
                    "clue",
                    "word,",
                    "there",
                    "may",
                    "be",
                    "bad",
                    "words",
                    "of",
                    "a",
                    "different",
                    "color",
                    "that",
                    "are",
                    "only",
                    "very",
                    "slightly",
                    "less",
                    "similar",
                    "to",
                    "the",
                    "clue."
                ],
                [
                    "We",
                    "can",
                    "assume",
                    "that",
                    "in",
                    "this",
                    "case,",
                    "agents",
                    "are",
                    "less",
                    "likely",
                    "to",
                    "choose",
                    "the",
                    "targeted",
                    "words,",
                    "or",
                    "in",
                    "general,",
                    "the",
                    "smaller",
                    "the",
                    "difference",
                    "between",
                    "the",
                    "distances",
                    "of",
                    "two",
                    "words",
                    "from",
                    "the",
                    "clue",
                    "according",
                    "to",
                    "our",
                    "distance",
                    "function,",
                    "the",
                    "more",
                    "likely",
                    "the",
                    "human",
                    "player",
                    "will",
                    "perceive",
                    "the",
                    "order",
                    "of",
                    "the",
                    "two",
                    "words",
                    "reversed."
                ]
            ],
            "context": [
                3,
                3,
                1,
                0,
                0
            ]
        },
        "input": "sent0: Say that the agent plays in the blue team, i.e.\n sent1: we want to generate clues associated to the blue words, based on the distance functions above.\n sent2: The functions of #TARGET_REF (see ( 1)) determined the score of a possible reference based on relatedness of the clue word to the least related blue word targeted.\n sent3: The shortcoming of this, however, is that in addition to blue (good) words that are similar to the clue word, there may be bad words of a different color that are only very slightly less similar to the clue.\n sent4: We can assume that in this case, agents are less likely to choose the targeted words, or in general, the smaller the difference between the distances of two words from the clue according to our distance function, the more likely the human player will perceive the order of the two words reversed.\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Hanu",
                    "and",
                    "Unitary",
                    "team",
                    "(2020)",
                    "introduced",
                    "Detoxify,",
                    "a",
                    "comment",
                    "detection",
                    "library",
                    "modeled",
                    "using",
                    "HuggingFace's",
                    "transformers",
                    "#TARGET_REF",
                    "to",
                    "identify",
                    "inappropriate",
                    "or",
                    "harmful",
                    "text",
                    "online",
                    "as",
                    "a",
                    "result",
                    "of",
                    "participation",
                    "in",
                    "three",
                    "such",
                    "challenges."
                ],
                [
                    "In",
                    "a",
                    "contemporary",
                    "work,",
                    "#REF",
                    "discuss",
                    "context",
                    "requirement",
                    "for",
                    "toxicity",
                    "detection."
                ]
            ],
            "context": [
                2,
                0
            ]
        },
        "input": "sent0: Hanu and Unitary team (2020) introduced Detoxify, a comment detection library modeled using HuggingFace's transformers #TARGET_REF to identify inappropriate or harmful text online as a result of participation in three such challenges.\n sent1: In a contemporary work, #REF discuss context requirement for toxicity detection.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Recent",
                    "read-write",
                    "policies",
                    "can",
                    "be",
                    "divided",
                    "into",
                    "two",
                    "categories:",
                    "fixed",
                    "policies",
                    "such",
                    "as",
                    "wait-k",
                    "#REF",
                    ",",
                    "wait-if*",
                    "#REF",
                    ",",
                    "and",
                    "adaptive",
                    "policies",
                    "such",
                    "as",
                    "MoChA",
                    "#REF",
                    ",",
                    "MILk",
                    "#TARGET_REF",
                    "and",
                    "MU",
                    "#REF",
                    "."
                ],
                [
                    "Fixed",
                    "policies",
                    "are",
                    "simple",
                    "to",
                    "implement,",
                    "but",
                    "they",
                    "neglect",
                    "contextual",
                    "information,",
                    "which",
                    "might",
                    "result",
                    "in",
                    "quality",
                    "reduction."
                ],
                [
                    "Dynamic",
                    "policies",
                    "are",
                    "more",
                    "flexible,",
                    "they",
                    "can",
                    "learn",
                    "from",
                    "data",
                    "to",
                    "achieve",
                    "better",
                    "quality/latency",
                    "trade-offs,",
                    "but",
                    "accordingly",
                    "difficult",
                    "to",
                    "train."
                ]
            ],
            "context": [
                3,
                0,
                0
            ]
        },
        "input": "sent0: Recent read-write policies can be divided into two categories: fixed policies such as wait-k #REF , wait-if* #REF , and adaptive policies such as MoChA #REF , MILk #TARGET_REF and MU #REF .\n sent1: Fixed policies are simple to implement, but they neglect contextual information, which might result in quality reduction.\n sent2: Dynamic policies are more flexible, they can learn from data to achieve better quality/latency trade-offs, but accordingly difficult to train.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "After",
                    "obtaining",
                    "natural",
                    "language",
                    "actions,",
                    "we",
                    "enrich",
                    "the",
                    "dialogues",
                    "as",
                    "{(c",
                    "t",
                    ",",
                    "l(x",
                    "t",
                    "),",
                    "x",
                    "t",
                    ")|1",
                    "≤",
                    "t",
                    "≤",
                    "n",
                    "d",
                    "},",
                    "where",
                    "l(x",
                    "t",
                    ")",
                    "is",
                    "the",
                    "natural",
                    "language",
                    "action",
                    "of",
                    "utterance",
                    "x",
                    "t",
                    "."
                ],
                [
                    "We",
                    "could",
                    "then",
                    "run",
                    "conditioned",
                    "response",
                    "generation",
                    "to",
                    "train",
                    "content",
                    "planning",
                    "and",
                    "language",
                    "generation",
                    "models",
                    "as",
                    "Eqn."
                ],
                [
                    "1-3."
                ],
                [
                    "The",
                    "learning",
                    "efficiency",
                    "can",
                    "be",
                    "improved",
                    "by",
                    "the",
                    "more",
                    "compact",
                    "and",
                    "noise-free",
                    "action",
                    "space."
                ],
                [
                    "Moreover,",
                    "the",
                    "natural",
                    "language",
                    "actions",
                    "present",
                    "abundant",
                    "information",
                    "of",
                    "correlations",
                    "among",
                    "actions,",
                    "which",
                    "allows",
                    "for",
                    "better",
                    "generalization",
                    "over",
                    "actions",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "sent0: After obtaining natural language actions, we enrich the dialogues as {(c t , l(x t ), x t )|1 ≤ t ≤ n d }, where l(x t ) is the natural language action of utterance x t .\n sent1: We could then run conditioned response generation to train content planning and language generation models as Eqn.\n sent2: 1-3.\n sent3: The learning efficiency can be improved by the more compact and noise-free action space.\n sent4: Moreover, the natural language actions present abundant information of correlations among actions, which allows for better generalization over actions #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [\"sent4\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "the",
                    "translation",
                    "process",
                    "in",
                    "terms",
                    "of",
                    "complexity,",
                    "we",
                    "have",
                    "also",
                    "analysed",
                    "the",
                    "linguistic",
                    "complexity",
                    "of",
                    "the",
                    "translations",
                    "produced",
                    "by",
                    "the",
                    "participants."
                ],
                [
                    "Based",
                    "on",
                    "the",
                    "linguistic",
                    "analysis",
                    "presented",
                    "in",
                    "#TARGET_REF",
                    ",",
                    "we",
                    "have",
                    "calculated",
                    "the",
                    "average",
                    "occurrences",
                    "of",
                    "a",
                    "number",
                    "of",
                    "linguistic",
                    "features",
                    "(including",
                    "lexical,",
                    "morphological,",
                    "syntactic",
                    "and",
                    "pragmatic",
                    "features)",
                    "present",
                    "in",
                    "the",
                    "translations",
                    "and",
                    "post-edited",
                    "versions",
                    "of",
                    "Texts",
                    "A",
                    "and",
                    "B",
                    "(see",
                    "Table",
                    "8)."
                ],
                [
                    "We",
                    "observe",
                    "that",
                    "out",
                    "of",
                    "the",
                    "96",
                    "features",
                    "studied,",
                    "Text",
                    "B",
                    "has",
                    "a",
                    "higher",
                    "number",
                    "of",
                    "occurrences",
                    "for",
                    "63",
                    "for",
                    "both",
                    "translators",
                    "and",
                    "users,",
                    "and",
                    "Text",
                    "A",
                    "for",
                    "25",
                    "and",
                    "17,",
                    "for",
                    "translators",
                    "and",
                    "users,",
                    "respectively,",
                    "having",
                    "no",
                    "occurrences",
                    "for",
                    "8",
                    "and",
                    "6",
                    "features."
                ],
                [
                    "Additioanlly,",
                    "we",
                    "have",
                    "considered",
                    "the",
                    "10",
                    "most",
                    "predictive",
                    "features",
                    "for",
                    "complexity",
                    "according",
                    "to",
                    "the",
                    "same",
                    "authors,",
                    "which",
                    "include",
                    "a",
                    "number",
                    "of",
                    "the",
                    "most",
                    "predictive",
                    "features",
                    "according",
                    "to",
                    "#REF",
                    ",",
                    "namely,",
                    "partof-speech",
                    "ratios",
                    "for",
                    "nouns."
                ],
                [
                    "We",
                    "see",
                    "that",
                    "Text",
                    "B",
                    "appears",
                    "to",
                    "be",
                    "more",
                    "complex,",
                    "scoring",
                    "higher",
                    "in",
                    "7",
                    "out",
                    "of",
                    "the",
                    "10",
                    "features."
                ],
                [
                    "A",
                    "final",
                    "aspect",
                    "that",
                    "is",
                    "worth",
                    "noting",
                    "is",
                    "text",
                    "expansion",
                    "rates."
                ],
                [
                    "English",
                    "is",
                    "an",
                    "analytic",
                    "language",
                    "and",
                    "Basque",
                    "is",
                    "an",
                    "agglutinative",
                    "language,",
                    "which",
                    "usually",
                    "means",
                    "that",
                    "word-counts",
                    "contract",
                    "when",
                    "translating",
                    "into",
                    "Basque."
                ],
                [
                    "For",
                    "translators,",
                    "on",
                    "average,",
                    "Text",
                    "A",
                    "has",
                    "contracted",
                    "to",
                    "90.25%",
                    "and",
                    "Text",
                    "B",
                    "has",
                    "expanded",
                    "to",
                    "103.85%",
                    "with",
                    "respect",
                    "to",
                    "the",
                    "English",
                    "source."
                ],
                [
                    "For",
                    "users,",
                    "both",
                    "texts",
                    "contract",
                    "but",
                    "whereas",
                    "Text",
                    "A",
                    "goes",
                    "down",
                    "to",
                    "85.21%,",
                    "Text",
                    "B",
                    "still",
                    "remains",
                    "at",
                    "a",
                    "high",
                    "98.21%."
                ],
                [
                    "The",
                    "fact",
                    "that",
                    "an",
                    "expansion",
                    "has",
                    "occurred",
                    "in",
                    "Text",
                    "B",
                    "might",
                    "be",
                    "due",
                    "to",
                    "participants",
                    "tending",
                    "to",
                    "over-explain",
                    "or",
                    "paraphrase."
                ],
                [
                    "This",
                    "might",
                    "be",
                    "a",
                    "result",
                    "of",
                    "the",
                    "complexity",
                    "of",
                    "the",
                    "content."
                ]
            ],
            "context": [
                0,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: the translation process in terms of complexity, we have also analysed the linguistic complexity of the translations produced by the participants.\n sent1: Based on the linguistic analysis presented in #TARGET_REF , we have calculated the average occurrences of a number of linguistic features (including lexical, morphological, syntactic and pragmatic features) present in the translations and post-edited versions of Texts A and B (see Table 8).\n sent2: We observe that out of the 96 features studied, Text B has a higher number of occurrences for 63 for both translators and users, and Text A for 25 and 17, for translators and users, respectively, having no occurrences for 8 and 6 features.\n sent3: Additioanlly, we have considered the 10 most predictive features for complexity according to the same authors, which include a number of the most predictive features according to #REF , namely, partof-speech ratios for nouns.\n sent4: We see that Text B appears to be more complex, scoring higher in 7 out of the 10 features.\n sent5: A final aspect that is worth noting is text expansion rates.\n sent6: English is an analytic language and Basque is an agglutinative language, which usually means that word-counts contract when translating into Basque.\n sent7: For translators, on average, Text A has contracted to 90.25% and Text B has expanded to 103.85% with respect to the English source.\n sent8: For users, both texts contract but whereas Text A goes down to 85.21%, Text B still remains at a high 98.21%.\n sent9: The fact that an expansion has occurred in Text B might be due to participants tending to over-explain or paraphrase.\n sent10: This might be a result of the complexity of the content.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "For",
                    "each",
                    "pair",
                    "of",
                    "normative",
                    "claim",
                    "C",
                    "and",
                    "statement",
                    "S,",
                    "we",
                    "annotate",
                    "the",
                    "following",
                    "information:",
                    "(1a)",
                    "Whether",
                    "C",
                    "advocates",
                    "for",
                    "or",
                    "opposes",
                    "its",
                    "norm",
                    "target,",
                    "and",
                    "(1b)",
                    "the",
                    "norm",
                    "target",
                    "T",
                    "(Figure",
                    "1",
                    "TASK",
                    "1),",
                    "(2a)",
                    "Whether",
                    "S",
                    "uses",
                    "a",
                    "norm,",
                    "consequence,",
                    "or",
                    "property",
                    "for",
                    "justification,",
                    "and",
                    "(2b)",
                    "the",
                    "justification",
                    "J",
                    "(Figure",
                    "1",
                    "TASK",
                    "2),",
                    "(3a)",
                    "Whether",
                    "J's",
                    "focus",
                    "is",
                    "on",
                    "advocating",
                    "for",
                    "T",
                    "or",
                    "opposing",
                    "T",
                    ",",
                    "and",
                    "(3b)",
                    "whether",
                    "J",
                    "is",
                    "positive",
                    "or",
                    "negative",
                    "(Figure",
                    "1",
                    "TASK",
                    "3)."
                ],
                [
                    "6",
                    "Our",
                    "annotation",
                    "schema",
                    "is",
                    "richer",
                    "than",
                    "existing",
                    "ones",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Due",
                    "to",
                    "the",
                    "increased",
                    "complexity,",
                    "however,",
                    "our",
                    "annotation",
                    "is",
                    "split",
                    "into",
                    "three",
                    "pipelined",
                    "tasks."
                ],
                [
                    "For",
                    "this",
                    "annotation,",
                    "we",
                    "randomly",
                    "sampled",
                    "1,000",
                    "arguments",
                    "from",
                    "Kialo",
                    "whose",
                    "claims",
                    "are",
                    "normative",
                    "(see",
                    "§6",
                    "and",
                    "Table",
                    "4",
                    "for",
                    "details)."
                ]
            ],
            "context": [
                3,
                2,
                0,
                0
            ]
        },
        "input": "sent0: For each pair of normative claim C and statement S, we annotate the following information: (1a) Whether C advocates for or opposes its norm target, and (1b) the norm target T (Figure 1 TASK 1), (2a) Whether S uses a norm, consequence, or property for justification, and (2b) the justification J (Figure 1 TASK 2), (3a) Whether J's focus is on advocating for T or opposing T , and (3b) whether J is positive or negative (Figure 1 TASK 3).\n sent1: 6 Our annotation schema is richer than existing ones #TARGET_REF .\n sent2: Due to the increased complexity, however, our annotation is split into three pipelined tasks.\n sent3: For this annotation, we randomly sampled 1,000 arguments from Kialo whose claims are normative (see §6 and Table 4 for details).\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "conduct",
                    "all",
                    "experiments",
                    "with",
                    "the",
                    "deep",
                    "learning",
                    "framework",
                    "PaddlePaddle",
                    "#TARGET_REF",
                    "on",
                    "up",
                    "to",
                    "eight",
                    "NVIDIA",
                    "Tesla",
                    "V100",
                    "GPUs",
                    "(with",
                    "32G",
                    "RAM)."
                ]
            ],
            "context": [
                1
            ]
        },
        "input": "sent0: We conduct all experiments with the deep learning framework PaddlePaddle #TARGET_REF on up to eight NVIDIA Tesla V100 GPUs (with 32G RAM).\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "After",
                    "the",
                    "selection",
                    "of",
                    "documents",
                    "for",
                    "analysis,",
                    "a",
                    "list",
                    "of",
                    "keywords",
                    "was",
                    "prepared",
                    "independently",
                    "by",
                    "the",
                    "authors",
                    "and",
                    "then",
                    "compiled."
                ],
                [
                    "As",
                    "traditionally",
                    "held,",
                    "an",
                    "apology",
                    "consists",
                    "of",
                    "five",
                    "major",
                    "parts",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "These",
                    "are",
                    "the",
                    "following:"
                ]
            ],
            "context": [
                0,
                1,
                3
            ]
        },
        "input": "sent0: After the selection of documents for analysis, a list of keywords was prepared independently by the authors and then compiled.\n sent1: As traditionally held, an apology consists of five major parts #TARGET_REF .\n sent2: These are the following:\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "MT",
                    "output",
                    "quality",
                    "is",
                    "also",
                    "essential",
                    "in",
                    "post-editing",
                    "measurements,",
                    "a",
                    "factor",
                    "that",
                    "is",
                    "often",
                    "neglected",
                    "when",
                    "reporting",
                    "productivity",
                    "gain."
                ],
                [
                    "An",
                    "exception",
                    "is",
                    "a",
                    "seminal",
                    "work",
                    "by",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "They",
                    "studied",
                    "the",
                    "relation",
                    "between",
                    "MT",
                    "quality",
                    "and",
                    "post-editing,",
                    "and",
                    "concluded",
                    "that",
                    "differences",
                    "in",
                    "post-editing",
                    "skills",
                    "might",
                    "be",
                    "more",
                    "decisive",
                    "than",
                    "MT",
                    "quality",
                    "to",
                    "foresee",
                    "productivity",
                    "gain",
                    "when",
                    "comparing",
                    "systems",
                    "within",
                    "the",
                    "same",
                    "quality",
                    "range."
                ],
                [
                    "Our",
                    "findings",
                    "show",
                    "that",
                    "the",
                    "text",
                    "for",
                    "which",
                    "a",
                    "higher",
                    "increase",
                    "in",
                    "productivity",
                    "was",
                    "obtained",
                    "seems",
                    "to",
                    "be",
                    "slightly",
                    "easier",
                    "and",
                    "better",
                    "suited",
                    "for",
                    "our",
                    "MT",
                    "system."
                ]
            ],
            "context": [
                3,
                3,
                1,
                2
            ]
        },
        "input": "sent0: MT output quality is also essential in post-editing measurements, a factor that is often neglected when reporting productivity gain.\n sent1: An exception is a seminal work by #TARGET_REF .\n sent2: They studied the relation between MT quality and post-editing, and concluded that differences in post-editing skills might be more decisive than MT quality to foresee productivity gain when comparing systems within the same quality range.\n sent3: Our findings show that the text for which a higher increase in productivity was obtained seems to be slightly easier and better suited for our MT system.\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [\"sent3\"], \"BACKGROUND\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "First",
                    "and",
                    "foremost,",
                    "we",
                    "observed",
                    "the",
                    "fact",
                    "that,",
                    "due",
                    "to",
                    "a",
                    "mix",
                    "of",
                    "factors",
                    "such",
                    "as",
                    "greater",
                    "media",
                    "vigilance,",
                    "and",
                    "the",
                    "viral",
                    "nature",
                    "of",
                    "social",
                    "media,",
                    "there",
                    "is",
                    "certainly",
                    "an",
                    "increased",
                    "willingness",
                    "to",
                    "issue",
                    "public",
                    "apologies",
                    "in",
                    "India",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "However,",
                    "apologies",
                    "available",
                    "in",
                    "the",
                    "public",
                    "domain",
                    "are",
                    "still",
                    "limited,",
                    "and",
                    "so",
                    "we",
                    "cannot",
                    "draw",
                    "any",
                    "generalizations",
                    "from",
                    "them."
                ],
                [
                    "Hence,",
                    "we",
                    "can",
                    "put",
                    "forth",
                    "certain",
                    "trends",
                    "and",
                    "suggestions",
                    "which",
                    "need",
                    "to",
                    "be",
                    "tested",
                    "further",
                    "on",
                    "a",
                    "much",
                    "bigger",
                    "corpus."
                ]
            ],
            "context": [
                1,
                0,
                0
            ]
        },
        "input": "sent0: First and foremost, we observed the fact that, due to a mix of factors such as greater media vigilance, and the viral nature of social media, there is certainly an increased willingness to issue public apologies in India #TARGET_REF .\n sent1: However, apologies available in the public domain are still limited, and so we cannot draw any generalizations from them.\n sent2: Hence, we can put forth certain trends and suggestions which need to be tested further on a much bigger corpus.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Finally,",
                    "denoting",
                    "the",
                    "mask",
                    "set",
                    "M",
                    "=",
                    "{m",
                    "n",
                    "}",
                    "N",
                    "n=1",
                    ",",
                    "we",
                    "have",
                    "the",
                    "overall",
                    "PRA",
                    "loss",
                    "function",
                    "as",
                    "follows:L",
                    "PRA",
                    "=",
                    "E",
                    "(I,D)∼X",
                    "L",
                    "PRA",
                    "({α",
                    "z,n",
                    "}",
                    "|P|,N",
                    "z,n=1",
                    ",",
                    "M,",
                    "P,",
                    "V,",
                    "W)",
                    "(5)Masked",
                    "Language",
                    "Modeling",
                    "(MLM)",
                    "We",
                    "take",
                    "the",
                    "same",
                    "masking",
                    "strategy",
                    "(15%",
                    "prob."
                ],
                [
                    "to",
                    "mask)",
                    "as",
                    "in",
                    "BERT",
                    "#TARGET_REF",
                    "to",
                    "randomly",
                    "mask",
                    "out",
                    "the",
                    "input",
                    "word",
                    "tokens."
                ],
                [
                    "Here,",
                    "MLM",
                    "aims",
                    "to",
                    "predict",
                    "the",
                    "original",
                    "word",
                    "index",
                    "in",
                    "vocabulary",
                    "space",
                    "for",
                    "each",
                    "masked",
                    "token",
                    "based",
                    "on",
                    "the",
                    "whole",
                    "image",
                    "and",
                    "its",
                    "surrounding",
                    "language",
                    "context",
                    "via",
                    "the",
                    "Transformer."
                ],
                [
                    "Hence",
                    "a",
                    "cross-entropy",
                    "loss",
                    "is",
                    "adopted:L",
                    "MLM",
                    "=",
                    "−E",
                    "(I,D)∼X",
                    "logP",
                    "(w",
                    "j",
                    "|V,",
                    "W",
                    "\\j",
                    ")",
                    "(6)Image-Text",
                    "Matching",
                    "(ITM)",
                    "In",
                    "ITM,",
                    "the",
                    "multilayer",
                    "Transformer",
                    "is",
                    "trained",
                    "to",
                    "distinguish",
                    "whether",
                    "the",
                    "input",
                    "image-text",
                    "pairs",
                    "are",
                    "semantically",
                    "matched",
                    "based",
                    "on",
                    "the",
                    "final",
                    "layer",
                    "[cls]",
                    "token",
                    "representation",
                    "h",
                    "cls",
                    "."
                ],
                [
                    "To",
                    "construct",
                    "the",
                    "training",
                    "samples,",
                    "we",
                    "randomly",
                    "replace",
                    "the",
                    "text",
                    "for",
                    "each",
                    "image-text",
                    "pair",
                    "with",
                    "another",
                    "text",
                    "from",
                    "dataset",
                    "with",
                    "a",
                    "probability",
                    "of",
                    "0.5."
                ],
                [
                    "Thus,",
                    "the",
                    "output",
                    "label",
                    "can",
                    "be",
                    "defined",
                    "as",
                    "y",
                    "∈",
                    "{0,",
                    "1}",
                    "where",
                    "y",
                    "=",
                    "1",
                    "indicates",
                    "matched",
                    "pair."
                ],
                [
                    "The",
                    "training",
                    "objective",
                    "for",
                    "the",
                    "ITM",
                    "task",
                    "is",
                    "to",
                    "minimize",
                    "binary",
                    "cross-entropy",
                    "loss:L",
                    "ITM",
                    "=",
                    "−E",
                    "(I,D)∼X",
                    "logP",
                    "(y|V,",
                    "W)",
                    "(7)4",
                    "Experiments"
                ]
            ],
            "context": [
                0,
                2,
                3,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Finally, denoting the mask set M = {m n } N n=1 , we have the overall PRA loss function as follows:L PRA = E (I,D)∼X L PRA ({α z,n } |P|,N z,n=1 , M, P, V, W) (5)Masked Language Modeling (MLM) We take the same masking strategy (15% prob.\n sent1: to mask) as in BERT #TARGET_REF to randomly mask out the input word tokens.\n sent2: Here, MLM aims to predict the original word index in vocabulary space for each masked token based on the whole image and its surrounding language context via the Transformer.\n sent3: Hence a cross-entropy loss is adopted:L MLM = −E (I,D)∼X logP (w j |V, W \\j ) (6)Image-Text Matching (ITM) In ITM, the multilayer Transformer is trained to distinguish whether the input image-text pairs are semantically matched based on the final layer [cls] token representation h cls .\n sent4: To construct the training samples, we randomly replace the text for each image-text pair with another text from dataset with a probability of 0.5.\n sent5: Thus, the output label can be defined as y ∈ {0, 1} where y = 1 indicates matched pair.\n sent6: The training objective for the ITM task is to minimize binary cross-entropy loss:L ITM = −E (I,D)∼X logP (y|V, W) (7)4 Experiments\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "natural",
                    "language",
                    "in",
                    "a",
                    "situated",
                    "manner."
                ],
                [
                    "Our",
                    "methods",
                    "use",
                    "both",
                    "large",
                    "language",
                    "models",
                    "and",
                    "deep",
                    "reinforcement",
                    "learning",
                    "and",
                    "are",
                    "prone",
                    "to",
                    "the",
                    "pitfalls",
                    "that",
                    "other",
                    "contemporary",
                    "methods",
                    "using",
                    "these",
                    "techniques",
                    "face,",
                    "especially",
                    "in",
                    "the",
                    "areas",
                    "of",
                    "dialogue",
                    "and",
                    "text",
                    "game",
                    "systems."
                ],
                [
                    "We",
                    "mitigate",
                    "this",
                    "first",
                    "pitfall",
                    "by",
                    "restricting",
                    "our",
                    "current",
                    "system",
                    "to",
                    "a",
                    "retrieval",
                    "based",
                    "dialogue,",
                    "ensuring",
                    "that",
                    "we",
                    "can",
                    "filter",
                    "out",
                    "non-normative",
                    "dialogue",
                    "usages",
                    "beforehand,",
                    "though",
                    "we",
                    "will",
                    "note",
                    "that",
                    "the",
                    "system",
                    "can",
                    "be",
                    "extended",
                    "to",
                    "generative",
                    "systems",
                    "as",
                    "described",
                    "in",
                    "#REF",
                    "."
                ],
                [
                    "Further,",
                    "the",
                    "LIGHT",
                    "dataset",
                    "is",
                    "crowdsourced",
                    "and",
                    "contains",
                    "data",
                    "biases",
                    "that",
                    "can",
                    "be",
                    "attributed",
                    "to",
                    "the",
                    "crowdworkers",
                    "tasked",
                    "with",
                    "creating",
                    "the",
                    "data."
                ],
                [
                    "Dinan",
                    "et",
                    "al."
                ],
                [
                    "(",
                    "2020)",
                    "provides",
                    "an",
                    "in",
                    "depth",
                    "discussion",
                    "regarding",
                    "the",
                    "inherent",
                    "dataset",
                    "biases,",
                    "such",
                    "as",
                    "gender",
                    "bias",
                    "in",
                    "the",
                    "distribution",
                    "of",
                    "characters,",
                    "in",
                    "LIGHT",
                    "and",
                    "techniques",
                    "to",
                    "mitigate",
                    "them-we",
                    "follow",
                    "these",
                    "methods",
                    "to",
                    "reduce",
                    "their",
                    "effects",
                    "on",
                    "both",
                    "the",
                    "environment",
                    "generation",
                    "and",
                    "agent",
                    "training",
                    "procedures."
                ],
                [
                    "The",
                    "LIGHT",
                    "environment",
                    "further",
                    "allows",
                    "us",
                    "to",
                    "factorize",
                    "the",
                    "overall",
                    "action",
                    "space",
                    "A",
                    "into",
                    "A",
                    "as",
                    "the",
                    "set",
                    "of",
                    "possible",
                    "textual",
                    "actions",
                    "or",
                    "commands",
                    "(e.g."
                ],
                [
                    "get",
                    "sword,",
                    "steal",
                    "coins",
                    "from",
                    "merchant),",
                    "and",
                    "U",
                    "as",
                    "the",
                    "set",
                    "of",
                    "possible",
                    "dialogues",
                    "that",
                    "can",
                    "be",
                    "uttered",
                    "by",
                    "an",
                    "agent,",
                    "thus",
                    "making",
                    "it",
                    "a",
                    "factored",
                    "POMDP",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "This",
                    "in",
                    "turn",
                    "means",
                    "that,",
                    "for",
                    "a",
                    "given",
                    "quest",
                    "q,",
                    "each",
                    "expert",
                    "human",
                    "demonstration",
                    "D(q)",
                    "=",
                    "α",
                    "*",
                    "0",
                    ",",
                    "α",
                    "*",
                    "1",
                    "...α",
                    "*",
                    "n",
                    "can",
                    "be",
                    "factorized",
                    "into",
                    "two",
                    "sub-sequences",
                    "of",
                    "expert",
                    "demonstrations",
                    "of",
                    "actions",
                    "and",
                    "dialogue",
                    "D",
                    "A",
                    "(q)",
                    "=",
                    "a",
                    "*",
                    "0",
                    ",",
                    "a",
                    "*",
                    "1",
                    ",",
                    "...a",
                    "*",
                    "n",
                    "and",
                    "D",
                    "U",
                    "(q)",
                    "=",
                    "u",
                    "*",
                    "0",
                    ",",
                    "u",
                    "*",
                    "1",
                    ",",
                    "...u",
                    "*",
                    "m",
                    "respectively."
                ],
                [
                    "The",
                    "factorized",
                    "action",
                    "spaces",
                    "A",
                    "and",
                    "U",
                    "are",
                    "constructed",
                    "by",
                    "enumerating",
                    "all",
                    "possible",
                    "actions/dialogue",
                    "utterances",
                    "in",
                    "the",
                    "all",
                    "human",
                    "demonstrations",
                    "in",
                    "LIGHT-quests."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                3,
                0
            ]
        },
        "input": "sent0: natural language in a situated manner.\n sent1: Our methods use both large language models and deep reinforcement learning and are prone to the pitfalls that other contemporary methods using these techniques face, especially in the areas of dialogue and text game systems.\n sent2: We mitigate this first pitfall by restricting our current system to a retrieval based dialogue, ensuring that we can filter out non-normative dialogue usages beforehand, though we will note that the system can be extended to generative systems as described in #REF .\n sent3: Further, the LIGHT dataset is crowdsourced and contains data biases that can be attributed to the crowdworkers tasked with creating the data.\n sent4: Dinan et al.\n sent5: ( 2020) provides an in depth discussion regarding the inherent dataset biases, such as gender bias in the distribution of characters, in LIGHT and techniques to mitigate them-we follow these methods to reduce their effects on both the environment generation and agent training procedures.\n sent6: The LIGHT environment further allows us to factorize the overall action space A into A as the set of possible textual actions or commands (e.g.\n sent7: get sword, steal coins from merchant), and U as the set of possible dialogues that can be uttered by an agent, thus making it a factored POMDP #TARGET_REF .\n sent8: This in turn means that, for a given quest q, each expert human demonstration D(q) = α * 0 , α * 1 ...α * n can be factorized into two sub-sequences of expert demonstrations of actions and dialogue D A (q) = a * 0 , a * 1 , ...a * n and D U (q) = u * 0 , u * 1 , ...u * m respectively.\n sent9: The factorized action spaces A and U are constructed by enumerating all possible actions/dialogue utterances in the all human demonstrations in LIGHT-quests.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent6\", \"sent7\", \"sent8\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Increasing",
                    "the",
                    "Beam",
                    "Width",
                    "While",
                    "theoretically",
                    "a",
                    "larger",
                    "beam",
                    "width",
                    "(i.e."
                ],
                [
                    "the",
                    "number",
                    "of",
                    "candidates",
                    "maintained",
                    "during",
                    "beam",
                    "search)",
                    "would",
                    "allow",
                    "more",
                    "candidates",
                    "to",
                    "be",
                    "considered",
                    "and",
                    "therefore",
                    "increase",
                    "the",
                    "upper",
                    "bound",
                    "of",
                    "the",
                    "performance,",
                    "in",
                    "practice",
                    "model",
                    "performance",
                    "may",
                    "be",
                    "lower",
                    "if",
                    "the",
                    "beam",
                    "width",
                    "is",
                    "too",
                    "large."
                ],
                [
                    "The",
                    "reason",
                    "for",
                    "this",
                    "phenomenon",
                    "is",
                    "closely",
                    "related",
                    "to",
                    "the",
                    "low",
                    "sequence-level",
                    "coordination",
                    "of",
                    "the",
                    "generator."
                ],
                [
                    "Specifically,",
                    "increasing",
                    "the",
                    "beam",
                    "width",
                    "may",
                    "introduce",
                    "candidates",
                    "with",
                    "lower",
                    "quality",
                    "#TARGET_REF",
                    ",",
                    "and",
                    "the",
                    "generator",
                    "may",
                    "not",
                    "be",
                    "able",
                    "to",
                    "differentiate",
                    "them",
                    "from",
                    "high-quality",
                    "candidates."
                ],
                [
                    "In",
                    "Tab."
                ],
                [
                    "5,",
                    "we",
                    "compare",
                    "the",
                    "performance",
                    "of",
                    "the",
                    "pre-trained",
                    "BART",
                    "and",
                    "our",
                    "model",
                    "(BRIO-Mul)",
                    "with",
                    "different",
                    "beam",
                    "widths",
                    "used",
                    "during",
                    "inference."
                ],
                [
                    "We",
                    "observe",
                    "that",
                    "the",
                    "performance",
                    "of",
                    "BART",
                    "goes",
                    "down",
                    "as",
                    "the",
                    "beam",
                    "width",
                    "increases."
                ],
                [
                    "On",
                    "the",
                    "other",
                    "hand,",
                    "our",
                    "model",
                    "is",
                    "able",
                    "to",
                    "achieve",
                    "better",
                    "performance",
                    "with",
                    "a",
                    "larger",
                    "number",
                    "of",
                    "beams,",
                    "demonstrating",
                    "that",
                    "our",
                    "training",
                    "method",
                    "can",
                    "improve",
                    "the",
                    "coordination",
                    "of",
                    "the",
                    "model",
                    "by",
                    "encouraging",
                    "the",
                    "model",
                    "to",
                    "assign",
                    "estimated",
                    "probabilities",
                    "to",
                    "candidate",
                    "summaries",
                    "wellcorrelated",
                    "with",
                    "their",
                    "quality."
                ],
                [
                    "Training",
                    "with",
                    "Different",
                    "Evaluation",
                    "Metrics",
                    "In",
                    "the",
                    "previous",
                    "experiments,",
                    "we",
                    "used",
                    "ROUGE",
                    "as",
                    "the",
                    "evaluation",
                    "metric",
                    "to",
                    "define",
                    "the",
                    "target",
                    "ordering",
                    "of",
                    "the",
                    "candidate",
                    "summaries",
                    "(Eq.7)."
                ],
                [
                    "To",
                    "evaluate",
                    "our",
                    "method's",
                    "performance",
                    "beyond",
                    "ROUGE,",
                    "we",
                    "use",
                    "a",
                    "model-based",
                    "semantic",
                    "similarity",
                    "metric,",
                    "BERTScore",
                    "#REF",
                    ",",
                    "7",
                    "as",
                    "the",
                    "evaluation",
                    "metric",
                    "M",
                    "in",
                    "Eq.7",
                    "to",
                    "compare",
                    "the",
                    "performance",
                    "of",
                    "different",
                    "candidate",
                    "summaries."
                ],
                [
                    "Then,",
                    "we",
                    "trained",
                    "another",
                    "version",
                    "of",
                    "BRIO-Mul",
                    "based",
                    "on",
                    "the",
                    "order",
                    "of",
                    "candidate",
                    "summaries",
                    "calculated",
                    "by",
                    "BERTScore."
                ]
            ],
            "context": [
                3,
                0,
                0,
                2,
                3,
                3,
                2,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Increasing the Beam Width While theoretically a larger beam width (i.e.\n sent1: the number of candidates maintained during beam search) would allow more candidates to be considered and therefore increase the upper bound of the performance, in practice model performance may be lower if the beam width is too large.\n sent2: The reason for this phenomenon is closely related to the low sequence-level coordination of the generator.\n sent3: Specifically, increasing the beam width may introduce candidates with lower quality #TARGET_REF , and the generator may not be able to differentiate them from high-quality candidates.\n sent4: In Tab.\n sent5: 5, we compare the performance of the pre-trained BART and our model (BRIO-Mul) with different beam widths used during inference.\n sent6: We observe that the performance of BART goes down as the beam width increases.\n sent7: On the other hand, our model is able to achieve better performance with a larger number of beams, demonstrating that our training method can improve the coordination of the model by encouraging the model to assign estimated probabilities to candidate summaries wellcorrelated with their quality.\n sent8: Training with Different Evaluation Metrics In the previous experiments, we used ROUGE as the evaluation metric to define the target ordering of the candidate summaries (Eq.7).\n sent9: To evaluate our method's performance beyond ROUGE, we use a model-based semantic similarity metric, BERTScore #REF , 7 as the evaluation metric M in Eq.7 to compare the performance of different candidate summaries.\n sent10: Then, we trained another version of BRIO-Mul based on the order of candidate summaries calculated by BERTScore.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent3\", \"sent6\"], \"BACKGROUND\": [\"sent0\", \"sent4\", \"sent5\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "EBMT",
                    "proceeds",
                    "by",
                    "finding",
                    "suitable",
                    "examples",
                    "in",
                    "the",
                    "database",
                    "and",
                    "then",
                    "'recombining'",
                    "them",
                    "appropriately."
                ],
                [
                    "Key",
                    "factors",
                    "are",
                    "therefore",
                    "the",
                    "efficient",
                    "retrieval",
                    "of",
                    "texts",
                    "from",
                    "the",
                    "database",
                    "which",
                    "are",
                    "sufficiently",
                    "similar",
                    "to",
                    "the",
                    "given",
                    "text",
                    "(",
                    "#TARGET_REF",
                    "),",
                    "and",
                    "the",
                    "alignment",
                    "of",
                    "translation",
                    "pairs,",
                    "given",
                    "a",
                    "bilingual",
                    "corpus",
                    "#REF",
                    ")."
                ]
            ],
            "context": [
                0,
                1
            ]
        },
        "input": "sent0: EBMT proceeds by finding suitable examples in the database and then 'recombining' them appropriately.\n sent1: Key factors are therefore the efficient retrieval of texts from the database which are sufficiently similar to the given text ( #TARGET_REF ), and the alignment of translation pairs, given a bilingual corpus #REF ).\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "first",
                    "compare",
                    "RocketQA",
                    "with",
                    "the",
                    "previous",
                    "state-of-the-art",
                    "approaches",
                    "on",
                    "passage",
                    "retrieval."
                ],
                [
                    "We",
                    "consider",
                    "both",
                    "sparse",
                    "and",
                    "dense",
                    "passage",
                    "retriever",
                    "baselines."
                ],
                [
                    "The",
                    "sparse",
                    "retrievers",
                    "include",
                    "the",
                    "traditional",
                    "retriever",
                    "BM25",
                    "#REF",
                    ",",
                    "and",
                    "four",
                    "traditional",
                    "retrievers",
                    "enhanced",
                    "by",
                    "neural",
                    "networks,",
                    "including",
                    "doc2query",
                    "#REF",
                    ",",
                    "DeepCT",
                    "#REF",
                    ",",
                    "docTTTT-Tquery",
                    "#REF",
                    "and",
                    "GAR",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Both",
                    "doc2query",
                    "and",
                    "docTTTTTquery",
                    "employ",
                    "neural",
                    "question",
                    "generation",
                    "to",
                    "expand",
                    "documents."
                ],
                [
                    "In",
                    "contrast,",
                    "GAR",
                    "employs",
                    "neural",
                    "generation",
                    "models",
                    "to",
                    "expand",
                    "questions."
                ],
                [
                    "Different",
                    "from",
                    "them,",
                    "DeepCT",
                    "utilizes",
                    "BERT",
                    "to",
                    "learn",
                    "the",
                    "term",
                    "weight."
                ],
                [
                    "The",
                    "dense",
                    "passage",
                    "retrievers",
                    "include",
                    "DPR",
                    "#REF",
                    ",",
                    "ME-BERT",
                    "#REF",
                    "and",
                    "ANCE",
                    "#REF",
                    "."
                ],
                [
                    "Both",
                    "DRP",
                    "and",
                    "ME-BERT",
                    "use",
                    "in-batch",
                    "random",
                    "sampling",
                    "and",
                    "hard",
                    "negative",
                    "sampling",
                    "from",
                    "the",
                    "results",
                    "retrieved",
                    "by",
                    "BM25,",
                    "while",
                    "ANCE",
                    "enhances",
                    "the",
                    "hard",
                    "negative",
                    "sampling",
                    "by",
                    "using",
                    "the",
                    "dense",
                    "retriever."
                ]
            ],
            "context": [
                0,
                0,
                3,
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "sent0: We first compare RocketQA with the previous state-of-the-art approaches on passage retrieval.\n sent1: We consider both sparse and dense passage retriever baselines.\n sent2: The sparse retrievers include the traditional retriever BM25 #REF , and four traditional retrievers enhanced by neural networks, including doc2query #REF , DeepCT #REF , docTTTT-Tquery #REF and GAR #TARGET_REF .\n sent3: Both doc2query and docTTTTTquery employ neural question generation to expand documents.\n sent4: In contrast, GAR employs neural generation models to expand questions.\n sent5: Different from them, DeepCT utilizes BERT to learn the term weight.\n sent6: The dense passage retrievers include DPR #REF , ME-BERT #REF and ANCE #REF .\n sent7: Both DRP and ME-BERT use in-batch random sampling and hard negative sampling from the results retrieved by BM25, while ANCE enhances the hard negative sampling by using the dense retriever.\n",
        "output": "{\"INFORMATION\": [\"sent4\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "follow",
                    "the",
                    "setup",
                    "from",
                    "the",
                    "robust",
                    "training",
                    "literature",
                    "#TARGET_REF",
                    "and",
                    "experiment",
                    "with",
                    "both",
                    "the",
                    "base",
                    "(non-robust)",
                    "and",
                    "robustly",
                    "trained",
                    "models."
                ],
                [
                    "We",
                    "train",
                    "the",
                    "binary",
                    "sentiment",
                    "classifiers",
                    "on",
                    "the",
                    "SST-2",
                    "dataset",
                    "with",
                    "bag-ofwords",
                    "(BoW),",
                    "CNN,",
                    "LSTM,",
                    "and",
                    "attention-basedOriginal:",
                    "70%",
                    "Negative",
                    "Input",
                    "Example:in",
                    "its",
                    "best",
                    "moments",
                    ",",
                    "resembles",
                    "a",
                    "bad",
                    "high",
                    "school",
                    "production",
                    "of",
                    "grease",
                    ",",
                    "without",
                    "benefit",
                    "of",
                    "song",
                    "."
                ]
            ],
            "context": [
                2,
                0
            ]
        },
        "input": "sent0: We follow the setup from the robust training literature #TARGET_REF and experiment with both the base (non-robust) and robustly trained models.\n sent1: We train the binary sentiment classifiers on the SST-2 dataset with bag-ofwords (BoW), CNN, LSTM, and attention-basedOriginal: 70% Negative Input Example:in its best moments , resembles a bad high school production of grease , without benefit of song .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Compared",
                    "to",
                    "prior",
                    "methods,",
                    "top-k",
                    "attention",
                    "has",
                    "multiple",
                    "attractive",
                    "properties:",
                    "•",
                    "Top-k",
                    "attention",
                    "has",
                    "the",
                    "same",
                    "memory",
                    "footprint",
                    "as",
                    "Performer",
                    "#TARGET_REF",
                    ",",
                    "a",
                    "stateof-the-art",
                    "attention",
                    "variant",
                    "with",
                    "linear",
                    "time",
                    "and",
                    "memory",
                    "complexity,",
                    "on",
                    "very",
                    "long",
                    "inputs",
                    "(orange",
                    "curve,",
                    "Fig."
                ],
                [
                    "1,",
                    "top-right),",
                    "while",
                    "being",
                    "as",
                    "fast",
                    "as",
                    "vanilla",
                    "attention,",
                    "and",
                    "even",
                    "faster",
                    "than",
                    "linear",
                    "variants",
                    "on",
                    "inputs",
                    "of",
                    "length",
                    "up",
                    "to",
                    "4K",
                    "(Figure",
                    "1,",
                    "bottom-left)."
                ],
                [
                    "This",
                    "allows",
                    "us,",
                    "e.g.,",
                    "to",
                    "train",
                    "a",
                    "typical",
                    "12-layer",
                    "Transformer",
                    "decoder",
                    "over",
                    "32K-long",
                    "inputs",
                    "on",
                    "a",
                    "30GiB",
                    "GPU",
                    "(Figure",
                    "3a)."
                ]
            ],
            "context": [
                1,
                2,
                0
            ]
        },
        "input": "sent0: Compared to prior methods, top-k attention has multiple attractive properties: • Top-k attention has the same memory footprint as Performer #TARGET_REF , a stateof-the-art attention variant with linear time and memory complexity, on very long inputs (orange curve, Fig.\n sent1: 1, top-right), while being as fast as vanilla attention, and even faster than linear variants on inputs of length up to 4K (Figure 1, bottom-left).\n sent2: This allows us, e.g., to train a typical 12-layer Transformer decoder over 32K-long inputs on a 30GiB GPU (Figure 3a).\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "which",
                    "are",
                    "well-known",
                    "challenges",
                    "of",
                    "reinforcement",
                    "learning",
                    "methods."
                ],
                [
                    "Minimum",
                    "risk",
                    "training",
                    "#REF",
                    "and",
                    "other",
                    "online",
                    "sampling",
                    "based",
                    "methods",
                    "#REF",
                    "belong",
                    "to",
                    "another",
                    "school",
                    "of",
                    "methods",
                    "used",
                    "to",
                    "circumvent",
                    "the",
                    "problem",
                    "of",
                    "non-differentiability."
                ],
                [
                    "However,",
                    "they",
                    "also",
                    "exhibit",
                    "similar",
                    "problems",
                    "of",
                    "stability",
                    "as",
                    "reinforcement",
                    "learning."
                ],
                [
                    "Contrastive",
                    "Learning",
                    "Recently,",
                    "contrastive",
                    "learning",
                    "#REF",
                    "has",
                    "been",
                    "introduced",
                    "into",
                    "several",
                    "conditional",
                    "text",
                    "generation",
                    "tasks,",
                    "such",
                    "as",
                    "machine",
                    "translation",
                    "#REF",
                    ",",
                    "text",
                    "summarization",
                    "#REF",
                    ",",
                    "and",
                    "other",
                    "tasks",
                    "#REF",
                    "."
                ],
                [
                    "Among",
                    "these",
                    "application",
                    "scenarios,",
                    "most",
                    "work",
                    "deployed",
                    "contrastive",
                    "learning",
                    "in",
                    "the",
                    "latent",
                    "representation",
                    "space,",
                    "following",
                    "the",
                    "framework",
                    "proposed",
                    "in",
                    "#REF",
                    "."
                ],
                [
                    "However,",
                    "in",
                    "this",
                    "work",
                    "we",
                    "adopt",
                    "contrastive",
                    "learning",
                    "over",
                    "the",
                    "discrete",
                    "space",
                    "of",
                    "the",
                    "generated",
                    "texts."
                ],
                [
                    "Besides,",
                    "instead",
                    "of",
                    "constructing",
                    "the",
                    "contrastive",
                    "learning",
                    "examples",
                    "by",
                    "rule-based",
                    "methods",
                    "(e.g."
                ],
                [
                    "perturbing",
                    "the",
                    "reference",
                    "output),",
                    "we",
                    "use",
                    "the",
                    "generation",
                    "models",
                    "to",
                    "construct",
                    "the",
                    "examples,",
                    "which",
                    "makes",
                    "the",
                    "contrastive",
                    "learning",
                    "task",
                    "closer",
                    "to",
                    "the",
                    "generation",
                    "task."
                ],
                [
                    "Sun",
                    "and",
                    "Li",
                    "(2021)",
                    "also",
                    "adopted",
                    "contrastive",
                    "learning",
                    "on",
                    "the",
                    "generated",
                    "texts."
                ],
                [
                    "However,",
                    "their",
                    "formulation",
                    "belongs",
                    "to",
                    "the",
                    "margin-based",
                    "losses."
                ],
                [
                    "We",
                    "have",
                    "discussed",
                    "the",
                    "difference",
                    "between",
                    "our",
                    "method",
                    "and",
                    "the",
                    "margin-based",
                    "losses",
                    "in",
                    "the",
                    "previous",
                    "paragraphs."
                ],
                [
                    "Discriminative",
                    "Reranking",
                    "Discriminative",
                    "reranking",
                    "has",
                    "been",
                    "widely",
                    "studied",
                    "for",
                    "conditional",
                    "generation",
                    "tasks",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Some",
                    "recent",
                    "works",
                    "#REF",
                    "have",
                    "also",
                    "explored",
                    "discriminative",
                    "reranking",
                    "of",
                    "candidates",
                    "from",
                    "neural",
                    "natural",
                    "language",
                    "generation",
                    "models,",
                    "which",
                    "adopt",
                    "large",
                    "pre-trained",
                    "language",
                    "models",
                    "(e.g."
                ],
                [
                    "BERT",
                    "#REF",
                    ")",
                    "as",
                    "the",
                    "reranker."
                ],
                [
                    "In",
                    "this",
                    "work,",
                    "we",
                    "factorize",
                    "the",
                    "Seq2Seq",
                    "model",
                    "(e.g.,",
                    "BART)",
                    "trained",
                    "on",
                    "the",
                    "same",
                    "dataset",
                    "as",
                    "the",
                    "reranking",
                    "model,",
                    "which",
                    "maximizes",
                    "the",
                    "parameter",
                    "sharing",
                    "across",
                    "two",
                    "stages."
                ],
                [
                    "Besides,",
                    "our",
                    "approach",
                    "contributes",
                    "an",
                    "instance",
                    "of",
                    "leveraging",
                    "large",
                    "pre-trained",
                    "Seq2Seq",
                    "models",
                    "as",
                    "a",
                    "quality",
                    "estimation",
                    "model",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: which are well-known challenges of reinforcement learning methods.\n sent1: Minimum risk training #REF and other online sampling based methods #REF belong to another school of methods used to circumvent the problem of non-differentiability.\n sent2: However, they also exhibit similar problems of stability as reinforcement learning.\n sent3: Contrastive Learning Recently, contrastive learning #REF has been introduced into several conditional text generation tasks, such as machine translation #REF , text summarization #REF , and other tasks #REF .\n sent4: Among these application scenarios, most work deployed contrastive learning in the latent representation space, following the framework proposed in #REF .\n sent5: However, in this work we adopt contrastive learning over the discrete space of the generated texts.\n sent6: Besides, instead of constructing the contrastive learning examples by rule-based methods (e.g.\n sent7: perturbing the reference output), we use the generation models to construct the examples, which makes the contrastive learning task closer to the generation task.\n sent8: Sun and Li (2021) also adopted contrastive learning on the generated texts.\n sent9: However, their formulation belongs to the margin-based losses.\n sent10: We have discussed the difference between our method and the margin-based losses in the previous paragraphs.\n sent11: Discriminative Reranking Discriminative reranking has been widely studied for conditional generation tasks #TARGET_REF .\n sent12: Some recent works #REF have also explored discriminative reranking of candidates from neural natural language generation models, which adopt large pre-trained language models (e.g.\n sent13: BERT #REF ) as the reranker.\n sent14: In this work, we factorize the Seq2Seq model (e.g., BART) trained on the same dataset as the reranking model, which maximizes the parameter sharing across two stages.\n sent15: Besides, our approach contributes an instance of leveraging large pre-trained Seq2Seq models as a quality estimation model #REF .\n",
        "output": "{\"INFORMATION\": [\"sent11\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "One",
                    "of",
                    "the",
                    "most",
                    "exciting",
                    "outcomes",
                    "is",
                    "the",
                    "deteriorated",
                    "performance",
                    "of",
                    "the",
                    "multilingual",
                    "models",
                    "using",
                    "BT",
                    "data,",
                    "as",
                    "we",
                    "usually",
                    "expect",
                    "that",
                    "added",
                    "backtranslated",
                    "texts",
                    "would",
                    "benefit",
                    "performance."
                ],
                [
                    "Using",
                    "tags",
                    "#REF",
                    ")",
                    "to",
                    "differentiate",
                    "which",
                    "data",
                    "is",
                    "synthetic",
                    "or",
                    "not",
                    "is",
                    "only",
                    "a",
                    "simple",
                    "step",
                    "to",
                    "address",
                    "this",
                    "issue,",
                    "however,",
                    "there",
                    "could",
                    "be",
                    "evaluated",
                    "more",
                    "informed",
                    "strategies",
                    "for",
                    "denoising",
                    "or",
                    "performing",
                    "online",
                    "data",
                    "selection",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                2,
                2
            ]
        },
        "input": "sent0: One of the most exciting outcomes is the deteriorated performance of the multilingual models using BT data, as we usually expect that added backtranslated texts would benefit performance.\n sent1: Using tags #REF ) to differentiate which data is synthetic or not is only a simple step to address this issue, however, there could be evaluated more informed strategies for denoising or performing online data selection #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\", \"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "task",
                    "of",
                    "fact",
                    "extraction",
                    "and",
                    "verification",
                    "aims",
                    "to",
                    "extract",
                    "evidence",
                    "and",
                    "verify",
                    "a",
                    "given",
                    "claim."
                ],
                [
                    "Previous",
                    "efforts",
                    "focus",
                    "on",
                    "dealing",
                    "with",
                    "text",
                    "format",
                    "evidence",
                    "from",
                    "unstructured",
                    "documents",
                    "#REF",
                    "or",
                    "evidence",
                    "from",
                    "a",
                    "single",
                    "given",
                    "table",
                    "#REF",
                    "."
                ],
                [
                    "Recently,",
                    "#TARGET_REF",
                    "propose",
                    "a",
                    "new",
                    "realistic",
                    "setting,",
                    "FEVEROUS,",
                    "i.e.,",
                    "fact",
                    "extraction",
                    "and",
                    "verification",
                    "over",
                    "unstructured",
                    "and",
                    "structured",
                    "information."
                ],
                [
                    "In",
                    "FEVEROUS,",
                    "models",
                    "should",
                    "not",
                    "only",
                    "extract",
                    "evidence",
                    "sentences/table",
                    "cells",
                    "from",
                    "millions",
                    "of",
                    "passages,",
                    "but",
                    "also",
                    "combine",
                    "the",
                    "evidence",
                    "in",
                    "different",
                    "formats",
                    "to",
                    "verify",
                    "a",
                    "given",
                    "claim."
                ]
            ],
            "context": [
                0,
                3,
                1,
                1
            ]
        },
        "input": "sent0: The task of fact extraction and verification aims to extract evidence and verify a given claim.\n sent1: Previous efforts focus on dealing with text format evidence from unstructured documents #REF or evidence from a single given table #REF .\n sent2: Recently, #TARGET_REF propose a new realistic setting, FEVEROUS, i.e., fact extraction and verification over unstructured and structured information.\n sent3: In FEVEROUS, models should not only extract evidence sentences/table cells from millions of passages, but also combine the evidence in different formats to verify a given claim.\n",
        "output": "{\"INFORMATION\": [\"sent2\", \"sent3\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "DRS",
                    "parsers",
                    "follow",
                    "the",
                    "encoderdecoder",
                    "paradigm",
                    "pioneered",
                    "for",
                    "machine",
                    "translation",
                    "by",
                    "#REF",
                    ":",
                    "the",
                    "input",
                    "sequence",
                    "is",
                    "encoded",
                    "by",
                    "a",
                    "neural",
                    "network",
                    "into",
                    "a",
                    "vector,",
                    "then",
                    "another",
                    "network",
                    "predicts",
                    "the",
                    "output",
                    "sequence",
                    "(or",
                    "in",
                    "this",
                    "case:",
                    "output",
                    "DRS)",
                    "from",
                    "that",
                    "vector."
                ],
                [
                    "Rather",
                    "than",
                    "improve",
                    "upon",
                    "the",
                    "accuracy",
                    "of",
                    "such",
                    "parsers",
                    "on",
                    "standard",
                    "benchmarks,",
                    "our",
                    "aim",
                    "in",
                    "this",
                    "paper",
                    "is",
                    "to",
                    "achieve",
                    "some",
                    "of",
                    "their",
                    "benefits",
                    "(ability",
                    "to",
                    "learn",
                    "from",
                    "examples,",
                    "high",
                    "accuracy,",
                    "low",
                    "computational",
                    "complexity,",
                    "robustness",
                    "to",
                    "atypical",
                    "input,",
                    "utilization",
                    "of",
                    "off-the-shelf",
                    "language",
                    "models,",
                    "conceptual",
                    "simplicity)",
                    "while",
                    "also",
                    "having",
                    "a",
                    "degree",
                    "of",
                    "compositionality,",
                    "traditionally",
                    "a",
                    "property",
                    "of",
                    "grammar-based",
                    "systems."
                ],
                [
                    "Specifically,",
                    "our",
                    "system",
                    "learns",
                    "to",
                    "assign",
                    "each",
                    "token",
                    "of",
                    "an",
                    "utterance",
                    "one",
                    "of",
                    "a",
                    "finite",
                    "set",
                    "of",
                    "abstract",
                    "meaning",
                    "fragments",
                    "that",
                    "are",
                    "deterministically",
                    "combined",
                    "to",
                    "give",
                    "the",
                    "meaning",
                    "of",
                    "the",
                    "whole",
                    "utterance."
                ],
                [
                    "While",
                    "our",
                    "system",
                    "may",
                    "not",
                    "fulfill",
                    "all",
                    "criteria",
                    "of",
                    "compositionality",
                    "according",
                    "to",
                    "some",
                    "definitions,",
                    "it",
                    "can",
                    "arguably",
                    "reap",
                    "some",
                    "of",
                    "compositionality's",
                    "benefits,",
                    "which",
                    "make",
                    "it",
                    "suitable",
                    "for",
                    "use",
                    "in",
                    "semi-automatic",
                    "annotation",
                    "workflows."
                ],
                [
                    "We",
                    "discuss",
                    "this",
                    "further",
                    "in",
                    "Section",
                    "5."
                ],
                [
                    "Previous",
                    "work",
                    "has",
                    "introduced",
                    "trainable",
                    "compositional",
                    "semantic",
                    "parsers",
                    "for",
                    "AMR",
                    "#TARGET_REF",
                    "and",
                    "DRS",
                    "#REF",
                    "."
                ],
                [
                    "In",
                    "this",
                    "paper,",
                    "we",
                    "improve",
                    "upon",
                    "the",
                    "latter",
                    "parser",
                    "using",
                    "a",
                    "novel",
                    "way",
                    "to",
                    "encode",
                    "anchored",
                    "DRSs",
                    "as",
                    "sequences,",
                    "and",
                    "thereby",
                    "cast",
                    "DRS",
                    "parsing",
                    "simply",
                    "as",
                    "a",
                    "sequence",
                    "labeling",
                    "task",
                    "(",
                    "§2)."
                ],
                [
                    "We",
                    "use",
                    "a",
                    "standard",
                    "transformer-based",
                    "model",
                    "to",
                    "learn",
                    "this",
                    "task,",
                    "followed",
                    "by",
                    "post-processing",
                    "to",
                    "ensure",
                    "well-formed",
                    "DRSs",
                    "(",
                    "§3)."
                ],
                [
                    "We",
                    "use",
                    "training",
                    "data",
                    "from",
                    "the",
                    "Parallel",
                    "Meaning",
                    "Bank",
                    "(",
                    "§4)."
                ],
                [
                    "The",
                    "accuracy",
                    "of",
                    "our",
                    "model",
                    "approaches",
                    "the",
                    "state",
                    "of",
                    "the",
                    "art",
                    "with",
                    "the",
                    "additional",
                    "benefit",
                    "of",
                    "being,",
                    "to",
                    "a",
                    "degree,",
                    "compositional",
                    "(",
                    "§5)."
                ],
                [
                    "We",
                    "give",
                    "an",
                    "error",
                    "analysis",
                    "in",
                    "§6",
                    "and",
                    "conclude",
                    "in",
                    "§7."
                ]
            ],
            "context": [
                0,
                0,
                0,
                2,
                0,
                3,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: DRS parsers follow the encoderdecoder paradigm pioneered for machine translation by #REF : the input sequence is encoded by a neural network into a vector, then another network predicts the output sequence (or in this case: output DRS) from that vector.\n sent1: Rather than improve upon the accuracy of such parsers on standard benchmarks, our aim in this paper is to achieve some of their benefits (ability to learn from examples, high accuracy, low computational complexity, robustness to atypical input, utilization of off-the-shelf language models, conceptual simplicity) while also having a degree of compositionality, traditionally a property of grammar-based systems.\n sent2: Specifically, our system learns to assign each token of an utterance one of a finite set of abstract meaning fragments that are deterministically combined to give the meaning of the whole utterance.\n sent3: While our system may not fulfill all criteria of compositionality according to some definitions, it can arguably reap some of compositionality's benefits, which make it suitable for use in semi-automatic annotation workflows.\n sent4: We discuss this further in Section 5.\n sent5: Previous work has introduced trainable compositional semantic parsers for AMR #TARGET_REF and DRS #REF .\n sent6: In this paper, we improve upon the latter parser using a novel way to encode anchored DRSs as sequences, and thereby cast DRS parsing simply as a sequence labeling task ( §2).\n sent7: We use a standard transformer-based model to learn this task, followed by post-processing to ensure well-formed DRSs ( §3).\n sent8: We use training data from the Parallel Meaning Bank ( §4).\n sent9: The accuracy of our model approaches the state of the art with the additional benefit of being, to a degree, compositional ( §5).\n sent10: We give an error analysis in §6 and conclude in §7.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent3\"], \"BACKGROUND\": [\"sent5\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "For",
                    "the",
                    "final-stage",
                    "neural",
                    "reranker,",
                    "we",
                    "experiment",
                    "with",
                    "BERT-large",
                    "and",
                    "T5-base",
                    "in",
                    "the",
                    "PyGaggle",
                    "library",
                    "fine-tuned",
                    "on",
                    "the",
                    "MS",
                    "MARCO",
                    "passage",
                    "data."
                ],
                [
                    "4",
                    "We",
                    "simply",
                    "use",
                    "checkpoints",
                    "provided",
                    "by",
                    "the",
                    "library,",
                    "as",
                    "our",
                    "work",
                    "is",
                    "not",
                    "specifically",
                    "focused",
                    "on",
                    "final-stage",
                    "neural",
                    "reranking."
                ],
                [
                    "Previous",
                    "evaluations",
                    "#TARGET_REF",
                    "have",
                    "already",
                    "verified",
                    "that",
                    "these",
                    "two",
                    "models",
                    "serve",
                    "as",
                    "competitive",
                    "baselines."
                ],
                [
                    "We",
                    "pad",
                    "all",
                    "the",
                    "token",
                    "sequences",
                    "in",
                    "the",
                    "batch",
                    "to",
                    "have",
                    "the",
                    "same",
                    "length",
                    "and",
                    "truncate",
                    "them",
                    "if",
                    "their",
                    "lengths",
                    "exceed",
                    "512",
                    "tokens."
                ]
            ],
            "context": [
                0,
                0,
                1,
                0
            ]
        },
        "input": "sent0: For the final-stage neural reranker, we experiment with BERT-large and T5-base in the PyGaggle library fine-tuned on the MS MARCO passage data.\n sent1: 4 We simply use checkpoints provided by the library, as our work is not specifically focused on final-stage neural reranking.\n sent2: Previous evaluations #TARGET_REF have already verified that these two models serve as competitive baselines.\n sent3: We pad all the token sequences in the batch to have the same length and truncate them if their lengths exceed 512 tokens.\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "the",
                    "specification",
                    "phase",
                    "we",
                    "have",
                    "taken",
                    "into",
                    "account",
                    "requirements",
                    "of",
                    "NLP",
                    "applications",
                    "and",
                    "tasks",
                    "(parsing,",
                    "generation,",
                    "machine",
                    "translation,",
                    "word",
                    "sense",
                    "disambiguation,",
                    "cross-language",
                    "information",
                    "retrieval,",
                    "etc.)"
                ],
                [
                    "-also",
                    "as",
                    "stated",
                    "in",
                    "the",
                    "EAGLES",
                    "report",
                    "of",
                    "the",
                    "Lexicon/Semantics",
                    "Working",
                    "Group",
                    "#TARGET_REF",
                    ")",
                    "-for",
                    "the",
                    "decisions",
                    "on",
                    "the",
                    "basic",
                    "semantic",
                    "notions",
                    "and",
                    "the",
                    "more",
                    "specific",
                    "types",
                    "of",
                    "semantic",
                    "information",
                    "to",
                    "be",
                    "encoded."
                ],
                [
                    "This",
                    "is",
                    "of",
                    "utmost",
                    "importance",
                    "given",
                    "the",
                    "applicative",
                    "objectives",
                    "of",
                    "the",
                    "PAROLE/SIMPLE",
                    "lexicons."
                ],
                [
                    "A",
                    "dichotomy",
                    "at",
                    "stake",
                    "here",
                    "is",
                    "the",
                    "one",
                    "between",
                    "generality",
                    "of",
                    "a",
                    "LR",
                    "vs.",
                    "usefulness",
                    "for",
                    "applications."
                ],
                [
                    "In",
                    "principle,",
                    "only",
                    "when",
                    "we",
                    "know",
                    "the",
                    "actual",
                    "specific",
                    "use",
                    "we",
                    "intend",
                    "to",
                    "do",
                    "of",
                    "a",
                    "LR",
                    "can",
                    "we",
                    "build",
                    "the",
                    "'very",
                    "best'",
                    "LR",
                    "for",
                    "that",
                    "use,",
                    "but",
                    "this",
                    "has",
                    "proved",
                    "to",
                    "be",
                    "too",
                    "expensive",
                    "and",
                    "not",
                    "realistic."
                ],
                [
                    "In",
                    "practice,",
                    "however,",
                    "there",
                    "exists",
                    "a",
                    "large",
                    "core",
                    "of",
                    "information",
                    "that",
                    "can",
                    "be",
                    "shared",
                    "by",
                    "many",
                    "applicative",
                    "uses,",
                    "and",
                    "this",
                    "leads",
                    "to",
                    "the",
                    "concept",
                    "of",
                    "\"generic\"",
                    "LR,",
                    "which",
                    "is",
                    "at",
                    "the",
                    "basis",
                    "of",
                    "the",
                    "EAGLES",
                    "initiative",
                    "and",
                    "of",
                    "the",
                    "PAROLE/SIMPLE",
                    "projects."
                ],
                [
                    "This",
                    "generic",
                    "shareable",
                    "core",
                    "of",
                    "information",
                    "must",
                    "then",
                    "be",
                    "enhanced",
                    "and",
                    "tuned",
                    "with",
                    "other",
                    "means",
                    "(see",
                    "sections",
                    "3",
                    "and",
                    "4)."
                ]
            ],
            "context": [
                2,
                3,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: In the specification phase we have taken into account requirements of NLP applications and tasks (parsing, generation, machine translation, word sense disambiguation, cross-language information retrieval, etc.)\n sent1: -also as stated in the EAGLES report of the Lexicon/Semantics Working Group #TARGET_REF ) -for the decisions on the basic semantic notions and the more specific types of semantic information to be encoded.\n sent2: This is of utmost importance given the applicative objectives of the PAROLE/SIMPLE lexicons.\n sent3: A dichotomy at stake here is the one between generality of a LR vs. usefulness for applications.\n sent4: In principle, only when we know the actual specific use we intend to do of a LR can we build the 'very best' LR for that use, but this has proved to be too expensive and not realistic.\n sent5: In practice, however, there exists a large core of information that can be shared by many applicative uses, and this leads to the concept of \"generic\" LR, which is at the basis of the EAGLES initiative and of the PAROLE/SIMPLE projects.\n sent6: This generic shareable core of information must then be enhanced and tuned with other means (see sections 3 and 4).\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "to",
                    "form",
                    "reasoning",
                    "chains",
                    "that",
                    "support",
                    "the",
                    "correct",
                    "answer",
                    "(see",
                    "Figure",
                    "1)."
                ],
                [
                    "As",
                    "such,",
                    "multi-hop",
                    "inference",
                    "represents",
                    "a",
                    "crucial",
                    "step",
                    "towards",
                    "explainability",
                    "in",
                    "complex",
                    "question",
                    "answering,",
                    "as",
                    "the",
                    "set",
                    "of",
                    "supporting",
                    "facts",
                    "can",
                    "be",
                    "interpreted",
                    "as",
                    "an",
                    "explanation",
                    "for",
                    "the",
                    "underlying",
                    "inference",
                    "process",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                3,
                2
            ]
        },
        "input": "sent0: to form reasoning chains that support the correct answer (see Figure 1).\n sent1: As such, multi-hop inference represents a crucial step towards explainability in complex question answering, as the set of supporting facts can be interpreted as an explanation for the underlying inference process #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "use",
                    "of",
                    "synthetic",
                    "datasets",
                    "is",
                    "the",
                    "common",
                    "practice",
                    "for",
                    "evaluating",
                    "disentanglement",
                    "in",
                    "image",
                    "domain",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Generative",
                    "simplistic",
                    "datasets",
                    "in",
                    "image",
                    "domain",
                    "define",
                    "independent",
                    "generative",
                    "factors",
                    "(e.g."
                ],
                [
                    "shape,",
                    "color)",
                    "behind",
                    "the",
                    "data",
                    "generation."
                ],
                [
                    "However,",
                    "a",
                    "comparable",
                    "resource",
                    "is",
                    "missing",
                    "in",
                    "text",
                    "domain."
                ],
                [
                    "We",
                    "develop",
                    "two",
                    "synthetic",
                    "generative",
                    "datasets",
                    "with",
                    "varying",
                    "degrees",
                    "of",
                    "difficulty",
                    "to",
                    "analyse",
                    "and",
                    "measure",
                    "disentanglement:",
                    "The",
                    "YNOC",
                    "dataset",
                    "(",
                    "§3.1)",
                    "which",
                    "has",
                    "only",
                    "three",
                    "structures",
                    "and",
                    "generative",
                    "factors",
                    "appearing",
                    "in",
                    "every",
                    "sentence,",
                    "and",
                    "the",
                    "POS",
                    "dataset",
                    "(",
                    "§3.2)",
                    "which",
                    "has",
                    "more",
                    "structures",
                    "while",
                    "some",
                    "generative",
                    "factors",
                    "are",
                    "not",
                    "guaranteed",
                    "to",
                    "appear",
                    "in",
                    "every",
                    "sentence."
                ],
                [
                    "The",
                    "YNOC",
                    "dataset",
                    "offers",
                    "a",
                    "simpler",
                    "setting",
                    "for",
                    "disentanglement."
                ],
                [
                    "The",
                    "templates",
                    "were",
                    "then",
                    "converted",
                    "into",
                    "real",
                    "sentences",
                    "using",
                    "10",
                    "years,",
                    "40",
                    "names,",
                    "20",
                    "occupations,",
                    "and",
                    "30",
                    "cities."
                ],
                [
                    "This",
                    "amounted",
                    "to",
                    "a",
                    "total",
                    "of",
                    "720K",
                    "sentences,",
                    "split",
                    "as",
                    "(60%,20%,20%)",
                    "into",
                    "training,",
                    "validation,",
                    "and",
                    "test",
                    "sets."
                ]
            ],
            "context": [
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: The use of synthetic datasets is the common practice for evaluating disentanglement in image domain #TARGET_REF .\n sent1: Generative simplistic datasets in image domain define independent generative factors (e.g.\n sent2: shape, color) behind the data generation.\n sent3: However, a comparable resource is missing in text domain.\n sent4: We develop two synthetic generative datasets with varying degrees of difficulty to analyse and measure disentanglement: The YNOC dataset ( §3.1) which has only three structures and generative factors appearing in every sentence, and the POS dataset ( §3.2) which has more structures while some generative factors are not guaranteed to appear in every sentence.\n sent5: The YNOC dataset offers a simpler setting for disentanglement.\n sent6: The templates were then converted into real sentences using 10 years, 40 names, 20 occupations, and 30 cities.\n sent7: This amounted to a total of 720K sentences, split as (60%,20%,20%) into training, validation, and test sets.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "For",
                    "the",
                    "cosine",
                    "similarity",
                    "calculation,we",
                    "use",
                    "word2vec",
                    "to",
                    "calculate",
                    "the",
                    "sentence",
                    "vector",
                    "as",
                    "sum",
                    "of",
                    "the",
                    "word",
                    "vectors",
                    "of",
                    "the",
                    "words",
                    "in",
                    "the",
                    "sentence."
                ],
                [
                    "The",
                    "calculation",
                    "of",
                    "sentence",
                    "vector",
                    "was",
                    "to",
                    "take",
                    "advantage",
                    "of",
                    "the",
                    "compositionality",
                    "property",
                    "using",
                    "word2vec",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "We",
                    "used",
                    "word",
                    "vectors",
                    "of",
                    "dimension",
                    "100",
                    "trained",
                    "on",
                    "the",
                    "2015",
                    "wikidump."
                ]
            ],
            "context": [
                2,
                1,
                2
            ]
        },
        "input": "sent0: For the cosine similarity calculation,we use word2vec to calculate the sentence vector as sum of the word vectors of the words in the sentence.\n sent1: The calculation of sentence vector was to take advantage of the compositionality property using word2vec #TARGET_REF .\n sent2: We used word vectors of dimension 100 trained on the 2015 wikidump.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [\"sent0\", \"sent2\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Data",
                    "Augmentation",
                    "The",
                    "third",
                    "strategy",
                    "aims",
                    "to",
                    "alleviate",
                    "the",
                    "issue",
                    "of",
                    "limited",
                    "training",
                    "data."
                ],
                [
                    "Since",
                    "the",
                    "cross-encoder",
                    "is",
                    "more",
                    "powerful",
                    "in",
                    "measuring",
                    "the",
                    "similarity",
                    "between",
                    "questions",
                    "and",
                    "passages,",
                    "we",
                    "utilize",
                    "it",
                    "to",
                    "annotate",
                    "unlabeled",
                    "questions",
                    "for",
                    "data",
                    "augmentation."
                ],
                [
                    "Specifically,",
                    "we",
                    "incorporate",
                    "a",
                    "new",
                    "collection",
                    "of",
                    "unlabeled",
                    "questions,",
                    "while",
                    "reuse",
                    "the",
                    "passage",
                    "collection."
                ],
                [
                    "Then,",
                    "we",
                    "use",
                    "the",
                    "learned",
                    "crossencoder",
                    "to",
                    "predict",
                    "the",
                    "passage",
                    "labels",
                    "for",
                    "the",
                    "new",
                    "questions."
                ],
                [
                    "To",
                    "ensure",
                    "the",
                    "quality",
                    "of",
                    "the",
                    "automatically",
                    "labeled",
                    "data,",
                    "we",
                    "only",
                    "select",
                    "the",
                    "predicted",
                    "positive",
                    "and",
                    "negative",
                    "passages",
                    "with",
                    "high",
                    "confidence",
                    "scores",
                    "estimated",
                    "by",
                    "the",
                    "cross-encoder."
                ],
                [
                    "Finally,",
                    "the",
                    "automatically",
                    "labeled",
                    "data",
                    "is",
                    "used",
                    "as",
                    "augmented",
                    "training",
                    "data",
                    "to",
                    "learn",
                    "the",
                    "dual",
                    "encoder."
                ],
                [
                    "Another",
                    "view",
                    "of",
                    "the",
                    "data",
                    "augmentation",
                    "is",
                    "knowledge",
                    "distillation",
                    "#TARGET_REF",
                    ",",
                    "where",
                    "the",
                    "cross-encoder",
                    "is",
                    "the",
                    "teacher",
                    "and",
                    "the",
                    "dual-encoder",
                    "is",
                    "the",
                    "student."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "sent0: Data Augmentation The third strategy aims to alleviate the issue of limited training data.\n sent1: Since the cross-encoder is more powerful in measuring the similarity between questions and passages, we utilize it to annotate unlabeled questions for data augmentation.\n sent2: Specifically, we incorporate a new collection of unlabeled questions, while reuse the passage collection.\n sent3: Then, we use the learned crossencoder to predict the passage labels for the new questions.\n sent4: To ensure the quality of the automatically labeled data, we only select the predicted positive and negative passages with high confidence scores estimated by the cross-encoder.\n sent5: Finally, the automatically labeled data is used as augmented training data to learn the dual encoder.\n sent6: Another view of the data augmentation is knowledge distillation #TARGET_REF , where the cross-encoder is the teacher and the dual-encoder is the student.\n",
        "output": "{\"INFORMATION\": [\"sent6\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Semantic",
                    "Similarity",
                    "In",
                    "addition",
                    "to",
                    "the",
                    "BLEU",
                    "score,",
                    "we",
                    "use",
                    "the",
                    "sentence-transformers",
                    "toolkit",
                    "#TARGET_REF",
                    "to",
                    "convert",
                    "the",
                    "generated",
                    "definitions",
                    "and",
                    "references",
                    "into",
                    "sentence",
                    "vectors,",
                    "and",
                    "calculate",
                    "cosine",
                    "similarity",
                    "between",
                    "them."
                ]
            ],
            "context": [
                2
            ]
        },
        "input": "sent0: Semantic Similarity In addition to the BLEU score, we use the sentence-transformers toolkit #TARGET_REF to convert the generated definitions and references into sentence vectors, and calculate cosine similarity between them.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "To",
                    "deal",
                    "with",
                    "toxic",
                    "comments,",
                    "most",
                    "approaches",
                    "adopt",
                    "supervised-machine",
                    "learning",
                    "techniques",
                    "and",
                    "are",
                    "mainly",
                    "focused",
                    "on",
                    "the",
                    "English",
                    "language",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "These",
                    "approaches",
                    "range",
                    "from",
                    "surface-level",
                    "features,",
                    "as",
                    "Bag-Of-Words",
                    "#REF",
                    ",",
                    "linguistics",
                    "features,",
                    "as",
                    "Part-Of-Speech",
                    "information",
                    "#REF",
                    ",",
                    "deep",
                    "neural",
                    "networks,",
                    "as",
                    "Long",
                    "Short-Term",
                    "Memory",
                    "(LSTM)",
                    "#REF",
                    "and",
                    "Convolutional",
                    "Neural",
                    "Networks",
                    "(CNN)",
                    "#REF",
                    "to",
                    "Transformer",
                    "architectures",
                    "#REF",
                    "."
                ],
                [
                    "Despite",
                    "interesting",
                    "results",
                    "achieved",
                    "by",
                    "Transformer",
                    "architectures,",
                    "there",
                    "are",
                    "still",
                    "several",
                    "rooms",
                    "to",
                    "be",
                    "explored",
                    "in",
                    "this",
                    "research",
                    "area."
                ]
            ],
            "context": [
                1,
                0,
                0
            ]
        },
        "input": "sent0: To deal with toxic comments, most approaches adopt supervised-machine learning techniques and are mainly focused on the English language #TARGET_REF .\n sent1: These approaches range from surface-level features, as Bag-Of-Words #REF , linguistics features, as Part-Of-Speech information #REF , deep neural networks, as Long Short-Term Memory (LSTM) #REF and Convolutional Neural Networks (CNN) #REF to Transformer architectures #REF .\n sent2: Despite interesting results achieved by Transformer architectures, there are still several rooms to be explored in this research area.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "With",
                    "the",
                    "same",
                    "parameters",
                    "as",
                    "for",
                    "English,",
                    "we",
                    "used",
                    "language-specific",
                    "BERT",
                    "models",
                    "from",
                    "Huggingface",
                    "transformers",
                    "#TARGET_REF",
                    "for",
                    "the",
                    "Arabic,",
                    "Chinese,",
                    "Dutch,",
                    "Finnish,",
                    "German",
                    "and",
                    "Turkish",
                    "datasets",
                    "with",
                    "5-fold",
                    "cross-validation."
                ],
                [
                    "The",
                    "annotated",
                    "Finnish",
                    "dataset",
                    "achieves",
                    "an",
                    "f1",
                    "score",
                    "of",
                    "0.51."
                ],
                [
                    "The",
                    "projected",
                    "annotations",
                    "achieve",
                    "slightly",
                    "worse",
                    "f1",
                    "scores",
                    "than",
                    "the",
                    "annotated",
                    "dataset",
                    "at",
                    "0.45",
                    "for",
                    "Finnish",
                    "(see",
                    "table",
                    "9)."
                ],
                [
                    "The",
                    "other",
                    "datasets",
                    "achieve",
                    "similar",
                    "f1",
                    "scores,",
                    "with",
                    "the",
                    "Germanic",
                    "languages",
                    "of",
                    "German",
                    "and",
                    "Dutch",
                    "achieving",
                    "almost",
                    "as",
                    "high",
                    "scores",
                    "as",
                    "the",
                    "original",
                    "English",
                    "dataset."
                ],
                [
                    "This",
                    "is",
                    "likely",
                    "a",
                    "reflection",
                    "of",
                    "typological,",
                    "cultural,",
                    "and",
                    "linguistic",
                    "similarities",
                    "between",
                    "the",
                    "languages",
                    "making",
                    "the",
                    "translation",
                    "to",
                    "begin",
                    "with",
                    "more",
                    "similar",
                    "to",
                    "the",
                    "original",
                    "and",
                    "therefore",
                    "minimizing",
                    "information",
                    "loss."
                ]
            ],
            "context": [
                2,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: With the same parameters as for English, we used language-specific BERT models from Huggingface transformers #TARGET_REF for the Arabic, Chinese, Dutch, Finnish, German and Turkish datasets with 5-fold cross-validation.\n sent1: The annotated Finnish dataset achieves an f1 score of 0.51.\n sent2: The projected annotations achieve slightly worse f1 scores than the annotated dataset at 0.45 for Finnish (see table 9).\n sent3: The other datasets achieve similar f1 scores, with the Germanic languages of German and Dutch achieving almost as high scores as the original English dataset.\n sent4: This is likely a reflection of typological, cultural, and linguistic similarities between the languages making the translation to begin with more similar to the original and therefore minimizing information loss.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Japanese",
                    "and",
                    "English",
                    "Twitter",
                    "conversations",
                    "were",
                    "extracted",
                    "from",
                    "Twitter",
                    "replies,",
                    "adjacent",
                    "tweets",
                    "as",
                    "pairs",
                    "of",
                    "#REF",
                    "to",
                    "construct",
                    "a",
                    "single-turn",
                    "dialogue",
                    "dataset",
                    "of",
                    "one",
                    "million",
                    "dialogue",
                    "pairs",
                    "for",
                    "each",
                    "language."
                ],
                [
                    "SentencePiece",
                    "#TARGET_REF",
                    "was",
                    "trained",
                    "using",
                    "a",
                    "dataset",
                    "with",
                    "a",
                    "vocabulary",
                    "of",
                    "32,000",
                    "for",
                    "both",
                    "Japanese",
                    "and",
                    "English",
                    "data."
                ],
                [
                    "We",
                    "then",
                    "used",
                    "these",
                    "SentencePiece",
                    "models",
                    "to",
                    "tokenize",
                    "the",
                    "training",
                    "set",
                    "into",
                    "subwords."
                ],
                [
                    "Each",
                    "of",
                    "the",
                    "verification",
                    "set",
                    "and",
                    "the",
                    "test",
                    "set",
                    "consists",
                    "of",
                    "1024",
                    "pairs."
                ]
            ],
            "context": [
                0,
                2,
                2,
                3
            ]
        },
        "input": "sent0: Japanese and English Twitter conversations were extracted from Twitter replies, adjacent tweets as pairs of #REF to construct a single-turn dialogue dataset of one million dialogue pairs for each language.\n sent1: SentencePiece #TARGET_REF was trained using a dataset with a vocabulary of 32,000 for both Japanese and English data.\n sent2: We then used these SentencePiece models to tokenize the training set into subwords.\n sent3: Each of the verification set and the test set consists of 1024 pairs.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\", \"sent2\"], \"BACKGROUND\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "One",
                    "of",
                    "the",
                    "known",
                    "drawbacks",
                    "of",
                    "many",
                    "NLP",
                    "datasets",
                    "is",
                    "that",
                    "they",
                    "contain",
                    "artifacts."
                ],
                [
                    "2",
                    "Models",
                    "tend",
                    "to",
                    "ex-ploit",
                    "these",
                    "easy-to-learn",
                    "patterns",
                    "in",
                    "the",
                    "early",
                    "stages",
                    "of",
                    "training",
                    "#TARGET_REF",
                    ",",
                    "and",
                    "therefore,",
                    "they",
                    "may",
                    "not",
                    "focus",
                    "on",
                    "learning",
                    "harder",
                    "patterns",
                    "of",
                    "the",
                    "data",
                    "that",
                    "are",
                    "useful",
                    "for",
                    "solving",
                    "the",
                    "underlying",
                    "task."
                ],
                [
                    "As",
                    "a",
                    "result,",
                    "overfitting",
                    "to",
                    "dataset-specific",
                    "artifacts",
                    "limits",
                    "the",
                    "robustness",
                    "and",
                    "generalization",
                    "of",
                    "NLP",
                    "models."
                ]
            ],
            "context": [
                0,
                3,
                3
            ]
        },
        "input": "sent0: One of the known drawbacks of many NLP datasets is that they contain artifacts.\n sent1: 2 Models tend to ex-ploit these easy-to-learn patterns in the early stages of training #TARGET_REF , and therefore, they may not focus on learning harder patterns of the data that are useful for solving the underlying task.\n sent2: As a result, overfitting to dataset-specific artifacts limits the robustness and generalization of NLP models.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "compare",
                    "the",
                    "output",
                    "of",
                    "Epitran",
                    "and",
                    "Allosaurus",
                    "on",
                    "the",
                    "ALFFA",
                    "dataset."
                ],
                [
                    "Following",
                    "the",
                    "practice",
                    "of",
                    "#REF",
                    ",",
                    "we",
                    "used",
                    "the",
                    "editdistance",
                    "10",
                    "library",
                    "to",
                    "calculate",
                    "the",
                    "Phone",
                    "Error",
                    "Rate",
                    "(PER)."
                ],
                [
                    "Having",
                    "no",
                    "ground",
                    "truth",
                    "phone",
                    "annotations,",
                    "we",
                    "instead",
                    "take",
                    "Epitran's",
                    "outputs",
                    "as",
                    "\"ground",
                    "truth\"",
                    "for",
                    "comparison."
                ],
                [
                    "The",
                    "mean",
                    "PER",
                    "between",
                    "the",
                    "outputs",
                    "is",
                    "23.7%."
                ],
                [
                    "This",
                    "result",
                    "is",
                    "consistent",
                    "with",
                    "#TARGET_REF",
                    ",",
                    "which",
                    "finds",
                    "PERs",
                    "as",
                    "high",
                    "as",
                    "72.8%",
                    "when",
                    "testing",
                    "on",
                    "on",
                    "the",
                    "Bukusu",
                    "(bxk),",
                    "Saamia",
                    "(lsm)",
                    "and",
                    "East",
                    "Tusom",
                    "languages",
                    "(an",
                    "endangered",
                    "subdialect",
                    "of",
                    "the",
                    "Tungkhulic",
                    "language",
                    "family)."
                ],
                [
                    "However,",
                    "by",
                    "training",
                    "the",
                    "phone",
                    "recognizer",
                    "on",
                    "even",
                    "minimal",
                    "amounts",
                    "of",
                    "data",
                    "in",
                    "these",
                    "languages,",
                    "PERs",
                    "were",
                    "improved",
                    "significantly."
                ]
            ],
            "context": [
                0,
                0,
                0,
                2,
                1,
                2
            ]
        },
        "input": "sent0: We compare the output of Epitran and Allosaurus on the ALFFA dataset.\n sent1: Following the practice of #REF , we used the editdistance 10 library to calculate the Phone Error Rate (PER).\n sent2: Having no ground truth phone annotations, we instead take Epitran's outputs as \"ground truth\" for comparison.\n sent3: The mean PER between the outputs is 23.7%.\n sent4: This result is consistent with #TARGET_REF , which finds PERs as high as 72.8% when testing on on the Bukusu (bxk), Saamia (lsm) and East Tusom languages (an endangered subdialect of the Tungkhulic language family).\n sent5: However, by training the phone recognizer on even minimal amounts of data in these languages, PERs were improved significantly.\n",
        "output": "{\"INFORMATION\": [\"sent4\"], \"PERCEPTION\": [\"sent3\", \"sent5\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Logical",
                    "mechanisms",
                    "have",
                    "not",
                    "been",
                    "actively",
                    "studied",
                    "in",
                    "argumentative",
                    "relation",
                    "classification."
                ],
                [
                    "Models",
                    "based",
                    "on",
                    "hand-crafted",
                    "features",
                    "have",
                    "used",
                    "relatively",
                    "simple",
                    "lexical",
                    "features,",
                    "such",
                    "as",
                    "n-grams,",
                    "discourse",
                    "markers,",
                    "and",
                    "sentiment",
                    "agreement",
                    "and",
                    "word",
                    "overlap",
                    "between",
                    "two",
                    "statements",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Recently,",
                    "neural",
                    "models",
                    "have",
                    "become",
                    "dominant",
                    "approaches",
                    "#REF",
                    "."
                ],
                [
                    "Despite",
                    "their",
                    "high",
                    "accuracy",
                    "and",
                    "finding",
                    "of",
                    "some",
                    "word-level",
                    "interactions",
                    "between",
                    "statements",
                    "#REF",
                    ",",
                    "they",
                    "provide",
                    "quite",
                    "limited",
                    "insight",
                    "into",
                    "governing",
                    "mechanisms",
                    "in",
                    "argumentative",
                    "relations."
                ],
                [
                    "Indeed,",
                    "more",
                    "and",
                    "more",
                    "evidence",
                    "suggests",
                    "that",
                    "supervised",
                    "models",
                    "learn",
                    "to",
                    "overly",
                    "rely",
                    "on",
                    "superficial",
                    "cues,",
                    "such",
                    "as",
                    "discourse",
                    "markers",
                    "#REF",
                    ",",
                    "negating",
                    "words",
                    "#REF",
                    ",",
                    "and",
                    "sentiment",
                    "#REF",
                    "behind",
                    "the",
                    "scenes."
                ],
                [
                    "We",
                    "instead",
                    "use",
                    "an",
                    "interpretable",
                    "method",
                    "based",
                    "on",
                    "PSL",
                    "to",
                    "examine",
                    "logical",
                    "mechanisms",
                    "(",
                    "§7)",
                    "and",
                    "then",
                    "show",
                    "evidence",
                    "that",
                    "these",
                    "mechanisms",
                    "can",
                    "inform",
                    "supervised",
                    "models",
                    "in",
                    "intuitive",
                    "ways",
                    "(",
                    "§8)."
                ]
            ],
            "context": [
                0,
                2,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Logical mechanisms have not been actively studied in argumentative relation classification.\n sent1: Models based on hand-crafted features have used relatively simple lexical features, such as n-grams, discourse markers, and sentiment agreement and word overlap between two statements #TARGET_REF .\n sent2: Recently, neural models have become dominant approaches #REF .\n sent3: Despite their high accuracy and finding of some word-level interactions between statements #REF , they provide quite limited insight into governing mechanisms in argumentative relations.\n sent4: Indeed, more and more evidence suggests that supervised models learn to overly rely on superficial cues, such as discourse markers #REF , negating words #REF , and sentiment #REF behind the scenes.\n sent5: We instead use an interpretable method based on PSL to examine logical mechanisms ( §7) and then show evidence that these mechanisms can inform supervised models in intuitive ways ( §8).\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "use",
                    "the",
                    "entity",
                    "typing",
                    "model",
                    "trained",
                    "on",
                    "the",
                    "Wiki-Context",
                    "data",
                    "(see",
                    "Section",
                    "4)",
                    "to",
                    "get",
                    "the",
                    "mention",
                    "and",
                    "context",
                    "representation",
                    "t.",
                    "In",
                    "the",
                    "CoNLL-YAGO",
                    "setting,",
                    "similar",
                    "to",
                    "past",
                    "work",
                    "#TARGET_REF",
                    ",",
                    "we",
                    "prepend",
                    "the",
                    "document",
                    "title",
                    "and",
                    "the",
                    "first",
                    "sentence",
                    "to",
                    "the",
                    "input",
                    "to",
                    "enrich",
                    "the",
                    "context",
                    "information."
                ],
                [
                    "To",
                    "obtain",
                    "the",
                    "candidate",
                    "representations",
                    "{c",
                    "1",
                    ",",
                    "c",
                    "2",
                    ",",
                    "...,",
                    "c",
                    "j",
                    ",",
                    "...},",
                    "we",
                    "use",
                    "the",
                    "model",
                    "trained",
                    "on",
                    "the",
                    "Wiki-Description",
                    "data,",
                    "which",
                    "is",
                    "specialized",
                    "for",
                    "entity",
                    "descriptions",
                    "(see",
                    "Section",
                    "4)",
                    "similar",
                    "to",
                    "#REF",
                    "."
                ],
                [
                    "We",
                    "choose",
                    "Wikipedia",
                    "datasets",
                    "here",
                    "because",
                    "UFET",
                    "does",
                    "not",
                    "support",
                    "entity",
                    "descriptions."
                ],
                [
                    "We",
                    "rank",
                    "the",
                    "candidate",
                    "entities",
                    "based",
                    "on",
                    "cosine",
                    "similarity",
                    "between",
                    "t",
                    "and",
                    "c",
                    "j",
                    ",",
                    "and",
                    "the",
                    "entity",
                    "with",
                    "the",
                    "highest",
                    "score",
                    "is",
                    "our",
                    "model's",
                    "prediction."
                ]
            ],
            "context": [
                2,
                0,
                0,
                0
            ]
        },
        "input": "sent0: We use the entity typing model trained on the Wiki-Context data (see Section 4) to get the mention and context representation t. In the CoNLL-YAGO setting, similar to past work #TARGET_REF , we prepend the document title and the first sentence to the input to enrich the context information.\n sent1: To obtain the candidate representations {c 1 , c 2 , ..., c j , ...}, we use the model trained on the Wiki-Description data, which is specialized for entity descriptions (see Section 4) similar to #REF .\n sent2: We choose Wikipedia datasets here because UFET does not support entity descriptions.\n sent3: We rank the candidate entities based on cosine similarity between t and c j , and the entity with the highest score is our model's prediction.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Implementation",
                    "In",
                    "all",
                    "experiments",
                    "reported",
                    "below,",
                    "stochastic",
                    "gradient",
                    "descent",
                    "is",
                    "performed",
                    "using",
                    "the",
                    "Adam",
                    "algorithm",
                    "with",
                    "default",
                    "initial",
                    "learning",
                    "rate",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "All",
                    "experiments",
                    "are",
                    "implemented",
                    "in",
                    "PyTorch",
                    "with",
                    "use",
                    "of",
                    "opt_einsum",
                    "to",
                    "compute",
                    "the",
                    "partition",
                    "function",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                1,
                0
            ]
        },
        "input": "sent0: Implementation In all experiments reported below, stochastic gradient descent is performed using the Adam algorithm with default initial learning rate #TARGET_REF .\n sent1: All experiments are implemented in PyTorch with use of opt_einsum to compute the partition function #REF .\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "To",
                    "select",
                    "candidate",
                    "jobs",
                    "for",
                    "experiments,",
                    "we",
                    "use",
                    "the",
                    "list",
                    "of",
                    "jobs",
                    "in",
                    "the",
                    "UK",
                    "ASHE",
                    "report",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                1
            ]
        },
        "input": "sent0: To select candidate jobs for experiments, we use the list of jobs in the UK ASHE report #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "•",
                    "Sorry",
                    "(adjective)",
                    "-is",
                    "a",
                    "kind",
                    "of",
                    "feeling,",
                    "with",
                    "a",
                    "high",
                    "negative",
                    "score",
                    "and",
                    "emotion",
                    "label",
                    "of",
                    "regret-sorrow."
                ],
                [
                    "This",
                    "keyword",
                    "can",
                    "be",
                    "effective",
                    "in",
                    "situations",
                    "where",
                    "emotions",
                    "and",
                    "sentiments",
                    "are",
                    "strongly",
                    "involved."
                ],
                [
                    "Its",
                    "use",
                    "can",
                    "also",
                    "make",
                    "the",
                    "communication",
                    "sound",
                    "like",
                    "a",
                    "heartfelt",
                    "apology."
                ],
                [
                    "Also,",
                    "to",
                    "be",
                    "noted",
                    "is",
                    "the",
                    "fact",
                    "that",
                    "though",
                    "the",
                    "adjective",
                    "sorry",
                    "is",
                    "found",
                    "to",
                    "be",
                    "the",
                    "most",
                    "commonly-used",
                    "form",
                    "in",
                    "different",
                    "spoken",
                    "corpora."
                ],
                [
                    "#TARGET_REF",
                    "),",
                    "yet",
                    "in",
                    "our",
                    "data,",
                    "the",
                    "word",
                    "sorry",
                    "has",
                    "a",
                    "higher",
                    "occurrence",
                    "in",
                    "written",
                    "apologies",
                    "given",
                    "by",
                    "individuals-in-a",
                    "role",
                    "and",
                    "organizations."
                ]
            ],
            "context": [
                3,
                3,
                3,
                2,
                3
            ]
        },
        "input": "sent0: • Sorry (adjective) -is a kind of feeling, with a high negative score and emotion label of regret-sorrow.\n sent1: This keyword can be effective in situations where emotions and sentiments are strongly involved.\n sent2: Its use can also make the communication sound like a heartfelt apology.\n sent3: Also, to be noted is the fact that though the adjective sorry is found to be the most commonly-used form in different spoken corpora.\n sent4: #TARGET_REF ), yet in our data, the word sorry has a higher occurrence in written apologies given by individuals-in-a role and organizations.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent3\"], \"BACKGROUND\": [\"sent0\", \"sent1\", \"sent2\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Our",
                    "departure",
                    "point",
                    "for",
                    "this",
                    "work",
                    "has",
                    "been",
                    "the",
                    "notion",
                    "of",
                    "an",
                    "implicit",
                    "argument",
                    "of",
                    "a",
                    "noun,",
                    "that",
                    "is,",
                    "nouns",
                    "such",
                    "as",
                    "''brother''",
                    "or",
                    "''price''",
                    "that",
                    "are",
                    "incomplete",
                    "on",
                    "their",
                    "own,",
                    "and",
                    "require",
                    "an",
                    "argument",
                    "to",
                    "be",
                    "complete."
                ],
                [
                    "In",
                    "linguistics,",
                    "these",
                    "are",
                    "referred",
                    "to",
                    "as",
                    "relational",
                    "nouns",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "In",
                    "contrast,",
                    "nouns",
                    "like",
                    "''plant'',",
                    "or",
                    "''sofa''",
                    "are",
                    "called",
                    "sortal",
                    "and",
                    "are",
                    "conceived",
                    "as",
                    "''complete'',",
                    "their",
                    "denotation",
                    "need",
                    "not",
                    "rely",
                    "on",
                    "the",
                    "relation",
                    "to",
                    "other",
                    "nouns,",
                    "and",
                    "can",
                    "be",
                    "fully",
                    "determined."
                ]
            ],
            "context": [
                2,
                1,
                3
            ]
        },
        "input": "sent0: Our departure point for this work has been the notion of an implicit argument of a noun, that is, nouns such as ''brother'' or ''price'' that are incomplete on their own, and require an argument to be complete.\n sent1: In linguistics, these are referred to as relational nouns #TARGET_REF .\n sent2: In contrast, nouns like ''plant'', or ''sofa'' are called sortal and are conceived as ''complete'', their denotation need not rely on the relation to other nouns, and can be fully determined.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "CCG",
                    "provides",
                    "many",
                    "advantages",
                    "when",
                    "using",
                    "it",
                    "in",
                    "SMT",
                    "in",
                    "comparison",
                    "with",
                    "phrase-structure",
                    "grammar."
                ],
                [
                    "Firstly,",
                    "CCG",
                    "has",
                    "more",
                    "flexible",
                    "structures",
                    "in",
                    "comparison",
                    "with",
                    "phrase-structure",
                    "grammar."
                ],
                [
                    "This",
                    "flexibility",
                    "results",
                    "from",
                    "the",
                    "ability",
                    "to",
                    "combine",
                    "CCG",
                    "supertags",
                    "using",
                    "simple",
                    "combinatory",
                    "operators,",
                    "which",
                    "makes",
                    "it",
                    "possible",
                    "to",
                    "assign",
                    "a",
                    "CCG",
                    "category",
                    "to",
                    "a",
                    "phrase",
                    "that",
                    "does",
                    "not",
                    "represent",
                    "a",
                    "traditional",
                    "constituent",
                    "in",
                    "phrasestructure",
                    "grammar."
                ],
                [
                    "This",
                    "is",
                    "very",
                    "important",
                    "for",
                    "SMT",
                    "systems",
                    "as",
                    "the",
                    "power",
                    "of",
                    "SMT",
                    "lies",
                    "in",
                    "using",
                    "statistically",
                    "extracted",
                    "phrases",
                    "which",
                    "do",
                    "not",
                    "necessarily",
                    "correspond",
                    "to",
                    "syntactic",
                    "constituents."
                ],
                [
                    "Secondly,",
                    "CCG",
                    "categories",
                    "reflect",
                    "rich",
                    "information",
                    "about",
                    "the",
                    "syntactic",
                    "structure",
                    "to",
                    "which",
                    "the",
                    "word/phrase",
                    "belongs",
                    "at",
                    "the",
                    "lexical",
                    "level",
                    "without",
                    "the",
                    "need",
                    "to",
                    "build",
                    "a",
                    "full",
                    "parse",
                    "tree",
                    "for",
                    "the",
                    "sentence."
                ],
                [
                    "Thirdly,",
                    "CCG",
                    "parsing",
                    "is",
                    "more",
                    "efficient",
                    "in",
                    "comparison",
                    "to",
                    "phrase-structure",
                    "grammar",
                    "parsing."
                ],
                [
                    "Because",
                    "most",
                    "of",
                    "the",
                    "CCG",
                    "grammar",
                    "is",
                    "contained",
                    "in",
                    "the",
                    "lexicon,",
                    "the",
                    "process",
                    "of",
                    "supertagging,",
                    "which",
                    "is",
                    "to",
                    "assign",
                    "supertags",
                    "(i.e."
                ],
                [
                    "complex",
                    "CCG",
                    "categories)",
                    "to",
                    "the",
                    "words",
                    "in",
                    "a",
                    "sentence,",
                    "is",
                    "considered",
                    "\"almost",
                    "parsing\"",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "After",
                    "supertagging,",
                    "the",
                    "CCG",
                    "parser",
                    "is",
                    "only",
                    "required",
                    "to",
                    "combine",
                    "the",
                    "supertags",
                    "using",
                    "CCG",
                    "simple",
                    "combinatory",
                    "operators."
                ],
                [
                    "For",
                    "the",
                    "aforementioned",
                    "reasons,",
                    "CCG",
                    "is",
                    "considered",
                    "more",
                    "suitable",
                    "to",
                    "be",
                    "used",
                    "in",
                    "SMT",
                    "than",
                    "phrase-structure",
                    "grammar."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                1,
                0,
                0
            ]
        },
        "input": "sent0: CCG provides many advantages when using it in SMT in comparison with phrase-structure grammar.\n sent1: Firstly, CCG has more flexible structures in comparison with phrase-structure grammar.\n sent2: This flexibility results from the ability to combine CCG supertags using simple combinatory operators, which makes it possible to assign a CCG category to a phrase that does not represent a traditional constituent in phrasestructure grammar.\n sent3: This is very important for SMT systems as the power of SMT lies in using statistically extracted phrases which do not necessarily correspond to syntactic constituents.\n sent4: Secondly, CCG categories reflect rich information about the syntactic structure to which the word/phrase belongs at the lexical level without the need to build a full parse tree for the sentence.\n sent5: Thirdly, CCG parsing is more efficient in comparison to phrase-structure grammar parsing.\n sent6: Because most of the CCG grammar is contained in the lexicon, the process of supertagging, which is to assign supertags (i.e.\n sent7: complex CCG categories) to the words in a sentence, is considered \"almost parsing\" #TARGET_REF .\n sent8: After supertagging, the CCG parser is only required to combine the supertags using CCG simple combinatory operators.\n sent9: For the aforementioned reasons, CCG is considered more suitable to be used in SMT than phrase-structure grammar.\n",
        "output": "{\"INFORMATION\": [\"sent7\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent6\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "the",
                    "second",
                    "objective,",
                    "we",
                    "quantify",
                    "the",
                    "contribution",
                    "of",
                    "each",
                    "modality",
                    "and",
                    "their",
                    "aspects",
                    "given",
                    "the",
                    "situation",
                    "to",
                    "mimic",
                    "human",
                    "heuristic",
                    "processing",
                    "capability."
                ],
                [
                    "Language",
                    "comprehension",
                    "involves",
                    "complex",
                    "sequential",
                    "decision",
                    "making",
                    "and",
                    "is",
                    "affected",
                    "by",
                    "both",
                    "uncertainty",
                    "about",
                    "the",
                    "current",
                    "input",
                    "and",
                    "lack",
                    "of",
                    "knowledge",
                    "about",
                    "the",
                    "upcoming",
                    "material."
                ],
                [
                    "Thus,",
                    "people",
                    "use",
                    "-to",
                    "a",
                    "large",
                    "extent",
                    "-fast",
                    "and",
                    "frugal",
                    "heuristics,",
                    "i.e."
                ],
                [
                    "choosing",
                    "a",
                    "good-enough",
                    "representation",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "The",
                    "heuristic",
                    "view",
                    "provides",
                    "a",
                    "valid",
                    "explanation",
                    "for",
                    "scenarios",
                    "with",
                    "a",
                    "conversation",
                    "inside",
                    "noisy",
                    "conditions."
                ],
                [
                    "Instead",
                    "of",
                    "waiting",
                    "/",
                    "asking",
                    "for",
                    "clarification,",
                    "the",
                    "model",
                    "will",
                    "reach",
                    "a",
                    "good-enough",
                    "decision",
                    "based",
                    "on",
                    "all",
                    "information",
                    "gathered",
                    "through",
                    "all",
                    "available",
                    "input",
                    "channels."
                ],
                [
                    "In",
                    "order",
                    "to",
                    "do",
                    "that,",
                    "the",
                    "set",
                    "of",
                    "important",
                    "features",
                    "given",
                    "the",
                    "situated",
                    "setting",
                    "should",
                    "be",
                    "chosen",
                    "automatically."
                ]
            ],
            "context": [
                0,
                3,
                3,
                1,
                0,
                0,
                0
            ]
        },
        "input": "sent0: In the second objective, we quantify the contribution of each modality and their aspects given the situation to mimic human heuristic processing capability.\n sent1: Language comprehension involves complex sequential decision making and is affected by both uncertainty about the current input and lack of knowledge about the upcoming material.\n sent2: Thus, people use -to a large extent -fast and frugal heuristics, i.e.\n sent3: choosing a good-enough representation #TARGET_REF .\n sent4: The heuristic view provides a valid explanation for scenarios with a conversation inside noisy conditions.\n sent5: Instead of waiting / asking for clarification, the model will reach a good-enough decision based on all information gathered through all available input channels.\n sent6: In order to do that, the set of important features given the situated setting should be chosen automatically.\n",
        "output": "{\"INFORMATION\": [\"sent3\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "subject",
                    "NP",
                    "'Bill'",
                    "is",
                    "coindexed",
                    "with",
                    "the",
                    "trace",
                    "in",
                    "the",
                    "more",
                    "deeply",
                    "embedded",
                    "relative",
                    "clause."
                ],
                [
                    "If",
                    "we",
                    "assume,",
                    "following",
                    "#TARGET_REF",
                    ",",
                    "that",
                    "relative",
                    "clause",
                    "formation",
                    "involves",
                    "movement",
                    "from",
                    "an",
                    "inner",
                    "clause",
                    "into",
                    "an",
                    "outer",
                    "subject",
                    "position,",
                    "then",
                    "the",
                    "grammaticality",
                    "of",
                    "the",
                    "above",
                    "example",
                    "suggests",
                    "that",
                    "the",
                    "Trace",
                    "theory",
                    "must",
                    "be",
                    "parameterized",
                    "so",
                    "that",
                    "crossing",
                    "more",
                    "than",
                    "one",
                    "barrier",
                    "is",
                    "allowed",
                    "in",
                    "Korean."
                ],
                [
                    "Our",
                    "formulation",
                    "of",
                    "this",
                    "parametric",
                    "distinction",
                    "is",
                    "as",
                    "follows:"
                ]
            ],
            "context": [
                3,
                2,
                0
            ]
        },
        "input": "sent0: The subject NP 'Bill' is coindexed with the trace in the more deeply embedded relative clause.\n sent1: If we assume, following #TARGET_REF , that relative clause formation involves movement from an inner clause into an outer subject position, then the grammaticality of the above example suggests that the Trace theory must be parameterized so that crossing more than one barrier is allowed in Korean.\n sent2: Our formulation of this parametric distinction is as follows:\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "apply",
                    "the",
                    "proposed",
                    "framework",
                    "and",
                    "quantify",
                    "second-order",
                    "robustness",
                    "through",
                    "two",
                    "second-order",
                    "attacks",
                    "(",
                    "§3)."
                ],
                [
                    "We",
                    "experiment",
                    "with",
                    "English",
                    "sentiment",
                    "classification",
                    "on",
                    "the",
                    "SST-2",
                    "dataset",
                    "#TARGET_REF",
                    "across",
                    "various",
                    "model",
                    "architectures."
                ],
                [
                    "Surprisingly,",
                    "although",
                    "robustly",
                    "trained",
                    "CNN",
                    "#REF",
                    "and",
                    "Transformer",
                    "#REF",
                    "can",
                    "achieve",
                    "high",
                    "robustness",
                    "under",
                    "strong",
                    "attacks",
                    "#REF",
                    ")",
                    "(23.0%-71.6%",
                    "success",
                    "rates),",
                    "for",
                    "around",
                    "96.0%",
                    "of",
                    "the",
                    "test",
                    "examples",
                    "our",
                    "attacks",
                    "can",
                    "find",
                    "a",
                    "vulnerable",
                    "example",
                    "by",
                    "perturbing",
                    "1.3",
                    "words",
                    "on",
                    "average."
                ],
                [
                    "This",
                    "finding",
                    "indicates",
                    "that",
                    "these",
                    "robustly",
                    "trained",
                    "models,",
                    "despite",
                    "being",
                    "first-order",
                    "robust,",
                    "are",
                    "not",
                    "second-order",
                    "robust."
                ]
            ],
            "context": [
                0,
                2,
                0,
                0
            ]
        },
        "input": "sent0: We apply the proposed framework and quantify second-order robustness through two second-order attacks ( §3).\n sent1: We experiment with English sentiment classification on the SST-2 dataset #TARGET_REF across various model architectures.\n sent2: Surprisingly, although robustly trained CNN #REF and Transformer #REF can achieve high robustness under strong attacks #REF ) (23.0%-71.6% success rates), for around 96.0% of the test examples our attacks can find a vulnerable example by perturbing 1.3 words on average.\n sent3: This finding indicates that these robustly trained models, despite being first-order robust, are not second-order robust.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Once",
                    "an",
                    "agent",
                    "acts",
                    "or",
                    "talks,",
                    "the",
                    "partner",
                    "agent-in",
                    "this",
                    "case",
                    "also",
                    "a",
                    "polyencoder",
                    "#REF",
                    "trained",
                    "to",
                    "react",
                    "to",
                    "agents",
                    "with",
                    "motivations-also",
                    "acts",
                    "or",
                    "talks",
                    "and",
                    "this",
                    "information",
                    "is",
                    "processed",
                    "by",
                    "the",
                    "environment."
                ],
                [
                    "As",
                    "recommended",
                    "by",
                    "Prabhumoye",
                    "et",
                    "al."
                ],
                [
                    "(",
                    "2020),",
                    "#REF",
                    ",",
                    "we",
                    "keep",
                    "the",
                    "partner",
                    "model",
                    "fixed",
                    "during",
                    "the",
                    "episodes",
                    "where",
                    "the",
                    "LIGHT",
                    "agent",
                    "trains",
                    "to",
                    "ensure",
                    "that",
                    "it",
                    "retains",
                    "natural",
                    "English",
                    "semantics-avoiding",
                    "the",
                    "problem",
                    "of",
                    "language",
                    "drift",
                    "by",
                    "learning",
                    "an",
                    "emergent",
                    "language",
                    "with",
                    "that",
                    "must",
                    "agree",
                    "with",
                    "the",
                    "partner's",
                    "usage",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                3,
                0,
                2
            ]
        },
        "input": "sent0: Once an agent acts or talks, the partner agent-in this case also a polyencoder #REF trained to react to agents with motivations-also acts or talks and this information is processed by the environment.\n sent1: As recommended by Prabhumoye et al.\n sent2: ( 2020), #REF , we keep the partner model fixed during the episodes where the LIGHT agent trains to ensure that it retains natural English semantics-avoiding the problem of language drift by learning an emergent language with that must agree with the partner's usage #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Apart",
                    "from",
                    "the",
                    "small",
                    "gold",
                    "set",
                    "whose",
                    "quality",
                    "is",
                    "guaranteed",
                    "by",
                    "human",
                    "annotators,",
                    "PMB",
                    "3.0.0",
                    "also",
                    "contains",
                    "silver",
                    "and",
                    "bronze",
                    "data",
                    "with",
                    "partial",
                    "or",
                    "no",
                    "manual",
                    "checking",
                    "of",
                    "the",
                    "annotations."
                ],
                [
                    "Their",
                    "lower",
                    "quality",
                    "is",
                    "compensated",
                    "for",
                    "by",
                    "quantity."
                ],
                [
                    "#TARGET_REF",
                    "report",
                    "a",
                    "large",
                    "improvement",
                    "for",
                    "their",
                    "DRS",
                    "parser",
                    "when",
                    "first",
                    "training",
                    "on",
                    "the",
                    "bronze",
                    "and",
                    "silver",
                    "data,",
                    "then",
                    "\"fine-tuning\"",
                    "on",
                    "gold",
                    "data."
                ],
                [
                    "Since",
                    "we",
                    "are",
                    "using",
                    "a",
                    "Transformer",
                    "model",
                    "like",
                    "them,",
                    "we",
                    "expected",
                    "this",
                    "technique",
                    "could",
                    "also",
                    "boost",
                    "our",
                    "parser's",
                    "performance."
                ],
                [
                    "Thus,",
                    "we",
                    "tested",
                    "our",
                    "model",
                    "with",
                    "5",
                    "epochs",
                    "training",
                    "on",
                    "silver",
                    "and",
                    "bronze",
                    "followed",
                    "by",
                    "5",
                    "epochs",
                    "on",
                    "gold."
                ],
                [
                    "The",
                    "results",
                    "are",
                    "shown",
                    "in",
                    "Table",
                    "5."
                ],
                [
                    "They",
                    "confirm",
                    "that",
                    "more",
                    "data",
                    "means",
                    "better",
                    "results",
                    "even",
                    "when",
                    "the",
                    "data",
                    "is",
                    "not",
                    "perfect."
                ],
                [
                    "Although",
                    "the",
                    "bigger",
                    "training",
                    "set",
                    "also",
                    "increases",
                    "the",
                    "number",
                    "of",
                    "classes",
                    "for",
                    "all",
                    "three",
                    "labels",
                    "more",
                    "than",
                    "10-fold,",
                    "the",
                    "model",
                    "seems",
                    "to",
                    "handle",
                    "it",
                    "just",
                    "fine."
                ],
                [
                    "The",
                    "only",
                    "downside",
                    "is",
                    "the",
                    "longer",
                    "training",
                    "time:",
                    "as",
                    "the",
                    "silver",
                    "and",
                    "bronze",
                    "sets",
                    "for",
                    "English",
                    "are,",
                    "respectively,",
                    "21",
                    "and",
                    "25",
                    "times",
                    "larger",
                    "than",
                    "the",
                    "gold",
                    "one,",
                    "the",
                    "time",
                    "consumption",
                    "jumps",
                    "from",
                    "a",
                    "few",
                    "minutes",
                    "to",
                    "more",
                    "than",
                    "10",
                    "hours."
                ],
                [
                    "2019)",
                    "with",
                    "the",
                    "addition",
                    "of",
                    "BERT",
                    "embeddings,",
                    "and",
                    "their",
                    "\"best\"",
                    "model",
                    "encodes",
                    "the",
                    "character",
                    "embedding",
                    "and",
                    "the",
                    "BERT",
                    "embedding",
                    "separately",
                    "before",
                    "feeding",
                    "their",
                    "concatenated",
                    "vector",
                    "into",
                    "the",
                    "decoder,",
                    "which",
                    "achieved",
                    "state-of-the-art",
                    "results."
                ],
                [
                    "Worth",
                    "noting",
                    "is",
                    "their",
                    "claim",
                    "that",
                    "it's",
                    "best",
                    "to",
                    "keep",
                    "BERT",
                    "parameters",
                    "\"frozen\",",
                    "which",
                    "we",
                    "did",
                    "not",
                    "find",
                    "to",
                    "be",
                    "the",
                    "case",
                    "for",
                    "our",
                    "model:",
                    "in",
                    "preliminary",
                    "experimentation,",
                    "finetuning",
                    "BERT",
                    "parameters",
                    "with",
                    "our",
                    "model",
                    "outperformed",
                    "a",
                    "corresponding",
                    "frozen",
                    "model",
                    "by",
                    "20%",
                    "in",
                    "Counter",
                    "f-score."
                ]
            ],
            "context": [
                3,
                0,
                1,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Apart from the small gold set whose quality is guaranteed by human annotators, PMB 3.0.0 also contains silver and bronze data with partial or no manual checking of the annotations.\n sent1: Their lower quality is compensated for by quantity.\n sent2: #TARGET_REF report a large improvement for their DRS parser when first training on the bronze and silver data, then \"fine-tuning\" on gold data.\n sent3: Since we are using a Transformer model like them, we expected this technique could also boost our parser's performance.\n sent4: Thus, we tested our model with 5 epochs training on silver and bronze followed by 5 epochs on gold.\n sent5: The results are shown in Table 5.\n sent6: They confirm that more data means better results even when the data is not perfect.\n sent7: Although the bigger training set also increases the number of classes for all three labels more than 10-fold, the model seems to handle it just fine.\n sent8: The only downside is the longer training time: as the silver and bronze sets for English are, respectively, 21 and 25 times larger than the gold one, the time consumption jumps from a few minutes to more than 10 hours.\n sent9: 2019) with the addition of BERT embeddings, and their \"best\" model encodes the character embedding and the BERT embedding separately before feeding their concatenated vector into the decoder, which achieved state-of-the-art results.\n sent10: Worth noting is their claim that it's best to keep BERT parameters \"frozen\", which we did not find to be the case for our model: in preliminary experimentation, finetuning BERT parameters with our model outperformed a corresponding frozen model by 20% in Counter f-score.\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [\"sent3\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "use",
                    "diverse",
                    "beam",
                    "search",
                    "#REF",
                    "to",
                    "generate",
                    "16",
                    "candidates",
                    "for",
                    "each",
                    "data",
                    "sample."
                ],
                [
                    "On",
                    "CNNDM",
                    "and",
                    "XSum,",
                    "we",
                    "use",
                    "the",
                    "pre-trained",
                    "BART",
                    "12",
                    "and",
                    "PEGASUS",
                    "13",
                    "models",
                    "from",
                    "the",
                    "Transformers",
                    "#TARGET_REF",
                    "library",
                    "as",
                    "the",
                    "base",
                    "abstractive",
                    "models",
                    "for",
                    "candidate",
                    "summary",
                    "generation",
                    "and",
                    "model",
                    "finetuning",
                    "respectively."
                ],
                [
                    "On",
                    "NYT,",
                    "we",
                    "first",
                    "fine-tuned",
                    "a",
                    "BART",
                    "model",
                    "14",
                    "with",
                    "MLE",
                    "training",
                    "as",
                    "the",
                    "base",
                    "abstractive",
                    "model,",
                    "since",
                    "our",
                    "data",
                    "pre-processing",
                    "is",
                    "sightly",
                    "different",
                    "from",
                    "the",
                    "previous",
                    "work",
                    "and",
                    "there",
                    "are",
                    "no",
                    "available",
                    "pre-trained",
                    "checkpoints."
                ],
                [
                    "We",
                    "where",
                    "warmup",
                    "denotes",
                    "the",
                    "warmup",
                    "steps,",
                    "which",
                    "is",
                    "set",
                    "to",
                    "10000,",
                    "step",
                    "is",
                    "the",
                    "number",
                    "of",
                    "updating",
                    "steps,",
                    "lr",
                    "is",
                    "the",
                    "learning",
                    "rate."
                ],
                [
                    "We",
                    "set",
                    "the",
                    "length",
                    "penalty",
                    "factor",
                    "α",
                    "in",
                    "the",
                    "scoring",
                    "function",
                    "(Eq."
                ],
                [
                    "9)",
                    "to",
                    "the",
                    "same",
                    "value",
                    "as",
                    "used",
                    "in",
                    "the",
                    "original",
                    "beam",
                    "search."
                ],
                [
                    "We",
                    "search",
                    "the",
                    "value",
                    "of",
                    "the",
                    "margin",
                    "λ",
                    "in",
                    "the",
                    "contrastive",
                    "loss",
                    "(Eq."
                ],
                [
                    "8)",
                    "within",
                    "the",
                    "range",
                    "[1",
                    "×",
                    "10",
                    "−5",
                    ",",
                    "1],",
                    "and",
                    "decide",
                    "the",
                    "value",
                    "based",
                    "on",
                    "the",
                    "model",
                    "performance",
                    "on",
                    "the",
                    "validation",
                    "set."
                ],
                [
                    "We",
                    "also",
                    "performed",
                    "extensive",
                    "search",
                    "for",
                    "the",
                    "coefficient",
                    "γ",
                    "in",
                    "Eq."
                ],
                [
                    "10."
                ],
                [
                    "The",
                    "specific",
                    "hyper-parameter",
                    "setting",
                    "is",
                    "reported",
                    "in",
                    "Tab."
                ],
                [
                    "13."
                ]
            ],
            "context": [
                0,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: We use diverse beam search #REF to generate 16 candidates for each data sample.\n sent1: On CNNDM and XSum, we use the pre-trained BART 12 and PEGASUS 13 models from the Transformers #TARGET_REF library as the base abstractive models for candidate summary generation and model finetuning respectively.\n sent2: On NYT, we first fine-tuned a BART model 14 with MLE training as the base abstractive model, since our data pre-processing is sightly different from the previous work and there are no available pre-trained checkpoints.\n sent3: We where warmup denotes the warmup steps, which is set to 10000, step is the number of updating steps, lr is the learning rate.\n sent4: We set the length penalty factor α in the scoring function (Eq.\n sent5: 9) to the same value as used in the original beam search.\n sent6: We search the value of the margin λ in the contrastive loss (Eq.\n sent7: 8) within the range [1 × 10 −5 , 1], and decide the value based on the model performance on the validation set.\n sent8: We also performed extensive search for the coefficient γ in Eq.\n sent9: 10.\n sent10: The specific hyper-parameter setting is reported in Tab.\n sent11: 13.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Another",
                    "graph,",
                    "created",
                    "as",
                    "an",
                    "alternative",
                    "for",
                    "word",
                    "embeddings,",
                    "is",
                    "GraphGlove",
                    "#REF",
                    ",",
                    "where",
                    "the",
                    "edges",
                    "of",
                    "the",
                    "graph",
                    "are",
                    "optimized",
                    "by",
                    "the",
                    "cost",
                    "function",
                    "of",
                    "GloVe",
                    "#TARGET_REF",
                    ",",
                    "so",
                    "that",
                    "the",
                    "shortest",
                    "path",
                    "between",
                    "two",
                    "vertices",
                    "gives",
                    "the",
                    "distance",
                    "of",
                    "the",
                    "corresponding",
                    "words."
                ]
            ],
            "context": [
                2
            ]
        },
        "input": "sent0: Another graph, created as an alternative for word embeddings, is GraphGlove #REF , where the edges of the graph are optimized by the cost function of GloVe #TARGET_REF , so that the shortest path between two vertices gives the distance of the corresponding words.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "As",
                    "proposed",
                    "by",
                    "#TARGET_REF",
                    ",",
                    "adding",
                    "the",
                    "word",
                    "embedding",
                    "or",
                    "distributive",
                    "representation",
                    "for",
                    "the",
                    "candidate",
                    "strings",
                    "can",
                    "improve",
                    "the",
                    "performance",
                    "of",
                    "the",
                    "model."
                ],
                [
                    "The",
                    "embeddings",
                    "of",
                    "the",
                    "two",
                    "candidate",
                    "terms",
                    "in",
                    "the",
                    "entity-attribute",
                    "pair",
                    "are",
                    "concatenated",
                    "to",
                    "each",
                    "side",
                    "of",
                    "the",
                    "aggregated",
                    "sentences",
                    "vector",
                    "described",
                    "in",
                    "the",
                    "previous",
                    "section."
                ],
                [
                    "The",
                    "embeddings",
                    "for",
                    "e",
                    "and",
                    "a",
                    "are",
                    "simply",
                    "−",
                    "→",
                    "v",
                    "e",
                    "and",
                    "−",
                    "→",
                    "v",
                    "a",
                    "."
                ],
                [
                    "Thus",
                    "the",
                    "full",
                    "representation",
                    "of",
                    "a",
                    "entity-attribute",
                    "pair",
                    "is:−",
                    "→",
                    "v",
                    "(e,a)",
                    "=",
                    "[",
                    "−",
                    "→",
                    "v",
                    "e",
                    ",",
                    "−",
                    "→",
                    "v",
                    "sents(e,a)",
                    ",",
                    "−",
                    "→",
                    "v",
                    "a",
                    "]"
                ]
            ],
            "context": [
                1,
                1,
                3,
                3
            ]
        },
        "input": "sent0: As proposed by #TARGET_REF , adding the word embedding or distributive representation for the candidate strings can improve the performance of the model.\n sent1: The embeddings of the two candidate terms in the entity-attribute pair are concatenated to each side of the aggregated sentences vector described in the previous section.\n sent2: The embeddings for e and a are simply − → v e and − → v a .\n sent3: Thus the full representation of a entity-attribute pair is:− → v (e,a) = [ − → v e , − → v sents(e,a) , − → v a ]\n",
        "output": "{\"INFORMATION\": [\"sent0\", \"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "As",
                    "we",
                    "discussed",
                    "in",
                    "Section",
                    "4.1.2,",
                    "we",
                    "computed",
                    "the",
                    "annotation",
                    "structure",
                    "accuracy",
                    "(ASA),",
                    "and",
                    "it",
                    "turns",
                    "out",
                    "that",
                    "its",
                    "range",
                    "was",
                    "from",
                    "98.9",
                    "%",
                    "to",
                    "100.0",
                    "%."
                ],
                [
                    "This",
                    "means",
                    "that",
                    "the",
                    "proposed",
                    "joint",
                    "model",
                    "can",
                    "consistently",
                    "predict",
                    "transcriptions",
                    "and",
                    "the",
                    "linguistic",
                    "annotations",
                    "in",
                    "the",
                    "correct",
                    "order",
                    "almost",
                    "perfectly."
                ],
                [
                    "We",
                    "found",
                    "that",
                    "almost",
                    "all",
                    "errors",
                    "of",
                    "the",
                    "transition",
                    "occurred",
                    "in",
                    "the",
                    "last",
                    "word,",
                    "which",
                    "might",
                    "be",
                    "caused",
                    "by",
                    "beam",
                    "search",
                    "errors."
                ],
                [
                    "Pipeline",
                    "is",
                    "ASR",
                    "predicting",
                    "transcriptions",
                    "followed",
                    "by",
                    "the",
                    "NLP-based",
                    "linguistic",
                    "annotation",
                    "system",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Proposed",
                    "predicts",
                    "graphemes",
                    "and",
                    "phonemes",
                    "followed",
                    "by",
                    "POS",
                    "tags",
                    "from",
                    "speech."
                ],
                [
                    "Note",
                    "that",
                    "we",
                    "used",
                    "only",
                    "the",
                    "sentences",
                    "whose",
                    "hypothesized",
                    "ASR",
                    "transcript",
                    "is",
                    "predicted",
                    "correctly",
                    "for",
                    "evaluation."
                ]
            ],
            "context": [
                0,
                0,
                0,
                3,
                0,
                0
            ]
        },
        "input": "sent0: As we discussed in Section 4.1.2, we computed the annotation structure accuracy (ASA), and it turns out that its range was from 98.9 % to 100.0 %.\n sent1: This means that the proposed joint model can consistently predict transcriptions and the linguistic annotations in the correct order almost perfectly.\n sent2: We found that almost all errors of the transition occurred in the last word, which might be caused by beam search errors.\n sent3: Pipeline is ASR predicting transcriptions followed by the NLP-based linguistic annotation system #TARGET_REF .\n sent4: Proposed predicts graphemes and phonemes followed by POS tags from speech.\n sent5: Note that we used only the sentences whose hypothesized ASR transcript is predicted correctly for evaluation.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Several",
                    "robust",
                    "parsing",
                    "systems",
                    "exploit",
                    "the",
                    "comparative",
                    "success",
                    "of",
                    "part-of-speech",
                    "(PoS)",
                    "taggers,",
                    "such",
                    "as",
                    "Fidditch",
                    "#REF",
                    "or",
                    "#REF",
                    ",",
                    "by",
                    "reducing",
                    "the",
                    "input",
                    "to",
                    "a",
                    "determinate",
                    "sequence",
                    "of",
                    "extended",
                    "PoS",
                    "labels",
                    "of",
                    "the",
                    "type",
                    "which",
                    "can",
                    "be",
                    "practically",
                    "disambiguated",
                    "in",
                    "context",
                    "using",
                    "a",
                    "(H)MM",
                    "PoS",
                    "tagger",
                    "(e.g."
                ],
                [
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Such",
                    "approaches,",
                    "by",
                    "definition,",
                    "cannot",
                    "exploit",
                    "subcategorisation,",
                    "and",
                    "probably",
                    "achieve",
                    "some",
                    "of",
                    "their",
                    "robustness",
                    "as",
                    "a",
                    "result."
                ],
                [
                    "However,",
                    "such",
                    "parsers",
                    "typically",
                    "also",
                    "employ",
                    "heuristic",
                    "rules,",
                    "such",
                    "as",
                    "'low'",
                    "attachment",
                    "of",
                    "PPs",
                    "to",
                    "produce",
                    "unique",
                    "'canonical'",
                    "analyses."
                ],
                [
                    "This",
                    "latter",
                    "step",
                    "complicates",
                    "the",
                    "recovery",
                    "of",
                    "predicate",
                    "argument",
                    "structure",
                    "and",
                    "does",
                    "not",
                    "integrate",
                    "with",
                    "a",
                    "probabilistic",
                    "approach",
                    "to",
                    "parsing."
                ]
            ],
            "context": [
                2,
                1,
                3,
                0,
                3
            ]
        },
        "input": "sent0: Several robust parsing systems exploit the comparative success of part-of-speech (PoS) taggers, such as Fidditch #REF or #REF , by reducing the input to a determinate sequence of extended PoS labels of the type which can be practically disambiguated in context using a (H)MM PoS tagger (e.g.\n sent1: #TARGET_REF .\n sent2: Such approaches, by definition, cannot exploit subcategorisation, and probably achieve some of their robustness as a result.\n sent3: However, such parsers typically also employ heuristic rules, such as 'low' attachment of PPs to produce unique 'canonical' analyses.\n sent4: This latter step complicates the recovery of predicate argument structure and does not integrate with a probabilistic approach to parsing.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": [\"sent2\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "main",
                    "motivation",
                    "behind",
                    "the",
                    "TransWiC",
                    "architecture",
                    "is",
                    "the",
                    "success",
                    "transformer-based",
                    "architectures",
                    "had",
                    "in",
                    "various",
                    "natural",
                    "language",
                    "processing",
                    "tasks",
                    "like",
                    "offensive",
                    "language",
                    "identification",
                    "#TARGET_REF",
                    ",",
                    "offensive",
                    "spans",
                    "identification",
                    "#REF",
                    ",",
                    "language",
                    "detection",
                    "#REF",
                    "Lang."
                ]
            ],
            "context": [
                3
            ]
        },
        "input": "sent0: The main motivation behind the TransWiC architecture is the success transformer-based architectures had in various natural language processing tasks like offensive language identification #TARGET_REF , offensive spans identification #REF , language detection #REF Lang.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Dagan",
                    "and",
                    "Church",
                    "developed,",
                    "Termight,",
                    "a",
                    "tool",
                    "that",
                    "was",
                    "meant",
                    "assist",
                    "professional",
                    "translators",
                    "and",
                    "terminologists",
                    "develop",
                    "bilingual",
                    "term",
                    "lists",
                    "and",
                    "technical",
                    "terminology",
                    "in",
                    "particular",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Like",
                    "Kupiec's",
                    "work,",
                    "they",
                    "also",
                    "presume",
                    "the",
                    "availability",
                    "of",
                    "POS-tagging",
                    "and",
                    "work",
                    "with",
                    "noun",
                    "phrases",
                    "extracted",
                    "from",
                    "sentence-aligned",
                    "corpora."
                ],
                [
                    "A",
                    "distinctive",
                    "feature",
                    "of",
                    "their",
                    "approach",
                    "is",
                    "using",
                    "word-level",
                    "alignments",
                    "to",
                    "score",
                    "translations,",
                    "this",
                    "enables",
                    "identification",
                    "of",
                    "correct",
                    "translations",
                    "even",
                    "with",
                    "the",
                    "correct",
                    "source",
                    "term",
                    "/",
                    "target",
                    "term",
                    "correspondence",
                    "is",
                    "observed",
                    "once",
                    "or",
                    "twice",
                    "in",
                    "the",
                    "bilingual",
                    "data."
                ],
                [
                    "(This",
                    "scenario,",
                    "when",
                    "term",
                    "frequency",
                    "is",
                    "small,",
                    "makes",
                    "translation",
                    "using",
                    "contingency",
                    "table",
                    "methods",
                    "such",
                    "as",
                    "Dice",
                    "coefficients",
                    "problematic.)"
                ],
                [
                    "They",
                    "report",
                    "finding",
                    "a",
                    "correct",
                    "translation",
                    "at",
                    "rank",
                    "1",
                    "40%",
                    "of",
                    "the",
                    "time",
                    "and",
                    "at",
                    "rank",
                    "2",
                    "an",
                    "additional",
                    "7%",
                    "of",
                    "the",
                    "time",
                    "for",
                    "a",
                    "list",
                    "of",
                    "192",
                    "English/German",
                    "technical",
                    "terms."
                ]
            ],
            "context": [
                1,
                3,
                2,
                2,
                2
            ]
        },
        "input": "sent0: Dagan and Church developed, Termight, a tool that was meant assist professional translators and terminologists develop bilingual term lists and technical terminology in particular #TARGET_REF .\n sent1: Like Kupiec's work, they also presume the availability of POS-tagging and work with noun phrases extracted from sentence-aligned corpora.\n sent2: A distinctive feature of their approach is using word-level alignments to score translations, this enables identification of correct translations even with the correct source term / target term correspondence is observed once or twice in the bilingual data.\n sent3: (This scenario, when term frequency is small, makes translation using contingency table methods such as Dice coefficients problematic.)\n sent4: They report finding a correct translation at rank 1 40% of the time and at rank 2 an additional 7% of the time for a list of 192 English/German technical terms.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [\"sent2\", \"sent3\", \"sent4\"], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "organizers",
                    "of",
                    "IWSLT",
                    "provide",
                    "several",
                    "task",
                    "specific",
                    "corpora",
                    "that",
                    "can",
                    "be",
                    "used",
                    "to",
                    "train",
                    "and",
                    "optimize",
                    "the",
                    "translation",
                    "system."
                ],
                [
                    "The",
                    "characteristics",
                    "of",
                    "these",
                    "corpora",
                    "are",
                    "summarized",
                    "in",
                    "Table",
                    "1."
                ],
                [
                    "It",
                    "is",
                    "known",
                    "that",
                    "the",
                    "choice",
                    "of",
                    "the",
                    "development",
                    "and",
                    "internal",
                    "test",
                    "data",
                    "may",
                    "have",
                    "an",
                    "important",
                    "impact",
                    "on",
                    "the",
                    "quality",
                    "of",
                    "the",
                    "system,",
                    "in",
                    "particular",
                    "when",
                    "the",
                    "available",
                    "corpora",
                    "have",
                    "different",
                    "characteristics",
                    "(for",
                    "instance",
                    "what",
                    "concerns",
                    "the",
                    "average",
                    "sentence",
                    "length)."
                ],
                [
                    "We",
                    "decided",
                    "to",
                    "develop",
                    "."
                ],
                [
                    "Therefore,",
                    "we",
                    "decided",
                    "to",
                    "add",
                    "the",
                    "last",
                    "two",
                    "corpora",
                    "to",
                    "the",
                    "training",
                    "material",
                    "after",
                    "optimizing",
                    "the",
                    "system",
                    "and",
                    "to",
                    "retrain",
                    "the",
                    "full",
                    "system",
                    "keeping",
                    "all",
                    "settings",
                    "unmodified."
                ],
                [
                    "This",
                    "idea",
                    "was",
                    "already",
                    "successfully",
                    "proposed",
                    "in",
                    "previous",
                    "IWSLT",
                    "evaluations",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                2,
                2
            ]
        },
        "input": "sent0: The organizers of IWSLT provide several task specific corpora that can be used to train and optimize the translation system.\n sent1: The characteristics of these corpora are summarized in Table 1.\n sent2: It is known that the choice of the development and internal test data may have an important impact on the quality of the system, in particular when the available corpora have different characteristics (for instance what concerns the average sentence length).\n sent3: We decided to develop .\n sent4: Therefore, we decided to add the last two corpora to the training material after optimizing the system and to retrain the full system keeping all settings unmodified.\n sent5: This idea was already successfully proposed in previous IWSLT evaluations #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent4\", \"sent5\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "To",
                    "model",
                    "such",
                    "indirect",
                    "connections,",
                    "we",
                    "multiply",
                    "the",
                    "relatedness",
                    "matrix",
                    "by",
                    "itself,",
                    "and",
                    "use",
                    "the",
                    "values",
                    "of",
                    "the",
                    "squared",
                    "matrix",
                    "S",
                    "′",
                    "as",
                    "the",
                    "relatedness",
                    "measure",
                    "between",
                    "two",
                    "words."
                ],
                [
                    "By",
                    "the",
                    "definition",
                    "of",
                    "matrix",
                    "multiplication,S",
                    "′",
                    "ij",
                    "=",
                    "n",
                    "k=1",
                    "s",
                    "i,k",
                    "•",
                    "s",
                    "k,j",
                    ",that",
                    "is,",
                    "if",
                    "we",
                    "define",
                    "G",
                    "0",
                    "as",
                    "a",
                    "graph",
                    "whose",
                    "neighborhood",
                    "matrix",
                    "is",
                    "the",
                    "NPMI",
                    "matrix",
                    "then",
                    "S",
                    "′",
                    "ij",
                    "is",
                    "the",
                    "sum",
                    "of",
                    "the",
                    "product",
                    "of",
                    "the",
                    "weights",
                    "on",
                    "all",
                    "two-length",
                    "paths",
                    "v",
                    "i",
                    "−",
                    "v",
                    "k",
                    "−",
                    "v",
                    "j",
                    "in",
                    "G",
                    "0",
                    "."
                ],
                [
                    "Since",
                    "all",
                    "edge",
                    "weights",
                    "are",
                    "between",
                    "0",
                    "and",
                    "1,",
                    "considering",
                    "the",
                    "weight",
                    "of",
                    "a",
                    "path",
                    "as",
                    "the",
                    "product",
                    "of",
                    "its",
                    "edge",
                    "weights",
                    "gives",
                    "a",
                    "valid",
                    "relatedness",
                    "measure:",
                    "longer",
                    "paths",
                    "and",
                    "paths",
                    "that",
                    "contain",
                    "smaller",
                    "weights",
                    "will",
                    "yield",
                    "to",
                    "smaller",
                    "relatedness",
                    "values."
                ],
                [
                    "#TARGET_REF",
                    "also",
                    "showed",
                    "on",
                    "word",
                    "embeddings,",
                    "that",
                    "different",
                    "powers",
                    "of",
                    "embedding",
                    "matrices",
                    "are",
                    "beneficial",
                    "for",
                    "word",
                    "similarity",
                    "and",
                    "word",
                    "relatedness",
                    "tasks,",
                    "and",
                    "that",
                    "the",
                    "optimal",
                    "power",
                    "is",
                    "higher",
                    "for",
                    "relatedness",
                    "than",
                    "for",
                    "similarity."
                ]
            ],
            "context": [
                0,
                0,
                0,
                1
            ]
        },
        "input": "sent0: To model such indirect connections, we multiply the relatedness matrix by itself, and use the values of the squared matrix S ′ as the relatedness measure between two words.\n sent1: By the definition of matrix multiplication,S ′ ij = n k=1 s i,k • s k,j ,that is, if we define G 0 as a graph whose neighborhood matrix is the NPMI matrix then S ′ ij is the sum of the product of the weights on all two-length paths v i − v k − v j in G 0 .\n sent2: Since all edge weights are between 0 and 1, considering the weight of a path as the product of its edge weights gives a valid relatedness measure: longer paths and paths that contain smaller weights will yield to smaller relatedness values.\n sent3: #TARGET_REF also showed on word embeddings, that different powers of embedding matrices are beneficial for word similarity and word relatedness tasks, and that the optimal power is higher for relatedness than for similarity.\n",
        "output": "{\"INFORMATION\": [\"sent3\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Another",
                    "straightforward",
                    "operation",
                    "is",
                    "to",
                    "sum",
                    "the",
                    "embeddings",
                    "(technique",
                    "used",
                    "in",
                    "#TARGET_REF",
                    ")",
                    "of",
                    "the",
                    "previous",
                    "lemma",
                    "with",
                    "the",
                    "embedding",
                    "of",
                    "the",
                    "previous",
                    "factors,",
                    "as",
                    "described",
                    "in",
                    "equation",
                    "7."
                ]
            ],
            "context": [
                2
            ]
        },
        "input": "sent0: Another straightforward operation is to sum the embeddings (technique used in #TARGET_REF ) of the previous lemma with the embedding of the previous factors, as described in equation 7.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "connection",
                    "of",
                    "all",
                    "this",
                    "to",
                    "ideograms",
                    "had",
                    "been",
                    "noted",
                    "by",
                    "Richards,",
                    "who",
                    "was",
                    "much",
                    "preoccupied",
                    "by",
                    "Chinese,",
                    "and",
                    "who",
                    "developed",
                    "English",
                    "through",
                    "pictures",
                    "#TARGET_REF",
                    ",",
                    "a",
                    "highly",
                    "successful",
                    "language",
                    "teaching",
                    "tool."
                ],
                [
                    "MMB",
                    "came",
                    "to",
                    "Chinese",
                    "through",
                    "Michael",
                    "Halliday,",
                    "then",
                    "a",
                    "Cambridge",
                    "University",
                    "lecturer",
                    "in",
                    "Chinese,",
                    "and",
                    "began",
                    "to",
                    "use",
                    "stick-pictures",
                    "as",
                    "representations",
                    "of",
                    "situations",
                    "but",
                    "which",
                    "could",
                    "also",
                    "provide",
                    "a",
                    "plausible",
                    "referential",
                    "underpinning",
                    "for",
                    "language:",
                    "something",
                    "universal,",
                    "and",
                    "outside",
                    "the",
                    "world",
                    "of",
                    "the",
                    "language",
                    "signs",
                    "themselves,",
                    "yet",
                    "which",
                    "did",
                    "not",
                    "fall",
                    "back",
                    "on",
                    "the",
                    "naive",
                    "referentialism",
                    "of",
                    "those",
                    "who",
                    "said",
                    "that",
                    "the",
                    "meanings",
                    "of",
                    "words",
                    "were",
                    "things",
                    "or",
                    "inexpressible",
                    "concepts."
                ]
            ],
            "context": [
                2,
                0
            ]
        },
        "input": "sent0: The connection of all this to ideograms had been noted by Richards, who was much preoccupied by Chinese, and who developed English through pictures #TARGET_REF , a highly successful language teaching tool.\n sent1: MMB came to Chinese through Michael Halliday, then a Cambridge University lecturer in Chinese, and began to use stick-pictures as representations of situations but which could also provide a plausible referential underpinning for language: something universal, and outside the world of the language signs themselves, yet which did not fall back on the naive referentialism of those who said that the meanings of words were things or inexpressible concepts.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Similar",
                    "to",
                    "other",
                    "NLP",
                    "applications,",
                    "the",
                    "most",
                    "recent",
                    "attention",
                    "in",
                    "this",
                    "area",
                    "has",
                    "been",
                    "on",
                    "neural",
                    "approaches."
                ],
                [
                    "#TARGET_REF",
                    "demonstrated",
                    "an",
                    "effective",
                    "way",
                    "of",
                    "applying",
                    "a",
                    "Bi-LSTM",
                    "to",
                    "the",
                    "POS",
                    "tagging",
                    "task,",
                    "achieving",
                    "97.4%",
                    "on",
                    "the",
                    "English",
                    "Penn",
                    "Treebank."
                ],
                [
                    "#REF",
                    "used",
                    "a",
                    "Bi-LSTM",
                    "in",
                    "their",
                    "work",
                    "on",
                    "Arabic",
                    "POS",
                    "tagging,",
                    "achieving",
                    "95.50%."
                ],
                [
                    "#REF",
                    "used",
                    "the",
                    "LSTM-RNN",
                    "model",
                    "on",
                    "the",
                    "Quranic",
                    "Arabic",
                    "Corpus",
                    "(QAC)."
                ],
                [
                    "They",
                    "reported",
                    "accuracy",
                    "of",
                    "99.76%",
                    "at",
                    "the",
                    "word",
                    "level",
                    "and",
                    "99.18%",
                    "at",
                    "the",
                    "morpheme",
                    "level."
                ],
                [
                    "They",
                    "also",
                    "compared",
                    "their",
                    "system",
                    "against",
                    "the",
                    "Word2Vec",
                    "POS",
                    "tagger,",
                    "for",
                    "which",
                    "they",
                    "reported",
                    "accuracy",
                    "levels",
                    "of",
                    "97.33%",
                    "and",
                    "99.55%",
                    "for",
                    "words",
                    "and",
                    "morphemes",
                    "respectively."
                ]
            ],
            "context": [
                3,
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Similar to other NLP applications, the most recent attention in this area has been on neural approaches.\n sent1: #TARGET_REF demonstrated an effective way of applying a Bi-LSTM to the POS tagging task, achieving 97.4% on the English Penn Treebank.\n sent2: #REF used a Bi-LSTM in their work on Arabic POS tagging, achieving 95.50%.\n sent3: #REF used the LSTM-RNN model on the Quranic Arabic Corpus (QAC).\n sent4: They reported accuracy of 99.76% at the word level and 99.18% at the morpheme level.\n sent5: They also compared their system against the Word2Vec POS tagger, for which they reported accuracy levels of 97.33% and 99.55% for words and morphemes respectively.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "the",
                    "languages",
                    "of",
                    "around",
                    "35",
                    "ethnolinguistic",
                    "groups,",
                    "including",
                    "the",
                    "Irula",
                    "and",
                    "Yerukula",
                    "languages",
                    "#REF",
                    "."
                ],
                [
                    "Malayalam",
                    "is",
                    "Tamil's",
                    "closest",
                    "significant",
                    "cousin,",
                    "the",
                    "two",
                    "began",
                    "splitting",
                    "during",
                    "the",
                    "9th",
                    "century",
                    "AD."
                ],
                [
                    "Although",
                    "several",
                    "variations",
                    "between",
                    "Tamil",
                    "and",
                    "Malayalam",
                    "indicate",
                    "a",
                    "pre-historic",
                    "break",
                    "of",
                    "the",
                    "western",
                    "dialect,",
                    "the",
                    "process",
                    "of",
                    "separating",
                    "into",
                    "a",
                    "different",
                    "language,",
                    "Malayalam,",
                    "did",
                    "not",
                    "occur",
                    "until",
                    "the",
                    "13th",
                    "or",
                    "14th",
                    "century",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Even",
                    "state-of-the-art",
                    "multilingual",
                    "NLP",
                    "systems",
                    "perform",
                    "sub-optimally",
                    "on",
                    "Dravidian",
                    "languages",
                    "#REF",
                    "."
                ],
                [
                    "This",
                    "can",
                    "be",
                    "explained",
                    "by",
                    "the",
                    "fact",
                    "that",
                    "multilingual",
                    "language",
                    "models",
                    "are",
                    "often",
                    "jointly",
                    "trained",
                    "on",
                    "100+",
                    "languages",
                    "and",
                    "Indian",
                    "languages",
                    "constitute",
                    "only",
                    "a",
                    "small",
                    "fraction",
                    "of",
                    "their",
                    "vocabulary",
                    "and",
                    "training",
                    "data",
                    "(as",
                    "shown",
                    "in",
                    "Figure",
                    "2)."
                ],
                [
                    "Machine",
                    "learning",
                    "models",
                    "and",
                    "tools",
                    "have",
                    "been",
                    "proposed",
                    "for",
                    "many",
                    "Natural",
                    "Language",
                    "Understanding",
                    "tasks."
                ],
                [
                    "In",
                    "this",
                    "work,",
                    "we",
                    "focus",
                    "on",
                    "Extractive",
                    "Question-Answering",
                    "(QA),",
                    "where",
                    "the",
                    "goal",
                    "is",
                    "to",
                    "localize",
                    "the",
                    "answer",
                    "to",
                    "a",
                    "question",
                    "within",
                    "a",
                    "large",
                    "context",
                    "(see",
                    "Figure",
                    "1)."
                ],
                [
                    "Specifically,",
                    "we",
                    "aim",
                    "to",
                    "develop",
                    "a",
                    "common",
                    "multilingual",
                    "question",
                    "answering",
                    "model",
                    "for",
                    "multiple",
                    "Indian",
                    "languages."
                ],
                [
                    "A",
                    "multilingual",
                    "model",
                    "has",
                    "several",
                    "advantages:",
                    "(1)",
                    "learning",
                    "of",
                    "cues",
                    "across",
                    "different",
                    "languages,",
                    "(2)",
                    "a",
                    "single",
                    "model",
                    "for",
                    "many",
                    "languages,",
                    "and",
                    "(3)",
                    "avoiding",
                    "dependency",
                    "on",
                    "English",
                    "translation",
                    "during",
                    "inference."
                ],
                [
                    "In",
                    "this",
                    "work,",
                    "we",
                    "start",
                    "with",
                    "a",
                    "pre-trained",
                    "multilingual",
                    "Bidi-",
                    "rectional",
                    "Encoder",
                    "Representations",
                    "from",
                    "Transformers",
                    "(mBERT)",
                    "model",
                    "and",
                    "further",
                    "pre-train",
                    "it",
                    "with",
                    "SQuAD",
                    "#REF",
                    ",",
                    "a",
                    "large-scale",
                    "question",
                    "answering",
                    "dataset",
                    "in",
                    "English."
                ],
                [
                    "The",
                    "resulting",
                    "English-language",
                    "mBERT-QA",
                    "model",
                    "is",
                    "fine-tuned",
                    "and",
                    "evaluated",
                    "for",
                    "Indian",
                    "languages",
                    "Tamil",
                    "and",
                    "Hindi",
                    "using",
                    "the",
                    "ChAII",
                    "dataset",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                0,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: the languages of around 35 ethnolinguistic groups, including the Irula and Yerukula languages #REF .\n sent1: Malayalam is Tamil's closest significant cousin, the two began splitting during the 9th century AD.\n sent2: Although several variations between Tamil and Malayalam indicate a pre-historic break of the western dialect, the process of separating into a different language, Malayalam, did not occur until the 13th or 14th century #TARGET_REF .\n sent3: Even state-of-the-art multilingual NLP systems perform sub-optimally on Dravidian languages #REF .\n sent4: This can be explained by the fact that multilingual language models are often jointly trained on 100+ languages and Indian languages constitute only a small fraction of their vocabulary and training data (as shown in Figure 2).\n sent5: Machine learning models and tools have been proposed for many Natural Language Understanding tasks.\n sent6: In this work, we focus on Extractive Question-Answering (QA), where the goal is to localize the answer to a question within a large context (see Figure 1).\n sent7: Specifically, we aim to develop a common multilingual question answering model for multiple Indian languages.\n sent8: A multilingual model has several advantages: (1) learning of cues across different languages, (2) a single model for many languages, and (3) avoiding dependency on English translation during inference.\n sent9: In this work, we start with a pre-trained multilingual Bidi- rectional Encoder Representations from Transformers (mBERT) model and further pre-train it with SQuAD #REF , a large-scale question answering dataset in English.\n sent10: The resulting English-language mBERT-QA model is fine-tuned and evaluated for Indian languages Tamil and Hindi using the ChAII dataset #REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Reversely,",
                    "back-translation",
                    "#TARGET_REF",
                    "first",
                    "trains",
                    "a",
                    "target-tosource",
                    "model,",
                    "which",
                    "then",
                    "utilizes",
                    "target-side",
                    "monolingual",
                    "data",
                    "to",
                    "synthesis",
                    "a",
                    "pseudo-parallel",
                    "corpus."
                ],
                [
                    "We",
                    "randomly",
                    "select",
                    "2M",
                    "English",
                    "sentences",
                    "from",
                    "the",
                    "CWMT",
                    "parallel",
                    "corpus",
                    "for",
                    "back-translation."
                ]
            ],
            "context": [
                1,
                2
            ]
        },
        "input": "sent0: Reversely, back-translation #TARGET_REF first trains a target-tosource model, which then utilizes target-side monolingual data to synthesis a pseudo-parallel corpus.\n sent1: We randomly select 2M English sentences from the CWMT parallel corpus for back-translation.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "All",
                    "baseline",
                    "methods",
                    "were",
                    "built",
                    "on",
                    "the",
                    "Transformer",
                    "architecture",
                    "with",
                    "6-layer",
                    "encoder",
                    "and",
                    "decoder,",
                    "and",
                    "initialized",
                    "with",
                    "pre-trained",
                    "parameters",
                    "from",
                    "BARTbase",
                    "#REF",
                    ",",
                    "which",
                    "is",
                    "one",
                    "of",
                    "the",
                    "stateof-the-art",
                    "pre-trained",
                    "Transformer",
                    "models",
                    "for",
                    "natural",
                    "language",
                    "generation",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "In",
                    "our",
                    "MoKGE,",
                    "the",
                    "Transformer",
                    "parameters",
                    "were",
                    "also",
                    "initialized",
                    "by",
                    "BART-base,",
                    "in",
                    "order",
                    "to",
                    "make",
                    "fair",
                    "comparison",
                    "with",
                    "all",
                    "baseline",
                    "methods."
                ],
                [
                    "The",
                    "R-GCN",
                    "parameters",
                    "were",
                    "random",
                    "initialized."
                ]
            ],
            "context": [
                2,
                0,
                0
            ]
        },
        "input": "sent0: All baseline methods were built on the Transformer architecture with 6-layer encoder and decoder, and initialized with pre-trained parameters from BARTbase #REF , which is one of the stateof-the-art pre-trained Transformer models for natural language generation #TARGET_REF .\n sent1: In our MoKGE, the Transformer parameters were also initialized by BART-base, in order to make fair comparison with all baseline methods.\n sent2: The R-GCN parameters were random initialized.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "Figure",
                    "4,",
                    "we",
                    "compare",
                    "our",
                    "dataset",
                    "with",
                    "other",
                    "toxicity",
                    "detection",
                    "datasets",
                    "using",
                    "the",
                    "metric",
                    "of",
                    "relative",
                    "frequency",
                    "of",
                    "toxic",
                    "utterances",
                    "of",
                    "each",
                    "length."
                ],
                [
                    "The",
                    "datasets",
                    "we",
                    "compare",
                    "with",
                    "are",
                    "1)",
                    "Waseem",
                    "#REF",
                    "which",
                    "consists",
                    "of",
                    "16.2k",
                    "tweets",
                    "binary",
                    "classified",
                    "as",
                    "racism/sexism",
                    "or",
                    "other,",
                    "2)",
                    "FoxNews",
                    "#TARGET_REF",
                    "which",
                    "is",
                    "1.5k",
                    "sentences",
                    "from",
                    "Fox",
                    "News",
                    "discussion",
                    "threads",
                    "classified",
                    "as",
                    "hateful/non-",
                    "The",
                    "distribution",
                    "in",
                    "CONDA",
                    "is",
                    "different",
                    "to",
                    "the",
                    "other",
                    "datasets",
                    "in",
                    "that",
                    "the",
                    "toxic",
                    "utterances",
                    "are",
                    "shorter."
                ],
                [
                    "This",
                    "is",
                    "due",
                    "to",
                    "the",
                    "terseness",
                    "of",
                    "in-game",
                    "chat",
                    "during",
                    "playing,",
                    "with",
                    "longer",
                    "utterances",
                    "occurring",
                    "in",
                    "pregame",
                    "and",
                    "post-game",
                    "discussion."
                ],
                [
                    "Waseem",
                    "has",
                    "a",
                    "particular",
                    "distribution",
                    "due",
                    "to",
                    "the",
                    "character",
                    "limit",
                    "in",
                    "Twitter",
                    "(140",
                    "characters",
                    "at",
                    "the",
                    "time)."
                ],
                [
                    "FoxNews",
                    "and",
                    "StormfrontWS",
                    "are",
                    "forums",
                    "which",
                    "foster",
                    "the",
                    "use",
                    "of",
                    "longer",
                    "sentences."
                ]
            ],
            "context": [
                3,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: In Figure 4, we compare our dataset with other toxicity detection datasets using the metric of relative frequency of toxic utterances of each length.\n sent1: The datasets we compare with are 1) Waseem #REF which consists of 16.2k tweets binary classified as racism/sexism or other, 2) FoxNews #TARGET_REF which is 1.5k sentences from Fox News discussion threads classified as hateful/non- The distribution in CONDA is different to the other datasets in that the toxic utterances are shorter.\n sent2: This is due to the terseness of in-game chat during playing, with longer utterances occurring in pregame and post-game discussion.\n sent3: Waseem has a particular distribution due to the character limit in Twitter (140 characters at the time).\n sent4: FoxNews and StormfrontWS are forums which foster the use of longer sentences.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "one",
                    "for",
                    "the",
                    "encoder",
                    "and",
                    "the",
                    "other",
                    "for",
                    "the",
                    "decoder."
                ],
                [
                    "The",
                    "encoder",
                    "maps",
                    "a",
                    "source",
                    "sequence",
                    "into",
                    "a",
                    "sequence",
                    "of",
                    "continuous",
                    "space",
                    "vectors",
                    "and",
                    "the",
                    "decoder",
                    "maps",
                    "this",
                    "representation",
                    "back",
                    "to",
                    "a",
                    "target",
                    "sequence."
                ],
                [
                    "Our",
                    "trained",
                    "neural",
                    "translation",
                    "models",
                    "are",
                    "based",
                    "on",
                    "an",
                    "encoder-decoder",
                    "deep",
                    "neural",
                    "network,",
                    "equipped",
                    "with",
                    "an",
                    "attention",
                    "mechanism",
                    "#TARGET_REF",
                    ",",
                    "as",
                    "described",
                    "in",
                    "Figure",
                    "2."
                ],
                [
                    "This",
                    "architecture",
                    "consists",
                    "of",
                    "a",
                    "bidirectional",
                    "RNN",
                    "encoder",
                    "(as",
                    "seen",
                    "in",
                    "stage",
                    "1",
                    "of",
                    "Figure",
                    "2)."
                ],
                [
                    "An",
                    "input",
                    "sentence",
                    "is",
                    "encoded",
                    "in",
                    "a",
                    "sequence",
                    "of",
                    "annotations",
                    "(one",
                    "for",
                    "each",
                    "input",
                    "word),",
                    "corresponding",
                    "to",
                    "the",
                    "concatenation",
                    "of",
                    "the",
                    "outputs",
                    "of",
                    "a",
                    "forward",
                    "and",
                    "a",
                    "backward",
                    "RNN."
                ],
                [
                    "Each",
                    "annotation",
                    "represents",
                    "the",
                    "full",
                    "sentence",
                    "with",
                    "a",
                    "strong",
                    "focus",
                    "on",
                    "the",
                    "current",
                    "word."
                ],
                [
                    "The",
                    "decoder",
                    "is",
                    "composed",
                    "of",
                    "a",
                    "conditional",
                    "RNN",
                    "as",
                    "provided",
                    "for",
                    "the",
                    "DL4MT",
                    "winter",
                    "school",
                    "1",
                    "(see",
                    "stage",
                    "3",
                    "of",
                    "Figure",
                    "2),",
                    "equipped",
                    "with",
                    "an",
                    "attention",
                    "mechanism",
                    "(stage",
                    "2)."
                ],
                [
                    "The",
                    "attention",
                    "mechanism",
                    "aims",
                    "at",
                    "providing",
                    "weights",
                    "for",
                    "each",
                    "annotation",
                    "in",
                    "order",
                    "to",
                    "generate",
                    "a",
                    "context",
                    "vector",
                    "(by",
                    "performing",
                    "a",
                    "weighted",
                    "sum",
                    "over",
                    "the",
                    "annotations)."
                ],
                [
                    "The",
                    "attention",
                    "mechanism",
                    "uses",
                    "the",
                    "hidden",
                    "state",
                    "at",
                    "timestep",
                    "j",
                    "of",
                    "the",
                    "decoder",
                    "RNN",
                    "along",
                    "with",
                    "the",
                    "annotation",
                    "h",
                    "i",
                    "to",
                    "generate",
                    "a",
                    "coefficient",
                    "e",
                    "ij",
                    "."
                ],
                [
                    "A",
                    "softmax",
                    "operation",
                    "is",
                    "performed",
                    "over",
                    "those",
                    "coefficients",
                    "to",
                    "generate",
                    "the",
                    "annotation",
                    "weights",
                    "α",
                    "ij",
                    "."
                ],
                [
                    "As",
                    "described",
                    "in",
                    "#REF",
                    ",",
                    "the",
                    "annotation",
                    "weights",
                    "can",
                    "be",
                    "used",
                    "to",
                    "align",
                    "the",
                    "input",
                    "words",
                    "to",
                    "the",
                    "output",
                    "words."
                ],
                [
                    "The",
                    "RNN",
                    "takes",
                    "as",
                    "input",
                    "the",
                    "context",
                    "vector,",
                    "the",
                    "embedding",
                    "of",
                    "the",
                    "previous",
                    "output",
                    "word",
                    "(stage",
                    "4",
                    "of",
                    "Figure",
                    "2),",
                    "and",
                    "of",
                    "course,",
                    "its",
                    "hidden",
                    "state."
                ],
                [
                    "Finally,",
                    "on",
                    "stage",
                    "5",
                    "of",
                    "the",
                    "Figure",
                    "2,",
                    "the",
                    "output",
                    "probabilities",
                    "of",
                    "the",
                    "target",
                    "vocabulary",
                    "are",
                    "computed."
                ],
                [
                    "The",
                    "word",
                    "with",
                    "the",
                    "highest",
                    "probability",
                    "is",
                    "selected",
                    "to",
                    "be",
                    "the",
                    "translation",
                    "output",
                    "at",
                    "each",
                    "timestep."
                ],
                [
                    "The",
                    "encoder",
                    "and",
                    "the",
                    "decoder",
                    "are",
                    "trained",
                    "jointly",
                    "to",
                    "maximize",
                    "the",
                    "conditional",
                    "probability",
                    "of",
                    "the",
                    "correct",
                    "translation."
                ]
            ],
            "context": [
                3,
                0,
                1,
                1,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                3,
                3,
                3,
                2
            ]
        },
        "input": "sent0: one for the encoder and the other for the decoder.\n sent1: The encoder maps a source sequence into a sequence of continuous space vectors and the decoder maps this representation back to a target sequence.\n sent2: Our trained neural translation models are based on an encoder-decoder deep neural network, equipped with an attention mechanism #TARGET_REF , as described in Figure 2.\n sent3: This architecture consists of a bidirectional RNN encoder (as seen in stage 1 of Figure 2).\n sent4: An input sentence is encoded in a sequence of annotations (one for each input word), corresponding to the concatenation of the outputs of a forward and a backward RNN.\n sent5: Each annotation represents the full sentence with a strong focus on the current word.\n sent6: The decoder is composed of a conditional RNN as provided for the DL4MT winter school 1 (see stage 3 of Figure 2), equipped with an attention mechanism (stage 2).\n sent7: The attention mechanism aims at providing weights for each annotation in order to generate a context vector (by performing a weighted sum over the annotations).\n sent8: The attention mechanism uses the hidden state at timestep j of the decoder RNN along with the annotation h i to generate a coefficient e ij .\n sent9: A softmax operation is performed over those coefficients to generate the annotation weights α ij .\n sent10: As described in #REF , the annotation weights can be used to align the input words to the output words.\n sent11: The RNN takes as input the context vector, the embedding of the previous output word (stage 4 of Figure 2), and of course, its hidden state.\n sent12: Finally, on stage 5 of the Figure 2, the output probabilities of the target vocabulary are computed.\n sent13: The word with the highest probability is selected to be the translation output at each timestep.\n sent14: The encoder and the decoder are trained jointly to maximize the conditional probability of the correct translation.\n",
        "output": "{\"INFORMATION\": [\"sent2\", \"sent3\"], \"PERCEPTION\": [\"sent14\"], \"BACKGROUND\": [\"sent0\", \"sent4\", \"sent5\", \"sent6\", \"sent7\", \"sent8\", \"sent9\", \"sent11\", \"sent12\", \"sent13\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "•",
                    "Slot-gated",
                    "#TARGET_REF",
                    ")",
                    "is",
                    "an",
                    "attentionbased",
                    "BiLSTM",
                    "model",
                    "which",
                    "builds",
                    "on",
                    "sepa-",
                    "rate",
                    "attended",
                    "context",
                    "for",
                    "slot",
                    "filling",
                    "and",
                    "intent",
                    "classification",
                    "while",
                    "explicitly",
                    "feeding",
                    "the",
                    "intent",
                    "context",
                    "into",
                    "the",
                    "process",
                    "of",
                    "slot",
                    "filling",
                    "via",
                    "a",
                    "gating",
                    "mechanism.Metrics",
                    "Model",
                    "UCA",
                    "U-F1(E)",
                    "U-F1(I)",
                    "U-F1(A)",
                    "U-F1(O)",
                    "T-F1",
                    "T-F1(T)",
                    "T-F1(S)",
                    "T-F1(C)",
                    "T-F1(D)",
                    "T-F1(P)",
                    "T-F1(O)",
                    "JSA",
                    "RNN-NLU",
                    "(Liu•",
                    "Inter-BiLSTM",
                    "#REF",
                    "combines",
                    "two",
                    "inter-connected",
                    "BiLSTMs",
                    "performing",
                    "slot",
                    "filling",
                    "and",
                    "intent",
                    "classification",
                    "respectively."
                ],
                [
                    "The",
                    "information",
                    "flow",
                    "between",
                    "the",
                    "two",
                    "tasks",
                    "occurs",
                    "by",
                    "passing",
                    "the",
                    "hidden",
                    "states",
                    "at",
                    "each",
                    "time",
                    "step",
                    "from",
                    "each",
                    "side",
                    "to",
                    "the",
                    "other",
                    "to",
                    "support",
                    "the",
                    "decoding",
                    "process."
                ]
            ],
            "context": [
                1,
                0
            ]
        },
        "input": "sent0: • Slot-gated #TARGET_REF ) is an attentionbased BiLSTM model which builds on sepa- rate attended context for slot filling and intent classification while explicitly feeding the intent context into the process of slot filling via a gating mechanism.Metrics Model UCA U-F1(E) U-F1(I) U-F1(A) U-F1(O) T-F1 T-F1(T) T-F1(S) T-F1(C) T-F1(D) T-F1(P) T-F1(O) JSA RNN-NLU (Liu• Inter-BiLSTM #REF combines two inter-connected BiLSTMs performing slot filling and intent classification respectively.\n sent1: The information flow between the two tasks occurs by passing the hidden states at each time step from each side to the other to support the decoding process.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Table",
                    "3",
                    "shows",
                    "the",
                    "query",
                    "latency",
                    "breakdown",
                    "for",
                    "a",
                    "few",
                    "representative",
                    "models."
                ],
                [
                    "Note",
                    "that",
                    "latency",
                    "is",
                    "dominated",
                    "by",
                    "final-stage",
                    "neural",
                    "reranking",
                    "latency,",
                    "which",
                    "scales",
                    "linearly,",
                    "so",
                    "smaller",
                    "N",
                    "values",
                    "(in",
                    "Table",
                    "2)",
                    "are",
                    "more",
                    "desirable."
                ],
                [
                    "However,",
                    "this",
                    "is",
                    "balanced",
                    "by",
                    "the",
                    "introduction",
                    "of",
                    "LTR",
                    "overhead,",
                    "both",
                    "feature",
                    "extraction",
                    "as",
                    "well",
                    "as",
                    "the",
                    "prediction",
                    "latency",
                    "itself."
                ],
                [
                    "Nevertheless,",
                    "this",
                    "is",
                    "a",
                    "worthwhile",
                    "tradeoff",
                    "as",
                    "we",
                    "observe",
                    "large",
                    "speedups",
                    "overall."
                ],
                [
                    "Since",
                    "T5-base",
                    "is",
                    "faster",
                    "than",
                    "BERT-large,",
                    "the",
                    "effect",
                    "of",
                    "the",
                    "LTR",
                    "overhead",
                    "is",
                    "relatively",
                    "larger",
                    "and",
                    "thus",
                    "the",
                    "speedup",
                    "is",
                    "lower."
                ],
                [
                    "We",
                    "can",
                    "see",
                    "that",
                    "increasing",
                    "the",
                    "initial",
                    "k",
                    "0",
                    "for",
                    "BoW",
                    "from",
                    "1k",
                    "to",
                    "10k",
                    "is",
                    "acceptable",
                    "as",
                    "LTR",
                    "overhead",
                    "remains",
                    "modest."
                ],
                [
                    "dex",
                    "for",
                    "each",
                    "document",
                    "is",
                    "the",
                    "most",
                    "expensive",
                    "single",
                    "step."
                ],
                [
                    "Our",
                    "experiments",
                    "show",
                    "that",
                    "the",
                    "number",
                    "of",
                    "features",
                    "does",
                    "not",
                    "affect",
                    "latency",
                    "substantially",
                    "because",
                    "we",
                    "only",
                    "need",
                    "to",
                    "load",
                    "the",
                    "document",
                    "once,",
                    "once",
                    "in",
                    "cache,",
                    "individual",
                    "feature",
                    "extraction",
                    "is",
                    "very",
                    "fast."
                ],
                [
                    "Note",
                    "that",
                    "we",
                    "have",
                    "not",
                    "spent",
                    "much",
                    "effort",
                    "optimizing",
                    "feature",
                    "extraction",
                    "(which",
                    "is",
                    "relatively",
                    "inefficient",
                    "Java",
                    "code)",
                    "and",
                    "that",
                    "more",
                    "engineering",
                    "effort,",
                    "for",
                    "example,",
                    "optimizations",
                    "proposed",
                    "by",
                    "#TARGET_REF",
                    ",",
                    "are",
                    "likely",
                    "to",
                    "further",
                    "increase",
                    "speedups."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2
            ]
        },
        "input": "sent0: Table 3 shows the query latency breakdown for a few representative models.\n sent1: Note that latency is dominated by final-stage neural reranking latency, which scales linearly, so smaller N values (in Table 2) are more desirable.\n sent2: However, this is balanced by the introduction of LTR overhead, both feature extraction as well as the prediction latency itself.\n sent3: Nevertheless, this is a worthwhile tradeoff as we observe large speedups overall.\n sent4: Since T5-base is faster than BERT-large, the effect of the LTR overhead is relatively larger and thus the speedup is lower.\n sent5: We can see that increasing the initial k 0 for BoW from 1k to 10k is acceptable as LTR overhead remains modest.\n sent6: dex for each document is the most expensive single step.\n sent7: Our experiments show that the number of features does not affect latency substantially because we only need to load the document once, once in cache, individual feature extraction is very fast.\n sent8: Note that we have not spent much effort optimizing feature extraction (which is relatively inefficient Java code) and that more engineering effort, for example, optimizations proposed by #TARGET_REF , are likely to further increase speedups.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent8\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "also",
                    "use",
                    "an",
                    "SVM",
                    "classifier",
                    "with",
                    "linear",
                    "kernel",
                    "and",
                    "regularization",
                    "parameter",
                    "of",
                    "1."
                ],
                [
                    "Word",
                    "unigrams,",
                    "bigrams",
                    "and",
                    "trigrams",
                    "were",
                    "used",
                    "as",
                    "features",
                    "in",
                    "this",
                    "case."
                ],
                [
                    "Implementation",
                    "was",
                    "done",
                    "using",
                    "the",
                    "LinearSVC",
                    "class",
                    "from",
                    "the",
                    "scikit-learn",
                    "library",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                3,
                2,
                1
            ]
        },
        "input": "sent0: We also use an SVM classifier with linear kernel and regularization parameter of 1.\n sent1: Word unigrams, bigrams and trigrams were used as features in this case.\n sent2: Implementation was done using the LinearSVC class from the scikit-learn library #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "propose",
                    "a",
                    "three-step",
                    "approach",
                    "to",
                    "Arabic",
                    "sequence",
                    "labeling."
                ],
                [
                    "The",
                    "first",
                    "step",
                    "is",
                    "to",
                    "automatically",
                    "diacritize",
                    "the",
                    "text",
                    "using",
                    "the",
                    "state-of-the-art",
                    "automatic",
                    "diacritization",
                    "system",
                    "Shakkala",
                    "#REF",
                    "."
                ],
                [
                    "The",
                    "second",
                    "step",
                    "is",
                    "the",
                    "individual",
                    "training",
                    "of",
                    "character",
                    "and",
                    "diacritic",
                    "embeddings",
                    "using",
                    "the",
                    "architecture",
                    "proposed",
                    "by",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "The",
                    "third",
                    "step",
                    "is",
                    "to",
                    "train",
                    "all",
                    "embedding",
                    "layers",
                    "together",
                    "using",
                    "a",
                    "combination",
                    "model",
                    "(see",
                    "section",
                    "4.3)."
                ],
                [
                    "There",
                    "are",
                    "two",
                    "main",
                    "advantages",
                    "in",
                    "adopting",
                    "this",
                    "architecture",
                    "for",
                    "EMIL."
                ],
                [
                    "First,",
                    "it",
                    "is",
                    "based",
                    "on",
                    "a",
                    "standard",
                    "approach",
                    "in",
                    "NER",
                    "and",
                    "sequence",
                    "labeling",
                    "in",
                    "general,",
                    "which",
                    "remains",
                    "very",
                    "close",
                    "to",
                    "the",
                    "state-ofthe-art."
                ],
                [
                    "Second,",
                    "it",
                    "is",
                    "a",
                    "relatively",
                    "light-weight",
                    "architecture",
                    "requiring",
                    "less",
                    "computational",
                    "resources",
                    "than",
                    "other",
                    "alternatives",
                    "(see",
                    "section",
                    "5.3)."
                ],
                [
                    "We",
                    "will",
                    "discuss",
                    "and",
                    "justify",
                    "our",
                    "design",
                    "choices",
                    "and",
                    "the",
                    "computational",
                    "aspects",
                    "of",
                    "the",
                    "architecture",
                    "further",
                    "in",
                    "Section",
                    "(5.3)",
                    "and",
                    "Section",
                    "(6)."
                ]
            ],
            "context": [
                2,
                2,
                2,
                2,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: We propose a three-step approach to Arabic sequence labeling.\n sent1: The first step is to automatically diacritize the text using the state-of-the-art automatic diacritization system Shakkala #REF .\n sent2: The second step is the individual training of character and diacritic embeddings using the architecture proposed by #TARGET_REF .\n sent3: The third step is to train all embedding layers together using a combination model (see section 4.3).\n sent4: There are two main advantages in adopting this architecture for EMIL.\n sent5: First, it is based on a standard approach in NER and sequence labeling in general, which remains very close to the state-ofthe-art.\n sent6: Second, it is a relatively light-weight architecture requiring less computational resources than other alternatives (see section 5.3).\n sent7: We will discuss and justify our design choices and the computational aspects of the architecture further in Section (5.3) and Section (6).\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\", \"sent1\", \"sent2\", \"sent3\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Lexical",
                    "complexity",
                    "is",
                    "one",
                    "of",
                    "the",
                    "main",
                    "reasons",
                    "leading",
                    "to",
                    "overall",
                    "text",
                    "complexity",
                    "and",
                    "thus",
                    "result",
                    "in",
                    "poor",
                    "reading",
                    "comprehension",
                    "for",
                    "readers",
                    "#REF",
                    "."
                ],
                [
                    "Different",
                    "from",
                    "the",
                    "Complex",
                    "Word",
                    "Identification",
                    "(CWI)",
                    "#REF",
                    "task,",
                    "which",
                    "aims",
                    "to",
                    "predict",
                    "whether",
                    "a",
                    "given",
                    "word",
                    "is",
                    "complex",
                    "or",
                    "not,",
                    "the",
                    "goal",
                    "of",
                    "lexical",
                    "complexity",
                    "prediction",
                    "(LCP)",
                    "is",
                    "to",
                    "predict",
                    "the",
                    "complexity",
                    "value",
                    "of",
                    "the",
                    "given",
                    "parts",
                    "from",
                    "contexts",
                    "as",
                    "shown",
                    "in",
                    "Figure",
                    "1."
                ],
                [
                    "The",
                    "underlined",
                    "parts",
                    "of",
                    "the",
                    "sentence",
                    "are",
                    "the",
                    "words",
                    "that",
                    "need",
                    "to",
                    "be",
                    "predicted",
                    "and",
                    "the",
                    "same",
                    "words",
                    "in",
                    "different",
                    "contexts",
                    "may",
                    "have",
                    "different",
                    "complexity",
                    "scores."
                ],
                [
                    "LCP",
                    "plays",
                    "an",
                    "important",
                    "role",
                    "in",
                    "the",
                    "usual",
                    "Lexical",
                    "Simplification",
                    "(LS)",
                    "#TARGET_REF",
                    "pipeline",
                    "since",
                    "it",
                    "can",
                    "help",
                    "simplifiers",
                    "find",
                    "the",
                    "challenging",
                    "words",
                    "and",
                    "replace",
                    "them",
                    "with",
                    "appropriate",
                    "alternatives",
                    "that",
                    "easy",
                    "to",
                    "understand."
                ],
                [
                    "Either",
                    "LCP",
                    "or",
                    "CWI",
                    "can",
                    "not",
                    "only",
                    "be",
                    "used",
                    "as",
                    "a",
                    "component",
                    "of",
                    "LS",
                    "systems",
                    "but",
                    "also",
                    "as",
                    "a",
                    "stand-alone",
                    "application",
                    "within",
                    "intelligent",
                    "tutoring",
                    "systems",
                    "for",
                    "second",
                    "language",
                    "learners",
                    "or",
                    "in",
                    "reading",
                    "devices",
                    "for",
                    "people",
                    "with",
                    "low",
                    "literacy",
                    "skills",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                0,
                0,
                0,
                2,
                3
            ]
        },
        "input": "sent0: Lexical complexity is one of the main reasons leading to overall text complexity and thus result in poor reading comprehension for readers #REF .\n sent1: Different from the Complex Word Identification (CWI) #REF task, which aims to predict whether a given word is complex or not, the goal of lexical complexity prediction (LCP) is to predict the complexity value of the given parts from contexts as shown in Figure 1.\n sent2: The underlined parts of the sentence are the words that need to be predicted and the same words in different contexts may have different complexity scores.\n sent3: LCP plays an important role in the usual Lexical Simplification (LS) #TARGET_REF pipeline since it can help simplifiers find the challenging words and replace them with appropriate alternatives that easy to understand.\n sent4: Either LCP or CWI can not only be used as a component of LS systems but also as a stand-alone application within intelligent tutoring systems for second language learners or in reading devices for people with low literacy skills #REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent3\"], \"BACKGROUND\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "One",
                    "of",
                    "the",
                    "crucial",
                    "issues",
                    "regarding",
                    "the",
                    "evaluation",
                    "of",
                    "multi-hop",
                    "inference",
                    "models",
                    "is",
                    "the",
                    "possibility",
                    "to",
                    "achieve",
                    "strong",
                    "overall",
                    "performance",
                    "without",
                    "using",
                    "real",
                    "compositional",
                    "methods",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Therefore,",
                    "in",
                    "order",
                    "to",
                    "evaluate",
                    "multi-hop",
                    "inference",
                    "more",
                    "explicitly,",
                    "we",
                    "break",
                    "down",
                    "the",
                    "performance",
                    "of",
                    "each",
                    "model",
                    "with",
                    "respect",
                    "to",
                    "the",
                    "difficulty",
                    "of",
                    "accessing",
                    "specific",
                    "facts",
                    "in",
                    "an",
                    "explanation",
                    "via",
                    "direct",
                    "lexical",
                    "overlap."
                ],
                [
                    "This",
                    "comes",
                    "from",
                    "the",
                    "assumption",
                    "that",
                    "facts",
                    "sharing",
                    "many",
                    "terms",
                    "with",
                    "question",
                    "or",
                    "answer",
                    "are",
                    "relatively",
                    "easier",
                    "to",
                    "find",
                    "and",
                    "rank",
                    "highly."
                ]
            ],
            "context": [
                1,
                2,
                3
            ]
        },
        "input": "sent0: One of the crucial issues regarding the evaluation of multi-hop inference models is the possibility to achieve strong overall performance without using real compositional methods #TARGET_REF .\n sent1: Therefore, in order to evaluate multi-hop inference more explicitly, we break down the performance of each model with respect to the difficulty of accessing specific facts in an explanation via direct lexical overlap.\n sent2: This comes from the assumption that facts sharing many terms with question or answer are relatively easier to find and rank highly.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "study",
                    "uses",
                    "a",
                    "self-built",
                    "corpus."
                ],
                [
                    "Since",
                    "the",
                    "phenomenon",
                    "of",
                    "public",
                    "apologies",
                    "is",
                    "relatively",
                    "recent",
                    "in",
                    "India,",
                    "we",
                    "could",
                    "only",
                    "access",
                    "a",
                    "corpus",
                    "of",
                    "18",
                    "apologies",
                    "available",
                    "in",
                    "the",
                    "digital",
                    "public",
                    "domain,",
                    "offered",
                    "during",
                    "2007-2017."
                ],
                [
                    "The",
                    "corpus",
                    "is",
                    "in",
                    "the",
                    "English",
                    "language",
                    "as",
                    "it",
                    "is",
                    "the",
                    "second",
                    "official",
                    "language",
                    "in",
                    "India."
                ],
                [
                    "It",
                    "is",
                    "the",
                    "lingua",
                    "franca",
                    "spoken",
                    "amongst",
                    "a",
                    "wide",
                    "proportion",
                    "of",
                    "the",
                    "population",
                    "and",
                    "has",
                    "about",
                    "125",
                    "million",
                    "speakers,",
                    "which",
                    "is,",
                    "country-wise,",
                    "the",
                    "second",
                    "highest",
                    "in",
                    "the",
                    "world,",
                    "only",
                    "below",
                    "United",
                    "States",
                    "of",
                    "America",
                    "4",
                    "."
                ],
                [
                    "We",
                    "employ",
                    "a",
                    "close",
                    "reading",
                    "approach",
                    "#TARGET_REF",
                    "for",
                    "the",
                    "analysis."
                ],
                [
                    "All",
                    "of",
                    "the",
                    "selected",
                    "apologies",
                    "were",
                    "delivered",
                    "in",
                    "India,",
                    "by",
                    "Indians",
                    "so",
                    "as",
                    "to",
                    "understand",
                    "any",
                    "cultural",
                    "implication",
                    "of",
                    "the",
                    "communication."
                ],
                [
                    "All",
                    "of",
                    "these",
                    "were",
                    "offered",
                    "by",
                    "senior",
                    "executives",
                    "of",
                    "the",
                    "company",
                    "or",
                    "prominent",
                    "public",
                    "personalities",
                    "in",
                    "India."
                ],
                [
                    "Of",
                    "these",
                    "two",
                    "were",
                    "electronic",
                    "mails,",
                    "seven",
                    "were",
                    "letters,",
                    "four",
                    "were",
                    "blog",
                    "posts,",
                    "four",
                    "were",
                    "tweets",
                    "out",
                    "of",
                    "which",
                    "two",
                    "are",
                    "related",
                    "to",
                    "the",
                    "same",
                    "event,",
                    "and",
                    "one",
                    "was",
                    "a",
                    "media",
                    "statement."
                ],
                [
                    "Out",
                    "of",
                    "the",
                    "18",
                    "apologies,",
                    "11",
                    "were",
                    "given",
                    "by",
                    "individual(s)",
                    "in",
                    "a",
                    "role,",
                    "3",
                    "were",
                    "given",
                    "by",
                    "organizations",
                    "and",
                    "4",
                    "were",
                    "given",
                    "by",
                    "individuals."
                ],
                [
                    "The",
                    "gender-wise",
                    "distribution",
                    "of",
                    "the",
                    "apology",
                    "givers",
                    "is",
                    "14",
                    "males",
                    "and",
                    "4",
                    "females."
                ],
                [
                    "The",
                    "apologies",
                    "selected",
                    "have",
                    "been",
                    "assigned",
                    "a",
                    "code",
                    "number",
                    "for",
                    "easy",
                    "reference."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                2,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: The study uses a self-built corpus.\n sent1: Since the phenomenon of public apologies is relatively recent in India, we could only access a corpus of 18 apologies available in the digital public domain, offered during 2007-2017.\n sent2: The corpus is in the English language as it is the second official language in India.\n sent3: It is the lingua franca spoken amongst a wide proportion of the population and has about 125 million speakers, which is, country-wise, the second highest in the world, only below United States of America 4 .\n sent4: We employ a close reading approach #TARGET_REF for the analysis.\n sent5: All of the selected apologies were delivered in India, by Indians so as to understand any cultural implication of the communication.\n sent6: All of these were offered by senior executives of the company or prominent public personalities in India.\n sent7: Of these two were electronic mails, seven were letters, four were blog posts, four were tweets out of which two are related to the same event, and one was a media statement.\n sent8: Out of the 18 apologies, 11 were given by individual(s) in a role, 3 were given by organizations and 4 were given by individuals.\n sent9: The gender-wise distribution of the apology givers is 14 males and 4 females.\n sent10: The apologies selected have been assigned a code number for easy reference.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent4\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Entity",
                    "typing",
                    "information",
                    "has",
                    "been",
                    "used",
                    "across",
                    "a",
                    "range",
                    "of",
                    "NLP",
                    "tasks,",
                    "including",
                    "models",
                    "for",
                    "entity",
                    "linking",
                    "and",
                    "coreference",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                1
            ]
        },
        "input": "sent0: Entity typing information has been used across a range of NLP tasks, including models for entity linking and coreference #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "the",
                    "second",
                    "approach,",
                    "they",
                    "first",
                    "recognize",
                    "examples",
                    "that",
                    "contain",
                    "artifacts,",
                    "and",
                    "use",
                    "this",
                    "knowledge",
                    "in",
                    "the",
                    "training",
                    "objective",
                    "to",
                    "either",
                    "skip",
                    "or",
                    "downweight",
                    "biased",
                    "examples",
                    "#TARGET_REF",
                    ",",
                    "or",
                    "to",
                    "regularize",
                    "the",
                    "confidence",
                    "of",
                    "the",
                    "model",
                    "on",
                    "those",
                    "examples",
                    "#REF",
                    "."
                ],
                [
                    "The",
                    "use",
                    "of",
                    "this",
                    "information",
                    "in",
                    "the",
                    "training",
                    "objective",
                    "improves",
                    "the",
                    "robustness",
                    "of",
                    "the",
                    "model",
                    "on",
                    "adversarial",
                    "datasets",
                    "#REF",
                    ",",
                    "i.e.,",
                    "datasets",
                    "that",
                    "contain",
                    "counterexamples",
                    "in",
                    "which",
                    "relying",
                    "on",
                    "the",
                    "bias",
                    "results",
                    "in",
                    "an",
                    "incorrect",
                    "prediction."
                ],
                [
                    "In",
                    "addition,",
                    "it",
                    "can",
                    "also",
                    "improve",
                    "in-domain",
                    "performances",
                    "as",
                    "well",
                    "as",
                    "generalization",
                    "across",
                    "various",
                    "datasets",
                    "that",
                    "represent",
                    "the",
                    "same",
                    "task",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                1,
                0,
                0
            ]
        },
        "input": "sent0: In the second approach, they first recognize examples that contain artifacts, and use this knowledge in the training objective to either skip or downweight biased examples #TARGET_REF , or to regularize the confidence of the model on those examples #REF .\n sent1: The use of this information in the training objective improves the robustness of the model on adversarial datasets #REF , i.e., datasets that contain counterexamples in which relying on the bias results in an incorrect prediction.\n sent2: In addition, it can also improve in-domain performances as well as generalization across various datasets that represent the same task #REF .\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "took",
                    "all",
                    "in-coverage",
                    "sentences",
                    "from",
                    "Susanne",
                    "of",
                    "length",
                    "8-40",
                    "words",
                    "inclusive",
                    "containing",
                    "internal",
                    "punctuation,",
                    "a",
                    "total",
                    "of",
                    "2449",
                    "sentences."
                ],
                [
                    "The",
                    "APB",
                    "for",
                    "this",
                    "set",
                    "was",
                    "1.273,",
                    "mean",
                    "length",
                    "22.5",
                    "words,",
                    "giving",
                    "an",
                    "expected",
                    "number",
                    "of",
                    "analyses",
                    "for",
                    "an",
                    "average",
                    "sentence",
                    "of",
                    "225."
                ],
                [
                    "We",
                    "then",
                    "re-moved",
                    "all",
                    "sentence-internal",
                    "punctuation",
                    "from",
                    "this",
                    "set",
                    "and",
                    "re-parsed",
                    "it."
                ],
                [
                    "Around",
                    "8%",
                    "of",
                    "sentences",
                    "now",
                    "failed",
                    "to",
                    "receive",
                    "an",
                    "analysis."
                ],
                [
                    "For",
                    "those",
                    "that",
                    "did",
                    "(mean",
                    "length",
                    "20.7",
                    "words),",
                    "the",
                    "APB",
                    "was",
                    "now",
                    "1",
                    ".320,",
                    "so",
                    "an",
                    "average",
                    "sentence",
                    "would",
                    "be",
                    "assigned",
                    "310",
                    "analyses,",
                    "38%",
                    "more",
                    "than",
                    "before."
                ],
                [
                    "On",
                    "closer",
                    "inspection,",
                    "the",
                    "increase",
                    "in",
                    "ambiguity",
                    "is",
                    "due",
                    "to",
                    "two",
                    "factors:",
                    "a)",
                    "a",
                    "significant",
                    "proportion",
                    "of",
                    "sentences",
                    "that",
                    "previously",
                    "received",
                    "1",
                    "...:..",
                    "9",
                    "analyses",
                    "now",
                    "receive",
                    "more,",
                    "and",
                    "b)",
                    "there",
                    "is",
                    "a",
                    "much",
                    "more",
                    "substantial",
                    "tail",
                    "in",
                    "the",
                    "distribution",
                    "of",
                    "sentence",
                    "length",
                    "vs.",
                    "number",
                    "of",
                    "parses,",
                    "due",
                    "to",
                    "some",
                    "longer",
                    "sentences",
                    "being",
                    "assigned",
                    "many",
                    "more",
                    "parses."
                ],
                [
                    "Manual",
                    "examination",
                    "of",
                    "100",
                    "depunctuated",
                    "examples",
                    "revealed",
                    "that",
                    "in",
                    "around",
                    "a",
                    "third",
                    "of",
                    "cases,",
                    "although",
                    "the",
                    "system",
                    "returned",
                    "global",
                    "analyses,",
                    "the",
                    "correct",
                    "one",
                    "was",
                    "not",
                    "in",
                    "this",
                    "set",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "With",
                    "a",
                    "more",
                    "constrained",
                    "(sub",
                    "categorised)",
                    "syntactic",
                    "grammar,",
                    "many",
                    "of",
                    "these",
                    "examples",
                    "would",
                    "not",
                    "have",
                    "received",
                    "any",
                    "global",
                    "syntactic",
                    "analysis."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                1,
                3
            ]
        },
        "input": "sent0: We took all in-coverage sentences from Susanne of length 8-40 words inclusive containing internal punctuation, a total of 2449 sentences.\n sent1: The APB for this set was 1.273, mean length 22.5 words, giving an expected number of analyses for an average sentence of 225.\n sent2: We then re-moved all sentence-internal punctuation from this set and re-parsed it.\n sent3: Around 8% of sentences now failed to receive an analysis.\n sent4: For those that did (mean length 20.7 words), the APB was now 1 .320, so an average sentence would be assigned 310 analyses, 38% more than before.\n sent5: On closer inspection, the increase in ambiguity is due to two factors: a) a significant proportion of sentences that previously received 1 ...:.. 9 analyses now receive more, and b) there is a much more substantial tail in the distribution of sentence length vs. number of parses, due to some longer sentences being assigned many more parses.\n sent6: Manual examination of 100 depunctuated examples revealed that in around a third of cases, although the system returned global analyses, the correct one was not in this set #TARGET_REF .\n sent7: With a more constrained (sub categorised) syntactic grammar, many of these examples would not have received any global syntactic analysis.\n",
        "output": "{\"INFORMATION\": [\"sent6\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent7\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "system",
                    "will",
                    "use",
                    "a",
                    "wide",
                    "variety",
                    "of",
                    "techniques,",
                    "many",
                    "of",
                    "which",
                    "are",
                    "at",
                    "the",
                    "forefront",
                    "of",
                    "research."
                ],
                [
                    "For",
                    "the",
                    "task",
                    "of",
                    "matching",
                    "of",
                    "user's",
                    "input",
                    "with",
                    "the",
                    "stored",
                    "examples,",
                    "parsing",
                    "of",
                    "a",
                    "traditional",
                    "nature",
                    "may",
                    "be",
                    "employed,",
                    "but",
                    "the",
                    "primary",
                    "technique",
                    "will",
                    "involve",
                    "stochastic",
                    "or",
                    "other",
                    "pattern",
                    "matching",
                    "techniques."
                ],
                [
                    "Because",
                    "the",
                    "aim",
                    "is",
                    "to",
                    "match",
                    "inputs",
                    "with",
                    "similar,",
                    "but",
                    "usually",
                    "not",
                    "identical",
                    "examples",
                    "in",
                    "the",
                    "database,",
                    "the",
                    "matching",
                    "techniques",
                    "must",
                    "be",
                    "flexible",
                    "enough",
                    "to",
                    "locate",
                    "a",
                    "range",
                    "of",
                    "candidate",
                    "matches",
                    "against",
                    "a",
                    "given",
                    "input."
                ],
                [
                    "For",
                    "this",
                    "reason,",
                    "pattern",
                    "matching",
                    "incorporating",
                    "a",
                    "similarity",
                    "measure",
                    "is",
                    "indicated,",
                    "such",
                    "as",
                    "the",
                    "techniques",
                    "proposed",
                    "in",
                    "#REF",
                    ",",
                    "though",
                    "we",
                    "are",
                    "also",
                    "experimenting",
                    "with",
                    "a",
                    "connectionist",
                    "approach",
                    "to",
                    "this",
                    "problem",
                    "(",
                    "#TARGET_REF",
                    ")."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: The system will use a wide variety of techniques, many of which are at the forefront of research.\n sent1: For the task of matching of user's input with the stored examples, parsing of a traditional nature may be employed, but the primary technique will involve stochastic or other pattern matching techniques.\n sent2: Because the aim is to match inputs with similar, but usually not identical examples in the database, the matching techniques must be flexible enough to locate a range of candidate matches against a given input.\n sent3: For this reason, pattern matching incorporating a similarity measure is indicated, such as the techniques proposed in #REF , though we are also experimenting with a connectionist approach to this problem ( #TARGET_REF ).\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "der",
                    "Goot",
                    "(2022)",
                    "(MaChAmp)",
                    "proposed",
                    "to",
                    "pretrain",
                    "a",
                    "language",
                    "model",
                    "and",
                    "re-finetune",
                    "after",
                    "multitask",
                    "learning",
                    "for",
                    "a",
                    "pre-defined",
                    "set",
                    "of",
                    "semantically",
                    "focused",
                    "NLP",
                    "tasks."
                ],
                [
                    "They",
                    "trained",
                    "a",
                    "multi-task",
                    "model",
                    "for",
                    "all",
                    "text-based",
                    "SemEval",
                    "tasks",
                    "that",
                    "include",
                    "annotation",
                    "on",
                    "the",
                    "word,",
                    "sentence,",
                    "or",
                    "paragraph",
                    "level."
                ],
                [
                    "They",
                    "compared",
                    "the",
                    "performance",
                    "with",
                    "models",
                    "using",
                    "mBERT",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "The",
                    "pretrained",
                    "multi-task",
                    "embedding",
                    "showed",
                    "a",
                    "consistent",
                    "improvement",
                    "across",
                    "many",
                    "tasks",
                    "against",
                    "the",
                    "mBERT",
                    "embedding."
                ]
            ],
            "context": [
                3,
                3,
                3,
                3
            ]
        },
        "input": "sent0: der Goot (2022) (MaChAmp) proposed to pretrain a language model and re-finetune after multitask learning for a pre-defined set of semantically focused NLP tasks.\n sent1: They trained a multi-task model for all text-based SemEval tasks that include annotation on the word, sentence, or paragraph level.\n sent2: They compared the performance with models using mBERT #TARGET_REF .\n sent3: The pretrained multi-task embedding showed a consistent improvement across many tasks against the mBERT embedding.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\", \"sent1\", \"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Thus",
                    "there",
                    "are",
                    "a",
                    "number",
                    "of",
                    "situations",
                    "in",
                    "which",
                    "gCNs",
                    "are",
                    "not",
                    "sufficient."
                ],
                [
                    "Given",
                    "that",
                    "gCNs",
                    "can",
                    "be",
                    "represented",
                    "by",
                    "Tree",
                    "Substitution",
                    "Grammars,",
                    "as",
                    "in",
                    "#TARGET_REF",
                    ",",
                    "which",
                    "are",
                    "in",
                    "fact",
                    "TAGs",
                    "that",
                    "do",
                    "not",
                    "allow",
                    "precisely",
                    "the",
                    "kind",
                    "of",
                    "unbounded",
                    "phenomena",
                    "described",
                    "by",
                    "TAGs,",
                    "this",
                    "would",
                    "suggest",
                    "that",
                    "using",
                    "a",
                    "TAG",
                    "grammar",
                    "to",
                    "describe",
                    "the",
                    "gNCNs",
                    "in",
                    "order",
                    "to",
                    "decompose",
                    "the",
                    "trees",
                    "would",
                    "be",
                    "feasible,",
                    "and",
                    "this",
                    "is",
                    "further",
                    "an",
                    "interesting",
                    "question",
                    "for",
                    "theoretical",
                    "reasons",
                    "described",
                    "below."
                ]
            ],
            "context": [
                0,
                2
            ]
        },
        "input": "sent0: Thus there are a number of situations in which gCNs are not sufficient.\n sent1: Given that gCNs can be represented by Tree Substitution Grammars, as in #TARGET_REF , which are in fact TAGs that do not allow precisely the kind of unbounded phenomena described by TAGs, this would suggest that using a TAG grammar to describe the gNCNs in order to decompose the trees would be feasible, and this is further an interesting question for theoretical reasons described below.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Among",
                    "four",
                    "submitted",
                    "systems,",
                    "MaChAmp",
                    "(der",
                    "Goot,",
                    "2022)",
                    "and",
                    "AN(L)P",
                    "#REF",
                    "teams",
                    "used",
                    "the",
                    "default",
                    "tokenizer",
                    "from",
                    "either",
                    "BERT",
                    "or",
                    "mBERT,",
                    "which",
                    "are",
                    "not",
                    "designed",
                    "for",
                    "scientific",
                    "documents."
                ],
                [
                    "Consequently,",
                    "they",
                    "are",
                    "unable",
                    "to",
                    "correctly",
                    "segment",
                    "the",
                    "mathematic",
                    "source,",
                    "hence,",
                    "they",
                    "#TARGET_REF",
                    "and",
                    "JBNU-CCLab",
                    "(Lee",
                    "and",
                    "Na,",
                    "2022)",
                    "achieved",
                    "much",
                    "higher",
                    "performances",
                    "thanks",
                    "to",
                    "SciBERT",
                    "tokenizer",
                    "because",
                    "it",
                    "is",
                    "trained",
                    "on",
                    "scientific",
                    "literature."
                ],
                [
                    "However,",
                    "the",
                    "SciBERT",
                    "tokenizer",
                    "is",
                    "far",
                    "from",
                    "perfect",
                    "such",
                    "that",
                    "JBNU-CCLab",
                    "further",
                    "proposed",
                    "to",
                    "tokenize",
                    "the",
                    "mathematical",
                    "formulae",
                    "using",
                    "a",
                    "customized",
                    "rule-based",
                    "tokenizer",
                    "based",
                    "on",
                    "capital",
                    "letters,",
                    "numbers,",
                    "and",
                    "special",
                    "characters(e.g."
                ],
                [
                    "%,",
                    "$,",
                    "{,",
                    "})."
                ],
                [
                    "Hence,",
                    "they",
                    "achieved",
                    "state-of-the-art",
                    "performance",
                    "on",
                    "both",
                    "NER",
                    "and",
                    "RE",
                    "subtasks."
                ]
            ],
            "context": [
                3,
                2,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Among four submitted systems, MaChAmp (der Goot, 2022) and AN(L)P #REF teams used the default tokenizer from either BERT or mBERT, which are not designed for scientific documents.\n sent1: Consequently, they are unable to correctly segment the mathematic source, hence, they #TARGET_REF and JBNU-CCLab (Lee and Na, 2022) achieved much higher performances thanks to SciBERT tokenizer because it is trained on scientific literature.\n sent2: However, the SciBERT tokenizer is far from perfect such that JBNU-CCLab further proposed to tokenize the mathematical formulae using a customized rule-based tokenizer based on capital letters, numbers, and special characters(e.g.\n sent3: %, $, {, }).\n sent4: Hence, they achieved state-of-the-art performance on both NER and RE subtasks.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Improving",
                    "content",
                    "diversity",
                    "in",
                    "NLG."
                ],
                [
                    "Most",
                    "of",
                    "the",
                    "existing",
                    "diversity-promoting",
                    "work",
                    "has",
                    "focused",
                    "on",
                    "improving",
                    "syntactic",
                    "and",
                    "lexical",
                    "diversity,",
                    "such",
                    "as",
                    "different",
                    "language",
                    "style",
                    "in",
                    "machine",
                    "translation",
                    "#TARGET_REF",
                    "and",
                    "word",
                    "variability",
                    "in",
                    "paraphrase",
                    "generation",
                    "#REF",
                    "."
                ],
                [
                    "Nevertheless,",
                    "methods",
                    "for",
                    "improving",
                    "content",
                    "diversity",
                    "in",
                    "NLG",
                    "systems",
                    "have",
                    "been",
                    "rarely",
                    "studied",
                    "in",
                    "the",
                    "existing",
                    "literature."
                ],
                [
                    "We",
                    "believe",
                    "that",
                    "generating",
                    "diverse",
                    "content",
                    "is",
                    "one",
                    "of",
                    "the",
                    "most",
                    "promising",
                    "aspects",
                    "of",
                    "machine",
                    "intelligence,",
                    "which",
                    "can",
                    "be",
                    "applied",
                    "to",
                    "a",
                    "wide",
                    "range",
                    "of",
                    "real-world",
                    "applications,",
                    "not",
                    "only",
                    "limited",
                    "to",
                    "commonsense",
                    "reasoning."
                ]
            ],
            "context": [
                0,
                2,
                0,
                0
            ]
        },
        "input": "sent0: Improving content diversity in NLG.\n sent1: Most of the existing diversity-promoting work has focused on improving syntactic and lexical diversity, such as different language style in machine translation #TARGET_REF and word variability in paraphrase generation #REF .\n sent2: Nevertheless, methods for improving content diversity in NLG systems have been rarely studied in the existing literature.\n sent3: We believe that generating diverse content is one of the most promising aspects of machine intelligence, which can be applied to a wide range of real-world applications, not only limited to commonsense reasoning.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "This",
                    "paper",
                    "reports",
                    "the",
                    "submissions",
                    "of",
                    "the",
                    "Am-rita_CEN_NLP",
                    "team",
                    "for",
                    "the",
                    "3C",
                    "Citation",
                    "Context",
                    "Classification",
                    "shared",
                    "task",
                    "#REF",
                    "."
                ],
                [
                    "We",
                    "used",
                    "deep",
                    "learning",
                    "and",
                    "machine",
                    "learning",
                    "models",
                    "developed",
                    "using",
                    "Bi-LSTM",
                    "and",
                    "Random",
                    "Forest",
                    "algorithms",
                    "#TARGET_REF",
                    ",",
                    "#REF",
                    ",",
                    "#REF",
                    "to",
                    "complete",
                    "the",
                    "subtasks."
                ],
                [
                    "They",
                    "will",
                    "be",
                    "elaborated",
                    "upon",
                    "in",
                    "Section",
                    "4."
                ]
            ],
            "context": [
                0,
                2,
                0
            ]
        },
        "input": "sent0: This paper reports the submissions of the Am-rita_CEN_NLP team for the 3C Citation Context Classification shared task #REF .\n sent1: We used deep learning and machine learning models developed using Bi-LSTM and Random Forest algorithms #TARGET_REF , #REF , #REF to complete the subtasks.\n sent2: They will be elaborated upon in Section 4.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Thanks",
                    "to",
                    "the",
                    "significant",
                    "improvement",
                    "of",
                    "machine",
                    "translation",
                    "(MT)",
                    "over",
                    "the",
                    "past",
                    "two",
                    "decades,",
                    "the",
                    "translation",
                    "industry",
                    "has",
                    "already",
                    "started",
                    "to",
                    "exploit",
                    "it,",
                    "mainly",
                    "by",
                    "combining",
                    "it",
                    "with",
                    "post-editing."
                ],
                [
                    "A",
                    "good",
                    "number",
                    "of",
                    "recent",
                    "works",
                    "report",
                    "a",
                    "productivity",
                    "increase",
                    "thanks",
                    "to",
                    "postediting",
                    "of",
                    "MT",
                    "output",
                    "as",
                    "compared",
                    "to",
                    "the",
                    "traditional",
                    "human",
                    "translation",
                    "(e.g.,",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                0,
                3
            ]
        },
        "input": "sent0: Thanks to the significant improvement of machine translation (MT) over the past two decades, the translation industry has already started to exploit it, mainly by combining it with post-editing.\n sent1: A good number of recent works report a productivity increase thanks to postediting of MT output as compared to the traditional human translation (e.g., #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Hope",
                    "and",
                    "Keller",
                    "(2013),",
                    "for",
                    "example,",
                    "use",
                    "a",
                    "graph",
                    "of",
                    "co-occurrences",
                    "for",
                    "word",
                    "sense",
                    "induction."
                ],
                [
                    "Later",
                    "#TARGET_REF",
                    "use",
                    "a",
                    "similar",
                    "method",
                    "to",
                    "disambiguate",
                    "word",
                    "embedding",
                    "models."
                ]
            ],
            "context": [
                3,
                1
            ]
        },
        "input": "sent0: Hope and Keller (2013), for example, use a graph of co-occurrences for word sense induction.\n sent1: Later #TARGET_REF use a similar method to disambiguate word embedding models.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "saying",
                    "sorry",
                    "does",
                    "not",
                    "come",
                    "easy",
                    "to",
                    "Indians",
                    "and",
                    "more",
                    "so",
                    "to",
                    "Indian",
                    "business",
                    "and",
                    "political",
                    "leaders."
                ],
                [
                    "This",
                    "hesitation",
                    "can",
                    "perhaps",
                    "be",
                    "linked",
                    "to",
                    "the",
                    "fact",
                    "that",
                    "in",
                    "India",
                    "a",
                    "public",
                    "apologyis",
                    "seen",
                    "as",
                    "an",
                    "admission",
                    "of",
                    "guilt",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "On",
                    "the",
                    "other",
                    "hand",
                    "it",
                    "is",
                    "a",
                    "common",
                    "occurrence",
                    "in",
                    "countries",
                    "like",
                    "Japan",
                    "and",
                    "Hong",
                    "Kong,",
                    "where",
                    "the",
                    "corporate",
                    "apology",
                    "is",
                    "an",
                    "expression",
                    "of",
                    "eagerness",
                    "to",
                    "repair",
                    "damage",
                    "and",
                    "relationships",
                    "and",
                    "does",
                    "not",
                    "imply",
                    "guilt",
                    "(ibid)."
                ],
                [
                    "In",
                    "the",
                    "past,",
                    "the",
                    "speech",
                    "act",
                    "of",
                    "apology",
                    "was",
                    "almost",
                    "absent",
                    "from",
                    "the",
                    "repertoire",
                    "of",
                    "Indian",
                    "corporates",
                    "and",
                    "public",
                    "figures",
                    "#REF",
                    "."
                ],
                [
                    "Even",
                    "written",
                    "apologies",
                    "were",
                    "very",
                    "few",
                    "and",
                    "were",
                    "offered",
                    "only",
                    "when",
                    "there",
                    "was",
                    "a",
                    "strong",
                    "demand",
                    "from",
                    "different",
                    "sections",
                    "of",
                    "society."
                ],
                [
                    "However,",
                    "the",
                    "new",
                    "generation",
                    "e-commerce",
                    "companies",
                    "seem",
                    "to",
                    "be",
                    "heralding",
                    "an",
                    "attitudinal",
                    "change",
                    "in",
                    "this",
                    "corporate",
                    "practice."
                ],
                [
                    "This",
                    "could",
                    "be",
                    "due",
                    "to",
                    "the",
                    "increasing",
                    "digital",
                    "customer",
                    "base",
                    "for",
                    "India",
                    "Inc.",
                    "India's",
                    "internet",
                    "user",
                    "base",
                    "has",
                    "grown",
                    "to",
                    "324.95",
                    "million",
                    "in",
                    "September",
                    "2015,",
                    "a",
                    "27.73%",
                    "YOY",
                    "growth",
                    "(TRAI,",
                    "2016)."
                ],
                [
                    "On",
                    "social",
                    "media",
                    "platforms",
                    "situations",
                    "can",
                    "escalate",
                    "rapidly,",
                    "breaking",
                    "down",
                    "the",
                    "traditional",
                    "barriers",
                    "of",
                    "time,",
                    "location,",
                    "and",
                    "gatekeepers",
                    "of",
                    "information",
                    "#REF",
                    "."
                ],
                [
                    "Thus,",
                    "in",
                    "stark",
                    "contrast",
                    "to",
                    "the",
                    "past,",
                    "we",
                    "see",
                    "a",
                    "spate",
                    "of",
                    "apology",
                    "e-mails,",
                    "tweets",
                    "and",
                    "blog",
                    "posts",
                    "being",
                    "offered",
                    "by",
                    "e-commerce",
                    "players",
                    "(ibid)."
                ],
                [
                    "Figure",
                    "1",
                    "shows",
                    "the",
                    "rising",
                    "trend",
                    "of",
                    "apologies",
                    "being",
                    "given",
                    "publicly",
                    "in",
                    "the",
                    "written",
                    "digital",
                    "media,",
                    "with",
                    "a",
                    "sharp",
                    "increase",
                    "from",
                    "the",
                    "year",
                    "2016",
                    "to",
                    "2017."
                ],
                [
                    "Since",
                    "the",
                    "practice",
                    "of",
                    "offering",
                    "a",
                    "public",
                    "apology",
                    "is",
                    "relatively",
                    "new",
                    "for",
                    "Indian",
                    "businesses,",
                    "it",
                    "is",
                    "to",
                    "be",
                    "understood",
                    "that",
                    "an",
                    "apology",
                    "not",
                    "delivered",
                    "effectively",
                    "rather",
                    "than",
                    "mitigating",
                    "the",
                    "damage,",
                    "can",
                    "escalate",
                    "the",
                    "damage",
                    "done."
                ],
                [
                    "In",
                    "this",
                    "context,",
                    "it",
                    "is",
                    "important",
                    "to",
                    "analyze",
                    "the",
                    "lexical",
                    "choice",
                    "made",
                    "in",
                    "these",
                    "apologies",
                    "and",
                    "the",
                    "implications",
                    "thereof."
                ]
            ],
            "context": [
                3,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: saying sorry does not come easy to Indians and more so to Indian business and political leaders.\n sent1: This hesitation can perhaps be linked to the fact that in India a public apologyis seen as an admission of guilt #TARGET_REF .\n sent2: On the other hand it is a common occurrence in countries like Japan and Hong Kong, where the corporate apology is an expression of eagerness to repair damage and relationships and does not imply guilt (ibid).\n sent3: In the past, the speech act of apology was almost absent from the repertoire of Indian corporates and public figures #REF .\n sent4: Even written apologies were very few and were offered only when there was a strong demand from different sections of society.\n sent5: However, the new generation e-commerce companies seem to be heralding an attitudinal change in this corporate practice.\n sent6: This could be due to the increasing digital customer base for India Inc. India's internet user base has grown to 324.95 million in September 2015, a 27.73% YOY growth (TRAI, 2016).\n sent7: On social media platforms situations can escalate rapidly, breaking down the traditional barriers of time, location, and gatekeepers of information #REF .\n sent8: Thus, in stark contrast to the past, we see a spate of apology e-mails, tweets and blog posts being offered by e-commerce players (ibid).\n sent9: Figure 1 shows the rising trend of apologies being given publicly in the written digital media, with a sharp increase from the year 2016 to 2017.\n sent10: Since the practice of offering a public apology is relatively new for Indian businesses, it is to be understood that an apology not delivered effectively rather than mitigating the damage, can escalate the damage done.\n sent11: In this context, it is important to analyze the lexical choice made in these apologies and the implications thereof.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Most",
                    "of",
                    "the",
                    "works",
                    "that",
                    "study",
                    "this",
                    "task",
                    "commonly",
                    "point",
                    "first",
                    "to",
                    "surface-level",
                    "features,",
                    "such",
                    "as",
                    "bag",
                    "of",
                    "words",
                    "and",
                    "lexicon-based",
                    "approaches,",
                    "with",
                    "negative",
                    "words",
                    "as",
                    "features",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                2
            ]
        },
        "input": "sent0: Most of the works that study this task commonly point first to surface-level features, such as bag of words and lexicon-based approaches, with negative words as features #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Reddit."
                ],
                [
                    "A",
                    "further",
                    "tuning",
                    "dataset",
                    "is",
                    "derived",
                    "from",
                    "an",
                    "existing",
                    "Reddit",
                    "dataset,",
                    "pushshift.io",
                    "#TARGET_REF",
                    "as",
                    "seen",
                    "in",
                    "#REF",
                    "."
                ],
                [
                    "This",
                    "dataset",
                    "has",
                    "been",
                    "used",
                    "in",
                    "several",
                    "existing",
                    "dialoguebased",
                    "studies",
                    "and",
                    "has",
                    "been",
                    "shown",
                    "to",
                    "result",
                    "in",
                    "more",
                    "natural",
                    "conversations",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                0,
                3,
                1
            ]
        },
        "input": "sent0: Reddit.\n sent1: A further tuning dataset is derived from an existing Reddit dataset, pushshift.io #TARGET_REF as seen in #REF .\n sent2: This dataset has been used in several existing dialoguebased studies and has been shown to result in more natural conversations #REF .\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "SCE",
                    "loss,",
                    "which",
                    "is",
                    "often",
                    "used",
                    "to",
                    "train",
                    "a",
                    "sequence-to-sequence",
                    "(Seq2Seq)",
                    "model",
                    "#REF",
                    ",",
                    "is",
                    "expressed",
                    "as",
                    "𝐿",
                    "𝑆𝐶𝐸",
                    "=",
                    "−𝑙𝑜𝑔{𝑠𝑜𝑓𝑡𝑚𝑎𝑥",
                    "𝑐",
                    "}",
                    ",",
                    "where𝑠𝑜𝑓𝑡𝑚𝑎𝑥",
                    "𝑐",
                    "=",
                    "𝑒",
                    "𝑑",
                    "𝑐",
                    "∑",
                    "𝑒",
                    "𝑑",
                    "𝑘",
                    "|𝑉|",
                    "𝑘."
                ],
                [
                    "Therein,",
                    "𝑉",
                    "represents",
                    "the",
                    "lexicon,",
                    "𝑑",
                    "𝑘",
                    "denotes",
                    "the",
                    "𝑘-th",
                    "element",
                    "of",
                    "the",
                    "output",
                    "𝑑",
                    "∈",
                    "ℝ",
                    "|𝑉|",
                    "."
                ],
                [
                    "#TARGET_REF",
                    "defined",
                    "Inverse",
                    "Token",
                    "Frequency",
                    "(ITF)",
                    "loss",
                    "as",
                    "shown",
                    "below."
                ]
            ],
            "context": [
                0,
                0,
                1
            ]
        },
        "input": "sent0: The SCE loss, which is often used to train a sequence-to-sequence (Seq2Seq) model #REF , is expressed as 𝐿 𝑆𝐶𝐸 = −𝑙𝑜𝑔{𝑠𝑜𝑓𝑡𝑚𝑎𝑥 𝑐 } , where𝑠𝑜𝑓𝑡𝑚𝑎𝑥 𝑐 = 𝑒 𝑑 𝑐 ∑ 𝑒 𝑑 𝑘 |𝑉| 𝑘.\n sent1: Therein, 𝑉 represents the lexicon, 𝑑 𝑘 denotes the 𝑘-th element of the output 𝑑 ∈ ℝ |𝑉| .\n sent2: #TARGET_REF defined Inverse Token Frequency (ITF) loss as shown below.\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "For",
                    "wh-word,",
                    "empty",
                    "question,",
                    "and",
                    "short",
                    "distance",
                    "reasoning,",
                    "we",
                    "use",
                    "the",
                    "TASE",
                    "model",
                    "#TARGET_REF",
                    "We",
                    "also",
                    "investigate",
                    "whether",
                    "these",
                    "biases",
                    "have",
                    "similar",
                    "ratios",
                    "in",
                    "a",
                    "coreference",
                    "resolution",
                    "dataset."
                ],
                [
                    "We",
                    "use",
                    "the",
                    "CoNLL-2012",
                    "coreference",
                    "resolution",
                    "dataset",
                    "#REF",
                    "and",
                    "convert",
                    "it",
                    "to",
                    "a",
                    "reading",
                    "comprehension",
                    "format,",
                    "i.e.,",
                    "CoNLL",
                    "bart",
                    "in",
                    "Section",
                    "5."
                ],
                [
                    "5",
                    "This",
                    "data",
                    "contains",
                    "question-answer",
                    "pairs",
                    "in",
                    "which",
                    "the",
                    "question",
                    "is",
                    "created",
                    "based",
                    "on",
                    "a",
                    "coreferring",
                    "expression",
                    "in",
                    "CoNLL-2012,",
                    "and",
                    "the",
                    "answer",
                    "is",
                    "its",
                    "closest",
                    "antecedent."
                ],
                [
                    "We",
                    "split",
                    "this",
                    "data",
                    "into",
                    "training",
                    "and",
                    "test",
                    "sets",
                    "and",
                    "train",
                    "bias",
                    "models",
                    "on",
                    "the",
                    "training",
                    "split."
                ],
                [
                    "The",
                    "CoNLL",
                    "bart",
                    "column",
                    "in",
                    "Table",
                    "1",
                    "shows",
                    "the",
                    "bias",
                    "proportions",
                    "on",
                    "this",
                    "data."
                ]
            ],
            "context": [
                2,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: For wh-word, empty question, and short distance reasoning, we use the TASE model #TARGET_REF We also investigate whether these biases have similar ratios in a coreference resolution dataset.\n sent1: We use the CoNLL-2012 coreference resolution dataset #REF and convert it to a reading comprehension format, i.e., CoNLL bart in Section 5.\n sent2: 5 This data contains question-answer pairs in which the question is created based on a coreferring expression in CoNLL-2012, and the answer is its closest antecedent.\n sent3: We split this data into training and test sets and train bias models on the training split.\n sent4: The CoNLL bart column in Table 1 shows the bias proportions on this data.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "LIGHT-Original."
                ],
                [
                    "The",
                    "task",
                    "itself",
                    "dervied",
                    "from",
                    "the",
                    "original",
                    "LIGHT",
                    "dataset",
                    "#TARGET_REF",
                    "and",
                    "involves",
                    "predicting",
                    "the",
                    "next",
                    "action",
                    "or",
                    "utterance",
                    "given",
                    "the",
                    "prior",
                    "dialogue",
                    "history",
                    "as",
                    "well",
                    "as",
                    "the",
                    "current",
                    "setting",
                    "and",
                    "persona",
                    "for",
                    "a",
                    "character."
                ],
                [
                    "They",
                    "are",
                    "collected",
                    "in",
                    "a",
                    "chit-chat",
                    "fashion,",
                    "with",
                    "no",
                    "notion",
                    "of",
                    "objectives,",
                    "and",
                    "so",
                    "provide",
                    "priors",
                    "on",
                    "how",
                    "to",
                    "generally",
                    "act",
                    "consistently",
                    "and",
                    "speak",
                    "in",
                    "a",
                    "fantasy",
                    "world,",
                    "but",
                    "not",
                    "directly",
                    "how",
                    "to",
                    "complete",
                    "quests."
                ]
            ],
            "context": [
                3,
                3,
                0
            ]
        },
        "input": "sent0: LIGHT-Original.\n sent1: The task itself dervied from the original LIGHT dataset #TARGET_REF and involves predicting the next action or utterance given the prior dialogue history as well as the current setting and persona for a character.\n sent2: They are collected in a chit-chat fashion, with no notion of objectives, and so provide priors on how to generally act consistently and speak in a fantasy world, but not directly how to complete quests.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Our",
                    "training",
                    "data",
                    "include",
                    "five",
                    "datasets",
                    "for",
                    "target-based",
                    "sentiment",
                    "classification:",
                    "SemEval17",
                    "#TARGET_REF",
                    ",",
                    "entities",
                    "#REF",
                    ",",
                    "open",
                    "domain",
                    "#REF",
                    ",",
                    "Irish",
                    "politics",
                    "#REF",
                    ",",
                    "and",
                    "our",
                    "annotations",
                    "of",
                    "positive/negative",
                    "norms",
                    "toward",
                    "norm",
                    "targets",
                    "(",
                    "§5.1)."
                ],
                [
                    "These",
                    "annotations",
                    "highly",
                    "improve",
                    "classification",
                    "of",
                    "sentiments",
                    "expressed",
                    "through",
                    "advocacy",
                    "and",
                    "opposition",
                    "in",
                    "normative",
                    "statements."
                ],
                [
                    "Pretraining",
                    "on",
                    "general",
                    "sentiment",
                    "resourcessubjectivity",
                    "lexicon",
                    "#REF",
                    "and",
                    "sen-timent140",
                    "#REF",
                    "-also",
                    "helps",
                    "(",
                    "Table",
                    "3:",
                    "Mapping",
                    "between",
                    "corpus-specific",
                    "labels",
                    "and",
                    "our",
                    "labels",
                    "for",
                    "the",
                    "causality",
                    "module."
                ],
                [
                    "†",
                    "The",
                    "order",
                    "of",
                    "two",
                    "input",
                    "texts",
                    "are",
                    "reversed."
                ],
                [
                    "‡",
                    "The",
                    "second",
                    "input",
                    "text",
                    "is",
                    "replaced",
                    "with",
                    "a",
                    "random",
                    "text",
                    "in",
                    "the",
                    "corpus."
                ]
            ],
            "context": [
                3,
                2,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Our training data include five datasets for target-based sentiment classification: SemEval17 #TARGET_REF , entities #REF , open domain #REF , Irish politics #REF , and our annotations of positive/negative norms toward norm targets ( §5.1).\n sent1: These annotations highly improve classification of sentiments expressed through advocacy and opposition in normative statements.\n sent2: Pretraining on general sentiment resourcessubjectivity lexicon #REF and sen-timent140 #REF -also helps ( Table 3: Mapping between corpus-specific labels and our labels for the causality module.\n sent3: † The order of two input texts are reversed.\n sent4: ‡ The second input text is replaced with a random text in the corpus.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "POS",
                    "tagger",
                    "from",
                    "Totale",
                    "#TARGET_REF",
                    ")",
                    "was",
                    "also",
                    "used",
                    "as",
                    "the",
                    "disambiguation",
                    "module",
                    "instead",
                    "of",
                    "the",
                    "original",
                    "apertium",
                    "tagger."
                ]
            ],
            "context": [
                2
            ]
        },
        "input": "sent0: POS tagger from Totale #TARGET_REF ) was also used as the disambiguation module instead of the original apertium tagger.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Nonetheless,",
                    "the",
                    "established",
                    "convention",
                    "incorporates",
                    "a",
                    "distancing",
                    "from",
                    "the",
                    "offence."
                ],
                [
                    "Also,",
                    "writers",
                    "use",
                    "apologies",
                    "when",
                    "they",
                    "are",
                    "apologising",
                    "in",
                    "a",
                    "role",
                    "(e.g."
                ],
                [
                    "as",
                    "the",
                    "representative",
                    "of",
                    "an",
                    "organisation)."
                ],
                [
                    "When",
                    "speaking",
                    "personally,",
                    "they",
                    "use",
                    "other",
                    "forms,",
                    "typically",
                    "sorry",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Another",
                    "possibility",
                    "is",
                    "that",
                    "use",
                    "of",
                    "the",
                    "noun",
                    "form",
                    "enables",
                    "the",
                    "writer",
                    "to",
                    "avoid",
                    "the",
                    "personal",
                    "pronoun,",
                    "creating",
                    "a",
                    "distance",
                    "between",
                    "the",
                    "writer",
                    "and",
                    "the",
                    "responsibility",
                    "for",
                    "the",
                    "offence",
                    "(ibid)."
                ],
                [
                    "In",
                    "our",
                    "data,",
                    "individuals",
                    "have",
                    "not",
                    "used",
                    "this",
                    "form",
                    "at",
                    "all",
                    "and",
                    "of",
                    "the",
                    "seven",
                    "occurrences",
                    "of",
                    "the",
                    "noun",
                    "form,",
                    "six",
                    "are",
                    "by",
                    "individuals",
                    "as",
                    "representative",
                    "of",
                    "an",
                    "organisation."
                ],
                [
                    "This",
                    "co-relates",
                    "to",
                    "Harrison's",
                    "finding",
                    "that",
                    "the",
                    "word",
                    "apology/",
                    "apologies",
                    "help",
                    "the",
                    "writers",
                    "to",
                    "distance",
                    "themselves",
                    "from",
                    "the",
                    "instance",
                    "or",
                    "event."
                ]
            ],
            "context": [
                0,
                2,
                3,
                1,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Nonetheless, the established convention incorporates a distancing from the offence.\n sent1: Also, writers use apologies when they are apologising in a role (e.g.\n sent2: as the representative of an organisation).\n sent3: When speaking personally, they use other forms, typically sorry #TARGET_REF .\n sent4: Another possibility is that use of the noun form enables the writer to avoid the personal pronoun, creating a distance between the writer and the responsibility for the offence (ibid).\n sent5: In our data, individuals have not used this form at all and of the seven occurrences of the noun form, six are by individuals as representative of an organisation.\n sent6: This co-relates to Harrison's finding that the word apology/ apologies help the writers to distance themselves from the instance or event.\n",
        "output": "{\"INFORMATION\": [\"sent3\"], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Simultaneous",
                    "translation",
                    "#TARGET_REF",
                    "consists",
                    "in",
                    "generating",
                    "a",
                    "translation",
                    "before",
                    "the",
                    "source",
                    "speaker",
                    "finishes",
                    "speaking."
                ],
                [
                    "It",
                    "is",
                    "widely",
                    "used",
                    "in",
                    "many",
                    "real-time",
                    "scenarios",
                    "such",
                    "as",
                    "international",
                    "conferences,",
                    "business",
                    "negotiations",
                    "and",
                    "legal",
                    "proceedings."
                ],
                [
                    "The",
                    "challenge",
                    "of",
                    "Simultaneous",
                    "machine",
                    "translation",
                    "is",
                    "to",
                    "find",
                    "a",
                    "read-write",
                    "policy",
                    "that",
                    "balances",
                    "translation",
                    "quality",
                    "and",
                    "latency."
                ],
                [
                    "The",
                    "translation",
                    "quality",
                    "will",
                    "decline",
                    "if",
                    "the",
                    "machine",
                    "translation",
                    "system",
                    "reads",
                    "insufficient",
                    "source",
                    "information."
                ],
                [
                    "When",
                    "reading",
                    "wider",
                    "source",
                    "text,",
                    "latency",
                    "will",
                    "increase."
                ]
            ],
            "context": [
                1,
                3,
                2,
                3,
                0
            ]
        },
        "input": "sent0: Simultaneous translation #TARGET_REF consists in generating a translation before the source speaker finishes speaking.\n sent1: It is widely used in many real-time scenarios such as international conferences, business negotiations and legal proceedings.\n sent2: The challenge of Simultaneous machine translation is to find a read-write policy that balances translation quality and latency.\n sent3: The translation quality will decline if the machine translation system reads insufficient source information.\n sent4: When reading wider source text, latency will increase.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": [\"sent1\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "first",
                    "review",
                    "three",
                    "efforts",
                    "to",
                    "translate",
                    "multiword",
                    "units",
                    "by",
                    "scoring",
                    "POS-tagged",
                    "phrases",
                    "or",
                    "by",
                    "fusing",
                    "individual",
                    "target",
                    "language",
                    "words",
                    "that",
                    "appear",
                    "correlated",
                    "to",
                    "a",
                    "source",
                    "language",
                    "phrase."
                ],
                [
                    "#TARGET_REF",
                    "examined",
                    "translation",
                    "of",
                    "noun",
                    "phrases",
                    "between",
                    "English",
                    "and",
                    "French",
                    "and",
                    "reported",
                    "90%",
                    "accuracy",
                    "in",
                    "an",
                    "informal",
                    "evaluation",
                    "of",
                    "the",
                    "one",
                    "hundred",
                    "translations",
                    "that",
                    "had",
                    "the",
                    "highest",
                    "confidence",
                    "scores."
                ],
                [
                    "His",
                    "method",
                    "requires",
                    "POS-tagging",
                    "each",
                    "sentence",
                    "in",
                    "an",
                    "aligned",
                    "parallel",
                    "text",
                    "(i.e.,",
                    "both",
                    "sides",
                    "are",
                    "tagged)."
                ],
                [
                    "Then",
                    "noun",
                    "phrases",
                    "are",
                    "scored",
                    "using",
                    "an",
                    "iterative",
                    "estimation",
                    "method."
                ],
                [
                    "Kupiec",
                    "notes",
                    "several",
                    "sources",
                    "of",
                    "error",
                    "caused",
                    "by",
                    "problems",
                    "using",
                    "POS",
                    "information",
                    "instead",
                    "of",
                    "constituent",
                    "parses,",
                    "such",
                    "as",
                    "an",
                    "inability",
                    "to",
                    "infer",
                    "prepositional",
                    "phrase",
                    "attachment."
                ],
                [
                    "The",
                    "method",
                    "relies",
                    "on",
                    "having",
                    "POS",
                    "tagging",
                    "or",
                    "parsing",
                    "in",
                    "both",
                    "languages."
                ]
            ],
            "context": [
                0,
                1,
                1,
                1,
                1,
                2
            ]
        },
        "input": "sent0: We first review three efforts to translate multiword units by scoring POS-tagged phrases or by fusing individual target language words that appear correlated to a source language phrase.\n sent1: #TARGET_REF examined translation of noun phrases between English and French and reported 90% accuracy in an informal evaluation of the one hundred translations that had the highest confidence scores.\n sent2: His method requires POS-tagging each sentence in an aligned parallel text (i.e., both sides are tagged).\n sent3: Then noun phrases are scored using an iterative estimation method.\n sent4: Kupiec notes several sources of error caused by problems using POS information instead of constituent parses, such as an inability to infer prepositional phrase attachment.\n sent5: The method relies on having POS tagging or parsing in both languages.\n",
        "output": "{\"INFORMATION\": [\"sent1\", \"sent2\", \"sent3\", \"sent4\"], \"PERCEPTION\": [\"sent5\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "•",
                    "Joint",
                    "BERT",
                    "directly",
                    "utilizes",
                    "the",
                    "merit",
                    "of",
                    "pretrained",
                    "BERT",
                    "#TARGET_REF",
                    "and",
                    "nonrecursively",
                    "conducts",
                    "the",
                    "joint",
                    "prediction",
                    "over",
                    "the",
                    "#REF",
                    "token",
                    "embedding",
                    "for",
                    "intent",
                    "and",
                    "the",
                    "sequence",
                    "of",
                    "token",
                    "embeddings",
                    "for",
                    "slots."
                ]
            ],
            "context": [
                3
            ]
        },
        "input": "sent0: • Joint BERT directly utilizes the merit of pretrained BERT #TARGET_REF and nonrecursively conducts the joint prediction over the #REF token embedding for intent and the sequence of token embeddings for slots.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Irrespective",
                    "of",
                    "how",
                    "the",
                    "suffix",
                    "array",
                    "is",
                    "created,",
                    "#TARGET_REF",
                    "demonstrated",
                    "how",
                    "given",
                    "a",
                    "suffix",
                    "array,",
                    "frequencies",
                    "of",
                    "occurrence",
                    "for",
                    "all",
                    "substrings",
                    "can",
                    "be",
                    "ascertained",
                    "in",
                    "linear",
                    "time."
                ],
                [
                    "More",
                    "precisely,",
                    "by",
                    "doing",
                    "O(N)",
                    "preprocessing,",
                    "frequency",
                    "information",
                    "about",
                    "any",
                    "substring",
                    "can",
                    "be",
                    "obtained",
                    "in",
                    "O(log",
                    "N)",
                    "time."
                ],
                [
                    "Enumerating",
                    "over",
                    "all",
                    "substrings",
                    "naturally",
                    "requires",
                    "quadratic",
                    "time."
                ],
                [
                    "However,",
                    "their",
                    "technique",
                    "works",
                    "by",
                    "partitioning",
                    "substrings",
                    "into",
                    "at",
                    "most",
                    "2N",
                    "classes",
                    "which",
                    "have",
                    "unique",
                    "collection",
                    "frequency,",
                    "the",
                    "number",
                    "of",
                    "times",
                    "the",
                    "string",
                    "occurs",
                    "in",
                    "the",
                    "text,",
                    "and",
                    "document",
                    "frequency,",
                    "the",
                    "number",
                    "of",
                    "separate",
                    "documents",
                    "the",
                    "string",
                    "occurs",
                    "in."
                ],
                [
                    "(The",
                    "text",
                    "may",
                    "contain",
                    "special",
                    "end-of-document",
                    "markers.)"
                ]
            ],
            "context": [
                1,
                1,
                3,
                1,
                3
            ]
        },
        "input": "sent0: Irrespective of how the suffix array is created, #TARGET_REF demonstrated how given a suffix array, frequencies of occurrence for all substrings can be ascertained in linear time.\n sent1: More precisely, by doing O(N) preprocessing, frequency information about any substring can be obtained in O(log N) time.\n sent2: Enumerating over all substrings naturally requires quadratic time.\n sent3: However, their technique works by partitioning substrings into at most 2N classes which have unique collection frequency, the number of times the string occurs in the text, and document frequency, the number of separate documents the string occurs in.\n sent4: (The text may contain special end-of-document markers.)\n",
        "output": "{\"INFORMATION\": [\"sent0\", \"sent1\", \"sent3\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent2\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "More",
                    "recently",
                    "and",
                    "following",
                    "the",
                    "general",
                    "trend",
                    "towards",
                    "neural",
                    "approaches,",
                    "#REF",
                    "developed",
                    "a",
                    "character",
                    "aware",
                    "neural",
                    "network",
                    "model",
                    "which",
                    "attempts",
                    "to",
                    "capture",
                    "contextual",
                    "characteristics",
                    "in",
                    "Arabic",
                    "by",
                    "placing",
                    "a",
                    "CRF",
                    "on",
                    "top",
                    "of",
                    "a",
                    "Bi-LSTM."
                ],
                [
                    "This",
                    "provided",
                    "a",
                    "hard",
                    "state-of-the-art",
                    "for",
                    "other",
                    "systems",
                    "to",
                    "beat",
                    "and",
                    "provides",
                    "the",
                    "foundation",
                    "of",
                    "our",
                    "own",
                    "approach."
                ],
                [
                    "Very",
                    "recently,",
                    "#REF",
                    "applied",
                    "a",
                    "neural",
                    "network",
                    "model",
                    "with",
                    "a",
                    "multi-attention",
                    "layer",
                    "to",
                    "extract",
                    "Arabic",
                    "NEs."
                ],
                [
                    "They",
                    "used",
                    "two",
                    "attention",
                    "units,",
                    "the",
                    "embedding",
                    "attention",
                    "layer,",
                    "and",
                    "the",
                    "self-attention",
                    "unit."
                ],
                [
                    "They",
                    "achieved",
                    "an",
                    "F1",
                    "score",
                    "of",
                    "91.31",
                    "to",
                    "achieve",
                    "a",
                    "new",
                    "stateof-the-art",
                    "on",
                    "a",
                    "large",
                    "dataset",
                    "proposed",
                    "for",
                    "evaluation",
                    "in",
                    "the",
                    "same",
                    "work."
                ],
                [
                    "At",
                    "the",
                    "same",
                    "time,",
                    "#REF",
                    "used",
                    "character",
                    "Convolutional",
                    "Neural",
                    "Networks",
                    "(CNN)",
                    "as",
                    "a",
                    "replacement",
                    "for",
                    "characterlevel",
                    "bidirectional",
                    "Long",
                    "Short-Term",
                    "Memory",
                    "(LSTM)",
                    "in",
                    "Arabic",
                    "NER."
                ],
                [
                    "Their",
                    "proposed",
                    "system",
                    "was",
                    "able",
                    "to",
                    "outperform",
                    "the",
                    "state-of-art",
                    "systems,",
                    "including",
                    "character-level",
                    "Bi-LSTM",
                    "on",
                    "various",
                    "standard",
                    "Arabic",
                    "NER",
                    "corpora."
                ],
                [
                    "#REF",
                    "proposed",
                    "AraBERTv0.1",
                    "which",
                    "involves",
                    "pretraining",
                    "the",
                    "BERT",
                    "transformer",
                    "model",
                    "for",
                    "the",
                    "Arabic",
                    "language."
                ],
                [
                    "They",
                    "compared",
                    "AraBERTv0.1",
                    "and",
                    "the",
                    "Bi-LSTM-CRF",
                    "model",
                    "on",
                    "ANERCorp,",
                    "the",
                    "former",
                    "achieved",
                    "84.2",
                    "F1",
                    "scores",
                    "whereas",
                    "the",
                    "later",
                    "achieved",
                    "81.7."
                ],
                [
                    "Most",
                    "recently,",
                    "#TARGET_REF",
                    "proposed",
                    "a",
                    "transfer",
                    "learning",
                    "approach",
                    "for",
                    "Arabic",
                    "NER",
                    "with",
                    "Deep",
                    "Neural",
                    "Networks",
                    "where",
                    "they",
                    "showed",
                    "that",
                    "their",
                    "model",
                    "outperformed",
                    "significantly",
                    "the",
                    "Bi-LSTM-CRF",
                    "model."
                ],
                [
                    "We",
                    "have",
                    "not",
                    "considered",
                    "this",
                    "approach",
                    "here",
                    "because",
                    "our",
                    "aim",
                    "is",
                    "not",
                    "to",
                    "create",
                    "a",
                    "new",
                    "state",
                    "of",
                    "the",
                    "art",
                    "model",
                    "but",
                    "to",
                    "show",
                    "the",
                    "effectiveness",
                    "of",
                    "incorporating",
                    "language",
                    "specific",
                    "characteristics",
                    "in",
                    "the",
                    "form",
                    "of",
                    "embeddings."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                1,
                2
            ]
        },
        "input": "sent0: More recently and following the general trend towards neural approaches, #REF developed a character aware neural network model which attempts to capture contextual characteristics in Arabic by placing a CRF on top of a Bi-LSTM.\n sent1: This provided a hard state-of-the-art for other systems to beat and provides the foundation of our own approach.\n sent2: Very recently, #REF applied a neural network model with a multi-attention layer to extract Arabic NEs.\n sent3: They used two attention units, the embedding attention layer, and the self-attention unit.\n sent4: They achieved an F1 score of 91.31 to achieve a new stateof-the-art on a large dataset proposed for evaluation in the same work.\n sent5: At the same time, #REF used character Convolutional Neural Networks (CNN) as a replacement for characterlevel bidirectional Long Short-Term Memory (LSTM) in Arabic NER.\n sent6: Their proposed system was able to outperform the state-of-art systems, including character-level Bi-LSTM on various standard Arabic NER corpora.\n sent7: #REF proposed AraBERTv0.1 which involves pretraining the BERT transformer model for the Arabic language.\n sent8: They compared AraBERTv0.1 and the Bi-LSTM-CRF model on ANERCorp, the former achieved 84.2 F1 scores whereas the later achieved 81.7.\n sent9: Most recently, #TARGET_REF proposed a transfer learning approach for Arabic NER with Deep Neural Networks where they showed that their model outperformed significantly the Bi-LSTM-CRF model.\n sent10: We have not considered this approach here because our aim is not to create a new state of the art model but to show the effectiveness of incorporating language specific characteristics in the form of embeddings.\n",
        "output": "{\"INFORMATION\": [\"sent9\"], \"PERCEPTION\": [\"sent10\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "built",
                    "our",
                    "HPB",
                    "baseline",
                    "using",
                    "the",
                    "Moses",
                    "Chart",
                    "Decoder",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Continuous",
                    "phrases",
                    "are",
                    "extracted",
                    "according",
                    "to",
                    "the",
                    "phrase",
                    "based",
                    "system",
                    "settings",
                    "explained",
                    "in",
                    "Section",
                    "3.1."
                ],
                [
                    "Maximum",
                    "phrase",
                    "length",
                    "and",
                    "maximum",
                    "rule",
                    "span",
                    "are",
                    "both",
                    "set",
                    "to",
                    "12",
                    "words."
                ],
                [
                    "The",
                    "maximum",
                    "span",
                    "for",
                    "the",
                    "chart",
                    "during",
                    "decoding",
                    "is",
                    "set",
                    "to",
                    "20",
                    "words,",
                    "above",
                    "which",
                    "only",
                    "monotone",
                    "concatenation",
                    "of",
                    "phrases",
                    "is",
                    "used."
                ],
                [
                    "Rules",
                    "extracted",
                    "contain",
                    "up",
                    "to",
                    "2",
                    "non-terminals."
                ],
                [
                    "Adjacent",
                    "non-terminals",
                    "on",
                    "the",
                    "source",
                    "side",
                    "are",
                    "not",
                    "allowed."
                ]
            ],
            "context": [
                2,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: We built our HPB baseline using the Moses Chart Decoder #TARGET_REF .\n sent1: Continuous phrases are extracted according to the phrase based system settings explained in Section 3.1.\n sent2: Maximum phrase length and maximum rule span are both set to 12 words.\n sent3: The maximum span for the chart during decoding is set to 20 words, above which only monotone concatenation of phrases is used.\n sent4: Rules extracted contain up to 2 non-terminals.\n sent5: Adjacent non-terminals on the source side are not allowed.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "the",
                    "domain",
                    "of",
                    "multi-lingual",
                    "and",
                    "cross",
                    "lingual",
                    "event",
                    "detection,",
                    "#REF",
                    "uses",
                    "a",
                    "combination",
                    "of",
                    "both",
                    "LSTMs",
                    "and",
                    "CNNs",
                    "for",
                    "creating",
                    "a",
                    "language",
                    "independent",
                    "architecture",
                    "for",
                    "capturing",
                    "events,",
                    "while",
                    "#TARGET_REF",
                    "used",
                    "stacked",
                    "RNNs",
                    "for",
                    "sequence",
                    "labeling",
                    "and",
                    "a",
                    "language",
                    "discriminator",
                    "to",
                    "learn",
                    "language",
                    "features."
                ],
                [
                    "The",
                    "latter",
                    "architecture",
                    "implements",
                    "the",
                    "use",
                    "of",
                    "the",
                    "character",
                    "embeddings,",
                    "but",
                    "does",
                    "not",
                    "identify",
                    "the",
                    "relevant",
                    "features",
                    "independent",
                    "of",
                    "the",
                    "word",
                    "embeddings."
                ]
            ],
            "context": [
                3,
                3
            ]
        },
        "input": "sent0: In the domain of multi-lingual and cross lingual event detection, #REF uses a combination of both LSTMs and CNNs for creating a language independent architecture for capturing events, while #TARGET_REF used stacked RNNs for sequence labeling and a language discriminator to learn language features.\n sent1: The latter architecture implements the use of the character embeddings, but does not identify the relevant features independent of the word embeddings.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "This",
                    "paper",
                    "describes",
                    "our",
                    "submission",
                    "to",
                    "SemEval-2021",
                    "Task",
                    "2",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Our",
                    "approach",
                    "is",
                    "mainly",
                    "focused",
                    "on",
                    "transformer-based",
                    "models",
                    "with",
                    "different",
                    "text",
                    "pair",
                    "classification",
                    "architectures."
                ],
                [
                    "We",
                    "remodel",
                    "the",
                    "default",
                    "text",
                    "pair",
                    "classification",
                    "architecture",
                    "and",
                    "introduce",
                    "several",
                    "strategies",
                    "that",
                    "outperform",
                    "the",
                    "default",
                    "text",
                    "pair",
                    "classification",
                    "architecture",
                    "for",
                    "this",
                    "task."
                ],
                [
                    "For",
                    "effortless",
                    "generalisation",
                    "across",
                    "the",
                    "languages,",
                    "we",
                    "do",
                    "not",
                    "use",
                    "any",
                    "language-specific",
                    "processing",
                    "and",
                    "resources."
                ],
                [
                    "In",
                    "the",
                    "subtasks",
                    "where",
                    "only",
                    "a",
                    "few",
                    "training",
                    "instances",
                    "were",
                    "available,",
                    "we",
                    "use",
                    "few-shot",
                    "learning",
                    "and",
                    "in",
                    "the",
                    "subtasks",
                    "where",
                    "there",
                    "were",
                    "no",
                    "training",
                    "instances",
                    "were",
                    "available,",
                    "we",
                    "use",
                    "zero-shot",
                    "learning",
                    "taking",
                    "advantage",
                    "of",
                    "the",
                    "cross-lingual",
                    "nature",
                    "of",
                    "the",
                    "multilingual",
                    "transformer",
                    "models."
                ]
            ],
            "context": [
                1,
                3,
                3,
                0,
                0
            ]
        },
        "input": "sent0: This paper describes our submission to SemEval-2021 Task 2 #TARGET_REF .\n sent1: Our approach is mainly focused on transformer-based models with different text pair classification architectures.\n sent2: We remodel the default text pair classification architecture and introduce several strategies that outperform the default text pair classification architecture for this task.\n sent3: For effortless generalisation across the languages, we do not use any language-specific processing and resources.\n sent4: In the subtasks where only a few training instances were available, we use few-shot learning and in the subtasks where there were no training instances were available, we use zero-shot learning taking advantage of the cross-lingual nature of the multilingual transformer models.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Early",
                    "studies",
                    "of",
                    "conditioned",
                    "response",
                    "generation",
                    "focus",
                    "on",
                    "enriching",
                    "the",
                    "meaning",
                    "representations",
                    "in",
                    "task-oriented",
                    "dialogues,",
                    "e.g.,",
                    "utilizing",
                    "graph",
                    "structures",
                    "and",
                    "hierarchies",
                    "among",
                    "actions",
                    "#REF",
                    ",",
                    "decomposing",
                    "into",
                    "fine-grained",
                    "actions",
                    "#REF",
                    ",",
                    "or",
                    "encoding",
                    "syntax",
                    "attributes",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Since",
                    "these",
                    "approaches",
                    "often",
                    "assume",
                    "expensive",
                    "action",
                    "annotations,",
                    "recent",
                    "years",
                    "have",
                    "seen",
                    "a",
                    "growing",
                    "interest",
                    "in",
                    "learning",
                    "latent",
                    "actions",
                    "in",
                    "an",
                    "unsupervised",
                    "way",
                    "#REF",
                    "."
                ],
                [
                    "These",
                    "approaches",
                    "build",
                    "on",
                    "either",
                    "adversarial",
                    "learning",
                    "#REF",
                    "or",
                    "variational",
                    "inference",
                    "(Kingma",
                    "and",
                    "Welling,",
                    "2014)",
                    "and",
                    "encode",
                    "all",
                    "system",
                    "utterances",
                    "via",
                    "a",
                    "self-reconstruction",
                    "task",
                    "or",
                    "distant",
                    "supervision",
                    "#REF",
                    "."
                ],
                [
                    "Due",
                    "to",
                    "their",
                    "implicit",
                    "nature,",
                    "latent",
                    "actions",
                    "are",
                    "difficult",
                    "to",
                    "generalize,",
                    "and",
                    "we",
                    "aim",
                    "to",
                    "overcome",
                    "this",
                    "limitation",
                    "by",
                    "learning",
                    "explicit",
                    "action",
                    "representations."
                ]
            ],
            "context": [
                3,
                2,
                0,
                0
            ]
        },
        "input": "sent0: Early studies of conditioned response generation focus on enriching the meaning representations in task-oriented dialogues, e.g., utilizing graph structures and hierarchies among actions #REF , decomposing into fine-grained actions #REF , or encoding syntax attributes #TARGET_REF .\n sent1: Since these approaches often assume expensive action annotations, recent years have seen a growing interest in learning latent actions in an unsupervised way #REF .\n sent2: These approaches build on either adversarial learning #REF or variational inference (Kingma and Welling, 2014) and encode all system utterances via a self-reconstruction task or distant supervision #REF .\n sent3: Due to their implicit nature, latent actions are difficult to generalize, and we aim to overcome this limitation by learning explicit action representations.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "At",
                    "the",
                    "sentence",
                    "level,",
                    "we",
                    "pick",
                    "the",
                    "top",
                    "1",
                    "sentence,",
                    "using",
                    "Okapi",
                    "BM25,",
                    "as",
                    "the",
                    "gold",
                    "standard."
                ],
                [
                    "To",
                    "retrieve",
                    "the",
                    "top",
                    "1",
                    "sentence",
                    "using",
                    "Okapi",
                    "BM25,",
                    "we",
                    "used",
                    "the",
                    "question",
                    "as",
                    "the",
                    "query",
                    "and",
                    "the",
                    "product",
                    "reviews",
                    "as",
                    "the",
                    "documents."
                ],
                [
                    "Okapi",
                    "BM25",
                    "is",
                    "still",
                    "widely",
                    "used",
                    "as",
                    "a",
                    "benchmark",
                    "in",
                    "similar",
                    "tasks",
                    "#REF",
                    ")."
                ],
                [
                    "An",
                    "advantage",
                    "of",
                    "using",
                    "the",
                    "Okapi",
                    "BM25",
                    "is",
                    "that",
                    "it",
                    "provides",
                    "us",
                    "with",
                    "a",
                    "tf-idf",
                    "based",
                    "benchmark",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Word",
                    "vectors",
                    "aim",
                    "to",
                    "reduce",
                    "problem",
                    "complexity",
                    "by",
                    "moving",
                    "away",
                    "from",
                    "tf-idf",
                    "methods",
                    "which",
                    "requires",
                    "us",
                    "to",
                    "one-hot-encode",
                    "the",
                    "entire",
                    "vocabulary."
                ]
            ],
            "context": [
                0,
                0,
                3,
                2,
                3
            ]
        },
        "input": "sent0: At the sentence level, we pick the top 1 sentence, using Okapi BM25, as the gold standard.\n sent1: To retrieve the top 1 sentence using Okapi BM25, we used the question as the query and the product reviews as the documents.\n sent2: Okapi BM25 is still widely used as a benchmark in similar tasks #REF ).\n sent3: An advantage of using the Okapi BM25 is that it provides us with a tf-idf based benchmark #TARGET_REF .\n sent4: Word vectors aim to reduce problem complexity by moving away from tf-idf methods which requires us to one-hot-encode the entire vocabulary.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent3\"], \"BACKGROUND\": [\"sent2\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Evaluation",
                    "Metrics:",
                    "Historically,",
                    "performance",
                    "on",
                    "the",
                    "explanation",
                    "regeneration",
                    "task",
                    "was",
                    "evaluated",
                    "using",
                    "Mean",
                    "Average",
                    "Precision",
                    "(MAP)",
                    ",",
                    "using",
                    "the",
                    "binary",
                    "ratings",
                    "(gold",
                    "or",
                    "not",
                    "gold)",
                    "associated",
                    "with",
                    "each",
                    "fact",
                    "for",
                    "a",
                    "given",
                    "explanation."
                ],
                [
                    "To",
                    "leverage",
                    "the",
                    "new",
                    "graded",
                    "annotation",
                    "schema,",
                    "here",
                    "we",
                    "switch",
                    "to",
                    "evaluate",
                    "system",
                    "performance",
                    "using",
                    "Normalized",
                    "Discounted",
                    "Cumulative",
                    "Gain",
                    "(NDCG)",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                0,
                0
            ]
        },
        "input": "sent0: Evaluation Metrics: Historically, performance on the explanation regeneration task was evaluated using Mean Average Precision (MAP) , using the binary ratings (gold or not gold) associated with each fact for a given explanation.\n sent1: To leverage the new graded annotation schema, here we switch to evaluate system performance using Normalized Discounted Cumulative Gain (NDCG) #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "However,",
                    "event",
                    "detection",
                    "as",
                    "a",
                    "problem",
                    "shifts",
                    "when",
                    "we",
                    "move",
                    "away",
                    "from",
                    "the",
                    "annotation",
                    "paradigm",
                    "of",
                    "datasets",
                    "such",
                    "as",
                    "ACE",
                    "#TARGET_REF",
                    "and",
                    "TAC",
                    "KBP",
                    "#REF",
                    "to",
                    "TimeML",
                    "datasets",
                    "such",
                    "as",
                    "TimeBank",
                    "#REF",
                    ",",
                    "which",
                    "are",
                    "used",
                    "in",
                    "this",
                    "paper."
                ],
                [
                    "There",
                    "has",
                    "been",
                    "limited",
                    "use",
                    "of",
                    "deep",
                    "learning",
                    "methods",
                    "on",
                    "TimeBanks",
                    "due",
                    "to",
                    "fewer",
                    "event",
                    "mentions",
                    "and",
                    "a",
                    "need",
                    "for",
                    "data",
                    "augmentation",
                    "and",
                    "bootstrapping."
                ],
                [
                    "However,",
                    "in",
                    "this",
                    "paper,",
                    "we",
                    "show",
                    "that",
                    "using",
                    "subword",
                    "level",
                    "information,",
                    "a",
                    "language",
                    "invariant",
                    "deep",
                    "learning",
                    "model",
                    "can",
                    "provide",
                    "similar",
                    "event",
                    "detection",
                    "accuracies",
                    "as",
                    "heavily",
                    "feature",
                    "engineered",
                    "language",
                    "specific",
                    "statistical",
                    "methods",
                    "without",
                    "using",
                    "any",
                    "augmented",
                    "data."
                ]
            ],
            "context": [
                2,
                0,
                0
            ]
        },
        "input": "sent0: However, event detection as a problem shifts when we move away from the annotation paradigm of datasets such as ACE #TARGET_REF and TAC KBP #REF to TimeML datasets such as TimeBank #REF , which are used in this paper.\n sent1: There has been limited use of deep learning methods on TimeBanks due to fewer event mentions and a need for data augmentation and bootstrapping.\n sent2: However, in this paper, we show that using subword level information, a language invariant deep learning model can provide similar event detection accuracies as heavily feature engineered language specific statistical methods without using any augmented data.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "SemEval-2022",
                    "task",
                    "2b",
                    "#TARGET_REF",
                    "encourages",
                    "the",
                    "creation",
                    "of",
                    "better",
                    "representations",
                    "of",
                    "idiomatic",
                    "expressions",
                    "across",
                    "multiple",
                    "languages",
                    "by",
                    "presenting",
                    "a",
                    "Semantic",
                    "Text",
                    "Similarity",
                    "(STS)",
                    "task",
                    "in",
                    "which",
                    "correct",
                    "STS",
                    "scores",
                    "are",
                    "required",
                    "whether",
                    "or",
                    "not",
                    "either",
                    "sentence",
                    "contains",
                    "an",
                    "idiomatic",
                    "expression."
                ],
                [
                    "The",
                    "sub-task",
                    "requires",
                    "the",
                    "creation",
                    "of",
                    "a",
                    "self-consistent",
                    "model",
                    "in",
                    "which",
                    "a",
                    "sentence",
                    "including",
                    "an",
                    "idiomatic",
                    "expression",
                    "and",
                    "one",
                    "containing",
                    "its",
                    "literal",
                    "meaning",
                    "('swan",
                    "song'",
                    "and",
                    "'final",
                    "performance')",
                    "are",
                    "exactly",
                    "similar",
                    "to",
                    "each",
                    "other",
                    "and",
                    "equally",
                    "similar",
                    "to",
                    "any",
                    "other",
                    "sentence."
                ]
            ],
            "context": [
                1,
                2
            ]
        },
        "input": "sent0: SemEval-2022 task 2b #TARGET_REF encourages the creation of better representations of idiomatic expressions across multiple languages by presenting a Semantic Text Similarity (STS) task in which correct STS scores are required whether or not either sentence contains an idiomatic expression.\n sent1: The sub-task requires the creation of a self-consistent model in which a sentence including an idiomatic expression and one containing its literal meaning ('swan song' and 'final performance') are exactly similar to each other and equally similar to any other sentence.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "number",
                    "of",
                    "unique",
                    "label",
                    "combinations",
                    "is",
                    "147,",
                    "including",
                    "single-label."
                ],
                [
                    "The",
                    "most",
                    "common",
                    "label",
                    "combinations",
                    "beyond",
                    "single-label",
                    "are",
                    "anger",
                    "with",
                    "disgust",
                    "(2.4%)",
                    "and",
                    "joy",
                    "with",
                    "trust",
                    "(2.1%)",
                    "followed",
                    "by",
                    "different",
                    "combinations",
                    "of",
                    "the",
                    "positive",
                    "emotions",
                    "of",
                    "anticipation,",
                    "joy,",
                    "and",
                    "trust."
                ],
                [
                    "These",
                    "findings",
                    "are",
                    "in",
                    "line",
                    "with",
                    "previous",
                    "findings",
                    "discussing",
                    "overlapping",
                    "categories",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "However,",
                    "these",
                    "are",
                    "followed",
                    "by",
                    "anger",
                    "combined",
                    "with",
                    "anticipation",
                    "and",
                    "sadness",
                    "with",
                    "surprise."
                ],
                [
                    "The",
                    "first",
                    "combination",
                    "is",
                    "possibly",
                    "a",
                    "reflection",
                    "of",
                    "the",
                    "genre,",
                    "as",
                    "a",
                    "common",
                    "theme",
                    "for",
                    "anger",
                    "with",
                    "anticipation",
                    "is",
                    "threats."
                ],
                [
                    "The",
                    "combination",
                    "of",
                    "surprise",
                    "with",
                    "negative",
                    "emotions",
                    "(anger,",
                    "disgust,",
                    "fear,",
                    "sadness)",
                    "is",
                    "much",
                    "more",
                    "common",
                    "than",
                    "a",
                    "combination",
                    "with",
                    "positive",
                    "emotions."
                ]
            ],
            "context": [
                0,
                3,
                2,
                0,
                0,
                0
            ]
        },
        "input": "sent0: The number of unique label combinations is 147, including single-label.\n sent1: The most common label combinations beyond single-label are anger with disgust (2.4%) and joy with trust (2.1%) followed by different combinations of the positive emotions of anticipation, joy, and trust.\n sent2: These findings are in line with previous findings discussing overlapping categories #TARGET_REF .\n sent3: However, these are followed by anger combined with anticipation and sadness with surprise.\n sent4: The first combination is possibly a reflection of the genre, as a common theme for anger with anticipation is threats.\n sent5: The combination of surprise with negative emotions (anger, disgust, fear, sadness) is much more common than a combination with positive emotions.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Models."
                ],
                [
                    "We",
                    "use",
                    "the",
                    "attention",
                    "scores",
                    "over",
                    "BERT",
                    "#TARGET_REF",
                    "based",
                    "classification",
                    "models",
                    "as",
                    "they",
                    "have",
                    "achieved",
                    "the",
                    "state-of-art",
                    "performance."
                ],
                [
                    "Note",
                    "that",
                    "our",
                    "proposed",
                    "framework",
                    "can",
                    "also",
                    "be",
                    "easily",
                    "extended",
                    "to",
                    "models",
                    "with",
                    "different",
                    "architectures."
                ]
            ],
            "context": [
                0,
                2,
                2
            ]
        },
        "input": "sent0: Models.\n sent1: We use the attention scores over BERT #TARGET_REF based classification models as they have achieved the state-of-art performance.\n sent2: Note that our proposed framework can also be easily extended to models with different architectures.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\", \"sent2\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "To",
                    "examine",
                    "the",
                    "performance",
                    "of",
                    "these",
                    "models",
                    "on",
                    "real-world",
                    "downstream",
                    "task",
                    "setting,",
                    "we",
                    "consider",
                    "the",
                    "classification",
                    "task."
                ],
                [
                    "For",
                    "our",
                    "classification",
                    "datasets,",
                    "we",
                    "use",
                    "DBpedia",
                    "(14",
                    "classes)",
                    "and",
                    "Yahoo",
                    "Question",
                    "(10",
                    "classes)",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Each",
                    "class",
                    "of",
                    "these",
                    "two",
                    "datasets",
                    "has",
                    "(10k,",
                    "1k,",
                    "1k)",
                    "randomly",
                    "chosen",
                    "sentences",
                    "in",
                    "(train,",
                    "dev,",
                    "test)",
                    "sets."
                ],
                [
                    "We",
                    "train",
                    "Vanilla-VAE,",
                    "β-VAE",
                    "(β",
                    "=",
                    "0.2),",
                    "CCI-VAE",
                    "(C",
                    "=",
                    "10),",
                    "and",
                    "MAT-VAE",
                    "(β",
                    "=",
                    "0.01,",
                    "λ",
                    "=",
                    "0.1)",
                    "from",
                    "Table",
                    "3",
                    "on",
                    "DBpedia",
                    "and",
                    "Yahoo",
                    "(without",
                    "the",
                    "labels),",
                    "then",
                    "freeze",
                    "the",
                    "trained",
                    "encoders",
                    "and",
                    "place",
                    "a",
                    "classifier",
                    "on",
                    "top",
                    "to",
                    "use",
                    "the",
                    "mean",
                    "vector",
                    "representations",
                    "from",
                    "the",
                    "encoder",
                    "as",
                    "a",
                    "feature",
                    "to",
                    "train",
                    "a",
                    "classifier."
                ]
            ],
            "context": [
                3,
                1,
                2,
                0
            ]
        },
        "input": "sent0: To examine the performance of these models on real-world downstream task setting, we consider the classification task.\n sent1: For our classification datasets, we use DBpedia (14 classes) and Yahoo Question (10 classes) #TARGET_REF .\n sent2: Each class of these two datasets has (10k, 1k, 1k) randomly chosen sentences in (train, dev, test) sets.\n sent3: We train Vanilla-VAE, β-VAE (β = 0.2), CCI-VAE (C = 10), and MAT-VAE (β = 0.01, λ = 0.1) from Table 3 on DBpedia and Yahoo (without the labels), then freeze the trained encoders and place a classifier on top to use the mean vector representations from the encoder as a feature to train a classifier.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Algorithm",
                    "1",
                    "Metric",
                    "of",
                    "#TARGET_REF",
                    "1",
                    ":",
                    "D",
                    "=",
                    "∅",
                    "2:",
                    "for",
                    "f",
                    "i",
                    "∈",
                    "F",
                    "do",
                    "3:",
                    "for",
                    "n",
                    "=",
                    "1,",
                    "2,",
                    "."
                ],
                [
                    "."
                ],
                [
                    "."
                ],
                [
                    ",",
                    "N",
                    "do",
                    "4:",
                    "Sample",
                    "s",
                    "n",
                    "from",
                    "j",
                    "S",
                    "ij",
                    "5:",
                    "Find",
                    "the",
                    "value",
                    "v",
                    "ij",
                    "on",
                    "f",
                    "i",
                    "for",
                    "s",
                    "n",
                    "6:",
                    "Sample",
                    "(z",
                    "(1)",
                    "1",
                    ",",
                    "."
                ],
                [
                    "."
                ],
                [
                    "."
                ],
                [
                    ",",
                    "z",
                    "(1)",
                    "L",
                    ")",
                    "from",
                    "R",
                    "ij",
                    "7:",
                    "Sample",
                    "(z",
                    "(2)",
                    "1",
                    ",",
                    "."
                ],
                [
                    "."
                ],
                [
                    "."
                ],
                [
                    ",",
                    "z",
                    "(2)",
                    "L",
                    ")",
                    "from",
                    "R",
                    "ij",
                    "8:",
                    "z",
                    "n",
                    "=",
                    "1",
                    "L",
                    "L",
                    "l=1",
                    "|z",
                    "(1)",
                    "l",
                    "−",
                    "z",
                    "(2)",
                    "l",
                    "|",
                    "9:",
                    "D",
                    "=",
                    "{(z",
                    "n",
                    ",",
                    "f",
                    "i",
                    ")}",
                    "D",
                    "10:",
                    "Split",
                    "D",
                    "into"
                ]
            ],
            "context": [
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1
            ]
        },
        "input": "sent0: Algorithm 1 Metric of #TARGET_REF 1 : D = ∅ 2: for f i ∈ F do 3: for n = 1, 2, .\n sent1: .\n sent2: .\n sent3: , N do 4: Sample s n from j S ij 5: Find the value v ij on f i for s n 6: Sample (z (1) 1 , .\n sent4: .\n sent5: .\n sent6: , z (1) L ) from R ij 7: Sample (z (2) 1 , .\n sent7: .\n sent8: .\n sent9: , z (2) L ) from R ij 8: z n = 1 L L l=1 |z (1) l − z (2) l | 9: D = {(z n , f i )} D 10: Split D into\n",
        "output": "{\"INFORMATION\": [\"sent0\", \"sent1\", \"sent2\", \"sent3\", \"sent4\", \"sent5\", \"sent6\", \"sent7\", \"sent8\", \"sent9\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Naturally,",
                    "the",
                    "WMT",
                    "Metrics",
                    "task,",
                    "most",
                    "recently",
                    "run",
                    "in",
                    "2020",
                    "#TARGET_REF",
                    ")",
                    "is",
                    "one",
                    "such",
                    "forum",
                    "for",
                    "the",
                    "evaluation",
                    "of",
                    "metrics."
                ],
                [
                    "In",
                    "this",
                    "last",
                    "iteration,",
                    "metrics",
                    "were",
                    "evaluated",
                    "based",
                    "on",
                    "their",
                    "correlation",
                    "with",
                    "human",
                    "judgment",
                    "scores",
                    "on",
                    "the",
                    "sentence,",
                    "paragraph,",
                    "and",
                    "document",
                    "level."
                ],
                [
                    "BERTScore",
                    "was",
                    "not",
                    "included",
                    "even",
                    "in",
                    "the",
                    "most",
                    "recent",
                    "iteration",
                    "of",
                    "the",
                    "metrics",
                    "task."
                ]
            ],
            "context": [
                1,
                3,
                3
            ]
        },
        "input": "sent0: Naturally, the WMT Metrics task, most recently run in 2020 #TARGET_REF ) is one such forum for the evaluation of metrics.\n sent1: In this last iteration, metrics were evaluated based on their correlation with human judgment scores on the sentence, paragraph, and document level.\n sent2: BERTScore was not included even in the most recent iteration of the metrics task.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "This",
                    "paper",
                    "examines",
                    "the",
                    "question",
                    "of",
                    "differences",
                    "between",
                    "a",
                    "traditional",
                    "interlingua",
                    "approach",
                    "and",
                    "a",
                    "transferbased",
                    "approach",
                    "that",
                    "uses",
                    "cross-linguistic",
                    "semantic",
                    "features",
                    "to",
                    "generalize",
                    "its",
                    "transfer",
                    "lexicon",
                    "entries,",
                    "and",
                    "concludes",
                    "that",
                    "the",
                    "two",
                    "approaches",
                    "share",
                    "a",
                    "common",
                    "interest",
                    "in",
                    "lexical",
                    "classifications",
                    "that",
                    "can",
                    "be",
                    "distinguished",
                    "by",
                    "cross-linguistic",
                    "semantic",
                    "features."
                ],
                [
                    "The",
                    "paper",
                    "goes",
                    "on",
                    "to",
                    "discuss",
                    "current",
                    "approaches",
                    "to",
                    "English",
                    "classification,",
                    "Levin",
                    "classes",
                    "#REF",
                    "and",
                    "WordNet",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "We",
                    "present",
                    "a",
                    "refinement",
                    "of",
                    "Levin",
                    "classes",
                    "-Intersective",
                    "Classes",
                    "-that",
                    "shows",
                    "interesting",
                    "correlations",
                    "to",
                    "WordNet",
                    "and",
                    "that",
                    "makes",
                    "more",
                    "explicit",
                    "the",
                    "semantic",
                    "components",
                    "that",
                    "serve",
                    "to",
                    "distinguish",
                    "different",
                    "classes."
                ]
            ],
            "context": [
                0,
                2,
                0
            ]
        },
        "input": "sent0: This paper examines the question of differences between a traditional interlingua approach and a transferbased approach that uses cross-linguistic semantic features to generalize its transfer lexicon entries, and concludes that the two approaches share a common interest in lexical classifications that can be distinguished by cross-linguistic semantic features.\n sent1: The paper goes on to discuss current approaches to English classification, Levin classes #REF and WordNet #TARGET_REF .\n sent2: We present a refinement of Levin classes -Intersective Classes -that shows interesting correlations to WordNet and that makes more explicit the semantic components that serve to distinguish different classes.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "When",
                    "the",
                    "parsing",
                    "is",
                    "done,",
                    "a",
                    "V-MARKER",
                    "is",
                    "passed",
                    "to",
                    "the",
                    "target",
                    "language",
                    "(Japanese)",
                    "expression",
                    "WANT-CIRCUM-J",
                    "from",
                    "WANT-CIRCUM,",
                    "and,",
                    "then,",
                    "to",
                    "the",
                    "first",
                    "CSE",
                    "of",
                    "WANT-CIRCUM-J."
                ],
                [
                    "Since",
                    "the",
                    "first",
                    "CSE",
                    "has",
                    "a",
                    "G-MARKER",
                    "pointing",
                    "to",
                    "JON,",
                    "'jon'",
                    "becomes",
                    "the",
                    "first",
                    "word",
                    "in",
                    "the",
                    "translated",
                    "Japanese",
                    "sentence",
                    "and",
                    "then",
                    "the",
                    "V-MARKER",
                    "is",
                    "passed",
                    "to",
                    "the",
                    "next",
                    "CSE."
                ],
                [
                    "See",
                    "#TARGET_REF",
                    "for",
                    "the",
                    "details",
                    "of",
                    "generation",
                    "process."
                ],
                [
                    "This",
                    "operation",
                    "is",
                    "repealed",
                    "for",
                    "all",
                    "CSEs",
                    "in",
                    "the",
                    "CSC."
                ],
                [
                    "Finally,",
                    "the",
                    "Japanese",
                    "sentence",
                    "tl",
                    "is",
                    "constructed",
                    "for",
                    "the",
                    "English",
                    "sentence",
                    "s1."
                ]
            ],
            "context": [
                3,
                3,
                1,
                3,
                3
            ]
        },
        "input": "sent0: When the parsing is done, a V-MARKER is passed to the target language (Japanese) expression WANT-CIRCUM-J from WANT-CIRCUM, and, then, to the first CSE of WANT-CIRCUM-J.\n sent1: Since the first CSE has a G-MARKER pointing to JON, 'jon' becomes the first word in the translated Japanese sentence and then the V-MARKER is passed to the next CSE.\n sent2: See #TARGET_REF for the details of generation process.\n sent3: This operation is repealed for all CSEs in the CSC.\n sent4: Finally, the Japanese sentence tl is constructed for the English sentence s1.\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\", \"sent1\", \"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "•",
                    "CNN-Text",
                    "#TARGET_REF",
                    "is",
                    "the",
                    "use",
                    "of",
                    "#REF",
                    "network",
                    "on",
                    "word",
                    "embeddings",
                    "to",
                    "perform",
                    "the",
                    "classification",
                    "tasks."
                ]
            ],
            "context": [
                1
            ]
        },
        "input": "sent0: • CNN-Text #TARGET_REF is the use of #REF network on word embeddings to perform the classification tasks.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "participate",
                    "in",
                    "the",
                    "Chinese-English",
                    "streaming",
                    "transcription",
                    "track",
                    ",",
                    "where",
                    "each",
                    "sentence",
                    "is",
                    "broken",
                    "into",
                    "lines",
                    "whose",
                    "length",
                    "is",
                    "incremented",
                    "by",
                    "one",
                    "word",
                    "until",
                    "the",
                    "sentence",
                    "is",
                    "completed."
                ],
                [
                    "An",
                    "example",
                    "is",
                    "shown",
                    "in",
                    "Table",
                    "1."
                ],
                [
                    "For",
                    "pre-training,",
                    "we",
                    "use",
                    "the",
                    "CWMT21",
                    "parallel",
                    "corpus",
                    "(9.1M)",
                    "2",
                    ",",
                    "and",
                    "we",
                    "fine-tune",
                    "the",
                    "pretrained",
                    "model",
                    "using",
                    "transcription",
                    "and",
                    "translation",
                    "of",
                    "the",
                    "BSTC",
                    "(Baidu",
                    "Speech",
                    "Translation",
                    "Corpus,37K)",
                    "#TARGET_REF",
                    ",",
                    "shown",
                    "in",
                    "Table",
                    "2."
                ],
                [
                    "We",
                    "also",
                    "use",
                    "CWMT's",
                    "10M",
                    "Chinese",
                    "monolingual",
                    "data",
                    "for",
                    "synthetic",
                    "data",
                    "generation."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: We participate in the Chinese-English streaming transcription track , where each sentence is broken into lines whose length is incremented by one word until the sentence is completed.\n sent1: An example is shown in Table 1.\n sent2: For pre-training, we use the CWMT21 parallel corpus (9.1M) 2 , and we fine-tune the pretrained model using transcription and translation of the BSTC (Baidu Speech Translation Corpus,37K) #TARGET_REF , shown in Table 2.\n sent3: We also use CWMT's 10M Chinese monolingual data for synthetic data generation.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Cross-batch",
                    "negatives",
                    "3",
                    "The",
                    "cross-batch",
                    "negative",
                    "sampling",
                    "is",
                    "implemented",
                    "with",
                    "differentiable",
                    "all-gather",
                    "operation",
                    "provided",
                    "in",
                    "FleetX",
                    "#TARGET_REF",
                    ",",
                    "that",
                    "is",
                    "a",
                    "highly",
                    "scalable",
                    "distributed",
                    "training",
                    "engine",
                    "of",
                    "PaddlePaddle."
                ],
                [
                    "The",
                    "all-gather",
                    "operator",
                    "makes",
                    "representation",
                    "of",
                    "passages",
                    "across",
                    "all",
                    "GPUs",
                    "visible",
                    "on",
                    "each",
                    "GPU",
                    "and",
                    "thus",
                    "the",
                    "cross-batch",
                    "negative",
                    "sampling",
                    "approach",
                    "can",
                    "be",
                    "applied",
                    "globally."
                ]
            ],
            "context": [
                2,
                0
            ]
        },
        "input": "sent0: Cross-batch negatives 3 The cross-batch negative sampling is implemented with differentiable all-gather operation provided in FleetX #TARGET_REF , that is a highly scalable distributed training engine of PaddlePaddle.\n sent1: The all-gather operator makes representation of passages across all GPUs visible on each GPU and thus the cross-batch negative sampling approach can be applied globally.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Our",
                    "experiments",
                    "are",
                    "also",
                    "based",
                    "on",
                    "the",
                    "large",
                    "monolingual",
                    "French",
                    "model",
                    "CamemBERT",
                    "#REF",
                    "as",
                    "well",
                    "as",
                    "on",
                    "the",
                    "two",
                    "large",
                    "multilingual",
                    "models:",
                    "XLM-R",
                    "#REF",
                    "and",
                    "mBERT",
                    "#TARGET_REF",
                    ",",
                    "both",
                    "pre-trained",
                    "from",
                    "massive",
                    "corpora",
                    "dataset",
                    "in",
                    "more",
                    "than",
                    "100",
                    "languages",
                    "such",
                    "as",
                    "the",
                    "Common",
                    "Crawl",
                    "(CC-100)",
                    "or",
                    "Wikipedia",
                    "(Wiki-100)."
                ],
                [
                    "We",
                    "also",
                    "exploit",
                    "two",
                    "compact",
                    "multilingual",
                    "models",
                    "with",
                    "a",
                    "distilled",
                    "version",
                    "of",
                    "mBERT:",
                    "distil-mBERT",
                    "#REF",
                    "and",
                    "small-mBERT",
                    "#REF",
                    ",",
                    "a",
                    "mBERT",
                    "model",
                    "whose",
                    "the",
                    "original",
                    "vocabulary",
                    "has",
                    "been",
                    "reduced",
                    "to",
                    "two",
                    "languages",
                    "(English",
                    "and",
                    "French)."
                ],
                [
                    "Table",
                    "1",
                    "gives",
                    "a",
                    "comparison",
                    "of",
                    "the",
                    "models."
                ]
            ],
            "context": [
                3,
                2,
                0
            ]
        },
        "input": "sent0: Our experiments are also based on the large monolingual French model CamemBERT #REF as well as on the two large multilingual models: XLM-R #REF and mBERT #TARGET_REF , both pre-trained from massive corpora dataset in more than 100 languages such as the Common Crawl (CC-100) or Wikipedia (Wiki-100).\n sent1: We also exploit two compact multilingual models with a distilled version of mBERT: distil-mBERT #REF and small-mBERT #REF , a mBERT model whose the original vocabulary has been reduced to two languages (English and French).\n sent2: Table 1 gives a comparison of the models.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "this",
                    "section,",
                    "we",
                    "discuss",
                    "individual",
                    "modules",
                    "for",
                    "operationalizing",
                    "the",
                    "PSL",
                    "rules."
                ],
                [
                    "For",
                    "each",
                    "module,",
                    "we",
                    "fine-tune",
                    "the",
                    "pretrained",
                    "uncased",
                    "BERT-base",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "We",
                    "use",
                    "the",
                    "Transformers",
                    "library",
                    "v3.3.0",
                    "#REF",
                    "for",
                    "high",
                    "reproducibility",
                    "and",
                    "low",
                    "development",
                    "costs."
                ],
                [
                    "But",
                    "any",
                    "other",
                    "models",
                    "could",
                    "be",
                    "used",
                    "instead."
                ],
                [
                    "Each",
                    "dataset",
                    "used",
                    "is",
                    "randomly",
                    "split",
                    "with",
                    "a",
                    "ratio",
                    "of",
                    "9:1",
                    "for",
                    "training",
                    "and",
                    "test."
                ],
                [
                    "Cross-entropy",
                    "and",
                    "Adam",
                    "are",
                    "used",
                    "for",
                    "optimization."
                ],
                [
                    "To",
                    "address",
                    "the",
                    "imbalance",
                    "of",
                    "classes",
                    "and",
                    "datasets,",
                    "the",
                    "loss",
                    "for",
                    "each",
                    "training",
                    "instance",
                    "is",
                    "scaled",
                    "by",
                    "a",
                    "weight",
                    "inversely",
                    "proportional",
                    "to",
                    "the",
                    "number",
                    "of",
                    "its",
                    "class",
                    "and",
                    "dataset."
                ]
            ],
            "context": [
                3,
                1,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: In this section, we discuss individual modules for operationalizing the PSL rules.\n sent1: For each module, we fine-tune the pretrained uncased BERT-base #TARGET_REF .\n sent2: We use the Transformers library v3.3.0 #REF for high reproducibility and low development costs.\n sent3: But any other models could be used instead.\n sent4: Each dataset used is randomly split with a ratio of 9:1 for training and test.\n sent5: Cross-entropy and Adam are used for optimization.\n sent6: To address the imbalance of classes and datasets, the loss for each training instance is scaled by a weight inversely proportional to the number of its class and dataset.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "define",
                    "realism",
                    "as",
                    "the",
                    "inability",
                    "to",
                    "distinguish",
                    "between",
                    "human-and",
                    "machine-generated",
                    "ads."
                ],
                [
                    "Human",
                    "annotators",
                    "are",
                    "best",
                    "placed",
                    "to",
                    "assess",
                    "realism",
                    "(e.g."
                ],
                [
                    "see",
                    "#TARGET_REF",
                    "but",
                    "employing",
                    "and",
                    "paying",
                    "them",
                    "to",
                    "assess",
                    "over",
                    "10,000",
                    "ads",
                    "was",
                    "not",
                    "feasible."
                ],
                [
                    "Therefore,",
                    "we",
                    "train",
                    "a",
                    "discriminator",
                    "model",
                    "tasked",
                    "with",
                    "the",
                    "binary",
                    "prediction",
                    "of",
                    "whether",
                    "a",
                    "given",
                    "input",
                    "text",
                    "was",
                    "generated",
                    "by",
                    "a",
                    "human",
                    "or",
                    "GPT-3",
                    "and",
                    "validate",
                    "a",
                    "sample",
                    "of",
                    "ads",
                    "using",
                    "human",
                    "annotators."
                ],
                [
                    "Real",
                    "ads",
                    "were",
                    "longer",
                    "(M",
                    "=",
                    "2,846",
                    "characters,",
                    "SD",
                    "=",
                    "2,038)",
                    "than",
                    "generated",
                    "ones",
                    "(M",
                    "=",
                    "514,",
                    "SD",
                    "=",
                    "210)",
                    "so",
                    "we",
                    "truncate",
                    "texts",
                    "to",
                    "500",
                    "characters."
                ],
                [
                    "For",
                    "prediction,",
                    "we",
                    "use",
                    "a",
                    "Multinominal",
                    "Naive-Bayes",
                    "(MNB)",
                    "model,",
                    "which",
                    "we",
                    "train,",
                    "validate",
                    "and",
                    "test",
                    "using",
                    "an",
                    "80:10:10",
                    "split",
                    "taken",
                    "from",
                    "the",
                    "real",
                    "and",
                    "generated",
                    "ads",
                    "(described",
                    "in",
                    "Sec."
                ],
                [
                    "3.2)."
                ],
                [
                    "6",
                    "For",
                    "our",
                    "realism",
                    "metric,",
                    "we",
                    "then",
                    "use",
                    "this",
                    "model's",
                    "predicted",
                    "probability",
                    "that",
                    "an",
                    "ad",
                    "is",
                    "real."
                ],
                [
                    "To",
                    "assess",
                    "the",
                    "robustness",
                    "of",
                    "this",
                    "metric,",
                    "we",
                    "randomly",
                    "sample",
                    "10",
                    "ads",
                    "from",
                    "each",
                    "job",
                    "category",
                    "(female-biased,",
                    "male-biased",
                    "and",
                    "neutral)",
                    "for",
                    "each",
                    "experimental",
                    "condition",
                    "(N",
                    "=",
                    "150)."
                ],
                [
                    "We",
                    "then",
                    "ask",
                    "three",
                    "independent",
                    "annotators",
                    "to",
                    "label",
                    "the",
                    "ad",
                    "for",
                    "whether",
                    "it",
                    "was",
                    "human-or",
                    "machine-generated",
                    "and",
                    "take",
                    "the",
                    "majority",
                    "vote."
                ],
                [
                    "7",
                    "The",
                    "accuracy",
                    "of",
                    "the",
                    "majority",
                    "label",
                    "compared",
                    "against",
                    "the",
                    "ground",
                    "truth",
                    "ad",
                    "origin",
                    "(real-world",
                    "or",
                    "GPT-3",
                    "generated)",
                    "proxies",
                    "ad",
                    "quality",
                    "and",
                    "realism."
                ]
            ],
            "context": [
                0,
                2,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: We define realism as the inability to distinguish between human-and machine-generated ads.\n sent1: Human annotators are best placed to assess realism (e.g.\n sent2: see #TARGET_REF but employing and paying them to assess over 10,000 ads was not feasible.\n sent3: Therefore, we train a discriminator model tasked with the binary prediction of whether a given input text was generated by a human or GPT-3 and validate a sample of ads using human annotators.\n sent4: Real ads were longer (M = 2,846 characters, SD = 2,038) than generated ones (M = 514, SD = 210) so we truncate texts to 500 characters.\n sent5: For prediction, we use a Multinominal Naive-Bayes (MNB) model, which we train, validate and test using an 80:10:10 split taken from the real and generated ads (described in Sec.\n sent6: 3.2).\n sent7: 6 For our realism metric, we then use this model's predicted probability that an ad is real.\n sent8: To assess the robustness of this metric, we randomly sample 10 ads from each job category (female-biased, male-biased and neutral) for each experimental condition (N = 150).\n sent9: We then ask three independent annotators to label the ad for whether it was human-or machine-generated and take the majority vote.\n sent10: 7 The accuracy of the majority label compared against the ground truth ad origin (real-world or GPT-3 generated) proxies ad quality and realism.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\", \"sent2\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "If",
                    "the",
                    "pattern",
                    "tree",
                    "we",
                    "want",
                    "to",
                    "complete",
                    "is",
                    "only",
                    "the",
                    "most",
                    "embedded-that",
                    "is,",
                    "it",
                    "is",
                    "not",
                    "possible",
                    "to",
                    "overlap",
                    "gNCNs-this",
                    "corresponds",
                    "to",
                    "the",
                    "operation",
                    "of",
                    "unrestricted",
                    "TAG",
                    "adjoining."
                ],
                [
                    "That",
                    "is,",
                    "from",
                    "the",
                    "example,",
                    "only",
                    "the",
                    "last",
                    "¬",
                    "½annotation",
                    "is",
                    "accessible,",
                    "so",
                    "the",
                    "obvious",
                    "model",
                    "is",
                    "a",
                    "stack."
                ],
                [
                    "The",
                    "procedure",
                    "is",
                    "then",
                    "an",
                    "implementation",
                    "of",
                    "some",
                    "form",
                    "of",
                    "bottom-up",
                    "tree",
                    "pushdown",
                    "automaton",
                    "(buTPDA)",
                    "#TARGET_REF",
                    ",",
                    "a",
                    "tree",
                    "automaton",
                    "augmented",
                    "with",
                    "a",
                    "stack,",
                    "in",
                    "the",
                    "same",
                    "way",
                    "a",
                    "pushdown",
                    "automaton",
                    "(PDA)",
                    "is",
                    "a",
                    "a",
                    "finite-state",
                    "automaton",
                    "(FSA)",
                    "plus",
                    "a",
                    "stack."
                ]
            ],
            "context": [
                3,
                0,
                3
            ]
        },
        "input": "sent0: If the pattern tree we want to complete is only the most embedded-that is, it is not possible to overlap gNCNs-this corresponds to the operation of unrestricted TAG adjoining.\n sent1: That is, from the example, only the last ¬ ½annotation is accessible, so the obvious model is a stack.\n sent2: The procedure is then an implementation of some form of bottom-up tree pushdown automaton (buTPDA) #TARGET_REF , a tree automaton augmented with a stack, in the same way a pushdown automaton (PDA) is a a finite-state automaton (FSA) plus a stack.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Table",
                    "1",
                    "and",
                    "Table",
                    "2",
                    "respectively",
                    "present",
                    "evaluation",
                    "results",
                    "for",
                    "Japanese",
                    "and",
                    "English",
                    "datasets",
                    "in",
                    "perplexity,",
                    "BLEU",
                    "#REF",
                    ",",
                    "DIST-N",
                    "#REF",
                    ",",
                    "ROUGE",
                    "#TARGET_REF",
                    ",",
                    "also",
                    "showing",
                    "length,",
                    "which",
                    "is",
                    "an",
                    "average",
                    "number",
                    "of",
                    "tokens",
                    "generated",
                    "in",
                    "a",
                    "sentence."
                ],
                [
                    "*",
                    "in",
                    "these",
                    "tables",
                    "indicate",
                    "significant",
                    "differences",
                    "between",
                    "baseline",
                    "models",
                    "and",
                    "INF",
                    "model",
                    "for",
                    "each",
                    "evaluation",
                    "metric",
                    "(p&lt,0.05)."
                ],
                [
                    "BLEU",
                    "and",
                    "ROUGE",
                    "were",
                    "used",
                    "to",
                    "assess",
                    "the",
                    "quality",
                    "of",
                    "the",
                    "generated",
                    "sentences,",
                    "whereas",
                    "DIST-N",
                    "was",
                    "used",
                    "to",
                    "calculate",
                    "the",
                    "proportion",
                    "of",
                    "different",
                    "n-grams",
                    "among",
                    "the",
                    "n-grams",
                    "included",
                    "in",
                    "the",
                    "generated",
                    "sentences,",
                    "and",
                    "therefore",
                    "to",
                    "assess",
                    "the",
                    "diversity",
                    "of",
                    "the",
                    "generated",
                    "sentences."
                ]
            ],
            "context": [
                3,
                0,
                0
            ]
        },
        "input": "sent0: Table 1 and Table 2 respectively present evaluation results for Japanese and English datasets in perplexity, BLEU #REF , DIST-N #REF , ROUGE #TARGET_REF , also showing length, which is an average number of tokens generated in a sentence.\n sent1: * in these tables indicate significant differences between baseline models and INF model for each evaluation metric (p&lt,0.05).\n sent2: BLEU and ROUGE were used to assess the quality of the generated sentences, whereas DIST-N was used to calculate the proportion of different n-grams among the n-grams included in the generated sentences, and therefore to assess the diversity of the generated sentences.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "use",
                    "pre-trained",
                    "BERT-large",
                    "uncased",
                    "(24-layer,",
                    "1024-hidden,",
                    "16-heads,",
                    "340M",
                    "parameters,",
                    "whole",
                    "word",
                    "masking)",
                    "#REF",
                    "for",
                    "our",
                    "mention",
                    "and",
                    "context",
                    "encoder."
                ],
                [
                    "All",
                    "BERT",
                    "hyperparameters",
                    "are",
                    "unchanged."
                ],
                [
                    "The",
                    "entity",
                    "embedding",
                    "matrix",
                    "contains",
                    "10M",
                    "(UFET",
                    "type",
                    "set)",
                    "or",
                    "60M",
                    "(Wiki",
                    "type",
                    "set)",
                    "parameters."
                ],
                [
                    "We",
                    "train",
                    "our",
                    "models",
                    "with",
                    "batch",
                    "size",
                    "32",
                    "(8",
                    "×",
                    "4",
                    "gradient",
                    "accumulation",
                    "steps)",
                    "using",
                    "one",
                    "NVIDIA",
                    "V100",
                    "GPU",
                    "for",
                    "a",
                    "week."
                ],
                [
                    "We",
                    "use",
                    "the",
                    "AdamW",
                    "optimizer",
                    "#REF",
                    "with",
                    "learning",
                    "rate",
                    "2e-5",
                    "for",
                    "BERT",
                    "parameters",
                    "and",
                    "learning",
                    "rate",
                    "1e-3",
                    "for",
                    "the",
                    "type",
                    "embedding",
                    "matrix."
                ],
                [
                    "We",
                    "use",
                    "Hugging-Face's",
                    "Transformers",
                    "library",
                    "#TARGET_REF",
                    "Table",
                    "2:",
                    "\"Out-of-the-box\"",
                    "accuracy",
                    "on",
                    "the",
                    "CAP",
                    "development",
                    "set."
                ],
                [
                    "We",
                    "compare",
                    "performance",
                    "of",
                    "BERT-base,",
                    "BERT-large,",
                    "and",
                    "the",
                    "mention",
                    "and",
                    "context",
                    "representation",
                    "of",
                    "the",
                    "embedding",
                    "model",
                    "with",
                    "ours,",
                    "using",
                    "just",
                    "cosine",
                    "similarity",
                    "and",
                    "no",
                    "classifier."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                3,
                2
            ]
        },
        "input": "sent0: We use pre-trained BERT-large uncased (24-layer, 1024-hidden, 16-heads, 340M parameters, whole word masking) #REF for our mention and context encoder.\n sent1: All BERT hyperparameters are unchanged.\n sent2: The entity embedding matrix contains 10M (UFET type set) or 60M (Wiki type set) parameters.\n sent3: We train our models with batch size 32 (8 × 4 gradient accumulation steps) using one NVIDIA V100 GPU for a week.\n sent4: We use the AdamW optimizer #REF with learning rate 2e-5 for BERT parameters and learning rate 1e-3 for the type embedding matrix.\n sent5: We use Hugging-Face's Transformers library #TARGET_REF Table 2: \"Out-of-the-box\" accuracy on the CAP development set.\n sent6: We compare performance of BERT-base, BERT-large, and the mention and context representation of the embedding model with ours, using just cosine similarity and no classifier.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent6\"], \"BACKGROUND\": [\"sent5\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "To",
                    "this",
                    "end,",
                    "the",
                    "alignment",
                    "process",
                    "involves",
                    "training",
                    "three",
                    "#REF",
                    "biencoder",
                    "retrieval",
                    "models",
                    "to",
                    "retrieve",
                    "the",
                    "most",
                    "likely",
                    "characters,",
                    "locations,",
                    "and",
                    "objects",
                    "required",
                    "flesh",
                    "the",
                    "environment",
                    "out",
                    "and",
                    "make",
                    "the",
                    "quest",
                    "achievable."
                ],
                [
                    "We",
                    "use",
                    "the",
                    "same",
                    "biencoder",
                    "architecture",
                    "proposed",
                    "by",
                    "#TARGET_REF",
                    "which",
                    "encodes",
                    "context",
                    "using",
                    "one",
                    "transformer",
                    "and",
                    "candidates",
                    "with",
                    "anotherscoring",
                    "candidates",
                    "via",
                    "inner",
                    "product",
                    "between",
                    "the",
                    "two",
                    "encoded",
                    "vectors."
                ],
                [
                    "The",
                    "character",
                    "retrieval",
                    "model",
                    "is",
                    "conditioned",
                    "on",
                    "the",
                    "initial",
                    "character,",
                    "quest,",
                    "and",
                    "location-producing",
                    "additional",
                    "characters",
                    "required",
                    "to",
                    "complete",
                    "the",
                    "world."
                ]
            ],
            "context": [
                0,
                1,
                0
            ]
        },
        "input": "sent0: To this end, the alignment process involves training three #REF biencoder retrieval models to retrieve the most likely characters, locations, and objects required flesh the environment out and make the quest achievable.\n sent1: We use the same biencoder architecture proposed by #TARGET_REF which encodes context using one transformer and candidates with anotherscoring candidates via inner product between the two encoded vectors.\n sent2: The character retrieval model is conditioned on the initial character, quest, and location-producing additional characters required to complete the world.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "distribution",
                    "over",
                    "the",
                    "vocabulary",
                    "of",
                    "the",
                    "next",
                    "word",
                    "is",
                    "evaluated",
                    "using",
                    "the",
                    "memory",
                    "output",
                    "o",
                    "t",
                    "as",
                    "in",
                    "Eq."
                ],
                [
                    "3",
                    "with",
                    "a",
                    "feed-forward",
                    "layer."
                ],
                [
                    "Then,",
                    "the",
                    "next",
                    "soft",
                    "word,",
                    "ŷt",
                    ",",
                    "is",
                    "sampled",
                    "using",
                    "the",
                    "Gumbelsoftmax",
                    "relaxation",
                    "#TARGET_REF",
                    "with",
                    "temperature",
                    "T",
                    "(Eq."
                ],
                [
                    "4)."
                ],
                [
                    "The",
                    "temperature",
                    "value",
                    "greatly",
                    "influences",
                    "the",
                    "quality-diversity",
                    "trade-off,",
                    "more",
                    "details",
                    "on",
                    "these",
                    "parameters",
                    "are",
                    "provided",
                    "in",
                    "Appendix",
                    "D."
                ]
            ],
            "context": [
                3,
                3,
                3,
                1,
                2
            ]
        },
        "input": "sent0: The distribution over the vocabulary of the next word is evaluated using the memory output o t as in Eq.\n sent1: 3 with a feed-forward layer.\n sent2: Then, the next soft word, ŷt , is sampled using the Gumbelsoftmax relaxation #TARGET_REF with temperature T (Eq.\n sent3: 4).\n sent4: The temperature value greatly influences the quality-diversity trade-off, more details on these parameters are provided in Appendix D.\n",
        "output": "{\"INFORMATION\": [\"sent3\"], \"PERCEPTION\": [\"sent4\"], \"BACKGROUND\": [\"sent0\", \"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Though",
                    "the",
                    "importance",
                    "of",
                    "categorizing",
                    "scientific",
                    "literature",
                    "according",
                    "to",
                    "context",
                    "is",
                    "apparent,",
                    "the",
                    "reported",
                    "amount",
                    "of",
                    "research",
                    "that",
                    "has",
                    "been",
                    "carried",
                    "out",
                    "is",
                    "insufficient."
                ],
                [
                    "Along",
                    "with",
                    "a",
                    "classification",
                    "model,",
                    "#REF",
                    "also",
                    "proposed",
                    "an",
                    "annotation",
                    "scheme",
                    "for",
                    "the",
                    "categorization",
                    "of",
                    "the",
                    "citations."
                ],
                [
                    "12",
                    "classes",
                    "were",
                    "considered",
                    "for",
                    "annotation."
                ],
                [
                    "From",
                    "116",
                    "articles,",
                    "2829",
                    "citation",
                    "samples",
                    "were",
                    "gathered."
                ],
                [
                    "These",
                    "were",
                    "used",
                    "to",
                    "train",
                    "the",
                    "machine",
                    "learning",
                    "model."
                ],
                [
                    "113K",
                    "algorithms",
                    "were",
                    "used",
                    "for",
                    "classification",
                    "with",
                    "hand-engineered",
                    "features."
                ],
                [
                    "One",
                    "of",
                    "such",
                    "features",
                    "was",
                    "cue",
                    "phrases."
                ],
                [
                    "Features",
                    "such",
                    "as",
                    "patternbased",
                    "features,",
                    "topic-based",
                    "features,",
                    "and",
                    "prototypical",
                    "argument",
                    "features",
                    "were",
                    "used",
                    "by",
                    "D.",
                    "#REF",
                    "to",
                    "separate",
                    "the",
                    "documents",
                    "into",
                    "its",
                    "6",
                    "corresponding",
                    "classes."
                ],
                [
                    "The",
                    "RandomForest",
                    "algorithm",
                    "was",
                    "used",
                    "for",
                    "classification."
                ],
                [
                    "#REF",
                    "also",
                    "utilised",
                    "Glove,",
                    "ELMO",
                    "word",
                    "embedding",
                    "features,",
                    "and",
                    "Bi-LSTM",
                    "with",
                    "attention",
                    "models",
                    "to",
                    "aid",
                    "in",
                    "the",
                    "classification",
                    "of",
                    "the",
                    "citations."
                ],
                [
                    "#TARGET_REF",
                    "organized",
                    "the",
                    "first",
                    "shared",
                    "task",
                    "on",
                    "citation",
                    "classification",
                    "in",
                    "2020,",
                    "where",
                    "different",
                    "teams",
                    "came",
                    "up",
                    "with",
                    "different",
                    "approaches",
                    "to",
                    "solve",
                    "3c",
                    "classification",
                    "problem."
                ]
            ],
            "context": [
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "sent0: Though the importance of categorizing scientific literature according to context is apparent, the reported amount of research that has been carried out is insufficient.\n sent1: Along with a classification model, #REF also proposed an annotation scheme for the categorization of the citations.\n sent2: 12 classes were considered for annotation.\n sent3: From 116 articles, 2829 citation samples were gathered.\n sent4: These were used to train the machine learning model.\n sent5: 113K algorithms were used for classification with hand-engineered features.\n sent6: One of such features was cue phrases.\n sent7: Features such as patternbased features, topic-based features, and prototypical argument features were used by D. #REF to separate the documents into its 6 corresponding classes.\n sent8: The RandomForest algorithm was used for classification.\n sent9: #REF also utilised Glove, ELMO word embedding features, and Bi-LSTM with attention models to aid in the classification of the citations.\n sent10: #TARGET_REF organized the first shared task on citation classification in 2020, where different teams came up with different approaches to solve 3c classification problem.\n",
        "output": "{\"INFORMATION\": [\"sent10\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "follow",
                    "the",
                    "mainstream",
                    "method",
                    "#TARGET_REF",
                    "to",
                    "concatenate",
                    "the",
                    "word",
                    "and",
                    "context",
                    "together",
                    "with",
                    "a",
                    "special",
                    "token",
                    "[SEP]",
                    "as",
                    "x",
                    "=",
                    "(w",
                    "*",
                    ",",
                    "[SEP],",
                    "c)."
                ],
                [
                    "The",
                    "entire",
                    "sequence",
                    "is",
                    "then",
                    "fed",
                    "into",
                    "SimpDefiner,",
                    "and",
                    "the",
                    "definition",
                    "is",
                    "obtained",
                    "by",
                    "the",
                    "following",
                    "language",
                    "model:"
                ]
            ],
            "context": [
                1,
                2
            ]
        },
        "input": "sent0: We follow the mainstream method #TARGET_REF to concatenate the word and context together with a special token [SEP] as x = (w * , [SEP], c).\n sent1: The entire sequence is then fed into SimpDefiner, and the definition is obtained by the following language model:\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "n",
                    "biased",
                    "words",
                    "n",
                    "words",
                    "Superlative",
                    "Prevalence",
                    "This",
                    "measure",
                    "is",
                    "based",
                    "on",
                    "a",
                    "correlation",
                    "identified",
                    "between",
                    "\"standout\"",
                    "words",
                    "to",
                    "describe",
                    "a",
                    "job",
                    "candidate",
                    "and",
                    "research",
                    "skill",
                    "when",
                    "describing",
                    "that",
                    "candidate",
                    "#REF",
                    "."
                ],
                [
                    "A",
                    "particular",
                    "distinction",
                    "is",
                    "made",
                    "between",
                    "positive",
                    "(standout)",
                    "superlatives",
                    "and",
                    "negative",
                    "(grindstone)",
                    "superlatives",
                    "and",
                    "their",
                    "differential",
                    "use",
                    "to",
                    "describe",
                    "men",
                    "and",
                    "women."
                ],
                [
                    "In",
                    "our",
                    "experiment,",
                    "we",
                    "measure",
                    "the",
                    "prevalence",
                    "of",
                    "a",
                    "set",
                    "of",
                    "superlatives",
                    "provided",
                    "by",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "The",
                    "calculation",
                    "is:",
                    "Gender-Laden",
                    "Scoring",
                    "A",
                    "previous",
                    "study",
                    "provides",
                    "a",
                    "list",
                    "of",
                    "2,311",
                    "words,",
                    "based",
                    "on",
                    "an",
                    "analysis",
                    "of",
                    "32",
                    "properties",
                    "related",
                    "to",
                    "a",
                    "set",
                    "of",
                    "norms",
                    "#REF",
                    "."
                ],
                [
                    "In",
                    "this",
                    "study,",
                    "words",
                    "are",
                    "scored",
                    "for",
                    "their",
                    "\"gender-ladenness\"",
                    "and",
                    "\"gender",
                    "replication\"."
                ],
                [
                    "Our",
                    "study",
                    "takes",
                    "a",
                    "count",
                    "of",
                    "the",
                    "former,",
                    "measuring",
                    "their",
                    "unweighted",
                    "prevalence",
                    "to",
                    "make",
                    "it",
                    "comparable",
                    "to",
                    "the",
                    "other",
                    "bias",
                    "measures."
                ],
                [
                    "The",
                    "calculation",
                    "is:n",
                    "biased",
                    "words",
                    "n",
                    "wordsConnotation",
                    "Frames",
                    "This",
                    "measure",
                    "is",
                    "based",
                    "on",
                    "the",
                    "concept",
                    "of",
                    "power",
                    "and",
                    "agency",
                    "connotation",
                    "frames",
                    "#REF",
                    "."
                ],
                [
                    "Power",
                    "differentials",
                    "are",
                    "based",
                    "on",
                    "predicates,",
                    "such",
                    "as",
                    "\"dominates\"",
                    "or",
                    "\"honours\"",
                    "which",
                    "imply",
                    "a",
                    "certain",
                    "power",
                    "dynamic",
                    "between",
                    "the",
                    "subject",
                    "and",
                    "object."
                ],
                [
                    "Agency",
                    "is",
                    "attributed",
                    "to",
                    "the",
                    "agent",
                    "of",
                    "the",
                    "verb."
                ],
                [
                    "A",
                    "set",
                    "of",
                    "transitive",
                    "verbs",
                    "(1,700",
                    "for",
                    "power",
                    "differentials",
                    "and",
                    "2,000",
                    "for",
                    "agency)",
                    "have",
                    "been",
                    "annotated",
                    "in",
                    "a",
                    "previous",
                    "study",
                    "on",
                    "modern",
                    "films",
                    "and",
                    "operationalised",
                    "in",
                    "our",
                    "scoring",
                    "#REF",
                    "."
                ],
                [
                    "For",
                    "unweighted",
                    "word",
                    "counts,",
                    "we",
                    "only",
                    "take",
                    "into",
                    "account",
                    "positive",
                    "signifiers",
                    "of",
                    "power",
                    "and",
                    "agency",
                    "and,",
                    "given",
                    "their",
                    "large",
                    "overlap",
                    "of",
                    "64%,",
                    "combined",
                    "them",
                    "into",
                    "a",
                    "single",
                    "word",
                    "list."
                ],
                [
                    "The",
                    "calculation",
                    "is:"
                ]
            ],
            "context": [
                0,
                3,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: n biased words n words Superlative Prevalence This measure is based on a correlation identified between \"standout\" words to describe a job candidate and research skill when describing that candidate #REF .\n sent1: A particular distinction is made between positive (standout) superlatives and negative (grindstone) superlatives and their differential use to describe men and women.\n sent2: In our experiment, we measure the prevalence of a set of superlatives provided by #TARGET_REF .\n sent3: The calculation is: Gender-Laden Scoring A previous study provides a list of 2,311 words, based on an analysis of 32 properties related to a set of norms #REF .\n sent4: In this study, words are scored for their \"gender-ladenness\" and \"gender replication\".\n sent5: Our study takes a count of the former, measuring their unweighted prevalence to make it comparable to the other bias measures.\n sent6: The calculation is:n biased words n wordsConnotation Frames This measure is based on the concept of power and agency connotation frames #REF .\n sent7: Power differentials are based on predicates, such as \"dominates\" or \"honours\" which imply a certain power dynamic between the subject and object.\n sent8: Agency is attributed to the agent of the verb.\n sent9: A set of transitive verbs (1,700 for power differentials and 2,000 for agency) have been annotated in a previous study on modern films and operationalised in our scoring #REF .\n sent10: For unweighted word counts, we only take into account positive signifiers of power and agency and, given their large overlap of 64%, combined them into a single word list.\n sent11: The calculation is:\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "compute",
                    "our",
                    "embeddings",
                    "from",
                    "an",
                    "entity",
                    "typing",
                    "model",
                    "trained",
                    "on",
                    "the",
                    "UFET",
                    "dataset",
                    "#TARGET_REF",
                    "for",
                    "CAP",
                    "(10k",
                    "types)."
                ],
                [
                    "We",
                    "choose",
                    "this",
                    "dataset",
                    "because",
                    "many",
                    "of",
                    "mention",
                    "spans",
                    "in",
                    "the",
                    "CAP",
                    "examples",
                    "are",
                    "nominal",
                    "expressions",
                    "or",
                    "pronouns,",
                    "and",
                    "the",
                    "Wiki-Context",
                    "dataset",
                    "includes",
                    "almost",
                    "entirely",
                    "mentions",
                    "of",
                    "proper",
                    "nouns."
                ],
                [
                    "To",
                    "make",
                    "a",
                    "prediction",
                    "if",
                    "two",
                    "mentions",
                    "are",
                    "coreferent,",
                    "we",
                    "compute",
                    "sim",
                    "cos",
                    "(t",
                    "1",
                    ",",
                    "t",
                    "2",
                    ")",
                    "over",
                    "the",
                    "type",
                    "vectors",
                    "for",
                    "each",
                    "mention",
                    "and",
                    "check",
                    "if",
                    "this",
                    "is",
                    "greater",
                    "than",
                    "a",
                    "threshold,",
                    "which",
                    "we",
                    "set",
                    "to",
                    "0.5."
                ],
                [
                    "Only",
                    "our",
                    "baselines",
                    "use",
                    "the",
                    "CAP",
                    "training",
                    "set,",
                    "our",
                    "model",
                    "does",
                    "not",
                    "train",
                    "on",
                    "this",
                    "data."
                ],
                [
                    "We",
                    "compare",
                    "our",
                    "approach",
                    "with",
                    "the",
                    "baselines",
                    "described",
                    "above",
                    "as",
                    "reported",
                    "in",
                    "#REF",
                    "."
                ],
                [
                    "Note",
                    "that",
                    "they",
                    "use",
                    "two",
                    "different",
                    "types",
                    "of",
                    "entity",
                    "representations:",
                    "one",
                    "based",
                    "on",
                    "entity",
                    "descriptions",
                    "and",
                    "another",
                    "based",
                    "on",
                    "entity",
                    "names",
                    "only."
                ]
            ],
            "context": [
                2,
                2,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: We compute our embeddings from an entity typing model trained on the UFET dataset #TARGET_REF for CAP (10k types).\n sent1: We choose this dataset because many of mention spans in the CAP examples are nominal expressions or pronouns, and the Wiki-Context dataset includes almost entirely mentions of proper nouns.\n sent2: To make a prediction if two mentions are coreferent, we compute sim cos (t 1 , t 2 ) over the type vectors for each mention and check if this is greater than a threshold, which we set to 0.5.\n sent3: Only our baselines use the CAP training set, our model does not train on this data.\n sent4: We compare our approach with the baselines described above as reported in #REF .\n sent5: Note that they use two different types of entity representations: one based on entity descriptions and another based on entity names only.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\", \"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Post-editing",
                    "is",
                    "the",
                    "process",
                    "whereby",
                    "a",
                    "human",
                    "user",
                    "corrects",
                    "the",
                    "output",
                    "of",
                    "a",
                    "machine",
                    "translation",
                    "system."
                ],
                [
                    "The",
                    "use",
                    "of",
                    "basic",
                    "post-editing",
                    "tools",
                    "by",
                    "bilingual",
                    "human",
                    "translators",
                    "has",
                    "been",
                    "shown",
                    "to",
                    "yield",
                    "substantial",
                    "increases",
                    "in",
                    "terms",
                    "of",
                    "productivity",
                    "#REF",
                    "as",
                    "well",
                    "as",
                    "improvements",
                    "in",
                    "translation",
                    "quality",
                    "#REF",
                    "when",
                    "compared",
                    "to",
                    "bilingual",
                    "human",
                    "translators",
                    "working",
                    "without",
                    "assistance",
                    "from",
                    "machine",
                    "translation",
                    "and",
                    "post-editing",
                    "tools."
                ],
                [
                    "More",
                    "sophisticated",
                    "interactive",
                    "interfaces",
                    "#REF",
                    "may",
                    "also",
                    "provide",
                    "benefit",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                0,
                3,
                1
            ]
        },
        "input": "sent0: Post-editing is the process whereby a human user corrects the output of a machine translation system.\n sent1: The use of basic post-editing tools by bilingual human translators has been shown to yield substantial increases in terms of productivity #REF as well as improvements in translation quality #REF when compared to bilingual human translators working without assistance from machine translation and post-editing tools.\n sent2: More sophisticated interactive interfaces #REF may also provide benefit #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "consider",
                    "three",
                    "baselines."
                ],
                [
                    "Random",
                    "assigns",
                    "a",
                    "relation",
                    "to",
                    "each",
                    "argument",
                    "randomly."
                ],
                [
                    "Sentiment",
                    "assigns",
                    "a",
                    "relation",
                    "based",
                    "on",
                    "the",
                    "claim",
                    "and",
                    "statement's",
                    "agreement",
                    "on",
                    "sentiment:",
                    "support",
                    "if",
                    "both",
                    "are",
                    "positive",
                    "or",
                    "negative,",
                    "attack",
                    "if",
                    "they",
                    "have",
                    "opposite",
                    "sentiments,",
                    "and",
                    "neutral",
                    "otherwise."
                ],
                [
                    "We",
                    "compute",
                    "a",
                    "sentiment",
                    "distribution",
                    "by",
                    "averaging",
                    "all",
                    "target-specific",
                    "sentiments",
                    "from",
                    "our",
                    "sentiment",
                    "classifier",
                    "(",
                    "§4.2)."
                ],
                [
                    "Textual",
                    "entailment",
                    "assigns",
                    "support",
                    "(attack)",
                    "if",
                    "the",
                    "statement",
                    "entails",
                    "(contradicts)",
                    "the",
                    "claim,",
                    "and",
                    "neutral",
                    "otherwise",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "We",
                    "use",
                    "our",
                    "textual",
                    "entailment",
                    "module",
                    "(",
                    "§4.1)."
                ],
                [
                    "For",
                    "Debatepedia,",
                    "we",
                    "choose",
                    "between",
                    "support",
                    "and",
                    "attack",
                    "whichever",
                    "has",
                    "a",
                    "higher",
                    "probability."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                3,
                2,
                0
            ]
        },
        "input": "sent0: We consider three baselines.\n sent1: Random assigns a relation to each argument randomly.\n sent2: Sentiment assigns a relation based on the claim and statement's agreement on sentiment: support if both are positive or negative, attack if they have opposite sentiments, and neutral otherwise.\n sent3: We compute a sentiment distribution by averaging all target-specific sentiments from our sentiment classifier ( §4.2).\n sent4: Textual entailment assigns support (attack) if the statement entails (contradicts) the claim, and neutral otherwise #TARGET_REF .\n sent5: We use our textual entailment module ( §4.1).\n sent6: For Debatepedia, we choose between support and attack whichever has a higher probability.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent5\"], \"BACKGROUND\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Lately,",
                    "many",
                    "works",
                    "built",
                    "upon",
                    "the",
                    "Seq2Seq",
                    "MT",
                    "model",
                    "#TARGET_REF",
                    "performed",
                    "well."
                ],
                [
                    "First",
                    "attempted",
                    "by",
                    "#REF",
                    ",",
                    "the",
                    "Seq2Seq",
                    "models",
                    "for",
                    "this",
                    "task",
                    "are",
                    "able",
                    "to",
                    "perform",
                    "lexical",
                    "simplification",
                    "and",
                    "content",
                    "reduction",
                    "simultaneously",
                    "by",
                    "training",
                    "on",
                    "complex-simple",
                    "sentence",
                    "pairs."
                ],
                [
                    "This",
                    "method",
                    "was",
                    "inherited",
                    "and",
                    "improved",
                    "by",
                    "many",
                    "subsequent",
                    "works,",
                    "such",
                    "as",
                    "combining",
                    "with",
                    "the",
                    "reinforcement",
                    "learning",
                    "method",
                    "by",
                    "setting",
                    "a",
                    "simplification",
                    "reward",
                    "#REF",
                    ",",
                    "augmenting",
                    "memory",
                    "capacities",
                    "#REF",
                    "or",
                    "training",
                    "with",
                    "multitasking",
                    "on",
                    "entailment",
                    "and",
                    "paraphrase",
                    "generation",
                    "#REF",
                    "."
                ],
                [
                    "#REF",
                    "proposed",
                    "to",
                    "prepend",
                    "additional",
                    "prompt",
                    "tokens",
                    "to",
                    "source",
                    "sentences",
                    "at",
                    "train",
                    "time,",
                    "which",
                    "enables",
                    "the",
                    "end-users",
                    "to",
                    "condition",
                    "the",
                    "simplifications",
                    "returned",
                    "by",
                    "the",
                    "model",
                    "on",
                    "attributes",
                    "like",
                    "length,",
                    "lexical",
                    "complexity,",
                    "and",
                    "syntactic",
                    "complexity."
                ],
                [
                    "This",
                    "controllable",
                    "simplification",
                    "system",
                    "(called",
                    "ACCESS)",
                    "and",
                    "its",
                    "improved",
                    "version",
                    "MUSS",
                    "#REF",
                    "achieved",
                    "SOTA",
                    "results",
                    "on",
                    "the",
                    "Turk",
                    "corpus",
                    "in",
                    "terms",
                    "of",
                    "the",
                    "SARI",
                    "metric",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                1,
                2,
                3,
                0,
                0
            ]
        },
        "input": "sent0: Lately, many works built upon the Seq2Seq MT model #TARGET_REF performed well.\n sent1: First attempted by #REF , the Seq2Seq models for this task are able to perform lexical simplification and content reduction simultaneously by training on complex-simple sentence pairs.\n sent2: This method was inherited and improved by many subsequent works, such as combining with the reinforcement learning method by setting a simplification reward #REF , augmenting memory capacities #REF or training with multitasking on entailment and paraphrase generation #REF .\n sent3: #REF proposed to prepend additional prompt tokens to source sentences at train time, which enables the end-users to condition the simplifications returned by the model on attributes like length, lexical complexity, and syntactic complexity.\n sent4: This controllable simplification system (called ACCESS) and its improved version MUSS #REF achieved SOTA results on the Turk corpus in terms of the SARI metric #REF .\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "difficult",
                    "challenge",
                    "to",
                    "face",
                    "is",
                    "designing",
                    "a",
                    "winning",
                    "vendor",
                    "pricing",
                    "model."
                ],
                [
                    "Even",
                    "in",
                    "the",
                    "cases",
                    "of",
                    "google",
                    "9",
                    "-like",
                    "machine",
                    "translation",
                    "output,",
                    "it",
                    "is",
                    "arduous",
                    "to",
                    "benefit",
                    "from",
                    "vendor",
                    "involvement",
                    "with",
                    "MT."
                ],
                [
                    "The",
                    "reason",
                    "why",
                    "originates",
                    "from",
                    "the",
                    "fact",
                    "that",
                    "when",
                    "incorporating",
                    "MT",
                    "output",
                    "in",
                    "the",
                    "translation",
                    "process",
                    "workflow,",
                    "per",
                    "segment",
                    "rates",
                    "are",
                    "directly",
                    "affected",
                    "and",
                    "reduced",
                    "(the",
                    "reductions",
                    "can",
                    "reach",
                    "50%",
                    "of",
                    "the",
                    "normal",
                    "price)."
                ],
                [
                    "From",
                    "vendors'",
                    "point",
                    "of",
                    "view,",
                    "MT",
                    "threatens",
                    "to",
                    "reduce",
                    "per",
                    "segment",
                    "rates,",
                    "which",
                    "in",
                    "return",
                    "leads",
                    "to",
                    "hourly",
                    "rate",
                    "destabilization."
                ],
                [
                    "Regardless",
                    "of",
                    "MT",
                    "output",
                    "quality,",
                    "vendors",
                    "tend",
                    "to",
                    "decline",
                    "jobs",
                    "due",
                    "to",
                    "the",
                    "fact",
                    "MT",
                    "output",
                    "is",
                    "incorporated",
                    "in",
                    "pre-translated",
                    "documents."
                ],
                [
                    "This",
                    "decrease",
                    "of",
                    "vendor",
                    "involvement",
                    "leads",
                    "us",
                    "to",
                    "a",
                    "conclusion",
                    "that",
                    "purely",
                    "showing",
                    "good",
                    "MT",
                    "results",
                    "is",
                    "not",
                    "sufficient",
                    "and",
                    "more",
                    "work",
                    "is",
                    "required",
                    "to",
                    "be",
                    "done",
                    "in",
                    "order",
                    "to",
                    "start",
                    "gaining",
                    "investment",
                    "back",
                    "and",
                    "increasing",
                    "the",
                    "ROI",
                    "ratio."
                ],
                [
                    "What",
                    "we",
                    "discovered",
                    "on",
                    "our",
                    "own,",
                    "stated",
                    "additionally",
                    "by",
                    "#TARGET_REF",
                    ",",
                    "is",
                    "that",
                    "human",
                    "translators'",
                    "engagement",
                    "is",
                    "MT",
                    "development",
                    "is",
                    "vital."
                ],
                [
                    "People",
                    "are",
                    "the",
                    "key",
                    "factor",
                    "of",
                    "success",
                    "of",
                    "every",
                    "business",
                    "and",
                    "winning",
                    "strategies",
                    "must",
                    "actively",
                    "involve",
                    "employees",
                    "on",
                    "different",
                    "organizational",
                    "levels",
                    "in",
                    "technological",
                    "(including",
                    "MT)",
                    "solution",
                    "processes."
                ],
                [
                    "In",
                    "euroscript",
                    "10",
                    ",",
                    "we",
                    "regularly",
                    "ask",
                    "for",
                    "in-house",
                    "translators'",
                    "MT",
                    "evaluation",
                    "feedback",
                    "in",
                    "terms",
                    "of",
                    "different",
                    "categories",
                    "of",
                    "MT",
                    "output",
                    "errors."
                ],
                [
                    "We",
                    "devote",
                    "time",
                    "to",
                    "correcting",
                    "these",
                    "mistakes,",
                    "informing",
                    "the",
                    "in-house",
                    "translators",
                    "of",
                    "the",
                    "improvements",
                    "made",
                    "and",
                    "engaging",
                    "ourselves",
                    "into",
                    "increasing",
                    "translators'",
                    "satisfaction",
                    "with",
                    "working",
                    "with",
                    "MT."
                ],
                [
                    "We",
                    "consider",
                    "different",
                    "options",
                    "of",
                    "post-editing",
                    "training",
                    "focused",
                    "on",
                    "translators",
                    "following",
                    "the",
                    "strategy",
                    "of",
                    "involving",
                    "translators",
                    "more",
                    "with",
                    "MT."
                ],
                [
                    "This",
                    "idea",
                    "is",
                    "also",
                    "supported",
                    "by:",
                    "Hernández-Lasa",
                    "(2011)",
                    "and",
                    "#REF",
                    "."
                ],
                [
                    "Our",
                    "preliminary",
                    "results",
                    "show",
                    "that",
                    "roughly",
                    "50%",
                    "of",
                    "our",
                    "in-house",
                    "translators,",
                    "who",
                    "work",
                    "with",
                    "MT,",
                    "consider",
                    "its",
                    "understandability",
                    "as",
                    "good",
                    "opposed",
                    "to",
                    "acceptable",
                    "or",
                    "bad",
                    "(see",
                    "#REF",
                    "for",
                    "MT",
                    "features",
                    "that",
                    "influence",
                    "MT",
                    "understandability)."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                2,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: difficult challenge to face is designing a winning vendor pricing model.\n sent1: Even in the cases of google 9 -like machine translation output, it is arduous to benefit from vendor involvement with MT.\n sent2: The reason why originates from the fact that when incorporating MT output in the translation process workflow, per segment rates are directly affected and reduced (the reductions can reach 50% of the normal price).\n sent3: From vendors' point of view, MT threatens to reduce per segment rates, which in return leads to hourly rate destabilization.\n sent4: Regardless of MT output quality, vendors tend to decline jobs due to the fact MT output is incorporated in pre-translated documents.\n sent5: This decrease of vendor involvement leads us to a conclusion that purely showing good MT results is not sufficient and more work is required to be done in order to start gaining investment back and increasing the ROI ratio.\n sent6: What we discovered on our own, stated additionally by #TARGET_REF , is that human translators' engagement is MT development is vital.\n sent7: People are the key factor of success of every business and winning strategies must actively involve employees on different organizational levels in technological (including MT) solution processes.\n sent8: In euroscript 10 , we regularly ask for in-house translators' MT evaluation feedback in terms of different categories of MT output errors.\n sent9: We devote time to correcting these mistakes, informing the in-house translators of the improvements made and engaging ourselves into increasing translators' satisfaction with working with MT.\n sent10: We consider different options of post-editing training focused on translators following the strategy of involving translators more with MT.\n sent11: This idea is also supported by: Hernández-Lasa (2011) and #REF .\n sent12: Our preliminary results show that roughly 50% of our in-house translators, who work with MT, consider its understandability as good opposed to acceptable or bad (see #REF for MT features that influence MT understandability).\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent6\", \"sent7\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Training",
                    "Following",
                    "#TARGET_REF",
                    ",",
                    "the",
                    "loss",
                    "is",
                    "a",
                    "sum",
                    "of",
                    "binary",
                    "cross-entropy",
                    "losses",
                    "over",
                    "all",
                    "entity",
                    "types",
                    "T",
                    "over",
                    "all",
                    "training",
                    "examples",
                    "D.",
                    "That",
                    "is,",
                    "we",
                    "treat",
                    "each",
                    "type",
                    "prediction",
                    "for",
                    "each",
                    "example",
                    "as",
                    "an",
                    "independent",
                    "binary",
                    "decision,",
                    "with",
                    "shared",
                    "parameters",
                    "in",
                    "the",
                    "BERT",
                    "encoder."
                ]
            ],
            "context": [
                1
            ]
        },
        "input": "sent0: Training Following #TARGET_REF , the loss is a sum of binary cross-entropy losses over all entity types T over all training examples D. That is, we treat each type prediction for each example as an independent binary decision, with shared parameters in the BERT encoder.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "evaluate",
                    "our",
                    "experiments",
                    "on",
                    "the",
                    "English",
                    "to",
                    "French",
                    "Spoken",
                    "Language",
                    "Translation",
                    "task",
                    "from",
                    "IWSLT",
                    "2015",
                    "evaluation",
                    "campaign",
                    "3",
                    "."
                ],
                [
                    "A",
                    "data",
                    "selection",
                    "method",
                    "#TARGET_REF",
                    "consisting",
                    "on",
                    "scoring",
                    "the",
                    "sentences",
                    "according",
                    "to",
                    "a",
                    "in-domain",
                    "language",
                    "model",
                    "has",
                    "been",
                    "applied."
                ],
                [
                    "We",
                    "have",
                    "used",
                    "as",
                    "available",
                    "parallel",
                    "corpora",
                    "(news-commentary,",
                    "united-nations,",
                    "europarl,",
                    "wikipedia,",
                    "and",
                    "two",
                    "crawled",
                    "corpora)",
                    "and",
                    "Technology",
                    "Entertainment",
                    "Design",
                    "(TED",
                    "4",
                    ")",
                    "corpus",
                    "as",
                    "in-domain",
                    "corpus."
                ],
                [
                    "The",
                    "data",
                    "selection",
                    "allows",
                    "us",
                    "to",
                    "train",
                    "the",
                    "models",
                    "in",
                    "a",
                    "faster",
                    "way",
                    "taking",
                    "into",
                    "account",
                    "the",
                    "sentences",
                    "which",
                    "contain",
                    "relevant",
                    "information",
                    "of",
                    "the",
                    "domain",
                    "and",
                    "avoids",
                    "noisy",
                    "data."
                ],
                [
                    "We",
                    "also",
                    "did",
                    "a",
                    "preprocessing",
                    "to",
                    "convert",
                    "html",
                    "entities",
                    "and",
                    "filter",
                    "out",
                    "the",
                    "sentences",
                    "with",
                    "more",
                    "than",
                    "50",
                    "words",
                    "for",
                    "both",
                    "source",
                    "and",
                    "target",
                    "languages."
                ],
                [
                    "We",
                    "finally",
                    "end",
                    "up",
                    "with",
                    "a",
                    "selected",
                    "corpus",
                    "of",
                    "2M",
                    "sentences",
                    "(50.5",
                    "millions",
                    "of",
                    "words),",
                    "leading",
                    "to",
                    "147K",
                    "unique",
                    "tokens",
                    "for",
                    "English",
                    "side",
                    "and",
                    "266K",
                    "unique",
                    "tokens",
                    "for",
                    "French",
                    "side."
                ]
            ],
            "context": [
                2,
                1,
                2,
                2,
                2,
                2
            ]
        },
        "input": "sent0: We evaluate our experiments on the English to French Spoken Language Translation task from IWSLT 2015 evaluation campaign 3 .\n sent1: A data selection method #TARGET_REF consisting on scoring the sentences according to a in-domain language model has been applied.\n sent2: We have used as available parallel corpora (news-commentary, united-nations, europarl, wikipedia, and two crawled corpora) and Technology Entertainment Design (TED 4 ) corpus as in-domain corpus.\n sent3: The data selection allows us to train the models in a faster way taking into account the sentences which contain relevant information of the domain and avoids noisy data.\n sent4: We also did a preprocessing to convert html entities and filter out the sentences with more than 50 words for both source and target languages.\n sent5: We finally end up with a selected corpus of 2M sentences (50.5 millions of words), leading to 147K unique tokens for English side and 266K unique tokens for French side.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [\"sent0\", \"sent2\", \"sent3\", \"sent4\", \"sent5\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Gender",
                    "bias",
                    "in",
                    "language",
                    "is",
                    "complex",
                    "and",
                    "no",
                    "single",
                    "measure",
                    "can",
                    "capture",
                    "all",
                    "presentations",
                    "of",
                    "societal",
                    "harms",
                    "#REF",
                    "."
                ],
                [
                    "Several",
                    "methodologies",
                    "to",
                    "measure",
                    "and",
                    "mitigate",
                    "bias",
                    "cannot",
                    "be",
                    "applied",
                    "in",
                    "our",
                    "setting",
                    "given",
                    "the",
                    "lack",
                    "of",
                    "public",
                    "access",
                    "to",
                    "GPT-3's",
                    "model",
                    "architecture",
                    "or",
                    "training",
                    "dataset,",
                    "and",
                    "the",
                    "enormous",
                    "resources",
                    "needed",
                    "to",
                    "retrain",
                    "the",
                    "model",
                    "from",
                    "scratch."
                ],
                [
                    "In",
                    "particular,",
                    "this",
                    "includes",
                    "training",
                    "data",
                    "augmentation",
                    "#REF",
                    ",",
                    "adjusting",
                    "model",
                    "behaviour",
                    "via",
                    "adversarial",
                    "learning",
                    "#REF",
                    ",",
                    "and",
                    "amending",
                    "model",
                    "embeddings",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                0,
                2,
                3
            ]
        },
        "input": "sent0: Gender bias in language is complex and no single measure can capture all presentations of societal harms #REF .\n sent1: Several methodologies to measure and mitigate bias cannot be applied in our setting given the lack of public access to GPT-3's model architecture or training dataset, and the enormous resources needed to retrain the model from scratch.\n sent2: In particular, this includes training data augmentation #REF , adjusting model behaviour via adversarial learning #REF , and amending model embeddings #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "3."
                ],
                [
                    "#TARGET_REF",
                    "establishes",
                    "the",
                    "current",
                    "state",
                    "of",
                    "the",
                    "art",
                    "for",
                    "data",
                    "driven",
                    "models",
                    "in",
                    "temporal",
                    "and",
                    "event",
                    "extent",
                    "information",
                    "in",
                    "Italian."
                ],
                [
                    "The",
                    "system",
                    "is",
                    "a",
                    "modification",
                    "of",
                    "the",
                    "TipSem",
                    "system."
                ],
                [
                    "We",
                    "compares",
                    "our",
                    "models",
                    "to",
                    "their",
                    "reported",
                    "scores."
                ],
                [
                    "However,",
                    "the",
                    "corpus",
                    "used",
                    "in",
                    "#REF",
                    "is",
                    "the",
                    "Ita-TimeBank",
                    "which",
                    "has",
                    "been",
                    "augmented",
                    "with",
                    "further",
                    "annotations",
                    "and",
                    "resources,",
                    "while",
                    "our",
                    "system",
                    "uses",
                    "just",
                    "the",
                    "Ita-TimeBank",
                    "for",
                    "event",
                    "extraction."
                ]
            ],
            "context": [
                0,
                1,
                2,
                2,
                0
            ]
        },
        "input": "sent0: 3.\n sent1: #TARGET_REF establishes the current state of the art for data driven models in temporal and event extent information in Italian.\n sent2: The system is a modification of the TipSem system.\n sent3: We compares our models to their reported scores.\n sent4: However, the corpus used in #REF is the Ita-TimeBank which has been augmented with further annotations and resources, while our system uses just the Ita-TimeBank for event extraction.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [\"sent2\", \"sent3\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "To",
                    "meet",
                    "these",
                    "constraints,",
                    "compact",
                    "models",
                    "represent",
                    "one",
                    "of",
                    "the",
                    "most",
                    "promising",
                    "solutions."
                ],
                [
                    "As",
                    "far",
                    "as",
                    "we",
                    "know,",
                    "they",
                    "have",
                    "only",
                    "been",
                    "evaluated",
                    "on",
                    "the",
                    "comprehension",
                    "tasks",
                    "covered",
                    "by",
                    "GLUE",
                    "#REF",
                    "and",
                    "the",
                    "question-answering",
                    "task",
                    "with",
                    "the",
                    "SQuAD",
                    "corpus",
                    "#REF",
                    "with",
                    "abundant",
                    "data,",
                    "in",
                    "English."
                ],
                [
                    "The",
                    "improvements",
                    "resulting",
                    "from",
                    "the",
                    "algorithmic",
                    "optimizations",
                    "of",
                    "the",
                    "models,",
                    "although",
                    "significant,",
                    "raise",
                    "questions",
                    "about",
                    "their",
                    "effectiveness",
                    "on",
                    "lower-scale",
                    "learning",
                    "problems",
                    "on",
                    "poorly",
                    "endowed",
                    "languages."
                ],
                [
                    "The",
                    "works",
                    "of",
                    "#TARGET_REF",
                    "and",
                    "#REF",
                    "have",
                    "furthermore",
                    "shown",
                    "degraded",
                    "performance",
                    "in",
                    "these",
                    "conditions."
                ],
                [
                    "These",
                    "two",
                    "reflections",
                    "are",
                    "at",
                    "the",
                    "origin",
                    "of",
                    "a",
                    "double",
                    "question",
                    "which",
                    "our",
                    "contribution",
                    "attempts",
                    "to",
                    "answer."
                ],
                [
                    "On",
                    "the",
                    "one",
                    "hand,",
                    "what",
                    "is",
                    "the",
                    "behavior",
                    "of",
                    "a",
                    "Transformer-based",
                    "model",
                    "in",
                    "the",
                    "context",
                    "of",
                    "a",
                    "question-answering",
                    "task",
                    "in",
                    "French,",
                    "a",
                    "task",
                    "that",
                    "is",
                    "poorly",
                    "endowed",
                    "in",
                    "this",
                    "language?"
                ],
                [
                    "On",
                    "the",
                    "other",
                    "hand,",
                    "what",
                    "are",
                    "the",
                    "impacts",
                    "of",
                    "algorithmic",
                    "improvements",
                    "of",
                    "these",
                    "same",
                    "models",
                    "in",
                    "this",
                    "context?"
                ]
            ],
            "context": [
                2,
                3,
                2,
                3,
                2,
                0,
                0
            ]
        },
        "input": "sent0: To meet these constraints, compact models represent one of the most promising solutions.\n sent1: As far as we know, they have only been evaluated on the comprehension tasks covered by GLUE #REF and the question-answering task with the SQuAD corpus #REF with abundant data, in English.\n sent2: The improvements resulting from the algorithmic optimizations of the models, although significant, raise questions about their effectiveness on lower-scale learning problems on poorly endowed languages.\n sent3: The works of #TARGET_REF and #REF have furthermore shown degraded performance in these conditions.\n sent4: These two reflections are at the origin of a double question which our contribution attempts to answer.\n sent5: On the one hand, what is the behavior of a Transformer-based model in the context of a question-answering task in French, a task that is poorly endowed in this language?\n sent6: On the other hand, what are the impacts of algorithmic improvements of these same models in this context?\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\", \"sent2\", \"sent4\"], \"BACKGROUND\": [\"sent1\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "get",
                    "an",
                    "increase",
                    "of",
                    "5.77%",
                    "on",
                    "the",
                    "FEVEROUS",
                    "score",
                    "and",
                    "7.91%",
                    "on",
                    "the",
                    "accuracy",
                    "over",
                    "the",
                    "previous",
                    "best",
                    "model",
                    "FaBULOUS",
                    "#REF",
                    "on",
                    "the",
                    "development",
                    "set."
                ],
                [
                    "For",
                    "the",
                    "test",
                    "set,",
                    "the",
                    "increase",
                    "is",
                    "6.96%",
                    "and",
                    "7.14%",
                    "in",
                    "Feverous",
                    "score",
                    "and",
                    "label",
                    "accuracy,",
                    "respectively."
                ],
                [
                    "These",
                    "results",
                    "suggest",
                    "the",
                    "effectiveness",
                    "of",
                    "our",
                    "proposed",
                    "DCUF",
                    "model."
                ],
                [
                    "The",
                    "evidence",
                    "format",
                    "of",
                    "a",
                    "global",
                    "evidence",
                    "table",
                    "is",
                    "consistent",
                    "to",
                    "the",
                    "input",
                    "of",
                    "pre-trained",
                    "table",
                    "models."
                ],
                [
                    "Thus,",
                    "DCUF",
                    "can",
                    "make",
                    "better",
                    "use",
                    "of",
                    "the",
                    "internal",
                    "ability",
                    "of",
                    "pre-trained",
                    "models",
                    "than",
                    "previous",
                    "works",
                    "which",
                    "concatenate",
                    "linearized",
                    "tables",
                    "or",
                    "max-pool",
                    "lots",
                    "of",
                    "claim-table",
                    "pair",
                    "encoding",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Moreover,",
                    "DCUF",
                    "also",
                    "performs",
                    "better",
                    "than",
                    "another",
                    "well-performing",
                    "model,",
                    "CARE",
                    "#REF",
                    "."
                ],
                [
                    "DCUF",
                    "converts",
                    "cells",
                    "to",
                    "meaningful",
                    "sentences",
                    "that",
                    "are",
                    "similar",
                    "to",
                    "the",
                    "inputs",
                    "of",
                    "PLMs",
                    "pre-training",
                    "stage,",
                    "which",
                    "makes",
                    "better",
                    "use",
                    "of",
                    "the",
                    "PLMs",
                    "ability."
                ]
            ],
            "context": [
                0,
                0,
                0,
                3,
                1,
                0,
                0
            ]
        },
        "input": "sent0: We get an increase of 5.77% on the FEVEROUS score and 7.91% on the accuracy over the previous best model FaBULOUS #REF on the development set.\n sent1: For the test set, the increase is 6.96% and 7.14% in Feverous score and label accuracy, respectively.\n sent2: These results suggest the effectiveness of our proposed DCUF model.\n sent3: The evidence format of a global evidence table is consistent to the input of pre-trained table models.\n sent4: Thus, DCUF can make better use of the internal ability of pre-trained models than previous works which concatenate linearized tables or max-pool lots of claim-table pair encoding #TARGET_REF .\n sent5: Moreover, DCUF also performs better than another well-performing model, CARE #REF .\n sent6: DCUF converts cells to meaningful sentences that are similar to the inputs of PLMs pre-training stage, which makes better use of the PLMs ability.\n",
        "output": "{\"INFORMATION\": [\"sent4\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "It",
                    "is",
                    "this",
                    "very",
                    "issue",
                    "that",
                    "the",
                    "current",
                    "wave",
                    "of",
                    "theories",
                    "labelled",
                    "'connectionist'",
                    "(e.g."
                ],
                [
                    "#TARGET_REF",
                    "seeks",
                    "to",
                    "tackle:",
                    "how",
                    "underlying",
                    "classifiers",
                    "can",
                    "emerge",
                    "spontaneously",
                    "from",
                    "data",
                    "by",
                    "using",
                    "no",
                    "more",
                    "than",
                    "association",
                    "and",
                    "classification",
                    "algorithms."
                ],
                [
                    "MMB",
                    "would",
                    "have",
                    "sympathised",
                    "with",
                    "its",
                    "anti-logicism,",
                    "but",
                    "would",
                    "have",
                    "found",
                    "its",
                    "statistical",
                    "basis",
                    "only",
                    "thin",
                    "mathematics,",
                    "and",
                    "would",
                    "have",
                    "not",
                    "been",
                    "sympathetic",
                    "to",
                    "its",
                    "anti-symbolic",
                    "disposition."
                ]
            ],
            "context": [
                3,
                1,
                0
            ]
        },
        "input": "sent0: It is this very issue that the current wave of theories labelled 'connectionist' (e.g.\n sent1: #TARGET_REF seeks to tackle: how underlying classifiers can emerge spontaneously from data by using no more than association and classification algorithms.\n sent2: MMB would have sympathised with its anti-logicism, but would have found its statistical basis only thin mathematics, and would have not been sympathetic to its anti-symbolic disposition.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "prediction",
                    "tasks,",
                    "it",
                    "is",
                    "important",
                    "that",
                    "label",
                    "predictions",
                    "generalize",
                    "to",
                    "unseen",
                    "data."
                ],
                [
                    "In",
                    "contrast",
                    "to",
                    "this,",
                    "the",
                    "numeric",
                    "part",
                    "of",
                    "referent",
                    "labels",
                    "in",
                    "clauses",
                    "are",
                    "not",
                    "meaningful",
                    "and",
                    "depend",
                    "on",
                    "the",
                    "number",
                    "of",
                    "referents",
                    "that",
                    "were",
                    "introduced",
                    "before",
                    "in",
                    "the",
                    "same",
                    "sentence,",
                    "so",
                    "they",
                    "would",
                    "generalize",
                    "poorly."
                ],
                [
                    "Thus,",
                    "in",
                    "row",
                    "(2),",
                    "we",
                    "change",
                    "the",
                    "referents",
                    "to",
                    "be",
                    "relative,",
                    "inspired",
                    "by",
                    "#TARGET_REF",
                    ":",
                    "referents",
                    "that",
                    "have",
                    "not",
                    "occurred",
                    "before",
                    "get",
                    "the",
                    "index",
                    "0",
                    "and",
                    "referents",
                    "that",
                    "have",
                    "occurred",
                    "get",
                    "a",
                    "negative",
                    "index,",
                    "indicating",
                    "how",
                    "long",
                    "ago",
                    "the",
                    "same",
                    "referent",
                    "last",
                    "occurred",
                    "(counting",
                    "back",
                    "among",
                    "all",
                    "occurrences",
                    "of",
                    "referents",
                    "of",
                    "the",
                    "same",
                    "type)."
                ]
            ],
            "context": [
                0,
                3,
                0
            ]
        },
        "input": "sent0: In prediction tasks, it is important that label predictions generalize to unseen data.\n sent1: In contrast to this, the numeric part of referent labels in clauses are not meaningful and depend on the number of referents that were introduced before in the same sentence, so they would generalize poorly.\n sent2: Thus, in row (2), we change the referents to be relative, inspired by #TARGET_REF : referents that have not occurred before get the index 0 and referents that have occurred get a negative index, indicating how long ago the same referent last occurred (counting back among all occurrences of referents of the same type).\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "factorization",
                    "concerns",
                    "largeclass",
                    "and",
                    "open-class",
                    "symbols,",
                    "viz."
                ],
                [
                    "(content-word)",
                    "word",
                    "senses,",
                    "names,",
                    "numbers,",
                    "and",
                    "time",
                    "expressions."
                ],
                [
                    "We",
                    "follow",
                    "#TARGET_REF",
                    "in",
                    "replacing",
                    "these",
                    "with",
                    "dummy",
                    "expressions",
                    "in",
                    "the",
                    "fragments",
                    "and",
                    "predicting",
                    "them",
                    "separately,",
                    "as",
                    "explained",
                    "below",
                    "in",
                    "Section",
                    "3."
                ],
                [
                    "We",
                    "also",
                    "follow",
                    "them",
                    "in",
                    "heuristically",
                    "changing",
                    "the",
                    "representation",
                    "of",
                    "first",
                    "and",
                    "second",
                    "person",
                    "pronouns,",
                    "which",
                    "introduce",
                    "\"speaker\"",
                    "and",
                    "\"hearer\"",
                    "constants",
                    "instead",
                    "of",
                    "discourse",
                    "referents",
                    "in",
                    "the",
                    "PMB,",
                    "for",
                    "more",
                    "consistent",
                    "representation",
                    "of",
                    "predicates.b0",
                    "REF",
                    "x0",
                    "b-1",
                    "Name",
                    "x-1",
                    "\"tom\"",
                    "b-1",
                    "PRESUPPOSITION",
                    "b0",
                    "b-2",
                    "male",
                    "\"n.02\"",
                    "x-1",
                    "b-2",
                    "REF",
                    "t0",
                    "b-1",
                    "TPR",
                    "t-1",
                    "\"now\"",
                    "b-1",
                    "Time",
                    "e-1",
                    "t-1",
                    "b-1",
                    "time",
                    "\"n.08\"",
                    "t-1",
                    "b-1",
                    "REF",
                    "e-1",
                    "b-1",
                    "Patient",
                    "e-1",
                    "x-1",
                    "b-1",
                    "trick",
                    "\"v.01\"",
                    "e-1",
                    "(3)",
                    "b0",
                    "REF",
                    "x0",
                    "b-1",
                    "Name",
                    "x-1",
                    "\"DUMMY\"",
                    "b-1",
                    "PRESUPPOSITION",
                    "b0",
                    "b-2",
                    "male",
                    "\"n.02\"",
                    "x-1",
                    "b-1",
                    "REF",
                    "t0",
                    "b-1",
                    "TPR",
                    "t-1",
                    "\"now\"",
                    "b-1",
                    "Time",
                    "e-1",
                    "t-1",
                    "b-1",
                    "time",
                    "\"n.08\"",
                    "t-1",
                    "b-1",
                    "REF",
                    "e-1",
                    "b-1",
                    "Patient",
                    "e-1",
                    "x-1",
                    "b-1",
                    "DUMMY",
                    "\"v.00\"",
                    "e-1",
                    "[b0",
                    "e0",
                    "n0",
                    "p0",
                    "s0",
                    "t0",
                    "x0]",
                    "[b-1",
                    "e0",
                    "n0",
                    "p0",
                    "s0",
                    "t0",
                    "x0]",
                    "[b0",
                    "e0",
                    "n0",
                    "p0",
                    "s0",
                    "t0",
                    "x0]",
                    "tom",
                    "-",
                    "trick",
                    "\"v.01\""
                ]
            ],
            "context": [
                3,
                3,
                2,
                0
            ]
        },
        "input": "sent0: factorization concerns largeclass and open-class symbols, viz.\n sent1: (content-word) word senses, names, numbers, and time expressions.\n sent2: We follow #TARGET_REF in replacing these with dummy expressions in the fragments and predicting them separately, as explained below in Section 3.\n sent3: We also follow them in heuristically changing the representation of first and second person pronouns, which introduce \"speaker\" and \"hearer\" constants instead of discourse referents in the PMB, for more consistent representation of predicates.b0 REF x0 b-1 Name x-1 \"tom\" b-1 PRESUPPOSITION b0 b-2 male \"n.02\" x-1 b-2 REF t0 b-1 TPR t-1 \"now\" b-1 Time e-1 t-1 b-1 time \"n.08\" t-1 b-1 REF e-1 b-1 Patient e-1 x-1 b-1 trick \"v.01\" e-1 (3) b0 REF x0 b-1 Name x-1 \"DUMMY\" b-1 PRESUPPOSITION b0 b-2 male \"n.02\" x-1 b-1 REF t0 b-1 TPR t-1 \"now\" b-1 Time e-1 t-1 b-1 time \"n.08\" t-1 b-1 REF e-1 b-1 Patient e-1 x-1 b-1 DUMMY \"v.00\" e-1 [b0 e0 n0 p0 s0 t0 x0] [b-1 e0 n0 p0 s0 t0 x0] [b0 e0 n0 p0 s0 t0 x0] tom - trick \"v.01\"\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "This",
                    "paper",
                    "presents",
                    "an",
                    "efficient,",
                    "implemented",
                    "approach",
                    "to",
                    "cross-linguistic",
                    "parsing",
                    "for",
                    "interlingual",
                    "MT."
                ],
                [
                    "Our",
                    "design",
                    "is",
                    "based",
                    "on",
                    "Government-Binding",
                    "(GB)",
                    "Theory",
                    "#REF",
                    "."
                ],
                [
                    "One",
                    "of",
                    "the",
                    "drawbacks",
                    "to",
                    "alternative",
                    "GBbased",
                    "parsing",
                    "approaches",
                    "is",
                    "that",
                    "they",
                    "generally",
                    "adopt",
                    "a",
                    "filter-based",
                    "paradigm,",
                    "generating",
                    "all",
                    "possible",
                    "candidate",
                    "structures",
                    "of",
                    "the",
                    "sentence",
                    "that",
                    "satisfy",
                    "Xbar",
                    "theory,",
                    "and",
                    "then",
                    "subsequently",
                    "applying",
                    "filters",
                    "to",
                    "eliminate",
                    "those",
                    "structures",
                    "that",
                    "violate",
                    "GB",
                    "principles."
                ],
                [
                    "(See,",
                    "for",
                    "example,",
                    "#REF",
                    ",",
                    "#REF",
                    ",",
                    "#TARGET_REF",
                    ",",
                    "#REF",
                    ",",
                    "#REF",
                    "The",
                    "current",
                    "approach",
                    "provides",
                    "an",
                    "alternative",
                    "to",
                    "filter-based",
                    "designs",
                    "which",
                    "avoids",
                    "these",
                    "difficulties",
                    "by",
                    "applying",
                    "principles",
                    "to",
                    "descriptions",
                    "of",
                    "structures",
                    "without",
                    "actually",
                    "building",
                    "the",
                    "structures",
                    "themselves."
                ],
                [
                    "In",
                    "effect,",
                    "structure",
                    "building",
                    "is",
                    "deferred",
                    "until",
                    "the",
                    "descriptions",
                    "satisfy",
                    "all",
                    "principles."
                ]
            ],
            "context": [
                0,
                0,
                3,
                3,
                0
            ]
        },
        "input": "sent0: This paper presents an efficient, implemented approach to cross-linguistic parsing for interlingual MT.\n sent1: Our design is based on Government-Binding (GB) Theory #REF .\n sent2: One of the drawbacks to alternative GBbased parsing approaches is that they generally adopt a filter-based paradigm, generating all possible candidate structures of the sentence that satisfy Xbar theory, and then subsequently applying filters to eliminate those structures that violate GB principles.\n sent3: (See, for example, #REF , #REF , #TARGET_REF , #REF , #REF The current approach provides an alternative to filter-based designs which avoids these difficulties by applying principles to descriptions of structures without actually building the structures themselves.\n sent4: In effect, structure building is deferred until the descriptions satisfy all principles.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "achieved",
                    "the",
                    "best",
                    "result",
                    "with",
                    "the",
                    "Gradient",
                    "Boosting",
                    "classifier",
                    "5",
                    "using",
                    "only",
                    "10%",
                    "of",
                    "the",
                    "prelabeled",
                    "nodes",
                    "i.e.,",
                    "the",
                    "classification",
                    "does",
                    "not",
                    "improve",
                    "after",
                    "this",
                    "percentage."
                ],
                [
                    "Table",
                    "3",
                    "Besides",
                    "our",
                    "approach,",
                    "we",
                    "evaluated",
                    "other",
                    "graph",
                    "models",
                    "of",
                    "different",
                    "structures."
                ],
                [
                    "First,",
                    "we",
                    "used",
                    "the",
                    "network",
                    "graph",
                    "developed",
                    "by",
                    "#REF",
                    "."
                ],
                [
                    "That",
                    "graph",
                    "does",
                    "not",
                    "use",
                    "weight",
                    "between",
                    "the",
                    "nodes."
                ],
                [
                    "Second,",
                    "we",
                    "used",
                    "the",
                    "Term",
                    "Frequency-Inverse",
                    "Document",
                    "Frequency",
                    "(TF-IDF)",
                    "as",
                    "weight",
                    "instead",
                    "of",
                    "the",
                    "average",
                    "of",
                    "embeddings."
                ],
                [
                    "Third,",
                    "we",
                    "used",
                    "bigrams",
                    "and",
                    "trigrams",
                    "as",
                    "nodes",
                    "rather",
                    "than",
                    "token",
                    "nodes."
                ],
                [
                    "Finally,",
                    "we",
                    "used",
                    "the",
                    "Pointwise",
                    "Mutual",
                    "Information",
                    "(PMI)",
                    "measure",
                    "#TARGET_REF",
                    "as",
                    "the",
                    "weight",
                    "between",
                    "the",
                    "bi",
                    "and",
                    "trigrams",
                    "nodes."
                ],
                [
                    "For",
                    "these",
                    "approaches,",
                    "we",
                    "adopted",
                    "the",
                    "same",
                    "regularization",
                    "algorithm,",
                    "ranging",
                    "the",
                    "pre-labeled",
                    "nodes",
                    "from",
                    "5%",
                    "to",
                    "30%."
                ],
                [
                    "In",
                    "Table",
                    "4,",
                    "we",
                    "present",
                    "the",
                    "bestachieved",
                    "results."
                ],
                [
                    "From",
                    "this",
                    "table,",
                    "our",
                    "graph",
                    "modeling",
                    "and",
                    "the",
                    "gradient",
                    "boosting",
                    "classifier",
                    "achieved",
                    "better",
                    "results",
                    "than",
                    "these",
                    "other",
                    "graphs,",
                    "as",
                    "well",
                    "as",
                    "classifier",
                    "variations."
                ],
                [
                    "This,",
                    "we",
                    "think,",
                    "is",
                    "because",
                    "of",
                    "the",
                    "embedding",
                    "value",
                    "among",
                    "the",
                    "graph",
                    "nodes",
                    "since",
                    "it",
                    "is",
                    "able",
                    "to",
                    "capture",
                    "morphological,",
                    "syntactic,",
                    "and",
                    "semantic",
                    "knowledge",
                    "of",
                    "a",
                    "word."
                ],
                [
                    "As",
                    "we",
                    "used",
                    "the",
                    "average",
                    "word",
                    "embedding",
                    "value,",
                    "it",
                    "includes",
                    "information",
                    "from",
                    "all",
                    "of",
                    "the",
                    "individual",
                    "vector",
                    "values,",
                    "working",
                    "as",
                    "an",
                    "overall",
                    "summary",
                    "of",
                    "all",
                    "vector",
                    "values."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                1,
                3,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: We achieved the best result with the Gradient Boosting classifier 5 using only 10% of the prelabeled nodes i.e., the classification does not improve after this percentage.\n sent1: Table 3 Besides our approach, we evaluated other graph models of different structures.\n sent2: First, we used the network graph developed by #REF .\n sent3: That graph does not use weight between the nodes.\n sent4: Second, we used the Term Frequency-Inverse Document Frequency (TF-IDF) as weight instead of the average of embeddings.\n sent5: Third, we used bigrams and trigrams as nodes rather than token nodes.\n sent6: Finally, we used the Pointwise Mutual Information (PMI) measure #TARGET_REF as the weight between the bi and trigrams nodes.\n sent7: For these approaches, we adopted the same regularization algorithm, ranging the pre-labeled nodes from 5% to 30%.\n sent8: In Table 4, we present the bestachieved results.\n sent9: From this table, our graph modeling and the gradient boosting classifier achieved better results than these other graphs, as well as classifier variations.\n sent10: This, we think, is because of the embedding value among the graph nodes since it is able to capture morphological, syntactic, and semantic knowledge of a word.\n sent11: As we used the average word embedding value, it includes information from all of the individual vector values, working as an overall summary of all vector values.\n",
        "output": "{\"INFORMATION\": [\"sent6\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent7\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "This",
                    "generation",
                    "task",
                    "adopts",
                    "the",
                    "traditional",
                    "image",
                    "captioning",
                    "evaluation",
                    "metric",
                    "using",
                    "the",
                    "open-source",
                    "tool",
                    "2",
                    "with",
                    "a",
                    "minor",
                    "modification",
                    "3",
                    "to",
                    "suit",
                    "with",
                    "LN-COCO,",
                    "including",
                    "BLEU",
                    "#REF",
                    ",",
                    "METEOR",
                    "#REF",
                    ",",
                    "ROUGE-L",
                    "#TARGET_REF",
                    ",",
                    "ROUGE-1-F1(Pont-Tuset",
                    "et",
                    "al.,",
                    "2020),",
                    "and",
                    "CIDEr-D",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                2
            ]
        },
        "input": "sent0: This generation task adopts the traditional image captioning evaluation metric using the open-source tool 2 with a minor modification 3 to suit with LN-COCO, including BLEU #REF , METEOR #REF , ROUGE-L #TARGET_REF , ROUGE-1-F1(Pont-Tuset et al., 2020), and CIDEr-D #REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "final",
                    "dataset",
                    "contained",
                    "17,520",
                    "unique",
                    "emotion-annotated",
                    "subtitles",
                    "as",
                    "shown",
                    "in",
                    "table",
                    "3."
                ],
                [
                    "In",
                    "addition",
                    "there",
                    "are",
                    "some",
                    "6.5k",
                    "subtitles",
                    "annotated",
                    "as",
                    "neutral."
                ],
                [
                    "The",
                    "label",
                    "distribution",
                    "can",
                    "be",
                    "seen",
                    "in",
                    "table",
                    "3",
                    "The",
                    "emotion",
                    "labels",
                    "are",
                    "surprisingly",
                    "balanced",
                    "with",
                    "the",
                    "exception",
                    "of",
                    "anger",
                    "and",
                    "anticipation,",
                    "which",
                    "are",
                    "more",
                    "common",
                    "than",
                    "the",
                    "other",
                    "labels."
                ],
                [
                    "In",
                    "comparison",
                    "with",
                    "one",
                    "of",
                    "the",
                    "most",
                    "well-known",
                    "emotion",
                    "datasets",
                    "using",
                    "the",
                    "same",
                    "annotation",
                    "scheme,",
                    "the",
                    "NRC",
                    "emotion",
                    "lexicon",
                    "(EmoLex)",
                    "#TARGET_REF",
                    ",",
                    "the",
                    "distribution",
                    "differs",
                    "somewhat."
                ],
                [
                    "Although",
                    "anger",
                    "is",
                    "a",
                    "large",
                    "category",
                    "in",
                    "both",
                    "datasets,",
                    "fear",
                    "is",
                    "average",
                    "in",
                    "our",
                    "dataset,",
                    "but",
                    "the",
                    "largest",
                    "category",
                    "in",
                    "EmoLex."
                ],
                [
                    "It",
                    "is",
                    "hard",
                    "to",
                    "speculate",
                    "why",
                    "this",
                    "is,",
                    "but",
                    "one",
                    "possible",
                    "reason",
                    "is",
                    "the",
                    "different",
                    "source",
                    "data."
                ]
            ],
            "context": [
                0,
                0,
                0,
                2,
                2,
                2
            ]
        },
        "input": "sent0: The final dataset contained 17,520 unique emotion-annotated subtitles as shown in table 3.\n sent1: In addition there are some 6.5k subtitles annotated as neutral.\n sent2: The label distribution can be seen in table 3 The emotion labels are surprisingly balanced with the exception of anger and anticipation, which are more common than the other labels.\n sent3: In comparison with one of the most well-known emotion datasets using the same annotation scheme, the NRC emotion lexicon (EmoLex) #TARGET_REF , the distribution differs somewhat.\n sent4: Although anger is a large category in both datasets, fear is average in our dataset, but the largest category in EmoLex.\n sent5: It is hard to speculate why this is, but one possible reason is the different source data.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent3\", \"sent4\", \"sent5\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Because",
                    "OPUS",
                    "open",
                    "subtitles",
                    "is",
                    "a",
                    "parallel",
                    "corpus",
                    "we",
                    "are",
                    "able",
                    "to",
                    "evaluate",
                    "our",
                    "annotated",
                    "datasets",
                    "across",
                    "languages",
                    "and",
                    "at",
                    "identical",
                    "levels",
                    "of",
                    "granularity."
                ],
                [
                    "Although",
                    "the",
                    "subtitles",
                    "might",
                    "be",
                    "translated",
                    "using",
                    "different",
                    "translation",
                    "philosophies",
                    "(favoring",
                    "e.g."
                ],
                [
                    "meaning,",
                    "mood,",
                    "or",
                    "idiomatic",
                    "language",
                    "as",
                    "the",
                    "prime",
                    "objective)",
                    "#REF",
                    ",",
                    "we",
                    "expect",
                    "the",
                    "translations",
                    "to",
                    "have",
                    "aimed",
                    "at",
                    "capturing",
                    "the",
                    "sentiments",
                    "and",
                    "emotions",
                    "originally",
                    "expressed",
                    "in",
                    "the",
                    "film",
                    "based",
                    "on",
                    "previous",
                    "studies",
                    "(e.g."
                ],
                [
                    "#REF",
                    ",",
                    "#TARGET_REF",
                    ",",
                    "#REF",
                    ",",
                    "#REF",
                    "and",
                    "Kajava",
                    "et",
                    "al."
                ],
                [
                    "(",
                    "2020))."
                ]
            ],
            "context": [
                3,
                2,
                2,
                1,
                1
            ]
        },
        "input": "sent0: Because OPUS open subtitles is a parallel corpus we are able to evaluate our annotated datasets across languages and at identical levels of granularity.\n sent1: Although the subtitles might be translated using different translation philosophies (favoring e.g.\n sent2: meaning, mood, or idiomatic language as the prime objective) #REF , we expect the translations to have aimed at capturing the sentiments and emotions originally expressed in the film based on previous studies (e.g.\n sent3: #REF , #TARGET_REF , #REF , #REF and Kajava et al.\n sent4: ( 2020)).\n",
        "output": "{\"INFORMATION\": [\"sent3\", \"sent4\"], \"PERCEPTION\": [\"sent1\", \"sent2\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "absolute",
                    "BLEU",
                    "points",
                    "(0.42%",
                    "and",
                    "0.51%",
                    "relative),",
                    "respectively."
                ],
                [
                    "However,",
                    "these",
                    "improvements",
                    "are",
                    "not",
                    "statistically",
                    "significant",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "The",
                    "results",
                    "also",
                    "show",
                    "that",
                    "dropping",
                    "features",
                    "from",
                    "the",
                    "CCG",
                    "categories",
                    "and",
                    "contextual",
                    "labels",
                    "had",
                    "a",
                    "negative",
                    "effect",
                    "on",
                    "performance."
                ],
                [
                    "Table",
                    "3",
                    "shows",
                    "the",
                    "evaluation",
                    "results",
                    "for",
                    "the",
                    "baseline",
                    "and",
                    "CCG-based",
                    "HPB",
                    "systems",
                    "on",
                    "Ar-En",
                    "translation",
                    "using",
                    "TED",
                    "data",
                    "for",
                    "the",
                    "translation",
                    "model",
                    "and",
                    "mixture",
                    "adapted",
                    "language",
                    "models."
                ],
                [
                    "Using",
                    "mixture",
                    "adaptation",
                    "of",
                    "language",
                    "model",
                    "leads",
                    "to",
                    "an",
                    "increase",
                    "of",
                    "5.99",
                    "absolute",
                    "BLEU",
                    "points",
                    "(25.41%",
                    "relative)",
                    "for",
                    "the",
                    "best",
                    "performing",
                    "system",
                    "(CCG",
                    "contextual",
                    "labels",
                    "system)",
                    "over",
                    "the",
                    "corresponding",
                    "TED-trained",
                    "model",
                    "score",
                    "in",
                    "Table",
                    "2."
                ],
                [
                    "Language",
                    "model",
                    "adaptation",
                    "also",
                    "caused",
                    "the",
                    "PB-SMT",
                    "model",
                    "scores",
                    "to",
                    "improve",
                    "by",
                    "5.16",
                    "absolute",
                    "BLEU",
                    "points",
                    "(21.99%",
                    "relative)",
                    "over",
                    "the",
                    "corresponding",
                    "unadapted",
                    "PBSMT",
                    "models."
                ],
                [
                    "As",
                    "with",
                    "the",
                    "unadapted",
                    "systems,",
                    "the",
                    "HPB-CCG",
                    "contextual",
                    "labels",
                    "system",
                    "is",
                    "also",
                    "the",
                    "best",
                    "performing",
                    "system",
                    "within",
                    "all",
                    "the",
                    "systems",
                    "with",
                    "adapted",
                    "language",
                    "models,",
                    "across",
                    "all",
                    "evaluation",
                    "metrics."
                ],
                [
                    "It",
                    "outperformed",
                    "the",
                    "mixture-model",
                    "adapted",
                    "HPB",
                    "systems",
                    "by",
                    "a",
                    "statistically",
                    "insignificant",
                    "0.1",
                    "absolute",
                    "BLEU",
                    "points",
                    "(0.34%",
                    "relative)."
                ],
                [
                    "However,",
                    "it",
                    "improved",
                    "over",
                    "the",
                    "UN-enhanced",
                    "mixture-model",
                    "adapted",
                    "PB",
                    "system",
                    "by",
                    "0.93",
                    "absolute",
                    "BLEU",
                    "points",
                    "(3.25%",
                    "relative)",
                    "providing",
                    "a",
                    "statistically",
                    "significance",
                    "at",
                    "p-level=0.05."
                ],
                [
                    "The",
                    "results",
                    "further",
                    "demonstrate",
                    "that",
                    "dropping",
                    "features",
                    "from",
                    "CCG",
                    "labels",
                    "caused",
                    "the",
                    "performance",
                    "of",
                    "the",
                    "CCG-based",
                    "systems",
                    "to",
                    "deteriorate."
                ],
                [
                    "For",
                    "the",
                    "Ar-En",
                    "translation",
                    "task,",
                    "the",
                    "best",
                    "performing",
                    "system",
                    "i.e."
                ],
                [
                    "the",
                    "HPB-CCG",
                    "contextual",
                    "labels",
                    "system",
                    "(HPB-CCG",
                    "context)",
                    "was",
                    "submitted",
                    "as",
                    "the",
                    "primary",
                    "run",
                    "in",
                    "the",
                    "evaluation",
                    "campaign."
                ]
            ],
            "context": [
                3,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: absolute BLEU points (0.42% and 0.51% relative), respectively.\n sent1: However, these improvements are not statistically significant #TARGET_REF .\n sent2: The results also show that dropping features from the CCG categories and contextual labels had a negative effect on performance.\n sent3: Table 3 shows the evaluation results for the baseline and CCG-based HPB systems on Ar-En translation using TED data for the translation model and mixture adapted language models.\n sent4: Using mixture adaptation of language model leads to an increase of 5.99 absolute BLEU points (25.41% relative) for the best performing system (CCG contextual labels system) over the corresponding TED-trained model score in Table 2.\n sent5: Language model adaptation also caused the PB-SMT model scores to improve by 5.16 absolute BLEU points (21.99% relative) over the corresponding unadapted PBSMT models.\n sent6: As with the unadapted systems, the HPB-CCG contextual labels system is also the best performing system within all the systems with adapted language models, across all evaluation metrics.\n sent7: It outperformed the mixture-model adapted HPB systems by a statistically insignificant 0.1 absolute BLEU points (0.34% relative).\n sent8: However, it improved over the UN-enhanced mixture-model adapted PB system by 0.93 absolute BLEU points (3.25% relative) providing a statistically significance at p-level=0.05.\n sent9: The results further demonstrate that dropping features from CCG labels caused the performance of the CCG-based systems to deteriorate.\n sent10: For the Ar-En translation task, the best performing system i.e.\n sent11: the HPB-CCG contextual labels system (HPB-CCG context) was submitted as the primary run in the evaluation campaign.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "This",
                    "parsing",
                    "algorithm",
                    "is",
                    "similar",
                    "to",
                    "the",
                    "shift-reduce",
                    "parser",
                    "except",
                    "that",
                    "our",
                    "algorithms",
                    "handles",
                    "ambiguities,",
                    "parallel",
                    "processing",
                    "of",
                    "each",
                    "hypothesis,",
                    "and",
                    "top-down",
                    "predictions",
                    "of",
                    "possible",
                    "next",
                    "input",
                    "symbol."
                ],
                [
                    "The",
                    "generation",
                    "algorithm",
                    "implemented",
                    "on",
                    "SNAP",
                    "is",
                    "a",
                    "version",
                    "of",
                    "the",
                    "lexically",
                    "guided",
                    "bottom-up",
                    "algorithm",
                    "which",
                    "is",
                    "described",
                    "in",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                0,
                2
            ]
        },
        "input": "sent0: This parsing algorithm is similar to the shift-reduce parser except that our algorithms handles ambiguities, parallel processing of each hypothesis, and top-down predictions of possible next input symbol.\n sent1: The generation algorithm implemented on SNAP is a version of the lexically guided bottom-up algorithm which is described in #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "use",
                    "BLEU",
                    "#TARGET_REF",
                    "10",
                    "and",
                    "Average",
                    "Lagging",
                    "(AL)",
                    "#REF",
                    "11",
                    "to",
                    "evaluate",
                    "translation",
                    "quality",
                    "and",
                    "latency",
                    "respectively."
                ],
                [
                    "AL",
                    "measures",
                    "the",
                    "degree",
                    "the",
                    "user",
                    "is",
                    "out",
                    "of",
                    "sync",
                    "with",
                    "the",
                    "speaker."
                ],
                [
                    "As",
                    "shown",
                    "in",
                    "Eq.9-10,",
                    "t",
                    "is",
                    "decoding",
                    "step,",
                    "τ",
                    "is",
                    "cut-off",
                    "decoding",
                    "step",
                    "where",
                    "source",
                    "sentence",
                    "is",
                    "finished,",
                    "g(t)",
                    "denotes",
                    "the",
                    "number",
                    "of",
                    "source",
                    "words",
                    "read",
                    "by",
                    "the",
                    "encoder",
                    "at",
                    "decoding",
                    "step",
                    "t,",
                    "and",
                    "r",
                    "=",
                    "|x|/|y|",
                    "is",
                    "the",
                    "target-to-source",
                    "length",
                    "ratio."
                ],
                [
                    "The",
                    "smaller",
                    "the",
                    "AL",
                    "(roughly",
                    "equivalent",
                    "to",
                    "k)",
                    "is,",
                    "the",
                    "more",
                    "real-time",
                    "the",
                    "simultaneous",
                    "translation",
                    "system",
                    "is.AL",
                    "g",
                    "(x,",
                    "y)",
                    "=",
                    "1",
                    "τ",
                    "τ",
                    "t=1",
                    "g(t)",
                    "−",
                    "t",
                    "−",
                    "1",
                    "r",
                    "(9)where",
                    "τ",
                    "g",
                    "(|x|)",
                    "=",
                    "min{t|g(t)",
                    "=",
                    "|x|}",
                    "(10)"
                ]
            ],
            "context": [
                2,
                0,
                0,
                0
            ]
        },
        "input": "sent0: We use BLEU #TARGET_REF 10 and Average Lagging (AL) #REF 11 to evaluate translation quality and latency respectively.\n sent1: AL measures the degree the user is out of sync with the speaker.\n sent2: As shown in Eq.9-10, t is decoding step, τ is cut-off decoding step where source sentence is finished, g(t) denotes the number of source words read by the encoder at decoding step t, and r = |x|/|y| is the target-to-source length ratio.\n sent3: The smaller the AL (roughly equivalent to k) is, the more real-time the simultaneous translation system is.AL g (x, y) = 1 τ τ t=1 g(t) − t − 1 r (9)where τ g (|x|) = min{t|g(t) = |x|} (10)\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "To",
                    "address",
                    "our",
                    "considerations",
                    "related",
                    "to",
                    "resource",
                    "constraints",
                    "we",
                    "perform",
                    "a",
                    "hyperparameter",
                    "optimization,",
                    "that",
                    "has",
                    "proven",
                    "to",
                    "lead",
                    "to",
                    "better",
                    "solutions",
                    "in",
                    "less",
                    "time."
                ],
                [
                    "It",
                    "is",
                    "based",
                    "on",
                    "a",
                    "population-based",
                    "learning",
                    "#TARGET_REF",
                    "in",
                    "which",
                    "a",
                    "population",
                    "of",
                    "models",
                    "and",
                    "their",
                    "hyperparameters",
                    "are",
                    "jointly",
                    "optimized."
                ],
                [
                    "To",
                    "this",
                    "end,",
                    "we",
                    "build",
                    "a",
                    "validation",
                    "set",
                    "by",
                    "randomly",
                    "extracting",
                    "10%",
                    "of",
                    "the",
                    "training",
                    "data."
                ]
            ],
            "context": [
                3,
                2,
                0
            ]
        },
        "input": "sent0: To address our considerations related to resource constraints we perform a hyperparameter optimization, that has proven to lead to better solutions in less time.\n sent1: It is based on a population-based learning #TARGET_REF in which a population of models and their hyperparameters are jointly optimized.\n sent2: To this end, we build a validation set by randomly extracting 10% of the training data.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "keystroke",
                    "logging",
                    "data",
                    "gathered",
                    "for",
                    "Spanish-English",
                    "post-editors",
                    "allowed",
                    "the",
                    "computation",
                    "of",
                    "Pause",
                    "to",
                    "Word",
                    "Ratio",
                    "(PWR)."
                ],
                [
                    "For",
                    "each",
                    "segment,",
                    "PWR",
                    "is",
                    "the",
                    "ratio",
                    "of",
                    "the",
                    "number",
                    "of",
                    "pauses",
                    "exceeding",
                    "300ms",
                    "to",
                    "the",
                    "number",
                    "of",
                    "words",
                    "in",
                    "the",
                    "MT",
                    "segment,",
                    "it",
                    "is",
                    "a",
                    "measure",
                    "of",
                    "cognitive",
                    "effort",
                    "in",
                    "post-editing",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Higher",
                    "PWR",
                    "corresponds",
                    "to",
                    "higher",
                    "cognitive",
                    "effort."
                ],
                [
                    "Contrary",
                    "to",
                    "expectation,",
                    "the",
                    "mean",
                    "PWR",
                    "for",
                    "Spanish-English",
                    "post-editors",
                    "was",
                    "slightly",
                    "higher",
                    "for",
                    "the",
                    "segments",
                    "with",
                    "alignment",
                    "(0.70)",
                    "than",
                    "for",
                    "those",
                    "without",
                    "alignment",
                    "(0.63)."
                ],
                [
                    "However,",
                    "the",
                    "numerical",
                    "difference",
                    "was",
                    "not",
                    "significant."
                ]
            ],
            "context": [
                0,
                1,
                3,
                3,
                0
            ]
        },
        "input": "sent0: The keystroke logging data gathered for Spanish-English post-editors allowed the computation of Pause to Word Ratio (PWR).\n sent1: For each segment, PWR is the ratio of the number of pauses exceeding 300ms to the number of words in the MT segment, it is a measure of cognitive effort in post-editing #TARGET_REF .\n sent2: Higher PWR corresponds to higher cognitive effort.\n sent3: Contrary to expectation, the mean PWR for Spanish-English post-editors was slightly higher for the segments with alignment (0.70) than for those without alignment (0.63).\n sent4: However, the numerical difference was not significant.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "are",
                    "currently",
                    "investigating",
                    "whether",
                    "our",
                    "model",
                    "is",
                    "consistent",
                    "with",
                    "human",
                    "language",
                    "processing",
                    "which",
                    "has",
                    "limited",
                    "memory",
                    "capacity",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                2
            ]
        },
        "input": "sent0: We are currently investigating whether our model is consistent with human language processing which has limited memory capacity #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "TAG",
                    "is",
                    "a",
                    "grammar",
                    "formalism",
                    "based",
                    "on",
                    "trees",
                    "rather",
                    "than",
                    "context",
                    "free",
                    "rules",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Elementary",
                    "trees",
                    "are",
                    "of",
                    "two",
                    "types,",
                    "initial",
                    "trees",
                    "and",
                    "auxiliary",
                    "trees."
                ],
                [
                    "Auxiliary",
                    "trees",
                    "have",
                    "a",
                    "designated",
                    "foot",
                    "node,",
                    "marked",
                    "with",
                    "a",
                    "*,",
                    "whose",
                    "label",
                    "is",
                    "the",
                    "same",
                    "as",
                    "that",
                    "of",
                    "the",
                    "root."
                ],
                [
                    "In",
                    "Figure",
                    "6,",
                    "«",
                    "½",
                    "and",
                    "«",
                    "¾",
                    "are",
                    "initial",
                    "trees,",
                    "¬",
                    "½",
                    "is",
                    "an",
                    "auxiliary",
                    "tree."
                ],
                [
                    "The",
                    "trees",
                    "are",
                    "combined",
                    "together",
                    "by",
                    "two",
                    "operations,",
                    "substitution",
                    "and",
                    "adjunction."
                ],
                [
                    "Under",
                    "substitution,",
                    "a",
                    "node",
                    "marked",
                    "for",
                    "substitution",
                    "5",
                    "in",
                    "a",
                    "tree",
                    "is",
                    "replaced",
                    "by",
                    "an",
                    "initial",
                    "tree",
                    "with",
                    "the",
                    "same",
                    "label",
                    "at",
                    "the",
                    "root,",
                    "under",
                    "adjunction,",
                    "an",
                    "internal",
                    "node",
                    "in",
                    "a",
                    "tree",
                    "is",
                    "'split",
                    "apart',",
                    "replaced",
                    "by",
                    "an",
                    "auxiliary",
                    "tree",
                    "with",
                    "the",
                    "same",
                    "label",
                    "at",
                    "the",
                    "root",
                    "and",
                    "foot."
                ],
                [
                    "In",
                    "the",
                    "DERIVED",
                    "TREE",
                    "for",
                    "the",
                    "string",
                    ",",
                    "in",
                    "Figure",
                    "6,",
                    "copies",
                    "of",
                    "¬",
                    "½",
                    "have",
                    "been",
                    "adjoined",
                    "either",
                    "at",
                    "the",
                    "root",
                    "node",
                    "labelled",
                    "of",
                    "other",
                    "nodes",
                    "¬",
                    "½",
                    "or",
                    "ultimately",
                    "at",
                    "the",
                    "node",
                    "of",
                    "«",
                    "½",
                    ",",
                    "an",
                    "«",
                    "¾",
                    "tree",
                    "has",
                    "been",
                    "substituted",
                    "into",
                    "each",
                    "¬",
                    "½",
                    "tree",
                    "at",
                    "the",
                    "node",
                    "labelled",
                    "."
                ],
                [
                    "The",
                    "derivation",
                    "history",
                    "is",
                    "recorded",
                    "in",
                    "the",
                    "DERIVATION",
                    "TREE",
                    "(Figure",
                    "6)."
                ],
                [
                    "It",
                    "can",
                    "be",
                    "seen",
                    "that",
                    "the",
                    "TAG",
                    "property",
                    "of",
                    "an",
                    "'extended",
                    "domain",
                    "of",
                    "locality'",
                    "can",
                    "allow",
                    "the",
                    "two",
                    "s",
                    "in",
                    "the",
                    "generated",
                    "string",
                    "to",
                    "be",
                    "separated",
                    "by",
                    "an",
                    "abitrary",
                    "amount",
                    "of",
                    "intervening",
                    "material,",
                    "this",
                    "characteristic",
                    "is",
                    "used",
                    "for",
                    "representation",
                    "of,",
                    "for",
                    "example,",
                    "WH-phenomena",
                    "when",
                    "TAG",
                    "derived",
                    "trees",
                    "are",
                    "used",
                    "for",
                    "a",
                    "linguistic",
                    "representation."
                ],
                [
                    "Of",
                    "more",
                    "interest",
                    "for",
                    "us",
                    "than",
                    "the",
                    "derived",
                    "string",
                    "is",
                    "the",
                    "nature",
                    "of",
                    "the",
                    "derived",
                    "tree:",
                    "the",
                    "branches",
                    "containing",
                    "the",
                    "nodes",
                    "in",
                    "the",
                    "derived",
                    "tree",
                    "are",
                    "also",
                    "separated",
                    "by",
                    "an",
                    "arbitary",
                    "distance."
                ]
            ],
            "context": [
                1,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: TAG is a grammar formalism based on trees rather than context free rules #TARGET_REF .\n sent1: Elementary trees are of two types, initial trees and auxiliary trees.\n sent2: Auxiliary trees have a designated foot node, marked with a *, whose label is the same as that of the root.\n sent3: In Figure 6, « ½ and « ¾ are initial trees, ¬ ½ is an auxiliary tree.\n sent4: The trees are combined together by two operations, substitution and adjunction.\n sent5: Under substitution, a node marked for substitution 5 in a tree is replaced by an initial tree with the same label at the root, under adjunction, an internal node in a tree is 'split apart', replaced by an auxiliary tree with the same label at the root and foot.\n sent6: In the DERIVED TREE for the string , in Figure 6, copies of ¬ ½ have been adjoined either at the root node labelled of other nodes ¬ ½ or ultimately at the node of « ½ , an « ¾ tree has been substituted into each ¬ ½ tree at the node labelled .\n sent7: The derivation history is recorded in the DERIVATION TREE (Figure 6).\n sent8: It can be seen that the TAG property of an 'extended domain of locality' can allow the two s in the generated string to be separated by an abitrary amount of intervening material, this characteristic is used for representation of, for example, WH-phenomena when TAG derived trees are used for a linguistic representation.\n sent9: Of more interest for us than the derived string is the nature of the derived tree: the branches containing the nodes in the derived tree are also separated by an arbitary distance.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\", \"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "There",
                    "remain,",
                    "too,",
                    "crucial",
                    "classes",
                    "of",
                    "cases",
                    "that",
                    "seem",
                    "to",
                    "need",
                    "symbolic",
                    "inference:",
                    "an",
                    "old,",
                    "self-serving,",
                    "one",
                    "will",
                    "do",
                    "such",
                    "as",
                    "\"The",
                    "soldiers",
                    "fired",
                    "at",
                    "the",
                    "women",
                    "and",
                    "I",
                    "saw",
                    "several",
                    "fall\"",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                1
            ]
        },
        "input": "sent0: There remain, too, crucial classes of cases that seem to need symbolic inference: an old, self-serving, one will do such as \"The soldiers fired at the women and I saw several fall\" #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Before",
                    "the",
                    "advent",
                    "in",
                    "research",
                    "pertaining",
                    "to",
                    "toxic",
                    "texts,",
                    "#TARGET_REF",
                    "modeled",
                    "hate",
                    "speech",
                    "as",
                    "a",
                    "word",
                    "sense",
                    "disambiguation",
                    "problem",
                    "where",
                    "SVM",
                    "was",
                    "used",
                    "for",
                    "classification",
                    "of",
                    "data."
                ],
                [
                    "#REF",
                    "used",
                    "RNN",
                    "Language",
                    "Model",
                    "with",
                    "character",
                    "and",
                    "token",
                    "based",
                    "methods",
                    "to",
                    "classify",
                    "the",
                    "text."
                ],
                [
                    "Recently,",
                    "however,",
                    "toxic",
                    "text",
                    "detection",
                    "has",
                    "garnered",
                    "a",
                    "lot",
                    "of",
                    "attention",
                    "#REF",
                    "."
                ],
                [
                    "The",
                    "increase",
                    "in",
                    "offensive",
                    "language",
                    "research",
                    "can",
                    "partly",
                    "be",
                    "credited",
                    "to",
                    "various",
                    "workshops",
                    "such",
                    "as",
                    "Abusive",
                    "Language",
                    "Online",
                    "1",
                    "#REF",
                    ",",
                    "as",
                    "well",
                    "as",
                    "other",
                    "fora,",
                    "such",
                    "as",
                    "GermEval",
                    "for",
                    "German",
                    "texts,",
                    "2",
                    "or",
                    "TRAC",
                    "#REF",
                    "and",
                    "Kaggle",
                    "challenges",
                    "3",
                    "."
                ]
            ],
            "context": [
                1,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Before the advent in research pertaining to toxic texts, #TARGET_REF modeled hate speech as a word sense disambiguation problem where SVM was used for classification of data.\n sent1: #REF used RNN Language Model with character and token based methods to classify the text.\n sent2: Recently, however, toxic text detection has garnered a lot of attention #REF .\n sent3: The increase in offensive language research can partly be credited to various workshops such as Abusive Language Online 1 #REF , as well as other fora, such as GermEval for German texts, 2 or TRAC #REF and Kaggle challenges 3 .\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Second,",
                    "we",
                    "use",
                    "the",
                    "PE",
                    "2",
                    "rr",
                    "dataset",
                    "#TARGET_REF",
                    ",",
                    "which",
                    "is",
                    "a",
                    "manually",
                    "annotated",
                    "error",
                    "analysis",
                    "of",
                    "MT",
                    "output."
                ],
                [
                    "Each",
                    "example",
                    "in",
                    "the",
                    "dataset",
                    "consists",
                    "of",
                    "a",
                    "source",
                    "sentence,",
                    "one",
                    "MT",
                    "output,",
                    "and",
                    "two",
                    "correct",
                    "translations,",
                    "along",
                    "with",
                    "two",
                    "error",
                    "annotations."
                ],
                [
                    "These",
                    "annotations",
                    "are",
                    "word-level",
                    "annotations",
                    "into",
                    "8",
                    "broadly",
                    "non-linguistic",
                    "classes,",
                    "such",
                    "as",
                    "German",
                    "Source:",
                    "Frauen",
                    ",",
                    "die",
                    "in",
                    "Burkina",
                    "Faso",
                    "zu",
                    "Hexen",
                    "abgestempelt",
                    "werden",
                    ",",
                    "weisen",
                    "in",
                    "der",
                    "Regel",
                    "einige",
                    "gemeinsame",
                    "gesellschaftliche",
                    "Merkmale",
                    "auf",
                    "."
                ],
                [
                    "Original",
                    "Translation",
                    "(Annotated):",
                    "Women",
                    "in",
                    "Burkina",
                    "Faso",
                    "[miss]",
                    "are",
                    "branded",
                    "as",
                    "witches",
                    ",",
                    "usually",
                    "[miss]",
                    "some",
                    "common",
                    "social",
                    "features",
                    "."
                ],
                [
                    "Post-edit:",
                    "Women",
                    "in",
                    "Burkina",
                    "Faso",
                    "who",
                    "are",
                    "branded",
                    "as",
                    "witches",
                    "usually",
                    "have",
                    "some",
                    "common",
                    "social",
                    "features",
                    "."
                ],
                [
                    "Original",
                    "Reference:",
                    "Women",
                    "declared",
                    "as",
                    "witches",
                    "in",
                    "Burkina",
                    "Faso",
                    "usually",
                    "have",
                    "several",
                    "common",
                    "characteristics",
                    "."
                ],
                [
                    "\"addition\",",
                    "\"lexical",
                    "error\",",
                    "or",
                    "\"untranslated\"."
                ],
                [
                    "See",
                    "Figure",
                    "2",
                    "for",
                    "an",
                    "example."
                ]
            ],
            "context": [
                1,
                1,
                3,
                3,
                3,
                3,
                3,
                3
            ]
        },
        "input": "sent0: Second, we use the PE 2 rr dataset #TARGET_REF , which is a manually annotated error analysis of MT output.\n sent1: Each example in the dataset consists of a source sentence, one MT output, and two correct translations, along with two error annotations.\n sent2: These annotations are word-level annotations into 8 broadly non-linguistic classes, such as German Source: Frauen , die in Burkina Faso zu Hexen abgestempelt werden , weisen in der Regel einige gemeinsame gesellschaftliche Merkmale auf .\n sent3: Original Translation (Annotated): Women in Burkina Faso [miss] are branded as witches , usually [miss] some common social features .\n sent4: Post-edit: Women in Burkina Faso who are branded as witches usually have some common social features .\n sent5: Original Reference: Women declared as witches in Burkina Faso usually have several common characteristics .\n sent6: \"addition\", \"lexical error\", or \"untranslated\".\n sent7: See Figure 2 for an example.\n",
        "output": "{\"INFORMATION\": [\"sent0\", \"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent2\", \"sent3\", \"sent4\", \"sent5\", \"sent6\", \"sent7\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "use",
                    "of",
                    "both",
                    "language",
                    "and",
                    "vision",
                    "transfer",
                    "learning",
                    "is",
                    "important",
                    "for",
                    "multimodal",
                    "tasks."
                ],
                [
                    "This",
                    "line",
                    "of",
                    "research",
                    "has",
                    "attracted",
                    "much",
                    "attention",
                    "with",
                    "various",
                    "new",
                    "language-vision",
                    "models,",
                    "such",
                    "as",
                    "VilBERT",
                    "#TARGET_REF",
                    ",",
                    "12-in-1",
                    "#REF",
                    "."
                ],
                [
                    "No",
                    "participants",
                    "employ",
                    "into",
                    "this",
                    "approach",
                    "in",
                    "the",
                    "ReINTEL",
                    "challenge",
                    "due",
                    "to",
                    "the",
                    "lack",
                    "of",
                    "language",
                    "and",
                    "vision",
                    "pre-trained",
                    "models",
                    "in",
                    "Vietnamese."
                ],
                [
                    "Moreover,",
                    "it",
                    "is",
                    "required",
                    "to",
                    "have",
                    "extensive",
                    "computer",
                    "resources",
                    "for",
                    "applying",
                    "this",
                    "approach",
                    "in",
                    "a",
                    "data",
                    "challenge."
                ],
                [
                    "In",
                    "the",
                    "future,",
                    "we",
                    "expect",
                    "to",
                    "see",
                    "more",
                    "research",
                    "done",
                    "in",
                    "this",
                    "direction",
                    "because",
                    "both",
                    "images",
                    "and",
                    "texts",
                    "are",
                    "essential",
                    "to",
                    "SNS",
                    "issues."
                ]
            ],
            "context": [
                3,
                3,
                0,
                0,
                0
            ]
        },
        "input": "sent0: The use of both language and vision transfer learning is important for multimodal tasks.\n sent1: This line of research has attracted much attention with various new language-vision models, such as VilBERT #TARGET_REF , 12-in-1 #REF .\n sent2: No participants employ into this approach in the ReINTEL challenge due to the lack of language and vision pre-trained models in Vietnamese.\n sent3: Moreover, it is required to have extensive computer resources for applying this approach in a data challenge.\n sent4: In the future, we expect to see more research done in this direction because both images and texts are essential to SNS issues.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "A",
                    "linear",
                    "indexed",
                    "grammar",
                    "is",
                    "a",
                    "tuple",
                    "(Vr,",
                    "V",
                    "N",
                    ",",
                    "Vi,",
                    "P,",
                    "S),",
                    "where",
                    "Vr",
                    "is",
                    "a",
                    "finite",
                    "set",
                    "of",
                    "terminals,",
                    "V",
                    "N",
                    "a",
                    "finite",
                    "set",
                    "of",
                    "non-terminals,",
                    "Vi",
                    "is",
                    "a",
                    "finite",
                    "set",
                    "of",
                    "indices,",
                    "SE",
                    "V",
                    "N",
                    "is",
                    "the",
                    "start",
                    "symbol",
                    "and",
                    "Pisa",
                    "finite",
                    "set",
                    "of",
                    "productions."
                ],
                [
                    "Following",
                    "#TARGET_REF",
                    "we",
                    "consider",
                    "productions",
                    "in",
                    "which",
                    "at",
                    "most",
                    "one",
                    "element",
                    "can",
                    "be",
                    "pushed",
                    "on",
                    "or",
                    "popped",
                    "from",
                    "a",
                    "stack",
                    "of",
                    "indices",
                    ":Ao",
                    "[oo",
                    "1",
                    "]",
                    "➔",
                    "Ai",
                    "[]",
                    "...",
                    "Ai",
                    "-i",
                    "l",
                    "]",
                    "Ai[oo,']",
                    "Ai-i",
                    "l",
                    "]",
                    "...",
                    "Am",
                    "[]",
                    "Ao",
                    "[",
                    "]",
                    "➔",
                    "awhere",
                    "m",
                    "is",
                    "the",
                    "length",
                    "of",
                    "the",
                    "production,",
                    "A",
                    "i",
                    "E",
                    "VN",
                    "for",
                    "each",
                    "O",
                    "-�",
                    "j",
                    "�",
                    "m,",
                    "Ai",
                    "is",
                    "the",
                    "dependent",
                    "child,",
                    "oo",
                    "is",
                    "the",
                    "part",
                    "of",
                    "the",
                    "indices",
                    "stack",
                    "transmitted",
                    "from",
                    "the",
                    "father",
                    "to",
                    "the",
                    "dependent",
                    "child,",
                    ",",
                    ",",
                    ",'",
                    "E",
                    "Vi",
                    "U",
                    "{",
                    "€}",
                    "and",
                    "for",
                    "each",
                    "production",
                    "either",
                    ",",
                    "or",
                    ",'",
                    "or",
                    "both",
                    "must",
                    "be",
                    "€",
                    "and",
                    "a",
                    "E",
                    "VT",
                    "U",
                    "{",
                    "€}."
                ]
            ],
            "context": [
                0,
                0
            ]
        },
        "input": "sent0: A linear indexed grammar is a tuple (Vr, V N , Vi, P, S), where Vr is a finite set of terminals, V N a finite set of non-terminals, Vi is a finite set of indices, SE V N is the start symbol and Pisa finite set of productions.\n sent1: Following #TARGET_REF we consider productions in which at most one element can be pushed on or popped from a stack of indices :Ao [oo 1 ] ➔ Ai [] ... Ai -i l ] Ai[oo,'] Ai-i l ] ... Am [] Ao [ ] ➔ awhere m is the length of the production, A i E VN for each O -� j � m, Ai is the dependent child, oo is the part of the indices stack transmitted from the father to the dependent child, , , ,' E Vi U { €} and for each production either , or ,' or both must be € and a E VT U { €}.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "This",
                    "2021",
                    "instantiation",
                    "of",
                    "the",
                    "Shared",
                    "Task",
                    "on",
                    "Explanation",
                    "Regeneration",
                    "focuses",
                    "on",
                    "the",
                    "theme",
                    "of",
                    "determining",
                    "relevance",
                    "in",
                    "large",
                    "multi-hop",
                    "explanations."
                ],
                [
                    "To",
                    "this",
                    "end,",
                    "participants",
                    "were",
                    "given",
                    "access",
                    "to",
                    "a",
                    "large",
                    "pre-release",
                    "dataset",
                    "of",
                    "approximately",
                    "250k",
                    "explanatory",
                    "relevancy",
                    "ratings",
                    "that",
                    "augment",
                    "the",
                    "2020",
                    "shared",
                    "task",
                    "data",
                    "#TARGET_REF",
                    ",",
                    "and",
                    "were",
                    "tasked",
                    "with",
                    "ranking",
                    "the",
                    "facts",
                    "most",
                    "critical",
                    "to",
                    "assembling",
                    "large",
                    "explanations",
                    "for",
                    "a",
                    "given",
                    "question",
                    "highest."
                ],
                [
                    "Similarly",
                    "to",
                    "the",
                    "previous",
                    "instances",
                    "of",
                    "our",
                    "competition,",
                    "the",
                    "shared",
                    "task",
                    "has",
                    "been",
                    "organized",
                    "on",
                    "the",
                    "CodaLab",
                    "platform."
                ],
                [
                    "1",
                    "We",
                    "released",
                    "train",
                    "and",
                    "development",
                    "datasets",
                    "along",
                    "with",
                    "the",
                    "baseline",
                    "solution",
                    "in",
                    "advance",
                    "to",
                    "allow",
                    "one",
                    "to",
                    "get",
                    "to",
                    "know",
                    "the",
                    "task",
                    "specifics."
                ],
                [
                    "2",
                    "We",
                    "ran",
                    "the",
                    "practice",
                    "phase",
                    "from",
                    "February",
                    "15",
                    "till",
                    "March",
                    "9,",
                    "2021."
                ],
                [
                    "Then",
                    "we",
                    "released",
                    "the",
                    "test",
                    "dataset",
                    "without",
                    "answers",
                    "and",
                    "ran",
                    "the",
                    "official",
                    "evaluation",
                    "phase",
                    "from",
                    "March",
                    "10",
                    "till",
                    "March",
                    "24,",
                    "2021."
                ],
                [
                    "After",
                    "that",
                    "we",
                    "established",
                    "postcompetition",
                    "phase",
                    "to",
                    "enable",
                    "long-term",
                    "evaluation",
                    "of",
                    "the",
                    "methods",
                    "beyond",
                    "our",
                    "shared",
                    "task."
                ],
                [
                    "Participating",
                    "systems",
                    "substantially",
                    "increased",
                    "task",
                    "performance",
                    "compared",
                    "to",
                    "a",
                    "supplied",
                    "baseline",
                    "system",
                    "by",
                    "32%,",
                    "while",
                    "achieving",
                    "moderate",
                    "overall",
                    "absolute",
                    "task",
                    "performance",
                    "-highlighting",
                    "both",
                    "the",
                    "success",
                    "of",
                    "this",
                    "shared",
                    "task,",
                    "as",
                    "well",
                    "as",
                    "the",
                    "continued",
                    "challenge",
                    "of",
                    "determining",
                    "relevancy",
                    "in",
                    "large",
                    "multi-hop",
                    "inference",
                    "problems."
                ]
            ],
            "context": [
                0,
                3,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: This 2021 instantiation of the Shared Task on Explanation Regeneration focuses on the theme of determining relevance in large multi-hop explanations.\n sent1: To this end, participants were given access to a large pre-release dataset of approximately 250k explanatory relevancy ratings that augment the 2020 shared task data #TARGET_REF , and were tasked with ranking the facts most critical to assembling large explanations for a given question highest.\n sent2: Similarly to the previous instances of our competition, the shared task has been organized on the CodaLab platform.\n sent3: 1 We released train and development datasets along with the baseline solution in advance to allow one to get to know the task specifics.\n sent4: 2 We ran the practice phase from February 15 till March 9, 2021.\n sent5: Then we released the test dataset without answers and ran the official evaluation phase from March 10 till March 24, 2021.\n sent6: After that we established postcompetition phase to enable long-term evaluation of the methods beyond our shared task.\n sent7: Participating systems substantially increased task performance compared to a supplied baseline system by 32%, while achieving moderate overall absolute task performance -highlighting both the success of this shared task, as well as the continued challenge of determining relevancy in large multi-hop inference problems.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "•",
                    "Top-k",
                    "attention",
                    "also",
                    "reduces",
                    "memory",
                    "consumption",
                    "in",
                    "Transformer",
                    "feed-forward",
                    "layers,",
                    "by",
                    "casting",
                    "this",
                    "layer",
                    "into",
                    "the",
                    "familiar",
                    "query-key-value",
                    "framework",
                    "using",
                    "ReLU",
                    "instead",
                    "of",
                    "the",
                    "row-wise",
                    "softmax",
                    "#REF",
                    "."
                ],
                [
                    "This",
                    "is",
                    "specifically",
                    "appealing",
                    "in",
                    "models",
                    "such",
                    "as",
                    "T5",
                    "#REF",
                    "and",
                    "GPT-3",
                    "#REF",
                    ",",
                    "where",
                    "for",
                    "short",
                    "inputs,",
                    "the",
                    "memory",
                    "consumption",
                    "is",
                    "dominated",
                    "by",
                    "the",
                    "feed-forward",
                    "layers,",
                    "as",
                    "the",
                    "number",
                    "of",
                    "keys,",
                    "corresponding",
                    "to",
                    "the",
                    "feedforward",
                    "hidden",
                    "dimension",
                    "size,",
                    "is",
                    "as",
                    "large",
                    "as",
                    "65K."
                ],
                [
                    "Conversely,",
                    "methods",
                    "that",
                    "rely",
                    "on",
                    "random",
                    "feature",
                    "approximations",
                    "of",
                    "attention,",
                    "such",
                    "as",
                    "Performer",
                    "#REF",
                    "and",
                    "RFA",
                    "#REF",
                    "do",
                    "not",
                    "admit",
                    "an",
                    "efficient",
                    "approximation",
                    "for",
                    "the",
                    "ReLU",
                    "activation",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                0,
                0,
                1
            ]
        },
        "input": "sent0: • Top-k attention also reduces memory consumption in Transformer feed-forward layers, by casting this layer into the familiar query-key-value framework using ReLU instead of the row-wise softmax #REF .\n sent1: This is specifically appealing in models such as T5 #REF and GPT-3 #REF , where for short inputs, the memory consumption is dominated by the feed-forward layers, as the number of keys, corresponding to the feedforward hidden dimension size, is as large as 65K.\n sent2: Conversely, methods that rely on random feature approximations of attention, such as Performer #REF and RFA #REF do not admit an efficient approximation for the ReLU activation #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "this",
                    "point",
                    "because",
                    "it",
                    "is",
                    "a",
                    "system",
                    "that",
                    "has",
                    "the",
                    "goal",
                    "of",
                    "exploring",
                    "the",
                    "feasibility",
                    "of",
                    "a",
                    "plug-and-play",
                    "architecture:",
                    "that",
                    "is,",
                    "necessary",
                    "components",
                    "such",
                    "as",
                    "a",
                    "parser",
                    "are",
                    "obtained",
                    "from",
                    "elsewhere,",
                    "with",
                    "a",
                    "given",
                    "output",
                    "structure",
                    "that",
                    "it",
                    "is",
                    "necessary",
                    "to",
                    "use."
                ],
                [
                    "Given",
                    "this,",
                    "gNCNs",
                    "are",
                    "required",
                    "either",
                    "directly",
                    "or",
                    "indirectly."
                ],
                [
                    "The",
                    "case",
                    "of",
                    "the",
                    "direct",
                    "relation,",
                    "using",
                    "these",
                    "structures",
                    "as",
                    "the",
                    "basis",
                    "for",
                    "a",
                    "transfer",
                    "component,",
                    "is",
                    "illustrated",
                    "already",
                    "in",
                    "the«DXD[the]",
                    "¬COMPs[which]",
                    "«DXD[the]",
                    "¬COMPs[which]",
                    "«DXD[the]",
                    "«NXdxN[floor]",
                    "¬N0nx0Vnx1[covered]",
                    "«NXdxN[dust]",
                    "¬N0nx0Vnx1[collected]",
                    "«NXdxN[jacket]",
                    "¬Vvx[is]",
                    "«nx0Ax1[tweed]",
                    "«DXD[the]",
                    "¬COMPs[which]",
                    "«DXD[the]",
                    "«NXdxN[dust]",
                    "¬N0nx0Vnx1[collected]",
                    "«NXdxN[jacket]",
                    "¬Vvx[is]",
                    "«NXN[it]",
                    "«DXD[the]",
                    "«NXdxN[floor]",
                    "«nx0Vnx1[covered]",
                    "¬sPUs[.]"
                ],
                [
                    "«nx0Ax1[tweed]Figure",
                    "5:",
                    "Derivation",
                    "tree",
                    "pair",
                    "for",
                    "example",
                    "(4)",
                    "lefthand",
                    "pair",
                    "of",
                    "Figure",
                    "4,",
                    "a",
                    "pairing",
                    "indirectly",
                    "involving",
                    "gNCNs",
                    "would",
                    "be",
                    "required",
                    "in",
                    "transforming",
                    "the",
                    "syntactic",
                    "representation",
                    "into",
                    "a",
                    "deeper",
                    "semantic",
                    "one",
                    "(the",
                    "one",
                    "used",
                    "in",
                    "translation),",
                    "as",
                    "in",
                    "the",
                    "righthand",
                    "pair",
                    "of",
                    "Figure",
                    "4."
                ],
                [
                    "This",
                    "latter",
                    "is",
                    "the",
                    "sort",
                    "of",
                    "relation",
                    "that",
                    "may",
                    "need",
                    "to",
                    "be",
                    "specified,",
                    "then,",
                    "in",
                    "a",
                    "formalism",
                    "with",
                    "multiple",
                    "levels,",
                    "such",
                    "as",
                    "MTT."
                ]
            ],
            "context": [
                2,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: this point because it is a system that has the goal of exploring the feasibility of a plug-and-play architecture: that is, necessary components such as a parser are obtained from elsewhere, with a given output structure that it is necessary to use.\n sent1: Given this, gNCNs are required either directly or indirectly.\n sent2: The case of the direct relation, using these structures as the basis for a transfer component, is illustrated already in the«DXD[the] ¬COMPs[which] «DXD[the] ¬COMPs[which] «DXD[the] «NXdxN[floor] ¬N0nx0Vnx1[covered] «NXdxN[dust] ¬N0nx0Vnx1[collected] «NXdxN[jacket] ¬Vvx[is] «nx0Ax1[tweed] «DXD[the] ¬COMPs[which] «DXD[the] «NXdxN[dust] ¬N0nx0Vnx1[collected] «NXdxN[jacket] ¬Vvx[is] «NXN[it] «DXD[the] «NXdxN[floor] «nx0Vnx1[covered] ¬sPUs[.]\n sent3: «nx0Ax1[tweed]Figure 5: Derivation tree pair for example (4) lefthand pair of Figure 4, a pairing indirectly involving gNCNs would be required in transforming the syntactic representation into a deeper semantic one (the one used in translation), as in the righthand pair of Figure 4.\n sent4: This latter is the sort of relation that may need to be specified, then, in a formalism with multiple levels, such as MTT.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "For",
                    "each",
                    "task,",
                    "we",
                    "downloaded",
                    "and",
                    "directly",
                    "used",
                    "the",
                    "vanilla",
                    "Transformer",
                    "code",
                    "offered",
                    "by",
                    "the",
                    "authors",
                    "#TARGET_REF",
                    "and",
                    "compared",
                    "the",
                    "performance",
                    "before",
                    "and",
                    "after",
                    "replacing",
                    "the",
                    "multi-head",
                    "attention",
                    "layers",
                    "with",
                    "top-128",
                    "attention,",
                    "using",
                    "identical",
                    "hyperparameters",
                    "for",
                    "both",
                    "cases",
                    "(details",
                    "in",
                    "§A.1)."
                ],
                [
                    "2",
                    "Test",
                    "accuracy",
                    "measured",
                    "at",
                    "the",
                    "training",
                    "checkpoint",
                    "with",
                    "the",
                    "highest",
                    "accuracy",
                    "on",
                    "the",
                    "development",
                    "set",
                    "is",
                    "reported",
                    "in",
                    "Table",
                    "1",
                    "and",
                    "the",
                    "learning",
                    "curves",
                    "on",
                    "the",
                    "development",
                    "and",
                    "test",
                    "sets",
                    "are",
                    "shown",
                    "in",
                    "Fig."
                ],
                [
                    "5."
                ],
                [
                    "On",
                    "IMDb",
                    "and",
                    "AAN,",
                    "the",
                    "performance",
                    "of",
                    "top-128",
                    "is",
                    "comparable",
                    "or",
                    "better",
                    "than",
                    "vanilla",
                    "attention."
                ],
                [
                    "For",
                    "ListOps,",
                    "there",
                    "is",
                    "a",
                    "minor",
                    "drop",
                    "in",
                    "performance",
                    "(1.5",
                    "points),",
                    "but",
                    "learning",
                    "curves",
                    "(Figure",
                    "5a)",
                    "exhibit",
                    "similar",
                    "behaviour."
                ]
            ],
            "context": [
                2,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: For each task, we downloaded and directly used the vanilla Transformer code offered by the authors #TARGET_REF and compared the performance before and after replacing the multi-head attention layers with top-128 attention, using identical hyperparameters for both cases (details in §A.1).\n sent1: 2 Test accuracy measured at the training checkpoint with the highest accuracy on the development set is reported in Table 1 and the learning curves on the development and test sets are shown in Fig.\n sent2: 5.\n sent3: On IMDb and AAN, the performance of top-128 is comparable or better than vanilla attention.\n sent4: For ListOps, there is a minor drop in performance (1.5 points), but learning curves (Figure 5a) exhibit similar behaviour.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "All",
                    "three",
                    "subtasks",
                    "are",
                    "addressed",
                    "simultaneously",
                    "using",
                    "a",
                    "Multi-Task",
                    "Learning",
                    "(MTL)",
                    "architecture",
                    "#TARGET_REF",
                    "that",
                    "leverages",
                    "acquired",
                    "knowledge",
                    "from",
                    "one",
                    "subtask",
                    "to",
                    "another."
                ],
                [
                    "Furthermore,",
                    "we",
                    "approached",
                    "the",
                    "challenge",
                    "of",
                    "unbalanced",
                    "classes",
                    "in",
                    "the",
                    "first",
                    "subtask",
                    "by",
                    "considering",
                    "class",
                    "weights",
                    "and",
                    "by",
                    "augmenting",
                    "the",
                    "training",
                    "data",
                    "set."
                ]
            ],
            "context": [
                1,
                3
            ]
        },
        "input": "sent0: All three subtasks are addressed simultaneously using a Multi-Task Learning (MTL) architecture #TARGET_REF that leverages acquired knowledge from one subtask to another.\n sent1: Furthermore, we approached the challenge of unbalanced classes in the first subtask by considering class weights and by augmenting the training data set.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "More",
                    "recently,",
                    "#REF",
                    "run",
                    "a",
                    "large-scale",
                    "comparison",
                    "of",
                    "MT",
                    "metrics,",
                    "including",
                    "BERTScore",
                    "using",
                    "a",
                    "large",
                    "dataset",
                    "of",
                    "translations",
                    "with",
                    "human",
                    "judgments,",
                    "they",
                    "find",
                    "that",
                    "BERTScore's",
                    "performance",
                    "is",
                    "middle-of-the-road,",
                    "though",
                    "better",
                    "than",
                    "BLEU,",
                    "and",
                    "recommend",
                    "COMET",
                    "#TARGET_REF",
                    "for",
                    "general",
                    "use."
                ]
            ],
            "context": [
                2
            ]
        },
        "input": "sent0: More recently, #REF run a large-scale comparison of MT metrics, including BERTScore using a large dataset of translations with human judgments, they find that BERTScore's performance is middle-of-the-road, though better than BLEU, and recommend COMET #TARGET_REF for general use.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "i",
                    "[CLS]",
                    "Strategy",
                    "-This",
                    "is",
                    "the",
                    "default",
                    "sentence",
                    "pair",
                    "classification",
                    "architecture",
                    "with",
                    "transformers",
                    "#TARGET_REF",
                    "where",
                    "the",
                    "two",
                    "sentences",
                    "are",
                    "concatenated",
                    "with",
                    "a",
                    "[SEP]",
                    "token",
                    "and",
                    "passed",
                    "through",
                    "a",
                    "transformer",
                    "model."
                ],
                [
                    "Then",
                    "the",
                    "output",
                    "of",
                    "the",
                    "#REF",
                    "token",
                    "is",
                    "fed",
                    "into",
                    "a",
                    "softmax",
                    "layer",
                    "to",
                    "predict",
                    "the",
                    "labels",
                    "(Figure",
                    "1)."
                ]
            ],
            "context": [
                1,
                3
            ]
        },
        "input": "sent0: i [CLS] Strategy -This is the default sentence pair classification architecture with transformers #TARGET_REF where the two sentences are concatenated with a [SEP] token and passed through a transformer model.\n sent1: Then the output of the #REF token is fed into a softmax layer to predict the labels (Figure 1).\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "latest",
                    "article",
                    "on",
                    "the",
                    "topic",
                    "is",
                    "#REF",
                    ",",
                    "in",
                    "which,",
                    "in",
                    "addition",
                    "to",
                    "the",
                    "previously",
                    "used",
                    "Skip-gram",
                    "and",
                    "GloVe",
                    "word",
                    "embeddings,",
                    "to",
                    "produce",
                    "their",
                    "similarity",
                    "matrices",
                    "they",
                    "use",
                    "•",
                    "FastText",
                    "#TARGET_REF",
                    ","
                ]
            ],
            "context": [
                2
            ]
        },
        "input": "sent0: The latest article on the topic is #REF , in which, in addition to the previously used Skip-gram and GloVe word embeddings, to produce their similarity matrices they use • FastText #TARGET_REF ,\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Intended",
                    "use."
                ],
                [
                    "One",
                    "primary",
                    "goal",
                    "of",
                    "NLP",
                    "models",
                    "is",
                    "the",
                    "generalization",
                    "to",
                    "real-world",
                    "inputs."
                ],
                [
                    "However,",
                    "existing",
                    "test",
                    "datasets",
                    "and",
                    "templates",
                    "are",
                    "often",
                    "not",
                    "comprehensive,",
                    "and",
                    "thus",
                    "it",
                    "is",
                    "difficult",
                    "to",
                    "evaluate",
                    "real-world",
                    "performance",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Our",
                    "work",
                    "sheds",
                    "a",
                    "light",
                    "on",
                    "quantifying",
                    "performance",
                    "for",
                    "inputs",
                    "beyond",
                    "the",
                    "test",
                    "dataset",
                    "and",
                    "help",
                    "uncover",
                    "model",
                    "weaknesses",
                    "prior",
                    "to",
                    "the",
                    "realworld",
                    "deployment."
                ],
                [
                    "Misuse",
                    "potential."
                ],
                [
                    "Similar",
                    "to",
                    "other",
                    "existing",
                    "adversarial",
                    "attack",
                    "methods",
                    "#REF",
                    ",",
                    "our",
                    "second-order",
                    "attacks",
                    "can",
                    "be",
                    "used",
                    "for",
                    "finding",
                    "vulnerable",
                    "examples",
                    "to",
                    "a",
                    "NLP",
                    "system."
                ],
                [
                    "Therefore,",
                    "it",
                    "is",
                    "essential",
                    "to",
                    "study",
                    "how",
                    "to",
                    "improve",
                    "the",
                    "robustness",
                    "of",
                    "NLP",
                    "models",
                    "second-order",
                    "attacks."
                ],
                [
                    "Limitations."
                ],
                [
                    "While",
                    "the",
                    "core",
                    "idea",
                    "about",
                    "the",
                    "double",
                    "perturbation",
                    "framework",
                    "is",
                    "general,",
                    "in",
                    "§4,",
                    "we",
                    "consider",
                    "only",
                    "binary",
                    "gender",
                    "in",
                    "the",
                    "analysis",
                    "of",
                    "counterfactual",
                    "fairness",
                    "due",
                    "to",
                    "the",
                    "restriction",
                    "of",
                    "the",
                    "English",
                    "corpus",
                    "we",
                    "used,",
                    "which",
                    "only",
                    "have",
                    "words",
                    "associated",
                    "with",
                    "binary",
                    "gender",
                    "such",
                    "as",
                    "he/she,",
                    "waiter/waitress,",
                    "etc."
                ]
            ],
            "context": [
                0,
                2,
                1,
                2,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Intended use.\n sent1: One primary goal of NLP models is the generalization to real-world inputs.\n sent2: However, existing test datasets and templates are often not comprehensive, and thus it is difficult to evaluate real-world performance #TARGET_REF .\n sent3: Our work sheds a light on quantifying performance for inputs beyond the test dataset and help uncover model weaknesses prior to the realworld deployment.\n sent4: Misuse potential.\n sent5: Similar to other existing adversarial attack methods #REF , our second-order attacks can be used for finding vulnerable examples to a NLP system.\n sent6: Therefore, it is essential to study how to improve the robustness of NLP models second-order attacks.\n sent7: Limitations.\n sent8: While the core idea about the double perturbation framework is general, in §4, we consider only binary gender in the analysis of counterfactual fairness due to the restriction of the English corpus we used, which only have words associated with binary gender such as he/she, waiter/waitress, etc.\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [\"sent1\", \"sent3\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "above",
                    "conditions",
                    "are,",
                    "of",
                    "course,",
                    "far",
                    "from",
                    "reality,",
                    "since",
                    "such",
                    "a",
                    "distance",
                    "function,",
                    "which",
                    "perfectly",
                    "corresponds",
                    "to",
                    "the",
                    "mental",
                    "representations",
                    "of",
                    "all",
                    "people,",
                    "certainly",
                    "does",
                    "not",
                    "exist."
                ],
                [
                    "This",
                    "is",
                    "clear",
                    "from",
                    "the",
                    "fact",
                    "that",
                    "in",
                    "classical",
                    "association",
                    "tests,",
                    "where",
                    "the",
                    "actual",
                    "task",
                    "is",
                    "to",
                    "find",
                    "nearest",
                    "neighbors,",
                    "the",
                    "subjects",
                    "never",
                    "give",
                    "the",
                    "same",
                    "answer",
                    "#TARGET_REF",
                    ")."
                ],
                [
                    "However,",
                    "it",
                    "is",
                    "a",
                    "meaningful",
                    "task",
                    "to",
                    "create",
                    "a",
                    "similarity",
                    "function",
                    "and",
                    "construct",
                    "a",
                    "similarity",
                    "matrix",
                    "S",
                    "∈",
                    "R",
                    "V",
                    "×V",
                    ",",
                    "in",
                    "which",
                    "S",
                    "ij",
                    "=",
                    "s(w",
                    "i",
                    ",",
                    "w",
                    "j",
                    ")",
                    "approximates",
                    "the",
                    "average",
                    "similarity",
                    "perceived",
                    "by",
                    "people."
                ]
            ],
            "context": [
                3,
                1,
                0
            ]
        },
        "input": "sent0: The above conditions are, of course, far from reality, since such a distance function, which perfectly corresponds to the mental representations of all people, certainly does not exist.\n sent1: This is clear from the fact that in classical association tests, where the actual task is to find nearest neighbors, the subjects never give the same answer #TARGET_REF ).\n sent2: However, it is a meaningful task to create a similarity function and construct a similarity matrix S ∈ R V ×V , in which S ij = s(w i , w j ) approximates the average similarity perceived by people.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Pruning",
                    "Based",
                    "on",
                    "Component."
                ],
                [
                    "Some",
                    "studies",
                    "show",
                    "that",
                    "heads",
                    "in",
                    "the",
                    "ED",
                    "component",
                    "are",
                    "most",
                    "important",
                    "while",
                    "those",
                    "in",
                    "the",
                    "ES",
                    "module",
                    "are",
                    "least",
                    "important",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "We",
                    "choose",
                    "4",
                    "different",
                    "pruning",
                    "percentages",
                    "and",
                    "in",
                    "each",
                    "case",
                    "consider",
                    "three",
                    "configurations",
                    "where",
                    "the",
                    "number",
                    "of",
                    "attention",
                    "heads",
                    "is",
                    "least",
                    "in",
                    "one",
                    "chosen",
                    "component",
                    "(ES,",
                    "ED,",
                    "DS)."
                ],
                [
                    "The",
                    "configurations",
                    "and",
                    "corresponding",
                    "BLEU",
                    "scores",
                    "on",
                    "the",
                    "EN-RU",
                    "dataset",
                    "are",
                    "shown",
                    "in",
                    "Table",
                    "3."
                ],
                [
                    "We",
                    "identify",
                    "no",
                    "consistent",
                    "preference",
                    "in",
                    "the",
                    "pruning",
                    "strategy:",
                    "In",
                    "the",
                    "4",
                    "cases",
                    "considered,",
                    "each",
                    "of",
                    "the",
                    "3",
                    "configurations",
                    "has",
                    "the",
                    "highest",
                    "BLEU",
                    "score",
                    "in",
                    "at",
                    "least",
                    "one",
                    "case."
                ],
                [
                    "Note",
                    "that",
                    "we",
                    "chose",
                    "the",
                    "number",
                    "of",
                    "heads",
                    "in",
                    "each",
                    "layer",
                    "#REF",
                    "to",
                    "be",
                    "consistent",
                    "with",
                    "those",
                    "used",
                    "in",
                    "#REF",
                    "."
                ],
                [
                    "Varying",
                    "Pruning",
                    "Percentage."
                ],
                [
                    "We",
                    "vary",
                    "the",
                    "pruning",
                    "percentage",
                    "from",
                    "10",
                    "to",
                    "90%",
                    "and",
                    "report",
                    "the",
                    "accuracy",
                    "on",
                    "the",
                    "4",
                    "GLUE",
                    "tasks:",
                    "MNLI-M,",
                    "QQP,",
                    "QNLI,",
                    "and",
                    "SST-2",
                    "(Table",
                    "4)."
                ],
                [
                    "We",
                    "observe",
                    "that",
                    "half",
                    "of",
                    "the",
                    "attention",
                    "heads",
                    "can",
                    "be",
                    "pruned",
                    "with",
                    "an",
                    "average",
                    "accuracy",
                    "drop",
                    "of",
                    "under",
                    "1%."
                ],
                [
                    "As",
                    "shown",
                    "in",
                    "Figure",
                    "1,",
                    "beyond",
                    "50%",
                    "pruning,",
                    "the",
                    "accuracy",
                    "drop",
                    "is",
                    "sharper."
                ]
            ],
            "context": [
                3,
                1,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Pruning Based on Component.\n sent1: Some studies show that heads in the ED component are most important while those in the ES module are least important #TARGET_REF .\n sent2: We choose 4 different pruning percentages and in each case consider three configurations where the number of attention heads is least in one chosen component (ES, ED, DS).\n sent3: The configurations and corresponding BLEU scores on the EN-RU dataset are shown in Table 3.\n sent4: We identify no consistent preference in the pruning strategy: In the 4 cases considered, each of the 3 configurations has the highest BLEU score in at least one case.\n sent5: Note that we chose the number of heads in each layer #REF to be consistent with those used in #REF .\n sent6: Varying Pruning Percentage.\n sent7: We vary the pruning percentage from 10 to 90% and report the accuracy on the 4 GLUE tasks: MNLI-M, QQP, QNLI, and SST-2 (Table 4).\n sent8: We observe that half of the attention heads can be pruned with an average accuracy drop of under 1%.\n sent9: As shown in Figure 1, beyond 50% pruning, the accuracy drop is sharper.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "this",
                    "paper",
                    "we",
                    "describe",
                    "our",
                    "Chinese-to-English",
                    "simultaneous",
                    "translation",
                    "system,",
                    "which",
                    "uses",
                    "a",
                    "deep",
                    "Transformer",
                    "to",
                    "improve",
                    "translation",
                    "quality",
                    "and",
                    "adopts",
                    "wait-k",
                    "policy",
                    "#REF",
                    "to",
                    "reduce",
                    "latency."
                ],
                [
                    "Besides,",
                    "for",
                    "better",
                    "domain",
                    "adaption,",
                    "we",
                    "combined",
                    "mixed",
                    "fine-tuning",
                    "#TARGET_REF",
                    "with",
                    "in-domain",
                    "data",
                    "filtering",
                    "(Moore",
                    "and",
                    "Lewis,",
                    "2010,",
                    "#REF",
                    "and",
                    "proposed",
                    "a",
                    "new",
                    "domain",
                    "adaption",
                    "method",
                    "called",
                    "\"in-domain",
                    "mixed",
                    "fine-tuning\",",
                    "which",
                    "is",
                    "empirically",
                    "more",
                    "effective",
                    "than",
                    "fine-tuning",
                    "and",
                    "mixed",
                    "fine-tuning."
                ]
            ],
            "context": [
                2,
                2
            ]
        },
        "input": "sent0: In this paper we describe our Chinese-to-English simultaneous translation system, which uses a deep Transformer to improve translation quality and adopts wait-k policy #REF to reduce latency.\n sent1: Besides, for better domain adaption, we combined mixed fine-tuning #TARGET_REF with in-domain data filtering (Moore and Lewis, 2010, #REF and proposed a new domain adaption method called \"in-domain mixed fine-tuning\", which is empirically more effective than fine-tuning and mixed fine-tuning.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\", \"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "SemEval",
                    "2020-Task",
                    "11",
                    "#REF",
                    ",",
                    "the",
                    "first",
                    "sub-task",
                    "-Span",
                    "Identification",
                    "-aims",
                    "at",
                    "detecting",
                    "the",
                    "beginning",
                    "and",
                    "the",
                    "end",
                    "offset",
                    "for",
                    "the",
                    "propaganda",
                    "spans",
                    "in",
                    "news",
                    "articles."
                ],
                [
                    "This",
                    "sub-task",
                    "is",
                    "similar",
                    "to",
                    "SemEval",
                    "2021-Task",
                    "5."
                ],
                [
                    "The",
                    "proposed",
                    "approaches",
                    "for",
                    "the",
                    "sub-task",
                    "can",
                    "be",
                    "broadly",
                    "classified",
                    "into",
                    "Span",
                    "Prediction",
                    "or",
                    "Token",
                    "Classification."
                ],
                [
                    "Most",
                    "teams",
                    "use",
                    "multi-granular",
                    "transformer-based",
                    "systems",
                    "for",
                    "token",
                    "classification/sequence",
                    "tagging",
                    "#REF",
                    "."
                ],
                [
                    "Inspired",
                    "by",
                    "#TARGET_REF",
                    ",",
                    "#REF",
                    "use",
                    "RoBERTa-CRF",
                    "based",
                    "systems."
                ],
                [
                    "#REF",
                    "use",
                    "a",
                    "variant",
                    "of",
                    "SpanBERT",
                    "span",
                    "prediction",
                    "system."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                2,
                0
            ]
        },
        "input": "sent0: In SemEval 2020-Task 11 #REF , the first sub-task -Span Identification -aims at detecting the beginning and the end offset for the propaganda spans in news articles.\n sent1: This sub-task is similar to SemEval 2021-Task 5.\n sent2: The proposed approaches for the sub-task can be broadly classified into Span Prediction or Token Classification.\n sent3: Most teams use multi-granular transformer-based systems for token classification/sequence tagging #REF .\n sent4: Inspired by #TARGET_REF , #REF use RoBERTa-CRF based systems.\n sent5: #REF use a variant of SpanBERT span prediction system.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent4\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "where",
                    "t",
                    "i",
                    "denotes",
                    "the",
                    "desired",
                    "output,",
                    "i.e.,",
                    "the",
                    "probability",
                    "should",
                    "be",
                    "1.0",
                    "for",
                    "the",
                    "next",
                    "word",
                    "in",
                    "the",
                    "training",
                    "sentence",
                    "and",
                    "0.0",
                    "for",
                    "all",
                    "the",
                    "other",
                    "ones."
                ],
                [
                    "The",
                    "first",
                    "part",
                    "of",
                    "this",
                    "equation",
                    "is",
                    "the",
                    "cross-entropy",
                    "between",
                    "the",
                    "output",
                    "and",
                    "the",
                    "target",
                    "probability",
                    "distributions,",
                    "and",
                    "the",
                    "second",
                    "part",
                    "is",
                    "a",
                    "regularization",
                    "term",
                    "that",
                    "aims",
                    "to",
                    "prevent",
                    "the",
                    "neural",
                    "network",
                    "from",
                    "over-fitting",
                    "the",
                    "training",
                    "data",
                    "(weight",
                    "decay)."
                ],
                [
                    "The",
                    "parameter",
                    "β",
                    "has",
                    "to",
                    "be",
                    "determined",
                    "experimentally."
                ],
                [
                    "Training",
                    "is",
                    "done",
                    "using",
                    "a",
                    "re-sampling",
                    "algorithm",
                    "as",
                    "described",
                    "in",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                0,
                0,
                0,
                1
            ]
        },
        "input": "sent0: where t i denotes the desired output, i.e., the probability should be 1.0 for the next word in the training sentence and 0.0 for all the other ones.\n sent1: The first part of this equation is the cross-entropy between the output and the target probability distributions, and the second part is a regularization term that aims to prevent the neural network from over-fitting the training data (weight decay).\n sent2: The parameter β has to be determined experimentally.\n sent3: Training is done using a re-sampling algorithm as described in #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [\"sent3\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "There",
                    "is",
                    "a",
                    "large",
                    "body",
                    "of",
                    "work",
                    "in",
                    "the",
                    "literature",
                    "showing",
                    "that",
                    "a",
                    "morphological",
                    "decomposition",
                    "of",
                    "the",
                    "Arabic",
                    "words",
                    "can",
                    "improve",
                    "the",
                    "word",
                    "coverage",
                    "and",
                    "by",
                    "these",
                    "means",
                    "the",
                    "translation",
                    "quality,",
                    "see",
                    "for",
                    "instance",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "This",
                    "is",
                    "in",
                    "particular",
                    "true",
                    "for",
                    "under-resourced",
                    "tasks",
                    "like",
                    "this",
                    "evaluation."
                ],
                [
                    "Most",
                    "of",
                    "the",
                    "published",
                    "work",
                    "is",
                    "based",
                    "on",
                    "the",
                    "freely",
                    "available",
                    "tools,",
                    "like",
                    "the",
                    "Buckwalter",
                    "transliterator",
                    "and",
                    "the",
                    "MADA",
                    "and",
                    "TOKAN",
                    "tools",
                    "for",
                    "morphological",
                    "analysis",
                    "from",
                    "Columbia",
                    "University."
                ]
            ],
            "context": [
                1,
                2,
                3
            ]
        },
        "input": "sent0: There is a large body of work in the literature showing that a morphological decomposition of the Arabic words can improve the word coverage and by these means the translation quality, see for instance #TARGET_REF .\n sent1: This is in particular true for under-resourced tasks like this evaluation.\n sent2: Most of the published work is based on the freely available tools, like the Buckwalter transliterator and the MADA and TOKAN tools for morphological analysis from Columbia University.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "this",
                    "work,",
                    "we",
                    "aim",
                    "to",
                    "design",
                    "an",
                    "E2E",
                    "pretraining",
                    "strategy",
                    "for",
                    "the",
                    "VLP",
                    "problem."
                ],
                [
                    "To",
                    "this",
                    "end,",
                    "we",
                    "adopt",
                    "a",
                    "modular",
                    "representation",
                    "network,",
                    "which",
                    "takes",
                    "image",
                    "grid",
                    "features",
                    "from",
                    "a",
                    "CNN-based",
                    "visual",
                    "network",
                    "and",
                    "the",
                    "corresponding",
                    "text",
                    "embeddings",
                    "into",
                    "a",
                    "multi-modal",
                    "Transformer",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Our",
                    "goal",
                    "is",
                    "to",
                    "learn",
                    "the",
                    "visual",
                    "network",
                    "and",
                    "the",
                    "Transformer",
                    "jointly,",
                    "and",
                    "yet",
                    "to",
                    "effectively",
                    "encode",
                    "object-level",
                    "visual",
                    "concepts",
                    "in",
                    "the",
                    "multimodal",
                    "representations."
                ],
                [
                    "This",
                    "enables",
                    "us",
                    "to",
                    "capture",
                    "rich",
                    "cross-modal",
                    "alignment",
                    "between",
                    "linguistic",
                    "entities",
                    "and",
                    "visual",
                    "semantic",
                    "concepts",
                    "for",
                    "the",
                    "downstream",
                    "tasks,",
                    "and",
                    "meanwhile",
                    "to",
                    "enjoy",
                    "the",
                    "benefits",
                    "of",
                    "an",
                    "efficient",
                    "E2E",
                    "network",
                    "design",
                    "without",
                    "relying",
                    "on",
                    "detectors",
                    "during",
                    "fine-tuning",
                    "and",
                    "inference."
                ]
            ],
            "context": [
                3,
                2,
                0,
                0
            ]
        },
        "input": "sent0: In this work, we aim to design an E2E pretraining strategy for the VLP problem.\n sent1: To this end, we adopt a modular representation network, which takes image grid features from a CNN-based visual network and the corresponding text embeddings into a multi-modal Transformer #TARGET_REF .\n sent2: Our goal is to learn the visual network and the Transformer jointly, and yet to effectively encode object-level visual concepts in the multimodal representations.\n sent3: This enables us to capture rich cross-modal alignment between linguistic entities and visual semantic concepts for the downstream tasks, and meanwhile to enjoy the benefits of an efficient E2E network design without relying on detectors during fine-tuning and inference.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "use",
                    "KnowBert-W+W,",
                    "which",
                    "has",
                    "been",
                    "trained",
                    "on",
                    "Wikipedia",
                    "and",
                    "WordNet",
                    "#TARGET_REF",
                    "as",
                    "an",
                    "embedding",
                    "model",
                    "that",
                    "incorporates",
                    "external",
                    "information,",
                    "note",
                    "that",
                    "we",
                    "are",
                    "not",
                    "using",
                    "this",
                    "as",
                    "an",
                    "entity",
                    "linking",
                    "system,",
                    "even",
                    "for",
                    "NED."
                ],
                [
                    "Similar",
                    "to",
                    "other",
                    "BERT",
                    "baselines,",
                    "we",
                    "feed",
                    "a",
                    "mention",
                    "span",
                    "m",
                    "and",
                    "context",
                    "s,",
                    "and",
                    "we",
                    "use",
                    "the",
                    "weighted",
                    "sum",
                    "of",
                    "the",
                    "[CLS]",
                    "vectors",
                    "from",
                    "all",
                    "15",
                    "layers."
                ]
            ],
            "context": [
                2,
                0
            ]
        },
        "input": "sent0: We use KnowBert-W+W, which has been trained on Wikipedia and WordNet #TARGET_REF as an embedding model that incorporates external information, note that we are not using this as an entity linking system, even for NED.\n sent1: Similar to other BERT baselines, we feed a mention span m and context s, and we use the weighted sum of the [CLS] vectors from all 15 layers.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Named",
                    "Entity",
                    "Recognition",
                    "(NER)",
                    "and",
                    "Part-of-Speech",
                    "(POS)",
                    "tagging",
                    "have",
                    "traditionally",
                    "been",
                    "used",
                    "as",
                    "preprocessing",
                    "steps",
                    "in",
                    "many",
                    "Natural",
                    "Language",
                    "Processing",
                    "(NLP)",
                    "applications."
                ],
                [
                    "For",
                    "example,",
                    "#REF",
                    "discussed",
                    "the",
                    "use",
                    "of",
                    "NER",
                    "across",
                    "question",
                    "answering,",
                    "information",
                    "retrieval,",
                    "co-reference",
                    "resolution,",
                    "topic",
                    "modeling,",
                    "and",
                    "machine",
                    "translation."
                ],
                [
                    "Similarly,",
                    "POS",
                    "tagging",
                    "is",
                    "often",
                    "applied",
                    "early",
                    "in",
                    "the",
                    "NLP",
                    "pipeline",
                    "for",
                    "many",
                    "applications",
                    "including",
                    "information",
                    "retrieval",
                    "systems,",
                    "syntax,",
                    "and",
                    "semantic",
                    "analysis,",
                    "speech",
                    "recognition",
                    "systems",
                    "and",
                    "machine",
                    "translation",
                    "#REF",
                    "."
                ],
                [
                    "In",
                    "recent",
                    "years,",
                    "Arabic",
                    "has",
                    "been",
                    "studied",
                    "increasingly",
                    "due",
                    "to",
                    "the",
                    "explosion",
                    "in",
                    "the",
                    "number",
                    "of",
                    "Arabic",
                    "users",
                    "on",
                    "social",
                    "media",
                    "and",
                    "the",
                    "internet",
                    "in",
                    "general."
                ],
                [
                    "Arabic",
                    "is",
                    "a",
                    "morphologically",
                    "rich",
                    "language",
                    "with",
                    "complex",
                    "grammatical",
                    "structure",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Arabic",
                    "NLP",
                    "researchers",
                    "have",
                    "used",
                    "two",
                    "types",
                    "of",
                    "approaches",
                    "and",
                    "sometimes",
                    "a",
                    "mixture",
                    "of",
                    "both",
                    "to",
                    "work",
                    "with",
                    "Arabic",
                    "text."
                ],
                [
                    "The",
                    "first",
                    "approach",
                    "is",
                    "the",
                    "simplification",
                    "approach",
                    "where",
                    "researchers",
                    "tend",
                    "to",
                    "apply",
                    "preprocessing",
                    "(transformation)",
                    "that",
                    "simplify",
                    "Arabic",
                    "text",
                    "such",
                    "as",
                    "letter",
                    "normalization",
                    "#REF",
                    "and",
                    "transliteration",
                    "#REF",
                    "."
                ],
                [
                    "The",
                    "second",
                    "approach",
                    "is",
                    "the",
                    "enrichment",
                    "approach",
                    "where",
                    "researchers",
                    "tend",
                    "to",
                    "apply",
                    "minimum",
                    "modification",
                    "to",
                    "the",
                    "Arabic",
                    "text",
                    "and",
                    "devise",
                    "a",
                    "way",
                    "of",
                    "incorporating",
                    "the",
                    "enriched",
                    "features",
                    "and",
                    "potentially",
                    "add",
                    "more",
                    "features",
                    "to",
                    "it."
                ]
            ],
            "context": [
                0,
                0,
                0,
                3,
                1,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Named Entity Recognition (NER) and Part-of-Speech (POS) tagging have traditionally been used as preprocessing steps in many Natural Language Processing (NLP) applications.\n sent1: For example, #REF discussed the use of NER across question answering, information retrieval, co-reference resolution, topic modeling, and machine translation.\n sent2: Similarly, POS tagging is often applied early in the NLP pipeline for many applications including information retrieval systems, syntax, and semantic analysis, speech recognition systems and machine translation #REF .\n sent3: In recent years, Arabic has been studied increasingly due to the explosion in the number of Arabic users on social media and the internet in general.\n sent4: Arabic is a morphologically rich language with complex grammatical structure #TARGET_REF .\n sent5: Arabic NLP researchers have used two types of approaches and sometimes a mixture of both to work with Arabic text.\n sent6: The first approach is the simplification approach where researchers tend to apply preprocessing (transformation) that simplify Arabic text such as letter normalization #REF and transliteration #REF .\n sent7: The second approach is the enrichment approach where researchers tend to apply minimum modification to the Arabic text and devise a way of incorporating the enriched features and potentially add more features to it.\n",
        "output": "{\"INFORMATION\": [\"sent4\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Systems",
                    "that",
                    "perform",
                    "shallow",
                    "semantic",
                    "parsing",
                    "on",
                    "Chinese",
                    "texts",
                    "are",
                    "likewise",
                    "based",
                    "on",
                    "classifiers",
                    "and",
                    "trained",
                    "on",
                    "the",
                    "Chinese",
                    "PropBank",
                    "and",
                    "the",
                    "bilingual",
                    "Chinese-English",
                    "Parallel",
                    "PropBank",
                    "#TARGET_REF",
                    ",",
                    "#REF",
                    ",",
                    "#REF",
                    ")."
                ],
                [
                    "It",
                    "is",
                    "interesting",
                    "to",
                    "note",
                    "that,",
                    "despite",
                    "the",
                    "very",
                    "different",
                    "characteristics",
                    "of",
                    "Chinese",
                    "verbs",
                    "#REF",
                    "from",
                    "those",
                    "in",
                    "English,",
                    "the",
                    "core",
                    "algorithm",
                    "of",
                    "a",
                    "shallow",
                    "semantic",
                    "parser",
                    "remains",
                    "the",
                    "same."
                ],
                [
                    "As",
                    "was",
                    "found",
                    "to",
                    "be",
                    "the",
                    "case",
                    "in",
                    "English,",
                    "SVM",
                    "classifiers",
                    "have",
                    "been",
                    "found",
                    "to",
                    "outperform",
                    "maximum",
                    "entropy",
                    "classifiers",
                    "for",
                    "this",
                    "task",
                    "#REF",
                    "."
                ],
                [
                    "The",
                    "primary",
                    "difference",
                    "lies",
                    "in",
                    "the",
                    "feature",
                    "set",
                    "chosen",
                    "to",
                    "represent",
                    "semantic",
                    "information."
                ]
            ],
            "context": [
                1,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Systems that perform shallow semantic parsing on Chinese texts are likewise based on classifiers and trained on the Chinese PropBank and the bilingual Chinese-English Parallel PropBank #TARGET_REF , #REF , #REF ).\n sent1: It is interesting to note that, despite the very different characteristics of Chinese verbs #REF from those in English, the core algorithm of a shallow semantic parser remains the same.\n sent2: As was found to be the case in English, SVM classifiers have been found to outperform maximum entropy classifiers for this task #REF .\n sent3: The primary difference lies in the feature set chosen to represent semantic information.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Regarding",
                    "the",
                    "conversion",
                    "process",
                    "of",
                    "text",
                    "and",
                    "audio",
                    "data,",
                    "we",
                    "leverage",
                    "recent",
                    "advances",
                    "to",
                    "transliterate",
                    "this",
                    "data",
                    "into",
                    "corresponding",
                    "sounds",
                    "represented",
                    "by",
                    "IPA",
                    "phonetic",
                    "symbols."
                ],
                [
                    "This",
                    "transliteration",
                    "is",
                    "possible",
                    "for",
                    "speech/audio",
                    "data",
                    "using",
                    "tools",
                    "such",
                    "as",
                    "the",
                    "Allosaurus",
                    "universal",
                    "phone",
                    "recognizer,",
                    "which",
                    "can",
                    "be",
                    "applied",
                    "without",
                    "additional",
                    "training",
                    "to",
                    "any",
                    "language",
                    "#REF",
                    ",",
                    "though",
                    "it",
                    "can",
                    "benefit",
                    "from",
                    "fine-tuning",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "To",
                    "convert",
                    "text",
                    "data",
                    "to",
                    "phonemes",
                    "we",
                    "can",
                    "use",
                    "tools",
                    "such",
                    "as",
                    "the",
                    "Epitran",
                    "grapheme-to-phoneme",
                    "converter",
                    "#REF",
                    ",",
                    "which",
                    "is",
                    "specifically",
                    "designed",
                    "to",
                    "provide",
                    "precise",
                    "phonetic",
                    "transliterations",
                    "in",
                    "low-resource",
                    "scenarios."
                ]
            ],
            "context": [
                3,
                2,
                0
            ]
        },
        "input": "sent0: Regarding the conversion process of text and audio data, we leverage recent advances to transliterate this data into corresponding sounds represented by IPA phonetic symbols.\n sent1: This transliteration is possible for speech/audio data using tools such as the Allosaurus universal phone recognizer, which can be applied without additional training to any language #REF , though it can benefit from fine-tuning #TARGET_REF .\n sent2: To convert text data to phonemes we can use tools such as the Epitran grapheme-to-phoneme converter #REF , which is specifically designed to provide precise phonetic transliterations in low-resource scenarios.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "2",
                    "Encoding",
                    "Anchored",
                    "DRSs",
                    "as",
                    "Sequences",
                    "#REF",
                    ",",
                    "#TARGET_REF",
                    ",",
                    "#REF",
                    "encode",
                    "syntax",
                    "trees",
                    "as",
                    "token",
                    "labels",
                    "to",
                    "cast",
                    "syntactic",
                    "parsing",
                    "as",
                    "a",
                    "sequence",
                    "labeling",
                    "task."
                ],
                [
                    "We",
                    "apply",
                    "a",
                    "similar",
                    "method",
                    "to",
                    "DRS",
                    "parsing."
                ],
                [
                    "We",
                    "will",
                    "use",
                    "a",
                    "simplified",
                    "example",
                    "from",
                    "the",
                    "Parallel",
                    "Meaning",
                    "Bank",
                    "(PMB,",
                    "#REF",
                    "for",
                    "exposition."
                ]
            ],
            "context": [
                3,
                0,
                0
            ]
        },
        "input": "sent0: 2 Encoding Anchored DRSs as Sequences #REF , #TARGET_REF , #REF encode syntax trees as token labels to cast syntactic parsing as a sequence labeling task.\n sent1: We apply a similar method to DRS parsing.\n sent2: We will use a simplified example from the Parallel Meaning Bank (PMB, #REF for exposition.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Opposed",
                    "to",
                    "#TARGET_REF",
                    "estimates",
                    "the",
                    "post-editing",
                    "costs",
                    "of",
                    "a",
                    "commercial",
                    "business",
                    "for",
                    "instant",
                    "communication",
                    "messaging",
                    "translation."
                ],
                [
                    "Interestingly,",
                    "his",
                    "cost",
                    "estimations",
                    "cover",
                    "corpus",
                    "management",
                    "and",
                    "acquisition,",
                    "5",
                    "to",
                    "200",
                    "GBP",
                    "per",
                    "year,",
                    "customization",
                    "costs,",
                    "human",
                    "resources",
                    "training,",
                    "5",
                    "to",
                    "10",
                    "GBP",
                    "per",
                    "year,",
                    "among",
                    "others."
                ],
                [
                    "He",
                    "measures",
                    "investment",
                    "cost",
                    "of",
                    "product",
                    "licenses",
                    "to",
                    "100",
                    "GBP",
                    "per",
                    "year."
                ],
                [
                    "Overall,",
                    "the",
                    "total",
                    "investment",
                    "costs",
                    "are",
                    "estimated",
                    "to",
                    "180",
                    "GBP",
                    "per",
                    "year."
                ],
                [
                    "The",
                    "predicted",
                    "ROI",
                    "for",
                    "the",
                    "first",
                    "year",
                    "after",
                    "successful",
                    "MT",
                    "implementation",
                    "equals",
                    "to",
                    "minus",
                    "30000",
                    "GBP,",
                    "therefore",
                    "is",
                    "negative",
                    "speaking",
                    "in",
                    "financial",
                    "terms."
                ],
                [
                    "The",
                    "vendor",
                    "pricing",
                    "models",
                    "is",
                    "estimated",
                    "to",
                    "65-85%",
                    "of",
                    "the",
                    "original",
                    "pay",
                    "per",
                    "word."
                ],
                [
                    "What",
                    "is",
                    "intriguing",
                    "is",
                    "the",
                    "fact",
                    "benefits",
                    "are",
                    "not",
                    "only",
                    "measured",
                    "in",
                    "money."
                ],
                [
                    "#REF",
                    ",",
                    "as",
                    "#REF",
                    ",",
                    "measures",
                    "the",
                    "user",
                    "satisfaction",
                    "rate,",
                    "which",
                    "in",
                    "the",
                    "case",
                    "of",
                    "Lionbridge",
                    "2",
                    "has",
                    "increased",
                    "with",
                    "30-50%."
                ],
                [
                    "The",
                    "success",
                    "rate",
                    "measured",
                    "varies",
                    "between",
                    "5%",
                    "and",
                    "25%."
                ]
            ],
            "context": [
                1,
                2,
                3,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Opposed to #TARGET_REF estimates the post-editing costs of a commercial business for instant communication messaging translation.\n sent1: Interestingly, his cost estimations cover corpus management and acquisition, 5 to 200 GBP per year, customization costs, human resources training, 5 to 10 GBP per year, among others.\n sent2: He measures investment cost of product licenses to 100 GBP per year.\n sent3: Overall, the total investment costs are estimated to 180 GBP per year.\n sent4: The predicted ROI for the first year after successful MT implementation equals to minus 30000 GBP, therefore is negative speaking in financial terms.\n sent5: The vendor pricing models is estimated to 65-85% of the original pay per word.\n sent6: What is intriguing is the fact benefits are not only measured in money.\n sent7: #REF , as #REF , measures the user satisfaction rate, which in the case of Lionbridge 2 has increased with 30-50%.\n sent8: The success rate measured varies between 5% and 25%.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Data",
                    "augmentation",
                    "Data",
                    "augmentation",
                    "is",
                    "the",
                    "technique",
                    "used",
                    "to",
                    "increase",
                    "the",
                    "amount",
                    "of",
                    "data",
                    "by",
                    "adding",
                    "slightly",
                    "modified",
                    "copies",
                    "of",
                    "already",
                    "existing",
                    "data",
                    "or",
                    "newly",
                    "created",
                    "synthetic",
                    "data",
                    "from",
                    "existing",
                    "data."
                ],
                [
                    "It",
                    "acts",
                    "as",
                    "a",
                    "regularizer",
                    "and",
                    "helps",
                    "reduce",
                    "overfitting",
                    "when",
                    "training",
                    "a",
                    "machine",
                    "learning",
                    "model."
                ],
                [
                    "In",
                    "this",
                    "paper,",
                    "data",
                    "augmentation",
                    "consists",
                    "of",
                    "two",
                    "parts."
                ],
                [
                    "We",
                    "first",
                    "add",
                    "the",
                    "dataset",
                    "released",
                    "by",
                    "CWI",
                    "2018",
                    "into",
                    "the",
                    "training",
                    "set."
                ],
                [
                    "Besides,",
                    "for",
                    "subtask",
                    "2,",
                    "since",
                    "its",
                    "training",
                    "dataset",
                    "is",
                    "small",
                    "which",
                    "only",
                    "contains",
                    "one",
                    "thousand",
                    "samples,",
                    "we",
                    "add",
                    "the",
                    "dataset",
                    "of",
                    "subtask",
                    "1",
                    "to",
                    "train",
                    "the",
                    "model",
                    "for",
                    "subtask",
                    "2."
                ],
                [
                    "Then,",
                    "for",
                    "a",
                    "given",
                    "sentence",
                    "in",
                    "the",
                    "training",
                    "set,",
                    "we",
                    "perform",
                    "the",
                    "operations",
                    "containing",
                    "synonym",
                    "replacement,",
                    "random",
                    "insertion,",
                    "random",
                    "swap,",
                    "and",
                    "random",
                    "deletion",
                    "introduced",
                    "by",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                2
            ]
        },
        "input": "sent0: Data augmentation Data augmentation is the technique used to increase the amount of data by adding slightly modified copies of already existing data or newly created synthetic data from existing data.\n sent1: It acts as a regularizer and helps reduce overfitting when training a machine learning model.\n sent2: In this paper, data augmentation consists of two parts.\n sent3: We first add the dataset released by CWI 2018 into the training set.\n sent4: Besides, for subtask 2, since its training dataset is small which only contains one thousand samples, we add the dataset of subtask 1 to train the model for subtask 2.\n sent5: Then, for a given sentence in the training set, we perform the operations containing synonym replacement, random insertion, random swap, and random deletion introduced by #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent5\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "size",
                    "of",
                    "aligned",
                    "corpora",
                    "that",
                    "we",
                    "were",
                    "able",
                    "to",
                    "use",
                    "was",
                    "limited",
                    "by",
                    "available",
                    "memory."
                ],
                [
                    "The",
                    "memory",
                    "footprint",
                    "chiefly",
                    "consists",
                    "of",
                    "the",
                    "text",
                    "and",
                    "8",
                    "bytes/word",
                    "to",
                    "hold",
                    "a",
                    "word",
                    "position",
                    "array,",
                    "and",
                    "the",
                    "suffix",
                    "array."
                ],
                [
                    "It",
                    "is",
                    "possible",
                    "that",
                    "algorithms",
                    "for",
                    "external",
                    "suffix",
                    "array",
                    "construction",
                    "could",
                    "be",
                    "employed,",
                    "such",
                    "as",
                    "the",
                    "DC3",
                    "algorithm",
                    "by",
                    "#TARGET_REF",
                    "so",
                    "that",
                    "even",
                    "larger",
                    "corpora",
                    "could",
                    "be",
                    "used."
                ]
            ],
            "context": [
                0,
                0,
                3
            ]
        },
        "input": "sent0: The size of aligned corpora that we were able to use was limited by available memory.\n sent1: The memory footprint chiefly consists of the text and 8 bytes/word to hold a word position array, and the suffix array.\n sent2: It is possible that algorithms for external suffix array construction could be employed, such as the DC3 algorithm by #TARGET_REF so that even larger corpora could be used.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "PLMs",
                    "such",
                    "as",
                    "BERT",
                    "(Bidirectional",
                    "Encoder",
                    "Representations",
                    "from",
                    "Transformers)",
                    "use",
                    "the",
                    "encoder",
                    "structure",
                    "of",
                    "the",
                    "Transformer",
                    "#TARGET_REF",
                    "for",
                    "deep",
                    "self-supervised",
                    "learning,",
                    "which",
                    "requires",
                    "task-specific",
                    "fine-tuning."
                ],
                [
                    "In",
                    "this",
                    "paper,",
                    "the",
                    "downstream",
                    "task",
                    "to",
                    "predict",
                    "the",
                    "complexity",
                    "scores,",
                    "a",
                    "real-value",
                    "in",
                    "the",
                    "range",
                    "of",
                    "[0,1],",
                    "of",
                    "given",
                    "words."
                ]
            ],
            "context": [
                2,
                0
            ]
        },
        "input": "sent0: PLMs such as BERT (Bidirectional Encoder Representations from Transformers) use the encoder structure of the Transformer #TARGET_REF for deep self-supervised learning, which requires task-specific fine-tuning.\n sent1: In this paper, the downstream task to predict the complexity scores, a real-value in the range of [0,1], of given words.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Pre-trained",
                    "language",
                    "models",
                    "are",
                    "increasingly",
                    "applied",
                    "in",
                    "ways",
                    "that",
                    "are",
                    "agnostic",
                    "to",
                    "targeted",
                    "downstream",
                    "tasks",
                    "#REF",
                    "."
                ],
                [
                    "This",
                    "usage",
                    "has",
                    "led",
                    "to",
                    "a",
                    "proliferation",
                    "of",
                    "large",
                    "language",
                    "models",
                    "trained",
                    "on",
                    "enormous",
                    "amounts",
                    "of",
                    "data."
                ],
                [
                    "For",
                    "example,",
                    "the",
                    "recent",
                    "Megatron-Turing",
                    "NLG",
                    "530B",
                    "model",
                    "was",
                    "trained",
                    "on",
                    "the",
                    "Pile,",
                    "which",
                    "includes",
                    "800GB+",
                    "of",
                    "text",
                    "#REF",
                    ",",
                    "and",
                    "other",
                    "large",
                    "language",
                    "models",
                    "utilize",
                    "large",
                    "portions",
                    "of",
                    "the",
                    "200TB+",
                    "common",
                    "crawl",
                    "data."
                ],
                [
                    "1",
                    "These",
                    "large",
                    "data",
                    "sets",
                    "include",
                    "impressive",
                    "amounts",
                    "of",
                    "text,",
                    "but",
                    "all",
                    "languages",
                    "are",
                    "not",
                    "represented",
                    "equally",
                    "(or",
                    "at",
                    "all)",
                    "in",
                    "that",
                    "text."
                ],
                [
                    "The",
                    "reality",
                    "is",
                    "that",
                    "only",
                    "a",
                    "negligible",
                    "fraction",
                    "of",
                    "the",
                    "7000+",
                    "currently",
                    "spoken",
                    "languages",
                    "#TARGET_REF",
                    "have",
                    "sufficient",
                    "text",
                    "corpora",
                    "to",
                    "train",
                    "state-of-theart",
                    "language",
                    "models."
                ],
                [
                    "This",
                    "data",
                    "scarcity",
                    "results",
                    "in",
                    "systematic",
                    "inequalities",
                    "in",
                    "the",
                    "performance",
                    "of",
                    "NLP",
                    "tasks",
                    "across",
                    "the",
                    "world's",
                    "languages",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                0,
                0,
                0,
                3,
                2,
                2
            ]
        },
        "input": "sent0: Pre-trained language models are increasingly applied in ways that are agnostic to targeted downstream tasks #REF .\n sent1: This usage has led to a proliferation of large language models trained on enormous amounts of data.\n sent2: For example, the recent Megatron-Turing NLG 530B model was trained on the Pile, which includes 800GB+ of text #REF , and other large language models utilize large portions of the 200TB+ common crawl data.\n sent3: 1 These large data sets include impressive amounts of text, but all languages are not represented equally (or at all) in that text.\n sent4: The reality is that only a negligible fraction of the 7000+ currently spoken languages #TARGET_REF have sufficient text corpora to train state-of-theart language models.\n sent5: This data scarcity results in systematic inequalities in the performance of NLP tasks across the world's languages #REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent4\", \"sent5\"], \"BACKGROUND\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Performance",
                    "is",
                    "plotted",
                    "on",
                    "the",
                    "vertical",
                    "axis",
                    "and",
                    "measured",
                    "for",
                    "different",
                    "frequency",
                    "bins."
                ],
                [
                    "The",
                    "Zipfian",
                    "distribution",
                    "of",
                    "terms",
                    "suggests",
                    "assessing",
                    "performance",
                    "by",
                    "frequency",
                    "ranges",
                    "growing",
                    "by",
                    "a",
                    "constant",
                    "factor",
                    "#TARGET_REF",
                    ",",
                    "accordingly",
                    "we",
                    "used",
                    "a",
                    "logarithmic",
                    "scale",
                    "with",
                    "base=3."
                ],
                [
                    "The",
                    "number",
                    "of",
                    "terms",
                    "per",
                    "bin",
                    "is",
                    "given",
                    "in",
                    "parentheses."
                ],
                [
                    "Some",
                    "of",
                    "the",
                    "previous",
                    "work",
                    "cited",
                    "in",
                    "Section",
                    "2",
                    "described",
                    "performance",
                    "for",
                    "selected",
                    "subsets",
                    "of",
                    "terms,",
                    "typically",
                    "high",
                    "frequency",
                    "terms",
                    "that",
                    "are",
                    "easier",
                    "to",
                    "translate."
                ],
                [
                    "We",
                    "believe",
                    "presenting",
                    "translation",
                    "accuracy",
                    "as",
                    "a",
                    "function",
                    "of",
                    "source",
                    "term",
                    "frequency",
                    "is",
                    "more",
                    "informative."
                ]
            ],
            "context": [
                0,
                3,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Performance is plotted on the vertical axis and measured for different frequency bins.\n sent1: The Zipfian distribution of terms suggests assessing performance by frequency ranges growing by a constant factor #TARGET_REF , accordingly we used a logarithmic scale with base=3.\n sent2: The number of terms per bin is given in parentheses.\n sent3: Some of the previous work cited in Section 2 described performance for selected subsets of terms, typically high frequency terms that are easier to translate.\n sent4: We believe presenting translation accuracy as a function of source term frequency is more informative.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "scheme",
                    "(GEIG,",
                    "see",
                    "e.g."
                ],
                [
                    "#TARGET_REF",
                    "."
                ],
                [
                    "This",
                    "compares",
                    "unlabelled",
                    "brack",
                    "etings",
                    "derived",
                    "from",
                    "corpus",
                    "treebanks",
                    "with",
                    "those",
                    "derived",
                    "from",
                    "parses",
                    "for",
                    "the",
                    "same",
                    "sentences",
                    "by",
                    "computing",
                    "recall,",
                    "the",
                    "ratio",
                    "of",
                    "matched",
                    "brackets",
                    "over",
                    "all",
                    "brackets",
                    "in",
                    "the",
                    "treebank,",
                    "precision,",
                    "the",
                    "ratio",
                    "of",
                    "matched",
                    "brackets",
                    "over",
                    "all",
                    "brackets",
                    "found",
                    "by",
                    "the",
                    "parser,",
                    "'crossing",
                    "'",
                    "brackets,",
                    "the",
                    "number",
                    "of",
                    "times",
                    "a",
                    "bracketed",
                    "sequence",
                    "output",
                    "by",
                    "the",
                    "parser",
                    "overlaps",
                    "with",
                    "one",
                    "from",
                    "the",
                    "treebank",
                    "but",
                    "neither",
                    "is",
                    "properly",
                    "contained",
                    "in",
                    "the",
                    "other,",
                    "and",
                    "minC,",
                    "the",
                    "number",
                    "of",
                    "sentences",
                    "for",
                    "which",
                    "all",
                    "of",
                    "the",
                    "analyses",
                    "had",
                    "one",
                    "or",
                    "more",
                    "crossings."
                ],
                [
                    "The",
                    "table",
                    "also",
                    "gives",
                    "an",
                    "indication",
                    "of",
                    "the",
                    "best",
                    "and",
                    "worst",
                    "possible",
                    "performance",
                    "of",
                    "the",
                    "disambiguation",
                    "component",
                    "of",
                    "the",
                    "system,",
                    "showing",
                    "the",
                    "results",
                    "obtained",
                    "when",
                    "parse",
                    "selection",
                    "is",
                    "replaced",
                    "by",
                    "a",
                    "simple",
                    "random",
                    "choice,",
                    "and",
                    "the",
                    "results",
                    "of",
                    "eval",
                    "uating",
                    "the",
                    "manually-created",
                    "treebank",
                    "against",
                    "the",
                    "corresponding",
                    "Susanne",
                    "bracketings."
                ],
                [
                    "In",
                    "this",
                    "latter",
                    "figure,",
                    "the",
                    "mean",
                    "number",
                    "of",
                    "crossings",
                    "is",
                    "greater",
                    "than",
                    "zero",
                    "mainly",
                    "because",
                    "of",
                    "compound",
                    "noun",
                    "bracketing",
                    "ambiguity",
                    "which",
                    "our",
                    "grammar",
                    "does",
                    "not",
                    "attempt",
                    "to",
                    "resolve,",
                    "always",
                    "returning",
                    "minC",
                    "Crossings",
                    "Recall",
                    "(%)",
                    "Precision",
                    "(",
                    "%",
                    ")",
                    "Probabilistic",
                    "parser",
                    "analyses",
                    "#REF",
                    "#REF",
                    "uses",
                    "the",
                    "crossing",
                    "brackets",
                    "measure",
                    "to",
                    "define",
                    "a",
                    "notion",
                    "of",
                    "structural",
                    "consistency,",
                    "where",
                    "the",
                    "structural",
                    "consistency",
                    "rate",
                    "for",
                    "the",
                    "grammar",
                    "is",
                    "defined",
                    "as",
                    "the",
                    "proportion",
                    "of",
                    "sentences",
                    "for",
                    "which",
                    "at",
                    "least",
                    "one",
                    "analysis",
                    "contains",
                    "no",
                    "crossing",
                    "brackets,",
                    "and",
                    "reports",
                    "a",
                    "rate",
                    "of",
                    "around",
                    "95%",
                    "for",
                    "the",
                    "IBM",
                    "grammar",
                    "tested",
                    "on",
                    "the",
                    "computer",
                    "manual",
                    "corpus."
                ],
                [
                    "The",
                    "problem",
                    "with",
                    "the",
                    "GEIG",
                    "scheme",
                    "and",
                    "with",
                    "structural",
                    "consistency",
                    "is",
                    "that",
                    "both",
                    "are",
                    "still",
                    "weak",
                    "measures",
                    "(",
                    "designed",
                    "to",
                    "avoid",
                    "problems",
                    "of",
                    "parser/treebank",
                    "representational",
                    "compatibility)",
                    "which",
                    "lead",
                    "to",
                    "unintuitive",
                    "numbers",
                    "whose",
                    "significance",
                    "still",
                    "depends",
                    "heavily",
                    "on",
                    "details",
                    "of",
                    "the",
                    "relationship",
                    "between",
                    "the",
                    "representations",
                    "compared",
                    "(c.f."
                ],
                [
                    "the",
                    "compound",
                    "noun",
                    "issue",
                    "mentioned",
                    "above)",
                    "."
                ]
            ],
            "context": [
                1,
                1,
                3,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: scheme (GEIG, see e.g.\n sent1: #TARGET_REF .\n sent2: This compares unlabelled brack etings derived from corpus treebanks with those derived from parses for the same sentences by computing recall, the ratio of matched brackets over all brackets in the treebank, precision, the ratio of matched brackets over all brackets found by the parser, 'crossing ' brackets, the number of times a bracketed sequence output by the parser overlaps with one from the treebank but neither is properly contained in the other, and minC, the number of sentences for which all of the analyses had one or more crossings.\n sent3: The table also gives an indication of the best and worst possible performance of the disambiguation component of the system, showing the results obtained when parse selection is replaced by a simple random choice, and the results of eval uating the manually-created treebank against the corresponding Susanne bracketings.\n sent4: In this latter figure, the mean number of crossings is greater than zero mainly because of compound noun bracketing ambiguity which our grammar does not attempt to resolve, always returning minC Crossings Recall (%) Precision ( % ) Probabilistic parser analyses #REF #REF uses the crossing brackets measure to define a notion of structural consistency, where the structural consistency rate for the grammar is defined as the proportion of sentences for which at least one analysis contains no crossing brackets, and reports a rate of around 95% for the IBM grammar tested on the computer manual corpus.\n sent5: The problem with the GEIG scheme and with structural consistency is that both are still weak measures ( designed to avoid problems of parser/treebank representational compatibility) which lead to unintuitive numbers whose significance still depends heavily on details of the relationship between the representations compared (c.f.\n sent6: the compound noun issue mentioned above) .\n",
        "output": "{\"INFORMATION\": [\"sent0\", \"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "vi",
                    "Entity",
                    "Pool",
                    "Strategy",
                    "-To",
                    "effectively",
                    "deal",
                    "with",
                    "rare",
                    "words,",
                    "transformer",
                    "models",
                    "use",
                    "sub-word",
                    "units",
                    "or",
                    "WordPiece",
                    "tokens",
                    "as",
                    "the",
                    "input",
                    "to",
                    "build",
                    "the",
                    "models",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Therefore,",
                    "there",
                    "is",
                    "a",
                    "possibility",
                    "that",
                    "one",
                    "target",
                    "word",
                    "can",
                    "be",
                    "separated",
                    "into",
                    "several",
                    "sub-words."
                ],
                [
                    "In",
                    "this",
                    "strategy,",
                    "we",
                    "generate",
                    "separate",
                    "fixed-length",
                    "embeddings",
                    "for",
                    "each",
                    "target",
                    "word",
                    "by",
                    "passing",
                    "its",
                    "sub-word",
                    "outputs",
                    "through",
                    "a",
                    "pooling",
                    "layer."
                ],
                [
                    "The",
                    "pooled",
                    "outputs",
                    "are",
                    "concatenated",
                    "and",
                    "fed",
                    "into",
                    "a",
                    "softmax",
                    "layer",
                    "to",
                    "predict",
                    "the",
                    "labels",
                    "(Figure",
                    "2e)."
                ]
            ],
            "context": [
                1,
                2,
                2,
                0
            ]
        },
        "input": "sent0: vi Entity Pool Strategy -To effectively deal with rare words, transformer models use sub-word units or WordPiece tokens as the input to build the models #TARGET_REF .\n sent1: Therefore, there is a possibility that one target word can be separated into several sub-words.\n sent2: In this strategy, we generate separate fixed-length embeddings for each target word by passing its sub-word outputs through a pooling layer.\n sent3: The pooled outputs are concatenated and fed into a softmax layer to predict the labels (Figure 2e).\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [\"sent1\", \"sent2\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "LIGHT",
                    "Questing",
                    "Environment."
                ],
                [
                    "The",
                    "LIGHT",
                    "game",
                    "environment",
                    "#TARGET_REF",
                    "1",
                    "is",
                    "a",
                    "multi-user",
                    "fantasy",
                    "text-adventure",
                    "game",
                    "consisting",
                    "of",
                    "a",
                    "rich,",
                    "diverse",
                    "set",
                    "of",
                    "1775",
                    "characters,",
                    "663",
                    "locations,",
                    "and",
                    "3462",
                    "objects."
                ],
                [
                    "Characters",
                    "are",
                    "able",
                    "to",
                    "perform",
                    "templated",
                    "actions",
                    "to",
                    "interact",
                    "with",
                    "both",
                    "objects",
                    "and",
                    "characters,",
                    "and",
                    "can",
                    "speak",
                    "to",
                    "other",
                    "characters",
                    "through",
                    "free",
                    "form",
                    "text",
                    "dialogues."
                ],
                [
                    "Actions",
                    "in",
                    "text",
                    "games",
                    "generally",
                    "consist",
                    "of",
                    "verb",
                    "phrases",
                    "(VP)",
                    "followed",
                    "optionally",
                    "by",
                    "prepositional",
                    "phrases",
                    "(VP",
                    "PP)."
                ],
                [
                    "For",
                    "example,",
                    "get",
                    "OBJ,",
                    "put",
                    "OBJ,",
                    "give",
                    "OBJ",
                    "to",
                    "CHAR,",
                    "etc.."
                ],
                [
                    "These",
                    "actions",
                    "change",
                    "the",
                    "state",
                    "of",
                    "the",
                    "world",
                    "which",
                    "is",
                    "expressed",
                    "through",
                    "text",
                    "descriptions."
                ]
            ],
            "context": [
                3,
                1,
                3,
                3,
                0,
                0
            ]
        },
        "input": "sent0: The LIGHT Questing Environment.\n sent1: The LIGHT game environment #TARGET_REF 1 is a multi-user fantasy text-adventure game consisting of a rich, diverse set of 1775 characters, 663 locations, and 3462 objects.\n sent2: Characters are able to perform templated actions to interact with both objects and characters, and can speak to other characters through free form text dialogues.\n sent3: Actions in text games generally consist of verb phrases (VP) followed optionally by prepositional phrases (VP PP).\n sent4: For example, get OBJ, put OBJ, give OBJ to CHAR, etc..\n sent5: These actions change the state of the world which is expressed through text descriptions.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\", \"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "future",
                    "work",
                    "we",
                    "hope",
                    "to",
                    "study",
                    "the",
                    "effect",
                    "of",
                    "corpus",
                    "size",
                    "and",
                    "corpus",
                    "diversity",
                    "on",
                    "translation",
                    "effectiveness."
                ],
                [
                    "We",
                    "also",
                    "hope",
                    "to",
                    "focus",
                    "on",
                    "evaluation",
                    "of",
                    "longer",
                    "MWEs",
                    "(i.e.,",
                    "trigrams",
                    "and",
                    "longer)",
                    "as",
                    "well",
                    "as",
                    "consider",
                    "the",
                    "possibility",
                    "that",
                    "efficient",
                    "suffix-based",
                    "wildcard",
                    "searches",
                    "#TARGET_REF",
                    "may",
                    "enable",
                    "correct",
                    "translation",
                    "of",
                    "non-contiguous",
                    "phrases."
                ]
            ],
            "context": [
                2,
                2
            ]
        },
        "input": "sent0: In future work we hope to study the effect of corpus size and corpus diversity on translation effectiveness.\n sent1: We also hope to focus on evaluation of longer MWEs (i.e., trigrams and longer) as well as consider the possibility that efficient suffix-based wildcard searches #TARGET_REF may enable correct translation of non-contiguous phrases.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\", \"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "What,",
                    "then,",
                    "is",
                    "known",
                    "about",
                    "BERT,",
                    "and",
                    "its",
                    "syntactic",
                    "and",
                    "semantic",
                    "capabilities?"
                ],
                [
                    "Of",
                    "the",
                    "two,",
                    "it",
                    "is",
                    "syntax",
                    "that",
                    "BERT",
                    "is",
                    "most",
                    "widely",
                    "claimed",
                    "to",
                    "capture",
                    "within",
                    "its",
                    "internal",
                    "representations:",
                    "#REF",
                    "use",
                    "structural",
                    "probing",
                    "to",
                    "find",
                    "dependency",
                    "trees",
                    "in",
                    "BERT's",
                    "vector",
                    "geometry,",
                    "while",
                    "#REF",
                    "use",
                    "probing",
                    "to",
                    "find",
                    "part",
                    "of",
                    "speech",
                    "tags",
                    "and",
                    "dependency",
                    "arc",
                    "labels,",
                    "among",
                    "other",
                    "types",
                    "of",
                    "syntactic",
                    "information."
                ],
                [
                    "Analysis",
                    "of",
                    "BERT's",
                    "attention",
                    "has",
                    "shown",
                    "that",
                    "certain",
                    "heads",
                    "attend",
                    "to",
                    "not",
                    "only",
                    "relevant",
                    "linguistic",
                    "units",
                    "such",
                    "as",
                    "determiners",
                    "of",
                    "nouns",
                    "and",
                    "coreferent",
                    "mentions",
                    "#REF",
                    ",",
                    "but",
                    "also",
                    "dependency",
                    "relations",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                0,
                0,
                1
            ]
        },
        "input": "sent0: What, then, is known about BERT, and its syntactic and semantic capabilities?\n sent1: Of the two, it is syntax that BERT is most widely claimed to capture within its internal representations: #REF use structural probing to find dependency trees in BERT's vector geometry, while #REF use probing to find part of speech tags and dependency arc labels, among other types of syntactic information.\n sent2: Analysis of BERT's attention has shown that certain heads attend to not only relevant linguistic units such as determiners of nouns and coreferent mentions #REF , but also dependency relations #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Earlier,",
                    "it",
                    "was",
                    "stated",
                    "that",
                    "the",
                    "design",
                    "of",
                    "the",
                    "system",
                    "was",
                    "in",
                    "response",
                    "to",
                    "perceived",
                    "weaknesses",
                    "in",
                    "the",
                    "classical",
                    "approaches",
                    "to",
                    "MT."
                ],
                [
                    "It",
                    "should",
                    "now",
                    "be",
                    "clear",
                    "that",
                    "the",
                    "approach",
                    "will",
                    "avoid",
                    "structurepreserving",
                    "translation",
                    "as",
                    "a",
                    "first",
                    "choice,",
                    "since",
                    "the",
                    "translation",
                    "process",
                    "involves",
                    "no",
                    "analysis",
                    "of",
                    "syntactic",
                    "structure",
                    "as",
                    "such,",
                    "the",
                    "stratificational",
                    "approach",
                    "to",
                    "linguistic",
                    "description",
                    "is",
                    "likewise",
                    "absent,",
                    "and",
                    "the",
                    "predominantly",
                    "bottom-up",
                    "compositional",
                    "theory-driven",
                    "translation",
                    "algorithm",
                    "is",
                    "replaced",
                    "by",
                    "a",
                    "more",
                    "global",
                    "data-driven",
                    "process,",
                    "finally,",
                    "the",
                    "use",
                    "of",
                    "examples",
                    "rather",
                    "than",
                    "rules",
                    "and",
                    "lexicons",
                    "derived",
                    "from",
                    "linguists'",
                    "introspection",
                    "mean",
                    "that",
                    "the",
                    "system",
                    "will",
                    "produce",
                    "more",
                    "natural",
                    "output",
                    "(especially",
                    "where",
                    "source",
                    "and",
                    "target",
                    "are",
                    "structurally",
                    "dissimilar:",
                    "so-called",
                    "metaphors",
                    "and",
                    "idioms),",
                    "will",
                    "be",
                    "more",
                    "robust,",
                    "easier",
                    "to",
                    "extend",
                    "and",
                    "to",
                    "debug",
                    "(",
                    "#TARGET_REF",
                    ")."
                ]
            ],
            "context": [
                3,
                2
            ]
        },
        "input": "sent0: Earlier, it was stated that the design of the system was in response to perceived weaknesses in the classical approaches to MT.\n sent1: It should now be clear that the approach will avoid structurepreserving translation as a first choice, since the translation process involves no analysis of syntactic structure as such, the stratificational approach to linguistic description is likewise absent, and the predominantly bottom-up compositional theory-driven translation algorithm is replaced by a more global data-driven process, finally, the use of examples rather than rules and lexicons derived from linguists' introspection mean that the system will produce more natural output (especially where source and target are structurally dissimilar: so-called metaphors and idioms), will be more robust, easier to extend and to debug ( #TARGET_REF ).\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "where",
                    "S",
                    "i",
                    "and",
                    "S",
                    "j",
                    "are",
                    "two",
                    "different",
                    "candidate",
                    "summaries",
                    "and",
                    "ROUGE(S",
                    "i",
                    ",",
                    "S",
                    "*",
                    ")",
                    "&gt,",
                    "ROUGE(S",
                    "j",
                    ",",
                    "S",
                    "*",
                    "),",
                    "∀i,",
                    "j,",
                    "i",
                    "&lt,",
                    "j.",
                    "λ",
                    "ij",
                    "is",
                    "the",
                    "margin",
                    "multiplied",
                    "by",
                    "the",
                    "difference",
                    "in",
                    "rank",
                    "between",
                    "the",
                    "candidates,",
                    "i.e.,λ",
                    "ij",
                    "=",
                    "(j",
                    "−",
                    "i)",
                    "*",
                    "λ.",
                    "f",
                    "(S",
                    "i",
                    ")",
                    "is",
                    "the",
                    "length-normalized",
                    "estimated",
                    "log-probability",
                    "3",
                    "f",
                    "(S)",
                    "=",
                    "l",
                    "t=1",
                    "log",
                    "p",
                    "g",
                    "θ",
                    "(s",
                    "t",
                    "|D,",
                    "S",
                    "&lt,t",
                    ",",
                    "θ)",
                    "|S|",
                    "α",
                    "(9)where",
                    "α",
                    "is",
                    "the",
                    "length",
                    "penalty",
                    "hyperparameter."
                ],
                [
                    "This",
                    "loss",
                    "gives",
                    "the",
                    "abstractive",
                    "model",
                    "a",
                    "dual",
                    "purpose,",
                    "first",
                    "as",
                    "a",
                    "reference-free",
                    "evaluation",
                    "model,",
                    "which",
                    "can",
                    "be",
                    "used",
                    "in",
                    "a",
                    "two-stage",
                    "summarization",
                    "pipeline,",
                    "where",
                    "it",
                    "is",
                    "used",
                    "to",
                    "score",
                    "the",
                    "candidates",
                    "generated",
                    "by",
                    "a",
                    "pre-trained",
                    "generation",
                    "model",
                    "and",
                    "select",
                    "the",
                    "final",
                    "output",
                    "from",
                    "them."
                ],
                [
                    "However,",
                    "since",
                    "the",
                    "autoregressive",
                    "generation",
                    "depends",
                    "on",
                    "both",
                    "the",
                    "token-level",
                    "prediction",
                    "accuracy",
                    "and",
                    "sequencelevel",
                    "coordination,",
                    "the",
                    "model",
                    "fine-tuned",
                    "with",
                    "the",
                    "contrastive",
                    "loss",
                    "alone",
                    "can",
                    "no",
                    "longer",
                    "be",
                    "used",
                    "as",
                    "a",
                    "generation",
                    "model."
                ],
                [
                    "Multi-task",
                    "Fine-tuning",
                    "Following",
                    "#TARGET_REF",
                    ",",
                    "we",
                    "combine",
                    "the",
                    "contrastive",
                    "(Eq."
                ],
                [
                    "8)",
                    "and",
                    "cross-entropy",
                    "(Eq."
                ],
                [
                    "3)",
                    "losses",
                    "to",
                    "preserve",
                    "the",
                    "generation",
                    "ability",
                    "of",
                    "the",
                    "pre-trained",
                    "abstractive",
                    "model:"
                ]
            ],
            "context": [
                0,
                0,
                0,
                1,
                3,
                3
            ]
        },
        "input": "sent0: where S i and S j are two different candidate summaries and ROUGE(S i , S * ) &gt, ROUGE(S j , S * ), ∀i, j, i &lt, j. λ ij is the margin multiplied by the difference in rank between the candidates, i.e.,λ ij = (j − i) * λ. f (S i ) is the length-normalized estimated log-probability 3 f (S) = l t=1 log p g θ (s t |D, S &lt,t , θ) |S| α (9)where α is the length penalty hyperparameter.\n sent1: This loss gives the abstractive model a dual purpose, first as a reference-free evaluation model, which can be used in a two-stage summarization pipeline, where it is used to score the candidates generated by a pre-trained generation model and select the final output from them.\n sent2: However, since the autoregressive generation depends on both the token-level prediction accuracy and sequencelevel coordination, the model fine-tuned with the contrastive loss alone can no longer be used as a generation model.\n sent3: Multi-task Fine-tuning Following #TARGET_REF , we combine the contrastive (Eq.\n sent4: 8) and cross-entropy (Eq.\n sent5: 3) losses to preserve the generation ability of the pre-trained abstractive model:\n",
        "output": "{\"INFORMATION\": [\"sent3\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent4\", \"sent5\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "where,",
                    "h",
                    "i",
                    "(f,",
                    "e)",
                    "denotes",
                    "the",
                    "different",
                    "components",
                    "for",
                    "translating",
                    "the",
                    "source",
                    "sentence",
                    "f",
                    "into",
                    "the",
                    "target",
                    "sentence",
                    "e.",
                    "K",
                    "is",
                    "the",
                    "number",
                    "of",
                    "components",
                    "(or",
                    "features)",
                    "used",
                    "and",
                    "λ",
                    "i",
                    "are",
                    "the",
                    "corresponding",
                    "weights",
                    "of",
                    "the",
                    "components."
                ],
                [
                    "The",
                    "Moses",
                    "SMT",
                    "system",
                    "#REF",
                    ",",
                    "which",
                    "implements",
                    "this",
                    "particular",
                    "model,",
                    "was",
                    "used",
                    "for",
                    "all",
                    "our",
                    "PBSMT",
                    "translation",
                    "experiments."
                ],
                [
                    "Different",
                    "component",
                    "weights",
                    "(λ",
                    "i",
                    ")",
                    "were",
                    "estimated",
                    "using",
                    "a",
                    "discriminative",
                    "training",
                    "method",
                    "known",
                    "as",
                    "Minimum",
                    "Error",
                    "Rate",
                    "Training",
                    "(MERT)",
                    "#TARGET_REF",
                    ",",
                    "on",
                    "a",
                    "held",
                    "out",
                    "development",
                    "set",
                    "(devset)."
                ]
            ],
            "context": [
                0,
                0,
                2
            ]
        },
        "input": "sent0: where, h i (f, e) denotes the different components for translating the source sentence f into the target sentence e. K is the number of components (or features) used and λ i are the corresponding weights of the components.\n sent1: The Moses SMT system #REF , which implements this particular model, was used for all our PBSMT translation experiments.\n sent2: Different component weights (λ i ) were estimated using a discriminative training method known as Minimum Error Rate Training (MERT) #TARGET_REF , on a held out development set (devset).\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "the",
                    "Localized",
                    "Narratives",
                    "dataset",
                    "#TARGET_REF",
                    ",",
                    "the",
                    "annotators",
                    "describe",
                    "the",
                    "image",
                    "while",
                    "drawing",
                    "the",
                    "traces",
                    "of",
                    "their",
                    "attention",
                    "movement,",
                    "which",
                    "presents",
                    "a",
                    "spatial",
                    "alignment",
                    "between",
                    "visual",
                    "objects",
                    "and",
                    "caption",
                    "tokens",
                    "as",
                    "well",
                    "as",
                    "a",
                    "temporal",
                    "alignment",
                    "between",
                    "user",
                    "intention(by",
                    "trace)",
                    "and",
                    "caption",
                    "sentences."
                ],
                [
                    "From",
                    "Figure",
                    "1,",
                    "we",
                    "see",
                    "that",
                    "the",
                    "caption",
                    "tokens,",
                    "e.g."
                ],
                [
                    "\"person\",",
                    "\"horse\",",
                    "\"trees\"",
                    "can",
                    "be",
                    "grounded",
                    "to",
                    "the",
                    "visual",
                    "objects",
                    "spatially,",
                    "and",
                    "the",
                    "order",
                    "of",
                    "caption",
                    "sentences",
                    "can",
                    "be",
                    "arranged",
                    "to",
                    "align",
                    "to",
                    "the",
                    "order",
                    "of",
                    "traces",
                    "temporally."
                ],
                [
                    "Although",
                    "it",
                    "is",
                    "easy",
                    "for",
                    "humans",
                    "to",
                    "recognize",
                    "which",
                    "visual",
                    "object",
                    "is",
                    "indicated",
                    "by",
                    "the",
                    "traces,",
                    "it",
                    "is",
                    "a",
                    "challenge",
                    "for",
                    "the",
                    "agent",
                    "to",
                    "recognize,",
                    "emphasize",
                    "and",
                    "arrange",
                    "visual",
                    "semantics",
                    "solely",
                    "based",
                    "on",
                    "several",
                    "tracepoints'",
                    "coordinates."
                ],
                [
                    "Thereby,",
                    "we",
                    "mainly",
                    "devote",
                    "our",
                    "effort",
                    "to",
                    "the",
                    "spatial",
                    "grounding",
                    "and",
                    "temporal",
                    "controllability",
                    "of",
                    "image",
                    "captioning."
                ]
            ],
            "context": [
                1,
                2,
                2,
                0,
                0
            ]
        },
        "input": "sent0: In the Localized Narratives dataset #TARGET_REF , the annotators describe the image while drawing the traces of their attention movement, which presents a spatial alignment between visual objects and caption tokens as well as a temporal alignment between user intention(by trace) and caption sentences.\n sent1: From Figure 1, we see that the caption tokens, e.g.\n sent2: \"person\", \"horse\", \"trees\" can be grounded to the visual objects spatially, and the order of caption sentences can be arranged to align to the order of traces temporally.\n sent3: Although it is easy for humans to recognize which visual object is indicated by the traces, it is a challenge for the agent to recognize, emphasize and arrange visual semantics solely based on several tracepoints' coordinates.\n sent4: Thereby, we mainly devote our effort to the spatial grounding and temporal controllability of image captioning.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [\"sent1\", \"sent2\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Our",
                    "system",
                    "outperforms",
                    "the",
                    "R-Net",
                    "baseline",
                    "(Rouge-L:",
                    "40.22)",
                    "used",
                    "by",
                    "#REF",
                    "."
                ],
                [
                    "Our",
                    "system",
                    "is",
                    "supposed",
                    "to",
                    "be",
                    "applied",
                    "at",
                    "the",
                    "sentence",
                    "level",
                    "and",
                    "the",
                    "results",
                    "indicate",
                    "that",
                    "a",
                    "unsupervised",
                    "system",
                    "such",
                    "as",
                    "ours",
                    "could",
                    "outperform",
                    "more",
                    "complicated",
                    "deep",
                    "learning",
                    "models."
                ],
                [
                    "If",
                    "there",
                    "is",
                    "a",
                    "trade-off",
                    "sought",
                    "between",
                    "computing",
                    "time",
                    "and",
                    "accuracy,",
                    "our",
                    "system",
                    "performs",
                    "similar",
                    "to",
                    "or",
                    "better",
                    "than",
                    "the",
                    "baseline",
                    "used",
                    "by",
                    "#TARGET_REF",
                    "ROUGE",
                    "score",
                    "is",
                    "not",
                    "the",
                    "best",
                    "metric",
                    "for",
                    "tasks",
                    "such",
                    "as",
                    "opinion",
                    "question",
                    "answering."
                ],
                [
                    "We",
                    "believe",
                    "the",
                    "cosine",
                    "similarity",
                    "is",
                    "a",
                    "better",
                    "metric",
                    "to",
                    "measure",
                    "how",
                    "close",
                    "the",
                    "retrieved",
                    "answer",
                    "is",
                    "to",
                    "the",
                    "gold",
                    "standard."
                ],
                [
                    "Overall",
                    "the",
                    "sim",
                    "method",
                    "is",
                    "able",
                    "to",
                    "provide",
                    "an",
                    "answer",
                    "more",
                    "than",
                    "70%",
                    "similar",
                    "to",
                    "the",
                    "gold",
                    "standard",
                    "answer",
                    "91.5%",
                    "of",
                    "the",
                    "time."
                ],
                [
                    "From",
                    "the",
                    "sentences",
                    "returned",
                    "by",
                    "our",
                    "system",
                    "as",
                    "candidate",
                    "answers,",
                    "72%",
                    "of",
                    "the",
                    "time",
                    "at",
                    "least",
                    "half",
                    "the",
                    "candidate",
                    "sentences",
                    "are",
                    "good",
                    "answers."
                ],
                [
                    "This",
                    "shows",
                    "that",
                    "our",
                    "system",
                    "is",
                    "consistent",
                    "and",
                    "accurate",
                    "at",
                    "providing",
                    "good",
                    "answers."
                ]
            ],
            "context": [
                0,
                0,
                2,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Our system outperforms the R-Net baseline (Rouge-L: 40.22) used by #REF .\n sent1: Our system is supposed to be applied at the sentence level and the results indicate that a unsupervised system such as ours could outperform more complicated deep learning models.\n sent2: If there is a trade-off sought between computing time and accuracy, our system performs similar to or better than the baseline used by #TARGET_REF ROUGE score is not the best metric for tasks such as opinion question answering.\n sent3: We believe the cosine similarity is a better metric to measure how close the retrieved answer is to the gold standard.\n sent4: Overall the sim method is able to provide an answer more than 70% similar to the gold standard answer 91.5% of the time.\n sent5: From the sentences returned by our system as candidate answers, 72% of the time at least half the candidate sentences are good answers.\n sent6: This shows that our system is consistent and accurate at providing good answers.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "If",
                    "recombination",
                    "or",
                    "rephrasing",
                    "is",
                    "required",
                    "during",
                    "the",
                    "composition",
                    "process,",
                    "this",
                    "implies",
                    "consultation",
                    "with",
                    "the",
                    "user."
                ],
                [
                    "Therefore,",
                    "the",
                    "system",
                    "must",
                    "be",
                    "able",
                    "to",
                    "interact",
                    "in",
                    "an",
                    "intelligent",
                    "manner",
                    "with",
                    "the",
                    "user",
                    "and,",
                    "consequently,",
                    "an",
                    "additional",
                    "module",
                    "in",
                    "the",
                    "form",
                    "of",
                    "a",
                    "human-machine",
                    "interaction",
                    "dialogue",
                    "model",
                    "is",
                    "required."
                ],
                [
                    "In",
                    "order",
                    "for",
                    "the",
                    "system",
                    "to",
                    "interact",
                    "'intelligently',",
                    "it",
                    "must",
                    "understand",
                    "the",
                    "communicative",
                    "intent",
                    "of",
                    "the",
                    "user",
                    "and",
                    "therefore",
                    "must",
                    "have",
                    "knowledge",
                    "of",
                    "the",
                    "domain."
                ],
                [
                    "We",
                    "will",
                    "not",
                    "address",
                    "here",
                    "this",
                    "aspect",
                    "of",
                    "the",
                    "proposed",
                    "system,",
                    "which",
                    "is",
                    "essentially",
                    "a",
                    "typical",
                    "problem",
                    "of",
                    "task-oriented",
                    "dialogue",
                    "(",
                    "#TARGET_REF",
                    ")."
                ]
            ],
            "context": [
                2,
                2,
                0,
                1
            ]
        },
        "input": "sent0: If recombination or rephrasing is required during the composition process, this implies consultation with the user.\n sent1: Therefore, the system must be able to interact in an intelligent manner with the user and, consequently, an additional module in the form of a human-machine interaction dialogue model is required.\n sent2: In order for the system to interact 'intelligently', it must understand the communicative intent of the user and therefore must have knowledge of the domain.\n sent3: We will not address here this aspect of the proposed system, which is essentially a typical problem of task-oriented dialogue ( #TARGET_REF ).\n",
        "output": "{\"INFORMATION\": [\"sent3\"], \"PERCEPTION\": [\"sent0\", \"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Figure",
                    "1",
                    "shows",
                    "an",
                    "example",
                    "in",
                    "the",
                    "commonsense",
                    "explanation",
                    "generation",
                    "(ComVE)",
                    "task."
                ],
                [
                    "The",
                    "dataset",
                    "has",
                    "collected",
                    "explanations",
                    "to",
                    "counterfactual",
                    "statements",
                    "for",
                    "sense-making",
                    "from",
                    "three",
                    "annotators",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "From",
                    "the",
                    "annotations,",
                    "we",
                    "observed",
                    "that",
                    "different",
                    "annotators",
                    "gave",
                    "explanations",
                    "to",
                    "the",
                    "unreasonable",
                    "statement",
                    "from",
                    "different",
                    "perspectives",
                    "to",
                    "make",
                    "them",
                    "diverse",
                    "in",
                    "terms",
                    "of",
                    "content,",
                    "e.g.,",
                    "wrong",
                    "effect",
                    "and",
                    "inappropriate",
                    "usage."
                ]
            ],
            "context": [
                3,
                1,
                2
            ]
        },
        "input": "sent0: Figure 1 shows an example in the commonsense explanation generation (ComVE) task.\n sent1: The dataset has collected explanations to counterfactual statements for sense-making from three annotators #TARGET_REF .\n sent2: From the annotations, we observed that different annotators gave explanations to the unreasonable statement from different perspectives to make them diverse in terms of content, e.g., wrong effect and inappropriate usage.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Furthermore,",
                    "Kumar",
                    "et",
                    "al."
                ],
                [
                    "(",
                    "2021)",
                    "studied",
                    "if",
                    "the",
                    "decisions",
                    "of",
                    "human",
                    "players",
                    "can",
                    "be",
                    "predicted",
                    "in",
                    "an",
                    "amended",
                    "version",
                    "of",
                    "Codenames."
                ],
                [
                    "For",
                    "the",
                    "predictions,",
                    "they",
                    "used",
                    "word2vec",
                    "and",
                    "GloVe",
                    "word",
                    "embeddings,",
                    "as",
                    "well",
                    "as",
                    "several",
                    "similarity",
                    "measures",
                    "on",
                    "free",
                    "association",
                    "datasets,",
                    "in",
                    "particular",
                    "SWOW",
                    "#REF",
                    "and",
                    "USF",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "They",
                    "found",
                    "that",
                    "similarity",
                    "based",
                    "on",
                    "random",
                    "walks",
                    "in",
                    "SWOW",
                    "performed",
                    "the",
                    "best,",
                    "from",
                    "which",
                    "they",
                    "concluded",
                    "that",
                    "not",
                    "only",
                    "direct",
                    "associations,",
                    "but",
                    "indirect",
                    "connections",
                    "are",
                    "also",
                    "important",
                    "in",
                    "this",
                    "game."
                ]
            ],
            "context": [
                3,
                3,
                2,
                3
            ]
        },
        "input": "sent0: Furthermore, Kumar et al.\n sent1: ( 2021) studied if the decisions of human players can be predicted in an amended version of Codenames.\n sent2: For the predictions, they used word2vec and GloVe word embeddings, as well as several similarity measures on free association datasets, in particular SWOW #REF and USF #TARGET_REF .\n sent3: They found that similarity based on random walks in SWOW performed the best, from which they concluded that not only direct associations, but indirect connections are also important in this game.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": [\"sent0\", \"sent1\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "There",
                    "are",
                    "two",
                    "general",
                    "approaches",
                    "to",
                    "tackle",
                    "such",
                    "artifacts:",
                    "(1)",
                    "adversarial",
                    "filtering",
                    "of",
                    "biased",
                    "examples,",
                    "i.e.,",
                    "examples",
                    "that",
                    "contain",
                    "artifacts,",
                    "and",
                    "(2)",
                    "debiasing",
                    "methods."
                ],
                [
                    "In",
                    "the",
                    "first",
                    "approach,",
                    "potentially",
                    "biased",
                    "examples",
                    "are",
                    "discarded",
                    "from",
                    "the",
                    "dataset,",
                    "either",
                    "after",
                    "the",
                    "dataset",
                    "creation",
                    "#TARGET_REF",
                    ",",
                    "or",
                    "while",
                    "creating",
                    "the",
                    "dataset",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                3,
                2
            ]
        },
        "input": "sent0: There are two general approaches to tackle such artifacts: (1) adversarial filtering of biased examples, i.e., examples that contain artifacts, and (2) debiasing methods.\n sent1: In the first approach, potentially biased examples are discarded from the dataset, either after the dataset creation #TARGET_REF , or while creating the dataset #REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "To",
                    "achieve",
                    "this",
                    "goal,",
                    "we",
                    "investigate",
                    "whether",
                    "due",
                    "to",
                    "the",
                    "similarity",
                    "between",
                    "idioms",
                    "and",
                    "rare-words",
                    "Schick",
                    "and",
                    "Schütze's",
                    "BERT",
                    "for",
                    "Attentive",
                    "Mimicking",
                    "#TARGET_REF",
                    ")",
                    "(BERTRAM)",
                    "model,",
                    "which",
                    "was",
                    "designed",
                    "for",
                    "use",
                    "with",
                    "rare-words,",
                    "can",
                    "be",
                    "used",
                    "to",
                    "explicitly",
                    "learn",
                    "high-quality",
                    "embeddings",
                    "for",
                    "idiomatic",
                    "expressions."
                ],
                [
                    "We",
                    "also",
                    "investigate",
                    "how",
                    "many",
                    "examples",
                    "of",
                    "each",
                    "idiom",
                    "are",
                    "required",
                    "to",
                    "create",
                    "embeddings",
                    "that",
                    "perform",
                    "well",
                    "on",
                    "the",
                    "task,",
                    "as",
                    "well",
                    "as",
                    "how",
                    "the",
                    "quality",
                    "of",
                    "contexts",
                    "fed",
                    "to",
                    "the",
                    "BERTRAM",
                    "model",
                    "effects",
                    "the",
                    "representations",
                    "and",
                    "performance",
                    "on",
                    "the",
                    "task."
                ]
            ],
            "context": [
                1,
                0
            ]
        },
        "input": "sent0: To achieve this goal, we investigate whether due to the similarity between idioms and rare-words Schick and Schütze's BERT for Attentive Mimicking #TARGET_REF ) (BERTRAM) model, which was designed for use with rare-words, can be used to explicitly learn high-quality embeddings for idiomatic expressions.\n sent1: We also investigate how many examples of each idiom are required to create embeddings that perform well on the task, as well as how the quality of contexts fed to the BERTRAM model effects the representations and performance on the task.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "CCG",
                    "#TARGET_REF",
                    "is",
                    "a",
                    "grammar",
                    "formalism",
                    "which",
                    "consists",
                    "of",
                    "a",
                    "lexicon",
                    "that",
                    "pairs",
                    "words",
                    "with",
                    "lexical",
                    "categories",
                    "(supertags)",
                    "and",
                    "a",
                    "set",
                    "of",
                    "combinatory",
                    "rules",
                    "which",
                    "specify",
                    "how",
                    "the",
                    "categories",
                    "are",
                    "combined."
                ],
                [
                    "A",
                    "supertag",
                    "is",
                    "a",
                    "rich",
                    "syntactic",
                    "description",
                    "that",
                    "specifies",
                    "the",
                    "local",
                    "syntactic",
                    "context",
                    "of",
                    "the",
                    "word",
                    "in",
                    "the",
                    "form",
                    "of",
                    "a",
                    "set",
                    "of",
                    "arguments."
                ],
                [
                    "Most",
                    "of",
                    "the",
                    "CCG",
                    "grammar",
                    "is",
                    "contained",
                    "in",
                    "the",
                    "lexicon,",
                    "that",
                    "is",
                    "why",
                    "CCG",
                    "has",
                    "simpler",
                    "combinatory",
                    "rules",
                    "in",
                    "comparison",
                    "to",
                    "CFG",
                    "production",
                    "rules."
                ],
                [
                    "CCG",
                    "categories",
                    "are",
                    "divided",
                    "into",
                    "atomic",
                    "and",
                    "complex",
                    "categories."
                ],
                [
                    "Examples",
                    "of",
                    "atomic",
                    "categories",
                    "are:",
                    "S",
                    "(sentence),",
                    "N",
                    "(noun),",
                    "NP",
                    "(noun",
                    "phrase),",
                    "etc."
                ],
                [
                    "Complex",
                    "categories",
                    "such",
                    "as",
                    "S\\NP",
                    "and",
                    "(S\\NP)/NP",
                    "are",
                    "functions",
                    "which",
                    "specify",
                    "the",
                    "type",
                    "and",
                    "directionality",
                    "of",
                    "their",
                    "arguments",
                    "and",
                    "results."
                ],
                [
                    "Complex",
                    "categories",
                    "have",
                    "the",
                    "following",
                    "formats:"
                ]
            ],
            "context": [
                1,
                1,
                2,
                1,
                3,
                0,
                0
            ]
        },
        "input": "sent0: CCG #TARGET_REF is a grammar formalism which consists of a lexicon that pairs words with lexical categories (supertags) and a set of combinatory rules which specify how the categories are combined.\n sent1: A supertag is a rich syntactic description that specifies the local syntactic context of the word in the form of a set of arguments.\n sent2: Most of the CCG grammar is contained in the lexicon, that is why CCG has simpler combinatory rules in comparison to CFG production rules.\n sent3: CCG categories are divided into atomic and complex categories.\n sent4: Examples of atomic categories are: S (sentence), N (noun), NP (noun phrase), etc.\n sent5: Complex categories such as S\\NP and (S\\NP)/NP are functions which specify the type and directionality of their arguments and results.\n sent6: Complex categories have the following formats:\n",
        "output": "{\"INFORMATION\": [\"sent0\", \"sent1\", \"sent3\"], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "For",
                    "sake",
                    "of",
                    "comparison,",
                    "we",
                    "have",
                    "computed",
                    "BLEU",
                    "at",
                    "word",
                    "level",
                    "using",
                    "BPE",
                    "method",
                    "#REF",
                    "."
                ],
                [
                    "We",
                    "computed",
                    "the",
                    "subwords",
                    "units",
                    "in",
                    "the",
                    "output",
                    "side",
                    "of",
                    "the",
                    "neural",
                    "network",
                    "as",
                    "done",
                    "with",
                    "Factored",
                    "approach."
                ],
                [
                    "We",
                    "set",
                    "the",
                    "number",
                    "of",
                    "merge",
                    "operations",
                    "for",
                    "the",
                    "BPE",
                    "algorithm,",
                    "as",
                    "explained",
                    "in",
                    "the",
                    "paper",
                    "#TARGET_REF",
                    ",",
                    "following",
                    "equation",
                    "12."
                ]
            ],
            "context": [
                2,
                2,
                2
            ]
        },
        "input": "sent0: For sake of comparison, we have computed BLEU at word level using BPE method #REF .\n sent1: We computed the subwords units in the output side of the neural network as done with Factored approach.\n sent2: We set the number of merge operations for the BPE algorithm, as explained in the paper #TARGET_REF , following equation 12.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\", \"sent1\", \"sent2\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "diversity",
                    "of",
                    "neural",
                    "dialogue",
                    "generation",
                    "has",
                    "been",
                    "studied",
                    "actively."
                ],
                [
                    "#REF",
                    "first",
                    "addressed",
                    "this",
                    "problem",
                    "using",
                    "Maximum",
                    "Mutual",
                    "Information",
                    "(MMI)",
                    "as",
                    "the",
                    "objective",
                    "function",
                    "of",
                    "the",
                    "neural",
                    "model."
                ],
                [
                    "#REF",
                    "used",
                    "Positive",
                    "Pointwise",
                    "Mutual",
                    "Information",
                    "(PPMI)",
                    "to",
                    "identify",
                    "keywords",
                    "in",
                    "the",
                    "dialogue",
                    "corpus",
                    "that",
                    "were",
                    "likely",
                    "to",
                    "appear",
                    "both",
                    "in",
                    "response",
                    "utterances",
                    "and",
                    "their",
                    "input",
                    "utterances."
                ],
                [
                    "#REF",
                    "proposed",
                    "a",
                    "model",
                    "that",
                    "uses",
                    "topic",
                    "words",
                    "extracted",
                    "from",
                    "conversations",
                    "to",
                    "simulate",
                    "human",
                    "prior",
                    "knowledge,",
                    "generating",
                    "informative",
                    "and",
                    "interesting",
                    "responses."
                ],
                [
                    "In",
                    "addition,",
                    "Variational",
                    "Au-toEncoder",
                    "(VAE)",
                    "and",
                    "Generative",
                    "Adversarial",
                    "Network",
                    "(GAN),",
                    "which",
                    "were",
                    "proposed",
                    "originally",
                    "for",
                    "image",
                    "generation,",
                    "have",
                    "also",
                    "been",
                    "applied",
                    "to",
                    "text",
                    "and",
                    "dialogue",
                    "generation",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Although",
                    "GAN",
                    "helps",
                    "to",
                    "reduce",
                    "response",
                    "text",
                    "ambiguity,",
                    "their",
                    "primary",
                    "purpose",
                    "was",
                    "not",
                    "diversity."
                ],
                [
                    "#REF",
                    "proposed",
                    "and",
                    "demonstrated",
                    "the",
                    "effectiveness",
                    "of",
                    "Adversarial",
                    "Information",
                    "Maximization",
                    "(AIM)",
                    "as",
                    "a",
                    "new",
                    "method",
                    "for",
                    "generating",
                    "informative",
                    "and",
                    "diverse",
                    "conversational",
                    "responses."
                ],
                [
                    "Their",
                    "work",
                    "also",
                    "resolved",
                    "instability",
                    "that",
                    "arose",
                    "when",
                    "training",
                    "the",
                    "GAN",
                    "model."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                3,
                0,
                0,
                0
            ]
        },
        "input": "sent0: The diversity of neural dialogue generation has been studied actively.\n sent1: #REF first addressed this problem using Maximum Mutual Information (MMI) as the objective function of the neural model.\n sent2: #REF used Positive Pointwise Mutual Information (PPMI) to identify keywords in the dialogue corpus that were likely to appear both in response utterances and their input utterances.\n sent3: #REF proposed a model that uses topic words extracted from conversations to simulate human prior knowledge, generating informative and interesting responses.\n sent4: In addition, Variational Au-toEncoder (VAE) and Generative Adversarial Network (GAN), which were proposed originally for image generation, have also been applied to text and dialogue generation #TARGET_REF .\n sent5: Although GAN helps to reduce response text ambiguity, their primary purpose was not diversity.\n sent6: #REF proposed and demonstrated the effectiveness of Adversarial Information Maximization (AIM) as a new method for generating informative and diverse conversational responses.\n sent7: Their work also resolved instability that arose when training the GAN model.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "order",
                    "to",
                    "create",
                    "diversity,",
                    "existing",
                    "methods",
                    "attempted",
                    "to",
                    "produce",
                    "uncertainty",
                    "by",
                    "introducing",
                    "random",
                    "noise",
                    "into",
                    "a",
                    "latent",
                    "variable",
                    "#TARGET_REF",
                    "or",
                    "sampling",
                    "next",
                    "token",
                    "widely",
                    "from",
                    "the",
                    "vo-",
                    "cabulary",
                    "#REF",
                    "."
                ],
                [
                    "However,",
                    "these",
                    "methods",
                    "were",
                    "not",
                    "able",
                    "to",
                    "explicitly",
                    "control",
                    "varying",
                    "semantics",
                    "units",
                    "and",
                    "produce",
                    "outputs",
                    "of",
                    "diverse",
                    "content."
                ],
                [
                    "Meanwhile,",
                    "the",
                    "input",
                    "text",
                    "alone",
                    "contains",
                    "too",
                    "limited",
                    "knowledge",
                    "to",
                    "support",
                    "diverse",
                    "reasoning",
                    "and",
                    "produce",
                    "multiple",
                    "reasonable",
                    "outputs",
                    "#REF",
                    "."
                ],
                [
                    "As",
                    "an",
                    "example,",
                    "Table",
                    "1",
                    "shows",
                    "the",
                    "human",
                    "evaluation",
                    "results",
                    "on",
                    "two",
                    "GCR",
                    "tasks."
                ],
                [
                    "While",
                    "human",
                    "annotators",
                    "were",
                    "able",
                    "to",
                    "produce",
                    "2.60",
                    "different",
                    "yet",
                    "reasonable",
                    "explanations",
                    "on",
                    "the",
                    "ComVE",
                    "dataset,",
                    "one",
                    "SoTA",
                    "diversity-promoting",
                    "method",
                    "(i.e.,",
                    "nucleus",
                    "sampling",
                    "#REF",
                    ")",
                    "could",
                    "produce",
                    "only",
                    "2.15",
                    "reasonable",
                    "explanations."
                ]
            ],
            "context": [
                3,
                2,
                2,
                3,
                0
            ]
        },
        "input": "sent0: In order to create diversity, existing methods attempted to produce uncertainty by introducing random noise into a latent variable #TARGET_REF or sampling next token widely from the vo- cabulary #REF .\n sent1: However, these methods were not able to explicitly control varying semantics units and produce outputs of diverse content.\n sent2: Meanwhile, the input text alone contains too limited knowledge to support diverse reasoning and produce multiple reasonable outputs #REF .\n sent3: As an example, Table 1 shows the human evaluation results on two GCR tasks.\n sent4: While human annotators were able to produce 2.60 different yet reasonable explanations on the ComVE dataset, one SoTA diversity-promoting method (i.e., nucleus sampling #REF ) could produce only 2.15 reasonable explanations.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\", \"sent2\"], \"BACKGROUND\": [\"sent0\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Pretrained",
                    "transformers",
                    "such",
                    "as",
                    "BERT",
                    "#REF",
                    "have",
                    "dramatically",
                    "increased",
                    "retrieval",
                    "effectiveness",
                    "in",
                    "many",
                    "tasks",
                    "across",
                    "a",
                    "multitude",
                    "of",
                    "domains",
                    "#REF",
                    "."
                ],
                [
                    "Nevertheless,",
                    "in",
                    "a",
                    "standard",
                    "\"retrieve-then-rerank\"",
                    "setup,",
                    "the",
                    "application",
                    "of",
                    "pretrained",
                    "transformer-based",
                    "rerankers",
                    "incurs",
                    "large",
                    "computational",
                    "costs",
                    "and",
                    "long",
                    "query",
                    "latencies,",
                    "making",
                    "those",
                    "rerankers",
                    "unrealistic",
                    "for",
                    "many",
                    "real-world",
                    "applications."
                ],
                [
                    "For",
                    "example,",
                    "according",
                    "to",
                    "the",
                    "ColBERT",
                    "paper",
                    "#TARGET_REF",
                    ",",
                    "reranking",
                    "1000",
                    "hits",
                    "from",
                    "the",
                    "MS",
                    "MARCO",
                    "passage",
                    "dataset",
                    "takes",
                    "32.9",
                    "seconds",
                    "per",
                    "query."
                ],
                [
                    "Other",
                    "researchers",
                    "have",
                    "noted",
                    "the",
                    "computational",
                    "costs",
                    "of",
                    "transformer-based",
                    "rankers",
                    "(Hofstätter",
                    "and",
                    "Hanbury,",
                    "*",
                    "Equal",
                    "contribution",
                    "2019),",
                    "and",
                    "this",
                    "realization",
                    "has",
                    "compelled",
                    "the",
                    "field",
                    "to",
                    "explore",
                    "other",
                    "approaches,",
                    "for",
                    "example,",
                    "simplified",
                    "models",
                    "#REF",
                    "and",
                    "learned",
                    "dense",
                    "representations",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                3,
                2,
                3,
                3
            ]
        },
        "input": "sent0: Pretrained transformers such as BERT #REF have dramatically increased retrieval effectiveness in many tasks across a multitude of domains #REF .\n sent1: Nevertheless, in a standard \"retrieve-then-rerank\" setup, the application of pretrained transformer-based rerankers incurs large computational costs and long query latencies, making those rerankers unrealistic for many real-world applications.\n sent2: For example, according to the ColBERT paper #TARGET_REF , reranking 1000 hits from the MS MARCO passage dataset takes 32.9 seconds per query.\n sent3: Other researchers have noted the computational costs of transformer-based rankers (Hofstätter and Hanbury, * Equal contribution 2019), and this realization has compelled the field to explore other approaches, for example, simplified models #REF and learned dense representations #REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent0\", \"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "that",
                    "structural",
                    "correspondences",
                    "hold",
                    "for",
                    "entire",
                    "classes",
                    "of",
                    "lexical",
                    "items."
                ],
                [
                    "For",
                    "example,",
                    "a",
                    "classic",
                    "problem",
                    "is",
                    "the",
                    "translation",
                    "of",
                    "motion",
                    "verbs",
                    "from",
                    "English",
                    "to",
                    "French."
                ],
                [
                    "In",
                    "English",
                    "the",
                    "manner",
                    "of",
                    "motion",
                    "can",
                    "be",
                    "incorporated",
                    "into",
                    "the",
                    "matrix",
                    "verb,",
                    "with",
                    "the",
                    "direction",
                    "of",
                    "the",
                    "motion",
                    "being",
                    "adjoined",
                    "on",
                    "by",
                    "a",
                    "prepositional",
                    "phrase,",
                    "as",
                    "in",
                    "John",
                    "swam",
                    "across",
                    "the",
                    "lake."
                ],
                [
                    "In",
                    "many",
                    "cases,",
                    "this",
                    "is",
                    "not",
                    "allowed",
                    "in",
                    "French,",
                    "where",
                    "the",
                    "direction",
                    "becomes",
                    "incorporated",
                    "into",
                    "the",
                    "matrix",
                    "verb,",
                    "and",
                    "the",
                    "manner",
                    "is",
                    "adjoined",
                    "on",
                    "as",
                    "an",
                    "adverbial",
                    "or",
                    "a",
                    "prepositional",
                    "phrase,",
                    "as",
                    "in",
                    "Jean",
                    "a",
                    "traversé",
                    "le",
                    "lac",
                    "à",
                    "la",
                    "nage,",
                    "(Jean",
                    "crosses",
                    "the",
                    "lake",
                    "by",
                    "swimming)."
                ],
                [
                    "This",
                    "type",
                    "of",
                    "structural",
                    "correspondence",
                    "has",
                    "been",
                    "typically",
                    "handled",
                    "best",
                    "by",
                    "interlingua",
                    "approaches,",
                    "since",
                    "traditional",
                    "transfer",
                    "approaches",
                    "required",
                    "that",
                    "every",
                    "possible",
                    "combination",
                    "of",
                    "manner",
                    "of",
                    "motion",
                    "verb",
                    "and",
                    "path",
                    "prepositional",
                    "phrase",
                    "be",
                    "listed",
                    "explicitly,",
                    "and",
                    "paired",
                    "with",
                    "its",
                    "target",
                    "language",
                    "equivalent."
                ],
                [
                    "The",
                    "lexico-structural",
                    "approach",
                    "allows",
                    "the",
                    "entire",
                    "class",
                    "of",
                    "English",
                    "manner",
                    "of",
                    "motion",
                    "verbs",
                    "that",
                    "have",
                    "adjoined",
                    "path",
                    "prepositional",
                    "phrases,",
                    "to",
                    "be",
                    "associated",
                    "in",
                    "a",
                    "single",
                    "transfer",
                    "lexicon",
                    "entry",
                    "with",
                    "the",
                    "class",
                    "of",
                    "French",
                    "directed",
                    "motion",
                    "verbs",
                    "with",
                    "adjoined",
                    "manners",
                    "of",
                    "motion."
                ],
                [
                    "This",
                    "is",
                    "effected",
                    "by",
                    "treating",
                    "manner",
                    "of",
                    "motion,",
                    "path",
                    "and",
                    "directed",
                    "motion",
                    "as",
                    "cross-linguistic",
                    "semantic",
                    "features",
                    "that",
                    "occur",
                    "in",
                    "both",
                    "languages,",
                    "and",
                    "serve",
                    "to",
                    "anchor",
                    "the",
                    "correspondences",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "These",
                    "are",
                    "the",
                    "same",
                    "basic",
                    "components",
                    "that",
                    "Jackendoff",
                    "ascribes",
                    "to",
                    "change-of-location",
                    "verbs",
                    "in",
                    "his",
                    "Lexical",
                    "Conceptual",
                    "Structures",
                    "(LCS),",
                    "GO,",
                    "PATH",
                    "and",
                    "MANNER,",
                    "#REF",
                    "."
                ],
                [
                    "A",
                    "similar",
                    "interlingua",
                    "treatment,",
                    "also",
                    "based",
                    "on",
                    "LCS,",
                    "would",
                    "decompose",
                    "the",
                    "English",
                    "phrase",
                    "swim",
                    "across",
                    "the",
                    "lake",
                    "into",
                    "the",
                    "same",
                    "three",
                    "separate",
                    "components",
                    "which",
                    "would",
                    "constitute",
                    "the",
                    "predicates",
                    "of",
                    "the",
                    "predicate-argument",
                    "structure."
                ],
                [
                    "This",
                    "predicate",
                    "argument",
                    "structure,",
                    "the",
                    "LCS,",
                    "then",
                    "also",
                    "serves",
                    "as",
                    "the",
                    "representation",
                    "for",
                    "the",
                    "French",
                    "translation",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                3,
                1,
                3,
                0,
                0
            ]
        },
        "input": "sent0: that structural correspondences hold for entire classes of lexical items.\n sent1: For example, a classic problem is the translation of motion verbs from English to French.\n sent2: In English the manner of motion can be incorporated into the matrix verb, with the direction of the motion being adjoined on by a prepositional phrase, as in John swam across the lake.\n sent3: In many cases, this is not allowed in French, where the direction becomes incorporated into the matrix verb, and the manner is adjoined on as an adverbial or a prepositional phrase, as in Jean a traversé le lac à la nage, (Jean crosses the lake by swimming).\n sent4: This type of structural correspondence has been typically handled best by interlingua approaches, since traditional transfer approaches required that every possible combination of manner of motion verb and path prepositional phrase be listed explicitly, and paired with its target language equivalent.\n sent5: The lexico-structural approach allows the entire class of English manner of motion verbs that have adjoined path prepositional phrases, to be associated in a single transfer lexicon entry with the class of French directed motion verbs with adjoined manners of motion.\n sent6: This is effected by treating manner of motion, path and directed motion as cross-linguistic semantic features that occur in both languages, and serve to anchor the correspondences #TARGET_REF .\n sent7: These are the same basic components that Jackendoff ascribes to change-of-location verbs in his Lexical Conceptual Structures (LCS), GO, PATH and MANNER, #REF .\n sent8: A similar interlingua treatment, also based on LCS, would decompose the English phrase swim across the lake into the same three separate components which would constitute the predicates of the predicate-argument structure.\n sent9: This predicate argument structure, the LCS, then also serves as the representation for the French translation #REF .\n",
        "output": "{\"INFORMATION\": [\"sent6\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent5\", \"sent7\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "use",
                    "max-pooling",
                    "over",
                    "the",
                    "output",
                    "embedding",
                    "(instead",
                    "of",
                    "mean-pooling",
                    "as",
                    "it",
                    "better",
                    "incorporates",
                    "the",
                    "nature",
                    "of",
                    "natural",
                    "language",
                    "sequences",
                    "#TARGET_REF",
                    "))",
                    "as:w",
                    "c",
                    "i",
                    "=",
                    "max",
                    "i",
                    "e",
                    "w",
                    "i",
                    "i",
                    "(2)For",
                    "a",
                    "total",
                    "of",
                    "h",
                    "filters,",
                    "each",
                    "of",
                    "varying",
                    "widths,",
                    "we",
                    "get",
                    "different",
                    "representations",
                    "of",
                    "w",
                    "i",
                    "."
                ],
                [
                    "Thereforew",
                    "c",
                    "i",
                    "=",
                    "[w",
                    "c",
                    "1",
                    ",",
                    "w",
                    "c",
                    "2",
                    ",",
                    "."
                ],
                [
                    "."
                ],
                [
                    "."
                ],
                [
                    ",",
                    "w",
                    "c",
                    "h",
                    "]is",
                    "the",
                    "representation",
                    "of",
                    "the",
                    "ith",
                    "word."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: We use max-pooling over the output embedding (instead of mean-pooling as it better incorporates the nature of natural language sequences #TARGET_REF )) as:w c i = max i e w i i (2)For a total of h filters, each of varying widths, we get different representations of w i .\n sent1: Thereforew c i = [w c 1 , w c 2 , .\n sent2: .\n sent3: .\n sent4: , w c h ]is the representation of the ith word.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    ")where",
                    "k",
                    "is",
                    "the",
                    "number",
                    "of",
                    "language",
                    "models",
                    "which",
                    "are",
                    "being",
                    "interpolated,",
                    "µ",
                    "i",
                    "the",
                    "interpolation",
                    "weights",
                    "and",
                    "V",
                    "is",
                    "the",
                    "vocabulary",
                    "of",
                    "the",
                    "specific",
                    "language",
                    "model."
                ],
                [
                    "The",
                    "interpolation",
                    "weights",
                    "are",
                    "estimated",
                    "using",
                    "Expectation",
                    "Maximization",
                    "(EM)",
                    "#TARGET_REF",
                    "over",
                    "the",
                    "log-likelihood",
                    "in",
                    "#REF",
                    ":N",
                    "t=1",
                    "log",
                    "k",
                    "i=1",
                    "µ",
                    "i",
                    "(f",
                    "*",
                    "i",
                    "(w",
                    "t",
                    "|h",
                    "t",
                    ")",
                    "+",
                    "λ",
                    "i",
                    "(h",
                    "t",
                    ")P",
                    "r",
                    "mix",
                    "(w",
                    "t",
                    "|",
                    "ht",
                    "))",
                    "(6)where",
                    "the",
                    "index",
                    "t",
                    "scans",
                    "over",
                    "all",
                    "the",
                    "n-grams",
                    "in",
                    "the",
                    "training",
                    "corpora."
                ],
                [
                    "This",
                    "mixture",
                    "model",
                    "was",
                    "used",
                    "to",
                    "combine",
                    "the",
                    "'in-domain'",
                    "language",
                    "model",
                    "with",
                    "an",
                    "'out-of-domain'",
                    "one,",
                    "with",
                    "the",
                    "mixture",
                    "weights",
                    "being",
                    "estimated",
                    "on",
                    "the",
                    "'in-domain'",
                    "training",
                    "data",
                    "by",
                    "applying",
                    "a",
                    "cross-validation",
                    "scheme."
                ],
                [
                    "Further",
                    "improvements",
                    "on",
                    "this",
                    "mixture",
                    "models",
                    "were",
                    "achieved",
                    "using",
                    "parameter",
                    "tying",
                    "to",
                    "the",
                    "most-recent",
                    "context",
                    "words",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                0,
                3,
                0,
                0
            ]
        },
        "input": "sent0: )where k is the number of language models which are being interpolated, µ i the interpolation weights and V is the vocabulary of the specific language model.\n sent1: The interpolation weights are estimated using Expectation Maximization (EM) #TARGET_REF over the log-likelihood in #REF :N t=1 log k i=1 µ i (f * i (w t |h t ) + λ i (h t )P r mix (w t | ht )) (6)where the index t scans over all the n-grams in the training corpora.\n sent2: This mixture model was used to combine the 'in-domain' language model with an 'out-of-domain' one, with the mixture weights being estimated on the 'in-domain' training data by applying a cross-validation scheme.\n sent3: Further improvements on this mixture models were achieved using parameter tying to the most-recent context words #REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "O2O",
                    "model",
                    "trained",
                    "with",
                    "a",
                    "conditional",
                    "chain",
                    "mapping",
                    "#TARGET_REF",
                    "can",
                    "also",
                    "be",
                    "used",
                    "to",
                    "maximize",
                    "Eq."
                ],
                [
                    "(3)."
                ],
                [
                    "4))",
                    "used",
                    "in",
                    "the",
                    "O2M",
                    "model,",
                    "the",
                    "O2O",
                    "conditional",
                    "chain",
                    "mapping",
                    "model",
                    "is",
                    "trained",
                    "to",
                    "maximize",
                    "the",
                    "joint",
                    "log-likelihood",
                    "(Eq."
                ],
                [
                    "(",
                    "3))",
                    "via",
                    "a",
                    "recursive",
                    "expansion",
                    "of",
                    "the",
                    "probabilistic",
                    "chain",
                    "rule."
                ],
                [
                    "This",
                    "model",
                    "does",
                    "not",
                    "require",
                    "or",
                    "assume",
                    "conditional",
                    "independence",
                    "between",
                    "sequence",
                    "types."
                ],
                [
                    "Formally,",
                    "the",
                    "O2O",
                    "model",
                    "is",
                    "trained",
                    "to",
                    "maximize",
                    "the",
                    "following",
                    "loss",
                    "function:"
                ]
            ],
            "context": [
                2,
                2,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: The O2O model trained with a conditional chain mapping #TARGET_REF can also be used to maximize Eq.\n sent1: (3).\n sent2: 4)) used in the O2M model, the O2O conditional chain mapping model is trained to maximize the joint log-likelihood (Eq.\n sent3: ( 3)) via a recursive expansion of the probabilistic chain rule.\n sent4: This model does not require or assume conditional independence between sequence types.\n sent5: Formally, the O2O model is trained to maximize the following loss function:\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\", \"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "the",
                    "candidate",
                    "summaries."
                ],
                [
                    "In",
                    "other",
                    "words,",
                    "we",
                    "give",
                    "the",
                    "abstractive",
                    "model",
                    "a",
                    "dual",
                    "role:",
                    "as",
                    "a",
                    "generation",
                    "model,",
                    "it",
                    "generates",
                    "the",
                    "output",
                    "summaries",
                    "in",
                    "an",
                    "autoregressive",
                    "way,",
                    "as",
                    "an",
                    "evaluation",
                    "model,",
                    "it",
                    "can",
                    "be",
                    "used",
                    "to",
                    "score",
                    "the",
                    "quality",
                    "of",
                    "candidate",
                    "summaries",
                    "by",
                    "estimating",
                    "a",
                    "probability",
                    "distribution",
                    "over",
                    "candidate",
                    "outputs."
                ],
                [
                    "The",
                    "generation",
                    "model",
                    "is",
                    "trained",
                    "using",
                    "the",
                    "standard",
                    "MLE",
                    "loss,",
                    "but",
                    "to",
                    "train",
                    "the",
                    "evaluation",
                    "model",
                    "we",
                    "introduce",
                    "a",
                    "contrastive",
                    "loss",
                    "#TARGET_REF",
                    "defined",
                    "over",
                    "different",
                    "candidate",
                    "summaries",
                    "generated",
                    "by",
                    "pre-trained",
                    "abstractive",
                    "models",
                    "(Fig."
                ],
                [
                    "1),",
                    "following",
                    "previous",
                    "work",
                    "on",
                    "ranking-based",
                    "or",
                    "contrastive",
                    "learning",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                0,
                0,
                1,
                2
            ]
        },
        "input": "sent0: the candidate summaries.\n sent1: In other words, we give the abstractive model a dual role: as a generation model, it generates the output summaries in an autoregressive way, as an evaluation model, it can be used to score the quality of candidate summaries by estimating a probability distribution over candidate outputs.\n sent2: The generation model is trained using the standard MLE loss, but to train the evaluation model we introduce a contrastive loss #TARGET_REF defined over different candidate summaries generated by pre-trained abstractive models (Fig.\n sent3: 1), following previous work on ranking-based or contrastive learning #REF .\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [\"sent3\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "#REF",
                    ",",
                    "a",
                    "multilingual",
                    "dataset",
                    "was",
                    "provided",
                    "containing",
                    "English,",
                    "German,",
                    "Spanish",
                    "and",
                    "French",
                    "and",
                    "there",
                    "were",
                    "two",
                    "subtasks:",
                    "binary",
                    "classification",
                    "and",
                    "probabilistic",
                    "classification."
                ],
                [
                    "The",
                    "submitted",
                    "systems",
                    "mainly",
                    "use",
                    "traditional",
                    "machine",
                    "learning",
                    "classifiers(e.g."
                ],
                [
                    "SVM,",
                    "Random",
                    "Forests)",
                    "with",
                    "features",
                    "#REF",
                    ",",
                    "deep",
                    "learning",
                    "methods",
                    "#TARGET_REF",
                    "and",
                    "ensemble",
                    "methods",
                    "#REF",
                    "."
                ],
                [
                    "More",
                    "recently,",
                    "#REF",
                    "propose",
                    "a",
                    "new",
                    "perspective",
                    "by",
                    "treating",
                    "CWI",
                    "as",
                    "a",
                    "sequence",
                    "labeling",
                    "task",
                    "that",
                    "can",
                    "detect",
                    "both",
                    "complex",
                    "words",
                    "and",
                    "phrases."
                ],
                [
                    "All",
                    "these",
                    "methods",
                    "are",
                    "different",
                    "from",
                    "ours",
                    "which",
                    "utilizes",
                    "heterogeneous",
                    "PLMs",
                    "with",
                    "various",
                    "training",
                    "strategies."
                ]
            ],
            "context": [
                0,
                2,
                3,
                0,
                0
            ]
        },
        "input": "sent0: In #REF , a multilingual dataset was provided containing English, German, Spanish and French and there were two subtasks: binary classification and probabilistic classification.\n sent1: The submitted systems mainly use traditional machine learning classifiers(e.g.\n sent2: SVM, Random Forests) with features #REF , deep learning methods #TARGET_REF and ensemble methods #REF .\n sent3: More recently, #REF propose a new perspective by treating CWI as a sequence labeling task that can detect both complex words and phrases.\n sent4: All these methods are different from ours which utilizes heterogeneous PLMs with various training strategies.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Toxicity",
                    "Datasets",
                    "in",
                    "Online",
                    "Games",
                    "In",
                    "multiplayer",
                    "online",
                    "games,",
                    "prior",
                    "research",
                    "focused",
                    "on",
                    "analysis",
                    "of",
                    "anti-social",
                    "or",
                    "disruptive",
                    "behavior,",
                    "socalled",
                    "toxic",
                    "behavior",
                    "#TARGET_REF",
                    "including",
                    "cyberbullying",
                    "#REF",
                    "and",
                    "griefing",
                    "#REF",
                    "."
                ],
                [
                    "Although",
                    "these",
                    "terms",
                    "contain",
                    "similar",
                    "elements,",
                    "a",
                    "single",
                    "definition",
                    "of",
                    "toxic",
                    "behavior",
                    "is",
                    "yet",
                    "to",
                    "emerge."
                ],
                [
                    "Some",
                    "studies",
                    "have",
                    "conducted",
                    "data",
                    "annotation",
                    "using",
                    "pre-defined",
                    "lexicon",
                    "categories",
                    "#REF",
                    "or",
                    "toxic",
                    "player",
                    "information",
                    "#REF",
                    "."
                ],
                [
                    "These",
                    "annotation",
                    "methods",
                    "are",
                    "not",
                    "robust",
                    "enough",
                    "to",
                    "handle",
                    "unlabelled",
                    "toxicity",
                    "words",
                    "or",
                    "unreported",
                    "toxic",
                    "players."
                ]
            ],
            "context": [
                1,
                2,
                2,
                2
            ]
        },
        "input": "sent0: Toxicity Datasets in Online Games In multiplayer online games, prior research focused on analysis of anti-social or disruptive behavior, socalled toxic behavior #TARGET_REF including cyberbullying #REF and griefing #REF .\n sent1: Although these terms contain similar elements, a single definition of toxic behavior is yet to emerge.\n sent2: Some studies have conducted data annotation using pre-defined lexicon categories #REF or toxic player information #REF .\n sent3: These annotation methods are not robust enough to handle unlabelled toxicity words or unreported toxic players.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [\"sent1\", \"sent2\", \"sent3\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "this",
                    "research",
                    "proposal,",
                    "we",
                    "focus",
                    "on",
                    "three",
                    "factors",
                    "that",
                    "can",
                    "enhance",
                    "the",
                    "communication",
                    "between",
                    "humans",
                    "and",
                    "assistive",
                    "technologies."
                ],
                [
                    "The",
                    "first",
                    "one",
                    "is",
                    "the",
                    "encoding",
                    "of",
                    "the",
                    "referential",
                    "complexity",
                    "of",
                    "the",
                    "situated",
                    "settings",
                    "while",
                    "creating",
                    "multimodal",
                    "embeddings."
                ],
                [
                    "As",
                    "pointed",
                    "out",
                    "in",
                    "#TARGET_REF",
                    ",",
                    "pre-trained",
                    "models,",
                    "that",
                    "were",
                    "created",
                    "by",
                    "fusing",
                    "the",
                    "modalities",
                    "without",
                    "constraints,",
                    "are",
                    "expected",
                    "to",
                    "be",
                    "an",
                    "out-of-the-box",
                    "solution",
                    "and",
                    "work",
                    "well",
                    "for",
                    "a",
                    "variety",
                    "of",
                    "simpler",
                    "tasks."
                ],
                [
                    "In",
                    "this",
                    "research,",
                    "we",
                    "propose",
                    "to",
                    "encode",
                    "referential",
                    "complexity",
                    "during",
                    "the",
                    "training",
                    "phase",
                    "to",
                    "see",
                    "whether",
                    "the",
                    "complexity-sensitive",
                    "embeddings",
                    "will",
                    "improve",
                    "the",
                    "tasks",
                    "of",
                    "crossmodal",
                    "mapping",
                    "and",
                    "meaning",
                    "recovery."
                ],
                [
                    "We",
                    "believe",
                    "that",
                    "this",
                    "will",
                    "implicitly",
                    "direct",
                    "the",
                    "model",
                    "to",
                    "focus",
                    "on",
                    "various",
                    "textual",
                    "and",
                    "visual",
                    "forms",
                    "of",
                    "the",
                    "same",
                    "concepts."
                ]
            ],
            "context": [
                0,
                3,
                1,
                0,
                0
            ]
        },
        "input": "sent0: In this research proposal, we focus on three factors that can enhance the communication between humans and assistive technologies.\n sent1: The first one is the encoding of the referential complexity of the situated settings while creating multimodal embeddings.\n sent2: As pointed out in #TARGET_REF , pre-trained models, that were created by fusing the modalities without constraints, are expected to be an out-of-the-box solution and work well for a variety of simpler tasks.\n sent3: In this research, we propose to encode referential complexity during the training phase to see whether the complexity-sensitive embeddings will improve the tasks of crossmodal mapping and meaning recovery.\n sent4: We believe that this will implicitly direct the model to focus on various textual and visual forms of the same concepts.\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Pruthi",
                    "et",
                    "al."
                ],
                [
                    "(",
                    "2020)",
                    "derived",
                    "an",
                    "occupation",
                    "dataset",
                    "to",
                    "study",
                    "the",
                    "gender",
                    "bias",
                    "in",
                    "NLP",
                    "classification",
                    "tasks."
                ],
                [
                    "The",
                    "task",
                    "is",
                    "framed",
                    "as",
                    "a",
                    "binary",
                    "classification",
                    "task",
                    "to",
                    "distinguish",
                    "between",
                    "\"surgeons\"",
                    "and",
                    "\"physicians\"."
                ],
                [
                    "These",
                    "two",
                    "occupations",
                    "are",
                    "chosen",
                    "because",
                    "they",
                    "share",
                    "similar",
                    "words",
                    "in",
                    "their",
                    "biographies",
                    "and",
                    "a",
                    "majority",
                    "of",
                    "surgeons",
                    "are",
                    "male."
                ],
                [
                    "The",
                    "dataset",
                    "is",
                    "further",
                    "tuned",
                    "-downsample",
                    "minority",
                    "classes",
                    "(female",
                    "surgeons",
                    "and",
                    "male",
                    "physicians)",
                    "by",
                    "a",
                    "factor",
                    "of",
                    "ten",
                    "to",
                    "encourage",
                    "the",
                    "model",
                    "to",
                    "rely",
                    "on",
                    "gendered",
                    "words",
                    "to",
                    "make",
                    "predictions."
                ],
                [
                    "#TARGET_REF",
                    "also",
                    "provides",
                    "a",
                    "pre-specified",
                    "list",
                    "of",
                    "impermissible",
                    "tokens",
                    "8",
                    "that",
                    "a",
                    "robust",
                    "model",
                    "should",
                    "assign",
                    "low",
                    "attention",
                    "scores",
                    "to."
                ],
                [
                    "We",
                    "instead",
                    "treat",
                    "this",
                    "list",
                    "of",
                    "tokens",
                    "as",
                    "shortcuts",
                    "and",
                    "analyze",
                    "the",
                    "efficacy",
                    "of",
                    "our",
                    "proposed",
                    "framework",
                    "on",
                    "identifying",
                    "these",
                    "tokens."
                ],
                [
                    "These",
                    "impermissible",
                    "tokens",
                    "can",
                    "be",
                    "regarded",
                    "as",
                    "shortcuts",
                    "because",
                    "they",
                    "only",
                    "reflect",
                    "the",
                    "gender",
                    "of",
                    "the",
                    "person,",
                    "thus",
                    "by",
                    "definition",
                    "should",
                    "not",
                    "affect",
                    "the",
                    "decision",
                    "of",
                    "a",
                    "occupation",
                    "classification",
                    "model."
                ],
                [
                    "Table",
                    "6",
                    "presents",
                    "the",
                    "result",
                    "on",
                    "identifying",
                    "the",
                    "list",
                    "of",
                    "impermissible",
                    "tokens."
                ],
                [
                    "Among",
                    "the",
                    "top",
                    "ten",
                    "tokens",
                    "selected",
                    "by",
                    "our",
                    "method,",
                    "6",
                    "of",
                    "them",
                    "are",
                    "shortcuts."
                ],
                [
                    "Furthermore,",
                    "9",
                    "out",
                    "of",
                    "12",
                    "impermissible",
                    "tokens",
                    "are",
                    "captured",
                    "in",
                    "the",
                    "top",
                    "50",
                    "tokens",
                    "selected",
                    "by",
                    "our",
                    "method."
                ],
                [
                    "This",
                    "further",
                    "demonstrates",
                    "that",
                    "our",
                    "method",
                    "can",
                    "effectively",
                    "find",
                    "shortcuts",
                    "in",
                    "this",
                    "occupation",
                    "classification",
                    "task,",
                    "in",
                    "a",
                    "more",
                    "automated",
                    "way",
                    "compared",
                    "to",
                    "existing",
                    "approaches",
                    "that",
                    "rely",
                    "on",
                    "pre-defined",
                    "lists."
                ]
            ],
            "context": [
                3,
                3,
                3,
                3,
                3,
                1,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Pruthi et al.\n sent1: ( 2020) derived an occupation dataset to study the gender bias in NLP classification tasks.\n sent2: The task is framed as a binary classification task to distinguish between \"surgeons\" and \"physicians\".\n sent3: These two occupations are chosen because they share similar words in their biographies and a majority of surgeons are male.\n sent4: The dataset is further tuned -downsample minority classes (female surgeons and male physicians) by a factor of ten to encourage the model to rely on gendered words to make predictions.\n sent5: #TARGET_REF also provides a pre-specified list of impermissible tokens 8 that a robust model should assign low attention scores to.\n sent6: We instead treat this list of tokens as shortcuts and analyze the efficacy of our proposed framework on identifying these tokens.\n sent7: These impermissible tokens can be regarded as shortcuts because they only reflect the gender of the person, thus by definition should not affect the decision of a occupation classification model.\n sent8: Table 6 presents the result on identifying the list of impermissible tokens.\n sent9: Among the top ten tokens selected by our method, 6 of them are shortcuts.\n sent10: Furthermore, 9 out of 12 impermissible tokens are captured in the top 50 tokens selected by our method.\n sent11: This further demonstrates that our method can effectively find shortcuts in this occupation classification task, in a more automated way compared to existing approaches that rely on pre-defined lists.\n",
        "output": "{\"INFORMATION\": [\"sent5\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\", \"sent1\", \"sent2\", \"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Translation-based",
                    "features",
                    "Capturing",
                    "semantic",
                    "relationships",
                    "between",
                    "a",
                    "query",
                    "and",
                    "a",
                    "document",
                    "is",
                    "also",
                    "crucial",
                    "to",
                    "improving",
                    "retrieval",
                    "accuracy."
                ],
                [
                    "To",
                    "incorporate",
                    "such",
                    "features,",
                    "we",
                    "can",
                    "use",
                    "a",
                    "translation",
                    "model",
                    "#TARGET_REF",
                    "to",
                    "measure",
                    "the",
                    "log",
                    "translation",
                    "probability",
                    "between",
                    "queries",
                    "and",
                    "documents."
                ],
                [
                    "The",
                    "conditional",
                    "probability",
                    "we",
                    "need",
                    "p(q|d",
                    "n",
                    ")",
                    "is",
                    "generated",
                    "by",
                    "the",
                    "IBM",
                    "Model",
                    "1",
                    "translation",
                    "model,",
                    "and",
                    "the",
                    "final",
                    "query-document",
                    "feature",
                    "is",
                    "the",
                    "sum",
                    "of",
                    "all",
                    "individual",
                    "conditional",
                    "query",
                    "probabilities."
                ]
            ],
            "context": [
                3,
                2,
                3
            ]
        },
        "input": "sent0: Translation-based features Capturing semantic relationships between a query and a document is also crucial to improving retrieval accuracy.\n sent1: To incorporate such features, we can use a translation model #TARGET_REF to measure the log translation probability between queries and documents.\n sent2: The conditional probability we need p(q|d n ) is generated by the IBM Model 1 translation model, and the final query-document feature is the sum of all individual conditional query probabilities.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent0\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Task",
                    "Definition",
                    "The",
                    "Text-based",
                    "NP",
                    "Enrichment",
                    "task",
                    "is",
                    "deceptively",
                    "simple:",
                    "For",
                    "each",
                    "ordered",
                    "pair",
                    "(n",
                    "1",
                    ",",
                    "n",
                    "2",
                    ")",
                    "of",
                    "non-pronominal",
                    "base-NP",
                    "3",
                    "spans",
                    "in",
                    "an",
                    "input",
                    "text,",
                    "determine",
                    "if",
                    "there",
                    "exists",
                    "a",
                    "preposition-mediated",
                    "relation",
                    "between",
                    "n",
                    "1",
                    "and",
                    "n",
                    "2",
                    ",",
                    "and",
                    "if",
                    "there",
                    "is",
                    "one,",
                    "determine",
                    "the",
                    "preposition",
                    "that",
                    "best",
                    "describes",
                    "their",
                    "relation."
                ],
                [
                    "4",
                    "The",
                    "output",
                    "is",
                    "a",
                    "list",
                    "3",
                    "We",
                    "follow",
                    "the",
                    "definition",
                    "of",
                    "Base-NPs",
                    "as",
                    "defined",
                    "by",
                    "#TARGET_REF",
                    ":",
                    "initial",
                    "portions",
                    "of",
                    "non-recursive",
                    "noun-phrases,",
                    "including",
                    "pre-modifiers",
                    "such",
                    "as",
                    "determiners,",
                    "adjectives",
                    "and",
                    "noun-compounds,",
                    "but",
                    "not",
                    "including",
                    "post-modifiers",
                    "such",
                    "as",
                    "prepositional",
                    "phrases",
                    "and",
                    "clauses."
                ],
                [
                    "These",
                    "are",
                    "also",
                    "known",
                    "in",
                    "the",
                    "NLP",
                    "literature",
                    "as",
                    "''NP",
                    "Chunks''."
                ],
                [
                    "4",
                    "During",
                    "annotation,",
                    "we",
                    "noticed",
                    "that",
                    "annotators",
                    "often",
                    "tried",
                    "to",
                    "express",
                    "set-membership",
                    "using",
                    "prepositions,",
                    "which",
                    "resulted",
                    "in",
                    "awkward",
                    "and",
                    "unclear",
                    "annotations."
                ],
                [
                    "To",
                    "remedy",
                    "this,",
                    "we",
                    "found",
                    "it",
                    "effective",
                    "to",
                    "add",
                    "an",
                    "explicit",
                    "''member-of''",
                    "relation",
                    "as",
                    "an",
                    "allowed",
                    "annotation",
                    "option."
                ],
                [
                    "This",
                    "significantly",
                    "reduced",
                    "of",
                    "tuples",
                    "of",
                    "the",
                    "form",
                    "(n",
                    "i",
                    ",",
                    "prep,",
                    "n",
                    "j",
                    "),",
                    "where",
                    "n",
                    "i",
                    "is",
                    "called",
                    "the",
                    "anchor",
                    "and",
                    "n",
                    "j",
                    "is",
                    "called",
                    "the",
                    "complement",
                    "of",
                    "the",
                    "relation."
                ],
                [
                    "Figure",
                    "2",
                    "shows",
                    "an",
                    "example",
                    "of",
                    "text",
                    "where",
                    "each",
                    "NP",
                    "n",
                    "1",
                    "is",
                    "annotated",
                    "with",
                    "its",
                    "(prep,",
                    "n",
                    "2",
                    ")",
                    "NP-enrichments."
                ]
            ],
            "context": [
                0,
                1,
                2,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Task Definition The Text-based NP Enrichment task is deceptively simple: For each ordered pair (n 1 , n 2 ) of non-pronominal base-NP 3 spans in an input text, determine if there exists a preposition-mediated relation between n 1 and n 2 , and if there is one, determine the preposition that best describes their relation.\n sent1: 4 The output is a list 3 We follow the definition of Base-NPs as defined by #TARGET_REF : initial portions of non-recursive noun-phrases, including pre-modifiers such as determiners, adjectives and noun-compounds, but not including post-modifiers such as prepositional phrases and clauses.\n sent2: These are also known in the NLP literature as ''NP Chunks''.\n sent3: 4 During annotation, we noticed that annotators often tried to express set-membership using prepositions, which resulted in awkward and unclear annotations.\n sent4: To remedy this, we found it effective to add an explicit ''member-of'' relation as an allowed annotation option.\n sent5: This significantly reduced of tuples of the form (n i , prep, n j ), where n i is called the anchor and n j is called the complement of the relation.\n sent6: Figure 2 shows an example of text where each NP n 1 is annotated with its (prep, n 2 ) NP-enrichments.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "second",
                    "dataset",
                    "is",
                    "Debatepedia",
                    "arguments",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "A",
                    "total",
                    "of",
                    "508",
                    "topics",
                    "are",
                    "paired",
                    "with",
                    "15K",
                    "pro",
                    "and",
                    "con",
                    "responses,",
                    "and",
                    "we",
                    "treat",
                    "each",
                    "pair",
                    "as",
                    "an",
                    "argument",
                    "and",
                    "each",
                    "topic",
                    "and",
                    "response",
                    "as",
                    "claim",
                    "and",
                    "statement,",
                    "respectively."
                ]
            ],
            "context": [
                2,
                2
            ]
        },
        "input": "sent0: The second dataset is Debatepedia arguments #TARGET_REF .\n sent1: A total of 508 topics are paired with 15K pro and con responses, and we treat each pair as an argument and each topic and response as claim and statement, respectively.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\", \"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "middle",
                    "layers",
                    "in",
                    "BERT",
                    "have",
                    "been",
                    "shown",
                    "to",
                    "have",
                    "specific",
                    "characteristics",
                    "of",
                    "higher",
                    "attention",
                    "entropy",
                    "and",
                    "greater",
                    "attention",
                    "to",
                    "specific",
                    "tokens",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "We",
                    "thus",
                    "considered",
                    "configurations",
                    "where",
                    "we",
                    "compare",
                    "pruning",
                    "top",
                    "and",
                    "bottom",
                    "layers",
                    "against",
                    "pruning",
                    "middle",
                    "layers",
                    "(last",
                    "eight",
                    "rows",
                    "of",
                    "Table",
                    "5)."
                ],
                [
                    "The",
                    "results",
                    "indicate",
                    "a",
                    "clear",
                    "preference:",
                    "In",
                    "14",
                    "out",
                    "of",
                    "16",
                    "cases,",
                    "pruning",
                    "the",
                    "middle",
                    "layers",
                    "performs",
                    "worse",
                    "that",
                    "pruning",
                    "equal",
                    "number",
                    "of",
                    "layers",
                    "distributed",
                    "among",
                    "top/bottom",
                    "layers."
                ],
                [
                    "Indeed,",
                    "we",
                    "incur",
                    "an",
                    "additional",
                    "over",
                    "2%",
                    "average",
                    "drop",
                    "in",
                    "accuracy",
                    "for",
                    "QNLI",
                    "and",
                    "SST-2",
                    "tasks,",
                    "indicating",
                    "a",
                    "task-specific",
                    "sensitivity",
                    "to",
                    "pruning",
                    "middle",
                    "layers."
                ]
            ],
            "context": [
                2,
                0,
                0,
                0
            ]
        },
        "input": "sent0: The middle layers in BERT have been shown to have specific characteristics of higher attention entropy and greater attention to specific tokens #TARGET_REF .\n sent1: We thus considered configurations where we compare pruning top and bottom layers against pruning middle layers (last eight rows of Table 5).\n sent2: The results indicate a clear preference: In 14 out of 16 cases, pruning the middle layers performs worse that pruning equal number of layers distributed among top/bottom layers.\n sent3: Indeed, we incur an additional over 2% average drop in accuracy for QNLI and SST-2 tasks, indicating a task-specific sensitivity to pruning middle layers.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Linguistic",
                    "Embedding",
                    "For",
                    "the",
                    "language",
                    "D,",
                    "we",
                    "first",
                    "tokenize",
                    "the",
                    "sentence",
                    "into",
                    "a",
                    "sequence",
                    "of",
                    "word",
                    "tokens",
                    "using",
                    "WordPiece",
                    "#TARGET_REF",
                    ",",
                    "then",
                    "encode",
                    "them",
                    "into",
                    "word",
                    "embeddings",
                    "W",
                    "=",
                    "{w",
                    "j",
                    "}",
                    "T",
                    "j=1",
                    "where",
                    "w",
                    "j",
                    "∈",
                    "R",
                    "dw",
                    "is",
                    "the",
                    "feature",
                    "vector."
                ],
                [
                    "Similarly,",
                    "an",
                    "index",
                    "position",
                    "#REF",
                    "embedding",
                    "is",
                    "supplemented",
                    "to",
                    "each",
                    "word",
                    "embedding."
                ]
            ],
            "context": [
                2,
                0
            ]
        },
        "input": "sent0: Linguistic Embedding For the language D, we first tokenize the sentence into a sequence of word tokens using WordPiece #TARGET_REF , then encode them into word embeddings W = {w j } T j=1 where w j ∈ R dw is the feature vector.\n sent1: Similarly, an index position #REF embedding is supplemented to each word embedding.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "use",
                    "the",
                    "Chinese",
                    "sentences",
                    "as",
                    "system",
                    "input",
                    "and",
                    "their",
                    "corresponding",
                    "English",
                    "translations",
                    "as",
                    "the",
                    "reference",
                    "translations."
                ],
                [
                    "We",
                    "use",
                    "the",
                    "open",
                    "source",
                    "statistical",
                    "machine",
                    "translation",
                    "decoder",
                    "Moses",
                    "(?)"
                ],
                [
                    "for",
                    "the",
                    "experiments,",
                    "translating",
                    "the",
                    "PropBank",
                    "Chinese",
                    "sentences",
                    "into",
                    "English",
                    "with",
                    "the",
                    "same",
                    "model",
                    "trained",
                    "for",
                    "our",
                    "participation",
                    "in",
                    "the",
                    "IWSLT",
                    "2007",
                    "evaluation",
                    "campaign",
                    "#TARGET_REF",
                    ".The",
                    "English",
                    "translations",
                    "generated",
                    "by",
                    "the",
                    "decoder",
                    "are",
                    "the",
                    "system",
                    "output."
                ],
                [
                    "Based",
                    "on",
                    "the",
                    "system",
                    "input",
                    "and",
                    "the",
                    "reference",
                    "We",
                    "first",
                    "randomly",
                    "select",
                    "50",
                    "bi-sentences,",
                    "without",
                    "any",
                    "constraint",
                    "on",
                    "the",
                    "translation",
                    "accuracy",
                    "of",
                    "the",
                    "predicate",
                    "verbs,",
                    "to",
                    "form",
                    "the",
                    "first",
                    "observation",
                    "data",
                    "set",
                    "(data",
                    "set",
                    "A)."
                ]
            ],
            "context": [
                0,
                2,
                2,
                0
            ]
        },
        "input": "sent0: We use the Chinese sentences as system input and their corresponding English translations as the reference translations.\n sent1: We use the open source statistical machine translation decoder Moses (?)\n sent2: for the experiments, translating the PropBank Chinese sentences into English with the same model trained for our participation in the IWSLT 2007 evaluation campaign #TARGET_REF .The English translations generated by the decoder are the system output.\n sent3: Based on the system input and the reference We first randomly select 50 bi-sentences, without any constraint on the translation accuracy of the predicate verbs, to form the first observation data set (data set A).\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\", \"sent2\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Attention-guided",
                    "Grounding",
                    "A",
                    "cross-attention",
                    "matrix",
                    "is",
                    "generated",
                    "in",
                    "shape",
                    "(N,",
                    "T,",
                    "L,",
                    "H)",
                    "during",
                    "the",
                    "transformer's",
                    "decoding",
                    "steps."
                ],
                [
                    "Here",
                    "N",
                    "denotes",
                    "the",
                    "number",
                    "of",
                    "pre-detected",
                    "visual",
                    "objects,",
                    "T",
                    "denotes",
                    "the",
                    "number",
                    "of",
                    "tokens",
                    "in",
                    "a",
                    "caption",
                    "sentence",
                    "after",
                    "padding,",
                    "L",
                    "denotes",
                    "the",
                    "number",
                    "of",
                    "transformer",
                    "layers,",
                    "and",
                    "H",
                    "denotes",
                    "the",
                    "number",
                    "of",
                    "attention",
                    "heads",
                    "in",
                    "transformer",
                    "layers."
                ],
                [
                    "Two",
                    "linear",
                    "projections",
                    "and",
                    "layer",
                    "normalization",
                    "#TARGET_REF",
                    "are",
                    "applied",
                    "sequentially",
                    "on",
                    "dimension",
                    "L",
                    "and",
                    "H,",
                    "respectively",
                    "reducing",
                    "the",
                    "dimension",
                    "to",
                    "1."
                ],
                [
                    "Thus,",
                    "for",
                    "a",
                    "single",
                    "instance,",
                    "we",
                    "eventually",
                    "calculate",
                    "an",
                    "attention",
                    "matrix",
                    "A",
                    "∈",
                    "R",
                    "N",
                    "×T",
                    "."
                ]
            ],
            "context": [
                0,
                0,
                2,
                2
            ]
        },
        "input": "sent0: Attention-guided Grounding A cross-attention matrix is generated in shape (N, T, L, H) during the transformer's decoding steps.\n sent1: Here N denotes the number of pre-detected visual objects, T denotes the number of tokens in a caption sentence after padding, L denotes the number of transformer layers, and H denotes the number of attention heads in transformer layers.\n sent2: Two linear projections and layer normalization #TARGET_REF are applied sequentially on dimension L and H, respectively reducing the dimension to 1.\n sent3: Thus, for a single instance, we eventually calculate an attention matrix A ∈ R N ×T .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent2\", \"sent3\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "First,",
                    "we",
                    "observe",
                    "from",
                    "Table",
                    "2",
                    "that",
                    "just",
                    "having",
                    "intermediate",
                    "SQuAD",
                    "pre-training",
                    "in",
                    "English,",
                    "improves",
                    "the",
                    "overall",
                    "Jaccard",
                    "score",
                    "significantly",
                    "from",
                    "0.44",
                    "to",
                    "0.5."
                ],
                [
                    "Furthermore,",
                    "we",
                    "fine-tune",
                    "by",
                    "dividing",
                    "translated",
                    "and",
                    "transliterated",
                    "data",
                    "into",
                    "Indo-Aryan",
                    "and",
                    "Dravidian",
                    "language",
                    "families",
                    "to",
                    "study",
                    "how",
                    "translated",
                    "and",
                    "transliterated",
                    "pairs",
                    "serve",
                    "as",
                    "supervised",
                    "cross-lingual",
                    "signals",
                    "when",
                    "languages",
                    "share",
                    "semantics",
                    "and",
                    "structure",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Although",
                    "transliteration",
                    "improves",
                    "the",
                    "Jaccard",
                    "scores",
                    "in",
                    "certain",
                    "cases",
                    "compared",
                    "to",
                    "the",
                    "baseline,",
                    "the",
                    "trend",
                    "is",
                    "not",
                    "consistent."
                ],
                [
                    "Moreover,",
                    "contrastive",
                    "training",
                    "does",
                    "not",
                    "help",
                    "in",
                    "the",
                    "case",
                    "of",
                    "transliteration",
                    "as",
                    "shown",
                    "in",
                    "Table",
                    "3."
                ],
                [
                    "This",
                    "could",
                    "be",
                    "because",
                    "the",
                    "QA",
                    "model",
                    "is",
                    "pre-trained",
                    "only",
                    "with",
                    "regular",
                    "text",
                    "and",
                    "not",
                    "with",
                    "transliteration",
                    "style",
                    "text."
                ]
            ],
            "context": [
                0,
                2,
                0,
                0,
                0
            ]
        },
        "input": "sent0: First, we observe from Table 2 that just having intermediate SQuAD pre-training in English, improves the overall Jaccard score significantly from 0.44 to 0.5.\n sent1: Furthermore, we fine-tune by dividing translated and transliterated data into Indo-Aryan and Dravidian language families to study how translated and transliterated pairs serve as supervised cross-lingual signals when languages share semantics and structure #TARGET_REF .\n sent2: Although transliteration improves the Jaccard scores in certain cases compared to the baseline, the trend is not consistent.\n sent3: Moreover, contrastive training does not help in the case of transliteration as shown in Table 3.\n sent4: This could be because the QA model is pre-trained only with regular text and not with transliteration style text.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Paraphrase",
                    "Here",
                    "we",
                    "use",
                    "an",
                    "example,",
                    "(4),",
                    "from",
                    "#TARGET_REF",
                    ",",
                    "where",
                    "paraphrases",
                    "are",
                    "represented",
                    "by",
                    "pairing",
                    "TAG",
                    "derivation",
                    "trees",
                    "(Figure",
                    "5)."
                ],
                [
                    "This",
                    "is",
                    "again",
                    "similar",
                    "to",
                    "the",
                    "previous",
                    "MT",
                    "examples:",
                    "in",
                    "order",
                    "to",
                    "define",
                    "a",
                    "paraphrase",
                    "where",
                    "the",
                    "most",
                    "embedded",
                    "clause",
                    "becomes",
                    "a",
                    "separate",
                    "sentence,",
                    "it",
                    "is",
                    "necessary",
                    "to",
                    "form",
                    "a",
                    "gNCN",
                    "(those",
                    "nodes",
                    "in",
                    "bold",
                    "in",
                    "Figure",
                    "5)."
                ]
            ],
            "context": [
                1,
                0
            ]
        },
        "input": "sent0: Paraphrase Here we use an example, (4), from #TARGET_REF , where paraphrases are represented by pairing TAG derivation trees (Figure 5).\n sent1: This is again similar to the previous MT examples: in order to define a paraphrase where the most embedded clause becomes a separate sentence, it is necessary to form a gNCN (those nodes in bold in Figure 5).\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "There",
                    "have",
                    "been",
                    "a",
                    "series",
                    "of",
                    "attempts",
                    "to",
                    "utilize",
                    "phonetic",
                    "representations",
                    "of",
                    "language",
                    "to",
                    "improve",
                    "or",
                    "extend",
                    "automatic",
                    "speech",
                    "recognition",
                    "(ASR)",
                    "models."
                ],
                [
                    "Some",
                    "of",
                    "these",
                    "jointly",
                    "model",
                    "text",
                    "and",
                    "audio",
                    "data",
                    "using",
                    "sequences",
                    "of",
                    "phonemes",
                    "combined",
                    "with",
                    "sequences",
                    "of",
                    "text",
                    "characters."
                ],
                [
                    "#REF",
                    ",",
                    "for",
                    "example,",
                    "uses",
                    "a",
                    "joint",
                    "transformer",
                    "architecture",
                    "that",
                    "encodes",
                    "sequences",
                    "of",
                    "phonemes",
                    "and",
                    "sequences",
                    "of",
                    "text",
                    "simultaneously."
                ],
                [
                    "However,",
                    "this",
                    "joint",
                    "model",
                    "is",
                    "utilized",
                    "to",
                    "learn",
                    "representations",
                    "that",
                    "are",
                    "more",
                    "robust",
                    "to",
                    "transcription",
                    "errors."
                ],
                [
                    "The",
                    "architecture",
                    "still",
                    "requires",
                    "text",
                    "inputs",
                    "(from",
                    "ASR",
                    "transcriptions)",
                    "and",
                    "generates",
                    "outputs",
                    "in",
                    "both",
                    "text",
                    "and",
                    "phoneme",
                    "representations."
                ],
                [
                    "In",
                    "contrast,",
                    "our",
                    "approach",
                    "allows",
                    "for",
                    "text",
                    "input,",
                    "audio",
                    "input,",
                    "or",
                    "text",
                    "plus",
                    "audio",
                    "input",
                    "to",
                    "language",
                    "models."
                ],
                [
                    "Similarly,",
                    "in",
                    "#REF",
                    "and",
                    "#TARGET_REF",
                    "investigate",
                    "the",
                    "potential",
                    "of",
                    "phoneme-based",
                    "or",
                    "phoneme",
                    "aware",
                    "representations",
                    "and",
                    "models,",
                    "showing",
                    "gains",
                    "in",
                    "performance,",
                    "language",
                    "transfer,",
                    "and",
                    "flexibility",
                    "across",
                    "written",
                    "scripts."
                ],
                [
                    "These",
                    "works",
                    "conduct",
                    "training",
                    "on",
                    "text-based",
                    "data",
                    "only,",
                    "using",
                    "Epitran",
                    "to",
                    "convert",
                    "to",
                    "phonemes."
                ],
                [
                    "#REF",
                    "transforms",
                    "unlabeled",
                    "text",
                    "(i.e.,",
                    "not",
                    "aligned",
                    "with",
                    "corresponding",
                    "audio",
                    "files)",
                    "into",
                    "phonemes",
                    "in",
                    "a",
                    "scheme",
                    "to",
                    "train",
                    "speech",
                    "recognition",
                    "models",
                    "without",
                    "any",
                    "labeled",
                    "data."
                ],
                [
                    "This",
                    "scheme",
                    "involves",
                    "a",
                    "generator",
                    "model",
                    "trained",
                    "jointly",
                    "with",
                    "a",
                    "discriminator",
                    "model."
                ],
                [
                    "The",
                    "generator",
                    "model",
                    "converts",
                    "audio,",
                    "segmented",
                    "into",
                    "phonetic",
                    "units",
                    "into",
                    "predicted",
                    "phonemes,",
                    "and",
                    "the",
                    "discriminator",
                    "model",
                    "attempts",
                    "to",
                    "discriminate",
                    "between",
                    "these",
                    "predicted",
                    "phonemes",
                    "and",
                    "the",
                    "phonemes",
                    "transliterated",
                    "from",
                    "unlabeled",
                    "text."
                ],
                [
                    "Although",
                    "both",
                    "text",
                    "and",
                    "audio",
                    "are",
                    "utilized",
                    "in",
                    "this",
                    "work,",
                    "they",
                    "are",
                    "not",
                    "input",
                    "to",
                    "the",
                    "same",
                    "model",
                    "and",
                    "the",
                    "primary",
                    "output",
                    "of",
                    "the",
                    "training",
                    "scheme",
                    "is",
                    "a",
                    "model",
                    "that",
                    "creates",
                    "good",
                    "phonetic",
                    "speech",
                    "representations",
                    "from",
                    "input",
                    "audio."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                1,
                3,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: There have been a series of attempts to utilize phonetic representations of language to improve or extend automatic speech recognition (ASR) models.\n sent1: Some of these jointly model text and audio data using sequences of phonemes combined with sequences of text characters.\n sent2: #REF , for example, uses a joint transformer architecture that encodes sequences of phonemes and sequences of text simultaneously.\n sent3: However, this joint model is utilized to learn representations that are more robust to transcription errors.\n sent4: The architecture still requires text inputs (from ASR transcriptions) and generates outputs in both text and phoneme representations.\n sent5: In contrast, our approach allows for text input, audio input, or text plus audio input to language models.\n sent6: Similarly, in #REF and #TARGET_REF investigate the potential of phoneme-based or phoneme aware representations and models, showing gains in performance, language transfer, and flexibility across written scripts.\n sent7: These works conduct training on text-based data only, using Epitran to convert to phonemes.\n sent8: #REF transforms unlabeled text (i.e., not aligned with corresponding audio files) into phonemes in a scheme to train speech recognition models without any labeled data.\n sent9: This scheme involves a generator model trained jointly with a discriminator model.\n sent10: The generator model converts audio, segmented into phonetic units into predicted phonemes, and the discriminator model attempts to discriminate between these predicted phonemes and the phonemes transliterated from unlabeled text.\n sent11: Although both text and audio are utilized in this work, they are not input to the same model and the primary output of the training scheme is a model that creates good phonetic speech representations from input audio.\n",
        "output": "{\"INFORMATION\": [\"sent6\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent7\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "To",
                    "reduce",
                    "the",
                    "deviation",
                    "caused",
                    "by",
                    "different",
                    "implementation",
                    "details,",
                    "we",
                    "first",
                    "present",
                    "our",
                    "implementations'",
                    "performance",
                    "(with",
                    "*),",
                    "which",
                    "have",
                    "a",
                    "higher",
                    "score",
                    "than",
                    "#TARGET_REF",
                    "reported."
                ],
                [
                    "Thus,",
                    "we",
                    "have",
                    "a",
                    "more",
                    "strict",
                    "baseline",
                    "to",
                    "evaluate",
                    "the",
                    "improvement",
                    "purely",
                    "coming",
                    "from",
                    "our",
                    "innovative",
                    "method."
                ],
                [
                    "Compared",
                    "to",
                    "Baseline*",
                    "method,",
                    "the",
                    "performance",
                    "on",
                    "all",
                    "metrics",
                    "improves",
                    "significantly",
                    "when",
                    "controlling",
                    "captioning",
                    "using",
                    "the",
                    "mouse",
                    "trace",
                    "(+Trace*),",
                    "it",
                    "indicates",
                    "that",
                    "using",
                    "the",
                    "mouse",
                    "trace",
                    "enables",
                    "the",
                    "system",
                    "to",
                    "describe",
                    "better",
                    "those",
                    "user",
                    "intended",
                    "parts",
                    "of",
                    "the",
                    "image."
                ]
            ],
            "context": [
                2,
                2,
                0
            ]
        },
        "input": "sent0: To reduce the deviation caused by different implementation details, we first present our implementations' performance (with *), which have a higher score than #TARGET_REF reported.\n sent1: Thus, we have a more strict baseline to evaluate the improvement purely coming from our innovative method.\n sent2: Compared to Baseline* method, the performance on all metrics improves significantly when controlling captioning using the mouse trace (+Trace*), it indicates that using the mouse trace enables the system to describe better those user intended parts of the image.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\", \"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "improves",
                    "the",
                    "performance",
                    "on",
                    "some",
                    "coreferencerelated",
                    "datasets",
                    "#REF",
                    "."
                ],
                [
                    "There",
                    "are",
                    "also",
                    "various",
                    "datasets",
                    "for",
                    "the",
                    "task",
                    "of",
                    "reading",
                    "comprehension",
                    "on",
                    "which",
                    "the",
                    "model",
                    "requires",
                    "to",
                    "perform",
                    "coreference",
                    "reasoning",
                    "to",
                    "answer",
                    "some",
                    "of",
                    "the",
                    "questions,",
                    "e.g.,",
                    "DROP",
                    "#REF",
                    ",",
                    "DuoRC",
                    "#REF",
                    ",",
                    "MultiRC",
                    "#TARGET_REF",
                    ",",
                    "etc."
                ]
            ],
            "context": [
                3,
                2
            ]
        },
        "input": "sent0: improves the performance on some coreferencerelated datasets #REF .\n sent1: There are also various datasets for the task of reading comprehension on which the model requires to perform coreference reasoning to answer some of the questions, e.g., DROP #REF , DuoRC #REF , MultiRC #TARGET_REF , etc.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "recent",
                    "years",
                    "there",
                    "has",
                    "been",
                    "some",
                    "revival",
                    "of",
                    "interest",
                    "in",
                    "computational",
                    "lexicography",
                    "that",
                    "has",
                    "fulfilled",
                    "some",
                    "of",
                    "MMB's",
                    "hopes",
                    "and",
                    "dreams."
                ],
                [
                    "It",
                    "has",
                    "been",
                    "driven",
                    "to",
                    "some",
                    "extent",
                    "by",
                    "the",
                    "availability",
                    "from",
                    "publishers",
                    "of",
                    "machine-readable",
                    "English",
                    "dictionaries,",
                    "such",
                    "as",
                    "Longman's",
                    "dictionary",
                    "of",
                    "contemporary",
                    "English",
                    "(LDOCE)",
                    "and",
                    "Collins-Birmingham",
                    "University",
                    "International",
                    "Language",
                    "Database",
                    "(COBUILD),",
                    "with",
                    "definitions",
                    "written",
                    "in",
                    "a",
                    "semi-formal",
                    "way."
                ],
                [
                    "This",
                    "makes",
                    "it",
                    "much",
                    "easier",
                    "for",
                    "a",
                    "computational",
                    "parser",
                    "to",
                    "extract",
                    "information",
                    "from",
                    "them."
                ],
                [
                    "But",
                    "the",
                    "initial",
                    "work",
                    "in",
                    "the",
                    "current",
                    "wave",
                    "was",
                    "done",
                    "by",
                    "#TARGET_REF",
                    "at",
                    "Texas",
                    "using",
                    "Webster's,",
                    "an",
                    "oldfashioned",
                    "dinosaur",
                    "of",
                    "a",
                    "dictionary."
                ],
                [
                    "He",
                    "developed",
                    "a",
                    "notion",
                    "of",
                    "'tangled",
                    "hierarchies'",
                    "which",
                    "captures",
                    "the",
                    "notion",
                    "MMB",
                    "promoted",
                    "so",
                    "as",
                    "to",
                    "get",
                    "away",
                    "from",
                    "straightforward",
                    "tree-like",
                    "hierarchies."
                ]
            ],
            "context": [
                3,
                0,
                0,
                2,
                1
            ]
        },
        "input": "sent0: In recent years there has been some revival of interest in computational lexicography that has fulfilled some of MMB's hopes and dreams.\n sent1: It has been driven to some extent by the availability from publishers of machine-readable English dictionaries, such as Longman's dictionary of contemporary English (LDOCE) and Collins-Birmingham University International Language Database (COBUILD), with definitions written in a semi-formal way.\n sent2: This makes it much easier for a computational parser to extract information from them.\n sent3: But the initial work in the current wave was done by #TARGET_REF at Texas using Webster's, an oldfashioned dinosaur of a dictionary.\n sent4: He developed a notion of 'tangled hierarchies' which captures the notion MMB promoted so as to get away from straightforward tree-like hierarchies.\n",
        "output": "{\"INFORMATION\": [\"sent4\"], \"PERCEPTION\": [\"sent3\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "BERT-based",
                    "models",
                    "have",
                    "the",
                    "advantage",
                    "that",
                    "we",
                    "can",
                    "directly",
                    "use",
                    "the",
                    "attention",
                    "scores",
                    "as",
                    "explanations",
                    "of",
                    "model",
                    "decisions."
                ],
                [
                    "For",
                    "models",
                    "with",
                    "other",
                    "architectures,",
                    "we",
                    "can",
                    "use",
                    "explanation",
                    "techniques",
                    "such",
                    "as",
                    "LIME",
                    "#REF",
                    "or",
                    "Path",
                    "Integrated",
                    "Gradient",
                    "approaches",
                    "#TARGET_REF",
                    "to",
                    "provide",
                    "explanations."
                ]
            ],
            "context": [
                3,
                2
            ]
        },
        "input": "sent0: BERT-based models have the advantage that we can directly use the attention scores as explanations of model decisions.\n sent1: For models with other architectures, we can use explanation techniques such as LIME #REF or Path Integrated Gradient approaches #TARGET_REF to provide explanations.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Communities."
                ],
                [
                    "There",
                    "can",
                    "be",
                    "scenarios",
                    "where",
                    "whole",
                    "communities",
                    "of",
                    "users",
                    "on",
                    "a",
                    "platform",
                    "may",
                    "be",
                    "indulging",
                    "in",
                    "abusive",
                    "behavior,",
                    "e.g.,",
                    "by",
                    "widely",
                    "circulating",
                    "an",
                    "abusive",
                    "view",
                    "against",
                    "a",
                    "demographic",
                    "group",
                    "based",
                    "on",
                    "shared",
                    "beliefs,",
                    "common",
                    "stereotypes",
                    "or",
                    "other",
                    "homophilic",
                    "ties."
                ],
                [
                    "In",
                    "such",
                    "cases,",
                    "just",
                    "taking",
                    "down",
                    "specific",
                    "instances",
                    "of",
                    "abusive",
                    "language",
                    "and",
                    "providing",
                    "justifications",
                    "individually",
                    "to",
                    "the",
                    "respective",
                    "users",
                    "may",
                    "not",
                    "prove",
                    "effective."
                ],
                [
                    "Users",
                    "may",
                    "continue",
                    "to",
                    "promote",
                    "the",
                    "abusive",
                    "view,",
                    "defying",
                    "the",
                    "norms",
                    "of",
                    "the",
                    "platform",
                    "in",
                    "the",
                    "process",
                    "and",
                    "ignoring",
                    "the",
                    "justifications",
                    "given",
                    "to",
                    "them."
                ],
                [
                    "The",
                    "reason",
                    "for",
                    "this",
                    "comes",
                    "from",
                    "social",
                    "influence",
                    "theory",
                    "which",
                    "says",
                    "that",
                    "a",
                    "user's",
                    "behavior",
                    "is",
                    "affected",
                    "by",
                    "three",
                    "broad",
                    "varieties",
                    "of",
                    "social",
                    "influence",
                    "#REF",
                    ",",
                    "i.e.,",
                    "compliance,",
                    "identification,",
                    "and",
                    "internalization."
                ],
                [
                    "Compliance",
                    "occurs",
                    "when",
                    "the",
                    "user",
                    "behaves",
                    "a",
                    "certain",
                    "way",
                    "so",
                    "as",
                    "to",
                    "appear",
                    "in",
                    "congruence",
                    "with",
                    "opinions",
                    "of",
                    "others",
                    "who",
                    "matter",
                    "to",
                    "them,",
                    "identification",
                    "occurs",
                    "when",
                    "the",
                    "user",
                    "adopts",
                    "behaviors",
                    "in",
                    "order",
                    "to",
                    "associate",
                    "with",
                    "others",
                    "they",
                    "admire,",
                    "and",
                    "internalization",
                    "is",
                    "when",
                    "the",
                    "user",
                    "adopts",
                    "the",
                    "values",
                    "and",
                    "beliefs",
                    "of",
                    "others."
                ],
                [
                    "The",
                    "influences",
                    "occur",
                    "because",
                    "of",
                    "two",
                    "needs",
                    "of",
                    "the",
                    "user,",
                    "the",
                    "need",
                    "to",
                    "be",
                    "liked",
                    "(normative)",
                    "and",
                    "the",
                    "need",
                    "to",
                    "be",
                    "right",
                    "(informational)."
                ],
                [
                    "In",
                    "order",
                    "to",
                    "fulfill",
                    "the",
                    "latter,",
                    "people",
                    "may",
                    "accept",
                    "the",
                    "three",
                    "varieties",
                    "of",
                    "influence",
                    "when",
                    "there",
                    "is",
                    "lack",
                    "of",
                    "information,",
                    "a",
                    "concept",
                    "known",
                    "as",
                    "social",
                    "proof",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                3,
                2,
                2
            ]
        },
        "input": "sent0: Communities.\n sent1: There can be scenarios where whole communities of users on a platform may be indulging in abusive behavior, e.g., by widely circulating an abusive view against a demographic group based on shared beliefs, common stereotypes or other homophilic ties.\n sent2: In such cases, just taking down specific instances of abusive language and providing justifications individually to the respective users may not prove effective.\n sent3: Users may continue to promote the abusive view, defying the norms of the platform in the process and ignoring the justifications given to them.\n sent4: The reason for this comes from social influence theory which says that a user's behavior is affected by three broad varieties of social influence #REF , i.e., compliance, identification, and internalization.\n sent5: Compliance occurs when the user behaves a certain way so as to appear in congruence with opinions of others who matter to them, identification occurs when the user adopts behaviors in order to associate with others they admire, and internalization is when the user adopts the values and beliefs of others.\n sent6: The influences occur because of two needs of the user, the need to be liked (normative) and the need to be right (informational).\n sent7: In order to fulfill the latter, people may accept the three varieties of influence when there is lack of information, a concept known as social proof #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent6\", \"sent7\"], \"BACKGROUND\": [\"sent5\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Thee",
                    "Adjoining",
                    "Grammars",
                    "(TAG)",
                    "#TARGET_REF",
                    "and",
                    "Linear",
                    "Indexed",
                    "Grammars",
                    "(LIG)",
                    "#REF",
                    "are",
                    "extensions",
                    "of",
                    "Con",
                    "text",
                    "Free",
                    "Grammars",
                    "(CFG)."
                ],
                [
                    "Thee",
                    "adjoining",
                    "grammars",
                    "use",
                    "trees",
                    "instead",
                    "of",
                    "productions",
                    "as",
                    "primary",
                    "representing",
                    "structure",
                    "and",
                    "seems",
                    "to",
                    "be",
                    "adequate",
                    "to",
                    "describe",
                    "syntactic",
                    "phenomena",
                    "occurring",
                    "in",
                    "nat",
                    "ural",
                    "language,",
                    "due",
                    "to",
                    "their",
                    "extended",
                    "domain",
                    "of",
                    "locality",
                    "and",
                    "to",
                    "their",
                    "ability",
                    "for",
                    "factoring",
                    "recursion",
                    "from",
                    "the",
                    "domain",
                    "of",
                    "dependencies."
                ],
                [
                    "Linear",
                    "indexed",
                    "grammars",
                    "associate",
                    "a",
                    "stack",
                    "of",
                    "indices",
                    "with",
                    "each",
                    "non-terminal",
                    "symbol,",
                    "with",
                    "the",
                    "restriction",
                    "that",
                    "the",
                    "indices",
                    "stack",
                    "of",
                    "the",
                    "head",
                    "non-terminal",
                    "of",
                    "each",
                    "pro",
                    "duction",
                    "(the",
                    "fa",
                    "ther)",
                    "can",
                    "be",
                    "inherited",
                    "by",
                    "at",
                    "most",
                    "one",
                    "body",
                    "non-terminal",
                    "(the",
                    "dependent",
                    "child)",
                    "while",
                    "the",
                    "other",
                    "stacks",
                    "must",
                    "have",
                    "a",
                    "bounded",
                    "stack",
                    "size",
                    "."
                ]
            ],
            "context": [
                1,
                3,
                0
            ]
        },
        "input": "sent0: Thee Adjoining Grammars (TAG) #TARGET_REF and Linear Indexed Grammars (LIG) #REF are extensions of Con text Free Grammars (CFG).\n sent1: Thee adjoining grammars use trees instead of productions as primary representing structure and seems to be adequate to describe syntactic phenomena occurring in nat ural language, due to their extended domain of locality and to their ability for factoring recursion from the domain of dependencies.\n sent2: Linear indexed grammars associate a stack of indices with each non-terminal symbol, with the restriction that the indices stack of the head non-terminal of each pro duction (the fa ther) can be inherited by at most one body non-terminal (the dependent child) while the other stacks must have a bounded stack size .\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Trained",
                    "on",
                    "ImageNet",
                    "#TARGET_REF",
                    ")",
                    "YOLO",
                    "#REF",
                    "x",
                    "Trained",
                    "on",
                    "ImageNet",
                    "#REF",
                    ")",
                    "EfficientNet",
                    "B7",
                    "#REF",
                    "x",
                    "Trained",
                    "on",
                    "ImageNet",
                    "#REF",
                    "Table",
                    "1:",
                    "List",
                    "of",
                    "pre-trained",
                    "models",
                    "registered",
                    "by",
                    "all",
                    "participants",
                    "of",
                    "ReINTEL",
                    "challenge",
                    "in",
                    "2020."
                ]
            ],
            "context": [
                3
            ]
        },
        "input": "sent0: Trained on ImageNet #TARGET_REF ) YOLO #REF x Trained on ImageNet #REF ) EfficientNet B7 #REF x Trained on ImageNet #REF Table 1: List of pre-trained models registered by all participants of ReINTEL challenge in 2020.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Besides,",
                    "we",
                    "also",
                    "conduct",
                    "a",
                    "manual",
                    "evaluation",
                    "on",
                    "the",
                    "Chinese",
                    "test",
                    "set,",
                    "and",
                    "the",
                    "results",
                    "are",
                    "listed",
                    "in",
                    "Table",
                    "5."
                ],
                [
                    "From",
                    "the",
                    "averaged",
                    "scores,",
                    "we",
                    "observe",
                    "that",
                    "SimpDefiner",
                    "outperforms",
                    "MASS",
                    "by",
                    "0.2",
                    "in",
                    "terms",
                    "of",
                    "accuracy",
                    "(more",
                    "accurate)",
                    "and",
                    "0.18",
                    "in",
                    "terms",
                    "of",
                    "simplicity",
                    "(more",
                    "straightforward)."
                ],
                [
                    "On",
                    "the",
                    "accuracy",
                    "score,",
                    "all",
                    "three",
                    "annotators",
                    "agree",
                    "that",
                    "SimpDefiner",
                    "has",
                    "higher",
                    "accuracy",
                    "than",
                    "MASS,",
                    "which",
                    "shows",
                    "the",
                    "superiority",
                    "of",
                    "our",
                    "framework."
                ],
                [
                    "As",
                    "expected,",
                    "the",
                    "golden",
                    "definitions",
                    "have",
                    "the",
                    "highest",
                    "accuracy",
                    "in",
                    "the",
                    "table,",
                    "far",
                    "exceeding",
                    "the",
                    "definitions",
                    "generated",
                    "by",
                    "the",
                    "two",
                    "models."
                ],
                [
                    "We",
                    "believe",
                    "this",
                    "is",
                    "caused",
                    "by",
                    "insufficient",
                    "knowledge",
                    "in",
                    "the",
                    "model,",
                    "and",
                    "this",
                    "can",
                    "be",
                    "solved",
                    "by",
                    "using",
                    "larger",
                    "pretrained",
                    "models,",
                    "such",
                    "as",
                    "BART",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "On",
                    "the",
                    "simplicity",
                    "score,",
                    "three",
                    "annotators",
                    "agree",
                    "that",
                    "SimpDefiner",
                    "generates",
                    "simpler",
                    "definitions",
                    "than",
                    "MASS,",
                    "and",
                    "two",
                    "of",
                    "three",
                    "annotators",
                    "think",
                    "SimpDefiner",
                    "generates",
                    "simpler",
                    "definitions",
                    "than",
                    "the",
                    "golden",
                    "ones."
                ]
            ],
            "context": [
                0,
                0,
                0,
                3,
                2,
                0
            ]
        },
        "input": "sent0: Besides, we also conduct a manual evaluation on the Chinese test set, and the results are listed in Table 5.\n sent1: From the averaged scores, we observe that SimpDefiner outperforms MASS by 0.2 in terms of accuracy (more accurate) and 0.18 in terms of simplicity (more straightforward).\n sent2: On the accuracy score, all three annotators agree that SimpDefiner has higher accuracy than MASS, which shows the superiority of our framework.\n sent3: As expected, the golden definitions have the highest accuracy in the table, far exceeding the definitions generated by the two models.\n sent4: We believe this is caused by insufficient knowledge in the model, and this can be solved by using larger pretrained models, such as BART #TARGET_REF .\n sent5: On the simplicity score, three annotators agree that SimpDefiner generates simpler definitions than MASS, and two of three annotators think SimpDefiner generates simpler definitions than the golden ones.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent4\"], \"BACKGROUND\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Impact",
                    "on",
                    "Job",
                    "Applications",
                    "While",
                    "our",
                    "goal",
                    "was",
                    "to",
                    "generate",
                    "gender-neutral",
                    "job",
                    "ads,",
                    "it",
                    "remains",
                    "possible",
                    "that",
                    "neutrality",
                    "may",
                    "still",
                    "dissuade",
                    "a",
                    "particular",
                    "group",
                    "from",
                    "applying",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Our",
                    "work",
                    "cannot",
                    "comment",
                    "experimentally",
                    "on",
                    "whether",
                    "less-biased",
                    "ads",
                    "at",
                    "the",
                    "text-level",
                    "result",
                    "in",
                    "a",
                    "greater",
                    "diversity",
                    "of",
                    "applicants."
                ],
                [
                    "Further",
                    "social",
                    "science",
                    "and",
                    "experimental",
                    "research",
                    "is",
                    "thus",
                    "necessary",
                    "to",
                    "understand",
                    "the",
                    "effects",
                    "that",
                    "language",
                    "in",
                    "job",
                    "ads",
                    "has",
                    "on",
                    "applications",
                    "from",
                    "various",
                    "protected",
                    "groups."
                ]
            ],
            "context": [
                3,
                3,
                3
            ]
        },
        "input": "sent0: Impact on Job Applications While our goal was to generate gender-neutral job ads, it remains possible that neutrality may still dissuade a particular group from applying #TARGET_REF .\n sent1: Our work cannot comment experimentally on whether less-biased ads at the text-level result in a greater diversity of applicants.\n sent2: Further social science and experimental research is thus necessary to understand the effects that language in job ads has on applications from various protected groups.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\", \"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Early",
                    "work",
                    "in",
                    "opinion",
                    "question",
                    "answering",
                    "addressed",
                    "separating",
                    "facts",
                    "from",
                    "opinions",
                    "#TARGET_REF",
                    ",",
                    "and",
                    "the",
                    "authors",
                    "used",
                    "a",
                    "Naïve",
                    "Bayes",
                    "classifier",
                    "to",
                    "identify",
                    "polarity",
                    "of",
                    "the",
                    "opinions."
                ],
                [
                    "#REF",
                    "aimed",
                    "at",
                    "identifying",
                    "the",
                    "opinion",
                    "holder",
                    "of",
                    "the",
                    "opinions."
                ],
                [
                    "#REF",
                    "explained",
                    "the",
                    "differences",
                    "between",
                    "fact",
                    "based",
                    "and",
                    "opinionated",
                    "answers",
                    "and",
                    "how",
                    "traditional",
                    "QA",
                    "systems",
                    "will",
                    "not",
                    "be",
                    "able",
                    "to",
                    "handle",
                    "multiple",
                    "perspectives",
                    "for",
                    "answers."
                ],
                [
                    "Some",
                    "works",
                    "aimed",
                    "at",
                    "using",
                    "community",
                    "based",
                    "question-answers",
                    "to",
                    "provide",
                    "unique",
                    "answers",
                    "to",
                    "questions",
                    "#REF",
                    "."
                ],
                [
                    "#REF",
                    "made",
                    "use",
                    "of",
                    "online",
                    "reviews",
                    "to",
                    "answer",
                    "questions",
                    "on",
                    "aspects",
                    "of",
                    "a",
                    "product."
                ],
                [
                    "#REF",
                    "and",
                    "#REF",
                    "used",
                    "graphs",
                    "and",
                    "trees",
                    "to",
                    "answer",
                    "opinion",
                    "questions."
                ],
                [
                    "#REF",
                    "modeled",
                    "ambiguity",
                    "and",
                    "subjectivity",
                    "in",
                    "opinion",
                    "QA",
                    "using",
                    "statistical",
                    "models."
                ],
                [
                    "#REF",
                    "give",
                    "baselines",
                    "for",
                    "answer",
                    "generation",
                    "systems",
                    "given",
                    "the",
                    "question",
                    "and",
                    "reviews."
                ],
                [
                    "We",
                    "use",
                    "their",
                    "results",
                    "as",
                    "the",
                    "baseline",
                    "for",
                    "our",
                    "evaluation."
                ],
                [
                    "We",
                    "also",
                    "discuss",
                    "the",
                    "dataset",
                    "from",
                    "this",
                    "paper",
                    "in",
                    "4.2."
                ],
                [
                    "While",
                    "most",
                    "systems",
                    "used",
                    "in",
                    "the",
                    "works",
                    "described",
                    "above",
                    "are",
                    "supervised",
                    "learning",
                    "models,",
                    "our",
                    "system",
                    "used",
                    "unsupervised",
                    "learning",
                    "to",
                    "answer",
                    "binary",
                    "(yes/no)",
                    "questions."
                ]
            ],
            "context": [
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Early work in opinion question answering addressed separating facts from opinions #TARGET_REF , and the authors used a Naïve Bayes classifier to identify polarity of the opinions.\n sent1: #REF aimed at identifying the opinion holder of the opinions.\n sent2: #REF explained the differences between fact based and opinionated answers and how traditional QA systems will not be able to handle multiple perspectives for answers.\n sent3: Some works aimed at using community based question-answers to provide unique answers to questions #REF .\n sent4: #REF made use of online reviews to answer questions on aspects of a product.\n sent5: #REF and #REF used graphs and trees to answer opinion questions.\n sent6: #REF modeled ambiguity and subjectivity in opinion QA using statistical models.\n sent7: #REF give baselines for answer generation systems given the question and reviews.\n sent8: We use their results as the baseline for our evaluation.\n sent9: We also discuss the dataset from this paper in 4.2.\n sent10: While most systems used in the works described above are supervised learning models, our system used unsupervised learning to answer binary (yes/no) questions.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "There",
                    "are",
                    "several",
                    "reports",
                    "in",
                    "the",
                    "literature",
                    "showing",
                    "that",
                    "a",
                    "careful",
                    "design",
                    "of",
                    "the",
                    "interface",
                    "between",
                    "automatic",
                    "speech",
                    "recognition",
                    "(ASR)",
                    "and",
                    "machine",
                    "translation",
                    "can",
                    "be",
                    "important",
                    "to",
                    "limit",
                    "the",
                    "performance",
                    "degradation",
                    "observed",
                    "when",
                    "translating",
                    "an",
                    "automatic",
                    "transcription",
                    "(as",
                    "opposed",
                    "to",
                    "a",
                    "manual",
                    "transcription)."
                ],
                [
                    "These",
                    "works",
                    "include",
                    "the",
                    "translation",
                    "of",
                    "richer",
                    "data",
                    "structures",
                    "than",
                    "the",
                    "1-best",
                    "ASR",
                    "output,",
                    "see",
                    "for",
                    "instance",
                    "#TARGET_REF",
                    "or",
                    "various",
                    "aspects",
                    "of",
                    "case,",
                    "punctuation",
                    "and",
                    "word",
                    "normalization",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                0,
                0
            ]
        },
        "input": "sent0: There are several reports in the literature showing that a careful design of the interface between automatic speech recognition (ASR) and machine translation can be important to limit the performance degradation observed when translating an automatic transcription (as opposed to a manual transcription).\n sent1: These works include the translation of richer data structures than the 1-best ASR output, see for instance #TARGET_REF or various aspects of case, punctuation and word normalization #REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "memory",
                    "component",
                    "and",
                    "action",
                    "gate",
                    "are",
                    "endto-end",
                    "trained",
                    "in",
                    "a",
                    "self-supervised",
                    "way,",
                    "where",
                    "the",
                    "feedback",
                    "is",
                    "whether",
                    "an",
                    "utterance",
                    "and",
                    "its",
                    "action",
                    "representation",
                    "lead",
                    "to",
                    "similar",
                    "state",
                    "transitions,",
                    "We",
                    "can",
                    "measure",
                    "such",
                    "similarity",
                    "using",
                    "a",
                    "dialogue",
                    "state",
                    "tracking",
                    "(DST)",
                    "model."
                ],
                [
                    "However,",
                    "a",
                    "direct",
                    "application",
                    "of",
                    "the",
                    "DST",
                    "model",
                    "trained",
                    "by",
                    "Eqn."
                ],
                [
                    "4",
                    "might",
                    "be",
                    "prone",
                    "to",
                    "attribute",
                    "changes",
                    "between",
                    "original",
                    "utterances",
                    "and",
                    "compact",
                    "natural",
                    "language",
                    "actions,",
                    "which",
                    "results",
                    "in",
                    "insufficient",
                    "feedback."
                ],
                [
                    "To",
                    "address",
                    "this",
                    "issue,",
                    "we",
                    "adopt",
                    "a",
                    "denoising",
                    "training",
                    "strategy",
                    "inspired",
                    "by",
                    "unsupervised",
                    "machine",
                    "translation",
                    "#REF",
                    ",",
                    "and",
                    "obtain",
                    "a",
                    "DST",
                    "model",
                    "that",
                    "is",
                    "more",
                    "robust",
                    "to",
                    "the",
                    "attribute",
                    "transformation."
                ],
                [
                    "Specifically,",
                    "we",
                    "apply",
                    "a",
                    "noise",
                    "function",
                    "g(x)",
                    "to",
                    "the",
                    "utterances,",
                    "and",
                    "modify",
                    "the",
                    "DST",
                    "model",
                    "training",
                    "loss",
                    "as:L",
                    "dst",
                    "=",
                    "d",
                    "i",
                    "−",
                    "log(b",
                    "t",
                    "•",
                    "p",
                    "B",
                    "(g(u",
                    "t",
                    "),",
                    "g(x",
                    "t−1",
                    "),",
                    "c",
                    "t−1",
                    "))",
                    "(8)where",
                    "the",
                    "noise",
                    "function",
                    "corrupts",
                    "the",
                    "input",
                    "utterance",
                    "by",
                    "performing",
                    "word",
                    "drops",
                    "and",
                    "word",
                    "order",
                    "shuffling",
                    "as",
                    "specified",
                    "in",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                3,
                0,
                3,
                3,
                0
            ]
        },
        "input": "sent0: The memory component and action gate are endto-end trained in a self-supervised way, where the feedback is whether an utterance and its action representation lead to similar state transitions, We can measure such similarity using a dialogue state tracking (DST) model.\n sent1: However, a direct application of the DST model trained by Eqn.\n sent2: 4 might be prone to attribute changes between original utterances and compact natural language actions, which results in insufficient feedback.\n sent3: To address this issue, we adopt a denoising training strategy inspired by unsupervised machine translation #REF , and obtain a DST model that is more robust to the attribute transformation.\n sent4: Specifically, we apply a noise function g(x) to the utterances, and modify the DST model training loss as:L dst = d i − log(b t • p B (g(u t ), g(x t−1 ), c t−1 )) (8)where the noise function corrupts the input utterance by performing word drops and word order shuffling as specified in #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\", \"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Borrowing",
                    "from",
                    "techniques",
                    "used",
                    "on",
                    "languages",
                    "that",
                    "do",
                    "not",
                    "indicate",
                    "word",
                    "boundaries",
                    "by",
                    "the",
                    "use",
                    "of",
                    "whitespace,",
                    "we",
                    "address",
                    "the",
                    "problem",
                    "by",
                    "removing",
                    "all",
                    "whitespace",
                    "from",
                    "our",
                    "data",
                    "sets",
                    "after",
                    "phone",
                    "transliteration."
                ],
                [
                    "We",
                    "train",
                    "character-based",
                    "language",
                    "models",
                    "over",
                    "the",
                    "resulting",
                    "data."
                ],
                [
                    "Character-based",
                    "models",
                    "such",
                    "as",
                    "CharFormer",
                    "#TARGET_REF",
                    "or",
                    "ByT5",
                    "#REF",
                    "have",
                    "shown",
                    "promise",
                    "in",
                    "recent",
                    "years",
                    "for",
                    "language",
                    "modeling,",
                    "even",
                    "if",
                    "this",
                    "approach",
                    "is",
                    "known",
                    "to",
                    "have",
                    "some",
                    "trade",
                    "offs",
                    "related",
                    "to",
                    "shorter",
                    "context",
                    "windows."
                ]
            ],
            "context": [
                0,
                0,
                2
            ]
        },
        "input": "sent0: Borrowing from techniques used on languages that do not indicate word boundaries by the use of whitespace, we address the problem by removing all whitespace from our data sets after phone transliteration.\n sent1: We train character-based language models over the resulting data.\n sent2: Character-based models such as CharFormer #TARGET_REF or ByT5 #REF have shown promise in recent years for language modeling, even if this approach is known to have some trade offs related to shorter context windows.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Figure",
                    "2:",
                    "Amount",
                    "of",
                    "data",
                    "in",
                    "GB",
                    "(log-scale)",
                    "for",
                    "the",
                    "88",
                    "languages",
                    "that",
                    "appear",
                    "in",
                    "both",
                    "the",
                    "Wiki-100",
                    "#TARGET_REF",
                    "corpus",
                    "used",
                    "for",
                    "mBERT",
                    "and",
                    "XLM-100",
                    "#REF",
                    "."
                ],
                [
                    "None",
                    "of",
                    "the",
                    "Indian",
                    "languages",
                    "feature",
                    "among",
                    "top-25",
                    "languages",
                    "with",
                    "the",
                    "largest",
                    "amount",
                    "of",
                    "data."
                ]
            ],
            "context": [
                2,
                3
            ]
        },
        "input": "sent0: Figure 2: Amount of data in GB (log-scale) for the 88 languages that appear in both the Wiki-100 #TARGET_REF corpus used for mBERT and XLM-100 #REF .\n sent1: None of the Indian languages feature among top-25 languages with the largest amount of data.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "French",
                    "is",
                    "a",
                    "poorly",
                    "endowed",
                    "language",
                    "since",
                    "we",
                    "do",
                    "not",
                    "have",
                    "enough",
                    "annotated",
                    "data",
                    "to",
                    "train",
                    "a",
                    "deep",
                    "learning",
                    "model",
                    "on",
                    "QA",
                    "tasks."
                ],
                [
                    "Moreover,",
                    "unlike",
                    "the",
                    "only",
                    "two",
                    "large",
                    "monolingual",
                    "French",
                    "models:",
                    "CamemBERT",
                    "#REF",
                    "and",
                    "FlauBERT",
                    "#TARGET_REF",
                    ",",
                    "the",
                    "English",
                    "BERT",
                    "model",
                    "has",
                    "become",
                    "a",
                    "branching",
                    "point",
                    "from",
                    "which",
                    "a",
                    "growing",
                    "number",
                    "of",
                    "large",
                    "and",
                    "compact",
                    "English",
                    "pre-trained",
                    "models",
                    "have",
                    "emerged."
                ],
                [
                    "These",
                    "French",
                    "monolingual",
                    "models,",
                    "although",
                    "they",
                    "provide",
                    "good",
                    "performances,",
                    "do",
                    "not",
                    "reflect",
                    "the",
                    "rapid",
                    "evolution",
                    "of",
                    "the",
                    "field."
                ]
            ],
            "context": [
                0,
                2,
                3
            ]
        },
        "input": "sent0: French is a poorly endowed language since we do not have enough annotated data to train a deep learning model on QA tasks.\n sent1: Moreover, unlike the only two large monolingual French models: CamemBERT #REF and FlauBERT #TARGET_REF , the English BERT model has become a branching point from which a growing number of large and compact English pre-trained models have emerged.\n sent2: These French monolingual models, although they provide good performances, do not reflect the rapid evolution of the field.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Outside",
                    "of",
                    "speech",
                    "recognition",
                    "focused",
                    "work,",
                    "#REF",
                    "(and",
                    "other",
                    "researchers",
                    "cited",
                    "therein)",
                    "attempt",
                    "to",
                    "\"fuse\"",
                    "audio",
                    "and",
                    "text",
                    "at",
                    "the",
                    "word",
                    "level",
                    "for",
                    "emotion",
                    "recognition."
                ],
                [
                    "They",
                    "introduce",
                    "another",
                    "architecture",
                    "that",
                    "internally",
                    "represents",
                    "both",
                    "audio",
                    "and",
                    "text."
                ],
                [
                    "However,",
                    "the",
                    "so-called",
                    "WISE",
                    "framework",
                    "relies",
                    "on",
                    "speech",
                    "recognition",
                    "to",
                    "generate",
                    "the",
                    "text",
                    "corresponding",
                    "to",
                    "audio",
                    "frames",
                    "in",
                    "real-time."
                ],
                [
                    "The",
                    "current",
                    "work",
                    "explicitly",
                    "avoids",
                    "reliance",
                    "on",
                    "speech",
                    "recognition."
                ],
                [
                    "The",
                    "2021",
                    "Multimodal",
                    "Sentiment",
                    "Analysis",
                    "(MuSe)",
                    "challenge",
                    "continues",
                    "this",
                    "vein",
                    "of",
                    "research",
                    "integrating",
                    "audio,",
                    "video,",
                    "text,",
                    "and",
                    "physiology",
                    "data",
                    "in",
                    "an",
                    "emotion",
                    "recognition",
                    "task",
                    "#REF",
                    "."
                ],
                [
                    "Contributions",
                    "to",
                    "this",
                    "challenge,",
                    "such",
                    "as",
                    "#TARGET_REF",
                    ",",
                    "introduce",
                    "a",
                    "variety",
                    "of",
                    "ways",
                    "to",
                    "\"fuse\"",
                    "audio",
                    "and",
                    "text",
                    "inputs."
                ],
                [
                    "However,",
                    "these",
                    "contributions",
                    "are",
                    "squarely",
                    "focused",
                    "on",
                    "emotion/sentiment",
                    "analysis",
                    "and",
                    "do",
                    "not",
                    "propose",
                    "methods",
                    "for",
                    "flexible,",
                    "phonetic",
                    "language",
                    "models."
                ],
                [
                    "#REF",
                    "introduced",
                    "functionality",
                    "for",
                    "\"textless\"",
                    "NLP."
                ],
                [
                    "They",
                    "explored",
                    "the",
                    "possibility",
                    "of",
                    "creating",
                    "a",
                    "dialogue",
                    "system",
                    "from",
                    "only",
                    "audio",
                    "inputs",
                    "(i.e.,",
                    "without",
                    "text)."
                ],
                [
                    "As",
                    "part",
                    "of",
                    "this",
                    "system,",
                    "language",
                    "models",
                    "are",
                    "directly",
                    "trained",
                    "on",
                    "audio",
                    "units",
                    "without",
                    "any",
                    "text."
                ],
                [
                    "This",
                    "advances",
                    "the",
                    "state-of-the-art",
                    "with",
                    "regard",
                    "to",
                    "self-supervised",
                    "speech",
                    "methods,",
                    "but",
                    "it",
                    "does",
                    "not",
                    "provide",
                    "the",
                    "flexibility",
                    "in",
                    "audio",
                    "and/or",
                    "text",
                    "language",
                    "modeling",
                    "introduced",
                    "here."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                1,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Outside of speech recognition focused work, #REF (and other researchers cited therein) attempt to \"fuse\" audio and text at the word level for emotion recognition.\n sent1: They introduce another architecture that internally represents both audio and text.\n sent2: However, the so-called WISE framework relies on speech recognition to generate the text corresponding to audio frames in real-time.\n sent3: The current work explicitly avoids reliance on speech recognition.\n sent4: The 2021 Multimodal Sentiment Analysis (MuSe) challenge continues this vein of research integrating audio, video, text, and physiology data in an emotion recognition task #REF .\n sent5: Contributions to this challenge, such as #TARGET_REF , introduce a variety of ways to \"fuse\" audio and text inputs.\n sent6: However, these contributions are squarely focused on emotion/sentiment analysis and do not propose methods for flexible, phonetic language models.\n sent7: #REF introduced functionality for \"textless\" NLP.\n sent8: They explored the possibility of creating a dialogue system from only audio inputs (i.e., without text).\n sent9: As part of this system, language models are directly trained on audio units without any text.\n sent10: This advances the state-of-the-art with regard to self-supervised speech methods, but it does not provide the flexibility in audio and/or text language modeling introduced here.\n",
        "output": "{\"INFORMATION\": [\"sent5\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "ChrF",
                    "#TARGET_REF",
                    "a",
                    "character",
                    "n-gram",
                    "precision",
                    "and",
                    "recall",
                    "enhanced",
                    "with",
                    "word",
                    "n-grams."
                ],
                [
                    "Since",
                    "answers",
                    "are",
                    "largely",
                    "made",
                    "up",
                    "of",
                    "entities,",
                    "ChrF",
                    "score",
                    "integration",
                    "is",
                    "only",
                    "performed",
                    "when",
                    "the",
                    "answer",
                    "span",
                    "is",
                    "not",
                    "present",
                    "in",
                    "the",
                    "related",
                    "context."
                ],
                [
                    "In",
                    "order",
                    "to",
                    "evaluate",
                    "the",
                    "quality",
                    "of",
                    "the",
                    "translation,",
                    "we",
                    "manually",
                    "corrected",
                    "the",
                    "translation",
                    "errors",
                    "in",
                    "the",
                    "output",
                    "of",
                    "a",
                    "subset",
                    "of",
                    "the",
                    "corpus",
                    "composed",
                    "of",
                    "890",
                    "QA",
                    "pairs",
                    "and",
                    "107",
                    "contexts."
                ],
                [
                    "We",
                    "obtain",
                    "a",
                    "BLEU",
                    "score",
                    "#REF",
                    "We",
                    "also",
                    "explore",
                    "mixed",
                    "datasets",
                    "training",
                    "strategy",
                    "with",
                    "SQuAD-en",
                    "train",
                    "+",
                    "FQuAD",
                    "train",
                    "for",
                    "training",
                    "models",
                    "on",
                    "a",
                    "concatenation",
                    "of",
                    "the",
                    "training",
                    "data",
                    "covering",
                    "French-English",
                    "language",
                    "pairs",
                    "to",
                    "test",
                    "the",
                    "crosslingual",
                    "transfer",
                    "ability",
                    "of",
                    "multilingual",
                    "models."
                ]
            ],
            "context": [
                1,
                3,
                0,
                0
            ]
        },
        "input": "sent0: ChrF #TARGET_REF a character n-gram precision and recall enhanced with word n-grams.\n sent1: Since answers are largely made up of entities, ChrF score integration is only performed when the answer span is not present in the related context.\n sent2: In order to evaluate the quality of the translation, we manually corrected the translation errors in the output of a subset of the corpus composed of 890 QA pairs and 107 contexts.\n sent3: We obtain a BLEU score #REF We also explore mixed datasets training strategy with SQuAD-en train + FQuAD train for training models on a concatenation of the training data covering French-English language pairs to test the crosslingual transfer ability of multilingual models.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "From",
                    "our",
                    "source",
                    "data",
                    "we",
                    "can",
                    "extract",
                    "parallel",
                    "sentences",
                    "for",
                    "43",
                    "languages."
                ],
                [
                    "For",
                    "12",
                    "of",
                    "these",
                    "languages",
                    "we",
                    "have",
                    "over",
                    "10,000",
                    "sentences",
                    "available",
                    "for",
                    "projection",
                    "as",
                    "per",
                    "table",
                    "4."
                ],
                [
                    "We",
                    "removed",
                    "some",
                    "of",
                    "these",
                    "languages",
                    "for",
                    "having",
                    "fewer",
                    "than",
                    "950",
                    "lines,",
                    "resulting",
                    "in",
                    "a",
                    "total",
                    "of",
                    "32",
                    "languages",
                    "5",
                    "including",
                    "the",
                    "annotated",
                    "English",
                    "and",
                    "Finnish",
                    "data."
                ],
                [
                    "We",
                    "have",
                    "made",
                    "all",
                    "32",
                    "datasets",
                    "available",
                    "on",
                    "GitHub",
                    "plus",
                    "the",
                    "raw",
                    "data",
                    "for",
                    "all",
                    "43",
                    "languages",
                    "including",
                    "the",
                    "11",
                    "datasets",
                    "that",
                    "had",
                    "fewer",
                    "than",
                    "950",
                    "lines."
                ],
                [
                    "#TARGET_REF",
                    "Table",
                    "4:",
                    "Languages",
                    "(ISO",
                    "code)",
                    "with",
                    "over",
                    "10k",
                    "parallel",
                    "sentences",
                    "with",
                    "our",
                    "annotated",
                    "English",
                    "data."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "sent0: From our source data we can extract parallel sentences for 43 languages.\n sent1: For 12 of these languages we have over 10,000 sentences available for projection as per table 4.\n sent2: We removed some of these languages for having fewer than 950 lines, resulting in a total of 32 languages 5 including the annotated English and Finnish data.\n sent3: We have made all 32 datasets available on GitHub plus the raw data for all 43 languages including the 11 datasets that had fewer than 950 lines.\n sent4: #TARGET_REF Table 4: Languages (ISO code) with over 10k parallel sentences with our annotated English data.\n",
        "output": "{\"INFORMATION\": [\"sent4\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "(3)",
                    "the",
                    "subword",
                    "field,",
                    "which",
                    "breaks",
                    "tokens",
                    "into",
                    "subwords,",
                    "and",
                    "(4)",
                    "the",
                    "d2q",
                    "field,",
                    "which",
                    "includes",
                    "the",
                    "stemmed",
                    "tokens",
                    "from",
                    "the",
                    "concatenated",
                    "docTTTTTquery",
                    "predictions",
                    "(for",
                    "the",
                    "d2q",
                    "variants)."
                ],
                [
                    "For",
                    "each",
                    "querydocument",
                    "pair,",
                    "feature",
                    "extraction",
                    "is",
                    "repeated",
                    "over",
                    "all",
                    "applicable",
                    "fields."
                ],
                [
                    "In",
                    "total,",
                    "there",
                    "are",
                    "83",
                    "different",
                    "features",
                    "(per",
                    "field)",
                    "plus",
                    "four",
                    "translation-based",
                    "features",
                    "that",
                    "are",
                    "only",
                    "available",
                    "in",
                    "the",
                    "raw",
                    "and",
                    "subword",
                    "fields."
                ],
                [
                    "We",
                    "additionally",
                    "test",
                    "on",
                    "the",
                    "MS",
                    "MARCO",
                    "document",
                    "ranking",
                    "task",
                    "#TARGET_REF",
                    "in",
                    "a",
                    "zeroshot",
                    "manner."
                ],
                [
                    "For",
                    "this,",
                    "we",
                    "segment",
                    "each",
                    "document",
                    "into",
                    "multiple",
                    "passages",
                    "as",
                    "the",
                    "neural",
                    "models",
                    "cannot",
                    "process",
                    "long",
                    "documents."
                ],
                [
                    "Specifically,",
                    "we",
                    "use",
                    "the",
                    "sliding",
                    "window",
                    "strategy",
                    "of",
                    "#REF",
                    ",",
                    "where",
                    "the",
                    "window",
                    "length",
                    "is",
                    "ten",
                    "sentences",
                    "with",
                    "a",
                    "stride",
                    "of",
                    "five",
                    "sentences."
                ],
                [
                    "Retrieval",
                    "is",
                    "performed",
                    "at",
                    "the",
                    "passage",
                    "level,",
                    "and",
                    "the",
                    "document",
                    "score",
                    "is",
                    "computed",
                    "based",
                    "on",
                    "the",
                    "highest",
                    "relevance",
                    "score",
                    "among",
                    "its",
                    "passages."
                ]
            ],
            "context": [
                0,
                0,
                0,
                2,
                0,
                0,
                0
            ]
        },
        "input": "sent0: (3) the subword field, which breaks tokens into subwords, and (4) the d2q field, which includes the stemmed tokens from the concatenated docTTTTTquery predictions (for the d2q variants).\n sent1: For each querydocument pair, feature extraction is repeated over all applicable fields.\n sent2: In total, there are 83 different features (per field) plus four translation-based features that are only available in the raw and subword fields.\n sent3: We additionally test on the MS MARCO document ranking task #TARGET_REF in a zeroshot manner.\n sent4: For this, we segment each document into multiple passages as the neural models cannot process long documents.\n sent5: Specifically, we use the sliding window strategy of #REF , where the window length is ten sentences with a stride of five sentences.\n sent6: Retrieval is performed at the passage level, and the document score is computed based on the highest relevance score among its passages.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent3\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "•",
                    "SQuAD",
                    "(v1.1)",
                    "#TARGET_REF"
                ]
            ],
            "context": [
                1
            ]
        },
        "input": "sent0: • SQuAD (v1.1) #TARGET_REF\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "A",
                    "standard",
                    "buTPDA",
                    "is",
                    "not",
                    "quite",
                    "the",
                    "right",
                    "model."
                ],
                [
                    "#TARGET_REF",
                    "prove",
                    "that",
                    "TPDAs",
                    "are",
                    "necessary",
                    "for",
                    "operating",
                    "on",
                    "tree",
                    "sets",
                    "with",
                    "context-free",
                    "path",
                    "languages."
                ],
                [
                    "7",
                    "But",
                    "they",
                    "also",
                    "prove",
                    "that",
                    "the",
                    "yield",
                    "of",
                    "the",
                    "class",
                    "of",
                    "tree",
                    "languages",
                    "accepted",
                    "by",
                    "buTPDAs",
                    "is",
                    "the",
                    "indexed",
                    "languages."
                ],
                [
                    "For",
                    "the",
                    "nature",
                    "of",
                    "gNCNs",
                    "presented",
                    "in",
                    "this",
                    "paper,",
                    "the",
                    "string",
                    "language",
                    "should",
                    "be",
                    "within",
                    "the",
                    "mildly",
                    "context-sensitive",
                    "languages",
                    "(MCSLs),",
                    "thus",
                    "this",
                    "type",
                    "of",
                    "TPDA",
                    "is",
                    "too",
                    "powerful."
                ]
            ],
            "context": [
                3,
                1,
                1,
                2
            ]
        },
        "input": "sent0: A standard buTPDA is not quite the right model.\n sent1: #TARGET_REF prove that TPDAs are necessary for operating on tree sets with context-free path languages.\n sent2: 7 But they also prove that the yield of the class of tree languages accepted by buTPDAs is the indexed languages.\n sent3: For the nature of gNCNs presented in this paper, the string language should be within the mildly context-sensitive languages (MCSLs), thus this type of TPDA is too powerful.\n",
        "output": "{\"INFORMATION\": [\"sent1\", \"sent2\"], \"PERCEPTION\": [\"sent3\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Our",
                    "approach",
                    "to",
                    "compression",
                    "involves",
                    "learning",
                    "a",
                    "sparse",
                    "trainable",
                    "mask",
                    "that",
                    "restricts",
                    "the",
                    "set",
                    "of",
                    "types",
                    "considered."
                ],
                [
                    "We",
                    "parameterize",
                    "the",
                    "dot",
                    "product",
                    "6",
                    "and",
                    "cosine",
                    "similarity",
                    "7",
                    "operations",
                    "with",
                    "a",
                    "weight",
                    "matrix",
                    "W,",
                    "a",
                    "diagonal",
                    "matrix",
                    "diag(w",
                    "1",
                    ",",
                    "w",
                    "2",
                    ",",
                    "...,",
                    "w",
                    "|T",
                    "|",
                    ")",
                    "whose",
                    "components",
                    "correspond",
                    "to",
                    "the",
                    "entity",
                    "types",
                    "in",
                    "T",
                    "."
                ],
                [
                    "The",
                    "parameters",
                    "W",
                    "can",
                    "be",
                    "learned",
                    "directly",
                    "on",
                    "downstream",
                    "tasks",
                    "(e.g.,",
                    "CAP",
                    "and",
                    "NED)."
                ],
                [
                    "Note",
                    "that",
                    "in",
                    "the",
                    "cosine",
                    "scoring",
                    "function,",
                    "we",
                    "clip",
                    "these",
                    "parameter",
                    "values",
                    "to",
                    "be",
                    "between",
                    "0",
                    "and",
                    "1."
                ],
                [
                    "We",
                    "train",
                    "with",
                    "the",
                    "standard",
                    "downstream",
                    "task",
                    "objective,",
                    "but",
                    "with",
                    "an",
                    "additional",
                    "L",
                    "1",
                    "regularization",
                    "term",
                    "applied",
                    "to",
                    "W",
                    "#TARGET_REF",
                    "to",
                    "encourage",
                    "the",
                    "W",
                    "values",
                    "to",
                    "be",
                    "sparse."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                2
            ]
        },
        "input": "sent0: Our approach to compression involves learning a sparse trainable mask that restricts the set of types considered.\n sent1: We parameterize the dot product 6 and cosine similarity 7 operations with a weight matrix W, a diagonal matrix diag(w 1 , w 2 , ..., w |T | ) whose components correspond to the entity types in T .\n sent2: The parameters W can be learned directly on downstream tasks (e.g., CAP and NED).\n sent3: Note that in the cosine scoring function, we clip these parameter values to be between 0 and 1.\n sent4: We train with the standard downstream task objective, but with an additional L 1 regularization term applied to W #TARGET_REF to encourage the W values to be sparse.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent4\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "do,",
                    "however,",
                    "capture",
                    "the",
                    "preference",
                    "of",
                    "larger",
                    "pattern",
                    "trees",
                    "over",
                    "combinations",
                    "of",
                    "smaller",
                    "trees,",
                    "which",
                    "is",
                    "desireable,",
                    "see",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Tracing",
                    "through",
                    "the",
                    "example",
                    "again,",
                    "then,",
                    "this",
                    "time",
                    "with",
                    "costs,",
                    "at",
                    "the",
                    "lowest",
                    "node",
                    "the",
                    "annotation",
                    "«",
                    "has",
                    "cost",
                    "3,",
                    "the",
                    "other",
                    "two",
                    "annotations",
                    "«",
                    "½",
                    "½",
                    "and",
                    "«",
                    "½,",
                    "being",
                    "partial",
                    "pattern",
                    "trees,",
                    "have",
                    "no",
                    "cost."
                ],
                [
                    "At",
                    "the",
                    "next",
                    "higher",
                    "node,",
                    "the",
                    "annotations",
                    "¬",
                    "½",
                    "•",
                    "«",
                    "½",
                    "½",
                    "and",
                    "¬",
                    "½",
                    "•",
                    "«",
                    "½",
                    "have",
                    "cost",
                    "3,",
                    "«",
                    "¿",
                    "has",
                    "cost",
                    "6",
                    "(3",
                    "for",
                    "the",
                    "pattern",
                    "tree",
                    "«",
                    "¿",
                    ",",
                    "and",
                    "3",
                    "for",
                    "the",
                    "left",
                    "child",
                    "as",
                    "annotated",
                    "in",
                    "the",
                    "previous",
                    "step),",
                    "«",
                    "has",
                    "cost",
                    "5."
                ],
                [
                    "As",
                    "both",
                    "«",
                    "alternatives",
                    "span",
                    "the",
                    "same",
                    "subtree",
                    "from",
                    "this",
                    "node",
                    "down,",
                    "and",
                    "have",
                    "the",
                    "same",
                    "return",
                    "type",
                    "(sub),",
                    "it",
                    "is",
                    "possible",
                    "to",
                    "discard",
                    "the",
                    "annotation",
                    "«",
                    "¿",
                    ",",
                    "as",
                    "it",
                    "will",
                    "always",
                    "be",
                    "cheaper",
                    "to",
                    "use",
                    "«",
                    "at",
                    "this",
                    "point,",
                    "regardless",
                    "of",
                    "what",
                    "happens",
                    "further",
                    "up",
                    "the",
                    "tree."
                ],
                [
                    "At",
                    "the",
                    "next",
                    "higher",
                    "node,",
                    "the",
                    "annotations",
                    "¬",
                    "½",
                    "•",
                    "«",
                    "½",
                    "½",
                    "and",
                    "¬",
                    "½",
                    "•",
                    "«",
                    "½",
                    "have",
                    "cost",
                    "6,",
                    "and",
                    "«",
                    "¿",
                    "has",
                    "cost",
                    "8."
                ],
                [
                    "Finally,",
                    "at",
                    "the",
                    "top",
                    "Ë",
                    "node,",
                    "«",
                    "¾",
                    "has",
                    "cost",
                    "13",
                    "(5",
                    "for",
                    "the",
                    "pattern",
                    "tree,",
                    "8",
                    "for",
                    "the",
                    "left",
                    "child:",
                    "as",
                    "the",
                    "pattern",
                    "tree",
                    "can",
                    "only",
                    "accept",
                    "an",
                    "initial",
                    "tree",
                    "as",
                    "the",
                    "left",
                    "child,",
                    "only",
                    "«",
                    "¿",
                    "is",
                    "a",
                    "suitable",
                    "candidate),",
                    "but",
                    "«",
                    "½",
                    "has",
                    "cost",
                    "10",
                    "(4",
                    "for",
                    "the",
                    "pattern",
                    "tree,",
                    "6",
                    "for",
                    "the",
                    "intervening",
                    "auxiliary",
                    "trees)."
                ],
                [
                    "The",
                    "algorithm",
                    "in",
                    "Figure",
                    "8",
                    "is",
                    "modified",
                    "so",
                    "that",
                    "any",
                    "annotation",
                    "in",
                    "an",
                    "annotation",
                    "set",
                    "with",
                    "the",
                    "same",
                    "type",
                    "but",
                    "non-minimal",
                    "cost",
                    "is",
                    "discarded."
                ],
                [
                    "Thus",
                    "the",
                    "derivation",
                    "of",
                    "the",
                    "optimal",
                    "tree",
                    "parse,",
                    "top-down,",
                    "would",
                    "be",
                    "«",
                    "½",
                    "with",
                    "an",
                    "adjunction",
                    "of",
                    "¬",
                    "½",
                    "which",
                    "in",
                    "turn",
                    "has",
                    "an",
                    "adjunction",
                    "of",
                    "¬",
                    "½",
                    "."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: do, however, capture the preference of larger pattern trees over combinations of smaller trees, which is desireable, see #TARGET_REF .\n sent1: Tracing through the example again, then, this time with costs, at the lowest node the annotation « has cost 3, the other two annotations « ½ ½ and « ½, being partial pattern trees, have no cost.\n sent2: At the next higher node, the annotations ¬ ½ • « ½ ½ and ¬ ½ • « ½ have cost 3, « ¿ has cost 6 (3 for the pattern tree « ¿ , and 3 for the left child as annotated in the previous step), « has cost 5.\n sent3: As both « alternatives span the same subtree from this node down, and have the same return type (sub), it is possible to discard the annotation « ¿ , as it will always be cheaper to use « at this point, regardless of what happens further up the tree.\n sent4: At the next higher node, the annotations ¬ ½ • « ½ ½ and ¬ ½ • « ½ have cost 6, and « ¿ has cost 8.\n sent5: Finally, at the top Ë node, « ¾ has cost 13 (5 for the pattern tree, 8 for the left child: as the pattern tree can only accept an initial tree as the left child, only « ¿ is a suitable candidate), but « ½ has cost 10 (4 for the pattern tree, 6 for the intervening auxiliary trees).\n sent6: The algorithm in Figure 8 is modified so that any annotation in an annotation set with the same type but non-minimal cost is discarded.\n sent7: Thus the derivation of the optimal tree parse, top-down, would be « ½ with an adjunction of ¬ ½ which in turn has an adjunction of ¬ ½ .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Factored",
                    "Neural",
                    "Machine",
                    "Translation",
                    "(FNMT)",
                    "approach",
                    "handles",
                    "the",
                    "output",
                    "vocabulary",
                    "size",
                    "problem",
                    "using",
                    "factors",
                    "as",
                    "a",
                    "translation",
                    "unit."
                ],
                [
                    "The",
                    "main",
                    "motivation",
                    "behind",
                    "this",
                    "factored",
                    "representation",
                    "is",
                    "related",
                    "to",
                    "the",
                    "human",
                    "way",
                    "to",
                    "learn",
                    "how",
                    "to",
                    "construct",
                    "correct",
                    "sentences."
                ],
                [
                    "In",
                    "this",
                    "work,",
                    "the",
                    "factors",
                    "are",
                    "referring",
                    "to",
                    "the",
                    "linguistic",
                    "annotation",
                    "at",
                    "word",
                    "level",
                    "like",
                    "the",
                    "Part",
                    "of",
                    "Speech",
                    "(POS)",
                    "tags."
                ],
                [
                    "Some",
                    "works",
                    "have",
                    "used",
                    "factors",
                    "as",
                    "additional",
                    "information",
                    "for",
                    "language",
                    "modeling",
                    "#TARGET_REF",
                    "and",
                    "also",
                    "applied",
                    "for",
                    "neural",
                    "networks",
                    "language",
                    "models",
                    "#REF",
                    "."
                ],
                [
                    "Recently,",
                    "factors",
                    "have",
                    "been",
                    "used",
                    "as",
                    "additional",
                    "linguistic",
                    "input",
                    "features",
                    "to",
                    "improve",
                    "a",
                    "word-level",
                    "NMT",
                    "system",
                    "#REF",
                    "as",
                    "well."
                ]
            ],
            "context": [
                3,
                0,
                3,
                3,
                0
            ]
        },
        "input": "sent0: Factored Neural Machine Translation (FNMT) approach handles the output vocabulary size problem using factors as a translation unit.\n sent1: The main motivation behind this factored representation is related to the human way to learn how to construct correct sentences.\n sent2: In this work, the factors are referring to the linguistic annotation at word level like the Part of Speech (POS) tags.\n sent3: Some works have used factors as additional information for language modeling #TARGET_REF and also applied for neural networks language models #REF .\n sent4: Recently, factors have been used as additional linguistic input features to improve a word-level NMT system #REF as well.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\", \"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Monolingual",
                    "and",
                    "bilingual",
                    "dictionaries",
                    "were",
                    "constructed",
                    "using",
                    "a",
                    "large",
                    "bilingual",
                    "word",
                    "list",
                    "of",
                    "unchecked",
                    "quality."
                ],
                [
                    "Paradigms",
                    "were",
                    "hand-written",
                    "according",
                    "to",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                3,
                2
            ]
        },
        "input": "sent0: Monolingual and bilingual dictionaries were constructed using a large bilingual word list of unchecked quality.\n sent1: Paradigms were hand-written according to #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Eq."
                ],
                [
                    "6",
                    "implies",
                    "that",
                    "the",
                    "abstractive",
                    "model",
                    "g",
                    "should",
                    "be",
                    "able",
                    "to",
                    "assign",
                    "higher",
                    "estimated",
                    "probability",
                    "to",
                    "the",
                    "better",
                    "candidate",
                    "summary",
                    "during",
                    "inference."
                ],
                [
                    "However,",
                    "this",
                    "intuition",
                    "is",
                    "not",
                    "directly",
                    "captured",
                    "in",
                    "the",
                    "standard",
                    "MLE",
                    "objective",
                    "used",
                    "in",
                    "training",
                    "-a",
                    "model",
                    "obtaining",
                    "zero",
                    "MLE",
                    "loss",
                    "would",
                    "assign",
                    "zero",
                    "probability",
                    "to",
                    "any",
                    "candidate",
                    "summary",
                    "different",
                    "from",
                    "the",
                    "reference."
                ],
                [
                    "This",
                    "is",
                    "obviously",
                    "improper",
                    "for",
                    "any",
                    "task",
                    "where",
                    "multiple",
                    "reasonable",
                    "generations",
                    "may",
                    "exist",
                    "#TARGET_REF",
                    ",",
                    "and",
                    "also",
                    "does",
                    "not",
                    "say",
                    "anything",
                    "about",
                    "the",
                    "ordering",
                    "of",
                    "two",
                    "imperfect",
                    "references."
                ],
                [
                    "We",
                    "therefore",
                    "advocate",
                    "for",
                    "making",
                    "the",
                    "alternative",
                    "assumption",
                    "that",
                    "the",
                    "probability",
                    "of",
                    "one",
                    "candidate",
                    "should",
                    "be",
                    "well-correlated",
                    "with",
                    "its",
                    "quality",
                    "as",
                    "evaluated",
                    "by",
                    "an",
                    "automatic",
                    "metric",
                    "M",
                    "."
                ],
                [
                    "Since",
                    "it",
                    "is",
                    "intractable",
                    "to",
                    "enumerate",
                    "all",
                    "the",
                    "possible",
                    "candidate",
                    "outputs,",
                    "we",
                    "only",
                    "require",
                    "our",
                    "model",
                    "to",
                    "be",
                    "able",
                    "to",
                    "accurately",
                    "predict",
                    "the",
                    "ranking",
                    "order",
                    "of",
                    "a",
                    "set",
                    "of",
                    "the",
                    "most",
                    "probable",
                    "candidate",
                    "summaries",
                    "Ŝ,",
                    "which",
                    "are",
                    "its",
                    "own",
                    "beam",
                    "search",
                    "results."
                ],
                [
                    "In",
                    "order",
                    "to",
                    "achieve",
                    "this",
                    "objective,",
                    "we",
                    "slightly",
                    "modify",
                    "the",
                    "conditions",
                    "of",
                    "Eq."
                ],
                [
                    "5,",
                    "maintaining",
                    "the",
                    "general",
                    "functional",
                    "form,",
                    "but",
                    "instead",
                    "specifying",
                    "the",
                    "marginal",
                    "probability",
                    "of",
                    "the",
                    "non-reference",
                    "candidates",
                    "S",
                    "to",
                    "be",
                    "β,",
                    "and",
                    "encouraging",
                    "coordination",
                    "of",
                    "probabilities",
                    "and",
                    "qualities",
                    "among",
                    "non-reference",
                    "candidates",
                    "as",
                    "follows:"
                ]
            ],
            "context": [
                3,
                3,
                3,
                2,
                2,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Eq.\n sent1: 6 implies that the abstractive model g should be able to assign higher estimated probability to the better candidate summary during inference.\n sent2: However, this intuition is not directly captured in the standard MLE objective used in training -a model obtaining zero MLE loss would assign zero probability to any candidate summary different from the reference.\n sent3: This is obviously improper for any task where multiple reasonable generations may exist #TARGET_REF , and also does not say anything about the ordering of two imperfect references.\n sent4: We therefore advocate for making the alternative assumption that the probability of one candidate should be well-correlated with its quality as evaluated by an automatic metric M .\n sent5: Since it is intractable to enumerate all the possible candidate outputs, we only require our model to be able to accurately predict the ranking order of a set of the most probable candidate summaries Ŝ, which are its own beam search results.\n sent6: In order to achieve this objective, we slightly modify the conditions of Eq.\n sent7: 5, maintaining the general functional form, but instead specifying the marginal probability of the non-reference candidates S to be β, and encouraging coordination of probabilities and qualities among non-reference candidates as follows:\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent3\", \"sent4\"], \"BACKGROUND\": [\"sent0\", \"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "For",
                    "future",
                    "work,",
                    "we",
                    "would",
                    "like",
                    "to",
                    "include",
                    "linguistic",
                    "features",
                    "at",
                    "the",
                    "source",
                    "language."
                ],
                [
                    "It",
                    "is",
                    "known",
                    "that",
                    "this",
                    "can",
                    "be",
                    "helpful",
                    "for",
                    "NMT",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Extending",
                    "the",
                    "approach",
                    "with",
                    "input",
                    "factors",
                    "could",
                    "make",
                    "the",
                    "target",
                    "language",
                    "factors",
                    "generation",
                    "better."
                ],
                [
                    "Furthermore,",
                    "different",
                    "attention",
                    "mechanisms",
                    "for",
                    "each",
                    "output",
                    "will",
                    "be",
                    "explored",
                    "because",
                    "they",
                    "could",
                    "be",
                    "aligned",
                    "to",
                    "different",
                    "source",
                    "words."
                ],
                [
                    "The",
                    "proposed",
                    "FNMT",
                    "architecture",
                    "could",
                    "even",
                    "show",
                    "better",
                    "performance",
                    "if",
                    "applied",
                    "when",
                    "we",
                    "translate",
                    "to",
                    "highly",
                    "inflected",
                    "languages",
                    "like",
                    "German,",
                    "Arabic,",
                    "Czech",
                    "or",
                    "Russian."
                ],
                [
                    "Finally,",
                    "FNMT",
                    "approach",
                    "will",
                    "be",
                    "explored",
                    "in",
                    "multimodal",
                    "and",
                    "multilingual",
                    "tasks."
                ]
            ],
            "context": [
                1,
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: For future work, we would like to include linguistic features at the source language.\n sent1: It is known that this can be helpful for NMT #TARGET_REF .\n sent2: Extending the approach with input factors could make the target language factors generation better.\n sent3: Furthermore, different attention mechanisms for each output will be explored because they could be aligned to different source words.\n sent4: The proposed FNMT architecture could even show better performance if applied when we translate to highly inflected languages like German, Arabic, Czech or Russian.\n sent5: Finally, FNMT approach will be explored in multimodal and multilingual tasks.\n",
        "output": "{\"INFORMATION\": [\"sent0\", \"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Toxicity",
                    "Datasets",
                    "in",
                    "Online",
                    "Community",
                    "An",
                    "extensive",
                    "body",
                    "of",
                    "work",
                    "has",
                    "focused",
                    "on",
                    "datasets",
                    "to",
                    "detect",
                    "toxicity",
                    "including",
                    "hate",
                    "speech",
                    "#REF",
                    "and",
                    "abusive",
                    "language",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "However,",
                    "the",
                    "majority",
                    "of",
                    "toxicity",
                    "datasets",
                    "do",
                    "not",
                    "consider",
                    "the",
                    "context",
                    "of",
                    "a",
                    "conversation,",
                    "instead",
                    "simply",
                    "analysing",
                    "a",
                    "single",
                    "utterance."
                ],
                [
                    "Even",
                    "if",
                    "a",
                    "model",
                    "uses",
                    "contextual",
                    "information",
                    "#REF",
                    ",",
                    "it",
                    "is",
                    "limited",
                    "to",
                    "metainformation",
                    "(e.g."
                ],
                [
                    "news",
                    "title",
                    "or",
                    "user",
                    "name)",
                    "which",
                    "is",
                    "not",
                    "sufficient",
                    "to",
                    "understand",
                    "a",
                    "conversation."
                ],
                [
                    "In",
                    "our",
                    "research,",
                    "context",
                    "is",
                    "defined",
                    "as",
                    "linguistic",
                    "contextual",
                    "information,",
                    "particularly",
                    "previous",
                    "single",
                    "or",
                    "multiple",
                    "utterances."
                ],
                [
                    "Along",
                    "similar",
                    "lines,",
                    "recent",
                    "studies",
                    "have",
                    "focused",
                    "on",
                    "conversation",
                    "aiming",
                    "to",
                    "discover",
                    "warning",
                    "signals",
                    "#REF",
                    ",",
                    "to",
                    "generate",
                    "intervention",
                    "responses",
                    "#REF",
                    ",",
                    "or",
                    "to",
                    "measure",
                    "the",
                    "importance",
                    "of",
                    "context",
                    "#REF",
                    "."
                ],
                [
                    "Existing",
                    "toxicity",
                    "datasets",
                    "mainly",
                    "focus",
                    "on",
                    "annotating",
                    "at",
                    "utterance-level,",
                    "whereas",
                    "ours",
                    "conducts",
                    "a",
                    "dual-level",
                    "annotation",
                    "at",
                    "utterance",
                    "and",
                    "token-level,",
                    "while",
                    "also",
                    "providing",
                    "a",
                    "conversation",
                    "history",
                    "(see",
                    "Table",
                    "1)."
                ],
                [
                    "These",
                    "extra",
                    "features",
                    "are",
                    "what",
                    "distinguish",
                    "CONDA."
                ]
            ],
            "context": [
                0,
                2,
                0,
                0,
                0,
                0,
                2,
                0
            ]
        },
        "input": "sent0: Toxicity Datasets in Online Community An extensive body of work has focused on datasets to detect toxicity including hate speech #REF and abusive language #TARGET_REF .\n sent1: However, the majority of toxicity datasets do not consider the context of a conversation, instead simply analysing a single utterance.\n sent2: Even if a model uses contextual information #REF , it is limited to metainformation (e.g.\n sent3: news title or user name) which is not sufficient to understand a conversation.\n sent4: In our research, context is defined as linguistic contextual information, particularly previous single or multiple utterances.\n sent5: Along similar lines, recent studies have focused on conversation aiming to discover warning signals #REF , to generate intervention responses #REF , or to measure the importance of context #REF .\n sent6: Existing toxicity datasets mainly focus on annotating at utterance-level, whereas ours conducts a dual-level annotation at utterance and token-level, while also providing a conversation history (see Table 1).\n sent7: These extra features are what distinguish CONDA.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\", \"sent6\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Proposed",
                    "model",
                    "I",
                    "propose",
                    "a",
                    "log-bilinear",
                    "model",
                    "#TARGET_REF",
                    "using",
                    "word",
                    "embeddings",
                    "as",
                    "input:",
                    "1"
                ]
            ],
            "context": [
                2
            ]
        },
        "input": "sent0: Proposed model I propose a log-bilinear model #TARGET_REF using word embeddings as input: 1\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "DeKo",
                    "(for",
                    "Derivation",
                    "and",
                    "Komposition,",
                    "funded",
                    "by",
                    "the",
                    "state",
                    "of",
                    "Baden-Württemberg",
                    "from",
                    "Jan",
                    "2000",
                    "-June",
                    "2001)",
                    "is",
                    "designed",
                    "as",
                    "a",
                    "German",
                    "word",
                    "formation",
                    "component",
                    "in",
                    "a",
                    "larger",
                    "computational",
                    "linguistic",
                    "application."
                ],
                [
                    "13",
                    "It",
                    "was",
                    "done",
                    "on",
                    "a",
                    "much",
                    "smaller",
                    "scale",
                    "than",
                    "Word",
                    "Manager",
                    "and",
                    "is",
                    "not",
                    "used",
                    "in",
                    "any",
                    "commercial",
                    "products."
                ],
                [
                    "In",
                    "this",
                    "tutorial",
                    "we",
                    "focus",
                    "on",
                    "some",
                    "basic",
                    "design",
                    "features",
                    "-especially",
                    "where",
                    "they",
                    "differ",
                    "from",
                    "Word",
                    "Manager's",
                    "features."
                ],
                [
                    "More",
                    "details",
                    "can",
                    "be",
                    "found",
                    "in",
                    "#TARGET_REF",
                    "and",
                    "at",
                    "http://www.ims.uni-stuttgart.de/projekte/DeKo/."
                ],
                [
                    "Here",
                    "we",
                    "want",
                    "to",
                    "concentrate",
                    "on",
                    "the",
                    "corpus-based",
                    "acquisition",
                    "of",
                    "data,",
                    "the",
                    "item-and-arrangement",
                    "design,",
                    "analysis",
                    "and",
                    "structure,",
                    "and",
                    "the",
                    "interaction",
                    "between",
                    "DeKo",
                    "and",
                    "the",
                    "lexicon."
                ],
                [
                    "Although",
                    "the",
                    "DeKo",
                    "project",
                    "proper",
                    "is",
                    "finished,",
                    "work",
                    "is",
                    "still",
                    "being",
                    "done",
                    "to",
                    "improve",
                    "the",
                    "program",
                    "and",
                    "especially",
                    "to",
                    "extend",
                    "the",
                    "lexicon."
                ]
            ],
            "context": [
                0,
                0,
                0,
                2,
                0,
                0
            ]
        },
        "input": "sent0: DeKo (for Derivation and Komposition, funded by the state of Baden-Württemberg from Jan 2000 -June 2001) is designed as a German word formation component in a larger computational linguistic application.\n sent1: 13 It was done on a much smaller scale than Word Manager and is not used in any commercial products.\n sent2: In this tutorial we focus on some basic design features -especially where they differ from Word Manager's features.\n sent3: More details can be found in #TARGET_REF and at http://www.ims.uni-stuttgart.de/projekte/DeKo/.\n sent4: Here we want to concentrate on the corpus-based acquisition of data, the item-and-arrangement design, analysis and structure, and the interaction between DeKo and the lexicon.\n sent5: Although the DeKo project proper is finished, work is still being done to improve the program and especially to extend the lexicon.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent3\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Here,",
                    "we",
                    "summarize",
                    "the",
                    "pre-training",
                    "tasks",
                    "for",
                    "the",
                    "encoders",
                    "mentioned",
                    "in",
                    "Section",
                    "4.2."
                ],
                [
                    "These",
                    "tasks",
                    "are",
                    "unchanged",
                    "from",
                    "those",
                    "described",
                    "in",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                2,
                3
            ]
        },
        "input": "sent0: Here, we summarize the pre-training tasks for the encoders mentioned in Section 4.2.\n sent1: These tasks are unchanged from those described in #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Post-editing",
                    "research",
                    "has",
                    "so",
                    "far",
                    "focused",
                    "on",
                    "mainstream",
                    "languages."
                ],
                [
                    "An",
                    "added",
                    "challenge",
                    "of",
                    "this",
                    "work",
                    "is",
                    "the",
                    "use",
                    "of",
                    "an",
                    "English",
                    "to",
                    "Basque",
                    "MT",
                    "system",
                    "for",
                    "post-editing."
                ],
                [
                    "Research",
                    "on",
                    "Basque",
                    "MT",
                    "has",
                    "been",
                    "ongoing",
                    "for",
                    "a",
                    "few",
                    "years",
                    "now",
                    "(Diaz",
                    "de",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "However,",
                    "Basque",
                    "being",
                    "a",
                    "lowresourced",
                    "language,",
                    "researchers",
                    "and",
                    "developers",
                    "have",
                    "found",
                    "themselves",
                    "with",
                    "limited",
                    "resources",
                    "to",
                    "build",
                    "competitive",
                    "MT",
                    "systems",
                    "and",
                    "automated",
                    "translation",
                    "has",
                    "not",
                    "been",
                    "included",
                    "within",
                    "the",
                    "translation",
                    "processes",
                    "of",
                    "local",
                    "LSPs",
                    "yet."
                ],
                [
                    "To",
                    "our",
                    "knowledge,",
                    "this",
                    "is",
                    "the",
                    "first",
                    "(open)",
                    "productivity",
                    "experiment",
                    "done",
                    "for",
                    "the",
                    "English",
                    "to",
                    "Basque",
                    "translation",
                    "direction."
                ],
                [
                    "#REF",
                    "pointed",
                    "out",
                    "the",
                    "existence",
                    "of",
                    "many",
                    "communities",
                    "that",
                    "could",
                    "benefit",
                    "greatly",
                    "from",
                    "machine",
                    "translation",
                    "but,",
                    "as",
                    "in",
                    "the",
                    "case",
                    "presented",
                    "in",
                    "this",
                    "work,",
                    "have",
                    "not",
                    "yet",
                    "started",
                    "to",
                    "use",
                    "it,",
                    "as",
                    "authors",
                    "suggest,",
                    "either",
                    "due",
                    "to",
                    "lack",
                    "of",
                    "awareness",
                    "or",
                    "barriers",
                    "to",
                    "adoption."
                ],
                [
                    "The",
                    "work",
                    "in",
                    "#REF",
                    "presents",
                    "a",
                    "feasibility",
                    "study",
                    "to",
                    "introduce",
                    "MT",
                    "coupled",
                    "with",
                    "post-editing",
                    "in",
                    "local",
                    "and",
                    "regional",
                    "health",
                    "departments",
                    "in",
                    "the",
                    "United",
                    "States."
                ],
                [
                    "It",
                    "highlights",
                    "a",
                    "number",
                    "of",
                    "requirements",
                    "the",
                    "translation",
                    "platform",
                    "should",
                    "address,",
                    "such",
                    "as",
                    "being",
                    "intuitive",
                    "and",
                    "easy",
                    "to",
                    "install,",
                    "allowing",
                    "users",
                    "to",
                    "share",
                    "ongoing",
                    "and",
                    "completed",
                    "jobs."
                ],
                [
                    "Our",
                    "work",
                    "builds",
                    "on",
                    "this",
                    "first",
                    "feasibility",
                    "study",
                    "and",
                    "goes",
                    "a",
                    "step",
                    "forward",
                    "by",
                    "assessing",
                    "the",
                    "actual",
                    "translation",
                    "performance."
                ],
                [
                    "We",
                    "identify",
                    "a",
                    "suitable",
                    "tool",
                    "for",
                    "our",
                    "users",
                    "that",
                    "is",
                    "intuitive,",
                    "easy",
                    "to",
                    "access",
                    "and",
                    "allows",
                    "sharing",
                    "translation",
                    "resources",
                    "such",
                    "as",
                    "translation",
                    "memories",
                    "(TM)",
                    "or",
                    "specialized",
                    "MT",
                    "engines",
                    "and",
                    "measure",
                    "productivity",
                    "gain,",
                    "while",
                    "comparing",
                    "it",
                    "with",
                    "the",
                    "performance",
                    "of",
                    "professional",
                    "translators."
                ]
            ],
            "context": [
                3,
                3,
                1,
                1,
                2,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Post-editing research has so far focused on mainstream languages.\n sent1: An added challenge of this work is the use of an English to Basque MT system for post-editing.\n sent2: Research on Basque MT has been ongoing for a few years now (Diaz de #TARGET_REF .\n sent3: However, Basque being a lowresourced language, researchers and developers have found themselves with limited resources to build competitive MT systems and automated translation has not been included within the translation processes of local LSPs yet.\n sent4: To our knowledge, this is the first (open) productivity experiment done for the English to Basque translation direction.\n sent5: #REF pointed out the existence of many communities that could benefit greatly from machine translation but, as in the case presented in this work, have not yet started to use it, as authors suggest, either due to lack of awareness or barriers to adoption.\n sent6: The work in #REF presents a feasibility study to introduce MT coupled with post-editing in local and regional health departments in the United States.\n sent7: It highlights a number of requirements the translation platform should address, such as being intuitive and easy to install, allowing users to share ongoing and completed jobs.\n sent8: Our work builds on this first feasibility study and goes a step forward by assessing the actual translation performance.\n sent9: We identify a suitable tool for our users that is intuitive, easy to access and allows sharing translation resources such as translation memories (TM) or specialized MT engines and measure productivity gain, while comparing it with the performance of professional translators.\n",
        "output": "{\"INFORMATION\": [\"sent2\", \"sent3\"], \"PERCEPTION\": [\"sent4\"], \"BACKGROUND\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Gendered",
                    "Word",
                    "Lists",
                    "We",
                    "develop",
                    "our",
                    "bias",
                    "measure",
                    "using",
                    "dimensionality-reduction",
                    "over",
                    "six",
                    "existing",
                    "lists",
                    "of",
                    "gender-laden",
                    "words:",
                    "(1,",
                    "2)",
                    "Gender-Coded",
                    "Word",
                    "Prevalence:",
                    "#REF",
                    "define",
                    "masculine-and-feminine-themed",
                    "words",
                    "from",
                    "an",
                    "experiment",
                    "on",
                    "job",
                    "ads",
                    "that",
                    "discouraged",
                    "female",
                    "applicants."
                ],
                [
                    "(3)",
                    "Superlative",
                    "Prevalence",
                    "#REF",
                    "assess",
                    "the",
                    "relative",
                    "frequency",
                    "of",
                    "positive",
                    "and",
                    "negative",
                    "superlatives",
                    "used",
                    "to",
                    "describe",
                    "male",
                    "versus",
                    "female",
                    "job",
                    "candidates",
                    "in",
                    "recommendation",
                    "letters."
                ],
                [
                    "We",
                    "use",
                    "an",
                    "established",
                    "set",
                    "of",
                    "superlative",
                    "words",
                    "#REF",
                    "."
                ],
                [
                    "(4)",
                    "Gender-Laden",
                    "Scoring:",
                    "#TARGET_REF",
                    "analyse",
                    "32",
                    "properties",
                    "related",
                    "to",
                    "a",
                    "set",
                    "of",
                    "norms",
                    "to",
                    "score",
                    "2,311",
                    "words",
                    "based",
                    "on",
                    "their",
                    "\"gender-ladenness\"."
                ],
                [
                    "(5)",
                    "Connotation",
                    "Frames:",
                    "Sap",
                    "et",
                    "al."
                ],
                [
                    "(",
                    "2017)",
                    "define",
                    "linguistic",
                    "markers",
                    "of",
                    "power",
                    "and",
                    "agency",
                    "associated",
                    "with",
                    "female",
                    "versus",
                    "male",
                    "characters",
                    "in",
                    "modern",
                    "films."
                ],
                [
                    "(6)",
                    "NRC",
                    "VAD",
                    "Lexicon:",
                    "Mohammad",
                    "(2018)",
                    "presents",
                    "a",
                    "lexicon",
                    "of",
                    "words",
                    "coded",
                    "by",
                    "valence,",
                    "arousal,",
                    "and",
                    "dominance",
                    "whose",
                    "interpretation",
                    "may",
                    "interact",
                    "with",
                    "gender."
                ],
                [
                    "5",
                    "Dimensionality",
                    "Reduction",
                    "We",
                    "employ",
                    "principal",
                    "component",
                    "analysis",
                    "(PCA)",
                    "on",
                    "the",
                    "six",
                    "bias",
                    "measures",
                    "on",
                    "real-world",
                    "job",
                    "ads",
                    "to",
                    "collapse",
                    "them",
                    "into",
                    "interpretable",
                    "components."
                ],
                [
                    "We",
                    "then",
                    "replicate",
                    "the",
                    "PCA",
                    "on",
                    "synthetic",
                    "job",
                    "ads",
                    "(zero-shot)",
                    "and",
                    "project",
                    "all",
                    "data",
                    "points",
                    "onto",
                    "the",
                    "first",
                    "two",
                    "principal",
                    "components",
                    "of",
                    "real",
                    "job",
                    "ads",
                    "and",
                    "vice",
                    "versa."
                ]
            ],
            "context": [
                2,
                0,
                0,
                1,
                3,
                0,
                0,
                2,
                0
            ]
        },
        "input": "sent0: Gendered Word Lists We develop our bias measure using dimensionality-reduction over six existing lists of gender-laden words: (1, 2) Gender-Coded Word Prevalence: #REF define masculine-and-feminine-themed words from an experiment on job ads that discouraged female applicants.\n sent1: (3) Superlative Prevalence #REF assess the relative frequency of positive and negative superlatives used to describe male versus female job candidates in recommendation letters.\n sent2: We use an established set of superlative words #REF .\n sent3: (4) Gender-Laden Scoring: #TARGET_REF analyse 32 properties related to a set of norms to score 2,311 words based on their \"gender-ladenness\".\n sent4: (5) Connotation Frames: Sap et al.\n sent5: ( 2017) define linguistic markers of power and agency associated with female versus male characters in modern films.\n sent6: (6) NRC VAD Lexicon: Mohammad (2018) presents a lexicon of words coded by valence, arousal, and dominance whose interpretation may interact with gender.\n sent7: 5 Dimensionality Reduction We employ principal component analysis (PCA) on the six bias measures on real-world job ads to collapse them into interpretable components.\n sent8: We then replicate the PCA on synthetic job ads (zero-shot) and project all data points onto the first two principal components of real job ads and vice versa.\n",
        "output": "{\"INFORMATION\": [\"sent3\"], \"PERCEPTION\": [\"sent0\", \"sent7\"], \"BACKGROUND\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "and",
                    "translators'",
                    "satisfaction."
                ],
                [
                    "#REF",
                    "does",
                    "not",
                    "discuss",
                    "LSP",
                    "costs",
                    "for",
                    "internal",
                    "MT",
                    "development."
                ],
                [
                    "He",
                    "emphasizes",
                    "on",
                    "margin",
                    "shrinking,",
                    "which",
                    "is",
                    "directly",
                    "linked",
                    "to",
                    "investment",
                    "gain."
                ],
                [
                    "Hunnect's",
                    "translation",
                    "provision",
                    "process",
                    "involves",
                    "three",
                    "tasks",
                    "-translation,",
                    "bilingual",
                    "editing",
                    "and",
                    "proofing."
                ],
                [
                    "The",
                    "time",
                    "estimation",
                    "of",
                    "these",
                    "tasks",
                    "per",
                    "order,",
                    "shown",
                    "in",
                    "percentage,",
                    "is",
                    "the",
                    "following:",
                    "translation",
                    "requires",
                    "50%",
                    "of",
                    "the",
                    "time,",
                    "editing",
                    "-20%,",
                    "proofing",
                    "-5%,",
                    "and",
                    "the",
                    "rest",
                    "25%",
                    "is",
                    "the",
                    "marginal",
                    "buffer",
                    "time."
                ],
                [
                    "The",
                    "time",
                    "management",
                    "planning",
                    "for",
                    "the",
                    "post-edited",
                    "MT",
                    "output",
                    "is",
                    "as",
                    "follows:",
                    "20%",
                    "of",
                    "the",
                    "time",
                    "is",
                    "devoted",
                    "to",
                    "translation,",
                    "30%",
                    "-for",
                    "post-editing",
                    "(PE),",
                    "5%",
                    "for",
                    "proofing."
                ],
                [
                    "The",
                    "other",
                    "45%",
                    "of",
                    "the",
                    "time",
                    "is",
                    "the",
                    "margin",
                    "time",
                    "until",
                    "delivery."
                ],
                [
                    "In",
                    "reality,",
                    "Sojnóczky",
                    "(2013)",
                    "observes",
                    "the",
                    "margin",
                    "time",
                    "left",
                    "after",
                    "translation,",
                    "postediting",
                    "and",
                    "proofing",
                    "for",
                    "Hunnect's",
                    "case",
                    "is",
                    "50%."
                ],
                [
                    "Therefore,",
                    "the",
                    "use",
                    "of",
                    "MT",
                    "in",
                    "the",
                    "translation",
                    "process",
                    "reduced",
                    "the",
                    "delivery",
                    "time",
                    "or",
                    "35%",
                    "savings",
                    "of",
                    "time."
                ],
                [
                    "35%",
                    "savings",
                    "of",
                    "time",
                    "results",
                    "in",
                    "…",
                    "quicker",
                    "potential",
                    "ROI."
                ],
                [
                    "The",
                    "vendor",
                    "pricing",
                    "scheme",
                    "implemented",
                    "by",
                    "#REF",
                    "is",
                    "that",
                    "postedited",
                    "MT",
                    "is",
                    "paid",
                    "60%",
                    "of",
                    "the",
                    "original",
                    "price."
                ],
                [
                    "Consequently,",
                    "the",
                    "company",
                    "cost",
                    "for",
                    "delivering",
                    "an",
                    "order",
                    "is",
                    "reduced",
                    "by",
                    "40%",
                    "and",
                    "time",
                    "to",
                    "delivery",
                    "is",
                    "increased",
                    "by",
                    "35%."
                ],
                [
                    "As",
                    "mentioned",
                    "in",
                    "Section",
                    "7,",
                    "managers",
                    "need",
                    "to",
                    "be",
                    "careful",
                    "when",
                    "developing",
                    "vendor",
                    "pricing",
                    "model",
                    "strategies",
                    "as",
                    "it",
                    "is",
                    "a",
                    "sensitive",
                    "matter."
                ],
                [
                    "Additionally,",
                    "Sojnóczky",
                    "(2013)",
                    "observes",
                    "68%",
                    "increase",
                    "of",
                    "productivity",
                    "among",
                    "trained",
                    "to",
                    "post-edit",
                    "translators,",
                    "which",
                    "reflects",
                    "directly",
                    "the",
                    "translators'",
                    "pay."
                ],
                [
                    "The",
                    "statistics",
                    "#TARGET_REF",
                    "observes",
                    "show",
                    "that",
                    "the",
                    "translators'",
                    "pay",
                    "has",
                    "increased",
                    "with",
                    "at",
                    "least",
                    "1%",
                    "instead",
                    "of",
                    "decreasing",
                    "as",
                    "feared."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "sent0: and translators' satisfaction.\n sent1: #REF does not discuss LSP costs for internal MT development.\n sent2: He emphasizes on margin shrinking, which is directly linked to investment gain.\n sent3: Hunnect's translation provision process involves three tasks -translation, bilingual editing and proofing.\n sent4: The time estimation of these tasks per order, shown in percentage, is the following: translation requires 50% of the time, editing -20%, proofing -5%, and the rest 25% is the marginal buffer time.\n sent5: The time management planning for the post-edited MT output is as follows: 20% of the time is devoted to translation, 30% -for post-editing (PE), 5% for proofing.\n sent6: The other 45% of the time is the margin time until delivery.\n sent7: In reality, Sojnóczky (2013) observes the margin time left after translation, postediting and proofing for Hunnect's case is 50%.\n sent8: Therefore, the use of MT in the translation process reduced the delivery time or 35% savings of time.\n sent9: 35% savings of time results in … quicker potential ROI.\n sent10: The vendor pricing scheme implemented by #REF is that postedited MT is paid 60% of the original price.\n sent11: Consequently, the company cost for delivering an order is reduced by 40% and time to delivery is increased by 35%.\n sent12: As mentioned in Section 7, managers need to be careful when developing vendor pricing model strategies as it is a sensitive matter.\n sent13: Additionally, Sojnóczky (2013) observes 68% increase of productivity among trained to post-edit translators, which reflects directly the translators' pay.\n sent14: The statistics #TARGET_REF observes show that the translators' pay has increased with at least 1% instead of decreasing as feared.\n",
        "output": "{\"INFORMATION\": [\"sent14\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "•",
                    "Random",
                    "named",
                    "entity:",
                    "the",
                    "majority",
                    "of",
                    "answers",
                    "in",
                    "Quoref",
                    "are",
                    "person",
                    "names."
                ],
                [
                    "To",
                    "evaluate",
                    "this",
                    "artifact,",
                    "we",
                    "randomly",
                    "select",
                    "a",
                    "PERSON",
                    "named",
                    "entity",
                    "from",
                    "the",
                    "context",
                    "as",
                    "the",
                    "answer."
                ],
                [
                    "3",
                    "•",
                    "Wh-word",
                    "#REF",
                    ":",
                    "to",
                    "recognize",
                    "the",
                    "QA",
                    "pairs",
                    "that",
                    "can",
                    "be",
                    "answered",
                    "by",
                    "only",
                    "using",
                    "the",
                    "interrogative",
                    "adverbs",
                    "from",
                    "the",
                    "question,",
                    "we",
                    "train",
                    "a",
                    "model",
                    "on",
                    "a",
                    "variation",
                    "of",
                    "the",
                    "training",
                    "dataset",
                    "in",
                    "which",
                    "questions",
                    "only",
                    "contain",
                    "interrogative",
                    "adverbs."
                ],
                [
                    "•",
                    "Empty",
                    "question",
                    "#TARGET_REF",
                    ":",
                    "to",
                    "recognize",
                    "QA",
                    "pairs",
                    "that",
                    "are",
                    "answerable",
                    "without",
                    "considering",
                    "the",
                    "question,",
                    "4",
                    "we",
                    "train",
                    "a",
                    "QA",
                    "model",
                    "only",
                    "on",
                    "the",
                    "contexts",
                    "and",
                    "without",
                    "questions."
                ],
                [
                    "•",
                    "Semantic",
                    "overlap",
                    "#REF",
                    ":",
                    "for",
                    "this",
                    "artifact,",
                    "we",
                    "report",
                    "the",
                    "ratio",
                    "of",
                    "the",
                    "QA",
                    "pairs",
                    "whose",
                    "answers",
                    "lie",
                    "in",
                    "the",
                    "sentence",
                    "of",
                    "the",
                    "context",
                    "that",
                    "has",
                    "the",
                    "highest",
                    "semantic",
                    "similarity",
                    "to",
                    "the",
                    "question."
                ],
                [
                    "We",
                    "use",
                    "sentence-BERT",
                    "#REF",
                    "to",
                    "find",
                    "the",
                    "most",
                    "similar",
                    "sentence."
                ],
                [
                    "•",
                    "Short",
                    "distance",
                    "reasoning:",
                    "for",
                    "this",
                    "bias,",
                    "we",
                    "train",
                    "a",
                    "model",
                    "only",
                    "using",
                    "the",
                    "sentence",
                    "of",
                    "the",
                    "context",
                    "that",
                    "is",
                    "the",
                    "most",
                    "similar",
                    "to",
                    "the",
                    "question,",
                    "instead",
                    "of",
                    "the",
                    "whole",
                    "context."
                ],
                [
                    "We",
                    "exclude",
                    "the",
                    "question-answer",
                    "pairs",
                    "in",
                    "which",
                    "the",
                    "most",
                    "similar",
                    "sentence",
                    "does",
                    "not",
                    "contain",
                    "the",
                    "answer."
                ],
                [
                    "This",
                    "model",
                    "will",
                    "not",
                    "learn",
                    "to",
                    "perform",
                    "coreference",
                    "reasoning",
                    "when",
                    "the",
                    "related",
                    "coreferring",
                    "pairs",
                    "are",
                    "not",
                    "in",
                    "the",
                    "same",
                    "sentence."
                ]
            ],
            "context": [
                0,
                0,
                0,
                1,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: • Random named entity: the majority of answers in Quoref are person names.\n sent1: To evaluate this artifact, we randomly select a PERSON named entity from the context as the answer.\n sent2: 3 • Wh-word #REF : to recognize the QA pairs that can be answered by only using the interrogative adverbs from the question, we train a model on a variation of the training dataset in which questions only contain interrogative adverbs.\n sent3: • Empty question #TARGET_REF : to recognize QA pairs that are answerable without considering the question, 4 we train a QA model only on the contexts and without questions.\n sent4: • Semantic overlap #REF : for this artifact, we report the ratio of the QA pairs whose answers lie in the sentence of the context that has the highest semantic similarity to the question.\n sent5: We use sentence-BERT #REF to find the most similar sentence.\n sent6: • Short distance reasoning: for this bias, we train a model only using the sentence of the context that is the most similar to the question, instead of the whole context.\n sent7: We exclude the question-answer pairs in which the most similar sentence does not contain the answer.\n sent8: This model will not learn to perform coreference reasoning when the related coreferring pairs are not in the same sentence.\n",
        "output": "{\"INFORMATION\": [\"sent3\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "An",
                    "efficient",
                    "and",
                    "effective",
                    "document",
                    "retriever",
                    "is",
                    "required",
                    "since",
                    "the",
                    "Wikipedia",
                    "dump",
                    "containing",
                    "millions",
                    "of",
                    "pages."
                ],
                [
                    "We",
                    "first",
                    "narrow",
                    "the",
                    "search",
                    "space",
                    "to",
                    "several",
                    "hundred",
                    "pages",
                    "(m",
                    "0",
                    ")",
                    "with",
                    "an",
                    "efficient",
                    "information",
                    "retrieval",
                    "method",
                    "based",
                    "on",
                    "TF-IDF,",
                    "namely,",
                    "DRQA",
                    "#REF",
                    "."
                ],
                [
                    "A",
                    "RoBERTa-based",
                    "re-ranker",
                    "#TARGET_REF",
                    "and",
                    "a",
                    "BM25-based",
                    "re-ranker",
                    "are",
                    "then",
                    "applied",
                    "in",
                    "parallel",
                    "to",
                    "re-rank",
                    "the",
                    "m",
                    "0",
                    "document",
                    "candidates."
                ],
                [
                    "We",
                    "combine",
                    "the",
                    "results",
                    "of",
                    "two",
                    "re-rankers",
                    "and",
                    "keep",
                    "top",
                    "m",
                    "documents",
                    "since",
                    "BM25",
                    "focuses",
                    "more",
                    "on",
                    "entity",
                    "matching",
                    "and",
                    "RoBERTa-based",
                    "re-ranker",
                    "pays",
                    "more",
                    "attention",
                    "to",
                    "the",
                    "overall",
                    "sentence",
                    "structure."
                ],
                [
                    "The",
                    "document",
                    "scores",
                    "are",
                    "calculated",
                    "as",
                    "the",
                    "sum",
                    "of",
                    "their",
                    "rankings",
                    "in",
                    "the",
                    "two",
                    "re-rankers."
                ],
                [
                    "Documents",
                    "with",
                    "lower",
                    "scores",
                    "have",
                    "higher",
                    "priority."
                ]
            ],
            "context": [
                0,
                0,
                2,
                2,
                0,
                0
            ]
        },
        "input": "sent0: An efficient and effective document retriever is required since the Wikipedia dump containing millions of pages.\n sent1: We first narrow the search space to several hundred pages (m 0 ) with an efficient information retrieval method based on TF-IDF, namely, DRQA #REF .\n sent2: A RoBERTa-based re-ranker #TARGET_REF and a BM25-based re-ranker are then applied in parallel to re-rank the m 0 document candidates.\n sent3: We combine the results of two re-rankers and keep top m documents since BM25 focuses more on entity matching and RoBERTa-based re-ranker pays more attention to the overall sentence structure.\n sent4: The document scores are calculated as the sum of their rankings in the two re-rankers.\n sent5: Documents with lower scores have higher priority.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent2\", \"sent3\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Linguistic",
                    "variations."
                ],
                [
                    "Another",
                    "aspect",
                    "comes",
                    "from",
                    "looking",
                    "at",
                    "implicit",
                    "abuse,",
                    "whereby",
                    "a",
                    "user",
                    "may",
                    "utilize",
                    "novel",
                    "slangs",
                    "or",
                    "conventional",
                    "words",
                    "in",
                    "unconventional",
                    "ways,",
                    "e.g.,",
                    "as",
                    "a",
                    "racial",
                    "slur",
                    "or",
                    "as",
                    "a",
                    "name",
                    "for",
                    "some",
                    "specific",
                    "demographic",
                    "#REF",
                    "."
                ],
                [
                    "Information",
                    "about",
                    "how",
                    "a",
                    "term",
                    "is",
                    "being",
                    "used",
                    "by",
                    "other",
                    "members",
                    "of",
                    "a",
                    "user's",
                    "community,",
                    "e.g.,",
                    "in",
                    "abusive",
                    "contexts",
                    "or",
                    "otherwise,",
                    "can",
                    "help",
                    "decipher",
                    "linguistic",
                    "variations",
                    "that",
                    "come",
                    "up",
                    "from",
                    "time",
                    "to",
                    "time."
                ],
                [
                    "In",
                    "fact,",
                    "it",
                    "is",
                    "usually",
                    "the",
                    "users",
                    "with",
                    "strong",
                    "ties",
                    "who",
                    "are",
                    "responsible",
                    "for",
                    "popularizing",
                    "language",
                    "variations",
                    "as",
                    "well",
                    "as",
                    "for",
                    "spreading",
                    "hate",
                    "speech",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Therefore,",
                    "having",
                    "user",
                    "and",
                    "community",
                    "information",
                    "alongside",
                    "linguistic",
                    "features",
                    "helps",
                    "capture",
                    "linguistic",
                    "variations",
                    "and",
                    "their",
                    "diffusion."
                ]
            ],
            "context": [
                3,
                3,
                3,
                2,
                2
            ]
        },
        "input": "sent0: Linguistic variations.\n sent1: Another aspect comes from looking at implicit abuse, whereby a user may utilize novel slangs or conventional words in unconventional ways, e.g., as a racial slur or as a name for some specific demographic #REF .\n sent2: Information about how a term is being used by other members of a user's community, e.g., in abusive contexts or otherwise, can help decipher linguistic variations that come up from time to time.\n sent3: In fact, it is usually the users with strong ties who are responsible for popularizing language variations as well as for spreading hate speech #TARGET_REF .\n sent4: Therefore, having user and community information alongside linguistic features helps capture linguistic variations and their diffusion.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent3\", \"sent4\"], \"BACKGROUND\": [\"sent0\", \"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Natural",
                    "language",
                    "generation",
                    "(NLG)",
                    "is",
                    "gaining",
                    "increasing",
                    "attention",
                    "in",
                    "the",
                    "NLP",
                    "community",
                    "thanks",
                    "to",
                    "its",
                    "intriguing",
                    "complexity",
                    "and",
                    "central",
                    "role",
                    "in",
                    "many",
                    "tasks",
                    "and",
                    "applications."
                ],
                [
                    "Recently,",
                    "generative",
                    "adversarial",
                    "networks",
                    "(GANs)",
                    "#REF",
                    "have",
                    "started",
                    "to",
                    "display",
                    "promising",
                    "performance",
                    "also",
                    "in",
                    "NLG."
                ],
                [
                    "GANs",
                    "leverage",
                    "a",
                    "form",
                    "of",
                    "adversarial",
                    "learning",
                    "where",
                    "a",
                    "generator",
                    "incrementally",
                    "learns",
                    "to",
                    "generate",
                    "realistic",
                    "samples,",
                    "while",
                    "a",
                    "discriminator",
                    "simultaneously",
                    "learns",
                    "to",
                    "discriminate",
                    "between",
                    "real",
                    "and",
                    "generated",
                    "data."
                ],
                [
                    "They",
                    "had",
                    "originally",
                    "been",
                    "proposed",
                    "as",
                    "a",
                    "generative",
                    "approach",
                    "for",
                    "continuous",
                    "data,",
                    "such",
                    "as",
                    "images,",
                    "but",
                    "have",
                    "later",
                    "found",
                    "application",
                    "also",
                    "for",
                    "discrete",
                    "data,",
                    "despite",
                    "their",
                    "well-known",
                    "\"non-differentiability",
                    "issue\"."
                ],
                [
                    "In",
                    "fact,",
                    "several",
                    "GANs",
                    "have",
                    "recently",
                    "been",
                    "proposed",
                    "for",
                    "text",
                    "generation",
                    "#REF",
                    "and",
                    "have",
                    "achieved",
                    "encouraging",
                    "results",
                    "in",
                    "comparison",
                    "to",
                    "comparable",
                    "maximum",
                    "likelihood",
                    "approaches,",
                    "in",
                    "particular,",
                    "RelGAN",
                    "#TARGET_REF",
                    "has",
                    "outperformed",
                    "state-of-theart",
                    "(SOTA)",
                    "results."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                2
            ]
        },
        "input": "sent0: Natural language generation (NLG) is gaining increasing attention in the NLP community thanks to its intriguing complexity and central role in many tasks and applications.\n sent1: Recently, generative adversarial networks (GANs) #REF have started to display promising performance also in NLG.\n sent2: GANs leverage a form of adversarial learning where a generator incrementally learns to generate realistic samples, while a discriminator simultaneously learns to discriminate between real and generated data.\n sent3: They had originally been proposed as a generative approach for continuous data, such as images, but have later found application also for discrete data, despite their well-known \"non-differentiability issue\".\n sent4: In fact, several GANs have recently been proposed for text generation #REF and have achieved encouraging results in comparison to comparable maximum likelihood approaches, in particular, RelGAN #TARGET_REF has outperformed state-of-theart (SOTA) results.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent4\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "•",
                    "Byte-pair-encoding",
                    "(BPE)",
                    "#TARGET_REF",
                    "6",
                    ":",
                    "For",
                    "both",
                    "the",
                    "Chinese",
                    "and",
                    "English",
                    "sides,",
                    "we",
                    "use",
                    "BPE",
                    "with",
                    "32K",
                    "operations."
                ]
            ],
            "context": [
                1
            ]
        },
        "input": "sent0: • Byte-pair-encoding (BPE) #TARGET_REF 6 : For both the Chinese and English sides, we use BPE with 32K operations.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "One",
                    "of",
                    "the",
                    "central",
                    "subjects",
                    "of",
                    "artificial",
                    "intelligence",
                    "research",
                    "has",
                    "long",
                    "been",
                    "the",
                    "development",
                    "of",
                    "agents",
                    "that",
                    "play",
                    "various",
                    "games",
                    "at",
                    "the",
                    "human",
                    "level",
                    "or",
                    "better."
                ],
                [
                    "Most",
                    "studies",
                    "in",
                    "the",
                    "field",
                    "focus",
                    "on",
                    "combinatorial",
                    "games,",
                    "that",
                    "can",
                    "be",
                    "easily",
                    "formalized",
                    "mathematically,",
                    "such",
                    "as",
                    "chess",
                    "and",
                    "go",
                    "(see,",
                    "for",
                    "example,",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "The",
                    "popular",
                    "board",
                    "game",
                    "Codenames",
                    "is",
                    "different",
                    "from",
                    "these",
                    "in",
                    "many",
                    "aspects",
                    "and",
                    "may",
                    "provide",
                    "an",
                    "excellent",
                    "experimental",
                    "ground",
                    "in",
                    "areas",
                    "such",
                    "as",
                    "predicting",
                    "human",
                    "behavior",
                    "or",
                    "implementing",
                    "human-machine",
                    "cooperation."
                ]
            ],
            "context": [
                0,
                2,
                0
            ]
        },
        "input": "sent0: One of the central subjects of artificial intelligence research has long been the development of agents that play various games at the human level or better.\n sent1: Most studies in the field focus on combinatorial games, that can be easily formalized mathematically, such as chess and go (see, for example, #TARGET_REF .\n sent2: The popular board game Codenames is different from these in many aspects and may provide an excellent experimental ground in areas such as predicting human behavior or implementing human-machine cooperation.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "the",
                    "same",
                    "direction",
                    "but",
                    "with",
                    "the",
                    "other",
                    "output",
                    "information,",
                    "we",
                    "used",
                    "only",
                    "the",
                    "factors",
                    "embedding",
                    "as",
                    "feedback",
                    "(see",
                    "equation",
                    "6)."
                ],
                [
                    "fb(y",
                    "t−1",
                    ")",
                    "=",
                    "y",
                    "F",
                    "t−1",
                    "#TARGET_REF",
                    "where",
                    "y",
                    "F",
                    "t−1",
                    "is",
                    "the",
                    "embedding",
                    "of",
                    "the",
                    "factors",
                    "generated",
                    "at",
                    "the",
                    "previous",
                    "timestep."
                ]
            ],
            "context": [
                2,
                3
            ]
        },
        "input": "sent0: In the same direction but with the other output information, we used only the factors embedding as feedback (see equation 6).\n sent1: fb(y t−1 ) = y F t−1 #TARGET_REF where y F t−1 is the embedding of the factors generated at the previous timestep.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Apertium,",
                    "the",
                    "open-source",
                    "MT",
                    "platform",
                    "that",
                    "was",
                    "used",
                    "as",
                    "basis",
                    "in",
                    "the",
                    "case",
                    "study,",
                    "is",
                    "described",
                    "in",
                    "the",
                    "first",
                    "section",
                    "following",
                    "the",
                    "introduction."
                ],
                [
                    "Materials",
                    "and",
                    "methods",
                    "describe",
                    "already",
                    "available",
                    "language",
                    "processing",
                    "tools",
                    "and",
                    "materials,",
                    "mainly",
                    "corpora."
                ],
                [
                    "The",
                    "newly",
                    "developed",
                    "methods",
                    "are",
                    "described",
                    "in",
                    "the",
                    "same",
                    "section."
                ],
                [
                    "Following",
                    "section",
                    "describes",
                    "results",
                    "and",
                    "evaluation",
                    "methods."
                ],
                [
                    "The",
                    "last",
                    "section",
                    "describes",
                    "discussion",
                    "and",
                    "further",
                    "work."
                ],
                [
                    "The",
                    "modules",
                    "are",
                    "shown",
                    "on",
                    "Figure",
                    "1,",
                    "where",
                    "the",
                    "specially",
                    "addressed",
                    "modules",
                    "are",
                    "marked",
                    "with",
                    "a",
                    "new",
                    "colour",
                    "and",
                    "the",
                    "two",
                    "newly",
                    "added",
                    "modules",
                    "are",
                    "inserted."
                ],
                [
                    "Each",
                    "group's",
                    "data",
                    "creation",
                    "was",
                    "addressed",
                    "by",
                    "a",
                    "particular",
                    "method,",
                    "monolingual",
                    "dictionaries",
                    "were",
                    "constructed",
                    "using",
                    "bilingual",
                    "dictionary",
                    "data",
                    "and",
                    "applying",
                    "automatic",
                    "paradigm",
                    "tagging",
                    "techniques,",
                    "bilingual",
                    "dictionary",
                    "was",
                    "constructed",
                    "using",
                    "available",
                    "bilingual",
                    "word-list",
                    "but",
                    "a",
                    "few",
                    "methods",
                    "for",
                    "automatic",
                    "bilingual",
                    "dictionary",
                    "construction",
                    "were",
                    "investigated,",
                    "a",
                    "method",
                    "for",
                    "automatic",
                    "structural",
                    "shallow-transfer",
                    "rule",
                    "construction",
                    "(Sánchez-",
                    "#TARGET_REF",
                    "will",
                    "be",
                    "used",
                    "to",
                    "construct",
                    "a",
                    "set",
                    "of",
                    "structural",
                    "transfer",
                    "rules."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Apertium, the open-source MT platform that was used as basis in the case study, is described in the first section following the introduction.\n sent1: Materials and methods describe already available language processing tools and materials, mainly corpora.\n sent2: The newly developed methods are described in the same section.\n sent3: Following section describes results and evaluation methods.\n sent4: The last section describes discussion and further work.\n sent5: The modules are shown on Figure 1, where the specially addressed modules are marked with a new colour and the two newly added modules are inserted.\n sent6: Each group's data creation was addressed by a particular method, monolingual dictionaries were constructed using bilingual dictionary data and applying automatic paradigm tagging techniques, bilingual dictionary was constructed using available bilingual word-list but a few methods for automatic bilingual dictionary construction were investigated, a method for automatic structural shallow-transfer rule construction (Sánchez- #TARGET_REF will be used to construct a set of structural transfer rules.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "first",
                    "compile",
                    "rules",
                    "that",
                    "specify",
                    "evidence",
                    "for",
                    "the",
                    "support",
                    "and",
                    "attack",
                    "relations",
                    "between",
                    "claim",
                    "C",
                    "and",
                    "statement",
                    "S",
                    "(Table",
                    "1)."
                ],
                [
                    "2",
                    "These",
                    "rules",
                    "are",
                    "combined",
                    "via",
                    "PSL",
                    "#TARGET_REF",
                    "to",
                    "estimate",
                    "the",
                    "optimal",
                    "relation",
                    "between",
                    "C",
                    "and",
                    "S.",
                    "3",
                    "We",
                    "will",
                    "describe",
                    "individual",
                    "rules",
                    "in",
                    "four",
                    "categories:",
                    "factual",
                    "consistency,",
                    "sentiment",
                    "coherence,",
                    "causal",
                    "relation,",
                    "and",
                    "normative",
                    "relation,",
                    "followed",
                    "by",
                    "additional",
                    "chain",
                    "rules."
                ]
            ],
            "context": [
                2,
                0
            ]
        },
        "input": "sent0: We first compile rules that specify evidence for the support and attack relations between claim C and statement S (Table 1).\n sent1: 2 These rules are combined via PSL #TARGET_REF to estimate the optimal relation between C and S. 3 We will describe individual rules in four categories: factual consistency, sentiment coherence, causal relation, and normative relation, followed by additional chain rules.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "general,",
                    "for",
                    "linguistic",
                    "representation",
                    "it",
                    "is",
                    "the",
                    "derived",
                    "tree",
                    "that",
                    "is",
                    "used",
                    "as",
                    "the",
                    "primary",
                    "structure",
                    "of",
                    "representation,",
                    "so",
                    "the",
                    "labels",
                    "would",
                    "represent",
                    "words",
                    "in",
                    "a",
                    "typical",
                    "lexicalised",
                    "grammar",
                    "and",
                    "the",
                    "trees",
                    "«",
                    "½",
                    ",",
                    "«",
                    "¾",
                    "and",
                    "¬",
                    "½",
                    "would",
                    "represent",
                    "argument",
                    "structure",
                    "of",
                    "these",
                    "words."
                ],
                [
                    "However,",
                    "we",
                    "will",
                    "use",
                    "a",
                    "TAG",
                    "grammar",
                    "as",
                    "a",
                    "way",
                    "of",
                    "characterising",
                    "other",
                    "sorts",
                    "of",
                    "trees,",
                    "such",
                    "as",
                    "TAG",
                    "derivation",
                    "trees",
                    "or",
                    "dependency",
                    "trees,",
                    "this",
                    "is",
                    "thus",
                    "in",
                    "a",
                    "sense",
                    "an",
                    "extension",
                    "of",
                    "the",
                    "notion",
                    "of",
                    "the",
                    "meta-level",
                    "grammar",
                    "of",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "The",
                    "idea",
                    "is",
                    "then",
                    "to",
                    "use",
                    "a",
                    "TAG",
                    "grammar",
                    "to",
                    "break",
                    "down",
                    "some",
                    "tree",
                    "representation-which",
                    "may",
                    "be",
                    "a",
                    "dependency",
                    "tree,",
                    "a",
                    "TAG",
                    "derivation",
                    "tree,",
                    "6",
                    "or",
                    "other-into",
                    "component",
                    "trees",
                    "possibly",
                    "representing",
                    "non-contiguous",
                    "groupings."
                ],
                [
                    "The",
                    "aim",
                    "is",
                    "not",
                    "to",
                    "describe",
                    "every",
                    "decomposition",
                    "into",
                    "non-contiguous",
                    "groupings,",
                    "only",
                    "those",
                    "such",
                    "as",
                    "the",
                    "language-related",
                    "cases",
                    "presented",
                    "in",
                    "Section",
                    "2,",
                    "and",
                    "the",
                    "use",
                    "of",
                    "TAG",
                    "as",
                    "representation",
                    "allows",
                    "for",
                    "the",
                    "complexity",
                    "results",
                    "below."
                ],
                [
                    "We",
                    "now",
                    "present",
                    "an",
                    "algorithm",
                    "for",
                    "the",
                    "decomposition",
                    "in",
                    "Section",
                    "4."
                ]
            ],
            "context": [
                3,
                2,
                2,
                2,
                0
            ]
        },
        "input": "sent0: In general, for linguistic representation it is the derived tree that is used as the primary structure of representation, so the labels would represent words in a typical lexicalised grammar and the trees « ½ , « ¾ and ¬ ½ would represent argument structure of these words.\n sent1: However, we will use a TAG grammar as a way of characterising other sorts of trees, such as TAG derivation trees or dependency trees, this is thus in a sense an extension of the notion of the meta-level grammar of #TARGET_REF .\n sent2: The idea is then to use a TAG grammar to break down some tree representation-which may be a dependency tree, a TAG derivation tree, 6 or other-into component trees possibly representing non-contiguous groupings.\n sent3: The aim is not to describe every decomposition into non-contiguous groupings, only those such as the language-related cases presented in Section 2, and the use of TAG as representation allows for the complexity results below.\n sent4: We now present an algorithm for the decomposition in Section 4.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\", \"sent2\", \"sent3\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Table",
                    "2",
                    "shows",
                    "the",
                    "main",
                    "experimental",
                    "results."
                ],
                [
                    "We",
                    "can",
                    "see",
                    "that",
                    "RocketQA",
                    "significantly",
                    "outperforms",
                    "all",
                    "the",
                    "baselines",
                    "on",
                    "both",
                    "MSMARCO",
                    "and",
                    "NQ",
                    "datasets."
                ],
                [
                    "Another",
                    "observation",
                    "is",
                    "that",
                    "the",
                    "dense",
                    "retrievers",
                    "are",
                    "overall",
                    "better",
                    "than",
                    "the",
                    "sparse",
                    "retrievers."
                ],
                [
                    "Such",
                    "a",
                    "finding",
                    "has",
                    "also",
                    "been",
                    "reported",
                    "in",
                    "previous",
                    "studies",
                    "#TARGET_REF",
                    ",",
                    "which",
                    "indicates",
                    "the",
                    "effectiveness",
                    "of",
                    "the",
                    "dense",
                    "retrieval",
                    "approach."
                ]
            ],
            "context": [
                0,
                0,
                3,
                1
            ]
        },
        "input": "sent0: Table 2 shows the main experimental results.\n sent1: We can see that RocketQA significantly outperforms all the baselines on both MSMARCO and NQ datasets.\n sent2: Another observation is that the dense retrievers are overall better than the sparse retrievers.\n sent3: Such a finding has also been reported in previous studies #TARGET_REF , which indicates the effectiveness of the dense retrieval approach.\n",
        "output": "{\"INFORMATION\": [\"sent3\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Automatically",
                    "translating",
                    "the",
                    "context,",
                    "question",
                    "and",
                    "answer",
                    "triples",
                    "from",
                    "a",
                    "high-resource",
                    "language,",
                    "such",
                    "as",
                    "English",
                    "(called",
                    "source",
                    "domain)",
                    "to",
                    "lowresource",
                    "languages",
                    "(called",
                    "target",
                    "domains)",
                    "have",
                    "enabled",
                    "the",
                    "evaluation",
                    "of",
                    "models",
                    "for",
                    "languages",
                    "with",
                    "no",
                    "training",
                    "data",
                    "available",
                    "but",
                    "also",
                    "the",
                    "creation",
                    "of",
                    "large-scale",
                    "MT-based",
                    "QA",
                    "corpora",
                    "for",
                    "the",
                    "Italian",
                    "#TARGET_REF",
                    ",",
                    "Arabic",
                    "#REF",
                    "and",
                    "Korean",
                    "(Youngmin",
                    "Kim,",
                    "2020)",
                    "languages."
                ]
            ],
            "context": [
                3
            ]
        },
        "input": "sent0: Automatically translating the context, question and answer triples from a high-resource language, such as English (called source domain) to lowresource languages (called target domains) have enabled the evaluation of models for languages with no training data available but also the creation of large-scale MT-based QA corpora for the Italian #TARGET_REF , Arabic #REF and Korean (Youngmin Kim, 2020) languages.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Most",
                    "of",
                    "the",
                    "previous",
                    "studies",
                    "have",
                    "attempted",
                    "to",
                    "extract",
                    "and",
                    "link",
                    "at",
                    "formula",
                    "level",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "In",
                    "reality,",
                    "understanding",
                    "mathematical",
                    "formulae",
                    "requires",
                    "details",
                    "of",
                    "atomic",
                    "symbols",
                    "e.g."
                ],
                [
                    "superscript,",
                    "subscript,",
                    "function",
                    "arguments."
                ],
                [
                    "We",
                    "believe",
                    "that",
                    "addressing",
                    "the",
                    "problem",
                    "at",
                    "this",
                    "fine-grain",
                    "level",
                    "is",
                    "crucial",
                    "to",
                    "drive",
                    "future",
                    "research",
                    "toward",
                    "a",
                    "better",
                    "understanding",
                    "of",
                    "the",
                    "complex",
                    "symbol-description",
                    "extraction",
                    "task."
                ]
            ],
            "context": [
                1,
                3,
                0,
                2
            ]
        },
        "input": "sent0: Most of the previous studies have attempted to extract and link at formula level #TARGET_REF .\n sent1: In reality, understanding mathematical formulae requires details of atomic symbols e.g.\n sent2: superscript, subscript, function arguments.\n sent3: We believe that addressing the problem at this fine-grain level is crucial to drive future research toward a better understanding of the complex symbol-description extraction task.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [\"sent3\"], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "As",
                    "discussed",
                    "elsewhere,",
                    "e.g.,",
                    "by",
                    "#REF",
                    ",",
                    "Arabic",
                    "is",
                    "rich",
                    "morphology",
                    "language",
                    "with",
                    "complex",
                    "grammar",
                    "structure",
                    "which",
                    "poses",
                    "extra",
                    "challenges",
                    "to",
                    "systems",
                    "when",
                    "considering",
                    "Arabic",
                    "text",
                    "as",
                    "input."
                ],
                [
                    "#REF",
                    "discussed",
                    "the",
                    "script",
                    "differences",
                    "such",
                    "as",
                    "letter",
                    "shaping,",
                    "script",
                    "direction",
                    "(right",
                    "to",
                    "left)",
                    "and",
                    "obligatory",
                    "ligatures."
                ],
                [
                    "Also",
                    "#REF",
                    "discussed",
                    "the",
                    "lack",
                    "of",
                    "standard",
                    "orthographies:",
                    "e.g.,",
                    "and",
                    "both",
                    "mean",
                    "(gram)."
                ],
                [
                    "One",
                    "of",
                    "the",
                    "major",
                    "challenges",
                    "in",
                    "Arabic",
                    "NER",
                    "is",
                    "the",
                    "lack",
                    "of",
                    "capitalization",
                    "#REF",
                    "."
                ],
                [
                    "#TARGET_REF",
                    "also",
                    "discussed",
                    "the",
                    "agglutinative",
                    "nature",
                    "of",
                    "the",
                    "Arabic",
                    "language",
                    "where",
                    "new",
                    "words",
                    "and",
                    "sometimes",
                    "even",
                    "sentences",
                    "can",
                    "be",
                    "derived",
                    "by",
                    "adding",
                    "affixes",
                    "and",
                    "clitics",
                    "to",
                    "Arabic",
                    "words,",
                    "making",
                    "Arabic",
                    "a",
                    "morphologically",
                    "rich",
                    "language."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "sent0: As discussed elsewhere, e.g., by #REF , Arabic is rich morphology language with complex grammar structure which poses extra challenges to systems when considering Arabic text as input.\n sent1: #REF discussed the script differences such as letter shaping, script direction (right to left) and obligatory ligatures.\n sent2: Also #REF discussed the lack of standard orthographies: e.g., and both mean (gram).\n sent3: One of the major challenges in Arabic NER is the lack of capitalization #REF .\n sent4: #TARGET_REF also discussed the agglutinative nature of the Arabic language where new words and sometimes even sentences can be derived by adding affixes and clitics to Arabic words, making Arabic a morphologically rich language.\n",
        "output": "{\"INFORMATION\": [\"sent4\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Based",
                    "on",
                    "the",
                    "learned",
                    "state",
                    "tracking",
                    "model,",
                    "a",
                    "straightforward",
                    "idea",
                    "of",
                    "obtaining",
                    "salient",
                    "words",
                    "is",
                    "to",
                    "apply",
                    "importance",
                    "attribution",
                    "approaches."
                ],
                [
                    "Specifically,",
                    "these",
                    "approaches",
                    "measure",
                    "the",
                    "importance",
                    "of",
                    "each",
                    "word",
                    "by",
                    "observing",
                    "the",
                    "prediction",
                    "difference",
                    "caused",
                    "by",
                    "replacing",
                    "it",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "As",
                    "discussed",
                    "before,",
                    "this",
                    "would",
                    "result",
                    "in",
                    "different",
                    "action",
                    "representations",
                    "for",
                    "utterances",
                    "with",
                    "the",
                    "same",
                    "action."
                ],
                [
                    "To",
                    "address",
                    "this",
                    "issue,",
                    "we",
                    "consider",
                    "learning",
                    "action",
                    "representations",
                    "from",
                    "a",
                    "broader",
                    "vocabulary,",
                    "which",
                    "releases",
                    "the",
                    "constraint",
                    "of",
                    "selecting",
                    "salient",
                    "words",
                    "only",
                    "within",
                    "utterances."
                ]
            ],
            "context": [
                3,
                2,
                2,
                0
            ]
        },
        "input": "sent0: Based on the learned state tracking model, a straightforward idea of obtaining salient words is to apply importance attribution approaches.\n sent1: Specifically, these approaches measure the importance of each word by observing the prediction difference caused by replacing it #TARGET_REF .\n sent2: As discussed before, this would result in different action representations for utterances with the same action.\n sent3: To address this issue, we consider learning action representations from a broader vocabulary, which releases the constraint of selecting salient words only within utterances.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\", \"sent2\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "First,",
                    "there",
                    "exists",
                    "the",
                    "discrepancy",
                    "between",
                    "training",
                    "and",
                    "inference",
                    "for",
                    "the",
                    "dual-encoder",
                    "retriever."
                ],
                [
                    "During",
                    "inference,",
                    "the",
                    "retriever",
                    "needs",
                    "to",
                    "identify",
                    "positive",
                    "(or",
                    "relevant)",
                    "passages",
                    "for",
                    "each",
                    "question",
                    "from",
                    "a",
                    "large",
                    "collection",
                    "containing",
                    "millions",
                    "of",
                    "candidates."
                ],
                [
                    "However,",
                    "during",
                    "training,",
                    "the",
                    "model",
                    "is",
                    "learned",
                    "to",
                    "estimate",
                    "the",
                    "probabilities",
                    "of",
                    "positive",
                    "passages",
                    "in",
                    "a",
                    "small",
                    "candidate",
                    "set",
                    "for",
                    "each",
                    "question,",
                    "due",
                    "to",
                    "the",
                    "limited",
                    "memory",
                    "of",
                    "a",
                    "single",
                    "GPU",
                    "(or",
                    "other",
                    "device)."
                ],
                [
                    "To",
                    "reduce",
                    "such",
                    "a",
                    "discrepancy,",
                    "previous",
                    "work",
                    "tried",
                    "to",
                    "design",
                    "specific",
                    "mechanisms",
                    "for",
                    "selecting",
                    "a",
                    "few",
                    "hard",
                    "negatives",
                    "from",
                    "the",
                    "top-k",
                    "retrieved",
                    "candidates",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "However,",
                    "it",
                    "suffers",
                    "from",
                    "the",
                    "false",
                    "negative",
                    "issue",
                    "due",
                    "to",
                    "the",
                    "following",
                    "challenge."
                ]
            ],
            "context": [
                3,
                0,
                0,
                1,
                2
            ]
        },
        "input": "sent0: First, there exists the discrepancy between training and inference for the dual-encoder retriever.\n sent1: During inference, the retriever needs to identify positive (or relevant) passages for each question from a large collection containing millions of candidates.\n sent2: However, during training, the model is learned to estimate the probabilities of positive passages in a small candidate set for each question, due to the limited memory of a single GPU (or other device).\n sent3: To reduce such a discrepancy, previous work tried to design specific mechanisms for selecting a few hard negatives from the top-k retrieved candidates #TARGET_REF .\n sent4: However, it suffers from the false negative issue due to the following challenge.\n",
        "output": "{\"INFORMATION\": [\"sent3\"], \"PERCEPTION\": [\"sent4\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Lo-6",
                    "Sparsity",
                    "is",
                    "measured",
                    "using",
                    "Hoyer",
                    "#REF",
                    "."
                ],
                [
                    "In",
                    "this",
                    "paper",
                    "we",
                    "report",
                    "this",
                    "as",
                    "the",
                    "average",
                    "Hoyer",
                    "over",
                    "data",
                    "points'",
                    "posterior",
                    "means."
                ],
                [
                    "Hoyer",
                    "for",
                    "data",
                    "point",
                    "xi",
                    "with",
                    "posterior",
                    "mean",
                    "µi",
                    "is",
                    "calculated",
                    "as√",
                    "d−||",
                    "μi",
                    "||",
                    "1",
                    "/||",
                    "μi",
                    "||",
                    "2",
                    "√",
                    "d−1,",
                    "where",
                    "d",
                    "is",
                    "the",
                    "dimensionality",
                    "of",
                    "the",
                    "representations",
                    "and",
                    "μi",
                    "=",
                    "µi/σ(µ),",
                    "where",
                    "µ",
                    "=",
                    "{µ1,",
                    "...,",
                    "µn},",
                    "and",
                    "σ(.)"
                ],
                [
                    "is",
                    "the",
                    "standard",
                    "deviation."
                ],
                [
                    "catello",
                    "et",
                    "al."
                ],
                [
                    "(",
                    "2019)",
                    "on",
                    "image",
                    "domain."
                ],
                [
                    "In",
                    "Table",
                    "3",
                    "(Top-3",
                    "column)",
                    "we",
                    "report",
                    "the",
                    "number",
                    "of",
                    "appearances",
                    "of",
                    "a",
                    "model",
                    "among",
                    "the",
                    "top",
                    "3",
                    "highest",
                    "scoring",
                    "models",
                    "on",
                    "at",
                    "least",
                    "one",
                    "disentanglement",
                    "metric."
                ],
                [
                    "The",
                    "ranking",
                    "suggests",
                    "that",
                    "β-VAE",
                    "with",
                    "smaller",
                    "β",
                    "values",
                    "reach",
                    "better",
                    "disentangled",
                    "representations,",
                    "and",
                    "MAT-VAE",
                    "performing",
                    "superior",
                    "on",
                    "YNOC",
                    "and",
                    "poorly",
                    "on",
                    "POS,",
                    "highlighting",
                    "its",
                    "more",
                    "challenging",
                    "nature."
                ],
                [
                    "For",
                    "MAT-VAE",
                    "we",
                    "also",
                    "observe",
                    "an",
                    "interesting",
                    "correlation",
                    "between",
                    "sparsity",
                    "and",
                    "disentanglement:",
                    "for",
                    "instance",
                    "on",
                    "YNOC,",
                    "MAT-VAE",
                    "(β",
                    "=",
                    "0.01,",
                    "λ",
                    "=",
                    "0.1)",
                    "achieves",
                    "the",
                    "highest",
                    "Hoyer",
                    "(See",
                    "Table",
                    "4)",
                    "and",
                    "occurs",
                    "7",
                    "times",
                    "among",
                    "Top-3",
                    "(see",
                    "Table",
                    "3)."
                ],
                [
                    "Interestingly,",
                    "the",
                    "success",
                    "of",
                    "MAT-VAE",
                    "does",
                    "not",
                    "translate",
                    "to",
                    "POS",
                    "dataset,",
                    "where",
                    "it",
                    "underperforms",
                    "AE."
                ],
                [
                    "These",
                    "two",
                    "observations",
                    "suggest",
                    "that",
                    "sparsity",
                    "could",
                    "be",
                    "a",
                    "facilitator",
                    "for",
                    "disentanglement,",
                    "but",
                    "achieving",
                    "a",
                    "stable",
                    "level",
                    "of",
                    "sparsity",
                    "remains",
                    "as",
                    "a",
                    "challenge."
                ],
                [
                    "The",
                    "more",
                    "recent",
                    "development",
                    "in",
                    "the",
                    "direction",
                    "of",
                    "sparsity,",
                    "HSVAE",
                    "#TARGET_REF",
                    ",",
                    "addresses",
                    "the",
                    "stability",
                    "issue",
                    "of",
                    "MAT-VAE",
                    "but",
                    "we",
                    "leave",
                    "its",
                    "exploration",
                    "to",
                    "future",
                    "work."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                2,
                2,
                2
            ]
        },
        "input": "sent0: Lo-6 Sparsity is measured using Hoyer #REF .\n sent1: In this paper we report this as the average Hoyer over data points' posterior means.\n sent2: Hoyer for data point xi with posterior mean µi is calculated as√ d−|| μi || 1 /|| μi || 2 √ d−1, where d is the dimensionality of the representations and μi = µi/σ(µ), where µ = {µ1, ..., µn}, and σ(.)\n sent3: is the standard deviation.\n sent4: catello et al.\n sent5: ( 2019) on image domain.\n sent6: In Table 3 (Top-3 column) we report the number of appearances of a model among the top 3 highest scoring models on at least one disentanglement metric.\n sent7: The ranking suggests that β-VAE with smaller β values reach better disentangled representations, and MAT-VAE performing superior on YNOC and poorly on POS, highlighting its more challenging nature.\n sent8: For MAT-VAE we also observe an interesting correlation between sparsity and disentanglement: for instance on YNOC, MAT-VAE (β = 0.01, λ = 0.1) achieves the highest Hoyer (See Table 4) and occurs 7 times among Top-3 (see Table 3).\n sent9: Interestingly, the success of MAT-VAE does not translate to POS dataset, where it underperforms AE.\n sent10: These two observations suggest that sparsity could be a facilitator for disentanglement, but achieving a stable level of sparsity remains as a challenge.\n sent11: The more recent development in the direction of sparsity, HSVAE #TARGET_REF , addresses the stability issue of MAT-VAE but we leave its exploration to future work.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent9\", \"sent10\", \"sent11\"], \"BACKGROUND\": [\"sent8\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Explainability",
                    "is",
                    "an",
                    "important",
                    "concept",
                    "within",
                    "abusive",
                    "language",
                    "detection."
                ],
                [
                    "#TARGET_REF",
                    "noted",
                    "in",
                    "their",
                    "work",
                    "that",
                    "explainable",
                    "ML",
                    "techniques",
                    "can",
                    "promote",
                    "restorative",
                    "and",
                    "procedural",
                    "justice",
                    "by",
                    "surfacing",
                    "the",
                    "norms",
                    "that",
                    "have",
                    "been",
                    "violated",
                    "and",
                    "clarifying",
                    "how",
                    "they",
                    "have",
                    "been",
                    "violated."
                ],
                [
                    "That",
                    "said,",
                    "there",
                    "has",
                    "been",
                    "limited",
                    "discussion",
                    "of",
                    "the",
                    "issue",
                    "within",
                    "the",
                    "domain",
                    "of",
                    "abusive",
                    "language",
                    "detection."
                ],
                [
                    "In",
                    "this",
                    "section,",
                    "we",
                    "first",
                    "formalize",
                    "the",
                    "properties",
                    "that",
                    "an",
                    "explainable",
                    "detection",
                    "method",
                    "should",
                    "aim",
                    "to",
                    "exhibit",
                    "in",
                    "order",
                    "to",
                    "thoroughly",
                    "substantiate",
                    "its",
                    "decisions."
                ],
                [
                    "We",
                    "then",
                    "describe",
                    "how",
                    "user",
                    "and",
                    "community",
                    "information",
                    "play",
                    "an",
                    "important",
                    "role",
                    "in",
                    "the",
                    "realization",
                    "of",
                    "each",
                    "of",
                    "the",
                    "properties."
                ],
                [
                    "Finally,",
                    "we",
                    "discuss",
                    "what",
                    "it",
                    "means",
                    "to",
                    "operationalize",
                    "explainability",
                    "within",
                    "abusive",
                    "language",
                    "detection",
                    "in",
                    "an",
                    "effective",
                    "manner."
                ]
            ],
            "context": [
                3,
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Explainability is an important concept within abusive language detection.\n sent1: #TARGET_REF noted in their work that explainable ML techniques can promote restorative and procedural justice by surfacing the norms that have been violated and clarifying how they have been violated.\n sent2: That said, there has been limited discussion of the issue within the domain of abusive language detection.\n sent3: In this section, we first formalize the properties that an explainable detection method should aim to exhibit in order to thoroughly substantiate its decisions.\n sent4: We then describe how user and community information play an important role in the realization of each of the properties.\n sent5: Finally, we discuss what it means to operationalize explainability within abusive language detection in an effective manner.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "system",
                    "is",
                    "based",
                    "on",
                    "the",
                    "Moses",
                    "SMT",
                    "toolkit",
                    "#REF",
                    "and",
                    "constructed",
                    "as",
                    "follows."
                ],
                [
                    "First,",
                    "Giza++",
                    "is",
                    "used",
                    "to",
                    "perform",
                    "word",
                    "alignments",
                    "in",
                    "both",
                    "directions."
                ],
                [
                    "Second,",
                    "phrases",
                    "and",
                    "lexical",
                    "reorderings",
                    "are",
                    "extracted."
                ],
                [
                    "Both",
                    "steps",
                    "use",
                    "the",
                    "default",
                    "settings",
                    "of",
                    "the",
                    "Moses",
                    "SMT",
                    "toolkit."
                ],
                [
                    "A",
                    "4-gram",
                    "back-off",
                    "target",
                    "LM",
                    "is",
                    "then",
                    "constructed",
                    "as",
                    "detailed",
                    "in",
                    "section",
                    "3.2."
                ],
                [
                    "The",
                    "translation",
                    "itself",
                    "is",
                    "performed",
                    "in",
                    "two",
                    "passes:",
                    "first,",
                    "Moses",
                    "is",
                    "run",
                    "and",
                    "a",
                    "1000-best",
                    "list",
                    "is",
                    "generated",
                    "for",
                    "each",
                    "sentence."
                ],
                [
                    "The",
                    "parameters",
                    "of",
                    "this",
                    "first",
                    "pass",
                    "are",
                    "tuned",
                    "on",
                    "development",
                    "data",
                    "using",
                    "the",
                    "cmert",
                    "tool."
                ],
                [
                    "These",
                    "1000-best",
                    "lists",
                    "are",
                    "then",
                    "rescored",
                    "with",
                    "a",
                    "continuous",
                    "space",
                    "4-gram",
                    "LM",
                    "and",
                    "the",
                    "weights",
                    "of",
                    "the",
                    "feature",
                    "functions",
                    "are",
                    "again",
                    "optimized",
                    "using",
                    "the",
                    "open",
                    "source",
                    "numerical",
                    "optimization",
                    "toolkit",
                    "Condor",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "The",
                    "details",
                    "of",
                    "this",
                    "optimization",
                    "procedure",
                    "are",
                    "as",
                    "follows:"
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: The system is based on the Moses SMT toolkit #REF and constructed as follows.\n sent1: First, Giza++ is used to perform word alignments in both directions.\n sent2: Second, phrases and lexical reorderings are extracted.\n sent3: Both steps use the default settings of the Moses SMT toolkit.\n sent4: A 4-gram back-off target LM is then constructed as detailed in section 3.2.\n sent5: The translation itself is performed in two passes: first, Moses is run and a 1000-best list is generated for each sentence.\n sent6: The parameters of this first pass are tuned on development data using the cmert tool.\n sent7: These 1000-best lists are then rescored with a continuous space 4-gram LM and the weights of the feature functions are again optimized using the open source numerical optimization toolkit Condor #TARGET_REF .\n sent8: The details of this optimization procedure are as follows:\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "get",
                    "a",
                    "global",
                    "evidence",
                    "table",
                    "by",
                    "stacking",
                    "the",
                    "m",
                    "tables",
                    "from",
                    "sentences",
                    "and",
                    "n",
                    "tables",
                    "from",
                    "tabular",
                    "evidence,",
                    "as",
                    "illustrated",
                    "in",
                    "Figure",
                    "3."
                ],
                [
                    "Then,",
                    "we",
                    "feed",
                    "the",
                    "claim",
                    "and",
                    "the",
                    "global",
                    "evidence",
                    "table",
                    "to",
                    "a",
                    "pretrained",
                    "table",
                    "model,",
                    "TAPAS",
                    "#TARGET_REF",
                    ",",
                    "and",
                    "get",
                    "the",
                    "tabular",
                    "format",
                    "evidence",
                    "representation."
                ]
            ],
            "context": [
                0,
                2
            ]
        },
        "input": "sent0: We get a global evidence table by stacking the m tables from sentences and n tables from tabular evidence, as illustrated in Figure 3.\n sent1: Then, we feed the claim and the global evidence table to a pretrained table model, TAPAS #TARGET_REF , and get the tabular format evidence representation.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "approach",
                    "this",
                    "discussion",
                    "from",
                    "three",
                    "different",
                    "perspectives,",
                    "that",
                    "of",
                    "the",
                    "designers",
                    "of",
                    "the",
                    "detection",
                    "method,",
                    "that",
                    "of",
                    "the",
                    "user",
                    "creating",
                    "comments,",
                    "and",
                    "that",
                    "of",
                    "the",
                    "larger",
                    "communities."
                ],
                [
                    "By",
                    "breaking",
                    "the",
                    "discussion",
                    "down",
                    "in",
                    "this",
                    "manner,",
                    "we",
                    "explore",
                    "the",
                    "different",
                    "choices",
                    "that",
                    "exist",
                    "for",
                    "operationalization",
                    "and",
                    "the",
                    "purposes",
                    "they",
                    "can",
                    "serve."
                ],
                [
                    "Designers",
                    "of",
                    "the",
                    "method."
                ],
                [
                    "For",
                    "the",
                    "designers",
                    "of",
                    "the",
                    "detection",
                    "method,",
                    "explainability",
                    "can",
                    "serve",
                    "as",
                    "a",
                    "principled",
                    "mechanism",
                    "for",
                    "understanding",
                    "and",
                    "reasoning",
                    "about",
                    "the",
                    "behavior",
                    "of",
                    "their",
                    "method,",
                    "which",
                    "is",
                    "important",
                    "for",
                    "multiple",
                    "reasons."
                ],
                [
                    "Firstly,",
                    "if",
                    "the",
                    "detection",
                    "method",
                    "exhibits",
                    "all",
                    "the",
                    "four",
                    "properties",
                    "of",
                    "explain-ability,",
                    "then",
                    "the",
                    "designers",
                    "can",
                    "easily",
                    "gain",
                    "insights",
                    "into",
                    "the",
                    "factors",
                    "that",
                    "contributed",
                    "to",
                    "the",
                    "decision",
                    "made",
                    "by",
                    "the",
                    "method",
                    "given",
                    "a",
                    "comment."
                ],
                [
                    "This",
                    "can",
                    "allow",
                    "the",
                    "designers",
                    "to",
                    "recognize",
                    "when",
                    "the",
                    "method",
                    "may",
                    "be",
                    "overly",
                    "relying",
                    "on",
                    "a",
                    "specific",
                    "factor,",
                    "e.g.,",
                    "the",
                    "demographic",
                    "traits."
                ],
                [
                    "In",
                    "the",
                    "case",
                    "of",
                    "social",
                    "feature",
                    "engineering",
                    "and",
                    "user",
                    "embeddings",
                    "based",
                    "methods,",
                    "operationalization",
                    "of",
                    "explainability",
                    "via",
                    "feature",
                    "attribution",
                    "such",
                    "as",
                    "LIME",
                    "#REF",
                    "and",
                    "Integrated",
                    "Gradients",
                    "#TARGET_REF",
                    "can",
                    "be",
                    "effective",
                    "in",
                    "offering",
                    "such",
                    "insights."
                ],
                [
                    "For",
                    "social",
                    "graph",
                    "based",
                    "methods",
                    "that",
                    "employ",
                    "graph",
                    "neural",
                    "networks,",
                    "attribution",
                    "techniques",
                    "like",
                    "GNNExplainer",
                    "#REF",
                    "can",
                    "be",
                    "used",
                    "instead."
                ],
                [
                    "The",
                    "second",
                    "reason",
                    "why",
                    "explainability",
                    "is",
                    "important",
                    "for",
                    "the",
                    "designers",
                    "is",
                    "because",
                    "it",
                    "can",
                    "allow",
                    "them",
                    "to",
                    "optimize",
                    "the",
                    "method",
                    "by",
                    "removing",
                    "inputs",
                    "that",
                    "do",
                    "not",
                    "contribute",
                    "significantly."
                ],
                [
                    "Here",
                    "again,",
                    "explainability",
                    "via",
                    "feature",
                    "attribution",
                    "can",
                    "be",
                    "effective."
                ],
                [
                    "Lastly,",
                    "explainability",
                    "is",
                    "also",
                    "important",
                    "for",
                    "the",
                    "designers",
                    "to",
                    "understand",
                    "how",
                    "their",
                    "method",
                    "would",
                    "perform",
                    "in",
                    "cases",
                    "where",
                    "a",
                    "user",
                    "may",
                    "try",
                    "obfuscate",
                    "abusive",
                    "language",
                    "#REF",
                    "."
                ],
                [
                    "Counterfactual",
                    "explanations",
                    "can",
                    "constitute",
                    "an",
                    "effective",
                    "operationalization",
                    "for",
                    "the",
                    "designers",
                    "to",
                    "identify",
                    "the",
                    "parts",
                    "of",
                    "their",
                    "method",
                    "that",
                    "are",
                    "most",
                    "vulnerable",
                    "to",
                    "obfuscations."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: We approach this discussion from three different perspectives, that of the designers of the detection method, that of the user creating comments, and that of the larger communities.\n sent1: By breaking the discussion down in this manner, we explore the different choices that exist for operationalization and the purposes they can serve.\n sent2: Designers of the method.\n sent3: For the designers of the detection method, explainability can serve as a principled mechanism for understanding and reasoning about the behavior of their method, which is important for multiple reasons.\n sent4: Firstly, if the detection method exhibits all the four properties of explain-ability, then the designers can easily gain insights into the factors that contributed to the decision made by the method given a comment.\n sent5: This can allow the designers to recognize when the method may be overly relying on a specific factor, e.g., the demographic traits.\n sent6: In the case of social feature engineering and user embeddings based methods, operationalization of explainability via feature attribution such as LIME #REF and Integrated Gradients #TARGET_REF can be effective in offering such insights.\n sent7: For social graph based methods that employ graph neural networks, attribution techniques like GNNExplainer #REF can be used instead.\n sent8: The second reason why explainability is important for the designers is because it can allow them to optimize the method by removing inputs that do not contribute significantly.\n sent9: Here again, explainability via feature attribution can be effective.\n sent10: Lastly, explainability is also important for the designers to understand how their method would perform in cases where a user may try obfuscate abusive language #REF .\n sent11: Counterfactual explanations can constitute an effective operationalization for the designers to identify the parts of their method that are most vulnerable to obfuscations.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "automaton."
                ],
                [
                    "Thus",
                    "it",
                    "is",
                    "possible",
                    "for",
                    "the",
                    "stack",
                    "to",
                    "be",
                    "accessed",
                    "on",
                    "different",
                    "paths,",
                    "and",
                    "so",
                    "it",
                    "is",
                    "possible",
                    "for",
                    "paths",
                    "to",
                    "be",
                    "dependent",
                    "(e.g."
                ],
                [
                    "one",
                    "path",
                    "in",
                    "the",
                    "tree",
                    "is",
                    "Ò",
                    ",",
                    "another",
                    "is",
                    "Ò",
                    ")."
                ],
                [
                    "Grammars",
                    "that",
                    "generate",
                    "MCSLs",
                    "cannot",
                    "have",
                    "dependent",
                    "paths",
                    "#REF",
                    "."
                ],
                [
                    "But",
                    "if",
                    "access",
                    "to",
                    "the",
                    "stack",
                    "is",
                    "restricted",
                    "to",
                    "a",
                    "single",
                    "path-in",
                    "the",
                    "same",
                    "manner",
                    "that",
                    "restricting",
                    "stack",
                    "passing",
                    "to",
                    "a",
                    "single",
                    "non-terminal",
                    "child",
                    "in",
                    "an",
                    "indexed",
                    "grammar",
                    "produces",
                    "a",
                    "linear",
                    "indexed",
                    "grammar",
                    "#REF",
                    ",",
                    "which",
                    "generates",
                    "MCSLs-the",
                    "power",
                    "of",
                    "the",
                    "TPDA",
                    "is",
                    "suitably",
                    "restricted."
                ],
                [
                    "The",
                    "idea",
                    "is",
                    "related",
                    "to",
                    "the",
                    "Embedded",
                    "Pushdown",
                    "Automaton",
                    "(EPDA)",
                    "of",
                    "#TARGET_REF",
                    ",",
                    "although",
                    "this",
                    "is",
                    "of",
                    "course",
                    "a",
                    "string",
                    "automaton",
                    "rather",
                    "than",
                    "a",
                    "tree",
                    "automaton."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                3,
                2
            ]
        },
        "input": "sent0: automaton.\n sent1: Thus it is possible for the stack to be accessed on different paths, and so it is possible for paths to be dependent (e.g.\n sent2: one path in the tree is Ò , another is Ò ).\n sent3: Grammars that generate MCSLs cannot have dependent paths #REF .\n sent4: But if access to the stack is restricted to a single path-in the same manner that restricting stack passing to a single non-terminal child in an indexed grammar produces a linear indexed grammar #REF , which generates MCSLs-the power of the TPDA is suitably restricted.\n sent5: The idea is related to the Embedded Pushdown Automaton (EPDA) of #TARGET_REF , although this is of course a string automaton rather than a tree automaton.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent5\"], \"BACKGROUND\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Professional",
                    "translations",
                    "of",
                    "Doc",
                    "A",
                    "into",
                    "English",
                    "and",
                    "Doc",
                    "B",
                    "into",
                    "Russian",
                    "were",
                    "commissioned",
                    "as",
                    "part",
                    "of",
                    "the",
                    "WMT-14",
                    "shared",
                    "translation",
                    "task",
                    "#REF",
                    "."
                ],
                [
                    "The",
                    "Russian",
                    "version",
                    "of",
                    "each",
                    "text",
                    "was",
                    "translated",
                    "automatically",
                    "using",
                    "Moses",
                    "#REF",
                    "by",
                    "#TARGET_REF",
                    "as",
                    "part",
                    "of",
                    "their",
                    "WMT14",
                    "shared",
                    "task",
                    "submission."
                ],
                [
                    "As",
                    "a",
                    "side",
                    "effect",
                    "of",
                    "the",
                    "phrase-based",
                    "MT",
                    "process,",
                    "Moses",
                    "can",
                    "be",
                    "configured",
                    "to",
                    "produce",
                    "alignment",
                    "links,",
                    "indicating",
                    "which",
                    "target",
                    "language",
                    "words",
                    "were",
                    "produced",
                    "from",
                    "which",
                    "source",
                    "language",
                    "words."
                ],
                [
                    "To",
                    "enable",
                    "maximal",
                    "comparability",
                    "with",
                    "the",
                    "post-editing",
                    "results",
                    "of",
                    "#REF",
                    ",",
                    "we",
                    "make",
                    "use",
                    "of",
                    "Russian-English",
                    "machine",
                    "translation",
                    "results",
                    "and",
                    "alignments",
                    "from",
                    "that",
                    "work",
                    "here."
                ]
            ],
            "context": [
                0,
                1,
                0,
                0
            ]
        },
        "input": "sent0: Professional translations of Doc A into English and Doc B into Russian were commissioned as part of the WMT-14 shared translation task #REF .\n sent1: The Russian version of each text was translated automatically using Moses #REF by #TARGET_REF as part of their WMT14 shared task submission.\n sent2: As a side effect of the phrase-based MT process, Moses can be configured to produce alignment links, indicating which target language words were produced from which source language words.\n sent3: To enable maximal comparability with the post-editing results of #REF , we make use of Russian-English machine translation results and alignments from that work here.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "performance",
                    "of",
                    "QA",
                    "models",
                    "are",
                    "evaluated",
                    "using",
                    "the",
                    "Exact",
                    "Match",
                    "(EM)",
                    "and",
                    "F1",
                    "scores."
                ],
                [
                    "The",
                    "EM",
                    "score",
                    "is",
                    "the",
                    "percentage",
                    "of",
                    "system",
                    "outputs",
                    "that",
                    "match",
                    "exactly",
                    "with",
                    "the",
                    "ground",
                    "truth",
                    "answers."
                ],
                [
                    "The",
                    "F1",
                    "score",
                    "is",
                    "a",
                    "combined",
                    "measure",
                    "of",
                    "precision",
                    "and",
                    "recall",
                    "that",
                    "is",
                    "less",
                    "strict",
                    "than",
                    "EM."
                ],
                [
                    "The",
                    "evaluation",
                    "process",
                    "3",
                    "involves",
                    "post-processing",
                    "identical",
                    "to",
                    "that",
                    "presented",
                    "by",
                    "d'Hoffschmidt",
                    "et",
                    "al."
                ],
                [
                    "(",
                    "2020)",
                    "and",
                    "inspired",
                    "by",
                    "that",
                    "proposed",
                    "for",
                    "English",
                    "by",
                    "#TARGET_REF",
                    ",",
                    "which",
                    "consists",
                    "of",
                    "the",
                    "removal",
                    "of",
                    "punctuation",
                    "marks",
                    "and",
                    "determiners",
                    "4",
                    "as",
                    "well",
                    "as",
                    "a",
                    "down-casing",
                    "of",
                    "the",
                    "answers",
                    "(ground",
                    "truths",
                    "and",
                    "predictions)."
                ]
            ],
            "context": [
                0,
                0,
                0,
                3,
                1
            ]
        },
        "input": "sent0: The performance of QA models are evaluated using the Exact Match (EM) and F1 scores.\n sent1: The EM score is the percentage of system outputs that match exactly with the ground truth answers.\n sent2: The F1 score is a combined measure of precision and recall that is less strict than EM.\n sent3: The evaluation process 3 involves post-processing identical to that presented by d'Hoffschmidt et al.\n sent4: ( 2020) and inspired by that proposed for English by #TARGET_REF , which consists of the removal of punctuation marks and determiners 4 as well as a down-casing of the answers (ground truths and predictions).\n",
        "output": "{\"INFORMATION\": [\"sent4\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "For",
                    "all",
                    "BERT-based",
                    "models,",
                    "we",
                    "use",
                    "Hugging-Face's",
                    "transformers",
                    "#TARGET_REF",
                    "in",
                    "PyTorch."
                ],
                [
                    "For",
                    "CRF,",
                    "we",
                    "use",
                    "the",
                    "pytorch-crf",
                    "#REF",
                    "library."
                ],
                [
                    "We",
                    "use",
                    "a",
                    "batch",
                    "size",
                    "of",
                    "4,",
                    "train",
                    "for",
                    "3",
                    "epochs,",
                    "5",
                    "Our",
                    "code",
                    "can",
                    "be",
                    "found",
                    "at:",
                    "https://github.com/",
                    "gchhablani/toxic-spans-detection."
                ],
                [
                    "6",
                    "We",
                    "also",
                    "use",
                    "Integrated",
                    "Gradients",
                    "to",
                    "understand",
                    "what",
                    "the",
                    "models",
                    "focus",
                    "on."
                ],
                [
                    "For",
                    "discussion,",
                    "see",
                    "Appendix",
                    "B.",
                    "use",
                    "a",
                    "linear",
                    "learning",
                    "rate",
                    "decay,",
                    "and",
                    "an",
                    "AdamW",
                    "optimizer",
                    "with",
                    "a",
                    "weight",
                    "decay",
                    "of",
                    "0.01."
                ],
                [
                    "The",
                    "initial",
                    "learning",
                    "rate",
                    "is",
                    "2e−5."
                ],
                [
                    "During",
                    "tokenization,",
                    "the",
                    "maximum",
                    "length",
                    "allowed",
                    "is",
                    "384,",
                    "with",
                    "the",
                    "exception",
                    "of",
                    "RoBERTa",
                    "Span+Token",
                    "where",
                    "it",
                    "is",
                    "512."
                ],
                [
                    "We",
                    "use",
                    "LARGE",
                    "models",
                    "for",
                    "all",
                    "-BERT,",
                    "RoBERTa",
                    "and",
                    "SpanBERT,",
                    "unless",
                    "otherwise",
                    "specified."
                ]
            ],
            "context": [
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: For all BERT-based models, we use Hugging-Face's transformers #TARGET_REF in PyTorch.\n sent1: For CRF, we use the pytorch-crf #REF library.\n sent2: We use a batch size of 4, train for 3 epochs, 5 Our code can be found at: https://github.com/ gchhablani/toxic-spans-detection.\n sent3: 6 We also use Integrated Gradients to understand what the models focus on.\n sent4: For discussion, see Appendix B. use a linear learning rate decay, and an AdamW optimizer with a weight decay of 0.01.\n sent5: The initial learning rate is 2e−5.\n sent6: During tokenization, the maximum length allowed is 384, with the exception of RoBERTa Span+Token where it is 512.\n sent7: We use LARGE models for all -BERT, RoBERTa and SpanBERT, unless otherwise specified.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Integrating",
                    "eye-movements",
                    "of",
                    "the",
                    "speaker."
                ],
                [
                    "Many",
                    "eye-tracking",
                    "technologies",
                    "in",
                    "the",
                    "market",
                    "employ",
                    "a",
                    "sufficient",
                    "sampling",
                    "frequency",
                    "to",
                    "enable",
                    "gazecontingent",
                    "applications."
                ],
                [
                    "With",
                    "advancements",
                    "in",
                    "the",
                    "eye-tracking",
                    "technology,",
                    "incorporating",
                    "eye",
                    "movements",
                    "of",
                    "a",
                    "speaker",
                    "or",
                    "a",
                    "listener",
                    "enables",
                    "us",
                    "to",
                    "predict",
                    "/",
                    "resolve",
                    "which",
                    "entity",
                    "is",
                    "being",
                    "referred",
                    "to",
                    "in",
                    "a",
                    "complex",
                    "visual",
                    "environment",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "However,",
                    "these",
                    "studies",
                    "are",
                    "limited",
                    "to",
                    "relatively",
                    "simple",
                    "scenes."
                ],
                [
                    "Situated",
                    "language",
                    "understanding",
                    "in",
                    "a",
                    "referentially",
                    "complex",
                    "environment",
                    "or",
                    "under",
                    "noisy",
                    "situations",
                    "imposes",
                    "a",
                    "different",
                    "level",
                    "of",
                    "challenge",
                    "that",
                    "we",
                    "aim",
                    "to",
                    "address."
                ],
                [
                    "The",
                    "number",
                    "of",
                    "studies",
                    "that",
                    "utilize",
                    "gaze",
                    "features",
                    "#REF",
                    "is",
                    "very",
                    "limited."
                ],
                [
                    "In",
                    "this",
                    "study,",
                    "we",
                    "propose",
                    "to",
                    "incorporate",
                    "the",
                    "eye-movements",
                    "of",
                    "the",
                    "speaker",
                    "to",
                    "improve",
                    "the",
                    "crossmodal",
                    "mapping",
                    "performance."
                ],
                [
                    "This",
                    "additional",
                    "deictic",
                    "modality",
                    "may",
                    "improve",
                    "the",
                    "recovery",
                    "of",
                    "the",
                    "intended",
                    "meaning",
                    "especially",
                    "when",
                    "the",
                    "communication",
                    "is",
                    "noisy",
                    "(acoustically",
                    "or",
                    "visually)."
                ],
                [
                    "The",
                    "gaze",
                    "embeddings",
                    "will",
                    "be",
                    "created",
                    "by",
                    "using",
                    "existing",
                    "eyemovement",
                    "datasets."
                ],
                [
                    "However,",
                    "there",
                    "are",
                    "only",
                    "few",
                    "big-size",
                    "eye-movement",
                    "datasets",
                    "available",
                    "#REF",
                    "."
                ],
                [
                    "Thus,",
                    "to",
                    "enlarge",
                    "available",
                    "data,",
                    "we",
                    "will",
                    "conduct",
                    "a",
                    "set",
                    "of",
                    "experimental",
                    "studies",
                    "with",
                    "increasing",
                    "referential",
                    "complexity."
                ],
                [
                    "There,",
                    "we",
                    "will",
                    "record",
                    "participants'",
                    "instructions",
                    "on",
                    "a",
                    "task-oriented",
                    "scenario",
                    "and",
                    "their",
                    "eye-movements",
                    "regarding",
                    "target",
                    "objects."
                ]
            ],
            "context": [
                0,
                3,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Integrating eye-movements of the speaker.\n sent1: Many eye-tracking technologies in the market employ a sufficient sampling frequency to enable gazecontingent applications.\n sent2: With advancements in the eye-tracking technology, incorporating eye movements of a speaker or a listener enables us to predict / resolve which entity is being referred to in a complex visual environment #TARGET_REF .\n sent3: However, these studies are limited to relatively simple scenes.\n sent4: Situated language understanding in a referentially complex environment or under noisy situations imposes a different level of challenge that we aim to address.\n sent5: The number of studies that utilize gaze features #REF is very limited.\n sent6: In this study, we propose to incorporate the eye-movements of the speaker to improve the crossmodal mapping performance.\n sent7: This additional deictic modality may improve the recovery of the intended meaning especially when the communication is noisy (acoustically or visually).\n sent8: The gaze embeddings will be created by using existing eyemovement datasets.\n sent9: However, there are only few big-size eye-movement datasets available #REF .\n sent10: Thus, to enlarge available data, we will conduct a set of experimental studies with increasing referential complexity.\n sent11: There, we will record participants' instructions on a task-oriented scenario and their eye-movements regarding target objects.\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Shakkala",
                    "was",
                    "built",
                    "by",
                    "#REF",
                    "for",
                    "Arabic",
                    "text",
                    "diacritization",
                    "using",
                    "Bi-LSTM",
                    "networks",
                    "combined",
                    "with",
                    "character",
                    "embeddings."
                ],
                [
                    "#REF",
                    "demonstrated",
                    "the",
                    "superiority",
                    "of",
                    "the",
                    "neural",
                    "approach",
                    "of",
                    "Shakkala",
                    "compared",
                    "to",
                    "other",
                    "different",
                    "automatic",
                    "diacritization",
                    "systems",
                    "available",
                    "online",
                    "e.g.,",
                    "#REF",
                    "Some",
                    "recent",
                    "work",
                    "in",
                    "Arabic",
                    "NLP",
                    "has",
                    "started",
                    "to",
                    "make",
                    "use",
                    "of",
                    "such",
                    "systems."
                ],
                [
                    "For",
                    "example,",
                    "Al-Sallab",
                    "et",
                    "al."
                ],
                [
                    "(",
                    "2017)",
                    "proposed",
                    "AROMA,",
                    "a",
                    "recursive",
                    "deep",
                    "learning",
                    "model",
                    "for",
                    "opinion",
                    "mining",
                    "in",
                    "Arabic."
                ],
                [
                    "Preprocessing",
                    "in",
                    "AROMA",
                    "included",
                    "morphological",
                    "tokenization",
                    "and",
                    "automatic",
                    "diacritization",
                    "carried",
                    "out",
                    "by",
                    "MADAMIRA",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "This",
                    "resulted",
                    "in",
                    "improved",
                    "performance",
                    "in",
                    "classifying",
                    "opinion",
                    "as",
                    "positive",
                    "or",
                    "negative",
                    "on",
                    "a",
                    "range",
                    "of",
                    "different",
                    "Arabic",
                    "corpora."
                ],
                [
                    "Similarly,",
                    "#REF",
                    "used",
                    "a",
                    "Recursive",
                    "Neural",
                    "Tensor",
                    "Network",
                    "(RNTN)",
                    "for",
                    "sentiment",
                    "analysis",
                    "and",
                    "reported",
                    "that",
                    "adding",
                    "orthographic",
                    "features",
                    "such",
                    "as",
                    "diacritics",
                    "improved",
                    "the",
                    "performance."
                ],
                [
                    "They",
                    "incorporated",
                    "orthographic",
                    "features",
                    "such",
                    "as",
                    "diacritics",
                    "by",
                    "enlarging",
                    "the",
                    "vocabulary",
                    "to",
                    "have",
                    "distinct",
                    "word",
                    "forms",
                    "for",
                    "different",
                    "versions",
                    "of",
                    "the",
                    "word",
                    "(diacritized/undiacritized)",
                    "and",
                    "then",
                    "deriving",
                    "embeddings",
                    "by",
                    "training",
                    "a",
                    "Continuous",
                    "Bag",
                    "of",
                    "Words",
                    "(CBOW)",
                    "model",
                    "#REF",
                    "."
                ],
                [
                    "Similarly,",
                    "#REF",
                    "introduced",
                    "automatic",
                    "selective",
                    "diacritization",
                    "as",
                    "a",
                    "viable",
                    "step",
                    "in",
                    "lexical",
                    "disambiguation."
                ],
                [
                    "They",
                    "evaluated",
                    "the",
                    "system",
                    "in",
                    "downstream",
                    "tasks",
                    "including",
                    "POS",
                    "which",
                    "improved",
                    "from",
                    "97.99%",
                    "by",
                    "baseline",
                    "to",
                    "98.70%."
                ],
                [
                    "They",
                    "trained",
                    "word",
                    "embeddings",
                    "on",
                    "selectively-diacritized",
                    "dataset",
                    "to",
                    "enrich",
                    "the",
                    "vocabulary."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                1,
                3,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Shakkala was built by #REF for Arabic text diacritization using Bi-LSTM networks combined with character embeddings.\n sent1: #REF demonstrated the superiority of the neural approach of Shakkala compared to other different automatic diacritization systems available online e.g., #REF Some recent work in Arabic NLP has started to make use of such systems.\n sent2: For example, Al-Sallab et al.\n sent3: ( 2017) proposed AROMA, a recursive deep learning model for opinion mining in Arabic.\n sent4: Preprocessing in AROMA included morphological tokenization and automatic diacritization carried out by MADAMIRA #TARGET_REF .\n sent5: This resulted in improved performance in classifying opinion as positive or negative on a range of different Arabic corpora.\n sent6: Similarly, #REF used a Recursive Neural Tensor Network (RNTN) for sentiment analysis and reported that adding orthographic features such as diacritics improved the performance.\n sent7: They incorporated orthographic features such as diacritics by enlarging the vocabulary to have distinct word forms for different versions of the word (diacritized/undiacritized) and then deriving embeddings by training a Continuous Bag of Words (CBOW) model #REF .\n sent8: Similarly, #REF introduced automatic selective diacritization as a viable step in lexical disambiguation.\n sent9: They evaluated the system in downstream tasks including POS which improved from 97.99% by baseline to 98.70%.\n sent10: They trained word embeddings on selectively-diacritized dataset to enrich the vocabulary.\n",
        "output": "{\"INFORMATION\": [\"sent4\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent5\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "(2020)",
                    "that",
                    "focus",
                    "on",
                    "creating",
                    "content",
                    "especially",
                    "for",
                    "2D",
                    "visual",
                    "games",
                    "via",
                    "search",
                    "or",
                    "reinforcement",
                    "learning",
                    "based",
                    "methods."
                ],
                [
                    "#REF",
                    "use",
                    "knowledge",
                    "graphs",
                    "to",
                    "ground",
                    "language",
                    "and",
                    "produce",
                    "worlds",
                    "and",
                    "quests",
                    "separately",
                    "for",
                    "text",
                    "games",
                    "from",
                    "existing",
                    "corpora",
                    "such",
                    "as",
                    "stories."
                ],
                [
                    "#TARGET_REF",
                    "leverage",
                    "LIGHT",
                    "to",
                    "learn",
                    "to",
                    "generate",
                    "interactive",
                    "fiction",
                    "worlds",
                    "on",
                    "the",
                    "basis",
                    "of",
                    "locations,",
                    "characters,",
                    "and",
                    "objects-this",
                    "work",
                    "is",
                    "closest",
                    "in",
                    "spirit",
                    "to",
                    "our",
                    "own",
                    "World",
                    "Generation",
                    "module",
                    "later",
                    "on."
                ],
                [
                    "They",
                    "all",
                    "focus",
                    "on",
                    "either",
                    "generating",
                    "or",
                    "playing",
                    "games."
                ]
            ],
            "context": [
                0,
                0,
                1,
                2
            ]
        },
        "input": "sent0: (2020) that focus on creating content especially for 2D visual games via search or reinforcement learning based methods.\n sent1: #REF use knowledge graphs to ground language and produce worlds and quests separately for text games from existing corpora such as stories.\n sent2: #TARGET_REF leverage LIGHT to learn to generate interactive fiction worlds on the basis of locations, characters, and objects-this work is closest in spirit to our own World Generation module later on.\n sent3: They all focus on either generating or playing games.\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [\"sent3\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Ping",
                    "and",
                    "Chi",
                    "(2022)",
                    "(AN(L)P)",
                    "participated",
                    "in",
                    "the",
                    "Entity",
                    "Extraction",
                    "only."
                ],
                [
                    "They",
                    "finetuned",
                    "a",
                    "BERTlarge",
                    "model",
                    "#TARGET_REF",
                    "for",
                    "each",
                    "domain."
                ],
                [
                    "For",
                    "cs.ai",
                    "domain,",
                    "they",
                    "used",
                    "data",
                    "from",
                    "cs.ai",
                    "only,",
                    "whereas,",
                    "for",
                    "the",
                    "other",
                    "domain,",
                    "they",
                    "augmented",
                    "the",
                    "in-domain",
                    "data",
                    "with",
                    "the",
                    "data",
                    "from",
                    "cs.ai."
                ]
            ],
            "context": [
                0,
                3,
                3
            ]
        },
        "input": "sent0: Ping and Chi (2022) (AN(L)P) participated in the Entity Extraction only.\n sent1: They finetuned a BERTlarge model #TARGET_REF for each domain.\n sent2: For cs.ai domain, they used data from cs.ai only, whereas, for the other domain, they augmented the in-domain data with the data from cs.ai.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Table",
                    "5",
                    "presents",
                    "the",
                    "results",
                    "of",
                    "evaluating",
                    "the",
                    "impact",
                    "of",
                    "using",
                    "coreference",
                    "annotations",
                    "to",
                    "improve",
                    "coreference",
                    "reasoning",
                    "in",
                    "MRC."
                ],
                [
                    "We",
                    "report",
                    "the",
                    "re-sults",
                    "for",
                    "both",
                    "of",
                    "the",
                    "examined",
                    "state-of-the-art",
                    "models,",
                    "i."
                ],
                [
                    "#TARGET_REF",
                    "and",
                    "is",
                    "then",
                    "finetuned",
                    "on",
                    "Quoref."
                ]
            ],
            "context": [
                3,
                2,
                2
            ]
        },
        "input": "sent0: Table 5 presents the results of evaluating the impact of using coreference annotations to improve coreference reasoning in MRC.\n sent1: We report the re-sults for both of the examined state-of-the-art models, i.\n sent2: #TARGET_REF and is then finetuned on Quoref.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\", \"sent2\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "modeled",
                    "that",
                    "problem",
                    "as",
                    "a",
                    "heterogeneous",
                    "network."
                ],
                [
                    "The",
                    "structure",
                    "of",
                    "our",
                    "graph",
                    "was",
                    "inspired",
                    "by",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "These",
                    "authors",
                    "modeled",
                    "the",
                    "tasks",
                    "of",
                    "helpfulness",
                    "prediction",
                    "and",
                    "paraphrase",
                    "identification",
                    "as",
                    "a",
                    "heterogeneous",
                    "network,",
                    "respectively."
                ],
                [
                    "For",
                    "that,",
                    "they",
                    "defined",
                    "an",
                    "undirected",
                    "unweighted",
                    "graph",
                    "with",
                    "two",
                    "node",
                    "types:",
                    "sentence",
                    "and",
                    "token."
                ],
                [
                    "However,",
                    "we",
                    "have",
                    "created",
                    "a",
                    "weighted",
                    "graph",
                    "based",
                    "on",
                    "pre-trained",
                    "word",
                    "embeddings."
                ],
                [
                    "The",
                    "weight",
                    "between",
                    "sentence",
                    "and",
                    "token",
                    "nodes",
                    "is",
                    "the",
                    "average",
                    "of",
                    "the",
                    "embedding",
                    "values",
                    "for",
                    "that",
                    "token."
                ],
                [
                    "Figure",
                    "1",
                    "depicts",
                    "an",
                    "example",
                    "of",
                    "a",
                    "sentence",
                    "modeled",
                    "as",
                    "a",
                    "graph."
                ],
                [
                    "From",
                    "this",
                    "figure,",
                    "we",
                    "may",
                    "see",
                    "two",
                    "node",
                    "types:",
                    "token",
                    "and",
                    "sentence,",
                    "and",
                    "an",
                    "undirected",
                    "and",
                    "weighted",
                    "edges",
                    "between",
                    "the",
                    "sentence",
                    "and",
                    "tokens",
                    "nodes."
                ],
                [
                    "To",
                    "extract",
                    "features",
                    "from",
                    "the",
                    "graph",
                    "structure,",
                    "we",
                    "used",
                    "a",
                    "regularization",
                    "algorithm",
                    "that",
                    "propagates",
                    "labels",
                    "from",
                    "a",
                    "small",
                    "set",
                    "of",
                    "labeled",
                    "nodes",
                    "to",
                    "the",
                    "entire",
                    "graph.E(t)",
                    "E(t)",
                    "E(t)",
                    "E(t)",
                    "E(t)",
                    "E(t)We",
                    "evaluated",
                    "the",
                    "approach",
                    "using",
                    "the",
                    "ToLD-Br",
                    "corpus",
                    "#REF",
                    "."
                ],
                [
                    "It",
                    "has",
                    "twenty-one",
                    "thousand",
                    "annotated",
                    "tweets",
                    "as",
                    "either",
                    "toxic",
                    "or",
                    "non-toxic",
                    "language."
                ],
                [
                    "Also,",
                    "we",
                    "compared",
                    "our",
                    "strategy",
                    "with",
                    "different",
                    "graph-based",
                    "methods",
                    "and",
                    "with",
                    "transformerbased",
                    "methods."
                ],
                [
                    "Our",
                    "method",
                    "outperformed",
                    "all",
                    "graph-based",
                    "approaches",
                    "and",
                    "achieved",
                    "competitive",
                    "results",
                    "compared",
                    "to",
                    "transformer-based",
                    "methods,",
                    "using",
                    "only",
                    "10%",
                    "of",
                    "labeled",
                    "nodes."
                ]
            ],
            "context": [
                0,
                2,
                1,
                1,
                2,
                0,
                0,
                0,
                0,
                0,
                2,
                2
            ]
        },
        "input": "sent0: We modeled that problem as a heterogeneous network.\n sent1: The structure of our graph was inspired by #TARGET_REF .\n sent2: These authors modeled the tasks of helpfulness prediction and paraphrase identification as a heterogeneous network, respectively.\n sent3: For that, they defined an undirected unweighted graph with two node types: sentence and token.\n sent4: However, we have created a weighted graph based on pre-trained word embeddings.\n sent5: The weight between sentence and token nodes is the average of the embedding values for that token.\n sent6: Figure 1 depicts an example of a sentence modeled as a graph.\n sent7: From this figure, we may see two node types: token and sentence, and an undirected and weighted edges between the sentence and tokens nodes.\n sent8: To extract features from the graph structure, we used a regularization algorithm that propagates labels from a small set of labeled nodes to the entire graph.E(t) E(t) E(t) E(t) E(t) E(t)We evaluated the approach using the ToLD-Br corpus #REF .\n sent9: It has twenty-one thousand annotated tweets as either toxic or non-toxic language.\n sent10: Also, we compared our strategy with different graph-based methods and with transformerbased methods.\n sent11: Our method outperformed all graph-based approaches and achieved competitive results compared to transformer-based methods, using only 10% of labeled nodes.\n",
        "output": "{\"INFORMATION\": [\"sent2\", \"sent3\"], \"PERCEPTION\": [\"sent1\", \"sent4\", \"sent10\", \"sent11\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "evaluated",
                    "the",
                    "performance",
                    "of",
                    "different",
                    "generation",
                    "models",
                    "from",
                    "two",
                    "aspects:",
                    "quality",
                    "(or",
                    "say",
                    "accuracy)",
                    "and",
                    "diversity."
                ],
                [
                    "Quality",
                    "tests",
                    "the",
                    "appropriateness",
                    "of",
                    "the",
                    "generated",
                    "response",
                    "with",
                    "respect",
                    "to",
                    "the",
                    "context,",
                    "and",
                    "diversity",
                    "tests",
                    "the",
                    "lexical",
                    "and",
                    "semantic",
                    "diversity",
                    "of",
                    "the",
                    "appropriate",
                    "sequences",
                    "generated",
                    "by",
                    "the",
                    "model."
                ],
                [
                    "These",
                    "evaluation",
                    "metrics",
                    "have",
                    "been",
                    "widely",
                    "used",
                    "in",
                    "existing",
                    "work",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                3,
                0,
                2
            ]
        },
        "input": "sent0: We evaluated the performance of different generation models from two aspects: quality (or say accuracy) and diversity.\n sent1: Quality tests the appropriateness of the generated response with respect to the context, and diversity tests the lexical and semantic diversity of the appropriate sequences generated by the model.\n sent2: These evaluation metrics have been widely used in existing work #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Benchmarks",
                    "for",
                    "fact",
                    "verification",
                    "on",
                    "structured",
                    "evidence",
                    "are",
                    "built",
                    "on",
                    "tables",
                    "collected",
                    "from",
                    "Wikipedia",
                    "#REF",
                    "or",
                    "scientific",
                    "articles",
                    "#REF",
                    "."
                ],
                [
                    "Many",
                    "previous",
                    "works",
                    "search",
                    "latent",
                    "programs",
                    "as",
                    "an",
                    "intermediary",
                    "to",
                    "reason",
                    "over",
                    "the",
                    "given",
                    "table."
                ],
                [
                    "They",
                    "directly",
                    "encode",
                    "programs",
                    "#REF",
                    "or",
                    "construct",
                    "heterogeneous",
                    "graphs",
                    "#TARGET_REF",
                    "with",
                    "the",
                    "claim,",
                    "the",
                    "table",
                    "and",
                    "the",
                    "programs."
                ],
                [
                    "Another",
                    "way",
                    "is",
                    "to",
                    "linearize",
                    "the",
                    "input",
                    "table",
                    "and",
                    "perform",
                    "table",
                    "pre-training",
                    "#REF",
                    "and",
                    "add",
                    "additional",
                    "table-aware",
                    "embeddings",
                    "#REF",
                    "to",
                    "enhance",
                    "the",
                    "table",
                    "encoding."
                ],
                [
                    "However,",
                    "in",
                    "these",
                    "datasets,",
                    "the",
                    "evidence",
                    "is",
                    "only",
                    "one",
                    "given",
                    "table,",
                    "and",
                    "models",
                    "are",
                    "not",
                    "requested",
                    "to",
                    "find",
                    "out",
                    "the",
                    "evidence",
                    "cells",
                    "explicitly."
                ]
            ],
            "context": [
                0,
                3,
                1,
                0,
                0
            ]
        },
        "input": "sent0: Benchmarks for fact verification on structured evidence are built on tables collected from Wikipedia #REF or scientific articles #REF .\n sent1: Many previous works search latent programs as an intermediary to reason over the given table.\n sent2: They directly encode programs #REF or construct heterogeneous graphs #TARGET_REF with the claim, the table and the programs.\n sent3: Another way is to linearize the input table and perform table pre-training #REF and add additional table-aware embeddings #REF to enhance the table encoding.\n sent4: However, in these datasets, the evidence is only one given table, and models are not requested to find out the evidence cells explicitly.\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "To",
                    "date,",
                    "several",
                    "approaches",
                    "to",
                    "automated",
                    "detection",
                    "of",
                    "abusive",
                    "language",
                    "have",
                    "been",
                    "proposed,",
                    "including",
                    "rule-based",
                    "#REF",
                    ",",
                    "linguistic",
                    "and",
                    "social",
                    "feature",
                    "engineering",
                    "#REF",
                    ",",
                    "utilizing",
                    "distributed",
                    "representations",
                    "from",
                    "neural",
                    "networks",
                    "#REF",
                    "or",
                    "applying",
                    "deep",
                    "neural",
                    "networks",
                    "directly",
                    "#REF",
                    "."
                ],
                [
                    "Researchers",
                    "have",
                    "also",
                    "explored",
                    "multi-task",
                    "learning",
                    "settings",
                    "with",
                    "objectives",
                    "such",
                    "as",
                    "emotion",
                    "detection",
                    "#REF",
                    "."
                ],
                [
                    "We",
                    "refer",
                    "the",
                    "reader",
                    "to",
                    "recent",
                    "surveys",
                    "of",
                    "the",
                    "field",
                    "#TARGET_REF",
                    "for",
                    "a",
                    "detailed",
                    "literature",
                    "review."
                ]
            ],
            "context": [
                0,
                0,
                1
            ]
        },
        "input": "sent0: To date, several approaches to automated detection of abusive language have been proposed, including rule-based #REF , linguistic and social feature engineering #REF , utilizing distributed representations from neural networks #REF or applying deep neural networks directly #REF .\n sent1: Researchers have also explored multi-task learning settings with objectives such as emotion detection #REF .\n sent2: We refer the reader to recent surveys of the field #TARGET_REF for a detailed literature review.\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "this",
                    "paper,",
                    "our",
                    "metric",
                    "of",
                    "interest",
                    "is",
                    "BERTScore",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "To",
                    "compute",
                    "BERTScore,",
                    "we",
                    "first",
                    "feed",
                    "a",
                    "reference",
                    "and",
                    "candidate",
                    "translation",
                    "for",
                    "a",
                    "given",
                    "sentence",
                    "into",
                    "BERT,",
                    "and",
                    "retrieve",
                    "their",
                    "token",
                    "level",
                    "vector",
                    "representations."
                ],
                [
                    "Let",
                    "z",
                    "be",
                    "the",
                    "representations",
                    "of",
                    "the",
                    "reference",
                    "and",
                    "ẑ",
                    "those",
                    "of",
                    "the",
                    "candidate."
                ],
                [
                    "Then",
                    "we",
                    "compute",
                    "the",
                    "precision",
                    "and",
                    "recall",
                    "metrics",
                    "for",
                    "BERTScore",
                    "by",
                    "comparing",
                    "each",
                    "token",
                    "representation",
                    "z",
                    "i",
                    "of",
                    "the",
                    "reference",
                    "translation",
                    "to",
                    "each",
                    "token",
                    "representation",
                    "ẑj",
                    "of",
                    "the",
                    "candidate",
                    "translation",
                    "as",
                    "follows:P",
                    "BERT",
                    "=",
                    "1",
                    "|ẑ|",
                    "ẑj",
                    "∈ẑ",
                    "max",
                    "z",
                    "i",
                    "∈z",
                    "z",
                    "i",
                    "ẑj",
                    "R",
                    "BERT",
                    "=",
                    "1",
                    "|z|",
                    "z",
                    "i",
                    "∈z",
                    "max",
                    "ẑj",
                    "∈ẑ",
                    "z",
                    "i",
                    "ẑjThe",
                    "F",
                    "1",
                    "score",
                    "can",
                    "be",
                    "defined",
                    "as",
                    "usual."
                ],
                [
                    "As",
                    "BERTScore",
                    "can",
                    "range",
                    "from",
                    "-1",
                    "to",
                    "1,",
                    "but",
                    "most",
                    "often",
                    "inhabits",
                    "the",
                    "upper",
                    "end",
                    "of",
                    "that",
                    "range,",
                    "its",
                    "creators",
                    "suggest",
                    "the",
                    "use",
                    "of",
                    "baseline",
                    "scaling,",
                    "which",
                    "generally",
                    "leaves",
                    "BERTScore",
                    "in",
                    "the",
                    "range",
                    "[0,1],",
                    "as",
                    "desired",
                    "for",
                    "use",
                    "with",
                    "our",
                    "prior",
                    "formalization."
                ],
                [
                    "Baseline",
                    "rescaling",
                    "is",
                    "performed",
                    "for",
                    "P",
                    "BERT",
                    "asPBERT",
                    "=",
                    "P",
                    "BERT",
                    "−",
                    "a",
                    "1",
                    "−",
                    "aand",
                    "likewise",
                    "for",
                    "R",
                    "BERT",
                    ",",
                    "a",
                    "is",
                    "an",
                    "empirical",
                    "lower",
                    "bound",
                    "on",
                    "observed",
                    "BERTScore."
                ]
            ],
            "context": [
                2,
                2,
                3,
                2,
                3,
                2
            ]
        },
        "input": "sent0: In this paper, our metric of interest is BERTScore #TARGET_REF .\n sent1: To compute BERTScore, we first feed a reference and candidate translation for a given sentence into BERT, and retrieve their token level vector representations.\n sent2: Let z be the representations of the reference and ẑ those of the candidate.\n sent3: Then we compute the precision and recall metrics for BERTScore by comparing each token representation z i of the reference translation to each token representation ẑj of the candidate translation as follows:P BERT = 1 |ẑ| ẑj ∈ẑ max z i ∈z z i ẑj R BERT = 1 |z| z i ∈z max ẑj ∈ẑ z i ẑjThe F 1 score can be defined as usual.\n sent4: As BERTScore can range from -1 to 1, but most often inhabits the upper end of that range, its creators suggest the use of baseline scaling, which generally leaves BERTScore in the range [0,1], as desired for use with our prior formalization.\n sent5: Baseline rescaling is performed for P BERT asPBERT = P BERT − a 1 − aand likewise for R BERT , a is an empirical lower bound on observed BERTScore.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\", \"sent1\", \"sent3\", \"sent5\"], \"BACKGROUND\": [\"sent2\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Let",
                    "{d",
                    "i",
                    "|1",
                    "≤",
                    "i",
                    "≤",
                    "N",
                    "}",
                    "be",
                    "a",
                    "set",
                    "of",
                    "dialogues,",
                    "and",
                    "each",
                    "dialogue",
                    "contains",
                    "n",
                    "d",
                    "turns:d",
                    "i",
                    "=",
                    "{(c",
                    "t",
                    ",",
                    "a",
                    "t",
                    ",",
                    "x",
                    "t",
                    ")|1",
                    "≤",
                    "t",
                    "≤",
                    "n",
                    "d",
                    "},",
                    "where",
                    "c",
                    "t",
                    "is",
                    "the",
                    "context",
                    "at",
                    "turn",
                    "t,",
                    "and",
                    "a",
                    "t",
                    "is",
                    "the",
                    "dialogue",
                    "action",
                    "of",
                    "system",
                    "utterance",
                    "x",
                    "t",
                    "."
                ],
                [
                    "The",
                    "context",
                    "c",
                    "t",
                    "=",
                    "{u",
                    "1",
                    ",",
                    "x",
                    "1",
                    ",",
                    "...,",
                    "u",
                    "t",
                    "}",
                    "consists",
                    "of",
                    "the",
                    "dialogue",
                    "history",
                    "of",
                    "user",
                    "utterances",
                    "u",
                    "and",
                    "system",
                    "utterances",
                    "x."
                ],
                [
                    "Conditioned",
                    "response",
                    "generation",
                    "tackles",
                    "the",
                    "context-to-response",
                    "generation",
                    "problem",
                    "p(x|c)",
                    "via",
                    "two",
                    "consecutive",
                    "steps:",
                    "a",
                    "content",
                    "planning",
                    "step",
                    "decides",
                    "a",
                    "dialogue",
                    "action",
                    "to",
                    "proceed",
                    "the",
                    "dialogues",
                    "p",
                    "l",
                    "(a|c),",
                    "and",
                    "a",
                    "surface",
                    "realization",
                    "step",
                    "further",
                    "trans-forms",
                    "the",
                    "decided",
                    "action",
                    "into",
                    "naturally",
                    "sound",
                    "utterances",
                    "p",
                    "r",
                    "(x|a,",
                    "c)."
                ],
                [
                    "Using",
                    "the",
                    "two-step",
                    "process,",
                    "response",
                    "generation",
                    "could",
                    "be",
                    "optimized",
                    "towards",
                    "better",
                    "task",
                    "completion",
                    "while",
                    "maintaining",
                    "high-quality",
                    "language",
                    "quality",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "The",
                    "optimization",
                    "process",
                    "also",
                    "consists",
                    "of",
                    "two",
                    "parts."
                ],
                [
                    "Firstly,",
                    "context-action",
                    "pairs",
                    "are",
                    "used",
                    "to",
                    "train",
                    "the",
                    "content",
                    "planning",
                    "model",
                    "p",
                    "l",
                    "(a|c)",
                    "using",
                    "the",
                    "cross-entropy",
                    "loss.L",
                    "act",
                    "=",
                    "d",
                    "i",
                    "t=1:n",
                    "d",
                    "−",
                    "log(a",
                    "t",
                    "•",
                    "p",
                    "l",
                    "(a|c",
                    "t",
                    "))",
                    "(1)Then,",
                    "the",
                    "surface",
                    "realization",
                    "model",
                    "p",
                    "r",
                    "(x|a,",
                    "c)",
                    "is",
                    "optimized",
                    "from",
                    "the",
                    "(c",
                    "t",
                    ",",
                    "a",
                    "t",
                    ",",
                    "x",
                    "t",
                    ")",
                    "triples",
                    "to",
                    "maximize",
                    "the",
                    "likelihood",
                    "of",
                    "ground-truth",
                    "responses"
                ]
            ],
            "context": [
                0,
                0,
                3,
                1,
                0,
                0
            ]
        },
        "input": "sent0: Let {d i |1 ≤ i ≤ N } be a set of dialogues, and each dialogue contains n d turns:d i = {(c t , a t , x t )|1 ≤ t ≤ n d }, where c t is the context at turn t, and a t is the dialogue action of system utterance x t .\n sent1: The context c t = {u 1 , x 1 , ..., u t } consists of the dialogue history of user utterances u and system utterances x.\n sent2: Conditioned response generation tackles the context-to-response generation problem p(x|c) via two consecutive steps: a content planning step decides a dialogue action to proceed the dialogues p l (a|c), and a surface realization step further trans-forms the decided action into naturally sound utterances p r (x|a, c).\n sent3: Using the two-step process, response generation could be optimized towards better task completion while maintaining high-quality language quality #TARGET_REF .\n sent4: The optimization process also consists of two parts.\n sent5: Firstly, context-action pairs are used to train the content planning model p l (a|c) using the cross-entropy loss.L act = d i t=1:n d − log(a t • p l (a|c t )) (1)Then, the surface realization model p r (x|a, c) is optimized from the (c t , a t , x t ) triples to maximize the likelihood of ground-truth responses\n",
        "output": "{\"INFORMATION\": [\"sent3\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "extensively",
                    "evaluate",
                    "top-k",
                    "attention",
                    "on",
                    "a",
                    "wide",
                    "range",
                    "of",
                    "tasks",
                    "and",
                    "demonstrate",
                    "its",
                    "mentioned",
                    "advantages."
                ],
                [
                    "Training",
                    "from",
                    "scratch,",
                    "we",
                    "show",
                    "top-k",
                    "attention",
                    "performs",
                    "as",
                    "well",
                    "as",
                    "vanilla",
                    "self-attention",
                    "on",
                    "Long",
                    "Range",
                    "Arena,",
                    "a",
                    "benchmark",
                    "dedicated",
                    "to",
                    "evaluating",
                    "the",
                    "ability",
                    "of",
                    "transformers",
                    "to",
                    "handle",
                    "long",
                    "sequences,",
                    "and",
                    "in",
                    "a",
                    "language",
                    "modeling",
                    "task",
                    "(WikiText-103)."
                ],
                [
                    "Second,",
                    "we",
                    "show",
                    "top-k",
                    "attention",
                    "can",
                    "be",
                    "used",
                    "as",
                    "a",
                    "drop-in",
                    "replacement",
                    "for",
                    "vanilla",
                    "attention",
                    "at",
                    "inference",
                    "time",
                    "without",
                    "any",
                    "additional",
                    "training",
                    "at",
                    "the",
                    "feed-forward",
                    "layer",
                    "of",
                    "the",
                    "UnifiedQA",
                    "model",
                    "#TARGET_REF",
                    "on",
                    "12",
                    "different",
                    "question",
                    "answering",
                    "(QA)",
                    "datasets,",
                    "reducing",
                    "the",
                    "number",
                    "of",
                    "keys",
                    "used",
                    "per",
                    "query",
                    "by",
                    "more",
                    "than",
                    "99%."
                ],
                [
                    "Last,",
                    "we",
                    "show",
                    "top-k",
                    "attention",
                    "obtains",
                    "similar",
                    "performance",
                    "to",
                    "vanilla",
                    "attention",
                    "on",
                    "a",
                    "wide",
                    "range",
                    "of",
                    "QA",
                    "tasks",
                    "when",
                    "fine-tuning",
                    "T5",
                    "#REF",
                    ",",
                    "without",
                    "the",
                    "need",
                    "for",
                    "any",
                    "corrective",
                    "pre-training."
                ]
            ],
            "context": [
                2,
                2,
                2,
                0
            ]
        },
        "input": "sent0: We extensively evaluate top-k attention on a wide range of tasks and demonstrate its mentioned advantages.\n sent1: Training from scratch, we show top-k attention performs as well as vanilla self-attention on Long Range Arena, a benchmark dedicated to evaluating the ability of transformers to handle long sequences, and in a language modeling task (WikiText-103).\n sent2: Second, we show top-k attention can be used as a drop-in replacement for vanilla attention at inference time without any additional training at the feed-forward layer of the UnifiedQA model #TARGET_REF on 12 different question answering (QA) datasets, reducing the number of keys used per query by more than 99%.\n sent3: Last, we show top-k attention obtains similar performance to vanilla attention on a wide range of QA tasks when fine-tuning T5 #REF , without the need for any corrective pre-training.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\", \"sent1\", \"sent2\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "perplexities",
                    "on",
                    "the",
                    "development",
                    "data",
                    "are",
                    "summarized",
                    "in",
                    "Table",
                    "2."
                ],
                [
                    "It",
                    "is",
                    "not",
                    "surprising",
                    "to",
                    "see",
                    "that",
                    "adding",
                    "the",
                    "development",
                    "data",
                    "of",
                    "previous",
                    "evaluations",
                    "improves",
                    "the",
                    "perplexity",
                    "since",
                    "this",
                    "more",
                    "than",
                    "doubles",
                    "the",
                    "amount",
                    "of",
                    "in-domain",
                    "data."
                ],
                [
                    "The",
                    "small",
                    "Gale",
                    "as",
                    "well",
                    "as",
                    "the",
                    "large",
                    "Gigaword",
                    "corpus",
                    "have",
                    "also",
                    "a",
                    "noticeable",
                    "effect."
                ],
                [
                    "1",
                    "The",
                    "continuous",
                    "space",
                    "language",
                    "model",
                    "was",
                    "trained",
                    "on",
                    "all",
                    "the",
                    "available",
                    "data,",
                    "including",
                    "the",
                    "large",
                    "Gigaword",
                    "corpus,",
                    "using",
                    "a",
                    "resampling",
                    "algorithm",
                    "#REF",
                    "."
                ],
                [
                    "This",
                    "approach",
                    "achieved",
                    "a",
                    "reduction",
                    "in",
                    "perplexity",
                    "of",
                    "more",
                    "than",
                    "15%",
                    "in",
                    "comparison",
                    "to",
                    "the",
                    "large",
                    "back-off",
                    "language",
                    "model."
                ],
                [
                    "This",
                    "is",
                    "inline",
                    "with",
                    "results",
                    "obtained",
                    "in",
                    "previous",
                    "IWSLT",
                    "evaluations",
                    "#TARGET_REF",
                    ",",
                    "but",
                    "here",
                    "both",
                    "language",
                    "models",
                    "are",
                    "trained",
                    "on",
                    "substantially",
                    "more",
                    "data."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                3,
                2
            ]
        },
        "input": "sent0: The perplexities on the development data are summarized in Table 2.\n sent1: It is not surprising to see that adding the development data of previous evaluations improves the perplexity since this more than doubles the amount of in-domain data.\n sent2: The small Gale as well as the large Gigaword corpus have also a noticeable effect.\n sent3: 1 The continuous space language model was trained on all the available data, including the large Gigaword corpus, using a resampling algorithm #REF .\n sent4: This approach achieved a reduction in perplexity of more than 15% in comparison to the large back-off language model.\n sent5: This is inline with results obtained in previous IWSLT evaluations #TARGET_REF , but here both language models are trained on substantially more data.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent5\"], \"BACKGROUND\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "A",
                    "standard",
                    "and",
                    "probably",
                    "the",
                    "most",
                    "common",
                    "method",
                    "to",
                    "calculate",
                    "word",
                    "relatedness",
                    "from",
                    "co-occurrences",
                    "is",
                    "computing",
                    "the",
                    "pointwise",
                    "mutual",
                    "information",
                    "(PMI)",
                    "of",
                    "two",
                    "words."
                ],
                [
                    "However,",
                    "PMI",
                    "has",
                    "wellknown",
                    "shortcomings,",
                    "such",
                    "as",
                    "overvaluing",
                    "the",
                    "relatedness",
                    "of",
                    "rare",
                    "words,",
                    "and",
                    "lacking",
                    "a",
                    "fixed",
                    "upper",
                    "and",
                    "lower",
                    "bound."
                ],
                [
                    "#TARGET_REF",
                    "introduced",
                    "normalized",
                    "PMI",
                    "as",
                    "PMI",
                    "norm",
                    "(x,",
                    "y)",
                    "=",
                    "ln",
                    "p(x,",
                    "y)",
                    "p(x)p(y)",
                    "−",
                    "ln",
                    "p(x,",
                    "y),"
                ]
            ],
            "context": [
                2,
                3,
                3
            ]
        },
        "input": "sent0: A standard and probably the most common method to calculate word relatedness from co-occurrences is computing the pointwise mutual information (PMI) of two words.\n sent1: However, PMI has wellknown shortcomings, such as overvaluing the relatedness of rare words, and lacking a fixed upper and lower bound.\n sent2: #TARGET_REF introduced normalized PMI as PMI norm (x, y) = ln p(x, y) p(x)p(y) − ln p(x, y),\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "To",
                    "deal",
                    "with",
                    "the",
                    "absence",
                    "of",
                    "action",
                    "annotations,",
                    "latent",
                    "action",
                    "learning",
                    "has",
                    "been",
                    "introduced",
                    "#REF",
                    "."
                ],
                [
                    "System",
                    "utterances",
                    "are",
                    "represented",
                    "as",
                    "low-dimensional",
                    "latent",
                    "variables",
                    "by",
                    "an",
                    "auto-encoding",
                    "task",
                    "#TARGET_REF",
                    ",",
                    "and",
                    "utterances",
                    "with",
                    "the",
                    "same",
                    "representations",
                    "are",
                    "considered",
                    "to",
                    "convey",
                    "similar",
                    "meanings."
                ],
                [
                    "Such",
                    "action",
                    "representations",
                    "might",
                    "be",
                    "prone",
                    "to",
                    "overdependence",
                    "on",
                    "the",
                    "training",
                    "data,",
                    "which",
                    "restricts",
                    "the",
                    "model",
                    "generalization",
                    "capability,",
                    "especially",
                    "when",
                    "multiple",
                    "domains",
                    "are",
                    "considered."
                ],
                [
                    "This",
                    "is",
                    "because,",
                    "without",
                    "explicit",
                    "supervision,",
                    "the",
                    "desired",
                    "property",
                    "of",
                    "capturing",
                    "the",
                    "intentions",
                    "of",
                    "system",
                    "utterances",
                    "in",
                    "the",
                    "latent",
                    "space",
                    "cannot",
                    "be",
                    "enforced",
                    "#REF",
                    ",",
                    "which",
                    "in",
                    "turn",
                    "is",
                    "due",
                    "to",
                    "the",
                    "implicit",
                    "nature",
                    "of",
                    "latent",
                    "variables."
                ],
                [
                    "For",
                    "example,",
                    "variational",
                    "auto-encoder",
                    "(VAE),",
                    "which",
                    "is",
                    "often",
                    "used",
                    "for",
                    "latent",
                    "action",
                    "learning,",
                    "tends",
                    "to",
                    "produce",
                    "a",
                    "balanced",
                    "distribution",
                    "over",
                    "the",
                    "latent",
                    "variables",
                    "#REF",
                    ",",
                    "while",
                    "the",
                    "true",
                    "distribution",
                    "of",
                    "system",
                    "actions",
                    "is",
                    "highly",
                    "imbalanced",
                    "#REF",
                    "."
                ],
                [
                    "The",
                    "resulting",
                    "misaligned",
                    "action",
                    "representations",
                    "would",
                    "confuse",
                    "the",
                    "model",
                    "of",
                    "both",
                    "steps",
                    "and",
                    "degenerate",
                    "the",
                    "sample",
                    "efficiency",
                    "in",
                    "training."
                ]
            ],
            "context": [
                2,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: To deal with the absence of action annotations, latent action learning has been introduced #REF .\n sent1: System utterances are represented as low-dimensional latent variables by an auto-encoding task #TARGET_REF , and utterances with the same representations are considered to convey similar meanings.\n sent2: Such action representations might be prone to overdependence on the training data, which restricts the model generalization capability, especially when multiple domains are considered.\n sent3: This is because, without explicit supervision, the desired property of capturing the intentions of system utterances in the latent space cannot be enforced #REF , which in turn is due to the implicit nature of latent variables.\n sent4: For example, variational auto-encoder (VAE), which is often used for latent action learning, tends to produce a balanced distribution over the latent variables #REF , while the true distribution of system actions is highly imbalanced #REF .\n sent5: The resulting misaligned action representations would confuse the model of both steps and degenerate the sample efficiency in training.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Why",
                    "care",
                    "about",
                    "compositionality",
                    "in",
                    "semantic",
                    "parsing?"
                ],
                [
                    "If",
                    "the",
                    "goal",
                    "of",
                    "semantic",
                    "parsing",
                    "is",
                    "not",
                    "merely",
                    "to",
                    "automatically",
                    "obtain",
                    "a",
                    "representation",
                    "of",
                    "the",
                    "meaning",
                    "of",
                    "an",
                    "utterance",
                    "but",
                    "also",
                    "to",
                    "understand",
                    "why",
                    "the",
                    "parser",
                    "produced",
                    "that",
                    "answer,",
                    "i.e.,",
                    "an",
                    "explainable",
                    "and",
                    "transparent",
                    "system,",
                    "compositionality",
                    "can",
                    "help."
                ],
                [
                    "In",
                    "particular,",
                    "in",
                    "the",
                    "output",
                    "of",
                    "our",
                    "parser,",
                    "every",
                    "token",
                    "is",
                    "mapped",
                    "to",
                    "one",
                    "of",
                    "a",
                    "finite",
                    "number",
                    "of",
                    "meaning",
                    "fragments",
                    "(unlike",
                    "a",
                    "sequence-to-sequence",
                    "system",
                    "where",
                    "a",
                    "single",
                    "token",
                    "can",
                    "in",
                    "principle",
                    "give",
                    "rise",
                    "to",
                    "an",
                    "unbounded",
                    "number",
                    "of",
                    "output",
                    "symbols),",
                    "every",
                    "clause",
                    "belongs",
                    "to",
                    "one",
                    "of",
                    "these",
                    "fragments",
                    "(unlike",
                    "a",
                    "sequence-to-sequence",
                    "system",
                    "where",
                    "the",
                    "output",
                    "is",
                    "not",
                    "usually",
                    "anchored),",
                    "and",
                    "there",
                    "is",
                    "a",
                    "straightforward",
                    "rule",
                    "that",
                    "combines",
                    "fragments",
                    "into",
                    "utterance",
                    "meanings",
                    "(unlike",
                    "sequence-to-sequence",
                    "systems",
                    "where",
                    "the",
                    "interactions",
                    "between",
                    "tokens",
                    "are",
                    "opaque)."
                ],
                [
                    "This",
                    "type",
                    "of",
                    "transparency",
                    "is",
                    "especially",
                    "important",
                    "in",
                    "human-in-the-loop",
                    "annotation,",
                    "where",
                    "parsers",
                    "produce",
                    "an",
                    "initial",
                    "annotation",
                    "and",
                    "annotators",
                    "correct",
                    "them."
                ],
                [
                    "To",
                    "do",
                    "this",
                    "efficiently",
                    "and",
                    "consistenly,",
                    "annotators",
                    "need",
                    "to",
                    "pinpoint",
                    "where",
                    "an",
                    "error",
                    "arises,",
                    "and",
                    "word-meaning",
                    "pairings",
                    "with",
                    "a",
                    "finite",
                    "number",
                    "of",
                    "meanings",
                    "seem",
                    "a",
                    "good",
                    "handle",
                    "on",
                    "that."
                ],
                [
                    "#TARGET_REF",
                    "make",
                    "a",
                    "similar",
                    "argument",
                    "about",
                    "grammar-based",
                    "sembanking,",
                    "pointing",
                    "out",
                    "the",
                    "consistency,",
                    "comprehensiveness,",
                    "and",
                    "scalability",
                    "that",
                    "compositionality",
                    "afford."
                ],
                [
                    "sequence-to-sequence",
                    "ones",
                    "is",
                    "a",
                    "big",
                    "step",
                    "ahead",
                    "towards",
                    "transparent",
                    "DRS",
                    "parsing."
                ],
                [
                    "It",
                    "is",
                    "also",
                    "worth",
                    "noting",
                    "that",
                    "our",
                    "sequence",
                    "encoding",
                    "scheme",
                    "is",
                    "equally",
                    "applicable",
                    "to",
                    "incremental",
                    "parsers,",
                    "which",
                    "potentailly",
                    "afford",
                    "a",
                    "greater",
                    "degree",
                    "of",
                    "psycholinguistic",
                    "plausibility."
                ],
                [
                    "In",
                    "addition,",
                    "the",
                    "multi-task",
                    "architecture",
                    "of",
                    "our",
                    "approach",
                    "is",
                    "modular",
                    "and",
                    "allows",
                    "for",
                    "arbitrary",
                    "additional",
                    "sequence",
                    "labeling",
                    "tasks",
                    "and",
                    "factorizations."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Why care about compositionality in semantic parsing?\n sent1: If the goal of semantic parsing is not merely to automatically obtain a representation of the meaning of an utterance but also to understand why the parser produced that answer, i.e., an explainable and transparent system, compositionality can help.\n sent2: In particular, in the output of our parser, every token is mapped to one of a finite number of meaning fragments (unlike a sequence-to-sequence system where a single token can in principle give rise to an unbounded number of output symbols), every clause belongs to one of these fragments (unlike a sequence-to-sequence system where the output is not usually anchored), and there is a straightforward rule that combines fragments into utterance meanings (unlike sequence-to-sequence systems where the interactions between tokens are opaque).\n sent3: This type of transparency is especially important in human-in-the-loop annotation, where parsers produce an initial annotation and annotators correct them.\n sent4: To do this efficiently and consistenly, annotators need to pinpoint where an error arises, and word-meaning pairings with a finite number of meanings seem a good handle on that.\n sent5: #TARGET_REF make a similar argument about grammar-based sembanking, pointing out the consistency, comprehensiveness, and scalability that compositionality afford.\n sent6: sequence-to-sequence ones is a big step ahead towards transparent DRS parsing.\n sent7: It is also worth noting that our sequence encoding scheme is equally applicable to incremental parsers, which potentailly afford a greater degree of psycholinguistic plausibility.\n sent8: In addition, the multi-task architecture of our approach is modular and allows for arbitrary additional sequence labeling tasks and factorizations.\n",
        "output": "{\"INFORMATION\": [\"sent5\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "ture."
                ],
                [
                    "Dense",
                    "representations",
                    "of",
                    "entities",
                    "have",
                    "similarly",
                    "been",
                    "applied",
                    "to",
                    "entity",
                    "linking",
                    "#REF",
                    ",",
                    "as",
                    "well",
                    "as",
                    "relation",
                    "extraction",
                    "#TARGET_REF",
                    ",",
                    "entity",
                    "typing",
                    "#REF",
                    ",",
                    "and",
                    "question",
                    "answering",
                    "#REF",
                    "."
                ],
                [
                    "Those",
                    "approaches",
                    "use",
                    "millions",
                    "of",
                    "predefined",
                    "entities,",
                    "while",
                    "our",
                    "approach",
                    "uses",
                    "a",
                    "much",
                    "smaller",
                    "number",
                    "of",
                    "types",
                    "(10k",
                    "or",
                    "60k)."
                ],
                [
                    "This",
                    "makes",
                    "it",
                    "simultaneously",
                    "more",
                    "compact",
                    "and",
                    "also",
                    "more",
                    "flexible",
                    "when",
                    "generalizing",
                    "to",
                    "unknown",
                    "entities."
                ]
            ],
            "context": [
                0,
                3,
                0,
                0
            ]
        },
        "input": "sent0: ture.\n sent1: Dense representations of entities have similarly been applied to entity linking #REF , as well as relation extraction #TARGET_REF , entity typing #REF , and question answering #REF .\n sent2: Those approaches use millions of predefined entities, while our approach uses a much smaller number of types (10k or 60k).\n sent3: This makes it simultaneously more compact and also more flexible when generalizing to unknown entities.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "For",
                    "each",
                    "potential",
                    "shortcut",
                    "token,",
                    "we",
                    "extract",
                    "N",
                    "synonyms",
                    "by",
                    "leveraging",
                    "the",
                    "word",
                    "embeddings",
                    "curated",
                    "for",
                    "synonym",
                    "extraction",
                    "#TARGET_REF",
                    ",",
                    "plus",
                    "WordNet",
                    "#REF",
                    "and",
                    "DBpedia",
                    "#REF",
                    "."
                ],
                [
                    "More",
                    "specifically,",
                    "for",
                    "each",
                    "top",
                    "token",
                    "t",
                    "in",
                    "the",
                    "list",
                    "generated",
                    "by",
                    "the",
                    "previous",
                    "step,",
                    "we",
                    "first",
                    "search",
                    "counter-fitting",
                    "word",
                    "vectors",
                    "to",
                    "find",
                    "synonyms",
                    "with",
                    "cosine",
                    "similarity",
                    "larger",
                    "than",
                    "a",
                    "threshold",
                    "4",
                    "τ",
                    "."
                ],
                [
                    "Additionally",
                    "we",
                    "search",
                    "in",
                    "WordNet",
                    "and",
                    "DBpedia",
                    "to",
                    "obtain",
                    "a",
                    "maximum",
                    "of",
                    "N",
                    "synonyms",
                    "for",
                    "each",
                    "token",
                    "t.",
                    "Then",
                    "we",
                    "extract",
                    "a",
                    "subset",
                    "S",
                    "t",
                    "from",
                    "D,",
                    "which",
                    "consists",
                    "of",
                    "sentences",
                    "containing",
                    "t.",
                    "We",
                    "perturb",
                    "all",
                    "sentences",
                    "in",
                    "S",
                    "t",
                    "by",
                    "replacing",
                    "t",
                    "with",
                    "its",
                    "synonyms."
                ],
                [
                    "The",
                    "resulted",
                    "perturbed",
                    "set",
                    "S",
                    "′",
                    "t",
                    "is",
                    "N",
                    "times",
                    "of",
                    "the",
                    "original",
                    "set",
                    "S",
                    "t",
                    "."
                ],
                [
                    "We",
                    "apply",
                    "model",
                    "f",
                    "on",
                    "S",
                    "t",
                    "and",
                    "S",
                    "′",
                    "t",
                    "and",
                    "obtain",
                    "accuracy",
                    "acc",
                    "t",
                    "and",
                    "acc",
                    "′",
                    "t",
                    "."
                ],
                [
                    "Since",
                    "we",
                    "only",
                    "perturb",
                    "S",
                    "t",
                    "with",
                    "t's",
                    "synonyms,",
                    "the",
                    "semantic",
                    "meaning",
                    "of",
                    "perturbed",
                    "sentences",
                    "should",
                    "stay",
                    "close",
                    "to",
                    "the",
                    "original",
                    "sentences."
                ],
                [
                    "Thus,",
                    "if",
                    "t",
                    "is",
                    "a",
                    "genuine",
                    "token,",
                    "acc",
                    "′",
                    "t",
                    "is",
                    "expected",
                    "to",
                    "be",
                    "close",
                    "to",
                    "acc",
                    "t",
                    "."
                ],
                [
                    "On",
                    "the",
                    "other",
                    "hand,",
                    "if",
                    "t",
                    "is",
                    "a",
                    "shortcut,",
                    "model",
                    "prediction",
                    "can",
                    "be",
                    "different",
                    "even",
                    "the",
                    "semantic",
                    "meaning",
                    "of",
                    "the",
                    "sentence",
                    "does",
                    "not",
                    "change",
                    "a",
                    "lot",
                    "(see",
                    "examples",
                    "in",
                    "Table",
                    "2)."
                ],
                [
                    "Thus,",
                    "we",
                    "assume",
                    "tokens",
                    "with",
                    "larger",
                    "differences",
                    "between",
                    "acc",
                    "t",
                    "and",
                    "acc",
                    "′",
                    "t",
                    "are",
                    "more",
                    "likely",
                    "to",
                    "be",
                    "shortcuts",
                    "and",
                    "tokens",
                    "with",
                    "smaller",
                    "differences",
                    "are",
                    "more",
                    "likely",
                    "to",
                    "be",
                    "domain",
                    "specific",
                    "\"genuine\"",
                    "words."
                ],
                [
                    "From",
                    "the",
                    "potential",
                    "shortcut",
                    "token",
                    "list",
                    "computed",
                    "in",
                    "Sec",
                    "3.2,",
                    "we",
                    "remove",
                    "tokens",
                    "with",
                    "performance",
                    "difference",
                    "smaller",
                    "than",
                    "δ",
                    "to",
                    "further",
                    "filter",
                    "domain",
                    "specific",
                    "\"geniue\"",
                    "tokens",
                    "."
                ]
            ],
            "context": [
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: For each potential shortcut token, we extract N synonyms by leveraging the word embeddings curated for synonym extraction #TARGET_REF , plus WordNet #REF and DBpedia #REF .\n sent1: More specifically, for each top token t in the list generated by the previous step, we first search counter-fitting word vectors to find synonyms with cosine similarity larger than a threshold 4 τ .\n sent2: Additionally we search in WordNet and DBpedia to obtain a maximum of N synonyms for each token t. Then we extract a subset S t from D, which consists of sentences containing t. We perturb all sentences in S t by replacing t with its synonyms.\n sent3: The resulted perturbed set S ′ t is N times of the original set S t .\n sent4: We apply model f on S t and S ′ t and obtain accuracy acc t and acc ′ t .\n sent5: Since we only perturb S t with t's synonyms, the semantic meaning of perturbed sentences should stay close to the original sentences.\n sent6: Thus, if t is a genuine token, acc ′ t is expected to be close to acc t .\n sent7: On the other hand, if t is a shortcut, model prediction can be different even the semantic meaning of the sentence does not change a lot (see examples in Table 2).\n sent8: Thus, we assume tokens with larger differences between acc t and acc ′ t are more likely to be shortcuts and tokens with smaller differences are more likely to be domain specific \"genuine\" words.\n sent9: From the potential shortcut token list computed in Sec 3.2, we remove tokens with performance difference smaller than δ to further filter domain specific \"geniue\" tokens .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "all",
                    "experiments",
                    "involving",
                    "BERT,",
                    "we",
                    "use",
                    "the",
                    "BERT",
                    "Base-uncased",
                    "model",
                    "#REF",
                    "."
                ],
                [
                    "It",
                    "has",
                    "12",
                    "layers",
                    "and",
                    "each",
                    "layer",
                    "contains",
                    "12",
                    "attention",
                    "heads,",
                    "summing",
                    "to",
                    "144",
                    "attention",
                    "heads."
                ],
                [
                    "We",
                    "fine-tune",
                    "and",
                    "evaluate",
                    "the",
                    "pre-trained",
                    "model",
                    "2",
                    "on",
                    "sentence",
                    "entailment",
                    "task",
                    "MNLI-M,",
                    "the",
                    "question",
                    "similarity",
                    "task",
                    "QQP,",
                    "the",
                    "question-answering",
                    "task",
                    "QNLI,",
                    "and",
                    "the",
                    "movie",
                    "review",
                    "task",
                    "SST-2",
                    "from",
                    "the",
                    "GLUE",
                    "Benchmark",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "We",
                    "report",
                    "accuracies",
                    "on",
                    "the",
                    "official",
                    "development",
                    "sets",
                    "of",
                    "the",
                    "considered",
                    "GLUE",
                    "tasks."
                ],
                [
                    "For",
                    "each",
                    "of",
                    "the",
                    "four",
                    "GLUE",
                    "tasks,",
                    "namely",
                    "MNLI-M,",
                    "QQP,",
                    "QNLI",
                    "and",
                    "SST-2,",
                    "we",
                    "tried",
                    "combinations",
                    "of",
                    "batch",
                    "size",
                    "and",
                    "learning",
                    "rate",
                    "from",
                    "{8,",
                    "16,",
                    "32,",
                    "64,",
                    "128}",
                    "and",
                    "{2,",
                    "3,",
                    "4,",
                    "5}",
                    "×",
                    "10",
                    "−5",
                    "respectively",
                    "and",
                    "selected",
                    "the",
                    "best",
                    "performing",
                    "configuration."
                ],
                [
                    "The",
                    "exact",
                    "hyperparameters",
                    "used",
                    "for",
                    "each",
                    "of",
                    "the",
                    "tasks",
                    "have",
                    "been",
                    "made",
                    "available",
                    "with",
                    "the",
                    "code",
                    "released",
                    "3",
                    "."
                ],
                [
                    "Each",
                    "BERT",
                    "experiment",
                    "was",
                    "run",
                    "on",
                    "a",
                    "single",
                    "Cloud",
                    "TPU",
                    "(v2-8)."
                ]
            ],
            "context": [
                0,
                0,
                2,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: In all experiments involving BERT, we use the BERT Base-uncased model #REF .\n sent1: It has 12 layers and each layer contains 12 attention heads, summing to 144 attention heads.\n sent2: We fine-tune and evaluate the pre-trained model 2 on sentence entailment task MNLI-M, the question similarity task QQP, the question-answering task QNLI, and the movie review task SST-2 from the GLUE Benchmark #TARGET_REF .\n sent3: We report accuracies on the official development sets of the considered GLUE tasks.\n sent4: For each of the four GLUE tasks, namely MNLI-M, QQP, QNLI and SST-2, we tried combinations of batch size and learning rate from {8, 16, 32, 64, 128} and {2, 3, 4, 5} × 10 −5 respectively and selected the best performing configuration.\n sent5: The exact hyperparameters used for each of the tasks have been made available with the code released 3 .\n sent6: Each BERT experiment was run on a single Cloud TPU (v2-8).\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "More",
                    "recently,",
                    "neural",
                    "networks-based",
                    "strategies",
                    "and",
                    "transformer-based",
                    "architectures",
                    "has",
                    "been",
                    "applied",
                    "to",
                    "hate",
                    "speech",
                    "detection",
                    "due",
                    "to",
                    "the",
                    "good",
                    "results",
                    "achieved",
                    "in",
                    "various",
                    "tasks."
                ],
                [
                    "#REF",
                    "2021)",
                    "compared",
                    "two",
                    "pre-trained",
                    "language",
                    "models,",
                    "such",
                    "as",
                    "BERT",
                    "#TARGET_REF",
                    "and",
                    "XLM",
                    "(CONNEAU",
                    "and",
                    "Lample,",
                    "2019)",
                    "trained",
                    "to",
                    "detect",
                    "hate",
                    "speech",
                    "in",
                    "the",
                    "Spanish",
                    "language."
                ]
            ],
            "context": [
                3,
                2
            ]
        },
        "input": "sent0: More recently, neural networks-based strategies and transformer-based architectures has been applied to hate speech detection due to the good results achieved in various tasks.\n sent1: #REF 2021) compared two pre-trained language models, such as BERT #TARGET_REF and XLM (CONNEAU and Lample, 2019) trained to detect hate speech in the Spanish language.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "Peru,",
                    "before",
                    "NMT,",
                    "there",
                    "were",
                    "studies",
                    "in",
                    "rulebased",
                    "MT,",
                    "based",
                    "on",
                    "the",
                    "Apertium",
                    "platform",
                    "#REF",
                    ",",
                    "for",
                    "Quechua",
                    "Eastern",
                    "Apurimac",
                    "(qve)",
                    "and",
                    "Quechua",
                    "Cuzco",
                    "(quz)",
                    "#REF",
                    "."
                ],
                [
                    "Furthermore,",
                    "#REF",
                    "improved",
                    "alignments",
                    "for",
                    "quz",
                    "by",
                    "using",
                    "an",
                    "agglutinative",
                    "language",
                    "as",
                    "Finnish",
                    "as",
                    "a",
                    "pivot."
                ],
                [
                    "Apart",
                    "from",
                    "the",
                    "Quechua",
                    "variants,",
                    "only",
                    "Aymara",
                    "#REF",
                    "and",
                    "Shipibo-Konibo",
                    "#TARGET_REF",
                    "have",
                    "been",
                    "addressed",
                    "with",
                    "rule-based",
                    "and",
                    "statistical",
                    "MT,",
                    "respectively."
                ],
                [
                    "#REF",
                    "for",
                    "Southern",
                    "Quechua,",
                    "and",
                    "Gómez",
                    "#REF",
                    "for",
                    "Shipibo-Konibo,",
                    "are",
                    "the",
                    "only",
                    "studies",
                    "that",
                    "employed",
                    "sequence-tosequence",
                    "NMT",
                    "models."
                ],
                [
                    "They",
                    "also",
                    "performed",
                    "transfer",
                    "learning",
                    "experiments",
                    "with",
                    "potentially",
                    "related",
                    "language",
                    "pairs",
                    "(e.g."
                ],
                [
                    "Finnish",
                    "or",
                    "Turkish,",
                    "which",
                    "are",
                    "agglutinative",
                    "languages)."
                ],
                [
                    "However,",
                    "as",
                    "far",
                    "as",
                    "we",
                    "know,",
                    "this",
                    "is",
                    "the",
                    "first",
                    "study",
                    "that",
                    "trains",
                    "a",
                    "multilingual",
                    "model",
                    "for",
                    "some",
                    "language",
                    "spoken",
                    "in",
                    "Peru."
                ],
                [
                    "For",
                    "related",
                    "work",
                    "on",
                    "multilingual",
                    "NMT,",
                    "we",
                    "refer",
                    "the",
                    "readers",
                    "to",
                    "the",
                    "survey",
                    "of",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                0,
                0,
                1,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: In Peru, before NMT, there were studies in rulebased MT, based on the Apertium platform #REF , for Quechua Eastern Apurimac (qve) and Quechua Cuzco (quz) #REF .\n sent1: Furthermore, #REF improved alignments for quz by using an agglutinative language as Finnish as a pivot.\n sent2: Apart from the Quechua variants, only Aymara #REF and Shipibo-Konibo #TARGET_REF have been addressed with rule-based and statistical MT, respectively.\n sent3: #REF for Southern Quechua, and Gómez #REF for Shipibo-Konibo, are the only studies that employed sequence-tosequence NMT models.\n sent4: They also performed transfer learning experiments with potentially related language pairs (e.g.\n sent5: Finnish or Turkish, which are agglutinative languages).\n sent6: However, as far as we know, this is the first study that trains a multilingual model for some language spoken in Peru.\n sent7: For related work on multilingual NMT, we refer the readers to the survey of #REF .\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "positions",
                    "(e.g.,",
                    "(3,",
                    "5)",
                    "''the",
                    "young",
                    "boy'')."
                ],
                [
                    "The",
                    "output",
                    "is",
                    "a",
                    "set",
                    "R",
                    "of",
                    "relations",
                    "of",
                    "the",
                    "form",
                    "(n",
                    "i",
                    ",",
                    "prep,",
                    "n",
                    "j",
                    "),",
                    "where",
                    "i",
                    "=",
                    "j",
                    "and",
                    "prep",
                    "is",
                    "a",
                    "preposition",
                    "(or",
                    "a",
                    "set-membership",
                    "symbol)."
                ],
                [
                    "Each",
                    "text",
                    "is",
                    "also",
                    "associated",
                    "with",
                    "a",
                    "set",
                    "C",
                    "of",
                    "non-overlapping",
                    "coreference",
                    "clusters,",
                    "where",
                    "each",
                    "cluster",
                    "c",
                    "⊆",
                    "N",
                    "is",
                    "a",
                    "non-empty",
                    "list",
                    "of",
                    "NP",
                    "mentions."
                ],
                [
                    "The",
                    "set",
                    "of",
                    "clusters",
                    "is",
                    "not",
                    "provided",
                    "as",
                    "input,",
                    "but",
                    "for",
                    "correct",
                    "sets",
                    "R",
                    "it",
                    "holds",
                    "that",
                    "∀n",
                    "j",
                    "∈",
                    "c(n",
                    "j",
                    "),(n",
                    "i",
                    ",",
                    "prep,",
                    "n",
                    "j",
                    ")",
                    "∈",
                    "R",
                    "⇒",
                    "(n",
                    "i",
                    ",",
                    "prep,",
                    "n",
                    "j",
                    ")",
                    "∈",
                    "R,",
                    "where",
                    "c(n",
                    "j",
                    ")",
                    "∈",
                    "C",
                    "is",
                    "the",
                    "cluster",
                    "containing",
                    "n",
                    "j",
                    ".Completeness",
                    "and",
                    "Uniformity",
                    "The",
                    "kinds",
                    "of",
                    "preposition-mediated",
                    "relations",
                    "we",
                    "cover",
                    "originate",
                    "from",
                    "different",
                    "linguistic",
                    "or",
                    "cognitive",
                    "phenomena,",
                    "and",
                    "some",
                    "of",
                    "them",
                    "can",
                    "be",
                    "resolved",
                    "by",
                    "employing",
                    "different",
                    "linguistic",
                    "constructs."
                ],
                [
                    "For",
                    "example,",
                    "some",
                    "within-sentence",
                    "relations",
                    "can",
                    "be",
                    "extracted",
                    "deterministically",
                    "from",
                    "dependency",
                    "trees,",
                    "for",
                    "example,",
                    "by",
                    "following",
                    "syntactic",
                    "prepositional",
                    "attachment."
                ],
                [
                    "Other",
                    "relations",
                    "can",
                    "be",
                    "inferred",
                    "based",
                    "on",
                    "pronominal",
                    "coreference",
                    "(e.g.,",
                    "''his",
                    "school",
                    "[of",
                    "Adam]''",
                    "above",
                    "can",
                    "be",
                    "resolved",
                    "by",
                    "first",
                    "resolving",
                    "''his''",
                    "to",
                    "''Adam's''",
                    "via",
                    "a",
                    "coreference",
                    "engine,",
                    "and",
                    "then",
                    "normalizing",
                    "''Adam's",
                    "school''",
                    "→",
                    "''school",
                    "of",
                    "Adam'')."
                ],
                [
                    "Many",
                    "others",
                    "are",
                    "substantially",
                    "more",
                    "involved."
                ],
                [
                    "We",
                    "deliberately",
                    "chose",
                    "not",
                    "to",
                    "distinguish",
                    "between",
                    "the",
                    "different",
                    "cases,",
                    "and",
                    "expose",
                    "all",
                    "the",
                    "relations",
                    "to",
                    "the",
                    "user",
                    "(and",
                    "to",
                    "the",
                    "annotators)",
                    "via",
                    "the",
                    "same",
                    "uniform",
                    "interface."
                ],
                [
                    "This",
                    "approach",
                    "also",
                    "contributes",
                    "to",
                    "the",
                    "practical",
                    "usefulness",
                    "of",
                    "the",
                    "task:",
                    "Instead",
                    "of",
                    "running",
                    "several",
                    "different",
                    "processes",
                    "to",
                    "recover",
                    "different",
                    "kinds",
                    "of",
                    "links,",
                    "the",
                    "end-user",
                    "will",
                    "have",
                    "to",
                    "run",
                    "only",
                    "one",
                    "process",
                    "to",
                    "obtain",
                    "them",
                    "all."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: positions (e.g., (3, 5) ''the young boy'').\n sent1: The output is a set R of relations of the form (n i , prep, n j ), where i = j and prep is a preposition (or a set-membership symbol).\n sent2: Each text is also associated with a set C of non-overlapping coreference clusters, where each cluster c ⊆ N is a non-empty list of NP mentions.\n sent3: The set of clusters is not provided as input, but for correct sets R it holds that ∀n j ∈ c(n j ),(n i , prep, n j ) ∈ R ⇒ (n i , prep, n j ) ∈ R, where c(n j ) ∈ C is the cluster containing n j .Completeness and Uniformity The kinds of preposition-mediated relations we cover originate from different linguistic or cognitive phenomena, and some of them can be resolved by employing different linguistic constructs.\n sent4: For example, some within-sentence relations can be extracted deterministically from dependency trees, for example, by following syntactic prepositional attachment.\n sent5: Other relations can be inferred based on pronominal coreference (e.g., ''his school [of Adam]'' above can be resolved by first resolving ''his'' to ''Adam's'' via a coreference engine, and then normalizing ''Adam's school'' → ''school of Adam'').\n sent6: Many others are substantially more involved.\n sent7: We deliberately chose not to distinguish between the different cases, and expose all the relations to the user (and to the annotators) via the same uniform interface.\n sent8: This approach also contributes to the practical usefulness of the task: Instead of running several different processes to recover different kinds of links, the end-user will have to run only one process to obtain them all.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "BART",
                    "movistar",
                    "rider",
                    "alejandro",
                    "valverde",
                    "won",
                    "fleche",
                    "wallonne",
                    "on",
                    "wednesday."
                ],
                [
                    "team",
                    "sky's",
                    "chris",
                    "froome",
                    "fell",
                    "in",
                    "the",
                    "final",
                    "12km",
                    "but",
                    "finished",
                    "the",
                    "race."
                ],
                [
                    "philippe",
                    "gilbert",
                    "pulled",
                    "out",
                    "of",
                    "the",
                    "race",
                    "after",
                    "a",
                    "bad",
                    "crash",
                    "50km",
                    "from",
                    "the",
                    "end."
                ],
                [
                    "click",
                    "here",
                    "for",
                    "more",
                    "cycling",
                    "news."
                ],
                [
                    "2021,",
                    "#TARGET_REF",
                    "has",
                    "shown",
                    "that",
                    "few-shot",
                    "learning",
                    "can",
                    "be",
                    "an",
                    "effective",
                    "fine-tuning",
                    "method",
                    "of",
                    "pre-trained",
                    "models",
                    "for",
                    "text",
                    "generation",
                    "tasks."
                ],
                [
                    "Therefore,",
                    "we",
                    "investigate",
                    "our",
                    "model's",
                    "performance",
                    "in",
                    "a",
                    "few-shot",
                    "setting."
                ],
                [
                    "Specifically,",
                    "we",
                    "randomly",
                    "sample",
                    "100/1000",
                    "examples",
                    "from",
                    "the",
                    "training",
                    "set",
                    "of",
                    "CNNDM/XSum,",
                    "and",
                    "fine-tune",
                    "the",
                    "models",
                    "that",
                    "are",
                    "pre-trained",
                    "using",
                    "MLE",
                    "loss",
                    "on",
                    "those",
                    "examples."
                ],
                [
                    "More",
                    "training",
                    "details",
                    "can",
                    "be",
                    "found",
                    "in",
                    "Appendix",
                    "C.",
                    "The",
                    "results",
                    "are",
                    "shown",
                    "in",
                    "Tab."
                ],
                [
                    "11."
                ],
                [
                    "All",
                    "experiments",
                    "are",
                    "repeated",
                    "three",
                    "times,",
                    "and",
                    "the",
                    "reported",
                    "results",
                    "are",
                    "the",
                    "average",
                    "performance."
                ],
                [
                    "The",
                    "results",
                    "indicate",
                    "that",
                    "our",
                    "model",
                    "can",
                    "achieve",
                    "improvement",
                    "over",
                    "the",
                    "baseline",
                    "model",
                    "under",
                    "the",
                    "few-shot",
                    "learning",
                    "setting",
                    "with",
                    "a",
                    "small",
                    "computational",
                    "overhead."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                1,
                2,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: BART movistar rider alejandro valverde won fleche wallonne on wednesday.\n sent1: team sky's chris froome fell in the final 12km but finished the race.\n sent2: philippe gilbert pulled out of the race after a bad crash 50km from the end.\n sent3: click here for more cycling news.\n sent4: 2021, #TARGET_REF has shown that few-shot learning can be an effective fine-tuning method of pre-trained models for text generation tasks.\n sent5: Therefore, we investigate our model's performance in a few-shot setting.\n sent6: Specifically, we randomly sample 100/1000 examples from the training set of CNNDM/XSum, and fine-tune the models that are pre-trained using MLE loss on those examples.\n sent7: More training details can be found in Appendix C. The results are shown in Tab.\n sent8: 11.\n sent9: All experiments are repeated three times, and the reported results are the average performance.\n sent10: The results indicate that our model can achieve improvement over the baseline model under the few-shot learning setting with a small computational overhead.\n",
        "output": "{\"INFORMATION\": [\"sent4\"], \"PERCEPTION\": [\"sent5\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "baseline",
                    "system",
                    "built",
                    "for",
                    "the",
                    "task",
                    "is",
                    "a",
                    "simple",
                    "PBSMT",
                    "system",
                    "trained",
                    "only",
                    "on",
                    "the",
                    "'in-domain'",
                    "training",
                    "data",
                    "released",
                    "as",
                    "a",
                    "part",
                    "of",
                    "the",
                    "evaluation",
                    "campaign."
                ],
                [
                    "This",
                    "training",
                    "data",
                    "comprised",
                    "of",
                    "both",
                    "parallel",
                    "and",
                    "monolingual",
                    "data",
                    "from",
                    "the",
                    "TED",
                    "1",
                    "http://iwslt2011.org",
                    "Talks:",
                    "2",
                    "a",
                    "collection",
                    "of",
                    "public",
                    "speeches",
                    "on",
                    "a",
                    "variety",
                    "of",
                    "topics."
                ],
                [
                    "Out-of-domain",
                    "data",
                    "in",
                    "the",
                    "form",
                    "of",
                    "a",
                    "parallel",
                    "Multi-UN",
                    "corpus",
                    "3",
                    "was",
                    "also",
                    "available",
                    "to",
                    "enrich",
                    "the",
                    "models",
                    "trained",
                    "on",
                    "in-domain",
                    "data."
                ],
                [
                    "For",
                    "domain-adaptation",
                    "we",
                    "enhanced",
                    "the",
                    "language",
                    "models",
                    "built",
                    "on",
                    "the",
                    "TED",
                    "corpus",
                    "data",
                    "with",
                    "selected",
                    "data",
                    "from",
                    "the",
                    "UN",
                    "corpus."
                ],
                [
                    "Mixture",
                    "adaptation",
                    "#TARGET_REF",
                    "techniques",
                    "were",
                    "used",
                    "to",
                    "combine",
                    "models",
                    "from",
                    "multiple",
                    "sources",
                    "weighted",
                    "according",
                    "to",
                    "their",
                    "fit",
                    "with",
                    "respect",
                    "to",
                    "the",
                    "development",
                    "set."
                ],
                [
                    "The",
                    "adapted",
                    "language",
                    "models",
                    "provided",
                    "an",
                    "improvement",
                    "of",
                    "about",
                    "5.16",
                    "absolute",
                    "(21.99%",
                    "relative)",
                    "BLEU",
                    "points",
                    "for",
                    "Ar-En",
                    "and",
                    "1.25",
                    "absolute",
                    "(11.76%",
                    "relative)",
                    "BLEU",
                    "points",
                    "for",
                    "Zh-En",
                    "language",
                    "pairs",
                    "over",
                    "the",
                    "unadapted",
                    "baseline."
                ]
            ],
            "context": [
                0,
                0,
                0,
                2,
                2,
                0
            ]
        },
        "input": "sent0: The baseline system built for the task is a simple PBSMT system trained only on the 'in-domain' training data released as a part of the evaluation campaign.\n sent1: This training data comprised of both parallel and monolingual data from the TED 1 http://iwslt2011.org Talks: 2 a collection of public speeches on a variety of topics.\n sent2: Out-of-domain data in the form of a parallel Multi-UN corpus 3 was also available to enrich the models trained on in-domain data.\n sent3: For domain-adaptation we enhanced the language models built on the TED corpus data with selected data from the UN corpus.\n sent4: Mixture adaptation #TARGET_REF techniques were used to combine models from multiple sources weighted according to their fit with respect to the development set.\n sent5: The adapted language models provided an improvement of about 5.16 absolute (21.99% relative) BLEU points for Ar-En and 1.25 absolute (11.76% relative) BLEU points for Zh-En language pairs over the unadapted baseline.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent3\", \"sent4\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "community",
                    "refers",
                    "to",
                    "the",
                    "neighborhood",
                    "of",
                    "the",
                    "user",
                    "in",
                    "the",
                    "social",
                    "graph",
                    "of",
                    "the",
                    "platform."
                ],
                [
                    "Conversations",
                    "online",
                    "are",
                    "inherently",
                    "contextual."
                ],
                [
                    "Consequently,",
                    "abuse",
                    "on",
                    "online",
                    "platforms",
                    "can",
                    "only",
                    "be",
                    "effectively",
                    "interpreted",
                    "within",
                    "a",
                    "larger",
                    "context",
                    "#REF",
                    "rather",
                    "than",
                    "in",
                    "isolation."
                ],
                [
                    "This",
                    "is",
                    "especially",
                    "true",
                    "for",
                    "implicit",
                    "or",
                    "generalized",
                    "abuse,",
                    "which",
                    "are",
                    "harder",
                    "to",
                    "interpret",
                    "than",
                    "explicit",
                    "abuse",
                    "for",
                    "humans",
                    "and",
                    "machines",
                    "alike."
                ],
                [
                    "Information",
                    "of",
                    "the",
                    "user",
                    "who",
                    "posted",
                    "the",
                    "comment,",
                    "or",
                    "of",
                    "the",
                    "surrounding",
                    "community",
                    "including",
                    "the",
                    "targets",
                    "of",
                    "the",
                    "comment,",
                    "offers",
                    "insights",
                    "into",
                    "several",
                    "aspects",
                    "of",
                    "the",
                    "context",
                    "that",
                    "are",
                    "otherwise",
                    "not",
                    "accessible",
                    "through",
                    "the",
                    "linguistic",
                    "content",
                    "of",
                    "the",
                    "comment",
                    "alone."
                ],
                [
                    "Here,",
                    "information",
                    "may",
                    "refer",
                    "to",
                    "demographic",
                    "traits",
                    "like",
                    "age",
                    "or",
                    "gender,",
                    "knowledge",
                    "about",
                    "linguistic",
                    "behavior,",
                    "location",
                    "details,",
                    "etc."
                ],
                [
                    "Below",
                    "we",
                    "categorize",
                    "and",
                    "discuss",
                    "the",
                    "aspects",
                    "of",
                    "the",
                    "context",
                    "relevant",
                    "to",
                    "abusive",
                    "language",
                    "detection."
                ],
                [
                    "Sociolinguistic",
                    "norms."
                ],
                [
                    "Sociolinguistics",
                    "studies",
                    "the",
                    "effects",
                    "of",
                    "society",
                    "on",
                    "language",
                    "and",
                    "its",
                    "usage."
                ],
                [
                    "Researchers",
                    "in",
                    "the",
                    "past",
                    "have",
                    "explored",
                    "the",
                    "links",
                    "between",
                    "the",
                    "structures",
                    "and",
                    "norms",
                    "of",
                    "real-world",
                    "communities",
                    "and",
                    "the",
                    "linguistic",
                    "practices",
                    "of",
                    "people",
                    "(D'Arcy",
                    "and",
                    "#REF",
                    "."
                ],
                [
                    "As",
                    "in",
                    "the",
                    "physical",
                    "world,",
                    "individuals",
                    "and",
                    "communities",
                    "on",
                    "online",
                    "platforms",
                    "also",
                    "abide",
                    "by",
                    "certain",
                    "norms,",
                    "which",
                    "may",
                    "be",
                    "guided",
                    "by",
                    "their",
                    "cultural",
                    "backgrounds",
                    "and/or",
                    "are",
                    "based",
                    "on",
                    "the",
                    "standards",
                    "laid",
                    "down",
                    "by",
                    "the",
                    "platforms",
                    "themselves."
                ],
                [
                    "These",
                    "norms",
                    "and",
                    "standards",
                    "reflect",
                    "expectations",
                    "of",
                    "respectful",
                    "behavior,",
                    "local",
                    "customs",
                    "and",
                    "language",
                    "patterns",
                    "within",
                    "a",
                    "region,",
                    "etc."
                ],
                [
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Consequently,",
                    "the",
                    "decision",
                    "of",
                    "what",
                    "is",
                    "considered",
                    "abusive",
                    "must",
                    "be",
                    "made",
                    "taking",
                    "into",
                    "account",
                    "the",
                    "sociolinguistic",
                    "norms."
                ],
                [
                    "User",
                    "and",
                    "community",
                    "information,",
                    "when",
                    "leveraged",
                    "alongside",
                    "linguistic",
                    "features,",
                    "helps",
                    "capture",
                    "the",
                    "relevant",
                    "sociolinguistic",
                    "norms",
                    "in",
                    "a",
                    "myriad",
                    "of",
                    "ways."
                ],
                [
                    "For",
                    "example,",
                    "a",
                    "comment",
                    "may",
                    "contain",
                    "the",
                    "n-word,",
                    "but",
                    "interpretation",
                    "of",
                    "its",
                    "use",
                    "and",
                    "or",
                    "the",
                    "intent",
                    "is",
                    "greatly",
                    "facilitated",
                    "by",
                    "the",
                    "knowledge",
                    "of",
                    "the",
                    "ethnicity",
                    "of",
                    "the",
                    "user",
                    "who",
                    "wrote",
                    "the",
                    "comment",
                    "and/or",
                    "the",
                    "ethnicity",
                    "of",
                    "the",
                    "target",
                    "user",
                    "or",
                    "community."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                1,
                1,
                1,
                0,
                0,
                0
            ]
        },
        "input": "sent0: community refers to the neighborhood of the user in the social graph of the platform.\n sent1: Conversations online are inherently contextual.\n sent2: Consequently, abuse on online platforms can only be effectively interpreted within a larger context #REF rather than in isolation.\n sent3: This is especially true for implicit or generalized abuse, which are harder to interpret than explicit abuse for humans and machines alike.\n sent4: Information of the user who posted the comment, or of the surrounding community including the targets of the comment, offers insights into several aspects of the context that are otherwise not accessible through the linguistic content of the comment alone.\n sent5: Here, information may refer to demographic traits like age or gender, knowledge about linguistic behavior, location details, etc.\n sent6: Below we categorize and discuss the aspects of the context relevant to abusive language detection.\n sent7: Sociolinguistic norms.\n sent8: Sociolinguistics studies the effects of society on language and its usage.\n sent9: Researchers in the past have explored the links between the structures and norms of real-world communities and the linguistic practices of people (D'Arcy and #REF .\n sent10: As in the physical world, individuals and communities on online platforms also abide by certain norms, which may be guided by their cultural backgrounds and/or are based on the standards laid down by the platforms themselves.\n sent11: These norms and standards reflect expectations of respectful behavior, local customs and language patterns within a region, etc.\n sent12: #TARGET_REF .\n sent13: Consequently, the decision of what is considered abusive must be made taking into account the sociolinguistic norms.\n sent14: User and community information, when leveraged alongside linguistic features, helps capture the relevant sociolinguistic norms in a myriad of ways.\n sent15: For example, a comment may contain the n-word, but interpretation of its use and or the intent is greatly facilitated by the knowledge of the ethnicity of the user who wrote the comment and/or the ethnicity of the target user or community.\n",
        "output": "{\"INFORMATION\": [\"sent10\", \"sent11\", \"sent12\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "to",
                    "a",
                    "rich",
                    "set",
                    "of",
                    "applications",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "As",
                    "for",
                    "public",
                    "apologies",
                    "these",
                    "tasks",
                    "can",
                    "range",
                    "from",
                    "tracking",
                    "readers'",
                    "opinions",
                    "about",
                    "the",
                    "sincerity",
                    "of",
                    "the",
                    "communication",
                    "to",
                    "customer",
                    "relationship",
                    "management."
                ],
                [
                    "The",
                    "selected",
                    "synsets",
                    "of",
                    "the",
                    "keywords",
                    "were",
                    "searched",
                    "for",
                    "in",
                    "SentiWordNet."
                ],
                [
                    "The",
                    "sentiment",
                    "scores",
                    "of",
                    "each",
                    "of",
                    "them",
                    "were",
                    "recorded",
                    "and",
                    "the",
                    "results",
                    "were",
                    "analyzed."
                ],
                [
                    "Table",
                    "1",
                    "shows",
                    "the",
                    "sentiment",
                    "scores",
                    "for",
                    "positivity,",
                    "negativity",
                    "and",
                    "objectivity",
                    "for",
                    "each",
                    "of",
                    "the",
                    "keywords."
                ],
                [
                    "In",
                    "the",
                    "analysis",
                    "of",
                    "the",
                    "sentiments",
                    "associated",
                    "with",
                    "keywords,",
                    "of",
                    "particular",
                    "interest",
                    "are",
                    "the",
                    "objective",
                    "scores."
                ],
                [
                    "The",
                    "verb",
                    "apologize",
                    "has",
                    "the",
                    "highest",
                    "objective",
                    "score",
                    "(1.0)."
                ],
                [
                    "Its",
                    "negative",
                    "and",
                    "positive",
                    "scores",
                    "are",
                    "zero."
                ],
                [
                    "The",
                    "high",
                    "ObjScore",
                    "(Objective",
                    "Score)",
                    "of",
                    "one",
                    "(1.0)",
                    "implies",
                    "that",
                    "this",
                    "verb",
                    "does",
                    "not",
                    "convey",
                    "any",
                    "sentiment."
                ],
                [
                    "In",
                    "a",
                    "public",
                    "apology",
                    "act,",
                    "this",
                    "could",
                    "entail",
                    "that",
                    "when",
                    "an",
                    "organization",
                    "or",
                    "person",
                    "renders",
                    "an",
                    "apology",
                    "it",
                    "distances",
                    "itself",
                    "from",
                    "the",
                    "event",
                    "or",
                    "issue",
                    "and",
                    "takes",
                    "an",
                    "objective",
                    "position."
                ],
                [
                    "Similarly",
                    "the",
                    "next",
                    "highest",
                    "ObjScore",
                    "is",
                    "for",
                    "regret",
                    "as",
                    "a",
                    "verb",
                    "(0.75)."
                ],
                [
                    "Thus,",
                    "both",
                    "verbs",
                    "-apologize",
                    "and",
                    "regretdo",
                    "not",
                    "connect",
                    "with",
                    "the",
                    "negative",
                    "sentiments",
                    "associated",
                    "with",
                    "the",
                    "act",
                    "of",
                    "an",
                    "apology.Keywords",
                    "PosScore",
                    "[0,1]",
                    "NegScore",
                    "[0,1]",
                    "ObjScore",
                    "[0,The",
                    "highest",
                    "NegScore",
                    "(Negative",
                    "Score)",
                    "is",
                    "for",
                    "the",
                    "adjective",
                    "sorry",
                    "(0.75),",
                    "followed",
                    "by",
                    "the",
                    "noun",
                    "regret",
                    "which",
                    "has",
                    "a",
                    "NegScore",
                    "of",
                    "0.625."
                ],
                [
                    "The",
                    "strong",
                    "negative",
                    "connotation",
                    "of",
                    "the",
                    "adjective",
                    "sorry",
                    "could",
                    "help",
                    "the",
                    "writer",
                    "to",
                    "convey",
                    "his",
                    "genuine",
                    "feeling",
                    "of",
                    "remorse",
                    "and",
                    "hence",
                    "should",
                    "be",
                    "preferred",
                    "by",
                    "the",
                    "writer",
                    "to",
                    "connect",
                    "with",
                    "the",
                    "reader",
                    "at",
                    "an",
                    "emotional",
                    "level."
                ],
                [
                    "Since",
                    "adjectives",
                    "are",
                    "the",
                    "words",
                    "that",
                    "carry",
                    "the",
                    "most",
                    "notions",
                    "of",
                    "sentiment,",
                    "their",
                    "use",
                    "in",
                    "the",
                    "apology",
                    "can",
                    "carry",
                    "the",
                    "sentiment",
                    "most",
                    "effectively."
                ],
                [
                    "This",
                    "implies",
                    "that",
                    "the",
                    "adjective",
                    "sorry",
                    "carries",
                    "the",
                    "highest",
                    "sentimental",
                    "load",
                    "to",
                    "convey",
                    "the",
                    "feeling",
                    "associated",
                    "with",
                    "act",
                    "of",
                    "apology."
                ]
            ],
            "context": [
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: to a rich set of applications #TARGET_REF .\n sent1: As for public apologies these tasks can range from tracking readers' opinions about the sincerity of the communication to customer relationship management.\n sent2: The selected synsets of the keywords were searched for in SentiWordNet.\n sent3: The sentiment scores of each of them were recorded and the results were analyzed.\n sent4: Table 1 shows the sentiment scores for positivity, negativity and objectivity for each of the keywords.\n sent5: In the analysis of the sentiments associated with keywords, of particular interest are the objective scores.\n sent6: The verb apologize has the highest objective score (1.0).\n sent7: Its negative and positive scores are zero.\n sent8: The high ObjScore (Objective Score) of one (1.0) implies that this verb does not convey any sentiment.\n sent9: In a public apology act, this could entail that when an organization or person renders an apology it distances itself from the event or issue and takes an objective position.\n sent10: Similarly the next highest ObjScore is for regret as a verb (0.75).\n sent11: Thus, both verbs -apologize and regretdo not connect with the negative sentiments associated with the act of an apology.Keywords PosScore [0,1] NegScore [0,1] ObjScore [0,The highest NegScore (Negative Score) is for the adjective sorry (0.75), followed by the noun regret which has a NegScore of 0.625.\n sent12: The strong negative connotation of the adjective sorry could help the writer to convey his genuine feeling of remorse and hence should be preferred by the writer to connect with the reader at an emotional level.\n sent13: Since adjectives are the words that carry the most notions of sentiment, their use in the apology can carry the sentiment most effectively.\n sent14: This implies that the adjective sorry carries the highest sentimental load to convey the feeling associated with act of apology.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "parameterized",
                    "framework",
                    "described",
                    "above",
                    "has",
                    "been",
                    "implemented",
                    "in",
                    "C++",
                    "and",
                    "successfully",
                    "tested",
                    "on",
                    "well",
                    "known,",
                    "translationally",
                    "divergent",
                    "sentences."
                ],
                [
                    "#TARGET_REF",
                    "We",
                    "ran",
                    "the",
                    "parameterized",
                    "parser",
                    "on",
                    "both",
                    "the",
                    "English",
                    "and",
                    "Korean",
                    "sentences",
                    "shown",
                    "here."
                ],
                [
                    "The",
                    "results",
                    "shown",
                    "in",
                    "Figure",
                    "5",
                    "which",
                    "were",
                    "obtained",
                    "from",
                    "running",
                    "the",
                    "program",
                    "on",
                    "a",
                    "Sparcstation",
                    "ELC."
                ],
                [
                    "3",
                    "In",
                    "general,",
                    "the",
                    "times",
                    "demonstrate",
                    "a",
                    "speedup",
                    "of",
                    "2",
                    "to",
                    "3",
                    "orders",
                    "of",
                    "magnitude",
                    "over",
                    "previous",
                    "principlebased",
                    "parsers",
                    "on",
                    "analogous",
                    "examples",
                    "such",
                    "as",
                    "those",
                    "given",
                    "in",
                    "#REF",
                    "."
                ],
                [
                    "Even",
                    "more",
                    "significant",
                    "is",
                    "the",
                    "negligible",
                    "difference",
                    "in",
                    "processing",
                    "time",
                    "between",
                    "the",
                    "two",
                    "languages,",
                    "despite",
                    "radical",
                    "differences",
                    "in",
                    "structure,",
                    "particularly",
                    "with",
                    "respect",
                    "to",
                    "head-complement",
                    "positioning."
                ],
                [
                    "This",
                    "is",
                    "an",
                    "improvement",
                    "over",
                    "previous",
                    "parameterized",
                    "approaches",
                    "in",
                    "which",
                    "cross-linguistic",
                    "divergences",
                    "frequently",
                    "induced",
                    "timing",
                    "discrepancies",
                    "of",
                    "1-2",
                    "orders",
                    "of",
                    "magnitude",
                    "due",
                    "to",
                    "the",
                    "head-initial",
                    "bias",
                    "that",
                    "underlies",
                    "most",
                    "parsing",
                    "designs."
                ]
            ],
            "context": [
                1,
                2,
                2,
                0,
                0,
                0
            ]
        },
        "input": "sent0: The parameterized framework described above has been implemented in C++ and successfully tested on well known, translationally divergent sentences.\n sent1: #TARGET_REF We ran the parameterized parser on both the English and Korean sentences shown here.\n sent2: The results shown in Figure 5 which were obtained from running the program on a Sparcstation ELC.\n sent3: 3 In general, the times demonstrate a speedup of 2 to 3 orders of magnitude over previous principlebased parsers on analogous examples such as those given in #REF .\n sent4: Even more significant is the negligible difference in processing time between the two languages, despite radical differences in structure, particularly with respect to head-complement positioning.\n sent5: This is an improvement over previous parameterized approaches in which cross-linguistic divergences frequently induced timing discrepancies of 1-2 orders of magnitude due to the head-initial bias that underlies most parsing designs.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [\"sent1\", \"sent2\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "other",
                    "50%",
                    "consider",
                    "it",
                    "as",
                    "bad."
                ],
                [
                    "Moreover,",
                    "the",
                    "in-house",
                    "translators",
                    "classify",
                    "the",
                    "post-editing",
                    "effort",
                    "#TARGET_REF",
                    "as",
                    "intermediate,",
                    "but",
                    "this",
                    "information",
                    "might",
                    "be",
                    "unreliable",
                    "due",
                    "to",
                    "insufficient",
                    "numbers",
                    "of",
                    "answers",
                    "on",
                    "the",
                    "MT",
                    "output",
                    "understandability",
                    "question."
                ]
            ],
            "context": [
                0,
                2
            ]
        },
        "input": "sent0: The other 50% consider it as bad.\n sent1: Moreover, the in-house translators classify the post-editing effort #TARGET_REF as intermediate, but this information might be unreliable due to insufficient numbers of answers on the MT output understandability question.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "For",
                    "linear",
                    "SVM,",
                    "we",
                    "used",
                    "an",
                    "online",
                    "learning",
                    "implementation",
                    "of",
                    "SCIKIT-LEARN",
                    "#REF",
                    ",",
                    "based",
                    "on",
                    "LIBSVM",
                    "#TARGET_REF",
                    ",",
                    "with",
                    "hinge",
                    "loss",
                    "and",
                    "Stochastic",
                    "Gradient",
                    "Descent",
                    "(SGD)",
                    "optimiser."
                ],
                [
                    "Hyperparameters",
                    "were",
                    "set",
                    "to",
                    "default",
                    "except",
                    "for",
                    "the",
                    "SGD",
                    "regularisation",
                    "parameter",
                    "that",
                    "was",
                    "increased",
                    "to",
                    "α",
                    "=",
                    "0.75,",
                    "which",
                    "provided",
                    "better",
                    "classification",
                    "accuracy",
                    "on",
                    "the",
                    "dev",
                    "set."
                ],
                [
                    "The",
                    "parameter",
                    "α",
                    "is",
                    "inversely",
                    "proportional",
                    "to",
                    "the",
                    "C",
                    "parameter",
                    "in",
                    "the",
                    "standard",
                    "SVM",
                    "implementation."
                ],
                [
                    "The",
                    "online",
                    "implementation",
                    "also",
                    "allowed",
                    "us",
                    "to",
                    "select",
                    "the",
                    "best",
                    "model",
                    "using",
                    "early",
                    "stopping."
                ],
                [
                    "The",
                    "SVM",
                    "was",
                    "provided",
                    "with",
                    "EEG",
                    "activity",
                    "vectors",
                    "as",
                    "inputs,",
                    "i.e."
                ],
                [
                    "1",
                    "x",
                    "(EEG",
                    "channels",
                    "×",
                    "time",
                    "points)."
                ]
            ],
            "context": [
                2,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: For linear SVM, we used an online learning implementation of SCIKIT-LEARN #REF , based on LIBSVM #TARGET_REF , with hinge loss and Stochastic Gradient Descent (SGD) optimiser.\n sent1: Hyperparameters were set to default except for the SGD regularisation parameter that was increased to α = 0.75, which provided better classification accuracy on the dev set.\n sent2: The parameter α is inversely proportional to the C parameter in the standard SVM implementation.\n sent3: The online implementation also allowed us to select the best model using early stopping.\n sent4: The SVM was provided with EEG activity vectors as inputs, i.e.\n sent5: 1 x (EEG channels × time points).\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Nevertheless,",
                    "it",
                    "has",
                    "been",
                    "shown",
                    "by",
                    "#TARGET_REF",
                    "that",
                    "it",
                    "is",
                    "possible",
                    "to",
                    "improve",
                    "phone",
                    "recognition",
                    "with",
                    "even",
                    "small",
                    "amounts",
                    "(approximately",
                    "100",
                    "sentences)",
                    "of",
                    "annotation."
                ],
                [
                    "It",
                    "may",
                    "be",
                    "possible",
                    "to",
                    "improve",
                    "phonetic",
                    "language",
                    "modeling",
                    "results",
                    "by",
                    "performing",
                    "this",
                    "fine-tuning",
                    "in",
                    "the",
                    "target",
                    "language."
                ]
            ],
            "context": [
                3,
                2
            ]
        },
        "input": "sent0: Nevertheless, it has been shown by #TARGET_REF that it is possible to improve phone recognition with even small amounts (approximately 100 sentences) of annotation.\n sent1: It may be possible to improve phonetic language modeling results by performing this fine-tuning in the target language.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Consequently,",
                    "explainability",
                    "has",
                    "a",
                    "bigger",
                    "role",
                    "to",
                    "play",
                    "here",
                    "than",
                    "simply",
                    "being",
                    "a",
                    "tool",
                    "that",
                    "provides",
                    "interpretability",
                    "to",
                    "designers",
                    "or",
                    "offers",
                    "justifications",
                    "to",
                    "users."
                ],
                [
                    "Operationalizing",
                    "explainability",
                    "in",
                    "a",
                    "manner",
                    "that",
                    "spreads",
                    "awareness",
                    "about",
                    "existing",
                    "stereotypes",
                    "and",
                    "fills",
                    "the",
                    "information",
                    "gap",
                    "can",
                    "be",
                    "very",
                    "effective",
                    "#REF",
                    "."
                ],
                [
                    "One",
                    "way",
                    "to",
                    "achieve",
                    "this",
                    "is",
                    "by",
                    "having",
                    "generative",
                    "explanations",
                    "in",
                    "conjunction",
                    "with",
                    "information",
                    "retrieval",
                    "techniques",
                    "that",
                    "fulfill",
                    "the",
                    "property",
                    "of",
                    "elucidating",
                    "stereotypes",
                    "in",
                    "a",
                    "human-understandable",
                    "way",
                    "#TARGET_REF",
                    "while",
                    "offering",
                    "references",
                    "to",
                    "reliable",
                    "sources",
                    "on",
                    "the",
                    "stereotypes."
                ],
                [
                    "In",
                    "fact,",
                    "such",
                    "an",
                    "operationalization",
                    "that",
                    "elucidates",
                    "stereotypes",
                    "or",
                    "frames",
                    "of",
                    "bias",
                    "#REF",
                    "in",
                    "abusive",
                    "comments",
                    "at",
                    "a",
                    "community",
                    "level,",
                    "while",
                    "providing",
                    "information",
                    "to",
                    "debunk",
                    "the",
                    "stereotypes",
                    "themselves,",
                    "can",
                    "offer",
                    "validation",
                    "to",
                    "the",
                    "victims",
                    "of",
                    "abuse",
                    "by",
                    "communities,",
                    "e.g.,",
                    "minority",
                    "groups,",
                    "and",
                    "help",
                    "them",
                    "feel",
                    "safer",
                    "on",
                    "the",
                    "platform."
                ]
            ],
            "context": [
                0,
                3,
                1,
                3
            ]
        },
        "input": "sent0: Consequently, explainability has a bigger role to play here than simply being a tool that provides interpretability to designers or offers justifications to users.\n sent1: Operationalizing explainability in a manner that spreads awareness about existing stereotypes and fills the information gap can be very effective #REF .\n sent2: One way to achieve this is by having generative explanations in conjunction with information retrieval techniques that fulfill the property of elucidating stereotypes in a human-understandable way #TARGET_REF while offering references to reliable sources on the stereotypes.\n sent3: In fact, such an operationalization that elucidates stereotypes or frames of bias #REF in abusive comments at a community level, while providing information to debunk the stereotypes themselves, can offer validation to the victims of abuse by communities, e.g., minority groups, and help them feel safer on the platform.\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "SNAP",
                    "provides",
                    "four",
                    "knowledge",
                    "representation",
                    "elements:",
                    "node,",
                    "link,",
                    "node",
                    "color",
                    "and",
                    "link",
                    "value."
                ],
                [
                    "#REF",
                    ",",
                    "Conceptual",
                    "Graphs",
                    "#REF",
                    ",",
                    "KODIAK",
                    "#TARGET_REF",
                    ",",
                    "etc."
                ]
            ],
            "context": [
                3,
                3
            ]
        },
        "input": "sent0: SNAP provides four knowledge representation elements: node, link, node color and link value.\n sent1: #REF , Conceptual Graphs #REF , KODIAK #TARGET_REF , etc.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "built",
                    "a",
                    "Transformer-based",
                    "ASR",
                    "system",
                    "using",
                    "the",
                    "ESPnet",
                    "toolkit",
                    "#REF",
                    "."
                ],
                [
                    "The",
                    "Transformer",
                    "architecture",
                    "and",
                    "hyper-parameters",
                    "for",
                    "training/decoding",
                    "are",
                    "based",
                    "on",
                    "existing",
                    "recipes",
                    "in",
                    "ESPnet."
                ],
                [
                    "We",
                    "investigated",
                    "three",
                    "models:",
                    "selfattention-based",
                    "CTC",
                    "#TARGET_REF",
                    ",",
                    "the",
                    "Transformer",
                    "#REF",
                    ",",
                    "and",
                    "a",
                    "hybrid",
                    "Transformer",
                    "trained",
                    "with",
                    "an",
                    "auxiliary",
                    "CTC",
                    "objective",
                    "(Transformer+CTC)",
                    "#REF",
                    "."
                ],
                [
                    "The",
                    "CTC",
                    "model",
                    "was",
                    "used",
                    "in",
                    "prior",
                    "studies",
                    "based",
                    "on",
                    "O2O",
                    "models,",
                    "e.g.,",
                    "#REF",
                    "."
                ],
                [
                    "During",
                    "training,",
                    "the",
                    "CTC",
                    "model",
                    "was",
                    "regularized",
                    "with",
                    "the",
                    "Transformer",
                    "decoder",
                    "in",
                    "the",
                    "multitask",
                    "learning",
                    "fashion",
                    "similar",
                    "to",
                    "Transformer+CTC."
                ],
                [
                    "Such",
                    "regularization",
                    "techniques",
                    "yield",
                    "a",
                    "significant",
                    "improvement",
                    "over",
                    "a",
                    "pure",
                    "CTC",
                    "baseline",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                0,
                0,
                2,
                3,
                3,
                0
            ]
        },
        "input": "sent0: We built a Transformer-based ASR system using the ESPnet toolkit #REF .\n sent1: The Transformer architecture and hyper-parameters for training/decoding are based on existing recipes in ESPnet.\n sent2: We investigated three models: selfattention-based CTC #TARGET_REF , the Transformer #REF , and a hybrid Transformer trained with an auxiliary CTC objective (Transformer+CTC) #REF .\n sent3: The CTC model was used in prior studies based on O2O models, e.g., #REF .\n sent4: During training, the CTC model was regularized with the Transformer decoder in the multitask learning fashion similar to Transformer+CTC.\n sent5: Such regularization techniques yield a significant improvement over a pure CTC baseline #REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": [\"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "To",
                    "our",
                    "knowledge",
                    "these",
                    "are",
                    "the",
                    "first",
                    "experiments",
                    "which",
                    "objectively",
                    "demonstrate",
                    "the",
                    "utility",
                    "of",
                    "punctuation",
                    "for",
                    "resolving",
                    "syntactic",
                    "ambiguity",
                    "and",
                    "improving",
                    "parser",
                    "coverage."
                ],
                [
                    "They",
                    "extend",
                    "work",
                    "by",
                    "#REF",
                    "and",
                    "#TARGET_REF",
                    "by",
                    "applying",
                    "a",
                    "wide-coverage",
                    "text",
                    "grammar",
                    "to",
                    "substantial",
                    "quantities",
                    "of",
                    "naturally-punctuated",
                    "text",
                    "and",
                    "by",
                    "quantifying",
                    "the",
                    "contribution",
                    "of",
                    "punctuation",
                    "to",
                    "ambiguity",
                    "resolution",
                    "in",
                    "a",
                    "well-defined",
                    "probabilistic",
                    "parse",
                    "selection",
                    "model."
                ]
            ],
            "context": [
                2,
                1
            ]
        },
        "input": "sent0: To our knowledge these are the first experiments which objectively demonstrate the utility of punctuation for resolving syntactic ambiguity and improving parser coverage.\n sent1: They extend work by #REF and #TARGET_REF by applying a wide-coverage text grammar to substantial quantities of naturally-punctuated text and by quantifying the contribution of punctuation to ambiguity resolution in a well-defined probabilistic parse selection model.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Task",
                    "Definition",
                    "There",
                    "are",
                    "two",
                    "subtasks",
                    "in",
                    "the",
                    "LCP",
                    "task."
                ],
                [
                    "For",
                    "subtask",
                    "1,",
                    "the",
                    "goal",
                    "is",
                    "to",
                    "predict",
                    "the",
                    "complexity",
                    "score",
                    "for",
                    "a",
                    "single",
                    "word",
                    "from",
                    "the",
                    "given",
                    "context."
                ],
                [
                    "As",
                    "an",
                    "example",
                    "shown",
                    "in",
                    "Figure",
                    "1,",
                    "the",
                    "'refuge'",
                    "is",
                    "the",
                    "word",
                    "that",
                    "needs",
                    "to",
                    "be",
                    "predicted",
                    "and",
                    "since",
                    "the",
                    "meaning",
                    "of",
                    "it",
                    "is",
                    "harder",
                    "to",
                    "get",
                    "in",
                    "the",
                    "first",
                    "context,",
                    "its",
                    "complexity",
                    "score",
                    "in",
                    "the",
                    "first",
                    "context",
                    "is",
                    "much",
                    "higher."
                ],
                [
                    "For",
                    "subtask",
                    "2,",
                    "the",
                    "goal",
                    "is",
                    "to",
                    "predict",
                    "the",
                    "complexity",
                    "score",
                    "for",
                    "a",
                    "multi-word",
                    "expression",
                    "from",
                    "the",
                    "given",
                    "context."
                ],
                [
                    "An",
                    "example",
                    "is",
                    "also",
                    "shown",
                    "in",
                    "the",
                    "right",
                    "part",
                    "of",
                    "Figure",
                    "1.",
                    "a",
                    "5-point",
                    "Likert",
                    "scale:",
                    "one",
                    "for",
                    "very",
                    "easy,",
                    "two",
                    "for",
                    "easy,",
                    "three",
                    "for",
                    "neutral,",
                    "four",
                    "for",
                    "difficult,",
                    "and",
                    "five",
                    "for",
                    "very",
                    "difficult."
                ],
                [
                    "The",
                    "numerical",
                    "labels",
                    "were",
                    "transformed",
                    "to",
                    "a",
                    "0-1",
                    "range",
                    "as",
                    "shown",
                    "in",
                    "Figure",
                    "1."
                ],
                [
                    "To",
                    "add",
                    "further",
                    "variation",
                    "to",
                    "the",
                    "data,",
                    "three",
                    "corpora",
                    "were",
                    "selected",
                    "including",
                    "Bible,",
                    "Europarl",
                    "#REF",
                    "and",
                    "Biomedical",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Each",
                    "corpus",
                    "has",
                    "its",
                    "own",
                    "unique",
                    "language",
                    "features",
                    "and",
                    "styles."
                ],
                [
                    "In",
                    "addition",
                    "to",
                    "single",
                    "words,",
                    "multi-word",
                    "expressions",
                    "were",
                    "also",
                    "selected",
                    "for",
                    "annotating."
                ],
                [
                    "In",
                    "the",
                    "end,",
                    "there",
                    "were",
                    "9476",
                    "annotated",
                    "contexts",
                    "with",
                    "5166",
                    "unique",
                    "words."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                3,
                0,
                0
            ]
        },
        "input": "sent0: Task Definition There are two subtasks in the LCP task.\n sent1: For subtask 1, the goal is to predict the complexity score for a single word from the given context.\n sent2: As an example shown in Figure 1, the 'refuge' is the word that needs to be predicted and since the meaning of it is harder to get in the first context, its complexity score in the first context is much higher.\n sent3: For subtask 2, the goal is to predict the complexity score for a multi-word expression from the given context.\n sent4: An example is also shown in the right part of Figure 1. a 5-point Likert scale: one for very easy, two for easy, three for neutral, four for difficult, and five for very difficult.\n sent5: The numerical labels were transformed to a 0-1 range as shown in Figure 1.\n sent6: To add further variation to the data, three corpora were selected including Bible, Europarl #REF and Biomedical #TARGET_REF .\n sent7: Each corpus has its own unique language features and styles.\n sent8: In addition to single words, multi-word expressions were also selected for annotating.\n sent9: In the end, there were 9476 annotated contexts with 5166 unique words.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent6\"], \"BACKGROUND\": [\"sent7\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "two",
                    "structures",
                    "from",
                    "which",
                    "she",
                    "hoped",
                    "the",
                    "most",
                    "were",
                    "lattices",
                    "and",
                    "'fans',",
                    "a",
                    "notion",
                    "she",
                    "derived",
                    "from",
                    "some",
                    "work",
                    "of",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "MMB",
                    "believed",
                    "lattices",
                    "#REF",
                    "to",
                    "be",
                    "the",
                    "underlying",
                    "structure",
                    "of",
                    "thesauri",
                    "while",
                    "fans",
                    "#REF",
                    "mapped",
                    "the",
                    "spreading",
                    "out",
                    "of",
                    "the",
                    "new",
                    "senses",
                    "of",
                    "words,",
                    "indefinitely",
                    "into",
                    "the",
                    "future."
                ],
                [
                    "She",
                    "spent",
                    "some",
                    "time",
                    "trying",
                    "to",
                    "amalgamate",
                    "both",
                    "representations",
                    "into",
                    "a",
                    "single",
                    "structure."
                ],
                [
                    "These",
                    "efforts",
                    "have",
                    "not",
                    "met",
                    "with",
                    "much",
                    "success",
                    "nor",
                    "have",
                    "they",
                    "been",
                    "taken",
                    "up",
                    "by",
                    "others,",
                    "although",
                    "Zellig",
                    "Harris",
                    "did",
                    "at",
                    "one",
                    "time",
                    "toy",
                    "with",
                    "lattices",
                    "as",
                    "language",
                    "structures,",
                    "and",
                    "#REF",
                    "sought",
                    "to",
                    "link",
                    "lattice",
                    "structures",
                    "again",
                    "to",
                    "Halliday's",
                    "categories",
                    "of",
                    "grammar",
                    "and",
                    "semantics."
                ]
            ],
            "context": [
                1,
                3,
                0,
                0
            ]
        },
        "input": "sent0: The two structures from which she hoped the most were lattices and 'fans', a notion she derived from some work of #TARGET_REF .\n sent1: MMB believed lattices #REF to be the underlying structure of thesauri while fans #REF mapped the spreading out of the new senses of words, indefinitely into the future.\n sent2: She spent some time trying to amalgamate both representations into a single structure.\n sent3: These efforts have not met with much success nor have they been taken up by others, although Zellig Harris did at one time toy with lattices as language structures, and #REF sought to link lattice structures again to Halliday's categories of grammar and semantics.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "•",
                    "HuRIC",
                    "2.0",
                    "#TARGET_REF",
                    ":",
                    "audio",
                    "files",
                    "(656",
                    "sentences)",
                    "paired",
                    "with",
                    "their",
                    "transcriptions",
                    "referring",
                    "to",
                    "commands",
                    "for",
                    "a",
                    "robot",
                    "•",
                    "LAVA",
                    "#REF",
                    ":",
                    "237",
                    "sentences,",
                    "with",
                    "2",
                    "to",
                    "3",
                    "interpretations",
                    "per",
                    "sentence,",
                    "and",
                    "a",
                    "total",
                    "of",
                    "1679",
                    "videos",
                    "that",
                    "depict",
                    "visual",
                    "variations",
                    "of",
                    "each",
                    "interpretation",
                    "•",
                    "CLEVR-Ref+",
                    "#REF",
                    ":",
                    "100",
                    "K",
                    "synthetic",
                    "images",
                    "with",
                    "several",
                    "referring",
                    "expressions",
                    "•",
                    "Eye4Ref",
                    "(Alac",
                    "¸am",
                    "et",
                    "al.,",
                    "2020b):",
                    "86",
                    "systematically",
                    "controlled",
                    "sentence--image",
                    "pairs",
                    "and",
                    "2024",
                    "eye-movement",
                    "recordings",
                    "from",
                    "various",
                    "referentially",
                    "complex",
                    "situations",
                    "Multimodal",
                    "embeddings",
                    "will",
                    "be",
                    "created",
                    "from",
                    "this",
                    "pool",
                    "of",
                    "datasets."
                ],
                [
                    "Creating",
                    "embeddings",
                    "from",
                    "various",
                    "data",
                    "sources",
                    "will",
                    "allow",
                    "us",
                    "to",
                    "cover",
                    "concepts",
                    "from",
                    "various",
                    "aspects",
                    "such",
                    "as",
                    "linguistic,",
                    "auditory",
                    "and",
                    "visual",
                    "representations."
                ],
                [
                    "The",
                    "variety",
                    "on",
                    "the",
                    "visual",
                    "modality",
                    "will",
                    "also",
                    "help",
                    "us",
                    "to",
                    "capture",
                    "different",
                    "visual",
                    "depictions",
                    "in",
                    "a",
                    "range",
                    "from",
                    "synthetic",
                    "images",
                    "to",
                    "photographs."
                ],
                [
                    "This",
                    "will",
                    "increase",
                    "the",
                    "representativeness",
                    "of",
                    "the",
                    "concepts",
                    "in",
                    "the",
                    "training",
                    "dataset",
                    "that",
                    "will",
                    "in",
                    "return",
                    "improve",
                    "the",
                    "prediction",
                    "when",
                    "it",
                    "comes",
                    "to",
                    "unseen",
                    "environments",
                    "either",
                    "in",
                    "virtual",
                    "reality",
                    "or",
                    "in",
                    "a",
                    "real-world",
                    "setting."
                ],
                [
                    "70",
                    "%",
                    "of",
                    "this",
                    "collection",
                    "will",
                    "be",
                    "used",
                    "to",
                    "create",
                    "multimodal",
                    "concept",
                    "embeddings."
                ],
                [
                    "The",
                    "remaining",
                    "30",
                    "%",
                    "of",
                    "the",
                    "datasets",
                    "will",
                    "be",
                    "included",
                    "in",
                    "the",
                    "test",
                    "and",
                    "development",
                    "sets",
                    "after",
                    "semi-automatic",
                    "and",
                    "manual",
                    "annotation",
                    "of",
                    "contextual",
                    "representations,",
                    "target",
                    "words,",
                    "missing",
                    "words,",
                    "etc."
                ],
                [
                    "However,",
                    "Eye4REF",
                    "will",
                    "be",
                    "used",
                    "as",
                    "main",
                    "testset",
                    "since",
                    "it",
                    "was",
                    "systematically",
                    "created",
                    "to",
                    "involve",
                    "referentially",
                    "complex",
                    "situations."
                ]
            ],
            "context": [
                0,
                2,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: • HuRIC 2.0 #TARGET_REF : audio files (656 sentences) paired with their transcriptions referring to commands for a robot • LAVA #REF : 237 sentences, with 2 to 3 interpretations per sentence, and a total of 1679 videos that depict visual variations of each interpretation • CLEVR-Ref+ #REF : 100 K synthetic images with several referring expressions • Eye4Ref (Alac ¸am et al., 2020b): 86 systematically controlled sentence--image pairs and 2024 eye-movement recordings from various referentially complex situations Multimodal embeddings will be created from this pool of datasets.\n sent1: Creating embeddings from various data sources will allow us to cover concepts from various aspects such as linguistic, auditory and visual representations.\n sent2: The variety on the visual modality will also help us to capture different visual depictions in a range from synthetic images to photographs.\n sent3: This will increase the representativeness of the concepts in the training dataset that will in return improve the prediction when it comes to unseen environments either in virtual reality or in a real-world setting.\n sent4: 70 % of this collection will be used to create multimodal concept embeddings.\n sent5: The remaining 30 % of the datasets will be included in the test and development sets after semi-automatic and manual annotation of contextual representations, target words, missing words, etc.\n sent6: However, Eye4REF will be used as main testset since it was systematically created to involve referentially complex situations.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Distributional",
                    "similarity",
                    "information",
                    "has",
                    "been",
                    "used",
                    "to",
                    "improve",
                    "modeling",
                    "of",
                    "word",
                    "co-occurrence",
                    "probabilities",
                    "in",
                    "previous",
                    "work."
                ],
                [
                    "#REF",
                    "defined",
                    "a",
                    "kernel-based",
                    "interpolated",
                    "language",
                    "model",
                    "where",
                    "probability",
                    "mass",
                    "is",
                    "explicitly",
                    "spread",
                    "over",
                    "similar",
                    "words,",
                    "with",
                    "variant",
                    "models",
                    "along",
                    "these",
                    "lines",
                    "found",
                    "in",
                    "#REF",
                    "and",
                    "Yarlett",
                    "#REF",
                    "."
                ],
                [
                    "These",
                    "models",
                    "leverage",
                    "similarity",
                    "information",
                    "about",
                    "target",
                    "words",
                    "but",
                    "not",
                    "context",
                    "words."
                ],
                [
                    "In",
                    "contrast,",
                    "#TARGET_REF",
                    "proposed",
                    "a",
                    "method",
                    "which",
                    "uses",
                    "similarity",
                    "information",
                    "about",
                    "the",
                    "context",
                    "word",
                    "but",
                    "not",
                    "the",
                    "target",
                    "word."
                ],
                [
                    "#REF",
                    "developed",
                    "a",
                    "method",
                    "that",
                    "can",
                    "exploit",
                    "similarity",
                    "information",
                    "about",
                    "both",
                    "target",
                    "and",
                    "context,",
                    "using",
                    "a",
                    "Markov",
                    "Chain",
                    "algorithm",
                    "incorporating",
                    "distributional",
                    "and",
                    "WordNet",
                    "similarities."
                ],
                [
                    "None",
                    "of",
                    "this",
                    "previous",
                    "work",
                    "derived",
                    "word",
                    "similarity",
                    "information",
                    "from",
                    "pretrained",
                    "embeddings,",
                    "because",
                    "such",
                    "embeddings",
                    "did",
                    "not",
                    "exist",
                    "at",
                    "the",
                    "time."
                ]
            ],
            "context": [
                0,
                0,
                2,
                1,
                0,
                0
            ]
        },
        "input": "sent0: Distributional similarity information has been used to improve modeling of word co-occurrence probabilities in previous work.\n sent1: #REF defined a kernel-based interpolated language model where probability mass is explicitly spread over similar words, with variant models along these lines found in #REF and Yarlett #REF .\n sent2: These models leverage similarity information about target words but not context words.\n sent3: In contrast, #TARGET_REF proposed a method which uses similarity information about the context word but not the target word.\n sent4: #REF developed a method that can exploit similarity information about both target and context, using a Markov Chain algorithm incorporating distributional and WordNet similarities.\n sent5: None of this previous work derived word similarity information from pretrained embeddings, because such embeddings did not exist at the time.\n",
        "output": "{\"INFORMATION\": [\"sent3\"], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "What",
                    "MMB",
                    "sought",
                    "was",
                    "a",
                    "compromise",
                    "system",
                    "of",
                    "meaning",
                    "representation",
                    "for",
                    "MT:",
                    "one",
                    "that",
                    "was",
                    "fundamental",
                    "to",
                    "the",
                    "process",
                    "of",
                    "translation,",
                    "but",
                    "did",
                    "not",
                    "constitute",
                    "a",
                    "detailed",
                    "representation",
                    "of",
                    "all",
                    "the",
                    "relevant",
                    "knowledge",
                    "of",
                    "the",
                    "world."
                ],
                [
                    "She",
                    "believed",
                    "there",
                    "was",
                    "a",
                    "level",
                    "of",
                    "representation,",
                    "linguistic",
                    "if",
                    "you",
                    "will,",
                    "probably",
                    "vague",
                    "as",
                    "well,",
                    "but",
                    "which",
                    "was",
                    "sufficient",
                    "for",
                    "MT."
                ],
                [
                    "In",
                    "that",
                    "sense,",
                    "she",
                    "totally",
                    "denied",
                    "the",
                    "assumption",
                    "behind",
                    "Bar-Hillel's",
                    "critique",
                    "of",
                    "#TARGET_REF",
                    "-which",
                    "was",
                    "taken",
                    "up",
                    "by",
                    "some",
                    "artificial",
                    "intelligence",
                    "researchers",
                    "afterwards",
                    "(although",
                    "not,",
                    "of",
                    "course,",
                    "the",
                    "same",
                    "ones",
                    "as",
                    "referred",
                    "to",
                    "in",
                    "the",
                    "last",
                    "paragraph)",
                    "-that",
                    "MT",
                    "and",
                    "language",
                    "understanding",
                    "in",
                    "general",
                    "did",
                    "require",
                    "the",
                    "explicit",
                    "representation",
                    "of",
                    "all",
                    "world",
                    "knowledge."
                ],
                [
                    "This",
                    "position",
                    "of",
                    "hers",
                    "cannot",
                    "be",
                    "separated",
                    "from",
                    "her",
                    "quasi-idealist",
                    "belief",
                    "(see",
                    "further",
                    "below)",
                    "that",
                    "world",
                    "knowledge",
                    "cannot",
                    "be",
                    "represented",
                    "independently",
                    "of",
                    "some",
                    "language,",
                    "and",
                    "hence",
                    "that",
                    "any",
                    "true",
                    "distinction",
                    "between",
                    "meaning",
                    "representation",
                    "and",
                    "the",
                    "representation",
                    "of",
                    "world",
                    "knowledge",
                    "is,",
                    "ultimately,",
                    "misconceived",
                    "(see",
                    "her",
                    "discussion",
                    "of",
                    "Whorf",
                    "#REF",
                    ")."
                ],
                [
                    "The",
                    "only",
                    "dispute",
                    "can",
                    "be",
                    "about",
                    "the",
                    "'level'",
                    "or",
                    "'grain'",
                    "of",
                    "representation",
                    "that",
                    "particular",
                    "acts",
                    "of",
                    "translation",
                    "require."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: What MMB sought was a compromise system of meaning representation for MT: one that was fundamental to the process of translation, but did not constitute a detailed representation of all the relevant knowledge of the world.\n sent1: She believed there was a level of representation, linguistic if you will, probably vague as well, but which was sufficient for MT.\n sent2: In that sense, she totally denied the assumption behind Bar-Hillel's critique of #TARGET_REF -which was taken up by some artificial intelligence researchers afterwards (although not, of course, the same ones as referred to in the last paragraph) -that MT and language understanding in general did require the explicit representation of all world knowledge.\n sent3: This position of hers cannot be separated from her quasi-idealist belief (see further below) that world knowledge cannot be represented independently of some language, and hence that any true distinction between meaning representation and the representation of world knowledge is, ultimately, misconceived (see her discussion of Whorf #REF ).\n sent4: The only dispute can be about the 'level' or 'grain' of representation that particular acts of translation require.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Passage",
                    "retrieval",
                    "for",
                    "open-domain",
                    "QA",
                    "For",
                    "opendomain",
                    "QA,",
                    "a",
                    "passage",
                    "retriever",
                    "is",
                    "an",
                    "important",
                    "component",
                    "to",
                    "identify",
                    "relevant",
                    "passages",
                    "for",
                    "answer",
                    "extraction."
                ],
                [
                    "Traditional",
                    "approaches",
                    "#REF",
                    "implemented",
                    "term-based",
                    "passage",
                    "retrievers",
                    "(e.g."
                ],
                [
                    "TF-IDF",
                    "and",
                    "BM25),",
                    "which",
                    "have",
                    "limited",
                    "representation",
                    "capabilities."
                ],
                [
                    "Recently,",
                    "researchers",
                    "have",
                    "utilized",
                    "deep",
                    "learning",
                    "to",
                    "improve",
                    "traditional",
                    "passage",
                    "retrievers,",
                    "including",
                    "document",
                    "expansions",
                    "#TARGET_REF",
                    ",",
                    "question",
                    "expansions",
                    "#REF",
                    "and",
                    "term",
                    "weight",
                    "estimation",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                3,
                0,
                0,
                2
            ]
        },
        "input": "sent0: Passage retrieval for open-domain QA For opendomain QA, a passage retriever is an important component to identify relevant passages for answer extraction.\n sent1: Traditional approaches #REF implemented term-based passage retrievers (e.g.\n sent2: TF-IDF and BM25), which have limited representation capabilities.\n sent3: Recently, researchers have utilized deep learning to improve traditional passage retrievers, including document expansions #TARGET_REF , question expansions #REF and term weight estimation #REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent3\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "where",
                    "Y",
                    "*",
                    "denotes",
                    "a",
                    "set",
                    "of",
                    "all",
                    "possible",
                    "hypotheses."
                ],
                [
                    "The",
                    "Transformer",
                    "#TARGET_REF",
                    "is",
                    "a",
                    "stateof-the-art",
                    "NN",
                    "architecture",
                    "that",
                    "can",
                    "be",
                    "used",
                    "to",
                    "maximize",
                    "Eq."
                ],
                [
                    "(",
                    "1)."
                ],
                [
                    "The",
                    "Transformer",
                    "consists",
                    "of",
                    "two",
                    "NNs:",
                    "The",
                    "Encoder",
                    "network",
                    "and",
                    "the",
                    "Decoder",
                    "network."
                ],
                [
                    "Let",
                    "I",
                    "emb",
                    "and",
                    "D",
                    "emb",
                    "be",
                    "the",
                    "sequence",
                    "length",
                    "and",
                    "dimension",
                    "of",
                    "the",
                    "acoustic",
                    "embedding."
                ],
                [
                    "The",
                    "Encoder",
                    "network",
                    "generates",
                    "a",
                    "sequence",
                    "of",
                    "embeddings",
                    "of",
                    "the",
                    "acoustic",
                    "information",
                    "E",
                    "=",
                    "{e",
                    "i",
                    "∈",
                    "D",
                    "emb",
                    "}",
                    "I",
                    "emb",
                    "i=1",
                    "from",
                    "input",
                    "feature",
                    "sequences,",
                    "i.e."
                ],
                [
                    "E",
                    "=",
                    "Encoder(X)."
                ],
                [
                    "The",
                    "Decoder",
                    "network",
                    "predicts",
                    "the",
                    "output",
                    "of",
                    "the",
                    "M",
                    "-th",
                    "step",
                    "y",
                    "M",
                    "given",
                    "a",
                    "sub-sequence,",
                    "including",
                    "the",
                    "current",
                    "output",
                    "ȳ",
                    "=",
                    "{y",
                    "1",
                    ",",
                    "•",
                    "•",
                    "•",
                    ",",
                    "y",
                    "M",
                    "−1",
                    "}",
                    "and",
                    "E,",
                    "i.e."
                ],
                [
                    "y",
                    "M",
                    "=",
                    "Decoder(ȳ,",
                    "E)."
                ],
                [
                    "This",
                    "conditional",
                    "autoregressive",
                    "modeling",
                    "function",
                    "is",
                    "particularly",
                    "important",
                    "in",
                    "this",
                    "paper",
                    "since",
                    "it",
                    "can",
                    "explicitly",
                    "model",
                    "the",
                    "relationship",
                    "between",
                    "output",
                    "labels,",
                    "unlike",
                    "CTC."
                ]
            ],
            "context": [
                0,
                1,
                1,
                3,
                3,
                3,
                3,
                3,
                3,
                2
            ]
        },
        "input": "sent0: where Y * denotes a set of all possible hypotheses.\n sent1: The Transformer #TARGET_REF is a stateof-the-art NN architecture that can be used to maximize Eq.\n sent2: ( 1).\n sent3: The Transformer consists of two NNs: The Encoder network and the Decoder network.\n sent4: Let I emb and D emb be the sequence length and dimension of the acoustic embedding.\n sent5: The Encoder network generates a sequence of embeddings of the acoustic information E = {e i ∈ D emb } I emb i=1 from input feature sequences, i.e.\n sent6: E = Encoder(X).\n sent7: The Decoder network predicts the output of the M -th step y M given a sub-sequence, including the current output ȳ = {y 1 , • • • , y M −1 } and E, i.e.\n sent8: y M = Decoder(ȳ, E).\n sent9: This conditional autoregressive modeling function is particularly important in this paper since it can explicitly model the relationship between output labels, unlike CTC.\n",
        "output": "{\"INFORMATION\": [\"sent1\", \"sent2\"], \"PERCEPTION\": [\"sent9\"], \"BACKGROUND\": [\"sent3\", \"sent4\", \"sent5\", \"sent6\", \"sent7\", \"sent8\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "further",
                    "compared",
                    "our",
                    "strategy",
                    "with",
                    "other",
                    "graph-based",
                    "approaches:",
                    "Text",
                    "Graph",
                    "Convolutional",
                    "Network",
                    "(TextGCN)",
                    "#TARGET_REF",
                    "and",
                    "Heterogeneous",
                    "Graph",
                    "Attention",
                    "Network",
                    "(HGAT)",
                    "#REF",
                    "."
                ],
                [
                    "The",
                    "former",
                    "models",
                    "the",
                    "whole",
                    "text",
                    "corpus",
                    "as",
                    "a",
                    "document-word",
                    "graph",
                    "with",
                    "word",
                    "co-occurrence",
                    "relations",
                    "and",
                    "applies",
                    "GCN",
                    "for",
                    "classification."
                ],
                [
                    "The",
                    "latter",
                    "models",
                    "the",
                    "texts",
                    "using",
                    "a",
                    "heterogeneous",
                    "information",
                    "network",
                    "framework",
                    "and",
                    "adopts",
                    "heterogeneous",
                    "graph",
                    "attention",
                    "to",
                    "embed",
                    "that",
                    "framework",
                    "for",
                    "text",
                    "classification",
                    "based",
                    "on",
                    "a",
                    "dual-level",
                    "attention",
                    "mechanism."
                ],
                [
                    "Finally,",
                    "we",
                    "compared",
                    "our",
                    "approach",
                    "with",
                    "a",
                    "transformer-based",
                    "method",
                    "as",
                    "it",
                    "has",
                    "achieved",
                    "remarkable",
                    "results",
                    "in",
                    "several",
                    "areas",
                    "of",
                    "Natural",
                    "Language",
                    "Processing",
                    "(NLP)."
                ],
                [
                    "We",
                    "compared",
                    "our",
                    "strategy",
                    "with",
                    "BR-BERT",
                    "#REF",
                    ",",
                    "which",
                    "is",
                    "a",
                    "monolingual",
                    "BERT,",
                    "and",
                    "M-BERT",
                    "#REF",
                    ",",
                    "which",
                    "is",
                    "a",
                    "multilingual",
                    "BERT."
                ],
                [
                    "Table",
                    "5",
                    "shows",
                    "the",
                    "comparison",
                    "between",
                    "these",
                    "methods."
                ],
                [
                    "As",
                    "we",
                    "can",
                    "see",
                    "from",
                    "Table",
                    "5,",
                    "our",
                    "approach",
                    "outperformed",
                    "the",
                    "graph-based",
                    "methods",
                    "and",
                    "reached",
                    "a",
                    "competitive",
                    "result",
                    "compared",
                    "to",
                    "transformer",
                    "models."
                ],
                [
                    "Although",
                    "our",
                    "strategy",
                    "did",
                    "not",
                    "outperform",
                    "transformers,",
                    "we",
                    "believe",
                    "the",
                    "results",
                    "are",
                    "very",
                    "promising,",
                    "since",
                    "it",
                    "requires",
                    "much",
                    "less",
                    "computational",
                    "power",
                    "than",
                    "transformers."
                ],
                [
                    "Moreover,",
                    "our",
                    "method",
                    "requires",
                    "less",
                    "annotated",
                    "data",
                    "(only",
                    "10%)",
                    "than",
                    "transformers",
                    "to",
                    "achieve",
                    "interesting",
                    "results."
                ]
            ],
            "context": [
                2,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: We further compared our strategy with other graph-based approaches: Text Graph Convolutional Network (TextGCN) #TARGET_REF and Heterogeneous Graph Attention Network (HGAT) #REF .\n sent1: The former models the whole text corpus as a document-word graph with word co-occurrence relations and applies GCN for classification.\n sent2: The latter models the texts using a heterogeneous information network framework and adopts heterogeneous graph attention to embed that framework for text classification based on a dual-level attention mechanism.\n sent3: Finally, we compared our approach with a transformer-based method as it has achieved remarkable results in several areas of Natural Language Processing (NLP).\n sent4: We compared our strategy with BR-BERT #REF , which is a monolingual BERT, and M-BERT #REF , which is a multilingual BERT.\n sent5: Table 5 shows the comparison between these methods.\n sent6: As we can see from Table 5, our approach outperformed the graph-based methods and reached a competitive result compared to transformer models.\n sent7: Although our strategy did not outperform transformers, we believe the results are very promising, since it requires much less computational power than transformers.\n sent8: Moreover, our method requires less annotated data (only 10%) than transformers to achieve interesting results.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "propose",
                    "a",
                    "three-stage",
                    "pipeline",
                    "called",
                    "Multilingual",
                    "Constrative",
                    "Training",
                    "(MuCoT)",
                    "to",
                    "effectively",
                    "train",
                    "the",
                    "mBERT",
                    "model",
                    "for",
                    "question-answering",
                    "in",
                    "low-resource",
                    "languages."
                ],
                [
                    "An",
                    "illustration",
                    "of",
                    "this",
                    "pipeline",
                    "for",
                    "two",
                    "low-resource",
                    "languages,",
                    "namely",
                    "Tamil",
                    "and",
                    "Hindi,",
                    "is",
                    "shown",
                    "in",
                    "Figure",
                    "3."
                ],
                [
                    "The",
                    "first",
                    "stage",
                    "is",
                    "pre-training",
                    "the",
                    "baseline",
                    "multilingual",
                    "model",
                    "(mBERT)."
                ],
                [
                    "The",
                    "second",
                    "stage",
                    "involves",
                    "pre-training",
                    "the",
                    "QA",
                    "head",
                    "using",
                    "the",
                    "large-scale",
                    "dataset(s)",
                    "in",
                    "high",
                    "resource",
                    "language(s)."
                ],
                [
                    "In",
                    "Figure",
                    "3,",
                    "English",
                    "is",
                    "considered",
                    "the",
                    "high-resource",
                    "language",
                    "and",
                    "SQuAD",
                    "#REF",
                    "dataset",
                    "is",
                    "used",
                    "to",
                    "pre-train",
                    "the",
                    "QA",
                    "head",
                    "and",
                    "obtain",
                    "the",
                    "mBERT-QA",
                    "model."
                ],
                [
                    "The",
                    "final",
                    "stage",
                    "involves",
                    "fine-tuning",
                    "the",
                    "mBERT-QA",
                    "model",
                    "using",
                    "both",
                    "original",
                    "and",
                    "augmented",
                    "samples",
                    "from",
                    "the",
                    "target",
                    "low-resource",
                    "languages."
                ],
                [
                    "In",
                    "this",
                    "work,",
                    "ChAII",
                    "#TARGET_REF",
                    "dataset",
                    "is",
                    "used",
                    "for",
                    "obtaining",
                    "training",
                    "samples",
                    "in",
                    "Tamil",
                    "and",
                    "Hindi."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                3,
                1
            ]
        },
        "input": "sent0: We propose a three-stage pipeline called Multilingual Constrative Training (MuCoT) to effectively train the mBERT model for question-answering in low-resource languages.\n sent1: An illustration of this pipeline for two low-resource languages, namely Tamil and Hindi, is shown in Figure 3.\n sent2: The first stage is pre-training the baseline multilingual model (mBERT).\n sent3: The second stage involves pre-training the QA head using the large-scale dataset(s) in high resource language(s).\n sent4: In Figure 3, English is considered the high-resource language and SQuAD #REF dataset is used to pre-train the QA head and obtain the mBERT-QA model.\n sent5: The final stage involves fine-tuning the mBERT-QA model using both original and augmented samples from the target low-resource languages.\n sent6: In this work, ChAII #TARGET_REF dataset is used for obtaining training samples in Tamil and Hindi.\n",
        "output": "{\"INFORMATION\": [\"sent6\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent5\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "also",
                    "study",
                    "mitigating",
                    "shortcuts",
                    "by",
                    "masking",
                    "out",
                    "the",
                    "identified",
                    "shortcuts."
                ],
                [
                    "RM),",
                    "and",
                    "both",
                    "(Train",
                    "&amp,",
                    "Test",
                    "RM)",
                    "as",
                    "described",
                    "in",
                    "Sec",
                    "3.4."
                ],
                [
                    "We",
                    "evaluate",
                    "these",
                    "three",
                    "approaches",
                    "in",
                    "multiple",
                    "settings:",
                    "1)",
                    "domain",
                    "generalization,",
                    "2)",
                    "challenging",
                    "datasets,",
                    "3)",
                    "gender",
                    "bias."
                ],
                [
                    "As",
                    "shown",
                    "in",
                    "Table",
                    "5,",
                    "masking",
                    "out",
                    "shortcuts,",
                    "especially",
                    "in",
                    "training",
                    "data,",
                    "can",
                    "improve",
                    "model's",
                    "generalization",
                    "to",
                    "out-of-distribution",
                    "data."
                ],
                [
                    "Note",
                    "in",
                    "this",
                    "setting,",
                    "different",
                    "from",
                    "existing",
                    "domain",
                    "transfer",
                    "work",
                    "(Pan",
                    "and",
                    "Yang,",
                    "2010),",
                    "we",
                    "do",
                    "not",
                    "assume",
                    "access",
                    "to",
                    "labeled",
                    "data",
                    "in",
                    "the",
                    "target",
                    "domain",
                    "during",
                    "training,",
                    "instead",
                    "we",
                    "use",
                    "our",
                    "proposed",
                    "approach",
                    "to",
                    "identify",
                    "potential",
                    "shortcuts",
                    "that",
                    "can",
                    "generalize",
                    "to",
                    "unseen",
                    "target",
                    "domains."
                ],
                [
                    "As",
                    "a",
                    "result,",
                    "we",
                    "also",
                    "observe",
                    "model's",
                    "performance",
                    "improvement",
                    "on",
                    "challenging",
                    "datasets",
                    "(Table",
                    "7)."
                ],
                [
                    "fairer",
                    "model."
                ],
                [
                    "Note",
                    "the",
                    "original",
                    "performance",
                    "might",
                    "degrade",
                    "slightly",
                    "due",
                    "to",
                    "models",
                    "learning",
                    "different",
                    "but",
                    "more",
                    "robust",
                    "feature",
                    "representations,",
                    "consistent",
                    "with",
                    "findings",
                    "in",
                    "existing",
                    "work",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2
            ]
        },
        "input": "sent0: We also study mitigating shortcuts by masking out the identified shortcuts.\n sent1: RM), and both (Train &amp, Test RM) as described in Sec 3.4.\n sent2: We evaluate these three approaches in multiple settings: 1) domain generalization, 2) challenging datasets, 3) gender bias.\n sent3: As shown in Table 5, masking out shortcuts, especially in training data, can improve model's generalization to out-of-distribution data.\n sent4: Note in this setting, different from existing domain transfer work (Pan and Yang, 2010), we do not assume access to labeled data in the target domain during training, instead we use our proposed approach to identify potential shortcuts that can generalize to unseen target domains.\n sent5: As a result, we also observe model's performance improvement on challenging datasets (Table 7).\n sent6: fairer model.\n sent7: Note the original performance might degrade slightly due to models learning different but more robust feature representations, consistent with findings in existing work #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent7\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "acclaimed",
                    "success",
                    "of",
                    "Transformer-based",
                    "models",
                    "across",
                    "NLP",
                    "tasks",
                    "has",
                    "been",
                    "followed",
                    "by",
                    "two",
                    "important",
                    "directions",
                    "of",
                    "research."
                ],
                [
                    "In",
                    "the",
                    "first",
                    "direction,",
                    "interpretability",
                    "studies",
                    "aim",
                    "to",
                    "understand",
                    "how",
                    "these",
                    "models",
                    "work."
                ],
                [
                    "Given",
                    "that",
                    "multi-headed",
                    "attention",
                    "is",
                    "an",
                    "important",
                    "feature",
                    "of",
                    "these",
                    "models,",
                    "researchers",
                    "have",
                    "focused",
                    "on",
                    "attention",
                    "heads",
                    "as",
                    "the",
                    "units",
                    "of",
                    "interpretation."
                ],
                [
                    "These",
                    "studies",
                    "comment",
                    "on",
                    "the",
                    "role",
                    "of",
                    "each",
                    "attention",
                    "head",
                    "and",
                    "the",
                    "relation",
                    "between",
                    "a",
                    "head's",
                    "position",
                    "and",
                    "its",
                    "significance",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "These",
                    "studies",
                    "show",
                    "that",
                    "certain",
                    "heads",
                    "are",
                    "more",
                    "important",
                    "based",
                    "on",
                    "(i)",
                    "their",
                    "position",
                    "in",
                    "the",
                    "network",
                    "(top,",
                    "middle,",
                    "bottom),",
                    "or",
                    "(ii)",
                    "the",
                    "component",
                    "to",
                    "which",
                    "they",
                    "belong",
                    "(encoder",
                    "self-attention,",
                    "decoder",
                    "self-attention,",
                    "encoder-decoder",
                    "cross",
                    "attention),",
                    "or",
                    "(iii)",
                    "the",
                    "functional",
                    "role",
                    "they",
                    "play",
                    "(e.g.,",
                    "syntactic/semantic)."
                ]
            ],
            "context": [
                0,
                0,
                2,
                1,
                1
            ]
        },
        "input": "sent0: The acclaimed success of Transformer-based models across NLP tasks has been followed by two important directions of research.\n sent1: In the first direction, interpretability studies aim to understand how these models work.\n sent2: Given that multi-headed attention is an important feature of these models, researchers have focused on attention heads as the units of interpretation.\n sent3: These studies comment on the role of each attention head and the relation between a head's position and its significance #TARGET_REF .\n sent4: These studies show that certain heads are more important based on (i) their position in the network (top, middle, bottom), or (ii) the component to which they belong (encoder self-attention, decoder self-attention, encoder-decoder cross attention), or (iii) the functional role they play (e.g., syntactic/semantic).\n",
        "output": "{\"INFORMATION\": [\"sent3\", \"sent4\"], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "exploitation",
                    "of",
                    "pre-trained",
                    "models",
                    "followed",
                    "by",
                    "task-specific",
                    "fine-tuning",
                    "haved",
                    "pushed",
                    "the",
                    "state-ofthe-art",
                    "forwards,",
                    "while",
                    "requiring",
                    "much",
                    "less",
                    "computational",
                    "and",
                    "data",
                    "resources."
                ],
                [
                    "The",
                    "idea",
                    "behind",
                    "pre-training",
                    "is",
                    "to",
                    "reuse",
                    "the",
                    "weights",
                    "parameters",
                    "trained",
                    "on",
                    "a",
                    "set",
                    "of",
                    "source",
                    "tasks",
                    "and",
                    "continue",
                    "to",
                    "fine-tune",
                    "them",
                    "on",
                    "under-resourced",
                    "target",
                    "tasks",
                    "to",
                    "achieve",
                    "knowledge",
                    "transfer."
                ],
                [
                    "Dai",
                    "and",
                    "#REF",
                    "were",
                    "the",
                    "first",
                    "to",
                    "propose",
                    "to",
                    "pre-train",
                    "RNNs",
                    "using",
                    "auto-encoders",
                    "and",
                    "language",
                    "models",
                    "as",
                    "part",
                    "of",
                    "their",
                    "QA",
                    "encoding",
                    "layer."
                ],
                [
                    "#REF",
                    "and",
                    "#TARGET_REF",
                    "pre-trained",
                    "QA",
                    "models",
                    "before",
                    "applying",
                    "the",
                    "fine-tuning",
                    "process",
                    "between",
                    "the",
                    "source",
                    "and",
                    "the",
                    "target",
                    "domains."
                ],
                [
                    "Other",
                    "efforts",
                    "focused",
                    "on",
                    "pretraining",
                    "Transformer-based",
                    "models",
                    "multilingually",
                    "such",
                    "as",
                    "the",
                    "multilingual",
                    "version",
                    "of",
                    "BERT",
                    "(called",
                    "mBERT)",
                    "#REF",
                    "or",
                    "XLM-R",
                    "#REF",
                    "to",
                    "learn",
                    "cross-lingual",
                    "representations",
                    "which",
                    "are",
                    "transferable",
                    "across",
                    "languages."
                ]
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "sent0: The exploitation of pre-trained models followed by task-specific fine-tuning haved pushed the state-ofthe-art forwards, while requiring much less computational and data resources.\n sent1: The idea behind pre-training is to reuse the weights parameters trained on a set of source tasks and continue to fine-tune them on under-resourced target tasks to achieve knowledge transfer.\n sent2: Dai and #REF were the first to propose to pre-train RNNs using auto-encoders and language models as part of their QA encoding layer.\n sent3: #REF and #TARGET_REF pre-trained QA models before applying the fine-tuning process between the source and the target domains.\n sent4: Other efforts focused on pretraining Transformer-based models multilingually such as the multilingual version of BERT (called mBERT) #REF or XLM-R #REF to learn cross-lingual representations which are transferable across languages.\n",
        "output": "{\"INFORMATION\": [\"sent3\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "A",
                    "CSC",
                    "captures",
                    "ordering",
                    "constraints",
                    "of",
                    "natural",
                    "language,",
                    "and",
                    "it",
                    "roughly",
                    "corresponds",
                    "to",
                    "phrase",
                    "structure",
                    "rules."
                ],
                [
                    "CSCs",
                    "can",
                    "be",
                    "used",
                    "to",
                    "represent",
                    "syntax",
                    "and",
                    "semantics",
                    "of",
                    "sentences",
                    "at",
                    "different",
                    "levels",
                    "of",
                    "abstraction",
                    "from",
                    "instances",
                    "of",
                    "surface",
                    "sequence",
                    "to",
                    "linguistically",
                    "motivated",
                    "grammar",
                    "such",
                    "as",
                    "Lexical-Functional",
                    "Grammar",
                    "(LFG)",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "As",
                    "shown",
                    "in",
                    "figure",
                    "2,",
                    "a",
                    "CSC",
                    "consists",
                    "of",
                    "a",
                    "root",
                    "node",
                    "(CSR),",
                    "element",
                    "nodes",
                    "(CSE),",
                    "a",
                    "FIRST",
                    "link,",
                    "a",
                    "LAST",
                    "link,",
                    "NEXT",
                    "link(s)",
                    "and",
                    "ROLE",
                    "links."
                ],
                [
                    "A",
                    "CSR",
                    "is",
                    "a",
                    "representative",
                    "node",
                    "for",
                    "the",
                    "meaning",
                    "of",
                    "the",
                    "entire",
                    "CSC",
                    "structure,",
                    "CSRs",
                    "are",
                    "connected",
                    "to",
                    "their",
                    "designated",
                    "interlingual",
                    "concepts",
                    "by",
                    "ENG",
                    "or",
                    "JPN."
                ],
                [
                    "Each",
                    "CSC",
                    "has",
                    "one",
                    "or",
                    "more",
                    "CSEs",
                    "linked",
                    "to",
                    "a",
                    "CSR",
                    "by",
                    "ROLE",
                    "links."
                ],
                [
                    "The",
                    "ordering",
                    "constraints",
                    "between",
                    "two",
                    "concept",
                    "sequence",
                    "element",
                    "nodes",
                    "are",
                    "represented",
                    "by",
                    "NEXT",
                    "link."
                ],
                [
                    "FIRST",
                    "and",
                    "LAST",
                    "links",
                    "in",
                    "each",
                    "CSC",
                    "points",
                    "to",
                    "the",
                    "first",
                    "and",
                    "last",
                    "elements,",
                    "respectively."
                ],
                [
                    "Also,",
                    "each",
                    "CSE",
                    "represents",
                    "the",
                    "relevant",
                    "case",
                    "role,",
                    "and",
                    "the",
                    "case",
                    "role",
                    "has",
                    "a",
                    "selectional",
                    "restriction."
                ],
                [
                    "Since",
                    "we",
                    "want",
                    "to",
                    "avoid",
                    "heavy",
                    "symbolic",
                    "operations",
                    "during",
                    "parsing,",
                    "ROLE",
                    "links",
                    "and",
                    "associated",
                    "constraint",
                    "links",
                    "are",
                    "used",
                    "instead",
                    "of",
                    "performing",
                    "type",
                    "and",
                    "value",
                    "consistency",
                    "check",
                    "by",
                    "unification."
                ],
                [
                    "Therefore",
                    "each",
                    "CSE",
                    "is",
                    "used",
                    "for",
                    "both",
                    "enforcing",
                    "the",
                    "ordering",
                    "constraint",
                    "and",
                    "capturing",
                    "semantic",
                    "information."
                ]
            ],
            "context": [
                0,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: A CSC captures ordering constraints of natural language, and it roughly corresponds to phrase structure rules.\n sent1: CSCs can be used to represent syntax and semantics of sentences at different levels of abstraction from instances of surface sequence to linguistically motivated grammar such as Lexical-Functional Grammar (LFG) #TARGET_REF .\n sent2: As shown in figure 2, a CSC consists of a root node (CSR), element nodes (CSE), a FIRST link, a LAST link, NEXT link(s) and ROLE links.\n sent3: A CSR is a representative node for the meaning of the entire CSC structure, CSRs are connected to their designated interlingual concepts by ENG or JPN.\n sent4: Each CSC has one or more CSEs linked to a CSR by ROLE links.\n sent5: The ordering constraints between two concept sequence element nodes are represented by NEXT link.\n sent6: FIRST and LAST links in each CSC points to the first and last elements, respectively.\n sent7: Also, each CSE represents the relevant case role, and the case role has a selectional restriction.\n sent8: Since we want to avoid heavy symbolic operations during parsing, ROLE links and associated constraint links are used instead of performing type and value consistency check by unification.\n sent9: Therefore each CSE is used for both enforcing the ordering constraint and capturing semantic information.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "combine",
                    "the",
                    "benefits",
                    "of",
                    "the",
                    "message-passing",
                    "paradigm",
                    "with",
                    "the",
                    "benefits",
                    "of",
                    "the",
                    "parameterized",
                    "approach",
                    "to",
                    "build",
                    "a",
                    "more",
                    "efficient,",
                    "but",
                    "easily",
                    "extensible",
                    "system,",
                    "called",
                    "PRINCITRAN."
                ],
                [
                    "1",
                    "Our",
                    "work",
                    "extends",
                    "that",
                    "of",
                    "#REF",
                    "and",
                    "#TARGET_REF",
                    "in",
                    "that",
                    "it",
                    "provides",
                    "a",
                    "parameterization",
                    "mechanism",
                    "along",
                    "the",
                    "lines",
                    "of",
                    "Dorr",
                    "(1993b)",
                    "that",
                    "allows",
                    "the",
                    "system",
                    "to",
                    "be",
                    "ported",
                    "to",
                    "languages",
                    "other",
                    "than",
                    "English."
                ],
                [
                    "We",
                    "focus",
                    "particularly",
                    "on",
                    "the",
                    "problem",
                    "of",
                    "processing",
                    "head-final",
                    "languages",
                    "such",
                    "as",
                    "Korean."
                ],
                [
                    "The",
                    "algorithm",
                    "has",
                    "been",
                    "implemented",
                    "in",
                    "C++",
                    "and",
                    "successfully",
                    "tested",
                    "on",
                    "well-known,",
                    "translationally",
                    "divergent",
                    "sentences."
                ]
            ],
            "context": [
                2,
                2,
                2,
                0
            ]
        },
        "input": "sent0: We combine the benefits of the message-passing paradigm with the benefits of the parameterized approach to build a more efficient, but easily extensible system, called PRINCITRAN.\n sent1: 1 Our work extends that of #REF and #TARGET_REF in that it provides a parameterization mechanism along the lines of Dorr (1993b) that allows the system to be ported to languages other than English.\n sent2: We focus particularly on the problem of processing head-final languages such as Korean.\n sent3: The algorithm has been implemented in C++ and successfully tested on well-known, translationally divergent sentences.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\", \"sent1\", \"sent2\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "These",
                    "co-occurrence",
                    "probabilities",
                    "are",
                    "psycholinguistically",
                    "relevant",
                    "because",
                    "they",
                    "feed",
                    "into",
                    "information-theoretic",
                    "measures",
                    "of",
                    "'thematic",
                    "fit'",
                    "and",
                    "selectional",
                    "restriction",
                    "#REF",
                    "which",
                    "are",
                    "relevant",
                    "in",
                    "predicting",
                    "human",
                    "online",
                    "processing",
                    "difficulty",
                    "(e.g."
                ],
                [
                    "#TARGET_REF",
                    ",",
                    "and",
                    "play",
                    "a",
                    "key",
                    "role",
                    "in",
                    "language",
                    "acquisition",
                    "#REF",
                    "."
                ],
                [
                    "Most",
                    "prominently,",
                    "the",
                    "widely-used",
                    "pointwise",
                    "mutual",
                    "information",
                    "(PMI)",
                    "measure",
                    "of",
                    "association",
                    "strength,",
                    "PMI",
                    "(w,",
                    "c)",
                    "=",
                    "log",
                    "p(w|c)",
                    "p(w)",
                    "#REF",
                    ",",
                    "relies",
                    "on",
                    "these",
                    "condi-tional",
                    "probabilities",
                    "as",
                    "an",
                    "input."
                ],
                [
                    "PMI",
                    "makes",
                    "appearances",
                    "in",
                    "models",
                    "of",
                    "grammar",
                    "induction",
                    "from",
                    "text",
                    "(Magerman",
                    "and",
                    "#REF",
                    ",",
                    "online",
                    "sentence",
                    "comprehension",
                    "and",
                    "production",
                    "#REF",
                    ",",
                    "and",
                    "quantitative",
                    "theories",
                    "of",
                    "word",
                    "order",
                    "variation",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                3,
                3,
                0,
                0
            ]
        },
        "input": "sent0: These co-occurrence probabilities are psycholinguistically relevant because they feed into information-theoretic measures of 'thematic fit' and selectional restriction #REF which are relevant in predicting human online processing difficulty (e.g.\n sent1: #TARGET_REF , and play a key role in language acquisition #REF .\n sent2: Most prominently, the widely-used pointwise mutual information (PMI) measure of association strength, PMI (w, c) = log p(w|c) p(w) #REF , relies on these condi-tional probabilities as an input.\n sent3: PMI makes appearances in models of grammar induction from text (Magerman and #REF , online sentence comprehension and production #REF , and quantitative theories of word order variation #REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Model:",
                    "The",
                    "PAROLE",
                    "Lexicon",
                    "model",
                    "for",
                    "Morphosyntax",
                    "and",
                    "Syntax",
                    "is",
                    "based",
                    "on",
                    "the",
                    "results",
                    "of",
                    "EAGLES",
                    "#REF",
                    "and",
                    "EUREKA",
                    "GENELEX",
                    "#TARGET_REF",
                    ",",
                    "further",
                    "developed",
                    "within",
                    "the",
                    "PAROLE",
                    "project",
                    "#REF",
                    "."
                ],
                [
                    "Thanks",
                    "to",
                    "that",
                    "all",
                    "the",
                    "lexical",
                    "resources",
                    "are",
                    "declarative,",
                    "theory",
                    "and",
                    "application",
                    "independent,",
                    "harmonised,",
                    "multifunctional,",
                    "and",
                    "able",
                    "to",
                    "evolve",
                    "easily,",
                    "for",
                    "example",
                    "to",
                    "incorporate",
                    "other",
                    "levels",
                    "of",
                    "information",
                    "or",
                    "to",
                    "become",
                    "multilingual."
                ],
                [
                    "This",
                    "approach,",
                    "which",
                    "answers",
                    "to",
                    "the",
                    "requisites",
                    "of",
                    "generality,",
                    "explicitness,",
                    "and",
                    "variability",
                    "of",
                    "granularity,",
                    "guarantees",
                    "a",
                    "large",
                    "scale",
                    "reusability."
                ]
            ],
            "context": [
                3,
                2,
                0
            ]
        },
        "input": "sent0: Model: The PAROLE Lexicon model for Morphosyntax and Syntax is based on the results of EAGLES #REF and EUREKA GENELEX #TARGET_REF , further developed within the PAROLE project #REF .\n sent1: Thanks to that all the lexical resources are declarative, theory and application independent, harmonised, multifunctional, and able to evolve easily, for example to incorporate other levels of information or to become multilingual.\n sent2: This approach, which answers to the requisites of generality, explicitness, and variability of granularity, guarantees a large scale reusability.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "A",
                    "key",
                    "hypothesis",
                    "in",
                    "the",
                    "pursuit",
                    "towards",
                    "creating",
                    "goal-driven",
                    "natural",
                    "language-based",
                    "agents",
                    "posits",
                    "that",
                    "interactivity",
                    "and",
                    "environment",
                    "grounding",
                    "is",
                    "critical",
                    "for",
                    "effective",
                    "language",
                    "learning",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Text",
                    "games",
                    "provide",
                    "a",
                    "platform",
                    "on",
                    "which",
                    "to",
                    "interactively",
                    "train",
                    "agents",
                    "that",
                    "can",
                    "both",
                    "act",
                    "and",
                    "speak",
                    "in",
                    "a",
                    "situated",
                    "manner-producing",
                    "language",
                    "that",
                    "is",
                    "both",
                    "goal-driven",
                    "and",
                    "contextually",
                    "relevant."
                ]
            ],
            "context": [
                1,
                3
            ]
        },
        "input": "sent0: A key hypothesis in the pursuit towards creating goal-driven natural language-based agents posits that interactivity and environment grounding is critical for effective language learning #TARGET_REF .\n sent1: Text games provide a platform on which to interactively train agents that can both act and speak in a situated manner-producing language that is both goal-driven and contextually relevant.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Note",
                    "that",
                    "the",
                    "difference",
                    "between",
                    "total",
                    "annotations",
                    "excluding",
                    "neutral",
                    "(24,164)",
                    "and",
                    "the",
                    "combined",
                    "number",
                    "of",
                    "annotations",
                    "#TARGET_REF",
                    "differ",
                    "because",
                    "once",
                    "the",
                    "dataset",
                    "was",
                    "saved",
                    "as",
                    "a",
                    "Python",
                    "dictionary,",
                    "identical",
                    "lines",
                    "were",
                    "merged",
                    "as",
                    "one",
                    "(i.e."
                ],
                [
                    "some",
                    "common",
                    "movie",
                    "lines",
                    "like",
                    "\"All",
                    "right",
                    "then!\""
                ],
                [
                    "and",
                    "\"I",
                    "love",
                    "you\"",
                    "appeared",
                    "multiple",
                    "times",
                    "from",
                    "different",
                    "sources)."
                ],
                [
                    "Additionally,",
                    "lines",
                    "annotated",
                    "as",
                    "both",
                    "neutral",
                    "and",
                    "an",
                    "emotion",
                    "were",
                    "removed",
                    "from",
                    "the",
                    "neutral",
                    "set."
                ]
            ],
            "context": [
                2,
                3,
                3,
                0
            ]
        },
        "input": "sent0: Note that the difference between total annotations excluding neutral (24,164) and the combined number of annotations #TARGET_REF differ because once the dataset was saved as a Python dictionary, identical lines were merged as one (i.e.\n sent1: some common movie lines like \"All right then!\"\n sent2: and \"I love you\" appeared multiple times from different sources).\n sent3: Additionally, lines annotated as both neutral and an emotion were removed from the neutral set.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "A",
                    "three-fold",
                    "analysis",
                    "of",
                    "the",
                    "selected",
                    "keywords",
                    "was",
                    "done."
                ],
                [
                    "The",
                    "semantics",
                    "of",
                    "the",
                    "words",
                    "was",
                    "studied",
                    "by",
                    "using",
                    "WordNet."
                ],
                [
                    "In",
                    "dialogue",
                    "acts",
                    "such",
                    "as",
                    "apologizing,",
                    "thanking,",
                    "or",
                    "expressing",
                    "sympathy,",
                    "affective",
                    "language",
                    "is",
                    "often",
                    "employed",
                    "to",
                    "represent",
                    "and",
                    "convey",
                    "psychological",
                    "attitudes",
                    "#REF",
                    "."
                ],
                [
                    "Also,",
                    "there",
                    "is",
                    "what",
                    "is",
                    "called",
                    "a",
                    "'heartfelt",
                    "apology'",
                    "as",
                    "against",
                    "'routine",
                    "apology",
                    "#TARGET_REF",
                    ")."
                ],
                [
                    "Hence,",
                    "it",
                    "was",
                    "decided",
                    "to",
                    "further",
                    "explore",
                    "the",
                    "sentiments",
                    "and",
                    "emotions",
                    "associated",
                    "with",
                    "the",
                    "keywords."
                ],
                [
                    "The",
                    "sentiments",
                    "were",
                    "studied",
                    "using",
                    "SentiWordNet",
                    "and",
                    "the",
                    "emotion",
                    "labels",
                    "were",
                    "determined",
                    "through",
                    "WordNet-Affect."
                ],
                [
                    "The",
                    "analysis",
                    "and",
                    "conclusions",
                    "thus",
                    "drawn",
                    "are",
                    "presented",
                    "below."
                ]
            ],
            "context": [
                0,
                0,
                0,
                2,
                0,
                0,
                0
            ]
        },
        "input": "sent0: A three-fold analysis of the selected keywords was done.\n sent1: The semantics of the words was studied by using WordNet.\n sent2: In dialogue acts such as apologizing, thanking, or expressing sympathy, affective language is often employed to represent and convey psychological attitudes #REF .\n sent3: Also, there is what is called a 'heartfelt apology' as against 'routine apology #TARGET_REF ).\n sent4: Hence, it was decided to further explore the sentiments and emotions associated with the keywords.\n sent5: The sentiments were studied using SentiWordNet and the emotion labels were determined through WordNet-Affect.\n sent6: The analysis and conclusions thus drawn are presented below.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent3\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Modern",
                    "search",
                    "engines",
                    "often",
                    "attempt",
                    "to",
                    "provide",
                    "structured",
                    "search",
                    "results",
                    "that",
                    "reveal",
                    "more",
                    "facets",
                    "of",
                    "the",
                    "search",
                    "query",
                    "than",
                    "explicitly",
                    "requested."
                ],
                [
                    "These",
                    "results",
                    "rely",
                    "on",
                    "knowledge",
                    "bases",
                    "that",
                    "contain",
                    "tuples",
                    "of",
                    "the",
                    "form",
                    "(entity,",
                    "attribute,",
                    "value)."
                ],
                [
                    "However,",
                    "the",
                    "number",
                    "of",
                    "known",
                    "entities",
                    "and",
                    "attributes",
                    "in",
                    "these",
                    "knowledge",
                    "bases",
                    "is",
                    "limited",
                    "and",
                    "there",
                    "is",
                    "a",
                    "long",
                    "tail",
                    "of",
                    "both",
                    "entities",
                    "and",
                    "attributes",
                    "that",
                    "is",
                    "too",
                    "large",
                    "to",
                    "be",
                    "manually",
                    "curated."
                ],
                [
                    "The",
                    "goal",
                    "of",
                    "automatic",
                    "entityattribute",
                    "extraction",
                    "is",
                    "to",
                    "replace",
                    "manual",
                    "knowledge",
                    "acquisition",
                    "which",
                    "is",
                    "expensive",
                    "and",
                    "biased",
                    "towards",
                    "popular",
                    "entities",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Previous",
                    "studies",
                    "have",
                    "proposed",
                    "model-based",
                    "approaches",
                    "that",
                    "use",
                    "various",
                    "NLP",
                    "features,",
                    "distant",
                    "supervision",
                    "and",
                    "traditional",
                    "machine",
                    "learning",
                    "methods",
                    "for",
                    "entity-attribute",
                    "extraction",
                    "but",
                    "*",
                    "Work",
                    "done",
                    "during",
                    "an",
                    "internship",
                    "at",
                    "Google."
                ],
                [
                    "their",
                    "precision",
                    "has",
                    "not",
                    "been",
                    "high",
                    "enough",
                    "to",
                    "replace",
                    "manually",
                    "curated",
                    "knowledge",
                    "bases",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                0,
                0,
                0,
                1,
                3,
                3
            ]
        },
        "input": "sent0: Modern search engines often attempt to provide structured search results that reveal more facets of the search query than explicitly requested.\n sent1: These results rely on knowledge bases that contain tuples of the form (entity, attribute, value).\n sent2: However, the number of known entities and attributes in these knowledge bases is limited and there is a long tail of both entities and attributes that is too large to be manually curated.\n sent3: The goal of automatic entityattribute extraction is to replace manual knowledge acquisition which is expensive and biased towards popular entities #TARGET_REF .\n sent4: Previous studies have proposed model-based approaches that use various NLP features, distant supervision and traditional machine learning methods for entity-attribute extraction but * Work done during an internship at Google.\n sent5: their precision has not been high enough to replace manually curated knowledge bases #REF .\n",
        "output": "{\"INFORMATION\": [\"sent3\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent4\", \"sent5\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Our",
                    "training",
                    "data",
                    "include",
                    "four",
                    "datasets",
                    "about",
                    "causal",
                    "and",
                    "temporal",
                    "relations",
                    "between",
                    "event",
                    "texts."
                ],
                [
                    "PDTB",
                    "3.0",
                    "#REF",
                    "is",
                    "WSJ",
                    "articles",
                    "annotated",
                    "with",
                    "four",
                    "high-level",
                    "discourse",
                    "relations,",
                    "and",
                    "we",
                    "map",
                    "the",
                    "sub-relations",
                    "of",
                    "'Temporal'",
                    "to",
                    "our",
                    "classes."
                ],
                [
                    "5",
                    "BECauSE",
                    "2.0",
                    "#TARGET_REF",
                    "is",
                    "news",
                    "articles",
                    "annotated",
                    "with",
                    "linguistically",
                    "marked",
                    "causality."
                ],
                [
                    "WIQA",
                    "#REF",
                    "is",
                    "scientific",
                    "event",
                    "texts",
                    "annotated",
                    "with",
                    "causality",
                    "between",
                    "events."
                ],
                [
                    "ConceptNet",
                    "#REF",
                    ")",
                    "is",
                    "a",
                    "knowledge",
                    "graph",
                    "between",
                    "phrases,",
                    "and",
                    "relations",
                    "about",
                    "causality",
                    "are",
                    "mapped",
                    "to",
                    "our",
                    "classes."
                ],
                [
                    "To",
                    "prevent",
                    "overfitting",
                    "to",
                    "corpus-specific",
                    "characteristics,",
                    "we",
                    "add",
                    "adversarial",
                    "data",
                    "by",
                    "swapping",
                    "two",
                    "input",
                    "texts",
                    "(PDTB-R,",
                    "BECauSE-R,",
                    "ConceptNet-R)",
                    "or",
                    "pairing",
                    "random",
                    "texts",
                    "(WIQA-P)."
                ],
                [
                    "The",
                    "mapping",
                    "between",
                    "corpus-specific",
                    "labels",
                    "and",
                    "ours",
                    "is",
                    "in",
                    "Table",
                    "3,",
                    "and",
                    "the",
                    "module",
                    "accuracy",
                    "in",
                    "Table",
                    "2",
                    "rows",
                    "11-19."
                ],
                [
                    "5",
                    "We",
                    "use",
                    "explicit",
                    "relations",
                    "only",
                    "for",
                    "pretraining,",
                    "since",
                    "they",
                    "often",
                    "capture",
                    "linguistically",
                    "marked,",
                    "rather",
                    "than",
                    "true,",
                    "relations",
                    "between",
                    "events."
                ],
                [
                    "We",
                    "also",
                    "exclude",
                    "the",
                    "Contingency",
                    "relations",
                    "as",
                    "causal",
                    "and",
                    "non-causal",
                    "relations",
                    "(e.g.,",
                    "justification)",
                    "are",
                    "mixed."
                ]
            ],
            "context": [
                2,
                0,
                1,
                0,
                0,
                2,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Our training data include four datasets about causal and temporal relations between event texts.\n sent1: PDTB 3.0 #REF is WSJ articles annotated with four high-level discourse relations, and we map the sub-relations of 'Temporal' to our classes.\n sent2: 5 BECauSE 2.0 #TARGET_REF is news articles annotated with linguistically marked causality.\n sent3: WIQA #REF is scientific event texts annotated with causality between events.\n sent4: ConceptNet #REF ) is a knowledge graph between phrases, and relations about causality are mapped to our classes.\n sent5: To prevent overfitting to corpus-specific characteristics, we add adversarial data by swapping two input texts (PDTB-R, BECauSE-R, ConceptNet-R) or pairing random texts (WIQA-P).\n sent6: The mapping between corpus-specific labels and ours is in Table 3, and the module accuracy in Table 2 rows 11-19.\n sent7: 5 We use explicit relations only for pretraining, since they often capture linguistically marked, rather than true, relations between events.\n sent8: We also exclude the Contingency relations as causal and non-causal relations (e.g., justification) are mixed.\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [\"sent0\", \"sent5\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "To",
                    "model",
                    "the",
                    "relational",
                    "information",
                    "in",
                    "the",
                    "commonsen",
                    "KG,",
                    "we",
                    "employ",
                    "the",
                    "relational",
                    "graph",
                    "convolutional",
                    "network",
                    "(R-GCN)",
                    "#TARGET_REF",
                    "which",
                    "generalizes",
                    "GCN",
                    "with",
                    "relation",
                    "specific",
                    "weight",
                    "matrices."
                ],
                [
                    "We",
                    "follow",
                    "#REF",
                    "and",
                    "#REF",
                    "to",
                    "use",
                    "a",
                    "non-parametric",
                    "compositional",
                    "operation",
                    "ϕ(•)",
                    "to",
                    "combine",
                    "the",
                    "concept",
                    "node",
                    "embedding",
                    "and",
                    "the",
                    "relation",
                    "embedding."
                ],
                [
                    "Specifically,",
                    "given",
                    "the",
                    "input",
                    "subgraph",
                    "G",
                    "x",
                    "=",
                    "{V",
                    "x",
                    ",",
                    "E",
                    "x",
                    "}",
                    "and",
                    "an",
                    "R-GCN",
                    "with",
                    "L",
                    "layers,",
                    "we",
                    "update",
                    "the",
                    "embedding",
                    "of",
                    "each",
                    "node",
                    "v",
                    "∈",
                    "V",
                    "x",
                    "at",
                    "the",
                    "(l+1)-th",
                    "layer",
                    "by",
                    "aggregating",
                    "information",
                    "from",
                    "the",
                    "embeddings",
                    "of",
                    "its",
                    "neighbours",
                    "in",
                    "N",
                    "(v)",
                    "at",
                    "the",
                    "l-th",
                    "layer:"
                ]
            ],
            "context": [
                1,
                0,
                0
            ]
        },
        "input": "sent0: To model the relational information in the commonsen KG, we employ the relational graph convolutional network (R-GCN) #TARGET_REF which generalizes GCN with relation specific weight matrices.\n sent1: We follow #REF and #REF to use a non-parametric compositional operation ϕ(•) to combine the concept node embedding and the relation embedding.\n sent2: Specifically, given the input subgraph G x = {V x , E x } and an R-GCN with L layers, we update the embedding of each node v ∈ V x at the (l+1)-th layer by aggregating information from the embeddings of its neighbours in N (v) at the l-th layer:\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Bostan",
                    "and",
                    "#REF",
                    "analyze",
                    "14",
                    "existing",
                    "emotion",
                    "datasets",
                    "of",
                    "which",
                    "only",
                    "two",
                    "are",
                    "multilabel."
                ],
                [
                    "These",
                    "are",
                    "AffectiveText",
                    "#TARGET_REF",
                    "and",
                    "SSEC",
                    "#REF",
                    "."
                ],
                [
                    "Nearly",
                    "all",
                    "of",
                    "these",
                    "datasets",
                    "use",
                    "an",
                    "annotation",
                    "scheme",
                    "based",
                    "on",
                    "Ekman",
                    "#REF",
                    "with",
                    "many",
                    "adding",
                    "a",
                    "few",
                    "labels",
                    "often",
                    "following",
                    "Plutchik's",
                    "theory",
                    "of",
                    "emotions",
                    "#REF",
                    "."
                ],
                [
                    "A",
                    "typical",
                    "emotion",
                    "dataset",
                    "consists",
                    "of",
                    "6-8",
                    "categories."
                ],
                [
                    "The",
                    "exception",
                    "Bostan",
                    "and",
                    "#REF",
                    "mention",
                    "is",
                    "CrowdFlower",
                    "2",
                    "with",
                    "14",
                    "categories,",
                    "and",
                    "those",
                    "not",
                    "mentioned",
                    "in",
                    "Bostan",
                    "et",
                    "al."
                ],
                [
                    "are",
                    "e.g."
                ],
                [
                    "the",
                    "SemEval",
                    "2018",
                    "task",
                    "1",
                    "subtask",
                    "c",
                    "dataset",
                    "#REF",
                    "with",
                    "11",
                    "categories,",
                    "EmoNet",
                    "with",
                    "24",
                    "#REF",
                    ",",
                    "and",
                    "the",
                    "GoEmotions",
                    "dataset",
                    "#REF",
                    "with",
                    "27",
                    "categories."
                ]
            ],
            "context": [
                3,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Bostan and #REF analyze 14 existing emotion datasets of which only two are multilabel.\n sent1: These are AffectiveText #TARGET_REF and SSEC #REF .\n sent2: Nearly all of these datasets use an annotation scheme based on Ekman #REF with many adding a few labels often following Plutchik's theory of emotions #REF .\n sent3: A typical emotion dataset consists of 6-8 categories.\n sent4: The exception Bostan and #REF mention is CrowdFlower 2 with 14 categories, and those not mentioned in Bostan et al.\n sent5: are e.g.\n sent6: the SemEval 2018 task 1 subtask c dataset #REF with 11 categories, EmoNet with 24 #REF , and the GoEmotions dataset #REF with 27 categories.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "entity",
                    "linking",
                    "specifically,",
                    "typing",
                    "has",
                    "been",
                    "explored",
                    "for",
                    "cross-domain",
                    "entity",
                    "linking",
                    "#REF",
                    "."
                ],
                [
                    "Past",
                    "work",
                    "by",
                    "#TARGET_REF",
                    "has",
                    "also",
                    "explored",
                    "learning",
                    "a",
                    "type",
                    "system",
                    "for",
                    "this",
                    "task."
                ],
                [
                    "Our",
                    "approach",
                    "to",
                    "learning",
                    "types",
                    "starts",
                    "from",
                    "a",
                    "large",
                    "set",
                    "and",
                    "filters",
                    "it",
                    "down,",
                    "which",
                    "is",
                    "a",
                    "simpler",
                    "problem."
                ],
                [
                    "A",
                    "range",
                    "of",
                    "approaches",
                    "have",
                    "also",
                    "considered",
                    "augmenting",
                    "pretrained",
                    "models",
                    "with",
                    "type",
                    "information",
                    "#REF",
                    ",",
                    "however,",
                    "in",
                    "these",
                    "models,",
                    "the",
                    "types",
                    "inform",
                    "dense",
                    "embeddings",
                    "which",
                    "are",
                    "still",
                    "uninterpretable."
                ]
            ],
            "context": [
                3,
                3,
                0,
                0
            ]
        },
        "input": "sent0: In entity linking specifically, typing has been explored for cross-domain entity linking #REF .\n sent1: Past work by #TARGET_REF has also explored learning a type system for this task.\n sent2: Our approach to learning types starts from a large set and filters it down, which is a simpler problem.\n sent3: A range of approaches have also considered augmenting pretrained models with type information #REF , however, in these models, the types inform dense embeddings which are still uninterpretable.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Baselines",
                    "We",
                    "compare",
                    "the",
                    "SimpDefiner",
                    "with",
                    "generation-simplification",
                    "pipelines."
                ],
                [
                    "We",
                    "first",
                    "employ",
                    "LOG-CaD",
                    "#REF",
                    "and",
                    "MASS",
                    "#REF",
                    "models",
                    "to",
                    "generate",
                    "definitions,",
                    "and",
                    "then",
                    "employ",
                    "ACCESS",
                    "#TARGET_REF",
                    "and",
                    "MUSS",
                    "#REF",
                    "models",
                    "to",
                    "simplify",
                    "them."
                ],
                [
                    "Thus,",
                    "we",
                    "have",
                    "four",
                    "different",
                    "pipeline",
                    "baselines."
                ],
                [
                    "Since",
                    "these",
                    "models",
                    "are",
                    "not",
                    "available",
                    "in",
                    "Chinese,",
                    "we",
                    "only",
                    "apply",
                    "these",
                    "pipelines",
                    "to",
                    "English",
                    "datasets."
                ],
                [
                    "For",
                    "the",
                    "Chinese",
                    "SDG",
                    "task,",
                    "we",
                    "specially",
                    "pretrained",
                    "a",
                    "MASS-ZH",
                    "model",
                    "from",
                    "scratch",
                    "using",
                    "the",
                    "Chinese",
                    "Gigaword",
                    "Fifth",
                    "Edition",
                    "3",
                    "corpus."
                ],
                [
                    "Note",
                    "that",
                    "we",
                    "set",
                    "the",
                    "learning",
                    "rate",
                    "to",
                    "3e-4,",
                    "warmup",
                    "steps",
                    "to",
                    "500",
                    "when",
                    "fine-tuning",
                    "both",
                    "MASS",
                    "and",
                    "MASS-ZH."
                ]
            ],
            "context": [
                3,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Baselines We compare the SimpDefiner with generation-simplification pipelines.\n sent1: We first employ LOG-CaD #REF and MASS #REF models to generate definitions, and then employ ACCESS #TARGET_REF and MUSS #REF models to simplify them.\n sent2: Thus, we have four different pipeline baselines.\n sent3: Since these models are not available in Chinese, we only apply these pipelines to English datasets.\n sent4: For the Chinese SDG task, we specially pretrained a MASS-ZH model from scratch using the Chinese Gigaword Fifth Edition 3 corpus.\n sent5: Note that we set the learning rate to 3e-4, warmup steps to 500 when fine-tuning both MASS and MASS-ZH.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Most",
                    "previous",
                    "attempts",
                    "aim",
                    "to",
                    "describe",
                    "the",
                    "image",
                    "indicating",
                    "the",
                    "salient",
                    "objects",
                    "and",
                    "relations",
                    "without",
                    "considering",
                    "user",
                    "intention."
                ],
                [
                    "To",
                    "generate",
                    "controllable",
                    "and",
                    "explainable",
                    "captions,",
                    "recent",
                    "works",
                    "dedicated",
                    "to",
                    "establishing",
                    "a",
                    "new",
                    "controllable",
                    "image",
                    "captioning",
                    "task",
                    "to",
                    "generate",
                    "the",
                    "caption",
                    "at",
                    "will."
                ],
                [
                    "The",
                    "captioning",
                    "process",
                    "can",
                    "be",
                    "controlled",
                    "by",
                    "POS",
                    "tagging",
                    "#REF",
                    ",",
                    "sentiment",
                    "#TARGET_REF",
                    ",",
                    "length",
                    "#REF",
                    ",",
                    "bounding",
                    "boxes",
                    "#REF",
                    ",",
                    "and",
                    "mouse",
                    "traces",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                0,
                3,
                3
            ]
        },
        "input": "sent0: Most previous attempts aim to describe the image indicating the salient objects and relations without considering user intention.\n sent1: To generate controllable and explainable captions, recent works dedicated to establishing a new controllable image captioning task to generate the caption at will.\n sent2: The captioning process can be controlled by POS tagging #REF , sentiment #TARGET_REF , length #REF , bounding boxes #REF , and mouse traces #REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "(a)",
                    "the",
                    "back",
                    "vowels",
                    "a,",
                    "u,",
                    "o",
                    "and",
                    "the",
                    "diphthong",
                    "au,",
                    "can",
                    "be",
                    "fronted",
                    "#TARGET_REF"
                ]
            ],
            "context": [
                1
            ]
        },
        "input": "sent0: (a) the back vowels a, u, o and the diphthong au, can be fronted #TARGET_REF\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "CWI",
                    "2016",
                    "#REF",
                    ",",
                    "complexity",
                    "was",
                    "defined",
                    "as",
                    "whether",
                    "or",
                    "not",
                    "a",
                    "word",
                    "is",
                    "difficult",
                    "to",
                    "understand",
                    "for",
                    "non-native",
                    "English",
                    "speakers",
                    "and",
                    "the",
                    "words",
                    "in",
                    "the",
                    "dataset",
                    "are",
                    "tagged",
                    "as",
                    "complex",
                    "or",
                    "non-complex",
                    "by",
                    "400",
                    "non-native",
                    "English",
                    "speakers."
                ],
                [
                    "The",
                    "results",
                    "highlight",
                    "the",
                    "effectiveness",
                    "of",
                    "Decision",
                    "Trees",
                    "#REF",
                    "and",
                    "Ensemble",
                    "methods",
                    "#TARGET_REF",
                    "for",
                    "the",
                    "task."
                ]
            ],
            "context": [
                3,
                2
            ]
        },
        "input": "sent0: In CWI 2016 #REF , complexity was defined as whether or not a word is difficult to understand for non-native English speakers and the words in the dataset are tagged as complex or non-complex by 400 non-native English speakers.\n sent1: The results highlight the effectiveness of Decision Trees #REF and Ensemble methods #TARGET_REF for the task.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "task",
                    "of",
                    "neural",
                    "event",
                    "detection",
                    "has",
                    "been",
                    "attempted",
                    "using",
                    "a",
                    "combination",
                    "of",
                    "networks,",
                    "but",
                    "mostly",
                    "revolving",
                    "around",
                    "the",
                    "use",
                    "of",
                    "convolutional",
                    "neural",
                    "architectures."
                ],
                [
                    "Work",
                    "in",
                    "this",
                    "approach",
                    "focused",
                    "on",
                    "various",
                    "aspects",
                    "such",
                    "as",
                    "max-pooling",
                    "to",
                    "retrieve",
                    "the",
                    "structure",
                    "of",
                    "event",
                    "nugget",
                    "information",
                    "#TARGET_REF",
                    ",",
                    "modeling",
                    "the",
                    "skipgram",
                    "architecture",
                    "to",
                    "learn",
                    "lexical",
                    "feature",
                    "representations",
                    "#REF",
                    "as",
                    "well",
                    "as",
                    "using",
                    "dynamic",
                    "CNNs",
                    "in",
                    "order",
                    "to",
                    "extract",
                    "lexical",
                    "and",
                    "syntactic",
                    "features",
                    "in",
                    "parallel",
                    "#REF",
                    "."
                ],
                [
                    "Recurrent",
                    "neural",
                    "architectures",
                    "have",
                    "also",
                    "been",
                    "employed",
                    "for",
                    "this",
                    "task,",
                    "which",
                    "predict",
                    "the",
                    "location",
                    "of",
                    "the",
                    "trigger",
                    "based",
                    "on",
                    "combining",
                    "the",
                    "for-ward",
                    "and",
                    "backward",
                    "features",
                    "of",
                    "sentences",
                    "in",
                    "which",
                    "events",
                    "occur",
                    "#REF",
                    "."
                ],
                [
                    "Note",
                    "that",
                    "in",
                    "both",
                    "cases",
                    "architectures",
                    "focused",
                    "on",
                    "dealing",
                    "with",
                    "structural,",
                    "lexical",
                    "and",
                    "contextual",
                    "features."
                ]
            ],
            "context": [
                0,
                3,
                0,
                0
            ]
        },
        "input": "sent0: The task of neural event detection has been attempted using a combination of networks, but mostly revolving around the use of convolutional neural architectures.\n sent1: Work in this approach focused on various aspects such as max-pooling to retrieve the structure of event nugget information #TARGET_REF , modeling the skipgram architecture to learn lexical feature representations #REF as well as using dynamic CNNs in order to extract lexical and syntactic features in parallel #REF .\n sent2: Recurrent neural architectures have also been employed for this task, which predict the location of the trigger based on combining the for-ward and backward features of sentences in which events occur #REF .\n sent3: Note that in both cases architectures focused on dealing with structural, lexical and contextual features.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "AmazonQA",
                    "dataset",
                    "was",
                    "used",
                    "in",
                    "this",
                    "study",
                    "#REF",
                    "."
                ],
                [
                    "The",
                    "dataset",
                    "has",
                    "both",
                    "yes/no",
                    "(binary)",
                    "and",
                    "open-ended",
                    "questions."
                ],
                [
                    "The",
                    "fields",
                    "we",
                    "used",
                    "are",
                    "question",
                    "id,",
                    "question",
                    "Type,",
                    "question",
                    "Text,",
                    "answers,",
                    "review",
                    "snippets,",
                    "asin/",
                    "product",
                    "id,",
                    "and",
                    "category."
                ],
                [
                    "The",
                    "dataset",
                    "was",
                    "built",
                    "based",
                    "on",
                    "previous",
                    "parallel",
                    "datasets",
                    "provided",
                    "by",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                2,
                0,
                0,
                3
            ]
        },
        "input": "sent0: The AmazonQA dataset was used in this study #REF .\n sent1: The dataset has both yes/no (binary) and open-ended questions.\n sent2: The fields we used are question id, question Type, question Text, answers, review snippets, asin/ product id, and category.\n sent3: The dataset was built based on previous parallel datasets provided by #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Given",
                    "that",
                    "Task",
                    "1",
                    "was",
                    "present",
                    "in",
                    "previous",
                    "editions",
                    "of",
                    "the",
                    "SMM4h",
                    "shared",
                    "task",
                    "#REF",
                    ",",
                    "several",
                    "approaches",
                    "were",
                    "employed",
                    "to",
                    "address",
                    "its",
                    "challenges."
                ],
                [
                    "For",
                    "example,",
                    "the",
                    "winning",
                    "team",
                    "from",
                    "2019",
                    "#REF",
                    "used",
                    "an",
                    "ensemble",
                    "of",
                    "BioBERT-CRF",
                    "models",
                    "for",
                    "the",
                    "ADE",
                    "extraction",
                    "task,",
                    "while",
                    "addressing",
                    "the",
                    "resolution",
                    "task",
                    "as",
                    "a",
                    "classification."
                ],
                [
                    "The",
                    "system",
                    "proposed",
                    "by",
                    "#REF",
                    "ranked",
                    "first",
                    "at",
                    "the",
                    "end-to-end",
                    "2020",
                    "competition",
                    "using",
                    "the",
                    "pretrained",
                    "EnDR-BERT",
                    "#REF",
                    "and",
                    "the",
                    "CSIRO",
                    "Adverse",
                    "Drug",
                    "Event",
                    "Corpus",
                    "(CADEC)",
                    "#REF",
                    "for",
                    "further",
                    "training",
                    "the",
                    "model."
                ],
                [
                    "In",
                    "addition,",
                    "#TARGET_REF",
                    "showed",
                    "that",
                    "bidirectional",
                    "Transformers",
                    "trained",
                    "using",
                    "class",
                    "weighting,",
                    "together",
                    "with",
                    "ensembles",
                    "that",
                    "combine",
                    "various",
                    "configurations,",
                    "achieve",
                    "an",
                    "F1-score",
                    "of",
                    ".705",
                    "on",
                    "the",
                    "dataset",
                    "made",
                    "available",
                    "for",
                    "that",
                    "edition",
                    "of",
                    "the",
                    "competition."
                ]
            ],
            "context": [
                0,
                3,
                3,
                1
            ]
        },
        "input": "sent0: Given that Task 1 was present in previous editions of the SMM4h shared task #REF , several approaches were employed to address its challenges.\n sent1: For example, the winning team from 2019 #REF used an ensemble of BioBERT-CRF models for the ADE extraction task, while addressing the resolution task as a classification.\n sent2: The system proposed by #REF ranked first at the end-to-end 2020 competition using the pretrained EnDR-BERT #REF and the CSIRO Adverse Drug Event Corpus (CADEC) #REF for further training the model.\n sent3: In addition, #TARGET_REF showed that bidirectional Transformers trained using class weighting, together with ensembles that combine various configurations, achieve an F1-score of .705 on the dataset made available for that edition of the competition.\n",
        "output": "{\"INFORMATION\": [\"sent3\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "then",
                    "use",
                    "the",
                    "highway",
                    "network",
                    "#REF",
                    "on",
                    "the",
                    "combined",
                    "hidden",
                    "state",
                    "vector",
                    "h.",
                    "This",
                    "network",
                    "adaptively",
                    "\"carries\"",
                    "some",
                    "dimensions",
                    "of",
                    "h",
                    "to",
                    "the",
                    "output",
                    "for",
                    "predicting",
                    "the",
                    "correct",
                    "label",
                    "sequence."
                ],
                [
                    "Therefore,",
                    "the",
                    "hidden",
                    "states",
                    "undergo",
                    "the",
                    "following",
                    "transformation",
                    "#TARGET_REF",
                    ":h",
                    "i",
                    "=",
                    "ρ(h",
                    "i",
                    ")",
                    "g(W",
                    "H",
                    "•",
                    "hi",
                    "+b",
                    "H",
                    ")+(1−ρ(h))",
                    "hi",
                    "(9)The",
                    "function",
                    "ρ(h",
                    "w",
                    ")",
                    "=",
                    "σ(W",
                    "ρ",
                    "•",
                    "h",
                    "i",
                    "+",
                    "b",
                    "ρ",
                    "),",
                    "which",
                    "is",
                    "a",
                    "simple",
                    "activation",
                    "function."
                ],
                [
                    "g",
                    "is",
                    "any",
                    "non-linear",
                    "function,",
                    "such",
                    "as",
                    "sigmoid",
                    "or",
                    "hyperbolic",
                    "tangent."
                ],
                [
                    "Following",
                    "the",
                    "highway",
                    "network's",
                    "output,",
                    "we",
                    "pass",
                    "the",
                    "hidden",
                    "embeddings",
                    "to",
                    "a",
                    "dropout",
                    "layer,",
                    "which",
                    "effectively",
                    "reduces",
                    "the",
                    "number",
                    "of",
                    "hidden",
                    "units",
                    "by",
                    "a",
                    "fraction",
                    "d,",
                    "so",
                    "h",
                    "drop",
                    "∈",
                    "R",
                    "k/d×l",
                    ",",
                    "and",
                    "a",
                    "linear",
                    "layer,",
                    "which",
                    "maps",
                    "the",
                    "h",
                    "drop",
                    "to",
                    "a",
                    "smaller",
                    "embedding",
                    "space."
                ],
                [
                    "We",
                    "label",
                    "this",
                    "space",
                    "h",
                    "∈",
                    "R",
                    "k/d×f",
                    "(f",
                    "being",
                    "the",
                    "dimensions",
                    "of",
                    "the",
                    "feature",
                    "space)",
                    "for",
                    "brevity."
                ]
            ],
            "context": [
                3,
                3,
                3,
                0,
                0
            ]
        },
        "input": "sent0: We then use the highway network #REF on the combined hidden state vector h. This network adaptively \"carries\" some dimensions of h to the output for predicting the correct label sequence.\n sent1: Therefore, the hidden states undergo the following transformation #TARGET_REF :h i = ρ(h i ) g(W H • hi +b H )+(1−ρ(h)) hi (9)The function ρ(h w ) = σ(W ρ • h i + b ρ ), which is a simple activation function.\n sent2: g is any non-linear function, such as sigmoid or hyperbolic tangent.\n sent3: Following the highway network's output, we pass the hidden embeddings to a dropout layer, which effectively reduces the number of hidden units by a fraction d, so h drop ∈ R k/d×l , and a linear layer, which maps the h drop to a smaller embedding space.\n sent4: We label this space h ∈ R k/d×f (f being the dimensions of the feature space) for brevity.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\", \"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Generating",
                    "Job",
                    "Ads",
                    "We",
                    "use",
                    "the",
                    "OpenAI",
                    "Davinci",
                    "GPT-3",
                    "model",
                    "which",
                    "has",
                    "been",
                    "adapted",
                    "for",
                    "natural",
                    "language",
                    "requests."
                ],
                [
                    "We",
                    "use",
                    "default",
                    "parameters",
                    "values",
                    "and",
                    "500",
                    "maximum",
                    "tokens",
                    "per",
                    "completion",
                    "(see",
                    "Appendix",
                    "B",
                    "for",
                    "hyperparameter",
                    "details)."
                ],
                [
                    "Keeping",
                    "default",
                    "parameters",
                    "better",
                    "reflects",
                    "when",
                    "non-technical",
                    "users",
                    "apply",
                    "large-scale",
                    "generative",
                    "models",
                    "\"out-of-the-box\"",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                3,
                2,
                1
            ]
        },
        "input": "sent0: Generating Job Ads We use the OpenAI Davinci GPT-3 model which has been adapted for natural language requests.\n sent1: We use default parameters values and 500 maximum tokens per completion (see Appendix B for hyperparameter details).\n sent2: Keeping default parameters better reflects when non-technical users apply large-scale generative models \"out-of-the-box\" #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "For",
                    "this",
                    "study,",
                    "we",
                    "developed",
                    "a",
                    "novel",
                    "post-editing",
                    "interface,",
                    "based",
                    "on",
                    "the",
                    "open",
                    "source",
                    "software",
                    "used",
                    "and",
                    "released",
                    "by",
                    "#REF",
                    "."
                ],
                [
                    "Our",
                    "software",
                    "is",
                    "written",
                    "using",
                    "Scala",
                    "#REF",
                    ",",
                    "and",
                    "is",
                    "released",
                    "as",
                    "open",
                    "source",
                    "(see",
                    "the",
                    "software",
                    "supplement",
                    "that",
                    "accompanies",
                    "this",
                    "work)."
                ],
                [
                    "This",
                    "code",
                    "constitutes",
                    "a",
                    "ground-up",
                    "rewrite",
                    "of",
                    "the",
                    "Java-based",
                    "post-editing",
                    "interface",
                    "of",
                    "#TARGET_REF",
                    ",",
                    "written",
                    "using",
                    "a",
                    "strict",
                    "model-view-controller",
                    "software",
                    "design",
                    "pattern",
                    "to",
                    "be",
                    "easy",
                    "for",
                    "other",
                    "researchers",
                    "to",
                    "use",
                    "and",
                    "extend."
                ]
            ],
            "context": [
                0,
                0,
                0
            ]
        },
        "input": "sent0: For this study, we developed a novel post-editing interface, based on the open source software used and released by #REF .\n sent1: Our software is written using Scala #REF , and is released as open source (see the software supplement that accompanies this work).\n sent2: This code constitutes a ground-up rewrite of the Java-based post-editing interface of #TARGET_REF , written using a strict model-view-controller software design pattern to be easy for other researchers to use and extend.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "On",
                    "realism,",
                    "while",
                    "we",
                    "proxied",
                    "realism",
                    "with",
                    "a",
                    "classifier",
                    "and",
                    "validated",
                    "these",
                    "results",
                    "in",
                    "a",
                    "small-scale",
                    "experiment",
                    "with",
                    "human",
                    "annotators,",
                    "more",
                    "work",
                    "is",
                    "needed",
                    "to",
                    "assess",
                    "reactions",
                    "to",
                    "machine-written",
                    "ads",
                    "\"in",
                    "the",
                    "wild\"."
                ],
                [
                    "Furthermore,",
                    "while",
                    "fine-tuning",
                    "and",
                    "prompt-engineering",
                    "increased",
                    "realism",
                    "in",
                    "the",
                    "aggregate,",
                    "some",
                    "job",
                    "ads",
                    "were",
                    "still",
                    "nonsensical",
                    "or",
                    "simply",
                    "parroted",
                    "the",
                    "prompt",
                    "text,",
                    "e.g.,",
                    "\"The",
                    "job",
                    "ad",
                    "should",
                    "not",
                    "have",
                    "any",
                    "biases",
                    "in",
                    "it.\"."
                ],
                [
                    "We",
                    "briefly",
                    "assess",
                    "some",
                    "outputs",
                    "qualitatively",
                    "in",
                    "Appendix",
                    "F",
                    "but",
                    "make",
                    "our",
                    "bias",
                    "measure",
                    "generation",
                    "process",
                    "publicly",
                    "available",
                    "to",
                    "encourage",
                    "more",
                    "human-directed",
                    "assessments",
                    "of",
                    "bias",
                    "and",
                    "realism."
                ],
                [
                    "9",
                    "It",
                    "remains",
                    "to",
                    "be",
                    "seen",
                    "whether",
                    "realism",
                    "(as",
                    "measured",
                    "by",
                    "similarity",
                    "to",
                    "human-authored",
                    "ads)",
                    "is",
                    "a",
                    "necessary",
                    "characteristic",
                    "for",
                    "success",
                    "(as",
                    "measured",
                    "by",
                    "the",
                    "number",
                    "of",
                    "applications)."
                ],
                [
                    "Prior",
                    "research",
                    "identifies",
                    "fluency",
                    "and",
                    "a",
                    "clear",
                    "presentation",
                    "of",
                    "relevant",
                    "skills",
                    "and",
                    "experience",
                    "as",
                    "relevant",
                    "to",
                    "the",
                    "creation",
                    "of",
                    "a",
                    "\"good\"",
                    "job",
                    "ad",
                    "#TARGET_REF",
                    ",",
                    "but",
                    "it",
                    "is",
                    "not",
                    "clear",
                    "whether",
                    "an",
                    "ad",
                    "must",
                    "appear",
                    "human-written",
                    "to",
                    "achieve",
                    "this."
                ],
                [
                    "Our",
                    "assumption",
                    "for",
                    "this",
                    "project",
                    "is",
                    "that",
                    "human-written",
                    "job",
                    "ads",
                    "follow",
                    "styles,",
                    "conventions",
                    "and",
                    "a",
                    "level",
                    "of",
                    "detail",
                    "that",
                    "effectively",
                    "encourage",
                    "prospective",
                    "employees",
                    "to",
                    "apply,",
                    "but",
                    "further",
                    "research",
                    "is",
                    "required",
                    "to",
                    "understand",
                    "whether",
                    "ads",
                    "clearly",
                    "identified",
                    "as",
                    "machine-written",
                    "can",
                    "be",
                    "equally",
                    "or",
                    "more",
                    "effective",
                    "in",
                    "this",
                    "regard."
                ]
            ],
            "context": [
                0,
                0,
                0,
                3,
                1,
                0
            ]
        },
        "input": "sent0: On realism, while we proxied realism with a classifier and validated these results in a small-scale experiment with human annotators, more work is needed to assess reactions to machine-written ads \"in the wild\".\n sent1: Furthermore, while fine-tuning and prompt-engineering increased realism in the aggregate, some job ads were still nonsensical or simply parroted the prompt text, e.g., \"The job ad should not have any biases in it.\".\n sent2: We briefly assess some outputs qualitatively in Appendix F but make our bias measure generation process publicly available to encourage more human-directed assessments of bias and realism.\n sent3: 9 It remains to be seen whether realism (as measured by similarity to human-authored ads) is a necessary characteristic for success (as measured by the number of applications).\n sent4: Prior research identifies fluency and a clear presentation of relevant skills and experience as relevant to the creation of a \"good\" job ad #TARGET_REF , but it is not clear whether an ad must appear human-written to achieve this.\n sent5: Our assumption for this project is that human-written job ads follow styles, conventions and a level of detail that effectively encourage prospective employees to apply, but further research is required to understand whether ads clearly identified as machine-written can be equally or more effective in this regard.\n",
        "output": "{\"INFORMATION\": [\"sent4\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Another",
                    "advantage",
                    "is",
                    "that",
                    "it",
                    "provides",
                    "precomputed",
                    "contextual",
                    "word",
                    "representations."
                ],
                [
                    "QA",
                    "models",
                    "based",
                    "on",
                    "LSTMs",
                    "are",
                    "built",
                    "on",
                    "top",
                    "of",
                    "static",
                    "word",
                    "embeddings",
                    "models",
                    "such",
                    "as",
                    "GloVe",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Even",
                    "these",
                    "models",
                    "have",
                    "up",
                    "to",
                    "40",
                    "times",
                    "fewer",
                    "parameters",
                    "than",
                    "a",
                    "BERT-based",
                    "model,",
                    "they",
                    "rely",
                    "on",
                    "LSTM-based",
                    "encoders",
                    "to",
                    "produce",
                    "contextual",
                    "embeddings",
                    "which",
                    "considerably",
                    "lengthens",
                    "the",
                    "time",
                    "required",
                    "for",
                    "training",
                    "and",
                    "makes",
                    "the",
                    "dependence",
                    "on",
                    "supervised",
                    "data",
                    "more",
                    "important."
                ]
            ],
            "context": [
                2,
                3,
                2
            ]
        },
        "input": "sent0: Another advantage is that it provides precomputed contextual word representations.\n sent1: QA models based on LSTMs are built on top of static word embeddings models such as GloVe #TARGET_REF .\n sent2: Even these models have up to 40 times fewer parameters than a BERT-based model, they rely on LSTM-based encoders to produce contextual embeddings which considerably lengthens the time required for training and makes the dependence on supervised data more important.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\", \"sent2\"], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Wiggins",
                    "(",
                    "2013)",
                    "gives",
                    "yet",
                    "another",
                    "perspective",
                    "of",
                    "investment",
                    "and",
                    "MT",
                    "implementation",
                    "in",
                    "Asia",
                    "Online",
                    "3",
                    "business",
                    "environment."
                ],
                [
                    "Some",
                    "of",
                    "the",
                    "advantages",
                    "of",
                    "providing",
                    "MT-incorporated",
                    "solutions",
                    "#TARGET_REF",
                    "list",
                    "are",
                    "reduced",
                    "translation",
                    "costs,",
                    "faster",
                    "delivery",
                    "time,",
                    "expansion",
                    "of",
                    "existing",
                    "relationships",
                    "with",
                    "clients,",
                    "broadening",
                    "offered",
                    "functionality",
                    "and",
                    "opening",
                    "new",
                    "market",
                    "possibilities."
                ]
            ],
            "context": [
                0,
                1
            ]
        },
        "input": "sent0: Wiggins ( 2013) gives yet another perspective of investment and MT implementation in Asia Online 3 business environment.\n sent1: Some of the advantages of providing MT-incorporated solutions #TARGET_REF list are reduced translation costs, faster delivery time, expansion of existing relationships with clients, broadening offered functionality and opening new market possibilities.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Figure",
                    "7",
                    "shows",
                    "the",
                    "overall",
                    "architecture",
                    "and",
                    "training",
                    "pipeline-our",
                    "reinforcement",
                    "learning",
                    "pipeline",
                    "is",
                    "unchanged",
                    "from",
                    "that",
                    "shown",
                    "in",
                    "#TARGET_REF",
                    "with",
                    "the",
                    "exception",
                    "of",
                    "the",
                    "curriculum",
                    "of",
                    "quests",
                    "performed",
                    "by",
                    "the",
                    "agent",
                    "and",
                    "the",
                    "way",
                    "the",
                    "speech",
                    "rewards",
                    "are",
                    "designed."
                ],
                [
                    "An",
                    "encoder",
                    "first",
                    "takes",
                    "in",
                    "information",
                    "about",
                    "setting,",
                    "persona,",
                    "motivation",
                    "for",
                    "a",
                    "single",
                    "character",
                    "then",
                    "passes",
                    "it",
                    "onto",
                    "a",
                    "switch",
                    "module."
                ],
                [
                    "This",
                    "switch",
                    "module",
                    "is",
                    "a",
                    "meta",
                    "policy",
                    "that",
                    "decides",
                    "if",
                    "an",
                    "agent",
                    "should",
                    "act",
                    "or",
                    "talk",
                    "and",
                    "is",
                    "trained",
                    "to",
                    "mimic",
                    "how",
                    "often",
                    "human",
                    "experts",
                    "act",
                    "or",
                    "talk",
                    "while",
                    "performing",
                    "quests",
                    "via",
                    "demonstrations."
                ],
                [
                    "Two",
                    "separate",
                    "policy",
                    "networks",
                    "make",
                    "a",
                    "decision",
                    "on",
                    "which",
                    "action",
                    "to",
                    "perform",
                    "or",
                    "dialogue",
                    "to",
                    "say",
                    "given",
                    "the",
                    "current",
                    "context",
                    "and",
                    "a",
                    "single",
                    "shared",
                    "critic",
                    "attempts",
                    "to",
                    "measure",
                    "the",
                    "value",
                    "of",
                    "taking",
                    "an",
                    "action",
                    "in",
                    "a",
                    "particular",
                    "state."
                ]
            ],
            "context": [
                2,
                3,
                0,
                0
            ]
        },
        "input": "sent0: Figure 7 shows the overall architecture and training pipeline-our reinforcement learning pipeline is unchanged from that shown in #TARGET_REF with the exception of the curriculum of quests performed by the agent and the way the speech rewards are designed.\n sent1: An encoder first takes in information about setting, persona, motivation for a single character then passes it onto a switch module.\n sent2: This switch module is a meta policy that decides if an agent should act or talk and is trained to mimic how often human experts act or talk while performing quests via demonstrations.\n sent3: Two separate policy networks make a decision on which action to perform or dialogue to say given the current context and a single shared critic attempts to measure the value of taking an action in a particular state.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "downside",
                    "of",
                    "datasets",
                    "trained",
                    "on",
                    "Twitter",
                    "is",
                    "that",
                    "they",
                    "are",
                    "likely",
                    "not",
                    "that",
                    "good",
                    "at",
                    "classifying",
                    "anything",
                    "other",
                    "than",
                    "tweets."
                ],
                [
                    "It",
                    "is",
                    "plausible",
                    "that",
                    "datasets",
                    "trained",
                    "on",
                    "less",
                    "specific",
                    "data",
                    "such",
                    "as",
                    "XED",
                    "and",
                    "those",
                    "created",
                    "by",
                    "#TARGET_REF",
                    "and",
                    "#REF",
                    "are",
                    "better",
                    "at",
                    "crossing",
                    "domains",
                    "at",
                    "the",
                    "cost",
                    "of",
                    "evaluation",
                    "metrics."
                ]
            ],
            "context": [
                3,
                1
            ]
        },
        "input": "sent0: The downside of datasets trained on Twitter is that they are likely not that good at classifying anything other than tweets.\n sent1: It is plausible that datasets trained on less specific data such as XED and those created by #TARGET_REF and #REF are better at crossing domains at the cost of evaluation metrics.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Comparing",
                    "this",
                    "relatedness",
                    "measure",
                    "to",
                    "data",
                    "obtained",
                    "from",
                    "humans",
                    "#TARGET_REF",
                    ",",
                    "we",
                    "found",
                    "that",
                    "taking",
                    "the",
                    "square",
                    "root",
                    "of",
                    "PMI",
                    "norm",
                    "increases",
                    "the",
                    "Pearson",
                    "correlation",
                    "coefficient",
                    "between",
                    "human",
                    "annotations",
                    "and",
                    "our",
                    "calculated",
                    "relatedness",
                    "from",
                    "0.72",
                    "to",
                    "0.76",
                    "for",
                    "MEN,",
                    "and",
                    "from",
                    "0.57",
                    "to",
                    "0.63",
                    "for",
                    "WS-353."
                ],
                [
                    "Additionally,",
                    "in",
                    "our",
                    "following",
                    "methods,",
                    "it",
                    "is",
                    "beneficial",
                    "if",
                    "the",
                    "values",
                    "do",
                    "not",
                    "concentrate",
                    "around",
                    "zero,",
                    "therefore",
                    "we",
                    "use",
                    "the",
                    "square",
                    "root",
                    "of",
                    "normalized",
                    "PMI",
                    "hereinafter:",
                    "NPMI(x,",
                    "y)",
                    "=",
                    "PMI",
                    "norm",
                    "(x,",
                    "y)."
                ]
            ],
            "context": [
                2,
                0
            ]
        },
        "input": "sent0: Comparing this relatedness measure to data obtained from humans #TARGET_REF , we found that taking the square root of PMI norm increases the Pearson correlation coefficient between human annotations and our calculated relatedness from 0.72 to 0.76 for MEN, and from 0.57 to 0.63 for WS-353.\n sent1: Additionally, in our following methods, it is beneficial if the values do not concentrate around zero, therefore we use the square root of normalized PMI hereinafter: NPMI(x, y) = PMI norm (x, y).\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "DeKo",
                    "is",
                    "implemented",
                    "as",
                    "a",
                    "series",
                    "of",
                    "finite-state",
                    "transducers,",
                    "using",
                    "the",
                    "FST-suite",
                    "provided",
                    "by",
                    "AT&amp,T",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "A",
                    "more",
                    "detailed",
                    "description",
                    "of",
                    "the",
                    "architecture",
                    "is",
                    "given",
                    "in",
                    "#REF",
                    "and",
                    "examples",
                    "for",
                    "the",
                    "rule",
                    "format",
                    "are",
                    "provided",
                    "in",
                    "#REF",
                    "."
                ],
                [
                    "We",
                    "need",
                    "to",
                    "model",
                    "three",
                    "types",
                    "of",
                    "rules:"
                ]
            ],
            "context": [
                3,
                0,
                0
            ]
        },
        "input": "sent0: DeKo is implemented as a series of finite-state transducers, using the FST-suite provided by AT&amp,T #TARGET_REF .\n sent1: A more detailed description of the architecture is given in #REF and examples for the rule format are provided in #REF .\n sent2: We need to model three types of rules:\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "•",
                    "word",
                    "embeddings",
                    "extracted",
                    "from",
                    "the",
                    "first",
                    "layer",
                    "of",
                    "the",
                    "GPT2",
                    "language",
                    "model",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                3
            ]
        },
        "input": "sent0: • word embeddings extracted from the first layer of the GPT2 language model #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "For",
                    "the",
                    "definition",
                    "generation",
                    "dataset,",
                    "we",
                    "use",
                    "the",
                    "Chinese",
                    "WordNet",
                    "(CWN)",
                    "#TARGET_REF",
                    ",",
                    "which",
                    "is",
                    "a",
                    "semantic",
                    "lexicon",
                    "aiming",
                    "to",
                    "provide",
                    "a",
                    "knowledge",
                    "base",
                    "of",
                    "sense",
                    "distinction."
                ],
                [
                    "2",
                    "We",
                    "use",
                    "the",
                    "corresponding",
                    "words,",
                    "contexts,",
                    "and",
                    "definitions",
                    "in",
                    "CWN",
                    "for",
                    "the",
                    "definition",
                    "generation",
                    "task."
                ],
                [
                    "We",
                    "split",
                    "the",
                    "entire",
                    "dataset",
                    "into",
                    "training,",
                    "validation,",
                    "and",
                    "test",
                    "sets",
                    "roughly",
                    "according",
                    "to",
                    "the",
                    "ratio",
                    "of",
                    "8:1:1."
                ],
                [
                    "For",
                    "the",
                    "simple",
                    "text",
                    "corpus,",
                    "we",
                    "extract",
                    "58,867",
                    "sentences",
                    "from",
                    "a",
                    "number",
                    "of",
                    "primary",
                    "level",
                    "Chinese",
                    "as",
                    "Second",
                    "Language",
                    "textbooks,",
                    "with",
                    "an",
                    "average",
                    "sentence",
                    "length",
                    "of",
                    "14.62."
                ]
            ],
            "context": [
                2,
                0,
                0,
                0
            ]
        },
        "input": "sent0: For the definition generation dataset, we use the Chinese WordNet (CWN) #TARGET_REF , which is a semantic lexicon aiming to provide a knowledge base of sense distinction.\n sent1: 2 We use the corresponding words, contexts, and definitions in CWN for the definition generation task.\n sent2: We split the entire dataset into training, validation, and test sets roughly according to the ratio of 8:1:1.\n sent3: For the simple text corpus, we extract 58,867 sentences from a number of primary level Chinese as Second Language textbooks, with an average sentence length of 14.62.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "SDG",
                    "task",
                    "is",
                    "challenging",
                    "because",
                    "it",
                    "requires",
                    "a",
                    "model",
                    "to",
                    "learn",
                    "from",
                    "a",
                    "standard",
                    "dictionary",
                    "containing",
                    "complex",
                    "definitions",
                    "and",
                    "then",
                    "generate",
                    "simple",
                    "ones,",
                    "and",
                    "hence",
                    "fully",
                    "unsupervised."
                ],
                [
                    "A",
                    "seemingly",
                    "feasible",
                    "solution",
                    "is",
                    "to",
                    "generate",
                    "definitions",
                    "first",
                    "and",
                    "then",
                    "simplify",
                    "them,",
                    "i.e.,",
                    "the",
                    "generationsimplification",
                    "pipeline."
                ],
                [
                    "However,",
                    "the",
                    "simplification",
                    "task",
                    "requires",
                    "dataset",
                    "with",
                    "complex-simple",
                    "sentence",
                    "pairs,",
                    "and",
                    "such",
                    "data",
                    "is",
                    "also",
                    "difficult",
                    "to",
                    "find",
                    "in",
                    "languages",
                    "other",
                    "than",
                    "English",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Besides,",
                    "the",
                    "pipeline",
                    "methods",
                    "do",
                    "not",
                    "perform",
                    "well",
                    "due",
                    "to",
                    "accumulated",
                    "errors",
                    "(Section",
                    "6.1)."
                ]
            ],
            "context": [
                3,
                3,
                2,
                2
            ]
        },
        "input": "sent0: The SDG task is challenging because it requires a model to learn from a standard dictionary containing complex definitions and then generate simple ones, and hence fully unsupervised.\n sent1: A seemingly feasible solution is to generate definitions first and then simplify them, i.e., the generationsimplification pipeline.\n sent2: However, the simplification task requires dataset with complex-simple sentence pairs, and such data is also difficult to find in languages other than English #TARGET_REF .\n sent3: Besides, the pipeline methods do not perform well due to accumulated errors (Section 6.1).\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent2\", \"sent3\"], \"BACKGROUND\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Diversity",
                    "in",
                    "NLG",
                    "has",
                    "been",
                    "extensively",
                    "studied",
                    "for",
                    "various",
                    "tasks",
                    "in",
                    "the",
                    "past",
                    "few",
                    "years,",
                    "such",
                    "as",
                    "machine",
                    "translation",
                    "#TARGET_REF",
                    "and",
                    "paraphrase",
                    "§",
                    "Codes",
                    "of",
                    "our",
                    "model",
                    "and",
                    "baselines",
                    "are",
                    "available",
                    "at",
                    "https://github.com/DM2-ND/MoKGE."
                ]
            ],
            "context": [
                3
            ]
        },
        "input": "sent0: Diversity in NLG has been extensively studied for various tasks in the past few years, such as machine translation #TARGET_REF and paraphrase § Codes of our model and baselines are available at https://github.com/DM2-ND/MoKGE.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "English-Spanish",
                    "datasets",
                    "We",
                    "consider",
                    "the",
                    "Eu-roParl",
                    "(1.7M",
                    "sentences)",
                    "#TARGET_REF",
                    "and",
                    "the",
                    "NewsCommentary-v8",
                    "(174k",
                    "sentences)",
                    "corpora",
                    "for",
                    "pre-training."
                ]
            ],
            "context": [
                2
            ]
        },
        "input": "sent0: English-Spanish datasets We consider the Eu-roParl (1.7M sentences) #TARGET_REF and the NewsCommentary-v8 (174k sentences) corpora for pre-training.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "chose",
                    "to",
                    "use",
                    "prepositions",
                    "as",
                    "relation",
                    "labels,",
                    "despite",
                    "this",
                    "ambiguity."
                ],
                [
                    "This",
                    "follows",
                    "a",
                    "line",
                    "of",
                    "annotation",
                    "work",
                    "that",
                    "aims",
                    "to",
                    "express",
                    "semantic",
                    "relations",
                    "using",
                    "natural",
                    "language",
                    "#TARGET_REF",
                    ",",
                    "as",
                    "opposed",
                    "to",
                    "works",
                    "that",
                    "used",
                    "formal",
                    "linguistic",
                    "terms,",
                    "traditionally",
                    "relying",
                    "on",
                    "expert-defined",
                    "taxonomies",
                    "of",
                    "semantic",
                    "roles",
                    "and",
                    "discourse",
                    "relations."
                ],
                [
                    "The",
                    "aforementioned",
                    "works",
                    "label",
                    "predicateargument",
                    "relations",
                    "using",
                    "restricted",
                    "questions."
                ],
                [
                    "In",
                    "the",
                    "same",
                    "vein,",
                    "we",
                    "label",
                    "nominal",
                    "relations",
                    "using",
                    "prepositions."
                ]
            ],
            "context": [
                2,
                2,
                1,
                2
            ]
        },
        "input": "sent0: We chose to use prepositions as relation labels, despite this ambiguity.\n sent1: This follows a line of annotation work that aims to express semantic relations using natural language #TARGET_REF , as opposed to works that used formal linguistic terms, traditionally relying on expert-defined taxonomies of semantic roles and discourse relations.\n sent2: The aforementioned works label predicateargument relations using restricted questions.\n sent3: In the same vein, we label nominal relations using prepositions.\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [\"sent0\", \"sent1\", \"sent3\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "A",
                    "few",
                    "researchers",
                    "have",
                    "started",
                    "to",
                    "exploit",
                    "suffix",
                    "arrays",
                    "in",
                    "phrase-based",
                    "SMT",
                    "systems."
                ],
                [
                    "#REF",
                    "have",
                    "shown",
                    "how",
                    "parallel",
                    "suffix",
                    "arrays",
                    "can",
                    "be",
                    "used",
                    "to",
                    "efficiently",
                    "compute",
                    "phrase",
                    "translations",
                    "and",
                    "significantly",
                    "reduce",
                    "the",
                    "large",
                    "memory",
                    "footprints",
                    "that",
                    "phrased-based",
                    "SMT",
                    "systems",
                    "suffer",
                    "from",
                    "when",
                    "attempting",
                    "to",
                    "use",
                    "longer",
                    "(i.e.,",
                    "n&gt,3)",
                    "phrases."
                ],
                [
                    "#TARGET_REF",
                    "describe",
                    "a",
                    "dynamic",
                    "programming",
                    "algorithm",
                    "that",
                    "more",
                    "efficiently",
                    "retrieves",
                    "alignments",
                    "for",
                    "a",
                    "set",
                    "of",
                    "phrases",
                    "(such",
                    "as",
                    "all",
                    "substrings",
                    "from",
                    "a",
                    "sentence",
                    "that",
                    "is",
                    "to",
                    "be",
                    "translated),",
                    "which",
                    "outperforms",
                    "direct",
                    "comparison",
                    "binary",
                    "search",
                    "by",
                    "a",
                    "couple",
                    "of",
                    "orders",
                    "of",
                    "magnitude."
                ],
                [
                    "Their",
                    "improvement",
                    "allows",
                    "them",
                    "to",
                    "compute",
                    "phrase",
                    "alignments",
                    "online."
                ]
            ],
            "context": [
                3,
                0,
                1,
                2
            ]
        },
        "input": "sent0: A few researchers have started to exploit suffix arrays in phrase-based SMT systems.\n sent1: #REF have shown how parallel suffix arrays can be used to efficiently compute phrase translations and significantly reduce the large memory footprints that phrased-based SMT systems suffer from when attempting to use longer (i.e., n&gt,3) phrases.\n sent2: #TARGET_REF describe a dynamic programming algorithm that more efficiently retrieves alignments for a set of phrases (such as all substrings from a sentence that is to be translated), which outperforms direct comparison binary search by a couple of orders of magnitude.\n sent3: Their improvement allows them to compute phrase alignments online.\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [\"sent3\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "A",
                    "viable",
                    "approach",
                    "to",
                    "sentiment",
                    "analysis",
                    "of",
                    "newspaper",
                    "headlines",
                    "has",
                    "been",
                    "developed",
                    "by",
                    "using",
                    "linguistic",
                    "techniques",
                    "and",
                    "a",
                    "broad-coverage",
                    "lexicon",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                1
            ]
        },
        "input": "sent0: A viable approach to sentiment analysis of newspaper headlines has been developed by using linguistic techniques and a broad-coverage lexicon #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "need",
                    "to",
                    "establish",
                    "a",
                    "ground",
                    "zero",
                    "on",
                    "what",
                    "the",
                    "IBM",
                    "system",
                    "is:",
                    "their",
                    "rhetorical",
                    "claim",
                    "is",
                    "(or",
                    "perhaps",
                    "was)",
                    "that",
                    "they",
                    "are",
                    "a",
                    "pure",
                    "statistical",
                    "system,",
                    "different",
                    "from",
                    "their",
                    "competitors,",
                    "glorying",
                    "in",
                    "the",
                    "fact",
                    "that",
                    "they",
                    "did",
                    "not",
                    "even",
                    "need",
                    "French",
                    "speakers."
                ],
                [
                    "By",
                    "analogy",
                    "with",
                    "Searle's",
                    "Chinese",
                    "Room,",
                    "one",
                    "could",
                    "call",
                    "this",
                    "theirs",
                    "a",
                    "French",
                    "Room",
                    "position:",
                    "MT",
                    "without",
                    "a",
                    "glimmering",
                    "of",
                    "understanding",
                    "or",
                    "even",
                    "knowing",
                    "that",
                    "French",
                    "was",
                    "the",
                    "language",
                    "they",
                    "were",
                    "working",
                    "on!"
                ],
                [
                    "There",
                    "is",
                    "no",
                    "space",
                    "here",
                    "for",
                    "a",
                    "detailed",
                    "description",
                    "of",
                    "IBM's",
                    "claims",
                    "(see",
                    "#REF",
                    "."
                ],
                [
                    "In",
                    "essence,",
                    "the",
                    "method",
                    "is",
                    "an",
                    "adaptation",
                    "of",
                    "one",
                    "that",
                    "worked",
                    "well",
                    "for",
                    "speech",
                    "decoding",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                2,
                2,
                2,
                2
            ]
        },
        "input": "sent0: We need to establish a ground zero on what the IBM system is: their rhetorical claim is (or perhaps was) that they are a pure statistical system, different from their competitors, glorying in the fact that they did not even need French speakers.\n sent1: By analogy with Searle's Chinese Room, one could call this theirs a French Room position: MT without a glimmering of understanding or even knowing that French was the language they were working on!\n sent2: There is no space here for a detailed description of IBM's claims (see #REF .\n sent3: In essence, the method is an adaptation of one that worked well for speech decoding #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\", \"sent1\", \"sent2\", \"sent3\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "A",
                    "set",
                    "of",
                    "30",
                    "high-level",
                    "instructions",
                    "specific",
                    "to",
                    "semantic",
                    "network",
                    "processing",
                    "are",
                    "implemented",
                    "directly",
                    "in",
                    "hardware."
                ],
                [
                    "These",
                    "include",
                    "associative",
                    "search,",
                    "marker",
                    "setting",
                    "and",
                    "propagation,",
                    "logical/arithmetic",
                    "operations",
                    "involving",
                    "markers,",
                    "create",
                    "and",
                    "delete",
                    "nodes",
                    "and",
                    "relations,",
                    "and",
                    "collect",
                    "a",
                    "list",
                    "of",
                    "nodes",
                    "with",
                    "a",
                    "certain",
                    "marker",
                    "set."
                ],
                [
                    "Currently,",
                    "the",
                    "instruction",
                    "set",
                    "can",
                    "be",
                    "called",
                    "from",
                    "C",
                    "language",
                    "so",
                    "that",
                    "users",
                    "can",
                    "develop",
                    "applications",
                    "with",
                    "an",
                    "extended",
                    "version",
                    "of",
                    "C",
                    "language."
                ],
                [
                    "From",
                    "the",
                    "programming",
                    "level,",
                    "SNAP",
                    "provides",
                    "data-parallel",
                    "programming",
                    "environment",
                    "similar",
                    "to",
                    "C*",
                    "of",
                    "the",
                    "Connection",
                    "Machine",
                    "#TARGET_REF",
                    "],",
                    "but",
                    "specialized",
                    "for",
                    "semantic",
                    "network",
                    "processing",
                    "with",
                    "marker",
                    "passing."
                ]
            ],
            "context": [
                0,
                0,
                0,
                2
            ]
        },
        "input": "sent0: A set of 30 high-level instructions specific to semantic network processing are implemented directly in hardware.\n sent1: These include associative search, marker setting and propagation, logical/arithmetic operations involving markers, create and delete nodes and relations, and collect a list of nodes with a certain marker set.\n sent2: Currently, the instruction set can be called from C language so that users can develop applications with an extended version of C language.\n sent3: From the programming level, SNAP provides data-parallel programming environment similar to C* of the Connection Machine #TARGET_REF ], but specialized for semantic network processing with marker passing.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent3\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Data",
                    "acquisition",
                    "in",
                    "DeKo",
                    "was",
                    "done",
                    "on",
                    "the",
                    "basis",
                    "of",
                    "a",
                    "corpus:",
                    "we",
                    "used",
                    "German",
                    "newspaper",
                    "corpora,",
                    "which",
                    "were",
                    "tagged",
                    "with",
                    "the",
                    "TreeTagger",
                    "#TARGET_REF",
                    ")",
                    "and",
                    "lemmatized",
                    "with",
                    "DMOR",
                    "#REF",
                    ",",
                    "for",
                    "searching",
                    "and",
                    "pre-processing",
                    "we",
                    "used",
                    "the",
                    "Corpus",
                    "Query",
                    "Processor",
                    "#REF",
                    ")",
                    "and",
                    "a",
                    "number",
                    "of",
                    "Perl",
                    "scripts."
                ],
                [
                    "In",
                    "acquiring",
                    "and",
                    "systematizing",
                    "the",
                    "data",
                    "we",
                    "made",
                    "a",
                    "distinction",
                    "between",
                    "word",
                    "formation",
                    "involving",
                    "selecting",
                    "elements",
                    "(roughly",
                    "derivation)",
                    "and",
                    "word",
                    "formation",
                    "involving",
                    "only",
                    "categories",
                    "(compounding)."
                ],
                [
                    "For",
                    "expository",
                    "purposes",
                    "we",
                    "concentrate",
                    "on",
                    "a",
                    "derivation",
                    "process",
                    "here",
                    "and",
                    "only",
                    "briefly",
                    "describe",
                    "a",
                    "compounding",
                    "process",
                    "below."
                ]
            ],
            "context": [
                2,
                0,
                0
            ]
        },
        "input": "sent0: Data acquisition in DeKo was done on the basis of a corpus: we used German newspaper corpora, which were tagged with the TreeTagger #TARGET_REF ) and lemmatized with DMOR #REF , for searching and pre-processing we used the Corpus Query Processor #REF ) and a number of Perl scripts.\n sent1: In acquiring and systematizing the data we made a distinction between word formation involving selecting elements (roughly derivation) and word formation involving only categories (compounding).\n sent2: For expository purposes we concentrate on a derivation process here and only briefly describe a compounding process below.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "generator",
                    "is",
                    "based",
                    "on",
                    "a",
                    "Relational",
                    "Memory",
                    "with",
                    "self-attention",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "This",
                    "model",
                    "updates",
                    "its",
                    "\"internal",
                    "values\"",
                    "and",
                    "produces",
                    "its",
                    "final",
                    "output",
                    "by",
                    "selecting",
                    "from",
                    "its",
                    "memory",
                    "cells",
                    "with",
                    "a",
                    "self-attention",
                    "mechanism."
                ],
                [
                    "Leveraging",
                    "an",
                    "idea",
                    "similar",
                    "to",
                    "that",
                    "of",
                    "image-based",
                    "conditional",
                    "GANs",
                    "#REF",
                    ",",
                    "we",
                    "introduce",
                    "an",
                    "external",
                    "conditioning",
                    "into",
                    "the",
                    "generator."
                ],
                [
                    "First,",
                    "given",
                    "the",
                    "conditioning",
                    "input",
                    "c",
                    "2",
                    "R",
                    "d",
                    ",",
                    "the",
                    "model",
                    "computes",
                    "an",
                    "embedding",
                    "t",
                    "for",
                    "c",
                    "using",
                    "functionf",
                    "✓",
                    ":",
                    "R",
                    "d",
                    "!"
                ],
                [
                    "R",
                    "m",
                    ",",
                    "with",
                    "m",
                    "&lt,",
                    "d.Function",
                    "f",
                    "✓",
                    "has",
                    "been",
                    "implemented",
                    "using",
                    "a",
                    "feed-forward",
                    "neural",
                    "network",
                    "with",
                    "a",
                    "self-attention",
                    "layer."
                ],
                [
                    "The",
                    "conditioning",
                    "vector",
                    "c",
                    "may",
                    "originate",
                    "from",
                    "any",
                    "type",
                    "of",
                    "different",
                    "source",
                    "as",
                    "long",
                    "as",
                    "it",
                    "remains",
                    "consistent",
                    "during",
                    "the",
                    "individual",
                    "training."
                ],
                [
                    "Depending",
                    "on",
                    "the",
                    "required",
                    "task,",
                    "as",
                    "shown",
                    "in",
                    "the",
                    "experiment",
                    "phase,",
                    "it",
                    "will",
                    "change."
                ],
                [
                    "This",
                    "vector",
                    "c",
                    "is",
                    "the",
                    "only",
                    "link",
                    "between",
                    "the",
                    "conditioning",
                    "and",
                    "the",
                    "generative",
                    "model,",
                    "its",
                    "influence",
                    "on",
                    "the",
                    "final",
                    "output",
                    "will",
                    "be",
                    "crucial",
                    "for",
                    "the",
                    "conditioning",
                    "of",
                    "the",
                    "generated",
                    "sentence."
                ],
                [
                    "f",
                    "✓",
                    "has",
                    "been",
                    "adopted",
                    "to",
                    "give",
                    "the",
                    "model",
                    "the",
                    "ability",
                    "to",
                    "learn",
                    "the",
                    "best",
                    "manipulation",
                    "of",
                    "the",
                    "conditioning",
                    "vector",
                    "to",
                    "insert",
                    "into",
                    "the",
                    "memory."
                ]
            ],
            "context": [
                1,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: The generator is based on a Relational Memory with self-attention #TARGET_REF .\n sent1: This model updates its \"internal values\" and produces its final output by selecting from its memory cells with a self-attention mechanism.\n sent2: Leveraging an idea similar to that of image-based conditional GANs #REF , we introduce an external conditioning into the generator.\n sent3: First, given the conditioning input c 2 R d , the model computes an embedding t for c using functionf ✓ : R d !\n sent4: R m , with m &lt, d.Function f ✓ has been implemented using a feed-forward neural network with a self-attention layer.\n sent5: The conditioning vector c may originate from any type of different source as long as it remains consistent during the individual training.\n sent6: Depending on the required task, as shown in the experiment phase, it will change.\n sent7: This vector c is the only link between the conditioning and the generative model, its influence on the final output will be crucial for the conditioning of the generated sentence.\n sent8: f ✓ has been adopted to give the model the ability to learn the best manipulation of the conditioning vector to insert into the memory.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "combination",
                    "of",
                    "lexical",
                    "knowledge",
                    "and",
                    "rule",
                    "knowledge",
                    "enables",
                    "WM",
                    "to",
                    "function",
                    "as",
                    "a",
                    "full",
                    "morphological",
                    "component."
                ],
                [
                    "As",
                    "shown",
                    "by",
                    "ten",
                    "Hacken",
                    "(1998),",
                    "the",
                    "effects",
                    "of",
                    "this",
                    "coverage",
                    "are",
                    "particularly",
                    "striking",
                    "in",
                    "the",
                    "domain",
                    "of",
                    "word",
                    "formation,",
                    "for",
                    "which",
                    "a",
                    "system",
                    "taking",
                    "a",
                    "lexicon",
                    "as",
                    "modelled",
                    "in",
                    "Fig."
                ],
                [
                    "2B",
                    "as",
                    "a",
                    "basis",
                    "lacks",
                    "the",
                    "procedural",
                    "component."
                ],
                [
                    "Thus",
                    "a",
                    "formalism",
                    "such",
                    "as",
                    "DATR,",
                    "as",
                    "described",
                    "by",
                    "#TARGET_REF",
                    ",",
                    "though",
                    "able",
                    "to",
                    "represent",
                    "word",
                    "formation",
                    "relationships,",
                    "cannot",
                    "deal",
                    "with",
                    "unseen",
                    "words",
                    "without",
                    "a",
                    "separate",
                    "recognition",
                    "module."
                ],
                [
                    "In",
                    "WM,",
                    "word",
                    "formation",
                    "rules",
                    "are",
                    "at",
                    "the",
                    "same",
                    "time",
                    "available",
                    "declaratively,",
                    "as",
                    "the",
                    "structural",
                    "backbone",
                    "of",
                    "the",
                    "database,",
                    "and",
                    "procedurally",
                    "for",
                    "the",
                    "recognition",
                    "of",
                    "new",
                    "words."
                ]
            ],
            "context": [
                3,
                2,
                2,
                2,
                0
            ]
        },
        "input": "sent0: The combination of lexical knowledge and rule knowledge enables WM to function as a full morphological component.\n sent1: As shown by ten Hacken (1998), the effects of this coverage are particularly striking in the domain of word formation, for which a system taking a lexicon as modelled in Fig.\n sent2: 2B as a basis lacks the procedural component.\n sent3: Thus a formalism such as DATR, as described by #TARGET_REF , though able to represent word formation relationships, cannot deal with unseen words without a separate recognition module.\n sent4: In WM, word formation rules are at the same time available declaratively, as the structural backbone of the database, and procedurally for the recognition of new words.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\", \"sent2\", \"sent3\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "While",
                    "manual,",
                    "human",
                    "evaluation",
                    "of",
                    "machine",
                    "translation",
                    "(MT)",
                    "systems",
                    "is",
                    "still",
                    "the",
                    "gold",
                    "standard,",
                    "automatic",
                    "evaluation",
                    "metrics",
                    "have",
                    "long",
                    "been",
                    "used",
                    "for",
                    "their",
                    "relative",
                    "speed",
                    "and",
                    "inexpensiveness."
                ],
                [
                    "Early",
                    "automatic",
                    "metrics",
                    "were",
                    "easy",
                    "to",
                    "implement",
                    "and",
                    "somewhat",
                    "correlated",
                    "with",
                    "human",
                    "judgements,",
                    "but",
                    "have",
                    "clear",
                    "limitations:",
                    "BLEU",
                    "#TARGET_REF",
                    "relies",
                    "on",
                    "n-gram",
                    "overlap,",
                    "and",
                    "is",
                    "thus",
                    "not",
                    "robust",
                    "to",
                    "differing",
                    "word",
                    "order",
                    "or",
                    "choice."
                ],
                [
                    "In",
                    "contrast,",
                    "ME-TEOR",
                    "#REF",
                    "requires",
                    "training,",
                    "but",
                    "depends",
                    "on",
                    "token",
                    "alignment,",
                    "which",
                    "is",
                    "also",
                    "a",
                    "fraught",
                    "task."
                ]
            ],
            "context": [
                0,
                3,
                0
            ]
        },
        "input": "sent0: While manual, human evaluation of machine translation (MT) systems is still the gold standard, automatic evaluation metrics have long been used for their relative speed and inexpensiveness.\n sent1: Early automatic metrics were easy to implement and somewhat correlated with human judgements, but have clear limitations: BLEU #TARGET_REF relies on n-gram overlap, and is thus not robust to differing word order or choice.\n sent2: In contrast, ME-TEOR #REF requires training, but depends on token alignment, which is also a fraught task.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "CoNLL",
                    "bart",
                    ":",
                    "we",
                    "use",
                    "a",
                    "fine-tuned",
                    "BART",
                    "model",
                    "#TARGET_REF"
                ]
            ],
            "context": [
                2
            ]
        },
        "input": "sent0: CoNLL bart : we use a fine-tuned BART model #TARGET_REF\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "First,",
                    "we",
                    "compare",
                    "the",
                    "performance",
                    "of",
                    "UNI-FIEDQA",
                    "#TARGET_REF",
                    "before",
                    "and",
                    "after",
                    "replacing",
                    "its",
                    "feed-forward",
                    "layers",
                    "with",
                    "our",
                    "implementation",
                    "of",
                    "top-k",
                    "attention",
                    "and",
                    "directly",
                    "performing",
                    "inference",
                    "on",
                    "12",
                    "different",
                    "question",
                    "answering",
                    "(QA)",
                    "datasets",
                    "without",
                    "any",
                    "training."
                ],
                [
                    "UNIFIEDQA",
                    "is",
                    "a",
                    "T5-based",
                    "#REF",
                    "model",
                    "with",
                    "11B",
                    "parameters",
                    "#REF",
                    ",",
                    "fine-tuned",
                    "on",
                    "a",
                    "weighted",
                    "mixture",
                    "of",
                    "QA",
                    "datasets."
                ],
                [
                    "The",
                    "12",
                    "datasets",
                    "include",
                    "diverse",
                    "domains,",
                    "such",
                    "as",
                    "science",
                    "questions,",
                    "factoid",
                    "questions",
                    "over",
                    "Wikipedia,",
                    "commonsense",
                    "questions,",
                    "etc."
                ],
                [
                    "Details",
                    "regarding",
                    "the",
                    "datasets",
                    "and",
                    "metrics",
                    "can",
                    "be",
                    "found",
                    "in",
                    "§A.2."
                ]
            ],
            "context": [
                2,
                1,
                3,
                0
            ]
        },
        "input": "sent0: First, we compare the performance of UNI-FIEDQA #TARGET_REF before and after replacing its feed-forward layers with our implementation of top-k attention and directly performing inference on 12 different question answering (QA) datasets without any training.\n sent1: UNIFIEDQA is a T5-based #REF model with 11B parameters #REF , fine-tuned on a weighted mixture of QA datasets.\n sent2: The 12 datasets include diverse domains, such as science questions, factoid questions over Wikipedia, commonsense questions, etc.\n sent3: Details regarding the datasets and metrics can be found in §A.2.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Much",
                    "work",
                    "in",
                    "NLP",
                    "addresses",
                    "the",
                    "recovery",
                    "of",
                    "such",
                    "verb-mediated",
                    "relations",
                    "(SRL)",
                    "#REF",
                    ",",
                    "either",
                    "using",
                    "pre-specified",
                    "role",
                    "ontologies",
                    "such",
                    "as",
                    "PropBank",
                    "or",
                    "FrameNet",
                    "#REF",
                    ",",
                    "or,",
                    "more",
                    "recently,",
                    "using",
                    "natural-languagebased",
                    "representations",
                    "(QA-SRL)",
                    "#REF",
                    "."
                ],
                [
                    "Another",
                    "well-studied",
                    "kind",
                    "of",
                    "semantic",
                    "relations",
                    "between",
                    "NPs",
                    "is",
                    "that",
                    "of",
                    "coreference",
                    "#TARGET_REF",
                    ",",
                    "where",
                    "two",
                    "(or",
                    "more)",
                    "NPs",
                    "refer",
                    "to",
                    "the",
                    "same",
                    "entity."
                ]
            ],
            "context": [
                0,
                1
            ]
        },
        "input": "sent0: Much work in NLP addresses the recovery of such verb-mediated relations (SRL) #REF , either using pre-specified role ontologies such as PropBank or FrameNet #REF , or, more recently, using natural-languagebased representations (QA-SRL) #REF .\n sent1: Another well-studied kind of semantic relations between NPs is that of coreference #TARGET_REF , where two (or more) NPs refer to the same entity.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "1."
                ],
                [
                    "The",
                    "TempEval-3",
                    "TimeBank",
                    "dataset",
                    "was",
                    "used",
                    "for",
                    "English",
                    "#REF",
                    "."
                ],
                [
                    "The",
                    "corpus",
                    "consists",
                    "of",
                    "61,418",
                    "tokens",
                    "for",
                    "training",
                    "and",
                    "6,756",
                    "event",
                    "mentions."
                ],
                [
                    "3."
                ],
                [
                    "For",
                    "Italian,",
                    "we",
                    "use",
                    "Ita-TimeBank's",
                    "ILC",
                    "corpus",
                    "#TARGET_REF",
                    "the",
                    "Italian",
                    "corpus",
                    "annotated",
                    "using",
                    "ISO-TimeML",
                    "rules",
                    "for",
                    "events",
                    "and",
                    "temporal",
                    "information."
                ],
                [
                    "The",
                    "corpus",
                    "consists",
                    "of",
                    "68,000",
                    "tokens",
                    "and",
                    "10,591",
                    "event",
                    "mentions."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                3,
                3
            ]
        },
        "input": "sent0: 1.\n sent1: The TempEval-3 TimeBank dataset was used for English #REF .\n sent2: The corpus consists of 61,418 tokens for training and 6,756 event mentions.\n sent3: 3.\n sent4: For Italian, we use Ita-TimeBank's ILC corpus #TARGET_REF the Italian corpus annotated using ISO-TimeML rules for events and temporal information.\n sent5: The corpus consists of 68,000 tokens and 10,591 event mentions.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent4\", \"sent5\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Previous",
                    "studies",
                    "on",
                    "symbol-description",
                    "extraction",
                    "rely",
                    "on",
                    "pattern",
                    "matching",
                    "#TARGET_REF",
                    "and",
                    "rule-based",
                    "algorithms",
                    "#REF",
                    "."
                ],
                [
                    "These",
                    "methods",
                    "might",
                    "work",
                    "for",
                    "observed",
                    "patterns",
                    "with",
                    "an",
                    "assumption",
                    "of",
                    "close",
                    "proximity",
                    "between",
                    "symbol",
                    "and",
                    "description."
                ],
                [
                    "They",
                    "may",
                    "fail",
                    "to",
                    "capture",
                    "distant",
                    "symboldescription",
                    "pairs",
                    "and",
                    "symbols",
                    "in",
                    "very",
                    "complex",
                    "structures",
                    "such",
                    "as",
                    "algorithms",
                    "in",
                    "computer",
                    "science",
                    "literature."
                ]
            ],
            "context": [
                3,
                2,
                2
            ]
        },
        "input": "sent0: Previous studies on symbol-description extraction rely on pattern matching #TARGET_REF and rule-based algorithms #REF .\n sent1: These methods might work for observed patterns with an assumption of close proximity between symbol and description.\n sent2: They may fail to capture distant symboldescription pairs and symbols in very complex structures such as algorithms in computer science literature.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\", \"sent2\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "language)",
                    "and",
                    "Basque",
                    "(agglutinative",
                    "language)",
                    "tokens,",
                    "the",
                    "aligner",
                    "was",
                    "fed",
                    "with",
                    "segmented",
                    "words",
                    "for",
                    "the",
                    "agglutinative",
                    "language."
                ],
                [
                    "Several",
                    "segmentation",
                    "options",
                    "exist:",
                    "we",
                    "can",
                    "isolate",
                    "each",
                    "morpheme,",
                    "or",
                    "break",
                    "each",
                    "word",
                    "into",
                    "lemma",
                    "and",
                    "a",
                    "bag",
                    "of",
                    "suffixes,",
                    "we",
                    "can",
                    "establish",
                    "hand-written",
                    "rules",
                    "for",
                    "segmentation,",
                    "or",
                    "let",
                    "an",
                    "automatic",
                    "tool",
                    "define",
                    "and",
                    "process",
                    "the",
                    "words",
                    "unsupervised."
                ],
                [
                    "Based",
                    "on",
                    "the",
                    "results",
                    "from",
                    "Labaka",
                    "(2010),",
                    "we",
                    "opted",
                    "for",
                    "the",
                    "second",
                    "option",
                    "and",
                    "joined",
                    "together",
                    "all",
                    "the",
                    "suffixes",
                    "attached",
                    "to",
                    "a",
                    "particular",
                    "lemma",
                    "in",
                    "one",
                    "separate",
                    "token."
                ],
                [
                    "Thus,",
                    "on",
                    "splitting",
                    "a",
                    "word,",
                    "we",
                    "generated,",
                    "at",
                    "most,",
                    "three",
                    "tokens",
                    "(prefixes,",
                    "lemma",
                    "and",
                    "suffixes)."
                ],
                [
                    "Moses",
                    "was",
                    "trained",
                    "and",
                    "optimized",
                    "on",
                    "segmented",
                    "text."
                ],
                [
                    "Note",
                    "that",
                    "when",
                    "using",
                    "segmented",
                    "text",
                    "for",
                    "training,",
                    "the",
                    "output",
                    "of",
                    "the",
                    "system",
                    "is",
                    "also",
                    "segmented",
                    "text."
                ],
                [
                    "Real",
                    "words",
                    "are",
                    "not",
                    "available",
                    "to",
                    "the",
                    "statistical",
                    "decoder."
                ],
                [
                    "This",
                    "means",
                    "that",
                    "a",
                    "generation",
                    "postprocess",
                    "(unsegmentation",
                    "step)",
                    "is",
                    "needed",
                    "to",
                    "obtain",
                    "real",
                    "word",
                    "forms."
                ],
                [
                    "We",
                    "incorporated",
                    "a",
                    "second",
                    "language",
                    "model",
                    "(LM)",
                    "based",
                    "on",
                    "real",
                    "word",
                    "forms",
                    "to",
                    "be",
                    "used",
                    "after",
                    "the",
                    "morphological",
                    "postprocess."
                ],
                [
                    "We",
                    "implemented",
                    "the",
                    "word",
                    "form-based",
                    "LM",
                    "by",
                    "using",
                    "an",
                    "n-best",
                    "list,",
                    "as",
                    "was",
                    "done",
                    "in",
                    "#REF",
                    "."
                ],
                [
                    "We",
                    "first",
                    "asked",
                    "Moses",
                    "to",
                    "generate",
                    "a",
                    "translation",
                    "candidate",
                    "ranking",
                    "based",
                    "on",
                    "the",
                    "segmented",
                    "training",
                    "explained",
                    "above."
                ],
                [
                    "Next,",
                    "these",
                    "candidates",
                    "were",
                    "postprocessed."
                ],
                [
                    "We",
                    "then",
                    "recalculated",
                    "the",
                    "total",
                    "cost",
                    "of",
                    "each",
                    "candidate",
                    "by",
                    "including",
                    "the",
                    "cost",
                    "assigned",
                    "by",
                    "the",
                    "new",
                    "word",
                    "form-based",
                    "LM",
                    "in",
                    "the",
                    "models",
                    "used",
                    "during",
                    "decoding."
                ],
                [
                    "Finally,",
                    "the",
                    "candidate",
                    "list",
                    "was",
                    "re-ranked",
                    "according",
                    "to",
                    "this",
                    "new",
                    "total",
                    "cost."
                ],
                [
                    "This",
                    "somehow",
                    "revises",
                    "the",
                    "candidate",
                    "list",
                    "to",
                    "promote",
                    "the",
                    "ones",
                    "that",
                    "are",
                    "more",
                    "likely",
                    "to",
                    "be",
                    "real",
                    "word-form",
                    "sequences."
                ],
                [
                    "The",
                    "weight",
                    "for",
                    "the",
                    "word",
                    "formbased",
                    "LM",
                    "was",
                    "optimized",
                    "at",
                    "Minimum",
                    "Error",
                    "Rate",
                    "Training",
                    "#TARGET_REF",
                    "together",
                    "with",
                    "the",
                    "weights",
                    "for",
                    "the",
                    "rest",
                    "of",
                    "the",
                    "models."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2
            ]
        },
        "input": "sent0: language) and Basque (agglutinative language) tokens, the aligner was fed with segmented words for the agglutinative language.\n sent1: Several segmentation options exist: we can isolate each morpheme, or break each word into lemma and a bag of suffixes, we can establish hand-written rules for segmentation, or let an automatic tool define and process the words unsupervised.\n sent2: Based on the results from Labaka (2010), we opted for the second option and joined together all the suffixes attached to a particular lemma in one separate token.\n sent3: Thus, on splitting a word, we generated, at most, three tokens (prefixes, lemma and suffixes).\n sent4: Moses was trained and optimized on segmented text.\n sent5: Note that when using segmented text for training, the output of the system is also segmented text.\n sent6: Real words are not available to the statistical decoder.\n sent7: This means that a generation postprocess (unsegmentation step) is needed to obtain real word forms.\n sent8: We incorporated a second language model (LM) based on real word forms to be used after the morphological postprocess.\n sent9: We implemented the word form-based LM by using an n-best list, as was done in #REF .\n sent10: We first asked Moses to generate a translation candidate ranking based on the segmented training explained above.\n sent11: Next, these candidates were postprocessed.\n sent12: We then recalculated the total cost of each candidate by including the cost assigned by the new word form-based LM in the models used during decoding.\n sent13: Finally, the candidate list was re-ranked according to this new total cost.\n sent14: This somehow revises the candidate list to promote the ones that are more likely to be real word-form sequences.\n sent15: The weight for the word formbased LM was optimized at Minimum Error Rate Training #TARGET_REF together with the weights for the rest of the models.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent15\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "TNE",
                    "task",
                    "we",
                    "define",
                    "side-steps",
                    "all",
                    "the",
                    "above",
                    "issues."
                ],
                [
                    "It",
                    "is",
                    "based",
                    "on",
                    "the",
                    "text",
                    "alone,",
                    "without",
                    "revealing",
                    "additional",
                    "information",
                    "not",
                    "present",
                    "in",
                    "the",
                    "text."
                ],
                [
                    "The",
                    "exhaustive",
                    "nature",
                    "of",
                    "the",
                    "task",
                    "entails",
                    "looking",
                    "both",
                    "at",
                    "positive",
                    "instances",
                    "(where",
                    "a",
                    "relation",
                    "exists)",
                    "and",
                    "negative",
                    "ones",
                    "(where",
                    "it",
                    "doesn't),",
                    "making",
                    "it",
                    "harder",
                    "for",
                    "models",
                    "to",
                    "pick",
                    "up",
                    "shallow",
                    "heuristics."
                ],
                [
                    "We",
                    "don't",
                    "reveal",
                    "information",
                    "to",
                    "a",
                    "model,",
                    "beyond",
                    "the",
                    "information",
                    "that",
                    "the",
                    "two",
                    "NPs",
                    "appear",
                    "in",
                    "the",
                    "same",
                    "text."
                ],
                [
                    "Finally,",
                    "the",
                    "list",
                    "of",
                    "NPs",
                    "to",
                    "be",
                    "considered",
                    "is",
                    "pre-specified,",
                    "isolating",
                    "the",
                    "problem",
                    "of",
                    "understanding",
                    "the",
                    "relations",
                    "between",
                    "NPs",
                    "in",
                    "the",
                    "text",
                    "from",
                    "the",
                    "much",
                    "easier",
                    "yet",
                    "intervening",
                    "problem",
                    "of",
                    "identifying",
                    "NPs",
                    "and",
                    "agreeing",
                    "on",
                    "their",
                    "exact",
                    "spans."
                ],
                [
                    "#TARGET_REF",
                    "Table",
                    "1:",
                    "Prepositions",
                    "used",
                    "in",
                    "TNE."
                ],
                [
                    "Thus,",
                    "we",
                    "consider",
                    "TNE",
                    "a",
                    "less",
                    "biased",
                    "and",
                    "less",
                    "gameable",
                    "measure",
                    "of",
                    "RC",
                    "than",
                    "QA-based",
                    "benchmarks."
                ],
                [
                    "Of",
                    "course,",
                    "the",
                    "information",
                    "captured",
                    "by",
                    "TNE",
                    "is",
                    "limited",
                    "and",
                    "does",
                    "not",
                    "cover",
                    "all",
                    "levels",
                    "of",
                    "text",
                    "understanding."
                ],
                [
                    "Yet,",
                    "performing",
                    "the",
                    "task",
                    "correctly",
                    "entails",
                    "a",
                    "non-trivial",
                    "comprehension",
                    "of",
                    "texts,",
                    "which",
                    "human",
                    "readers",
                    "do",
                    "as",
                    "a",
                    "byproduct",
                    "of",
                    "reading."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                2,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: The TNE task we define side-steps all the above issues.\n sent1: It is based on the text alone, without revealing additional information not present in the text.\n sent2: The exhaustive nature of the task entails looking both at positive instances (where a relation exists) and negative ones (where it doesn't), making it harder for models to pick up shallow heuristics.\n sent3: We don't reveal information to a model, beyond the information that the two NPs appear in the same text.\n sent4: Finally, the list of NPs to be considered is pre-specified, isolating the problem of understanding the relations between NPs in the text from the much easier yet intervening problem of identifying NPs and agreeing on their exact spans.\n sent5: #TARGET_REF Table 1: Prepositions used in TNE.\n sent6: Thus, we consider TNE a less biased and less gameable measure of RC than QA-based benchmarks.\n sent7: Of course, the information captured by TNE is limited and does not cover all levels of text understanding.\n sent8: Yet, performing the task correctly entails a non-trivial comprehension of texts, which human readers do as a byproduct of reading.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent4\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Totale",
                    "toolkit",
                    "#REF",
                    "was",
                    "used",
                    "to",
                    "POS",
                    "tag",
                    "#TARGET_REF",
                    "and",
                    "lemmatize",
                    "#REF",
                    "words",
                    "in",
                    "the",
                    "bilingual",
                    "word",
                    "list,",
                    "POS",
                    "tagger",
                    "was",
                    "also",
                    "used",
                    "in",
                    "automatic",
                    "paradigm",
                    "classifying,",
                    "see",
                    "chapter",
                    "3.3.1",
                    "for",
                    "further",
                    "description."
                ]
            ],
            "context": [
                3
            ]
        },
        "input": "sent0: Totale toolkit #REF was used to POS tag #TARGET_REF and lemmatize #REF words in the bilingual word list, POS tagger was also used in automatic paradigm classifying, see chapter 3.3.1 for further description.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "There",
                    "are",
                    "two",
                    "ways",
                    "to",
                    "handle",
                    "sentences",
                    "with",
                    "unbounded",
                    "dependency."
                ],
                [
                    "The",
                    "first",
                    "approach",
                    "is",
                    "straightforward",
                    "memorybased",
                    "approach",
                    "which",
                    "simply",
                    "store",
                    "a",
                    "set",
                    "of",
                    "CSCs",
                    "involves",
                    "unbounded",
                    "dependency."
                ],
                [
                    "A",
                    "large",
                    "set",
                    "of",
                    "CSCs",
                    "would",
                    "have",
                    "to",
                    "be",
                    "prepared",
                    "for",
                    "this,",
                    "but",
                    "its",
                    "simplicity",
                    "minimized",
                    "computational",
                    "requirements."
                ],
                [
                    "Alternatively,",
                    "we",
                    "can",
                    "employ",
                    "somewhat",
                    "linguistic",
                    "treatment",
                    "of",
                    "this",
                    "phenomena",
                    "within",
                    "our",
                    "framework."
                ],
                [
                    "The",
                    "syntactic",
                    "constraint",
                    "network",
                    "has",
                    "a",
                    "node",
                    "representing",
                    "TOPIC",
                    "and",
                    "FOCUS",
                    "which",
                    "usually",
                    "bound",
                    "to",
                    "the",
                    "displaced",
                    "phrase."
                ],
                [
                    "An",
                    "address",
                    "of",
                    "CI",
                    "for",
                    "the",
                    "displaced",
                    "phrase",
                    "(such",
                    "as",
                    "'the",
                    "bug'",
                    "in",
                    "the",
                    "example",
                    "s9)",
                    "is",
                    "propagated",
                    "to",
                    "the",
                    "TOPIC",
                    "or",
                    "FOCUS",
                    "nodes",
                    "in",
                    "the",
                    "syntactic",
                    "constraint",
                    "network."
                ],
                [
                    "Further",
                    "propagation",
                    "of",
                    "the",
                    "address",
                    "of",
                    "the",
                    "CI",
                    "is",
                    "controlled",
                    "by",
                    "acti-vation",
                    "of",
                    "nodes",
                    "along",
                    "the",
                    "syntactic",
                    "constraint",
                    "network."
                ],
                [
                    "The",
                    "network",
                    "virtually",
                    "encodes",
                    "a",
                    "finite-state",
                    "transition",
                    "equivalent",
                    "to",
                    "{COMP|XCOMP}*GF-COMP",
                    "#TARGET_REF",
                    "where",
                    "GF-COMP",
                    "denotes",
                    "grammatical",
                    "functions",
                    "other",
                    "than",
                    "COMP."
                ],
                [
                    "The",
                    "address",
                    "of",
                    "the",
                    "CI",
                    "bound",
                    "to",
                    "TOPIC",
                    "or",
                    "FOCUS",
                    "can",
                    "propagate",
                    "through",
                    "the",
                    "path",
                    "based",
                    "on",
                    "the",
                    "activation",
                    "patterns",
                    "of",
                    "the",
                    "syntactic",
                    "constraint",
                    "network,",
                    "and",
                    "the",
                    "activation",
                    "patterns",
                    "are",
                    "essentially",
                    "controlled",
                    "by",
                    "markers",
                    "flow",
                    "from",
                    "the",
                    "memory",
                    "network."
                ],
                [
                    "When",
                    "the",
                    "CSC",
                    "is",
                    "accepted",
                    "and",
                    "there",
                    "is",
                    "a",
                    "case-role",
                    "not",
                    "bound",
                    "to",
                    "any",
                    "CI",
                    "(OBJECT",
                    "in",
                    "the",
                    "example),",
                    "the",
                    "CSE",
                    "for",
                    "the",
                    "case-role",
                    "bound",
                    "with",
                    "the",
                    "CI",
                    "propagated",
                    "from",
                    "the",
                    "syntactic",
                    "constraint",
                    "network."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                2,
                3,
                0
            ]
        },
        "input": "sent0: There are two ways to handle sentences with unbounded dependency.\n sent1: The first approach is straightforward memorybased approach which simply store a set of CSCs involves unbounded dependency.\n sent2: A large set of CSCs would have to be prepared for this, but its simplicity minimized computational requirements.\n sent3: Alternatively, we can employ somewhat linguistic treatment of this phenomena within our framework.\n sent4: The syntactic constraint network has a node representing TOPIC and FOCUS which usually bound to the displaced phrase.\n sent5: An address of CI for the displaced phrase (such as 'the bug' in the example s9) is propagated to the TOPIC or FOCUS nodes in the syntactic constraint network.\n sent6: Further propagation of the address of the CI is controlled by acti-vation of nodes along the syntactic constraint network.\n sent7: The network virtually encodes a finite-state transition equivalent to {COMP|XCOMP}*GF-COMP #TARGET_REF where GF-COMP denotes grammatical functions other than COMP.\n sent8: The address of the CI bound to TOPIC or FOCUS can propagate through the path based on the activation patterns of the syntactic constraint network, and the activation patterns are essentially controlled by markers flow from the memory network.\n sent9: When the CSC is accepted and there is a case-role not bound to any CI (OBJECT in the example), the CSE for the case-role bound with the CI propagated from the syntactic constraint network.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent7\"], \"BACKGROUND\": [\"sent6\", \"sent8\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "Semantic",
                    "Network",
                    "Array",
                    "Processor",
                    "(SNAP)",
                    "is",
                    "a",
                    "highly",
                    "parallel",
                    "array",
                    "processor",
                    "fully",
                    "optimized",
                    "for",
                    "semantic",
                    "network",
                    "processing",
                    "with",
                    "marker-passing",
                    "mechanism."
                ],
                [
                    "In",
                    "order",
                    "to",
                    "facilitate",
                    "efficient",
                    "propagation",
                    "of",
                    "markers",
                    "and",
                    "to",
                    "ease",
                    "development",
                    "of",
                    "applications,",
                    "a",
                    "set",
                    "of",
                    "marker",
                    "propagation",
                    "instructions",
                    "has",
                    "been",
                    "microcoded."
                ],
                [
                    "SNAP",
                    "supports",
                    "propagation",
                    "of",
                    "markers",
                    "containing",
                    "(1)",
                    "bit-vectors."
                ],
                [
                    "(2)",
                    "address,",
                    "and",
                    "(3)",
                    "numeric",
                    "value."
                ],
                [
                    "By",
                    "limiting",
                    "content",
                    "of",
                    "markers,",
                    "significant",
                    "reduction",
                    "in",
                    "cost",
                    "and",
                    "resource",
                    "has",
                    "been",
                    "attained",
                    "without",
                    "undermining",
                    "performance",
                    "requirements",
                    "for",
                    "knowledge",
                    "processing."
                ],
                [
                    "Several",
                    "AI",
                    "applications",
                    "such",
                    "as",
                    "natural",
                    "language",
                    "processing",
                    "system,",
                    "classification",
                    "system",
                    "#TARGET_REF",
                    ",",
                    "and",
                    "rule-based",
                    "system",
                    "has",
                    "been",
                    "developed",
                    "on",
                    "SNAP."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                3
            ]
        },
        "input": "sent0: The Semantic Network Array Processor (SNAP) is a highly parallel array processor fully optimized for semantic network processing with marker-passing mechanism.\n sent1: In order to facilitate efficient propagation of markers and to ease development of applications, a set of marker propagation instructions has been microcoded.\n sent2: SNAP supports propagation of markers containing (1) bit-vectors.\n sent3: (2) address, and (3) numeric value.\n sent4: By limiting content of markers, significant reduction in cost and resource has been attained without undermining performance requirements for knowledge processing.\n sent5: Several AI applications such as natural language processing system, classification system #TARGET_REF , and rule-based system has been developed on SNAP.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent5\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "MMB",
                    "believed",
                    "30",
                    "years",
                    "ago",
                    "that",
                    "constructed",
                    "entities",
                    "such",
                    "as",
                    "dictionaries",
                    "and",
                    "thesauri",
                    "(especially",
                    "the",
                    "latter)",
                    "constituted",
                    "real",
                    "resources",
                    "for",
                    "computational",
                    "language",
                    "processing",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "That",
                    "was",
                    "at",
                    "a",
                    "time",
                    "when",
                    "any",
                    "computational",
                    "operations",
                    "on",
                    "such",
                    "entities",
                    "were",
                    "often",
                    "dismissed",
                    "by",
                    "those",
                    "working",
                    "in",
                    "other",
                    "areas",
                    "of",
                    "computational",
                    "linguistics",
                    "as",
                    "low-grade",
                    "concordance",
                    "work."
                ],
                [
                    "Betty",
                    "May",
                    "compacted",
                    "the",
                    "whole",
                    "of",
                    "Roget's",
                    "thesaurus",
                    "for",
                    "MMB,",
                    "from",
                    "1,000",
                    "'heads'",
                    "to",
                    "800,",
                    "and",
                    "had",
                    "them",
                    "cardpunched."
                ],
                [
                    "That",
                    "formed",
                    "the",
                    "basis",
                    "for",
                    "a",
                    "range",
                    "of",
                    "experiments",
                    "on",
                    "Hollerith",
                    "sorting",
                    "machines",
                    "which",
                    "contributed",
                    "to",
                    "Karen",
                    "Sparck",
                    "Jones'",
                    "seminal",
                    "thesis",
                    "work",
                    "Synonymy",
                    "and",
                    "semantics",
                    "classification",
                    "#REF",
                    "."
                ],
                [
                    "MMB",
                    "believed",
                    "that",
                    "thesauri",
                    "such",
                    "as",
                    "Roget's",
                    "were",
                    "not",
                    "just",
                    "fallible",
                    "human",
                    "constructs",
                    "but",
                    "real",
                    "resources",
                    "with",
                    "some",
                    "mathematical",
                    "structure",
                    "that",
                    "was",
                    "also",
                    "a",
                    "guide",
                    "to",
                    "the",
                    "structures",
                    "which",
                    "humans",
                    "use",
                    "to",
                    "process",
                    "language."
                ],
                [
                    "She",
                    "would",
                    "often",
                    "refer",
                    "to",
                    "'Roget's",
                    "unconscious'",
                    "by",
                    "which",
                    "she",
                    "meant",
                    "that",
                    "the",
                    "patterns",
                    "of",
                    "cross-references",
                    "from",
                    "word",
                    "to",
                    "word",
                    "across",
                    "the",
                    "thesaurus",
                    "had",
                    "underlying",
                    "generalisations",
                    "and",
                    "patterns."
                ]
            ],
            "context": [
                1,
                2,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: MMB believed 30 years ago that constructed entities such as dictionaries and thesauri (especially the latter) constituted real resources for computational language processing #TARGET_REF .\n sent1: That was at a time when any computational operations on such entities were often dismissed by those working in other areas of computational linguistics as low-grade concordance work.\n sent2: Betty May compacted the whole of Roget's thesaurus for MMB, from 1,000 'heads' to 800, and had them cardpunched.\n sent3: That formed the basis for a range of experiments on Hollerith sorting machines which contributed to Karen Sparck Jones' seminal thesis work Synonymy and semantics classification #REF .\n sent4: MMB believed that thesauri such as Roget's were not just fallible human constructs but real resources with some mathematical structure that was also a guide to the structures which humans use to process language.\n sent5: She would often refer to 'Roget's unconscious' by which she meant that the patterns of cross-references from word to word across the thesaurus had underlying generalisations and patterns.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "To",
                    "address",
                    "the",
                    "above",
                    "issues,",
                    "we",
                    "propose",
                    "to",
                    "learn",
                    "natural",
                    "language",
                    "actions",
                    "that",
                    "represent",
                    "system",
                    "utterances",
                    "as",
                    "a",
                    "span",
                    "of",
                    "words,",
                    "which",
                    "explicitly",
                    "reveal",
                    "the",
                    "underlying",
                    "intentions."
                ],
                [
                    "Natural",
                    "language",
                    "provides",
                    "unique",
                    "compositional",
                    "structure",
                    "while",
                    "retaining",
                    "the",
                    "representation",
                    "flexibility."
                ],
                [
                    "These",
                    "properties",
                    "promote",
                    "model",
                    "generalization",
                    "and",
                    "thus",
                    "make",
                    "natural",
                    "language",
                    "a",
                    "flexible",
                    "representation",
                    "for",
                    "capturing",
                    "characteristics",
                    "with",
                    "minimal",
                    "assumptions",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Motivated",
                    "by",
                    "these",
                    "advantages,",
                    "we",
                    "learn",
                    "natural",
                    "language",
                    "actions",
                    "by",
                    "identifying",
                    "salient",
                    "words",
                    "of",
                    "system",
                    "utterances."
                ],
                [
                    "Salient",
                    "refers",
                    "to",
                    "indicative",
                    "for",
                    "a",
                    "prediction",
                    "task",
                    "(e.g.,",
                    "sentiment",
                    "analysis)",
                    "that",
                    "takes",
                    "as",
                    "input",
                    "the",
                    "original",
                    "utterance."
                ],
                [
                    "The",
                    "main",
                    "rationale",
                    "is",
                    "that",
                    "the",
                    "principal",
                    "information",
                    "that",
                    "the",
                    "task",
                    "concerns",
                    "can",
                    "be",
                    "preserved",
                    "by",
                    "just",
                    "the",
                    "salient",
                    "words."
                ],
                [
                    "For",
                    "example,",
                    "the",
                    "sentiment",
                    "of",
                    "sentence",
                    "\"The",
                    "movie",
                    "starts",
                    "out",
                    "as",
                    "competent",
                    "but",
                    "turn",
                    "bland\"",
                    "can",
                    "be",
                    "revealed",
                    "by",
                    "the",
                    "word",
                    "\"bland\"",
                    "when",
                    "it",
                    "is",
                    "identified",
                    "salient",
                    "by",
                    "considering",
                    "the",
                    "complete",
                    "context."
                ],
                [
                    "In",
                    "our",
                    "scenarios,",
                    "we",
                    "consider",
                    "measuring",
                    "word",
                    "saliency",
                    "in",
                    "terms",
                    "of",
                    "state",
                    "transitions."
                ],
                [
                    "This",
                    "is",
                    "because",
                    "state",
                    "transitions",
                    "reflect",
                    "how",
                    "the",
                    "intentions",
                    "of",
                    "a",
                    "system",
                    "utterance",
                    "influence",
                    "the",
                    "dialogue",
                    "progress,",
                    "and",
                    "action",
                    "representations",
                    "that",
                    "capture",
                    "such",
                    "influences",
                    "can",
                    "well",
                    "reveal",
                    "the",
                    "intentions",
                    "#REF",
                    "."
                ],
                [
                    "By",
                    "considering",
                    "salient",
                    "words",
                    "for",
                    "state",
                    "tracking",
                    "tasks",
                    "as",
                    "actions,",
                    "we",
                    "obtain",
                    "action",
                    "representations",
                    "that",
                    "enjoy",
                    "the",
                    "merits",
                    "of",
                    "natural",
                    "language",
                    "and",
                    "indeed",
                    "capture",
                    "the",
                    "characteristics",
                    "of",
                    "interest,",
                    "i.e.,",
                    "intentions",
                    "of",
                    "system",
                    "utterances."
                ]
            ],
            "context": [
                0,
                3,
                1,
                2,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: To address the above issues, we propose to learn natural language actions that represent system utterances as a span of words, which explicitly reveal the underlying intentions.\n sent1: Natural language provides unique compositional structure while retaining the representation flexibility.\n sent2: These properties promote model generalization and thus make natural language a flexible representation for capturing characteristics with minimal assumptions #TARGET_REF .\n sent3: Motivated by these advantages, we learn natural language actions by identifying salient words of system utterances.\n sent4: Salient refers to indicative for a prediction task (e.g., sentiment analysis) that takes as input the original utterance.\n sent5: The main rationale is that the principal information that the task concerns can be preserved by just the salient words.\n sent6: For example, the sentiment of sentence \"The movie starts out as competent but turn bland\" can be revealed by the word \"bland\" when it is identified salient by considering the complete context.\n sent7: In our scenarios, we consider measuring word saliency in terms of state transitions.\n sent8: This is because state transitions reflect how the intentions of a system utterance influence the dialogue progress, and action representations that capture such influences can well reveal the intentions #REF .\n sent9: By considering salient words for state tracking tasks as actions, we obtain action representations that enjoy the merits of natural language and indeed capture the characteristics of interest, i.e., intentions of system utterances.\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [\"sent3\"], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Another",
                    "domain",
                    "in",
                    "which",
                    "WM",
                    "databases",
                    "have",
                    "been",
                    "used",
                    "is",
                    "terminology."
                ],
                [
                    "In",
                    "collaboration",
                    "with",
                    "the",
                    "UBS",
                    "bank,",
                    "a",
                    "module",
                    "was",
                    "developed",
                    "which",
                    "recognizes",
                    "banking",
                    "terminology",
                    "in",
                    "unseen",
                    "text",
                    "and",
                    "provides",
                    "an",
                    "on-line",
                    "link",
                    "to",
                    "the",
                    "relevant",
                    "entry",
                    "in",
                    "a",
                    "terminological",
                    "database,",
                    "cf."
                ],
                [
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Here",
                    "the",
                    "WFRules",
                    "are",
                    "used",
                    "not",
                    "only",
                    "as",
                    "a",
                    "structuring",
                    "device",
                    "of",
                    "the",
                    "terminological",
                    "lexicon,",
                    "but",
                    "also",
                    "as",
                    "a",
                    "way",
                    "for",
                    "recognizing",
                    "terms",
                    "when",
                    "they",
                    "are",
                    "'hidden'",
                    "in",
                    "nominalizations,",
                    "compounds,",
                    "etc."
                ],
                [
                    "Thus,",
                    "Verwaltungsrat",
                    "('board",
                    "of",
                    "directors')",
                    "is",
                    "also",
                    "recognized",
                    "in",
                    "Verwaltungsratsvakanz",
                    "('vacancy",
                    "in",
                    "the",
                    "board",
                    "of",
                    "directors')."
                ],
                [
                    "One",
                    "of",
                    "the",
                    "reasons",
                    "why",
                    "WM",
                    "is",
                    "particularly",
                    "suited",
                    "to",
                    "this",
                    "task",
                    "in",
                    "a",
                    "multilingual",
                    "system",
                    "(German,",
                    "English,",
                    "Italian)",
                    "is",
                    "that",
                    "it",
                    "can",
                    "treat",
                    "singleword",
                    "and",
                    "multi-word",
                    "terms",
                    "equally."
                ]
            ],
            "context": [
                3,
                1,
                1,
                1,
                3,
                0
            ]
        },
        "input": "sent0: Another domain in which WM databases have been used is terminology.\n sent1: In collaboration with the UBS bank, a module was developed which recognizes banking terminology in unseen text and provides an on-line link to the relevant entry in a terminological database, cf.\n sent2: #TARGET_REF .\n sent3: Here the WFRules are used not only as a structuring device of the terminological lexicon, but also as a way for recognizing terms when they are 'hidden' in nominalizations, compounds, etc.\n sent4: Thus, Verwaltungsrat ('board of directors') is also recognized in Verwaltungsratsvakanz ('vacancy in the board of directors').\n sent5: One of the reasons why WM is particularly suited to this task in a multilingual system (German, English, Italian) is that it can treat singleword and multi-word terms equally.\n",
        "output": "{\"INFORMATION\": [\"sent1\", \"sent2\", \"sent3\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Returning",
                    "to",
                    "the",
                    "different",
                    "approaches",
                    "of",
                    "handling",
                    "Arabic",
                    "text."
                ],
                [
                    "As",
                    "discussed",
                    "in",
                    "the",
                    "previous",
                    "sections",
                    "letter",
                    "normalization",
                    "and",
                    "transliteration",
                    "are",
                    "examples",
                    "of",
                    "the",
                    "simplification",
                    "approach."
                ],
                [
                    "For",
                    "example,",
                    "letter",
                    "normalization",
                    "is",
                    "commonly",
                    "applied",
                    "to",
                    "reduce",
                    "the",
                    "noise",
                    "and",
                    "sparsity",
                    "in",
                    "the",
                    "data",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "For",
                    "transliteration,",
                    "Ameur",
                    "et",
                    "al."
                ],
                [
                    "(",
                    "2017)",
                    "applied",
                    "a",
                    "bidirectional",
                    "attention-based",
                    "encoder-decoder",
                    "model",
                    "for",
                    "the",
                    "task",
                    "of",
                    "machine",
                    "transliteration",
                    "between",
                    "Arabic",
                    "and",
                    "English."
                ]
            ],
            "context": [
                3,
                3,
                2,
                0,
                0
            ]
        },
        "input": "sent0: Returning to the different approaches of handling Arabic text.\n sent1: As discussed in the previous sections letter normalization and transliteration are examples of the simplification approach.\n sent2: For example, letter normalization is commonly applied to reduce the noise and sparsity in the data #TARGET_REF .\n sent3: For transliteration, Ameur et al.\n sent4: ( 2017) applied a bidirectional attention-based encoder-decoder model for the task of machine transliteration between Arabic and English.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Our",
                    "baseline",
                    "model",
                    "is",
                    "inspired",
                    "by",
                    "a",
                    "hypernym",
                    "classification",
                    "model",
                    "proposed",
                    "by",
                    "#TARGET_REF",
                    ",",
                    "also",
                    "using",
                    "a",
                    "pair",
                    "of",
                    "terms",
                    "with",
                    "a",
                    "set",
                    "of",
                    "support",
                    "sentences",
                    "where",
                    "the",
                    "terms",
                    "co-occur."
                ]
            ],
            "context": [
                1
            ]
        },
        "input": "sent0: Our baseline model is inspired by a hypernym classification model proposed by #TARGET_REF , also using a pair of terms with a set of support sentences where the terms co-occur.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "For",
                    "the",
                    "experiments,",
                    "we",
                    "used",
                    "a",
                    "Transformer-base",
                    "model",
                    "#TARGET_REF",
                    "with",
                    "the",
                    "default",
                    "configuration",
                    "in",
                    "Marian",
                    "NMT",
                    "#REF",
                    "."
                ],
                [
                    "The",
                    "steps",
                    "are",
                    "as",
                    "follows:"
                ]
            ],
            "context": [
                2,
                0
            ]
        },
        "input": "sent0: For the experiments, we used a Transformer-base model #TARGET_REF with the default configuration in Marian NMT #REF .\n sent1: The steps are as follows:\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "First,",
                    "there",
                    "is",
                    "no",
                    "agreed-upon",
                    "definition",
                    "of",
                    "bridging",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Consequently,",
                    "manual",
                    "annotation",
                    "of",
                    "bridging",
                    "relations,",
                    "and",
                    "the",
                    "use",
                    "of",
                    "these",
                    "annotations,",
                    "requires",
                    "substantial",
                    "expertise",
                    "and",
                    "effort."
                ],
                [
                    "In",
                    "contrast,",
                    "NP",
                    "Enrichment",
                    "is",
                    "compactly",
                    "defined,",
                    "and",
                    "is",
                    "amenable",
                    "to",
                    "large-scale",
                    "annotation",
                    "after",
                    "only",
                    "a",
                    "brief",
                    "annotator",
                    "training."
                ]
            ],
            "context": [
                3,
                3,
                0
            ]
        },
        "input": "sent0: First, there is no agreed-upon definition of bridging #TARGET_REF .\n sent1: Consequently, manual annotation of bridging relations, and the use of these annotations, requires substantial expertise and effort.\n sent2: In contrast, NP Enrichment is compactly defined, and is amenable to large-scale annotation after only a brief annotator training.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Subjective",
                    "evaluation",
                    "was",
                    "performed",
                    "after",
                    "first",
                    "poor",
                    "BLEU",
                    "results",
                    "triggered",
                    "some",
                    "distrust."
                ],
                [
                    "Many",
                    "authors",
                    "agree",
                    "that",
                    "BLEU",
                    "metric",
                    "systematically",
                    "penalizes",
                    "RBMT",
                    "systems",
                    "#REF",
                    "and",
                    "it",
                    "is",
                    "not",
                    "suited",
                    "for",
                    "highly",
                    "inflexible",
                    "languages."
                ],
                [
                    "Authors",
                    "of",
                    "METEOR",
                    "#TARGET_REF",
                    ",",
                    "#REF",
                    ")",
                    "state",
                    "that",
                    "their",
                    "system",
                    "fixes",
                    "most",
                    "of",
                    "the",
                    "problems",
                    "encountered",
                    "using",
                    "BLEU",
                    "metric,",
                    "they",
                    "state",
                    "that",
                    "METEOR",
                    "correlates",
                    "highly",
                    "with",
                    "human",
                    "judgement."
                ],
                [
                    "Unfortunately",
                    "METEOR",
                    "does",
                    "not",
                    "support",
                    "our",
                    "language",
                    "pair,",
                    "we",
                    "hope",
                    "to",
                    "change",
                    "this",
                    "in",
                    "the",
                    "near",
                    "future,",
                    "see",
                    "further",
                    "work",
                    "Separate",
                    "scales",
                    "for",
                    "fluency",
                    "and",
                    "adequacy",
                    "were",
                    "developed",
                    "under",
                    "the",
                    "assumption",
                    "that",
                    "a",
                    "translation",
                    "might",
                    "be",
                    "disfluent",
                    "but",
                    "contain",
                    "all",
                    "the",
                    "information",
                    "from",
                    "the",
                    "source."
                ]
            ],
            "context": [
                3,
                3,
                3,
                2
            ]
        },
        "input": "sent0: Subjective evaluation was performed after first poor BLEU results triggered some distrust.\n sent1: Many authors agree that BLEU metric systematically penalizes RBMT systems #REF and it is not suited for highly inflexible languages.\n sent2: Authors of METEOR #TARGET_REF , #REF ) state that their system fixes most of the problems encountered using BLEU metric, they state that METEOR correlates highly with human judgement.\n sent3: Unfortunately METEOR does not support our language pair, we hope to change this in the near future, see further work Separate scales for fluency and adequacy were developed under the assumption that a translation might be disfluent but contain all the information from the source.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent3\"], \"BACKGROUND\": [\"sent0\", \"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Curriculum",
                    "Learning."
                ],
                [
                    "Curriculums",
                    "in",
                    "reinforcement",
                    "learning",
                    "have",
                    "traditionally",
                    "been",
                    "used",
                    "to",
                    "set",
                    "goals",
                    "of",
                    "steadily",
                    "increasing",
                    "difficulty",
                    "for",
                    "an",
                    "agent",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "The",
                    "difficulty",
                    "of",
                    "these",
                    "curriculums",
                    "are",
                    "generally",
                    "measured",
                    "difficulty",
                    "via",
                    "proxy",
                    "of",
                    "agent",
                    "performance",
                    "(Narvekar",
                    "et",
                    "al.,",
                    "2020)-methods",
                    "either",
                    "choose",
                    "to",
                    "adversarially",
                    "set",
                    "goals",
                    "of",
                    "steadily",
                    "increasing",
                    "difficulty",
                    "#REF",
                    "or",
                    "to",
                    "maximize",
                    "learning",
                    "performance",
                    "based",
                    "on",
                    "environment",
                    "instances",
                    "an",
                    "agent",
                    "finds",
                    "difficult",
                    "historically",
                    "#REF",
                    "."
                ],
                [
                    "While",
                    "we",
                    "were",
                    "inspired",
                    "by",
                    "these",
                    "works,",
                    "they",
                    "all",
                    "focus",
                    "on",
                    "searching",
                    "for",
                    "goals",
                    "for",
                    "agents",
                    "which",
                    "can",
                    "be",
                    "difficult",
                    "to",
                    "scale",
                    "to",
                    "complex",
                    "tasks",
                    "such",
                    "our",
                    "own",
                    "natural",
                    "language",
                    "motivation-based",
                    "goals."
                ],
                [
                    "We'd",
                    "also",
                    "like",
                    "to",
                    "note",
                    "that",
                    "most",
                    "works",
                    "using",
                    "procedural",
                    "generation",
                    "to",
                    "benchmark",
                    "RL",
                    "agents",
                    "such",
                    "as",
                    "#REF",
                    ",",
                    "#REF",
                    ",",
                    "#REF",
                    "rely",
                    "on",
                    "the",
                    "underlying",
                    "richness",
                    "of",
                    "the",
                    "game",
                    "engines",
                    "to",
                    "generate",
                    "novel",
                    "environments",
                    "as",
                    "opposed",
                    "to",
                    "learning",
                    "to",
                    "generate."
                ]
            ],
            "context": [
                0,
                1,
                0,
                2,
                0
            ]
        },
        "input": "sent0: Curriculum Learning.\n sent1: Curriculums in reinforcement learning have traditionally been used to set goals of steadily increasing difficulty for an agent #TARGET_REF .\n sent2: The difficulty of these curriculums are generally measured difficulty via proxy of agent performance (Narvekar et al., 2020)-methods either choose to adversarially set goals of steadily increasing difficulty #REF or to maximize learning performance based on environment instances an agent finds difficult historically #REF .\n sent3: While we were inspired by these works, they all focus on searching for goals for agents which can be difficult to scale to complex tasks such our own natural language motivation-based goals.\n sent4: We'd also like to note that most works using procedural generation to benchmark RL agents such as #REF , #REF , #REF rely on the underlying richness of the game engines to generate novel environments as opposed to learning to generate.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [\"sent3\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Model."
                ],
                [
                    "The",
                    "proposed",
                    "method",
                    "will",
                    "be",
                    "able",
                    "to",
                    "process",
                    "several",
                    "modalities",
                    "that",
                    "play",
                    "a",
                    "crucial",
                    "role",
                    "in",
                    "communication,",
                    "(i)",
                    "Linguistic",
                    "Information",
                    "(at",
                    "syntactic",
                    "and",
                    "semantic",
                    "level),",
                    "(ii)",
                    "Situational",
                    "Information,",
                    "(iii)",
                    "Prototypical",
                    "Knowledge",
                    "and",
                    "Relations,",
                    "and",
                    "(iv)",
                    "Speech-accompanying",
                    "eye-movements",
                    "of",
                    "the",
                    "speaker."
                ],
                [
                    "The",
                    "initial",
                    "base",
                    "model",
                    "will",
                    "focus",
                    "on",
                    "the",
                    "first",
                    "three",
                    "capabilities",
                    "by",
                    "utilizing",
                    "data-driven",
                    "language",
                    "models",
                    "such",
                    "as",
                    "fasttext",
                    "#REF",
                    "and",
                    "commonsense",
                    "knowledge-bases",
                    "like",
                    "ConceptNet",
                    "#REF",
                    "."
                ],
                [
                    "At",
                    "the",
                    "same",
                    "time,",
                    "two",
                    "modules",
                    "that",
                    "(i)",
                    "incorporate",
                    "eye-movements",
                    "and",
                    "(ii)",
                    "perform",
                    "situation-specific",
                    "feature",
                    "adaptation",
                    "will",
                    "be",
                    "developed",
                    "from",
                    "scratch."
                ],
                [
                    "In",
                    "brief,",
                    "vocabulary",
                    "obtained",
                    "from",
                    "the",
                    "pre-trained",
                    "embeddings",
                    "is",
                    "used",
                    "as",
                    "a",
                    "bridge",
                    "between",
                    "the",
                    "modalities."
                ],
                [
                    "For",
                    "each",
                    "vocabulary",
                    "item,",
                    "multimodal",
                    "embeddings",
                    "will",
                    "be",
                    "created",
                    "by",
                    "processing",
                    "every",
                    "input",
                    "channel,",
                    "see",
                    "Figure",
                    "2."
                ],
                [
                    "For",
                    "each",
                    "modality",
                    "and",
                    "their",
                    "joint",
                    "training,",
                    "we",
                    "will",
                    "utilize",
                    "an",
                    "appropriate",
                    "encoder,",
                    "such",
                    "as",
                    "Fast-R-CNN",
                    "#REF",
                    "for",
                    "images",
                    "and",
                    "attention-based",
                    "bi-directional",
                    "LSTMs",
                    "(e.g."
                ],
                [
                    "#TARGET_REF",
                    ",",
                    "for",
                    "text",
                    "and",
                    "eye-movement",
                    "data."
                ],
                [
                    "A",
                    "neural",
                    "network",
                    "ensemble",
                    "model",
                    "will",
                    "be",
                    "trained",
                    "on",
                    "the",
                    "embeddings",
                    "for",
                    "the",
                    "task",
                    "of",
                    "intended",
                    "object",
                    "or",
                    "action",
                    "prediction",
                    "from",
                    "situated",
                    "settings",
                    "with",
                    "masked",
                    "information."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                2,
                3
            ]
        },
        "input": "sent0: Model.\n sent1: The proposed method will be able to process several modalities that play a crucial role in communication, (i) Linguistic Information (at syntactic and semantic level), (ii) Situational Information, (iii) Prototypical Knowledge and Relations, and (iv) Speech-accompanying eye-movements of the speaker.\n sent2: The initial base model will focus on the first three capabilities by utilizing data-driven language models such as fasttext #REF and commonsense knowledge-bases like ConceptNet #REF .\n sent3: At the same time, two modules that (i) incorporate eye-movements and (ii) perform situation-specific feature adaptation will be developed from scratch.\n sent4: In brief, vocabulary obtained from the pre-trained embeddings is used as a bridge between the modalities.\n sent5: For each vocabulary item, multimodal embeddings will be created by processing every input channel, see Figure 2.\n sent6: For each modality and their joint training, we will utilize an appropriate encoder, such as Fast-R-CNN #REF for images and attention-based bi-directional LSTMs (e.g.\n sent7: #TARGET_REF , for text and eye-movement data.\n sent8: A neural network ensemble model will be trained on the embeddings for the task of intended object or action prediction from situated settings with masked information.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent6\", \"sent7\"], \"BACKGROUND\": [\"sent8\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "From",
                    "the",
                    "point",
                    "of",
                    "view",
                    "of",
                    "communication",
                    "study,",
                    "most",
                    "of",
                    "the",
                    "research",
                    "on",
                    "public",
                    "apologies",
                    "is",
                    "focused",
                    "on",
                    "apology",
                    "as",
                    "a",
                    "speech",
                    "act",
                    "(e.g."
                ],
                [
                    "#REF",
                    "."
                ],
                [
                    "The",
                    "studies",
                    "are",
                    "based",
                    "on",
                    "two",
                    "perspectives."
                ],
                [
                    "The",
                    "first",
                    "is",
                    "from",
                    "the",
                    "point",
                    "of",
                    "view",
                    "of",
                    "the",
                    "offended",
                    "party",
                    "#REF",
                    "and",
                    "the",
                    "second",
                    "sees",
                    "apology",
                    "from",
                    "the",
                    "point",
                    "of",
                    "view",
                    "of",
                    "the",
                    "offender",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                3,
                0,
                3,
                3
            ]
        },
        "input": "sent0: From the point of view of communication study, most of the research on public apologies is focused on apology as a speech act (e.g.\n sent1: #REF .\n sent2: The studies are based on two perspectives.\n sent3: The first is from the point of view of the offended party #REF and the second sees apology from the point of view of the offender #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\", \"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "MMB",
                    "would",
                    "also",
                    "welcome",
                    "anecdotal",
                    "evidence,",
                    "of",
                    "the",
                    "sort",
                    "to",
                    "be",
                    "found",
                    "in",
                    "the",
                    "work",
                    "of",
                    "Cassirer,",
                    "that",
                    "metaphorical",
                    "uses",
                    "of",
                    "language",
                    "were",
                    "in",
                    "some",
                    "historical",
                    "sense",
                    "original,",
                    "and",
                    "not",
                    "a",
                    "later",
                    "luxury."
                ],
                [
                    "She",
                    "rejected",
                    "the",
                    "view",
                    "that",
                    "language",
                    "originally",
                    "consisted",
                    "of",
                    "simple,",
                    "unambiguous,",
                    "Augustinian",
                    "names",
                    "of",
                    "objects",
                    "-the",
                    "view",
                    "parodied",
                    "by",
                    "#TARGET_REF",
                    "in",
                    "the",
                    "opening",
                    "of",
                    "Philosophical",
                    "investigations",
                    "-but",
                    "preferred",
                    "the",
                    "idea",
                    "of",
                    "original",
                    "primitive",
                    "atoms",
                    "of",
                    "wide,",
                    "vague,",
                    "unspecific",
                    "meaning,",
                    "which",
                    "were",
                    "then",
                    "both",
                    "refined",
                    "to",
                    "specific",
                    "referents",
                    "in",
                    "use",
                    "and",
                    "constantly",
                    "extended",
                    "by",
                    "metaphor."
                ],
                [
                    "Here,",
                    "for",
                    "MMB,",
                    "was",
                    "the",
                    "root",
                    "not",
                    "only",
                    "of",
                    "the",
                    "metaphor,",
                    "but",
                    "also",
                    "of",
                    "metaphysics",
                    "itself,",
                    "which",
                    "consisted",
                    "for",
                    "her,",
                    "as",
                    "for",
                    "Wittgenstein,",
                    "of",
                    "words",
                    "used",
                    "outside",
                    "their",
                    "hitherto",
                    "normal",
                    "realm",
                    "of",
                    "application."
                ],
                [
                    "But,",
                    "whereas",
                    "he",
                    "thought",
                    "that",
                    "words",
                    "were",
                    "'on",
                    "holiday'",
                    "when",
                    "so",
                    "used,",
                    "for",
                    "her",
                    "it",
                    "was",
                    "part",
                    "of",
                    "their",
                    "everyday",
                    "work."
                ]
            ],
            "context": [
                0,
                2,
                0,
                0
            ]
        },
        "input": "sent0: MMB would also welcome anecdotal evidence, of the sort to be found in the work of Cassirer, that metaphorical uses of language were in some historical sense original, and not a later luxury.\n sent1: She rejected the view that language originally consisted of simple, unambiguous, Augustinian names of objects -the view parodied by #TARGET_REF in the opening of Philosophical investigations -but preferred the idea of original primitive atoms of wide, vague, unspecific meaning, which were then both refined to specific referents in use and constantly extended by metaphor.\n sent2: Here, for MMB, was the root not only of the metaphor, but also of metaphysics itself, which consisted for her, as for Wittgenstein, of words used outside their hitherto normal realm of application.\n sent3: But, whereas he thought that words were 'on holiday' when so used, for her it was part of their everyday work.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "benefits",
                    "of",
                    "a",
                    "New",
                    "York",
                    "Subway",
                    "system",
                    "is",
                    "that",
                    "a",
                    "person",
                    "can",
                    "get",
                    "from",
                    "A",
                    "to",
                    "B",
                    "without",
                    "being",
                    "stuck",
                    "in",
                    "traffic",
                    "and",
                    "subway",
                    "trains",
                    "are",
                    "faster",
                    "than",
                    "buses."
                ],
                [
                    "(Prediction:",
                    "Negative)",
                    "Natural",
                    "Language",
                    "Inference",
                    "#REF",
                    ",",
                    "synonym",
                    "substitutions",
                    "#TARGET_REF",
                    ",",
                    "or",
                    "adding",
                    "adversarial",
                    "sentences",
                    "for",
                    "QA",
                    "#REF",
                    "."
                ],
                [
                    "More",
                    "recent",
                    "work",
                    "on",
                    "testing",
                    "models'",
                    "behaviour",
                    "using",
                    "#REF",
                    ")",
                    "also",
                    "used",
                    "a",
                    "pre-defined",
                    "series",
                    "of",
                    "test",
                    "types,",
                    "e.g.,",
                    "adding",
                    "negation,",
                    "temporal",
                    "change,",
                    "and",
                    "switching",
                    "locations/person",
                    "names."
                ],
                [
                    "However,",
                    "for",
                    "safe",
                    "deployment",
                    "of",
                    "NLP",
                    "models",
                    "in",
                    "the",
                    "real",
                    "world,",
                    "in",
                    "addition",
                    "to",
                    "predefining",
                    "a",
                    "small",
                    "or",
                    "limited",
                    "set",
                    "of",
                    "patterns",
                    "which",
                    "the",
                    "model",
                    "could",
                    "be",
                    "vulnerable",
                    "to,",
                    "it",
                    "is",
                    "also",
                    "important",
                    "to",
                    "proactively",
                    "discover",
                    "and",
                    "identify",
                    "models'",
                    "unrobust",
                    "regions",
                    "automatically",
                    "and",
                    "comprehensively."
                ]
            ],
            "context": [
                0,
                3,
                3,
                0
            ]
        },
        "input": "sent0: The benefits of a New York Subway system is that a person can get from A to B without being stuck in traffic and subway trains are faster than buses.\n sent1: (Prediction: Negative) Natural Language Inference #REF , synonym substitutions #TARGET_REF , or adding adversarial sentences for QA #REF .\n sent2: More recent work on testing models' behaviour using #REF ) also used a pre-defined series of test types, e.g., adding negation, temporal change, and switching locations/person names.\n sent3: However, for safe deployment of NLP models in the real world, in addition to predefining a small or limited set of patterns which the model could be vulnerable to, it is also important to proactively discover and identify models' unrobust regions automatically and comprehensively.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "To",
                    "explore",
                    "the",
                    "generality",
                    "of",
                    "our",
                    "LTR",
                    "approach,",
                    "we",
                    "also",
                    "conduct",
                    "experiments",
                    "on",
                    "the",
                    "MS",
                    "MARCO",
                    "document",
                    "ranking",
                    "task."
                ],
                [
                    "We",
                    "emphasize",
                    "here",
                    "that",
                    "all",
                    "experiments",
                    "are",
                    "conducted",
                    "in",
                    "a",
                    "zero-shot",
                    "manner,",
                    "over",
                    "paragraph",
                    "extracts",
                    "from",
                    "the",
                    "collection,",
                    "what",
                    "is",
                    "commonly",
                    "known",
                    "as",
                    "the",
                    "MaxP",
                    "approach",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Both",
                    "the",
                    "final-stage",
                    "neural",
                    "reranker",
                    "and",
                    "our",
                    "LTR",
                    "module",
                    "are",
                    "trained",
                    "on",
                    "MS",
                    "MARCO",
                    "passage",
                    "data",
                    "only."
                ],
                [
                    "However,",
                    "comparing",
                    "our",
                    "effectiveness",
                    "results",
                    "with",
                    "the",
                    "official",
                    "leaderboard",
                    "reveals",
                    "that",
                    "our",
                    "configurations",
                    "are",
                    "competitive",
                    "compared",
                    "to",
                    "other",
                    "single-stage",
                    "rerankers."
                ]
            ],
            "context": [
                3,
                2,
                0,
                0
            ]
        },
        "input": "sent0: To explore the generality of our LTR approach, we also conduct experiments on the MS MARCO document ranking task.\n sent1: We emphasize here that all experiments are conducted in a zero-shot manner, over paragraph extracts from the collection, what is commonly known as the MaxP approach #TARGET_REF .\n sent2: Both the final-stage neural reranker and our LTR module are trained on MS MARCO passage data only.\n sent3: However, comparing our effectiveness results with the official leaderboard reveals that our configurations are competitive compared to other single-stage rerankers.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Hierarchical",
                    "Phrase-Based",
                    "(HPB)",
                    "SMT",
                    "#REF",
                    "is",
                    "a",
                    "tree-based",
                    "model",
                    "which",
                    "extracts",
                    "a",
                    "synchronous",
                    "Context-Free",
                    "Grammar",
                    "(CFG)",
                    "automatically",
                    "from",
                    "the",
                    "training",
                    "corpus."
                ],
                [
                    "HPB",
                    "SMT",
                    "is",
                    "based",
                    "on",
                    "phrases",
                    "extracted",
                    "according",
                    "to",
                    "the",
                    "PB",
                    "model",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Thus,",
                    "HPB",
                    "SMT",
                    "tries",
                    "to",
                    "build",
                    "upon",
                    "the",
                    "strengths",
                    "of",
                    "PB",
                    "SMT",
                    "and",
                    "adds",
                    "to",
                    "it",
                    "the",
                    "ability",
                    "to",
                    "translate",
                    "discontinuous",
                    "phrases",
                    "and",
                    "learn",
                    "phrase-reordering",
                    "in",
                    "hierarchical",
                    "rules",
                    "without",
                    "a",
                    "separate",
                    "reordering",
                    "model."
                ],
                [
                    "HPB",
                    "SMT",
                    "uses",
                    "hierarchical",
                    "rules",
                    "as",
                    "a",
                    "translation",
                    "unit."
                ],
                [
                    "These",
                    "rules",
                    "are",
                    "rewrite",
                    "rules",
                    "with",
                    "aligned",
                    "pairs",
                    "of",
                    "right-hand",
                    "sides,",
                    "taking",
                    "the",
                    "following",
                    "form:"
                ]
            ],
            "context": [
                3,
                2,
                2,
                0,
                0
            ]
        },
        "input": "sent0: Hierarchical Phrase-Based (HPB) SMT #REF is a tree-based model which extracts a synchronous Context-Free Grammar (CFG) automatically from the training corpus.\n sent1: HPB SMT is based on phrases extracted according to the PB model #TARGET_REF .\n sent2: Thus, HPB SMT tries to build upon the strengths of PB SMT and adds to it the ability to translate discontinuous phrases and learn phrase-reordering in hierarchical rules without a separate reordering model.\n sent3: HPB SMT uses hierarchical rules as a translation unit.\n sent4: These rules are rewrite rules with aligned pairs of right-hand sides, taking the following form:\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\", \"sent2\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "areas",
                    "such",
                    "as",
                    "image",
                    "processing,",
                    "the",
                    "same",
                    "question",
                    "has",
                    "been",
                    "receiving",
                    "a",
                    "lot",
                    "of",
                    "attention",
                    "and",
                    "inspired",
                    "a",
                    "wave",
                    "of",
                    "methods",
                    "for",
                    "learning",
                    "and",
                    "evaluating",
                    "unsupervised",
                    "representation",
                    "disentanglement",
                    "#TARGET_REF",
                    "and",
                    "creation",
                    "of",
                    "large",
                    "scale",
                    "datasets",
                    "#REF",
                    "."
                ],
                [
                    "It",
                    "has",
                    "been",
                    "argued",
                    "that",
                    "disentanglement",
                    "is",
                    "the",
                    "means",
                    "towards",
                    "representation",
                    "interpretability",
                    "#REF",
                    ",",
                    "generalization",
                    "#REF",
                    ",",
                    "and",
                    "robustness",
                    "#REF",
                    "."
                ],
                [
                    "However,",
                    "these",
                    "benefits",
                    "are",
                    "yet",
                    "to",
                    "be",
                    "realized",
                    "and",
                    "evaluated",
                    "in",
                    "text",
                    "domain."
                ]
            ],
            "context": [
                3,
                3,
                0
            ]
        },
        "input": "sent0: In areas such as image processing, the same question has been receiving a lot of attention and inspired a wave of methods for learning and evaluating unsupervised representation disentanglement #TARGET_REF and creation of large scale datasets #REF .\n sent1: It has been argued that disentanglement is the means towards representation interpretability #REF , generalization #REF , and robustness #REF .\n sent2: However, these benefits are yet to be realized and evaluated in text domain.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Neural",
                    "approaches",
                    "to",
                    "sequence",
                    "tagging",
                    "are",
                    "common",
                    "due",
                    "to",
                    "extensive",
                    "developments",
                    "in",
                    "named",
                    "entity",
                    "recognition."
                ],
                [
                    "#REF",
                    "introduced",
                    "and",
                    "cultivated",
                    "the",
                    "use",
                    "of",
                    "bidirectional",
                    "LSTMs",
                    "to",
                    "incorporate",
                    "features",
                    "that",
                    "could",
                    "be",
                    "used",
                    "for",
                    "sequence",
                    "tagging",
                    "using",
                    "a",
                    "CRF."
                ],
                [
                    "#REF",
                    "'s",
                    "architecture",
                    "and",
                    "the",
                    "NeuroNER",
                    "program",
                    "#REF",
                    "provided",
                    "a",
                    "basic",
                    "architecture",
                    "and",
                    "influenced",
                    "multiple",
                    "developments",
                    "to",
                    "most",
                    "sequence",
                    "labeling",
                    "tasks,",
                    "including",
                    "event",
                    "#REF",
                    "."
                ],
                [
                    "The",
                    "task",
                    "of",
                    "event",
                    "extraction",
                    "in",
                    "any",
                    "language",
                    "involves",
                    "the",
                    "identification",
                    "of",
                    "the",
                    "event",
                    "nugget",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Prominent",
                    "work",
                    "has",
                    "been",
                    "done",
                    "to",
                    "analyze",
                    "the",
                    "lexical",
                    "and",
                    "semantic",
                    "features",
                    "of",
                    "event",
                    "representation",
                    "#REF",
                    ",",
                    "which",
                    "served",
                    "as",
                    "a",
                    "basis",
                    "for",
                    "neural",
                    "event",
                    "nugget",
                    "detection",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                0,
                0,
                0,
                1,
                3
            ]
        },
        "input": "sent0: Neural approaches to sequence tagging are common due to extensive developments in named entity recognition.\n sent1: #REF introduced and cultivated the use of bidirectional LSTMs to incorporate features that could be used for sequence tagging using a CRF.\n sent2: #REF 's architecture and the NeuroNER program #REF provided a basic architecture and influenced multiple developments to most sequence labeling tasks, including event #REF .\n sent3: The task of event extraction in any language involves the identification of the event nugget #TARGET_REF .\n sent4: Prominent work has been done to analyze the lexical and semantic features of event representation #REF , which served as a basis for neural event nugget detection #REF .\n",
        "output": "{\"INFORMATION\": [\"sent3\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Attack",
                    "success",
                    "rate",
                    "(first-order)."
                ],
                [
                    "We",
                    "quantify",
                    "first-order",
                    "robustness",
                    "through",
                    "attack",
                    "success",
                    "rate,",
                    "which",
                    "measures",
                    "the",
                    "ratio",
                    "of",
                    "test",
                    "examples",
                    "that",
                    "an",
                    "adversarial",
                    "example",
                    "can",
                    "be",
                    "found."
                ],
                [
                    "We",
                    "use",
                    "firstorder",
                    "attacks",
                    "as",
                    "a",
                    "reference",
                    "due",
                    "to",
                    "the",
                    "lack",
                    "of",
                    "a",
                    "direct",
                    "baseline."
                ],
                [
                    "We",
                    "experiment",
                    "with",
                    "two",
                    "black-box",
                    "attacks:",
                    "(1)",
                    "The",
                    "Genetic",
                    "attack",
                    "#TARGET_REF",
                    ")",
                    "uses",
                    "a",
                    "population-based",
                    "op-timization",
                    "algorithm",
                    "that",
                    "generates",
                    "both",
                    "syntactically",
                    "and",
                    "semantically",
                    "similar",
                    "adversarial",
                    "examples,",
                    "by",
                    "replacing",
                    "words",
                    "within",
                    "the",
                    "list",
                    "of",
                    "counterfitted",
                    "synonyms."
                ]
            ],
            "context": [
                0,
                0,
                0,
                1
            ]
        },
        "input": "sent0: Attack success rate (first-order).\n sent1: We quantify first-order robustness through attack success rate, which measures the ratio of test examples that an adversarial example can be found.\n sent2: We use firstorder attacks as a reference due to the lack of a direct baseline.\n sent3: We experiment with two black-box attacks: (1) The Genetic attack #TARGET_REF ) uses a population-based op-timization algorithm that generates both syntactically and semantically similar adversarial examples, by replacing words within the list of counterfitted synonyms.\n",
        "output": "{\"INFORMATION\": [\"sent3\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Designing",
                    "neural",
                    "diversity",
                    "metrics."
                ],
                [
                    "In",
                    "spite",
                    "of",
                    "growing",
                    "interest",
                    "in",
                    "NLG",
                    "models",
                    "that",
                    "produce",
                    "diverse",
                    "outputs,",
                    "there",
                    "is",
                    "currently",
                    "no",
                    "principled",
                    "neu-ral",
                    "method",
                    "for",
                    "evaluating",
                    "the",
                    "diversity",
                    "of",
                    "an",
                    "NLG",
                    "system."
                ],
                [
                    "As",
                    "described",
                    "in",
                    "#TARGET_REF",
                    ",",
                    "existing",
                    "automatic",
                    "diversity",
                    "metrics",
                    "(e.g."
                ],
                [
                    "Self-BLEU)",
                    "perform",
                    "worse",
                    "than",
                    "humans",
                    "on",
                    "the",
                    "task",
                    "of",
                    "estimating",
                    "content",
                    "diversity,",
                    "indicating",
                    "a",
                    "low",
                    "correlation",
                    "between",
                    "metrics",
                    "and",
                    "human",
                    "judgments."
                ]
            ],
            "context": [
                0,
                2,
                3,
                3
            ]
        },
        "input": "sent0: Designing neural diversity metrics.\n sent1: In spite of growing interest in NLG models that produce diverse outputs, there is currently no principled neu-ral method for evaluating the diversity of an NLG system.\n sent2: As described in #TARGET_REF , existing automatic diversity metrics (e.g.\n sent3: Self-BLEU) perform worse than humans on the task of estimating content diversity, indicating a low correlation between metrics and human judgments.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Linguistic",
                    "analysis",
                    "of",
                    "social",
                    "discourse,",
                    "using",
                    "digital",
                    "lexical",
                    "resources",
                    "and",
                    "related",
                    "software,",
                    "has",
                    "been",
                    "an",
                    "upward",
                    "trend",
                    "in",
                    "the",
                    "recent",
                    "past."
                ],
                [
                    "WordNet",
                    "has",
                    "been",
                    "used",
                    "for",
                    "marking",
                    "the",
                    "event",
                    "profile",
                    "of",
                    "news",
                    "articles",
                    "as",
                    "a",
                    "function",
                    "of",
                    "verb",
                    "type",
                    "#REF",
                    ")."
                ],
                [
                    "An",
                    "Adversary-Intent-Target",
                    "(AIT)",
                    "model",
                    "has",
                    "been",
                    "developed",
                    "which",
                    "is",
                    "based",
                    "on",
                    "an",
                    "Ontology",
                    "for",
                    "the",
                    "Analysis",
                    "of",
                    "Terrorist",
                    "Attacks",
                    "#REF",
                    "."
                ],
                [
                    "DICTION",
                    "5.0",
                    "text",
                    "analysis",
                    "master",
                    "variable,",
                    "CERTAINTY",
                    "has",
                    "been",
                    "used",
                    "to",
                    "analyze",
                    "top",
                    "management",
                    "language",
                    "for",
                    "signals",
                    "of",
                    "possible",
                    "deception",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                0,
                0,
                0,
                1
            ]
        },
        "input": "sent0: Linguistic analysis of social discourse, using digital lexical resources and related software, has been an upward trend in the recent past.\n sent1: WordNet has been used for marking the event profile of news articles as a function of verb type #REF ).\n sent2: An Adversary-Intent-Target (AIT) model has been developed which is based on an Ontology for the Analysis of Terrorist Attacks #REF .\n sent3: DICTION 5.0 text analysis master variable, CERTAINTY has been used to analyze top management language for signals of possible deception #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [\"sent3\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "A",
                    "research",
                    "of",
                    "already",
                    "available",
                    "and",
                    "accessible",
                    "language",
                    "processing",
                    "tools",
                    "and",
                    "materials,",
                    "mostly",
                    "corpora,",
                    "revealed",
                    "that",
                    "there",
                    "is",
                    "a",
                    "reasonably",
                    "big",
                    "amount",
                    "of",
                    "work",
                    "already",
                    "done",
                    "for",
                    "Slovenian",
                    "language,",
                    "less",
                    "for",
                    "Serbian."
                ],
                [
                    "The",
                    "tools",
                    "for",
                    "Slovenian",
                    "language",
                    "are",
                    "(reasonable",
                    "or",
                    "even",
                    "good",
                    "quality):",
                    "part",
                    "of",
                    "speech",
                    "tagger",
                    "#TARGET_REF",
                    "and",
                    "#REF",
                    ",",
                    "lemmatizer",
                    "#REF",
                    "and",
                    "#REF",
                    ",",
                    "stemmer",
                    "#REF",
                    "and",
                    "#REF",
                    ",",
                    "none",
                    "of",
                    "these",
                    "tools",
                    "exists",
                    "for",
                    "Serbian",
                    "language."
                ],
                [
                    "Both",
                    "languages",
                    "have",
                    "solid",
                    "monolingual",
                    "reference",
                    "corpora",
                    "(going",
                    "into",
                    "hundreds",
                    "of",
                    "millions)",
                    "and",
                    "a",
                    "small",
                    "bilingual",
                    "corpus",
                    "#REF",
                    ")",
                    "that",
                    "was",
                    "used",
                    "mostly",
                    "for",
                    "evaluation",
                    "purposes."
                ]
            ],
            "context": [
                3,
                2,
                0
            ]
        },
        "input": "sent0: A research of already available and accessible language processing tools and materials, mostly corpora, revealed that there is a reasonably big amount of work already done for Slovenian language, less for Serbian.\n sent1: The tools for Slovenian language are (reasonable or even good quality): part of speech tagger #TARGET_REF and #REF , lemmatizer #REF and #REF , stemmer #REF and #REF , none of these tools exists for Serbian language.\n sent2: Both languages have solid monolingual reference corpora (going into hundreds of millions) and a small bilingual corpus #REF ) that was used mostly for evaluation purposes.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Inference",
                    "In",
                    "our",
                    "implementation,",
                    "we",
                    "use",
                    "FAISS",
                    "#TARGET_REF",
                    "to",
                    "index",
                    "the",
                    "dense",
                    "representations",
                    "of",
                    "all",
                    "passages."
                ],
                [
                    "Specifically,",
                    "we",
                    "use",
                    "IndexFlatIP",
                    "for",
                    "indexing",
                    "and",
                    "the",
                    "exact",
                    "maximum",
                    "inner",
                    "product",
                    "search",
                    "for",
                    "querying."
                ]
            ],
            "context": [
                2,
                0
            ]
        },
        "input": "sent0: Inference In our implementation, we use FAISS #TARGET_REF to index the dense representations of all passages.\n sent1: Specifically, we use IndexFlatIP for indexing and the exact maximum inner product search for querying.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "hypothesize",
                    "that",
                    "any",
                    "effects",
                    "of",
                    "word",
                    "alignment",
                    "visualization",
                    "on",
                    "post-editing",
                    "may",
                    "be",
                    "dependent",
                    "on",
                    "the",
                    "quality",
                    "of",
                    "the",
                    "underlying",
                    "machine",
                    "translations",
                    "displayed",
                    "to",
                    "the",
                    "post-editors."
                ],
                [
                    "Because",
                    "we",
                    "care",
                    "about",
                    "the",
                    "adequacy",
                    "of",
                    "post-edited",
                    "translations,",
                    "we",
                    "consider",
                    "actual",
                    "human",
                    "judgements",
                    "to",
                    "be",
                    "preferable",
                    "to",
                    "automated",
                    "metrics",
                    "such",
                    "as",
                    "BLEU",
                    "#REF",
                    ",",
                    "which",
                    "at",
                    "best",
                    "serve",
                    "as",
                    "a",
                    "flawed",
                    "proxy",
                    "for",
                    "human",
                    "judgements."
                ],
                [
                    "Instead,",
                    "following",
                    "#REF",
                    "and",
                    "#TARGET_REF",
                    ",",
                    "we",
                    "therefore",
                    "obtained",
                    "human",
                    "judgements",
                    "of",
                    "translation",
                    "adequacy",
                    "for",
                    "the",
                    "Russian-English",
                    "and",
                    "Spanish-English",
                    "machine",
                    "translations",
                    "used",
                    "in",
                    "this",
                    "study."
                ]
            ],
            "context": [
                0,
                2,
                3
            ]
        },
        "input": "sent0: We hypothesize that any effects of word alignment visualization on post-editing may be dependent on the quality of the underlying machine translations displayed to the post-editors.\n sent1: Because we care about the adequacy of post-edited translations, we consider actual human judgements to be preferable to automated metrics such as BLEU #REF , which at best serve as a flawed proxy for human judgements.\n sent2: Instead, following #REF and #TARGET_REF , we therefore obtained human judgements of translation adequacy for the Russian-English and Spanish-English machine translations used in this study.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Suffix",
                    "arrays",
                    "were",
                    "introduced",
                    "by",
                    "Manber",
                    "and",
                    "Myers",
                    "who",
                    "gave",
                    "a",
                    "Θ(N",
                    "log",
                    "N)",
                    "construction",
                    "algorithm",
                    "#REF",
                    "."
                ],
                [
                    "While",
                    "several",
                    "linear",
                    "time",
                    "suffix",
                    "array",
                    "construction",
                    "algorithms",
                    "have",
                    "now",
                    "been",
                    "introduced",
                    "#TARGET_REF",
                    "it",
                    "is",
                    "not",
                    "clear",
                    "that",
                    "their",
                    "asymptotic",
                    "gains",
                    "make",
                    "them",
                    "a",
                    "better",
                    "choice",
                    "than",
                    "well-tuned",
                    "supralinear",
                    "methods",
                    "on",
                    "corpora",
                    "of",
                    "interest",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                3,
                2
            ]
        },
        "input": "sent0: Suffix arrays were introduced by Manber and Myers who gave a Θ(N log N) construction algorithm #REF .\n sent1: While several linear time suffix array construction algorithms have now been introduced #TARGET_REF it is not clear that their asymptotic gains make them a better choice than well-tuned supralinear methods on corpora of interest #REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "results",
                    "in",
                    "Table",
                    "6",
                    "show",
                    "that",
                    "our",
                    "best",
                    "model",
                    "beats",
                    "all",
                    "available",
                    "previous",
                    "scores",
                    "on",
                    "the",
                    "English",
                    "PMB",
                    "3.0.0",
                    "test",
                    "set",
                    "except",
                    "for",
                    "Pro",
                    "#REF",
                    "and",
                    "is",
                    "also",
                    "very",
                    "competitive",
                    "on",
                    "the",
                    "dev",
                    "set."
                ],
                [
                    "Its",
                    "difference",
                    "with",
                    "the",
                    "state-of-theart",
                    "model",
                    "on",
                    "the",
                    "test",
                    "set",
                    "is",
                    "within",
                    "1%."
                ],
                [
                    "Compared",
                    "with",
                    "the",
                    "best",
                    "previous",
                    "fully",
                    "trainable",
                    "compositional",
                    "model",
                    "in",
                    "#TARGET_REF",
                    "2020),",
                    "shown",
                    "in",
                    "Table",
                    "7."
                ]
            ],
            "context": [
                0,
                0,
                1
            ]
        },
        "input": "sent0: The results in Table 6 show that our best model beats all available previous scores on the English PMB 3.0.0 test set except for Pro #REF and is also very competitive on the dev set.\n sent1: Its difference with the state-of-theart model on the test set is within 1%.\n sent2: Compared with the best previous fully trainable compositional model in #TARGET_REF 2020), shown in Table 7.\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Furthermore,",
                    "we",
                    "extend",
                    "the",
                    "double",
                    "perturbation",
                    "framework",
                    "to",
                    "evaluate",
                    "counterfactual",
                    "biases",
                    "#TARGET_REF",
                    ")",
                    "(",
                    "§4)",
                    "in",
                    "English."
                ],
                [
                    "When",
                    "the",
                    "test",
                    "dataset",
                    "is",
                    "small,",
                    "our",
                    "framework",
                    "can",
                    "help",
                    "improve",
                    "the",
                    "evaluation",
                    "robustness",
                    "by",
                    "revealing",
                    "the",
                    "hidden",
                    "biases",
                    "not",
                    "directly",
                    "shown",
                    "in",
                    "the",
                    "test",
                    "dataset."
                ],
                [
                    "Intuitively,",
                    "a",
                    "fair",
                    "model",
                    "should",
                    "make",
                    "the",
                    "same",
                    "prediction",
                    "for",
                    "nearly",
                    "identical",
                    "examples",
                    "referencing",
                    "different",
                    "groups",
                    "#REF",
                    "with",
                    "different",
                    "protected",
                    "attributes",
                    "(e.g.,",
                    "gender,",
                    "race)."
                ],
                [
                    "In",
                    "our",
                    "evaluation,",
                    "we",
                    "consider",
                    "a",
                    "model",
                    "biased",
                    "if",
                    "substituting",
                    "tokens",
                    "associated",
                    "with",
                    "protected",
                    "attributes",
                    "changes",
                    "the",
                    "expected",
                    "prediction,",
                    "which",
                    "is",
                    "the",
                    "average",
                    "prediction",
                    "among",
                    "all",
                    "examples",
                    "within",
                    "the",
                    "neighborhood."
                ],
                [
                    "For",
                    "instance,",
                    "a",
                    "toxicity",
                    "classifier",
                    "is",
                    "biased",
                    "if",
                    "it",
                    "tends",
                    "to",
                    "increase",
                    "the",
                    "toxicity",
                    "if",
                    "we",
                    "substitute",
                    "straight",
                    "→",
                    "gay",
                    "in",
                    "an",
                    "input",
                    "sentence",
                    "#REF",
                    "."
                ],
                [
                    "In",
                    "the",
                    "experiments,",
                    "we",
                    "evaluate",
                    "the",
                    "expected",
                    "sentiment",
                    "predictions",
                    "on",
                    "pairs",
                    "of",
                    "protected",
                    "tokens",
                    "(e.g.,",
                    "(he,",
                    "she),",
                    "(gay,",
                    "straight)),",
                    "and",
                    "demonstrate",
                    "that",
                    "our",
                    "method",
                    "is",
                    "able",
                    "to",
                    "reveal",
                    "the",
                    "hidden",
                    "model",
                    "biases."
                ]
            ],
            "context": [
                2,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Furthermore, we extend the double perturbation framework to evaluate counterfactual biases #TARGET_REF ) ( §4) in English.\n sent1: When the test dataset is small, our framework can help improve the evaluation robustness by revealing the hidden biases not directly shown in the test dataset.\n sent2: Intuitively, a fair model should make the same prediction for nearly identical examples referencing different groups #REF with different protected attributes (e.g., gender, race).\n sent3: In our evaluation, we consider a model biased if substituting tokens associated with protected attributes changes the expected prediction, which is the average prediction among all examples within the neighborhood.\n sent4: For instance, a toxicity classifier is biased if it tends to increase the toxicity if we substitute straight → gay in an input sentence #REF .\n sent5: In the experiments, we evaluate the expected sentiment predictions on pairs of protected tokens (e.g., (he, she), (gay, straight)), and demonstrate that our method is able to reveal the hidden model biases.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "this",
                    "paper,",
                    "we",
                    "propose",
                    "to",
                    "use",
                    "the",
                    "so-called",
                    "continuous",
                    "space",
                    "language",
                    "model."
                ],
                [
                    "The",
                    "basic",
                    "idea",
                    "of",
                    "this",
                    "approach",
                    "is",
                    "to",
                    "project",
                    "the",
                    "word",
                    "indices",
                    "onto",
                    "a",
                    "continuous",
                    "space",
                    "and",
                    "to",
                    "use",
                    "a",
                    "probability",
                    "estimator",
                    "operating",
                    "on",
                    "this",
                    "space",
                    "#REF",
                    "."
                ],
                [
                    "Since",
                    "the",
                    "resulting",
                    "probability",
                    "functions",
                    "are",
                    "smooth",
                    "functions",
                    "of",
                    "the",
                    "word",
                    "representation,",
                    "better",
                    "generalization",
                    "to",
                    "unknown",
                    "n-grams",
                    "can",
                    "be",
                    "expected."
                ],
                [
                    "A",
                    "neural",
                    "network",
                    "can",
                    "be",
                    "used",
                    "to",
                    "simultaneously",
                    "learn",
                    "the",
                    "projection",
                    "of",
                    "the",
                    "words",
                    "onto",
                    "the",
                    "continuous",
                    "space",
                    "and",
                    "to",
                    "estimate",
                    "the",
                    "n-gram",
                    "probabilities."
                ],
                [
                    "This",
                    "is",
                    "still",
                    "a",
                    "n-gram",
                    "approach,",
                    "but",
                    "the",
                    "language",
                    "model",
                    "posterior",
                    "probabilities",
                    "are",
                    "\"interpolated\"",
                    "for",
                    "any",
                    "possible",
                    "context",
                    "of",
                    "length",
                    "n",
                    "−",
                    "1",
                    "instead",
                    "of",
                    "backing-off",
                    "to",
                    "shorter",
                    "contexts."
                ],
                [
                    "This",
                    "approach",
                    "was",
                    "already",
                    "successfully",
                    "applied",
                    "in",
                    "statistical",
                    "machine",
                    "translation",
                    "systems,",
                    "ranging",
                    "from",
                    "small",
                    "IWSLT",
                    "systems",
                    "#TARGET_REF",
                    "to",
                    "large",
                    "NIST",
                    "systems",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                2,
                3,
                0,
                3,
                3,
                2
            ]
        },
        "input": "sent0: In this paper, we propose to use the so-called continuous space language model.\n sent1: The basic idea of this approach is to project the word indices onto a continuous space and to use a probability estimator operating on this space #REF .\n sent2: Since the resulting probability functions are smooth functions of the word representation, better generalization to unknown n-grams can be expected.\n sent3: A neural network can be used to simultaneously learn the projection of the words onto the continuous space and to estimate the n-gram probabilities.\n sent4: This is still a n-gram approach, but the language model posterior probabilities are \"interpolated\" for any possible context of length n − 1 instead of backing-off to shorter contexts.\n sent5: This approach was already successfully applied in statistical machine translation systems, ranging from small IWSLT systems #TARGET_REF to large NIST systems #REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\", \"sent5\"], \"BACKGROUND\": [\"sent1\", \"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "this",
                    "section,",
                    "we",
                    "benchmark",
                    "top-k",
                    "attention",
                    "in",
                    "terms",
                    "of",
                    "time",
                    "and",
                    "memory,",
                    "and",
                    "compare",
                    "it",
                    "to",
                    "vanilla",
                    "attention,",
                    "query-chunking",
                    "without",
                    "the",
                    "top-k",
                    "operation,",
                    "and",
                    "to",
                    "Performer",
                    "#TARGET_REF",
                    ",",
                    "as",
                    "a",
                    "representative",
                    "of",
                    "state-of-the-art",
                    "linear",
                    "attention",
                    "variants."
                ],
                [
                    "We",
                    "separately",
                    "benchmark",
                    "(a)",
                    "a",
                    "single",
                    "self-attention",
                    "layer",
                    "over",
                    "long",
                    "sequences,",
                    "(b)",
                    "a",
                    "single",
                    "feed-forward",
                    "layer",
                    "with",
                    "a",
                    "large",
                    "feedforward",
                    "dimension,",
                    "and",
                    "(c)",
                    "a",
                    "12-layer",
                    "Transformer",
                    "decoder",
                    "with",
                    "same",
                    "architecture",
                    "as",
                    "BERT-base",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                2,
                0
            ]
        },
        "input": "sent0: In this section, we benchmark top-k attention in terms of time and memory, and compare it to vanilla attention, query-chunking without the top-k operation, and to Performer #TARGET_REF , as a representative of state-of-the-art linear attention variants.\n sent1: We separately benchmark (a) a single self-attention layer over long sequences, (b) a single feed-forward layer with a large feedforward dimension, and (c) a 12-layer Transformer decoder with same architecture as BERT-base #REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "At",
                    "bottom,",
                    "she",
                    "believed",
                    "that",
                    "such",
                    "interlinguas",
                    "were",
                    "in",
                    "need",
                    "of",
                    "some",
                    "form",
                    "of",
                    "empirical",
                    "justification",
                    "and",
                    "could",
                    "not",
                    "be",
                    "treated",
                    "as",
                    "unprovable",
                    "and",
                    "arbitrary",
                    "assumptions",
                    "for",
                    "a",
                    "system,",
                    "in",
                    "the",
                    "way",
                    "#TARGET_REF",
                    "has",
                    "tried",
                    "to",
                    "do",
                    "by",
                    "arguing",
                    "from",
                    "the",
                    "role",
                    "of",
                    "assumed",
                    "entities",
                    "in",
                    "physics",
                    "and",
                    "mathematics."
                ],
                [
                    "There",
                    "was",
                    "one",
                    "weak",
                    "form",
                    "of",
                    "empirical",
                    "support",
                    "available:",
                    "statistics",
                    "derived",
                    "from",
                    "dictionaries",
                    "showed",
                    "that",
                    "the",
                    "first",
                    "100",
                    "commonest",
                    "defining",
                    "words",
                    "in",
                    "English",
                    "dictionaries",
                    "(exempting",
                    "'a'",
                    "and",
                    "'the')",
                    "corresponded",
                    "very",
                    "closely",
                    "indeed",
                    "to",
                    "the",
                    "primitives",
                    "of",
                    "NUDE."
                ],
                [
                    "But",
                    "MMB",
                    "wanted",
                    "something",
                    "more",
                    "structural",
                    "and",
                    "spent",
                    "some",
                    "considerable",
                    "time",
                    "trying",
                    "to",
                    "associate",
                    "the",
                    "NUDE",
                    "elements",
                    "with",
                    "the",
                    "classifying",
                    "principles",
                    "of",
                    "the",
                    "thesaurus",
                    "itself,",
                    "which",
                    "would",
                    "then",
                    "link",
                    "back",
                    "to",
                    "the",
                    "distributional",
                    "facts",
                    "about",
                    "texts",
                    "that",
                    "the",
                    "thesaurus",
                    "itself",
                    "represented."
                ],
                [
                    "In",
                    "this,",
                    "as",
                    "in",
                    "other",
                    "ways,",
                    "MMB",
                    "had",
                    "more",
                    "intuitive",
                    "sympathy",
                    "with",
                    "earlier",
                    "distributional",
                    "or",
                    "structural",
                    "linguistics",
                    "than",
                    "with",
                    "the",
                    "more",
                    "apparently",
                    "mathematical",
                    "and",
                    "symbolic",
                    "linguistics",
                    "of",
                    "Chomsky",
                    "and",
                    "his",
                    "followers."
                ]
            ],
            "context": [
                2,
                0,
                0,
                0
            ]
        },
        "input": "sent0: At bottom, she believed that such interlinguas were in need of some form of empirical justification and could not be treated as unprovable and arbitrary assumptions for a system, in the way #TARGET_REF has tried to do by arguing from the role of assumed entities in physics and mathematics.\n sent1: There was one weak form of empirical support available: statistics derived from dictionaries showed that the first 100 commonest defining words in English dictionaries (exempting 'a' and 'the') corresponded very closely indeed to the primitives of NUDE.\n sent2: But MMB wanted something more structural and spent some considerable time trying to associate the NUDE elements with the classifying principles of the thesaurus itself, which would then link back to the distributional facts about texts that the thesaurus itself represented.\n sent3: In this, as in other ways, MMB had more intuitive sympathy with earlier distributional or structural linguistics than with the more apparently mathematical and symbolic linguistics of Chomsky and his followers.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Feed-forward",
                    "as",
                    "attention",
                    "In",
                    "the",
                    "feed-forward",
                    "layer,",
                    "a",
                    "1-hidden",
                    "layer",
                    "fully-connected",
                    "network",
                    "is",
                    "applied",
                    "identically",
                    "to",
                    "every",
                    "input",
                    "token."
                ],
                [
                    "As",
                    "observed",
                    "in",
                    "past",
                    "work",
                    "#TARGET_REF",
                    ",",
                    "a",
                    "feed-forward",
                    "layer",
                    "can",
                    "be",
                    "cast",
                    "into",
                    "the",
                    "query-key-value",
                    "framework",
                    "as:"
                ]
            ],
            "context": [
                0,
                1
            ]
        },
        "input": "sent0: Feed-forward as attention In the feed-forward layer, a 1-hidden layer fully-connected network is applied identically to every input token.\n sent1: As observed in past work #TARGET_REF , a feed-forward layer can be cast into the query-key-value framework as:\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Our",
                    "post-editing",
                    "interface",
                    "can",
                    "be",
                    "seen",
                    "in",
                    "Figure",
                    "1",
                    "above."
                ],
                [
                    "Each",
                    "text",
                    "was",
                    "presented",
                    "to",
                    "posteditors",
                    "in",
                    "one",
                    "of",
                    "two",
                    "variant",
                    "modalities",
                    "-word-level",
                    "alignment",
                    "links",
                    "could",
                    "either",
                    "be",
                    "visualized",
                    "or",
                    "left",
                    "absent."
                ],
                [
                    "In",
                    "both",
                    "variants,",
                    "each",
                    "source",
                    "language",
                    "segment",
                    "was",
                    "presented",
                    "along",
                    "with",
                    "the",
                    "corresponding",
                    "machine",
                    "translated",
                    "English",
                    "segment,",
                    "a",
                    "text",
                    "field",
                    "(initially",
                    "populated",
                    "with",
                    "the",
                    "machine",
                    "translated",
                    "segment)",
                    "where",
                    "the",
                    "post-editor",
                    "could",
                    "make",
                    "changes",
                    "was",
                    "also",
                    "presented."
                ],
                [
                    "In",
                    "the",
                    "first",
                    "variant,",
                    "the",
                    "word-level",
                    "alignment",
                    "links",
                    "produced",
                    "by",
                    "the",
                    "machine",
                    "translation",
                    "decoder",
                    "(Moses",
                    "for",
                    "Russian-English,",
                    "Bing",
                    "Translator",
                    "for",
                    "Spanish-English)",
                    "were",
                    "graphically",
                    "displayed,",
                    "linking",
                    "source",
                    "words",
                    "to",
                    "their",
                    "corresponding",
                    "machine",
                    "translated",
                    "target",
                    "words."
                ],
                [
                    "In",
                    "the",
                    "second",
                    "variant,",
                    "the",
                    "word-level",
                    "alignment",
                    "links",
                    "were",
                    "omitted",
                    "from",
                    "the",
                    "visualization",
                    "interface."
                ],
                [
                    "Figure",
                    "2:",
                    "Percentage",
                    "of",
                    "segments",
                    "judged",
                    "to",
                    "be",
                    "in",
                    "each",
                    "adequacy",
                    "category."
                ],
                [
                    "For",
                    "each",
                    "language",
                    "pair,",
                    "we",
                    "report",
                    "percentages",
                    "for",
                    "raw",
                    "(unedited)",
                    "machine",
                    "translation",
                    "output,",
                    "as",
                    "well",
                    "as",
                    "output",
                    "postedited",
                    "by",
                    "a",
                    "bilingual",
                    "post-editor",
                    "with",
                    "access",
                    "to",
                    "alignments",
                    "and",
                    "without",
                    "access",
                    "to",
                    "alignments."
                ],
                [
                    "For",
                    "Russian-English,",
                    "we",
                    "additionally",
                    "report",
                    "percentages",
                    "for",
                    "output",
                    "post-edited",
                    "by",
                    "a",
                    "monolingual",
                    "post-editor",
                    "#TARGET_REF",
                    "with",
                    "access",
                    "to",
                    "alignments."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                3
            ]
        },
        "input": "sent0: Our post-editing interface can be seen in Figure 1 above.\n sent1: Each text was presented to posteditors in one of two variant modalities -word-level alignment links could either be visualized or left absent.\n sent2: In both variants, each source language segment was presented along with the corresponding machine translated English segment, a text field (initially populated with the machine translated segment) where the post-editor could make changes was also presented.\n sent3: In the first variant, the word-level alignment links produced by the machine translation decoder (Moses for Russian-English, Bing Translator for Spanish-English) were graphically displayed, linking source words to their corresponding machine translated target words.\n sent4: In the second variant, the word-level alignment links were omitted from the visualization interface.\n sent5: Figure 2: Percentage of segments judged to be in each adequacy category.\n sent6: For each language pair, we report percentages for raw (unedited) machine translation output, as well as output postedited by a bilingual post-editor with access to alignments and without access to alignments.\n sent7: For Russian-English, we additionally report percentages for output post-edited by a monolingual post-editor #TARGET_REF with access to alignments.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent7\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Supervised",
                    "systems",
                    "Supervised",
                    "systems",
                    "rely",
                    "on",
                    "semantically-annotated",
                    "corpora",
                    "for",
                    "training",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Early",
                    "approaches",
                    "were",
                    "based",
                    "on",
                    "traditional",
                    "machine",
                    "learning",
                    "algorithms",
                    "like",
                    "support",
                    "vector",
                    "machines",
                    "#REF",
                    "."
                ],
                [
                    "With",
                    "the",
                    "word",
                    "embedding-based",
                    "approaches",
                    "getting",
                    "popular",
                    "in",
                    "natural",
                    "language",
                    "processing",
                    "tasks,",
                    "more",
                    "recent",
                    "approaches",
                    "on",
                    "WSD",
                    "were",
                    "based",
                    "on",
                    "neural",
                    "network",
                    "architectures",
                    "#REF",
                    "."
                ],
                [
                    "However,",
                    "they",
                    "rely",
                    "on",
                    "large",
                    "manuallycurated",
                    "training",
                    "data",
                    "to",
                    "train",
                    "the",
                    "machine",
                    "learning",
                    "models",
                    "which",
                    "in",
                    "turn",
                    "hinders",
                    "the",
                    "ability",
                    "of",
                    "these",
                    "approaches",
                    "to",
                    "scale",
                    "over",
                    "unseen",
                    "words",
                    "and",
                    "new",
                    "languages."
                ],
                [
                    "More",
                    "recently,",
                    "contextual",
                    "representations",
                    "of",
                    "words",
                    "have",
                    "been",
                    "used",
                    "in",
                    "WSD",
                    "where",
                    "the",
                    "contextual",
                    "representations",
                    "have",
                    "been",
                    "employed",
                    "for",
                    "the",
                    "creation",
                    "of",
                    "sense",
                    "embeddings",
                    "#REF",
                    "."
                ],
                [
                    "However,",
                    "they",
                    "also",
                    "rely",
                    "on",
                    "sense-annotated",
                    "corpora",
                    "to",
                    "gather",
                    "contextual",
                    "information",
                    "for",
                    "each",
                    "sense,",
                    "and",
                    "hence",
                    "are",
                    "limited",
                    "to",
                    "languages",
                    "for",
                    "which",
                    "gold",
                    "annotations",
                    "are",
                    "available."
                ],
                [
                    "A",
                    "very",
                    "recent",
                    "approach",
                    "SensEmBERT",
                    "#REF",
                    "provide",
                    "WSD",
                    "by",
                    "leveraging",
                    "the",
                    "mapping",
                    "between",
                    "senses",
                    "and",
                    "Wikipedia",
                    "pages,",
                    "the",
                    "relations",
                    "among",
                    "BabelNet",
                    "synsets",
                    "and",
                    "the",
                    "expressiveness",
                    "of",
                    "contextualised",
                    "embeddings,",
                    "getting",
                    "rid",
                    "of",
                    "manual",
                    "annotations."
                ],
                [
                    "However,",
                    "SensEmBERT",
                    "#REF",
                    "only",
                    "supports",
                    "five",
                    "languages",
                    "making",
                    "it",
                    "difficult",
                    "to",
                    "use",
                    "with",
                    "other",
                    "languages."
                ]
            ],
            "context": [
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Supervised systems Supervised systems rely on semantically-annotated corpora for training #TARGET_REF .\n sent1: Early approaches were based on traditional machine learning algorithms like support vector machines #REF .\n sent2: With the word embedding-based approaches getting popular in natural language processing tasks, more recent approaches on WSD were based on neural network architectures #REF .\n sent3: However, they rely on large manuallycurated training data to train the machine learning models which in turn hinders the ability of these approaches to scale over unseen words and new languages.\n sent4: More recently, contextual representations of words have been used in WSD where the contextual representations have been employed for the creation of sense embeddings #REF .\n sent5: However, they also rely on sense-annotated corpora to gather contextual information for each sense, and hence are limited to languages for which gold annotations are available.\n sent6: A very recent approach SensEmBERT #REF provide WSD by leveraging the mapping between senses and Wikipedia pages, the relations among BabelNet synsets and the expressiveness of contextualised embeddings, getting rid of manual annotations.\n sent7: However, SensEmBERT #REF only supports five languages making it difficult to use with other languages.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "An",
                    "important",
                    "note",
                    "before",
                    "continuing:",
                    "when",
                    "I",
                    "refer",
                    "to",
                    "IBM",
                    "machine",
                    "translation",
                    "I",
                    "mean",
                    "only",
                    "the",
                    "systems",
                    "referred",
                    "to",
                    "at",
                    "the",
                    "end",
                    "by",
                    "Brown",
                    "et",
                    "al."
                ],
                [
                    "IBM",
                    "as",
                    "a",
                    "whole",
                    "supports",
                    "many",
                    "approaches",
                    "to",
                    "MT,",
                    "including",
                    "#TARGET_REF",
                    "Prolog-based",
                    "symbolic",
                    "approach,",
                    "as",
                    "well",
                    "as",
                    "symbolic",
                    "systems",
                    "in",
                    "Germany",
                    "and",
                    "Japan."
                ]
            ],
            "context": [
                2,
                2
            ]
        },
        "input": "sent0: An important note before continuing: when I refer to IBM machine translation I mean only the systems referred to at the end by Brown et al.\n sent1: IBM as a whole supports many approaches to MT, including #TARGET_REF Prolog-based symbolic approach, as well as symbolic systems in Germany and Japan.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\", \"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Given",
                    "a",
                    "question",
                    "for",
                    "an",
                    "image,",
                    "VCR",
                    "needs",
                    "to",
                    "1.)"
                ],
                [
                    "correctly",
                    "answer",
                    "(Q→A),",
                    "2.)"
                ],
                [
                    "provide",
                    "a",
                    "rationale",
                    "justifying",
                    "its",
                    "answer",
                    "(QA→R),",
                    "3.)"
                ],
                [
                    "reason",
                    "both",
                    "of",
                    "them",
                    "(Q→AR),",
                    "which",
                    "requires",
                    "higher-order",
                    "cognition",
                    "and",
                    "commonsense",
                    "reasoning",
                    "about",
                    "the",
                    "world."
                ],
                [
                    "Following",
                    "UNITER,",
                    "we",
                    "introduce",
                    "a",
                    "second-stage",
                    "pretraining",
                    "over",
                    "the",
                    "VCR",
                    "dataset",
                    "due",
                    "to",
                    "severe",
                    "difference",
                    "in",
                    "dataset",
                    "distribution",
                    "compared",
                    "to",
                    "indomain",
                    "image-text",
                    "corpus."
                ],
                [
                    "In",
                    "addition,",
                    "we",
                    "also",
                    "utilize",
                    "a",
                    "similar",
                    "person",
                    "grounding",
                    "#TARGET_REF",
                    "pretext",
                    "task",
                    "to",
                    "tightly",
                    "align",
                    "the",
                    "person",
                    "tags",
                    "in",
                    "text",
                    "and",
                    "their",
                    "visual",
                    "locations."
                ],
                [
                    "During",
                    "finetuning",
                    "stage,",
                    "we",
                    "concatenate",
                    "each",
                    "question",
                    "along",
                    "with",
                    "each",
                    "possible",
                    "answer",
                    "to",
                    "form",
                    "four",
                    "kinds",
                    "of",
                    "text",
                    "inputs,",
                    "and",
                    "feed",
                    "each",
                    "of",
                    "them",
                    "into",
                    "Transformer",
                    "network",
                    "with",
                    "corresponding",
                    "image",
                    "embeddings."
                ],
                [
                    "Finally,",
                    "a",
                    "binary",
                    "cross-entropy",
                    "loss",
                    "is",
                    "adopted",
                    "to",
                    "supervise",
                    "each",
                    "pair."
                ],
                [
                    "Since",
                    "VCR",
                    "questions",
                    "explicitly",
                    "reference",
                    "objects",
                    "at",
                    "specific",
                    "locations,",
                    "we",
                    "implement",
                    "coreferencing",
                    "between",
                    "text",
                    "and",
                    "image",
                    "by",
                    "replacing",
                    "referenced",
                    "entities",
                    "in",
                    "the",
                    "questions",
                    "with",
                    "their",
                    "corresponding",
                    "box",
                    "locations."
                ],
                [
                    "In",
                    "the",
                    "second",
                    "stage",
                    "pretraining",
                    "for",
                    "VCR,",
                    "we",
                    "reduce",
                    "the",
                    "learning",
                    "rate",
                    "to",
                    "a",
                    "constant",
                    "5e-05",
                    "and",
                    "trained",
                    "for",
                    "an",
                    "additional",
                    "9K",
                    "steps."
                ],
                [
                    "Due",
                    "to",
                    "longer",
                    "sequence",
                    "lengths",
                    "in",
                    "the",
                    "VCR",
                    "dataset,",
                    "a",
                    "training",
                    "batch-size",
                    "of",
                    "224",
                    "is",
                    "used."
                ],
                [
                    "We",
                    "also",
                    "use",
                    "a",
                    "step",
                    "size",
                    "of",
                    "2",
                    "for",
                    "gradient",
                    "accumulation."
                ],
                [
                    "After",
                    "pretraining,",
                    "we",
                    "finetuned",
                    "on",
                    "the",
                    "VCR",
                    "task",
                    "for",
                    "10K",
                    "steps",
                    "with",
                    "a",
                    "learning",
                    "rate",
                    "of",
                    "1e-04",
                    "for",
                    "both",
                    "the",
                    "Transformer",
                    "and",
                    "the",
                    "CNNs."
                ],
                [
                    "Linear",
                    "warmup",
                    "of",
                    "the",
                    "learning",
                    "rate",
                    "is",
                    "applied",
                    "for",
                    "1000",
                    "steps,",
                    "followed",
                    "by",
                    "a",
                    "linear",
                    "decay",
                    "ending",
                    "at",
                    "a",
                    "total",
                    "of",
                    "10K",
                    "steps."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Given a question for an image, VCR needs to 1.)\n sent1: correctly answer (Q→A), 2.)\n sent2: provide a rationale justifying its answer (QA→R), 3.)\n sent3: reason both of them (Q→AR), which requires higher-order cognition and commonsense reasoning about the world.\n sent4: Following UNITER, we introduce a second-stage pretraining over the VCR dataset due to severe difference in dataset distribution compared to indomain image-text corpus.\n sent5: In addition, we also utilize a similar person grounding #TARGET_REF pretext task to tightly align the person tags in text and their visual locations.\n sent6: During finetuning stage, we concatenate each question along with each possible answer to form four kinds of text inputs, and feed each of them into Transformer network with corresponding image embeddings.\n sent7: Finally, a binary cross-entropy loss is adopted to supervise each pair.\n sent8: Since VCR questions explicitly reference objects at specific locations, we implement coreferencing between text and image by replacing referenced entities in the questions with their corresponding box locations.\n sent9: In the second stage pretraining for VCR, we reduce the learning rate to a constant 5e-05 and trained for an additional 9K steps.\n sent10: Due to longer sequence lengths in the VCR dataset, a training batch-size of 224 is used.\n sent11: We also use a step size of 2 for gradient accumulation.\n sent12: After pretraining, we finetuned on the VCR task for 10K steps with a learning rate of 1e-04 for both the Transformer and the CNNs.\n sent13: Linear warmup of the learning rate is applied for 1000 steps, followed by a linear decay ending at a total of 10K steps.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent5\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "becomes",
                    "larger."
                ],
                [
                    "After",
                    "a",
                    "certain",
                    "point,",
                    "the",
                    "model",
                    "performance",
                    "starts",
                    "to",
                    "drop,",
                    "since",
                    "a",
                    "large",
                    "batch",
                    "size",
                    "may",
                    "bring",
                    "difficulty",
                    "for",
                    "optimization",
                    "on",
                    "training",
                    "data",
                    "with",
                    "limited",
                    "size."
                ],
                [
                    "We",
                    "say",
                    "that",
                    "there",
                    "should",
                    "be",
                    "a",
                    "balance",
                    "between",
                    "the",
                    "batch",
                    "size",
                    "and",
                    "the",
                    "number",
                    "of",
                    "negatives."
                ],
                [
                    "When",
                    "increasing",
                    "the",
                    "batch",
                    "size,",
                    "we",
                    "will",
                    "have",
                    "more",
                    "negatives",
                    "for",
                    "each",
                    "question."
                ],
                [
                    "However,",
                    "when",
                    "the",
                    "size",
                    "of",
                    "training",
                    "data",
                    "is",
                    "limited,",
                    "a",
                    "large",
                    "batch",
                    "size",
                    "will",
                    "bring",
                    "difficulty",
                    "for",
                    "optimization."
                ],
                [
                    "Second,",
                    "we",
                    "examine",
                    "the",
                    "effect",
                    "of",
                    "denoised",
                    "hard",
                    "negatives",
                    "from",
                    "the",
                    "top-k",
                    "passages",
                    "retrieved",
                    "by",
                    "the",
                    "dense",
                    "retriever."
                ],
                [
                    "As",
                    "shown",
                    "in",
                    "the",
                    "third",
                    "row",
                    "in",
                    "Table",
                    "3,",
                    "the",
                    "performance",
                    "of",
                    "the",
                    "retriever",
                    "significantly",
                    "decreases",
                    "by",
                    "introducing",
                    "hard",
                    "negatives",
                    "without",
                    "denoising."
                ],
                [
                    "We",
                    "speculate",
                    "that",
                    "it",
                    "is",
                    "caused",
                    "by",
                    "the",
                    "fact",
                    "that",
                    "there",
                    "are",
                    "a",
                    "large",
                    "number",
                    "of",
                    "unlabeled",
                    "positives."
                ],
                [
                    "Specifically,",
                    "we",
                    "manually",
                    "examine",
                    "the",
                    "topretrieved",
                    "passages",
                    "of",
                    "100",
                    "questions,",
                    "that",
                    "were",
                    "not",
                    "labeled",
                    "as",
                    "true",
                    "positives."
                ],
                [
                    "We",
                    "find",
                    "that",
                    "about",
                    "70%",
                    "of",
                    "them",
                    "are",
                    "actually",
                    "positives",
                    "or",
                    "highly",
                    "relevant."
                ],
                [
                    "Hence,",
                    "it",
                    "is",
                    "likely",
                    "to",
                    "bring",
                    "noise",
                    "if",
                    "we",
                    "simply",
                    "sample",
                    "hard",
                    "negatives",
                    "from",
                    "the",
                    "top-retrieved",
                    "passages",
                    "by",
                    "the",
                    "dense",
                    "retriever,",
                    "which",
                    "is",
                    "a",
                    "widely",
                    "adopted",
                    "strategy",
                    "to",
                    "sample",
                    "hard",
                    "negatives",
                    "in",
                    "previous",
                    "studies",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "As",
                    "a",
                    "comparison,",
                    "we",
                    "propose",
                    "denoised",
                    "hard",
                    "negatives",
                    "by",
                    "a",
                    "powerful",
                    "cross-encoder."
                ],
                [
                    "From",
                    "the",
                    "fourth",
                    "row",
                    "in",
                    "Table",
                    "3,",
                    "we",
                    "can",
                    "see",
                    "that",
                    "denoised",
                    "negatives",
                    "improve",
                    "the",
                    "performance",
                    "of",
                    "the",
                    "dense",
                    "retriever."
                ],
                [
                    "To",
                    "obtain",
                    "more",
                    "insights",
                    "about",
                    "denoised",
                    "hard",
                    "negatives,",
                    "Table",
                    "4",
                    "gives",
                    "the",
                    "sampled",
                    "hard",
                    "negatives",
                    "for",
                    "two",
                    "questions",
                    "before",
                    "and",
                    "after",
                    "denoising."
                ],
                [
                    "Figure",
                    "5",
                    "further",
                    "illustrates",
                    "the",
                    "ratio",
                    "of",
                    "filtered",
                    "passages",
                    "at",
                    "different",
                    "ranks."
                ],
                [
                    "We",
                    "can",
                    "see",
                    "that",
                    "there",
                    "are",
                    "more",
                    "passages",
                    "filtered",
                    "(i.e."
                ],
                [
                    "denoised)",
                    "at",
                    "lower",
                    "ranks,",
                    "since",
                    "it",
                    "is",
                    "likely",
                    "to",
                    "have",
                    "more",
                    "false",
                    "negatives",
                    "at",
                    "lower",
                    "ranks."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                0,
                0,
                2,
                2,
                3,
                3,
                3,
                0,
                0
            ]
        },
        "input": "sent0: becomes larger.\n sent1: After a certain point, the model performance starts to drop, since a large batch size may bring difficulty for optimization on training data with limited size.\n sent2: We say that there should be a balance between the batch size and the number of negatives.\n sent3: When increasing the batch size, we will have more negatives for each question.\n sent4: However, when the size of training data is limited, a large batch size will bring difficulty for optimization.\n sent5: Second, we examine the effect of denoised hard negatives from the top-k passages retrieved by the dense retriever.\n sent6: As shown in the third row in Table 3, the performance of the retriever significantly decreases by introducing hard negatives without denoising.\n sent7: We speculate that it is caused by the fact that there are a large number of unlabeled positives.\n sent8: Specifically, we manually examine the topretrieved passages of 100 questions, that were not labeled as true positives.\n sent9: We find that about 70% of them are actually positives or highly relevant.\n sent10: Hence, it is likely to bring noise if we simply sample hard negatives from the top-retrieved passages by the dense retriever, which is a widely adopted strategy to sample hard negatives in previous studies #TARGET_REF .\n sent11: As a comparison, we propose denoised hard negatives by a powerful cross-encoder.\n sent12: From the fourth row in Table 3, we can see that denoised negatives improve the performance of the dense retriever.\n sent13: To obtain more insights about denoised hard negatives, Table 4 gives the sampled hard negatives for two questions before and after denoising.\n sent14: Figure 5 further illustrates the ratio of filtered passages at different ranks.\n sent15: We can see that there are more passages filtered (i.e.\n sent16: denoised) at lower ranks, since it is likely to have more false negatives at lower ranks.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent10\", \"sent11\"], \"BACKGROUND\": [\"sent6\", \"sent7\", \"sent12\", \"sent13\", \"sent14\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "have",
                    "just",
                    "seen",
                    "that",
                    "certain",
                    "types",
                    "of",
                    "syntactic",
                    "parameterization",
                    "may",
                    "be",
                    "captured",
                    "in",
                    "the",
                    "grammar",
                    "network",
                    "(e.g.,",
                    "Xbar",
                    "parameters",
                    "such",
                    "as",
                    "constituent",
                    "order)."
                ],
                [
                    "In",
                    "addition",
                    "to",
                    "these,",
                    "there",
                    "are",
                    "syntactic",
                    "parameters",
                    "that",
                    "must",
                    "be",
                    "programmed",
                    "into",
                    "the",
                    "message-passing",
                    "mechanism",
                    "itself,",
                    "not",
                    "just",
                    "into",
                    "the",
                    "grammar",
                    "network."
                ],
                [
                    "Figure",
                    "2",
                    "shows",
                    "the",
                    "syntactic",
                    "parameter",
                    "settings",
                    "for",
                    "English",
                    "and",
                    "Korean."
                ],
                [
                    "The",
                    "English",
                    "settings",
                    "are",
                    "drawn",
                    "from",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "The",
                    "same",
                    "paradigm",
                    "was",
                    "followed",
                    "in",
                    "our",
                    "analysis",
                    "of",
                    "Korean",
                    "parameters."
                ],
                [
                    "The",
                    "remainder",
                    "of",
                    "this",
                    "paper",
                    "will",
                    "focus",
                    "on",
                    "how",
                    "we",
                    "automatically",
                    "precompile",
                    "the",
                    "English",
                    "and",
                    "Korean",
                    "parameter",
                    "settings",
                    "concerning",
                    "Xbar",
                    "theory",
                    "into",
                    "the",
                    "grammar",
                    "network",
                    "(i.e.,",
                    "Basic",
                    "Categories,",
                    "Pre-tenninals,",
                    "Constituent",
                    "Order,",
                    "Specifiers,",
                    "and",
                    "Adjunction)."
                ]
            ],
            "context": [
                0,
                3,
                3,
                2,
                0,
                3
            ]
        },
        "input": "sent0: We have just seen that certain types of syntactic parameterization may be captured in the grammar network (e.g., Xbar parameters such as constituent order).\n sent1: In addition to these, there are syntactic parameters that must be programmed into the message-passing mechanism itself, not just into the grammar network.\n sent2: Figure 2 shows the syntactic parameter settings for English and Korean.\n sent3: The English settings are drawn from #TARGET_REF .\n sent4: The same paradigm was followed in our analysis of Korean parameters.\n sent5: The remainder of this paper will focus on how we automatically precompile the English and Korean parameter settings concerning Xbar theory into the grammar network (i.e., Basic Categories, Pre-tenninals, Constituent Order, Specifiers, and Adjunction).\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent3\"], \"BACKGROUND\": [\"sent1\", \"sent2\", \"sent5\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Despite",
                    "of",
                    "the",
                    "advantages",
                    "of",
                    "using",
                    "CCG",
                    "categories",
                    "to",
                    "label",
                    "non-terminals",
                    "in",
                    "the",
                    "HPB",
                    "system",
                    "compared",
                    "with",
                    "SAMT",
                    "labels,",
                    "richness",
                    "of",
                    "CCG",
                    "categories",
                    "still",
                    "leads",
                    "to",
                    "a",
                    "large",
                    "number",
                    "of",
                    "different",
                    "non-terminal",
                    "labels."
                ],
                [
                    "This",
                    "causes",
                    "fragmentation",
                    "of",
                    "rule",
                    "probabilities",
                    "and",
                    "consequently",
                    "affects",
                    "translation",
                    "quality",
                    "negatively."
                ],
                [
                    "A",
                    "CCG",
                    "category",
                    "C",
                    "takes",
                    "the",
                    "form",
                    "of",
                    "C=(T\\L)/R",
                    "where",
                    "L",
                    "represents",
                    "the",
                    "left",
                    "argument",
                    "category,",
                    "R",
                    "the",
                    "right",
                    "argument",
                    "category,",
                    "and",
                    "T",
                    "the",
                    "resulting",
                    "category."
                ],
                [
                    "Each",
                    "of",
                    "these",
                    "constituent",
                    "categories",
                    "might",
                    "be",
                    "atomic",
                    "or",
                    "complex."
                ],
                [
                    "Furthermore,",
                    "some",
                    "atomic",
                    "CCG",
                    "categories",
                    "have",
                    "features",
                    "expressed",
                    "between",
                    "brackets",
                    "which",
                    "describe",
                    "certain",
                    "syntactic",
                    "information."
                ],
                [
                    "For",
                    "example,",
                    "the",
                    "atomic",
                    "category",
                    "S",
                    "might",
                    "have",
                    "a",
                    "feature",
                    "attached",
                    "to",
                    "it",
                    "which",
                    "distinguishes",
                    "types",
                    "of",
                    "sentences",
                    "such",
                    "as",
                    "declarative",
                    "S[dcl]",
                    "or",
                    "wh-question",
                    "S[wq]."
                ],
                [
                    "All",
                    "the",
                    "additional",
                    "information",
                    "represented",
                    "in",
                    "a",
                    "single",
                    "CCG",
                    "category",
                    "increases",
                    "the",
                    "number",
                    "of",
                    "different",
                    "CCG",
                    "categories",
                    "and",
                    "leads",
                    "to",
                    "label",
                    "sparsity",
                    "problem."
                ],
                [
                    "In",
                    "order",
                    "to",
                    "address",
                    "this",
                    "problem,",
                    "we",
                    "simplify",
                    "CCG",
                    "non-terminal",
                    "labels",
                    "by",
                    "reducing",
                    "the",
                    "amount",
                    "of",
                    "the",
                    "information",
                    "represented",
                    "in",
                    "them",
                    "using",
                    "the",
                    "following",
                    "approaches",
                    "#TARGET_REF",
                    ":"
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                1
            ]
        },
        "input": "sent0: Despite of the advantages of using CCG categories to label non-terminals in the HPB system compared with SAMT labels, richness of CCG categories still leads to a large number of different non-terminal labels.\n sent1: This causes fragmentation of rule probabilities and consequently affects translation quality negatively.\n sent2: A CCG category C takes the form of C=(T\\L)/R where L represents the left argument category, R the right argument category, and T the resulting category.\n sent3: Each of these constituent categories might be atomic or complex.\n sent4: Furthermore, some atomic CCG categories have features expressed between brackets which describe certain syntactic information.\n sent5: For example, the atomic category S might have a feature attached to it which distinguishes types of sentences such as declarative S[dcl] or wh-question S[wq].\n sent6: All the additional information represented in a single CCG category increases the number of different CCG categories and leads to label sparsity problem.\n sent7: In order to address this problem, we simplify CCG non-terminal labels by reducing the amount of the information represented in them using the following approaches #TARGET_REF :\n",
        "output": "{\"INFORMATION\": [\"sent7\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent6\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Early",
                    "work",
                    "on",
                    "the",
                    "detection",
                    "and",
                    "classification",
                    "of",
                    "Named",
                    "Entities",
                    "in",
                    "Arabic",
                    "NER",
                    "used",
                    "a",
                    "rule-based",
                    "or",
                    "grammar-based",
                    "approach",
                    "#REF",
                    "."
                ],
                [
                    "Subsequently,",
                    "the",
                    "field",
                    "shifted",
                    "generally",
                    "to",
                    "a",
                    "machine",
                    "learning",
                    "approach",
                    "-thus",
                    "avoiding",
                    "the",
                    "time-consuming",
                    "and",
                    "expensive",
                    "maintenance",
                    "of",
                    "rule-sets."
                ],
                [
                    "Within",
                    "this",
                    "general",
                    "approach",
                    "a",
                    "wide",
                    "variety",
                    "of",
                    "techniques",
                    "have",
                    "all",
                    "been",
                    "applied",
                    "to",
                    "the",
                    "Arabic",
                    "NER",
                    "problem",
                    "including",
                    "Support",
                    "Vector",
                    "Machines",
                    "(SVM)",
                    "#TARGET_REF",
                    ",",
                    "Conditional",
                    "Random",
                    "Fields",
                    "(CRF)",
                    "#REF",
                    ",",
                    "Maximum",
                    "Entropy",
                    "(ME)",
                    "#REF",
                    ",",
                    "Hidden",
                    "Markov",
                    "Models",
                    "(HMM)",
                    "and",
                    "Decision",
                    "Trees",
                    "#REF",
                    "."
                ],
                [
                    "Notably,",
                    "#REF",
                    "developed",
                    "an",
                    "Arabic",
                    "NER",
                    "system,",
                    "ANERsys",
                    "1.0,",
                    "which",
                    "employed",
                    "maximum",
                    "entropy",
                    "and",
                    "could",
                    "recognize",
                    "four",
                    "types",
                    "of",
                    "Named",
                    "Entities."
                ]
            ],
            "context": [
                0,
                0,
                3,
                0
            ]
        },
        "input": "sent0: Early work on the detection and classification of Named Entities in Arabic NER used a rule-based or grammar-based approach #REF .\n sent1: Subsequently, the field shifted generally to a machine learning approach -thus avoiding the time-consuming and expensive maintenance of rule-sets.\n sent2: Within this general approach a wide variety of techniques have all been applied to the Arabic NER problem including Support Vector Machines (SVM) #TARGET_REF , Conditional Random Fields (CRF) #REF , Maximum Entropy (ME) #REF , Hidden Markov Models (HMM) and Decision Trees #REF .\n sent3: Notably, #REF developed an Arabic NER system, ANERsys 1.0, which employed maximum entropy and could recognize four types of Named Entities.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "shows",
                    "butterfly",
                    "plots",
                    "for",
                    "the",
                    "effects",
                    "of",
                    "word",
                    "length,",
                    "frequency",
                    "and",
                    "class",
                    "across",
                    "64",
                    "electrodes."
                ],
                [
                    "Our",
                    "linear",
                    "SVM",
                    "decoding",
                    "analysis",
                    "replicates",
                    "the",
                    "temporal",
                    "cascade",
                    "of",
                    "word",
                    "length,",
                    "frequency",
                    "and",
                    "class",
                    "effects",
                    "previously",
                    "reported",
                    "for",
                    "EEG",
                    "responses",
                    "averaged",
                    "across",
                    "a",
                    "large",
                    "number",
                    "of",
                    "trials."
                ],
                [
                    "The",
                    "word",
                    "length",
                    "effect",
                    "arises",
                    "early",
                    "at",
                    "about",
                    "100",
                    "ms,",
                    "previously",
                    "associated",
                    "with",
                    "visual",
                    "word",
                    "processing",
                    "in",
                    "occipitotemporal",
                    "cortices",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Word",
                    "frequency",
                    "influenced",
                    "neural",
                    "processing",
                    "later",
                    "from",
                    "200",
                    "ms",
                    "onwards",
                    "with",
                    "a",
                    "slight",
                    "left-hemispheric",
                    "predominance",
                    "#REF",
                    "."
                ],
                [
                    "The",
                    "word",
                    "class",
                    "effect",
                    "emerged",
                    "in",
                    "early",
                    "and",
                    "late",
                    "time",
                    "windows",
                    "with",
                    "the",
                    "effect",
                    "at",
                    "about",
                    "550",
                    "ms",
                    "in",
                    "line",
                    "with",
                    "the",
                    "wellknown",
                    "P600",
                    "as",
                    "an",
                    "ERP",
                    "indicator",
                    "for",
                    "syntactic",
                    "processing",
                    "#REF",
                    "."
                ],
                [
                    "Word",
                    "length",
                    "and",
                    "frequency",
                    "effects",
                    "were",
                    "stronger",
                    "than",
                    "the",
                    "word",
                    "class",
                    "effect,",
                    "see",
                    "#REF",
                    "."
                ],
                [
                    "As",
                    "expected,",
                    "decoding",
                    "accuracy",
                    "increased",
                    "when",
                    "EEG",
                    "signals",
                    "were",
                    "averaged",
                    "across",
                    "trials."
                ],
                [
                    "Thus,",
                    "carefully",
                    "controlling",
                    "each",
                    "comparison",
                    "of",
                    "interest",
                    "(e.g."
                ],
                [
                    "word",
                    "class)",
                    "for",
                    "the",
                    "effects",
                    "of",
                    "no",
                    "interest",
                    "(e.g."
                ],
                [
                    "word",
                    "length",
                    "and",
                    "gressively",
                    "generates",
                    "dependencies",
                    "amongst",
                    "training",
                    "samples",
                    "thereby",
                    "limiting",
                    "their",
                    "additional",
                    "benefit",
                    "beyond",
                    "250k."
                ],
                [
                    "We",
                    "formally",
                    "assessed",
                    "whether",
                    "the",
                    "Transformer",
                    "that",
                    "scored",
                    "best",
                    "on",
                    "the",
                    "dev",
                    "set",
                    "obtained",
                    "better",
                    "decoding",
                    "accuracy",
                    "for",
                    "250K",
                    "than",
                    "for",
                    "the",
                    "original",
                    "20k",
                    "training",
                    "set",
                    "(n.b."
                ],
                [
                    "we",
                    "performed",
                    "this",
                    "statistical",
                    "test",
                    "on",
                    "the",
                    "test",
                    "set,",
                    "because",
                    "the",
                    "3",
                    "and",
                    "10",
                    "trial",
                    "averages",
                    "within",
                    "the",
                    "dev",
                    "set",
                    "were",
                    "not",
                    "independent",
                    "from",
                    "one",
                    "another",
                    "as",
                    "a",
                    "result",
                    "of",
                    "boostrapping)."
                ],
                [
                    "Indeed,",
                    "for",
                    "both",
                    "3",
                    "and",
                    "10",
                    "trial",
                    "averages",
                    "the",
                    "Transformer's",
                    "(but",
                    "not",
                    "the",
                    "SVM's)",
                    "decoding",
                    "accuracy",
                    "was",
                    "significantly",
                    "better",
                    "for",
                    "250k",
                    "than",
                    "the",
                    "original",
                    "20k",
                    "training",
                    "set",
                    "(p",
                    "&lt,",
                    "0.01,",
                    "Wilcoxon",
                    "signed-rank",
                    "test)."
                ]
            ],
            "context": [
                0,
                2,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: shows butterfly plots for the effects of word length, frequency and class across 64 electrodes.\n sent1: Our linear SVM decoding analysis replicates the temporal cascade of word length, frequency and class effects previously reported for EEG responses averaged across a large number of trials.\n sent2: The word length effect arises early at about 100 ms, previously associated with visual word processing in occipitotemporal cortices #TARGET_REF .\n sent3: Word frequency influenced neural processing later from 200 ms onwards with a slight left-hemispheric predominance #REF .\n sent4: The word class effect emerged in early and late time windows with the effect at about 550 ms in line with the wellknown P600 as an ERP indicator for syntactic processing #REF .\n sent5: Word length and frequency effects were stronger than the word class effect, see #REF .\n sent6: As expected, decoding accuracy increased when EEG signals were averaged across trials.\n sent7: Thus, carefully controlling each comparison of interest (e.g.\n sent8: word class) for the effects of no interest (e.g.\n sent9: word length and gressively generates dependencies amongst training samples thereby limiting their additional benefit beyond 250k.\n sent10: We formally assessed whether the Transformer that scored best on the dev set obtained better decoding accuracy for 250K than for the original 20k training set (n.b.\n sent11: we performed this statistical test on the test set, because the 3 and 10 trial averages within the dev set were not independent from one another as a result of boostrapping).\n sent12: Indeed, for both 3 and 10 trial averages the Transformer's (but not the SVM's) decoding accuracy was significantly better for 250k than the original 20k training set (p &lt, 0.01, Wilcoxon signed-rank test).\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "syntax",
                    "discriminator",
                    "takes",
                    "as",
                    "input",
                    "either",
                    "a",
                    "real",
                    "sentence,",
                    "r",
                    "=",
                    "(r",
                    "1",
                    ",",
                    "."
                ],
                [
                    "."
                ],
                [
                    "."
                ],
                [
                    ",",
                    "r",
                    "n",
                    "),",
                    "or",
                    "a",
                    "generated",
                    "one,g",
                    "=",
                    "(g",
                    "1",
                    ",",
                    "."
                ],
                [
                    "."
                ],
                [
                    "."
                ],
                [
                    ",",
                    "g",
                    "n",
                    ").Similarly",
                    "to",
                    "many",
                    "other",
                    "works",
                    "(e.g.,",
                    "#TARGET_REF",
                    "),",
                    "the",
                    "discriminator",
                    "first",
                    "transforms",
                    "its",
                    "input",
                    "into",
                    "an",
                    "embedding",
                    "matrix."
                ],
                [
                    "This",
                    "embedding",
                    "allows",
                    "learning",
                    "a",
                    "transformation",
                    "that",
                    "condenses",
                    "the",
                    "information",
                    "brought",
                    "in",
                    "by",
                    "each",
                    "word",
                    "optimally",
                    "for",
                    "any",
                    "given",
                    "task."
                ],
                [
                    "The",
                    "syntax",
                    "discriminator",
                    "is",
                    "then",
                    "built",
                    "using",
                    "two",
                    "convolutional",
                    "layers",
                    "with",
                    "ReLU",
                    "activation",
                    "functions,",
                    "followed",
                    "by",
                    "a",
                    "self-attention",
                    "layer,",
                    "again",
                    "followed",
                    "by",
                    "two",
                    "other",
                    "convolutional",
                    "layers",
                    "with",
                    "ReLU",
                    "activation",
                    "functions."
                ],
                [
                    "The",
                    "selfattention",
                    "layer",
                    "is",
                    "used",
                    "to",
                    "attend",
                    "to",
                    "the",
                    "output",
                    "of",
                    "the",
                    "previous",
                    "convolutional",
                    "layer",
                    "and",
                    "select",
                    "the",
                    "most",
                    "useful",
                    "features."
                ],
                [
                    "The",
                    "final",
                    "layers",
                    "generate",
                    "the",
                    "decision."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                3,
                0,
                0,
                0
            ]
        },
        "input": "sent0: The syntax discriminator takes as input either a real sentence, r = (r 1 , .\n sent1: .\n sent2: .\n sent3: , r n ), or a generated one,g = (g 1 , .\n sent4: .\n sent5: .\n sent6: , g n ).Similarly to many other works (e.g., #TARGET_REF ), the discriminator first transforms its input into an embedding matrix.\n sent7: This embedding allows learning a transformation that condenses the information brought in by each word optimally for any given task.\n sent8: The syntax discriminator is then built using two convolutional layers with ReLU activation functions, followed by a self-attention layer, again followed by two other convolutional layers with ReLU activation functions.\n sent9: The selfattention layer is used to attend to the output of the previous convolutional layer and select the most useful features.\n sent10: The final layers generate the decision.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent6\"], \"BACKGROUND\": [\"sent7\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "train",
                    "two",
                    "BART",
                    "#TARGET_REF",
                    ")",
                    "models",
                    "that",
                    "encodes",
                    "input",
                    "information",
                    "via",
                    "a",
                    "bidirectional",
                    "transformer",
                    "encoder",
                    "and",
                    "decodes",
                    "autoregressively:",
                    "the",
                    "first",
                    "takes",
                    "as",
                    "input",
                    "character",
                    "and",
                    "location",
                    "information",
                    "and",
                    "produces",
                    "a",
                    "short",
                    "motivation",
                    "(Section",
                    "2),",
                    "the",
                    "second",
                    "takes",
                    "as",
                    "input",
                    "character,",
                    "location",
                    "information,",
                    "short",
                    "motivation",
                    "and",
                    "produces",
                    "the",
                    "sequence",
                    "of",
                    "LIGHT",
                    "game",
                    "engine",
                    "executable",
                    "actions",
                    "needed",
                    "to",
                    "achieve",
                    "the",
                    "motivation."
                ],
                [
                    "This",
                    "sequence",
                    "of",
                    "actions",
                    "is",
                    "provided",
                    "by",
                    "the",
                    "human",
                    "expert",
                    "demonstrations",
                    "as",
                    "mentioned",
                    "in",
                    "Section",
                    "2."
                ]
            ],
            "context": [
                3,
                0
            ]
        },
        "input": "sent0: We train two BART #TARGET_REF ) models that encodes input information via a bidirectional transformer encoder and decodes autoregressively: the first takes as input character and location information and produces a short motivation (Section 2), the second takes as input character, location information, short motivation and produces the sequence of LIGHT game engine executable actions needed to achieve the motivation.\n sent1: This sequence of actions is provided by the human expert demonstrations as mentioned in Section 2.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "#TARGET_REF",
                    ",",
                    "authors",
                    "propose",
                    "to",
                    "carefully",
                    "organise",
                    "the",
                    "batches",
                    "so",
                    "that",
                    "only",
                    "a",
                    "subset",
                    "K",
                    "of",
                    "the",
                    "target",
                    "vocabulary",
                    "is",
                    "possibly",
                    "generated",
                    "at",
                    "training",
                    "time."
                ],
                [
                    "This",
                    "allows",
                    "the",
                    "system",
                    "to",
                    "train",
                    "a",
                    "model",
                    "with",
                    "much",
                    "larger",
                    "target",
                    "vocabulary",
                    "without",
                    "substantially",
                    "increasing",
                    "the",
                    "computational",
                    "complexity."
                ],
                [
                    "Another",
                    "alternative",
                    "is",
                    "proposed",
                    "by",
                    "#REF",
                    "where",
                    "a",
                    "structured",
                    "output",
                    "layer",
                    "(SOUL)",
                    "is",
                    "defined",
                    "to",
                    "handle",
                    "the",
                    "words",
                    "not",
                    "appearing",
                    "in",
                    "the",
                    "shortlist."
                ],
                [
                    "This",
                    "allows",
                    "the",
                    "system",
                    "to",
                    "always",
                    "apply",
                    "the",
                    "softmax",
                    "normalization",
                    "on",
                    "a",
                    "layer",
                    "with",
                    "reduced",
                    "size."
                ]
            ],
            "context": [
                1,
                2,
                0,
                0
            ]
        },
        "input": "sent0: In #TARGET_REF , authors propose to carefully organise the batches so that only a subset K of the target vocabulary is possibly generated at training time.\n sent1: This allows the system to train a model with much larger target vocabulary without substantially increasing the computational complexity.\n sent2: Another alternative is proposed by #REF where a structured output layer (SOUL) is defined to handle the words not appearing in the shortlist.\n sent3: This allows the system to always apply the softmax normalization on a layer with reduced size.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "For",
                    "each",
                    "language,",
                    "I",
                    "use",
                    "the",
                    "fastText",
                    "aligned",
                    "word",
                    "vectors",
                    "#TARGET_REF",
                    ",",
                    "3",
                    "limiting",
                    "the",
                    "vocabulary",
                    "set",
                    "V",
                    "to",
                    "the",
                    "top",
                    "200,000",
                    "vectors",
                    "by",
                    "frequency."
                ],
                [
                    "For",
                    "the",
                    "target",
                    "word",
                    "vocabulary",
                    "W",
                    ",",
                    "I",
                    "take",
                    "the",
                    "10,000",
                    "most",
                    "frequent",
                    "wordforms",
                    "among",
                    "all",
                    "attributive",
                    "adjectives",
                    "extracted",
                    "from",
                    "the",
                    "entire",
                    "CoNLL",
                    "Wikipedia",
                    "dataset."
                ]
            ],
            "context": [
                2,
                2
            ]
        },
        "input": "sent0: For each language, I use the fastText aligned word vectors #TARGET_REF , 3 limiting the vocabulary set V to the top 200,000 vectors by frequency.\n sent1: For the target word vocabulary W , I take the 10,000 most frequent wordforms among all attributive adjectives extracted from the entire CoNLL Wikipedia dataset.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\", \"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "an",
                    "Item",
                    "and",
                    "Process",
                    "approach",
                    "stem",
                    "changes",
                    "are",
                    "dealt",
                    "with",
                    "via",
                    "rules."
                ],
                [
                    "Since",
                    "stem",
                    "changes",
                    "in",
                    "German",
                    "are",
                    "not",
                    "regular",
                    "or",
                    "predictable",
                    "we",
                    "follow",
                    "#REF",
                    "and",
                    "#TARGET_REF",
                    "on",
                    "listing",
                    "the",
                    "forms",
                    "that",
                    "an",
                    "element",
                    "may",
                    "take."
                ],
                [
                    "These",
                    "forms",
                    "are",
                    "called",
                    "word",
                    "formation",
                    "stem",
                    "forms."
                ],
                [
                    "Free",
                    "and",
                    "bound",
                    "morphological",
                    "elements",
                    "have",
                    "one",
                    "or",
                    "more",
                    "derivation",
                    "stem",
                    "form(s)",
                    "and",
                    "one",
                    "or",
                    "more",
                    "compounding",
                    "stem",
                    "form(s)."
                ],
                [
                    "Very",
                    "often",
                    "these",
                    "stem",
                    "forms",
                    "look",
                    "just",
                    "like",
                    "the",
                    "regular",
                    "stem."
                ],
                [
                    "Consider",
                    "the",
                    "examples",
                    "in",
                    "Table",
                    "4",
                    "Here",
                    "you",
                    "can",
                    "see",
                    "that",
                    "the",
                    "suffix",
                    "-keit",
                    "has",
                    "no",
                    "derivation",
                    "stem",
                    "form."
                ],
                [
                    "That",
                    "means",
                    "that",
                    "it",
                    "cannot",
                    "be",
                    "used",
                    "in",
                    "further",
                    "derivation",
                    "(it",
                    "is",
                    "a",
                    "so-called",
                    "closing",
                    "suffix,",
                    "see",
                    "#REF",
                    ")."
                ],
                [
                    "It",
                    "has",
                    "a",
                    "compounding",
                    "stem",
                    "form,",
                    "however."
                ],
                [
                    "The",
                    "word",
                    "formation",
                    "stem",
                    "forms",
                    "are",
                    "propagated",
                    "in",
                    "complex",
                    "words",
                    "using",
                    "these",
                    "elements:",
                    "each",
                    "complex",
                    "word",
                    "ending",
                    "in",
                    "-keit,",
                    "for",
                    "example,",
                    "will",
                    "automatically",
                    "have",
                    "a",
                    "compounding",
                    "stem",
                    "form",
                    "ending",
                    "in",
                    "-keits."
                ],
                [
                    "17",
                    "Such",
                    "an",
                    "approach",
                    "minimized",
                    "the",
                    "ambiguities",
                    "that",
                    "stem",
                    "changes",
                    "can",
                    "cause."
                ],
                [
                    "However,",
                    "one",
                    "has",
                    "to",
                    "acquire",
                    "all",
                    "the",
                    "word",
                    "formation",
                    "stem",
                    "forms."
                ],
                [
                    "For",
                    "our",
                    "lexicon",
                    "this",
                    "is",
                    "done",
                    "semi-automatically",
                    "as",
                    "described",
                    "in",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                3,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: In an Item and Process approach stem changes are dealt with via rules.\n sent1: Since stem changes in German are not regular or predictable we follow #REF and #TARGET_REF on listing the forms that an element may take.\n sent2: These forms are called word formation stem forms.\n sent3: Free and bound morphological elements have one or more derivation stem form(s) and one or more compounding stem form(s).\n sent4: Very often these stem forms look just like the regular stem.\n sent5: Consider the examples in Table 4 Here you can see that the suffix -keit has no derivation stem form.\n sent6: That means that it cannot be used in further derivation (it is a so-called closing suffix, see #REF ).\n sent7: It has a compounding stem form, however.\n sent8: The word formation stem forms are propagated in complex words using these elements: each complex word ending in -keit, for example, will automatically have a compounding stem form ending in -keits.\n sent9: 17 Such an approach minimized the ambiguities that stem changes can cause.\n sent10: However, one has to acquire all the word formation stem forms.\n sent11: For our lexicon this is done semi-automatically as described in #REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Task",
                    "1:",
                    "Sentiment",
                    "classification."
                ],
                [
                    "For",
                    "the",
                    "task",
                    "of",
                    "sentiment",
                    "classification,",
                    "we",
                    "use",
                    "several",
                    "datasets",
                    "in",
                    "our",
                    "experiments."
                ],
                [
                    "To",
                    "find",
                    "shortcuts",
                    "in",
                    "Stanford",
                    "Sentiment",
                    "Treebank",
                    "(SST-2)",
                    "#TARGET_REF",
                    "dataset,",
                    "we",
                    "first",
                    "train",
                    "a",
                    "model",
                    "on",
                    "SST-2",
                    "training",
                    "set",
                    "which",
                    "consists",
                    "of",
                    "67,",
                    "349",
                    "sentences."
                ],
                [
                    "We",
                    "then",
                    "evaluate",
                    "the",
                    "model",
                    "on",
                    "SST-2",
                    "training",
                    "set",
                    "5",
                    "and",
                    "Yelp",
                    "(As-ghar,",
                    "2016)",
                    "test",
                    "set",
                    "and",
                    "obtain",
                    "attention",
                    "scores."
                ],
                [
                    "For",
                    "cross-dataset",
                    "analysis,",
                    "we",
                    "compare",
                    "the",
                    "important",
                    "tokens",
                    "extracted",
                    "from",
                    "SST-2",
                    "and",
                    "Yelp."
                ],
                [
                    "Similarly,",
                    "we",
                    "train",
                    "another",
                    "model",
                    "on",
                    "80,",
                    "000",
                    "amazon",
                    "kitchen",
                    "reviews",
                    "#REF",
                    ",",
                    "and",
                    "apply",
                    "it",
                    "on",
                    "the",
                    "kitchen",
                    "review",
                    "dev",
                    "set",
                    "and",
                    "the",
                    "amazon",
                    "electronics",
                    "dev",
                    "set,",
                    "both",
                    "having",
                    "10,",
                    "000",
                    "reviews."
                ]
            ],
            "context": [
                0,
                3,
                2,
                2,
                2,
                0
            ]
        },
        "input": "sent0: Task 1: Sentiment classification.\n sent1: For the task of sentiment classification, we use several datasets in our experiments.\n sent2: To find shortcuts in Stanford Sentiment Treebank (SST-2) #TARGET_REF dataset, we first train a model on SST-2 training set which consists of 67, 349 sentences.\n sent3: We then evaluate the model on SST-2 training set 5 and Yelp (As-ghar, 2016) test set and obtain attention scores.\n sent4: For cross-dataset analysis, we compare the important tokens extracted from SST-2 and Yelp.\n sent5: Similarly, we train another model on 80, 000 amazon kitchen reviews #REF , and apply it on the kitchen review dev set and the amazon electronics dev set, both having 10, 000 reviews.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent2\", \"sent3\", \"sent4\"], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "n",
                    "positive",
                    "signifiers",
                    "n",
                    "words",
                    "NCR",
                    "VAD",
                    "Lexicon",
                    "This",
                    "measure",
                    "is",
                    "based",
                    "on",
                    "a",
                    "list",
                    "of",
                    "words",
                    "rated",
                    "on",
                    "the",
                    "emotional",
                    "dimensions",
                    "of",
                    "valence,",
                    "arousal,",
                    "and",
                    "dominance",
                    "which",
                    "has",
                    "been",
                    "used",
                    "in",
                    "gender",
                    "bias",
                    "research."
                ],
                [
                    "In",
                    "particular,",
                    "weakness",
                    "(low",
                    "dominance),",
                    "passiveness",
                    "(low",
                    "arousal",
                    "or",
                    "agency),",
                    "and",
                    "badness",
                    "(valence)",
                    "may",
                    "be",
                    "associated",
                    "with",
                    "a",
                    "female",
                    "stereotype",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Given",
                    "the",
                    "size",
                    "of",
                    "the",
                    "lexicon",
                    "and",
                    "its",
                    "overlap",
                    "of",
                    "up",
                    "to",
                    "100%",
                    "with",
                    "other",
                    "word",
                    "lists,",
                    "we",
                    "only",
                    "counted",
                    "words",
                    "with",
                    "either",
                    "a",
                    "valence,",
                    "arousal,",
                    "or",
                    "dominance",
                    "rating",
                    "&gt,",
                    "0.75",
                    "on",
                    "a",
                    "scale",
                    "from",
                    "0",
                    "to",
                    "1."
                ],
                [
                    "The",
                    "calculation",
                    "is:"
                ]
            ],
            "context": [
                0,
                1,
                0,
                0
            ]
        },
        "input": "sent0: n positive signifiers n words NCR VAD Lexicon This measure is based on a list of words rated on the emotional dimensions of valence, arousal, and dominance which has been used in gender bias research.\n sent1: In particular, weakness (low dominance), passiveness (low arousal or agency), and badness (valence) may be associated with a female stereotype #TARGET_REF .\n sent2: Given the size of the lexicon and its overlap of up to 100% with other word lists, we only counted words with either a valence, arousal, or dominance rating &gt, 0.75 on a scale from 0 to 1.\n sent3: The calculation is:\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Table",
                    "5:",
                    "The",
                    "experimental",
                    "results",
                    "of",
                    "passage",
                    "reading",
                    "on",
                    "NQ",
                    "dataset."
                ],
                [
                    "In",
                    "this",
                    "paper,",
                    "we",
                    "focus",
                    "on",
                    "extractive",
                    "reader,",
                    "while",
                    "the",
                    "recent",
                    "generative",
                    "readers",
                    "#TARGET_REF",
                    "can",
                    "also",
                    "be",
                    "applied",
                    "here",
                    "and",
                    "may",
                    "lead",
                    "to",
                    "better",
                    "results."
                ]
            ],
            "context": [
                3,
                2
            ]
        },
        "input": "sent0: Table 5: The experimental results of passage reading on NQ dataset.\n sent1: In this paper, we focus on extractive reader, while the recent generative readers #TARGET_REF can also be applied here and may lead to better results.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Our",
                    "GB",
                    "parser",
                    "is",
                    "an",
                    "extension",
                    "of",
                    "the",
                    "message-passing",
                    "approach",
                    "proposed",
                    "by",
                    "#REF",
                    "and",
                    "#TARGET_REF",
                    ",",
                    "which",
                    "uses",
                    "a",
                    "network",
                    "to",
                    "encode",
                    "the",
                    "grammar."
                ],
                [
                    "The",
                    "nodes",
                    "in",
                    "the",
                    "grammar",
                    "network",
                    "represent",
                    "grammatical",
                    "categories",
                    "(e.g.,",
                    "NP,",
                    "Nbar,",
                    "N)",
                    "or",
                    "subcategories,",
                    "such",
                    "as",
                    "V:NP",
                    "(i.e.,",
                    "a",
                    "transitive",
                    "verb",
                    "that",
                    "takes",
                    "an",
                    "NP",
                    "as",
                    "complement)."
                ],
                [
                    "Figure",
                    "l.a",
                    "depicts",
                    "a",
                    "portion",
                    "of",
                    "the",
                    "grammar",
                    "network",
                    "for",
                    "English."
                ]
            ],
            "context": [
                1,
                0,
                0
            ]
        },
        "input": "sent0: Our GB parser is an extension of the message-passing approach proposed by #REF and #TARGET_REF , which uses a network to encode the grammar.\n sent1: The nodes in the grammar network represent grammatical categories (e.g., NP, Nbar, N) or subcategories, such as V:NP (i.e., a transitive verb that takes an NP as complement).\n sent2: Figure l.a depicts a portion of the grammar network for English.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Several",
                    "parsing",
                    "algorithms",
                    "have",
                    "been",
                    "proposed",
                    "for",
                    "TAG,",
                    "ranging",
                    "from",
                    "simple",
                    "bottom-up",
                    "algorithms,",
                    "like",
                    "CYK",
                    "#REF",
                    ",",
                    "to",
                    "sophisticated",
                    "extensions",
                    "of",
                    "the",
                    "Earley",
                    "'s",
                    "algorithm",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "In",
                    "order",
                    "to",
                    "improve",
                    "efficiency,",
                    "it",
                    "is",
                    "usual",
                    "to",
                    "translate",
                    "the",
                    "source",
                    "tree",
                    "adjoining",
                    "grammar",
                    "into",
                    "a",
                    "linear",
                    "indexed",
                    "grammar",
                    "#REF",
                    "."
                ],
                [
                    "However,",
                    "in",
                    "some",
                    "cases",
                    "is",
                    "not",
                    "possible",
                    "to",
                    "translate",
                    "the",
                    "parsing",
                    "strategy",
                    "from",
                    "TAG",
                    "to",
                    "LIG,",
                    "as",
                    "there",
                    "are",
                    "parsing",
                    "strategies",
                    "for",
                    "TAG",
                    "which",
                    "are",
                    "not",
                    "incorporated",
                    "in",
                    "any",
                    "parsing",
                    "algorithm",
                    "for",
                    "LIG."
                ],
                [
                    "To",
                    "eliminate",
                    "this",
                    "drawback,",
                    "we",
                    "present",
                    "in",
                    "this",
                    "paper",
                    "several",
                    "parsing",
                    "algorithms",
                    "for",
                    "LIG",
                    "which",
                    "mimic",
                    "the",
                    "most",
                    "popular",
                    "parsing",
                    "strategies",
                    "for",
                    "TAG",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                3,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Several parsing algorithms have been proposed for TAG, ranging from simple bottom-up algorithms, like CYK #REF , to sophisticated extensions of the Earley 's algorithm #TARGET_REF .\n sent1: In order to improve efficiency, it is usual to translate the source tree adjoining grammar into a linear indexed grammar #REF .\n sent2: However, in some cases is not possible to translate the parsing strategy from TAG to LIG, as there are parsing strategies for TAG which are not incorporated in any parsing algorithm for LIG.\n sent3: To eliminate this drawback, we present in this paper several parsing algorithms for LIG which mimic the most popular parsing strategies for TAG #REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Coreference",
                    "Arc",
                    "Prediction",
                    "(CAP)",
                    "We",
                    "use",
                    "the",
                    "English",
                    "CAP",
                    "dataset",
                    "derived",
                    "from",
                    "PreCo",
                    "#TARGET_REF",
                    "by",
                    "#REF",
                    "."
                ],
                [
                    "The",
                    "creators",
                    "of",
                    "the",
                    "dataset",
                    "partition",
                    "the",
                    "data",
                    "by",
                    "cosine",
                    "similarity",
                    "of",
                    "GloVe",
                    "#REF",
                    "embeddings",
                    "of",
                    "mention",
                    "spans",
                    "and",
                    "balance",
                    "the",
                    "number",
                    "of",
                    "positive",
                    "and",
                    "negative",
                    "examples",
                    "in",
                    "each",
                    "bucket,",
                    "so",
                    "that",
                    "models",
                    "do",
                    "not",
                    "solve",
                    "the",
                    "task",
                    "by",
                    "capturing",
                    "surface",
                    "features",
                    "of",
                    "entity",
                    "mention",
                    "spans."
                ],
                [
                    "The",
                    "original",
                    "data",
                    "split",
                    "provides",
                    "8k",
                    "examples",
                    "for",
                    "each",
                    "of",
                    "the",
                    "training,",
                    "development,",
                    "and",
                    "test",
                    "sets."
                ]
            ],
            "context": [
                2,
                0,
                0
            ]
        },
        "input": "sent0: Coreference Arc Prediction (CAP) We use the English CAP dataset derived from PreCo #TARGET_REF by #REF .\n sent1: The creators of the dataset partition the data by cosine similarity of GloVe #REF embeddings of mention spans and balance the number of positive and negative examples in each bucket, so that models do not solve the task by capturing surface features of entity mention spans.\n sent2: The original data split provides 8k examples for each of the training, development, and test sets.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Learning",
                    "task-agnostic",
                    "unsupervised",
                    "representations",
                    "of",
                    "data",
                    "has",
                    "been",
                    "the",
                    "center",
                    "of",
                    "attention",
                    "across",
                    "various",
                    "areas",
                    "of",
                    "Machine",
                    "Learning",
                    "and",
                    "more",
                    "specifically",
                    "NLP."
                ],
                [
                    "However,",
                    "little",
                    "is",
                    "known",
                    "about",
                    "the",
                    "way",
                    "these",
                    "continuous",
                    "representations",
                    "organise",
                    "information",
                    "about",
                    "data."
                ],
                [
                    "In",
                    "recent",
                    "years,",
                    "the",
                    "NLP",
                    "community",
                    "has",
                    "focused",
                    "on",
                    "the",
                    "question",
                    "of",
                    "design",
                    "and",
                    "selection",
                    "of",
                    "suitable",
                    "linguistic",
                    "tasks",
                    "to",
                    "probe",
                    "the",
                    "presence",
                    "of",
                    "syntactic",
                    "or",
                    "semantic",
                    "phenomena",
                    "in",
                    "representations",
                    "as",
                    "a",
                    "whole",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Nonetheless,",
                    "a",
                    "finegrain",
                    "understanding",
                    "of",
                    "information",
                    "organisation",
                    "in",
                    "coordinates",
                    "of",
                    "a",
                    "continuous",
                    "representation",
                    "is",
                    "yet",
                    "to",
                    "be",
                    "achieved."
                ]
            ],
            "context": [
                3,
                0,
                1,
                0
            ]
        },
        "input": "sent0: Learning task-agnostic unsupervised representations of data has been the center of attention across various areas of Machine Learning and more specifically NLP.\n sent1: However, little is known about the way these continuous representations organise information about data.\n sent2: In recent years, the NLP community has focused on the question of design and selection of suitable linguistic tasks to probe the presence of syntactic or semantic phenomena in representations as a whole #TARGET_REF .\n sent3: Nonetheless, a finegrain understanding of information organisation in coordinates of a continuous representation is yet to be achieved.\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "oddest",
                    "feature",
                    "of",
                    "MMB's",
                    "breath-group",
                    "work,",
                    "stretching",
                    "as",
                    "it",
                    "did",
                    "over",
                    "many",
                    "years",
                    "was",
                    "that",
                    "it",
                    "referred",
                    "constantly",
                    "to",
                    "breathing,",
                    "but",
                    "nothing",
                    "ever",
                    "rested",
                    "on",
                    "that:",
                    "partitions",
                    "were",
                    "always",
                    "inserted",
                    "into",
                    "text",
                    "intuitively",
                    "in",
                    "a",
                    "way",
                    "that,",
                    "to",
                    "me",
                    "at",
                    "least,",
                    "corresponded",
                    "more",
                    "naturally",
                    "to",
                    "the",
                    "criteria",
                    "just",
                    "listed",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Finally,",
                    "of",
                    "course,",
                    "it",
                    "would",
                    "be",
                    "overbold",
                    "to",
                    "assert",
                    "that",
                    "there",
                    "will",
                    "never",
                    "be",
                    "applications",
                    "of",
                    "Greek",
                    "rhetorical",
                    "figures",
                    "to",
                    "the",
                    "computer",
                    "understanding",
                    "of",
                    "natural",
                    "language,",
                    "but",
                    "none",
                    "have",
                    "as",
                    "yet",
                    "emerged,",
                    "except",
                    "their",
                    "explicit",
                    "and",
                    "obvious",
                    "use",
                    "as",
                    "forms",
                    "of",
                    "expression."
                ]
            ],
            "context": [
                3,
                0
            ]
        },
        "input": "sent0: The oddest feature of MMB's breath-group work, stretching as it did over many years was that it referred constantly to breathing, but nothing ever rested on that: partitions were always inserted into text intuitively in a way that, to me at least, corresponded more naturally to the criteria just listed #TARGET_REF .\n sent1: Finally, of course, it would be overbold to assert that there will never be applications of Greek rhetorical figures to the computer understanding of natural language, but none have as yet emerged, except their explicit and obvious use as forms of expression.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "β-VAE",
                    "#REF",
                    "adds",
                    "a",
                    "hyperparameter",
                    "β",
                    "to",
                    "control",
                    "the",
                    "regularisation",
                    "from",
                    "the",
                    "KL-term",
                    "via",
                    "the",
                    "following",
                    "objective",
                    "function:E",
                    "q",
                    "φ",
                    "(z|x)",
                    "log",
                    "p",
                    "θ",
                    "(x|z)",
                    "−",
                    "βD",
                    "KL",
                    "(q",
                    "φ",
                    "(z|x),",
                    "p(z))Reconstructing",
                    "under",
                    "β-VAE",
                    "(with",
                    "the",
                    "right",
                    "value",
                    "of",
                    "β)",
                    "framework",
                    "encourages",
                    "encoding",
                    "data",
                    "points",
                    "on",
                    "a",
                    "set",
                    "of",
                    "representational",
                    "axes",
                    "on",
                    "which",
                    "nearby",
                    "points",
                    "along",
                    "those",
                    "dimensions",
                    "are",
                    "also",
                    "close",
                    "in",
                    "original",
                    "data",
                    "space",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "#REF",
                    "extends",
                    "β-VAE",
                    "via",
                    "constraint",
                    "optimisation:"
                ]
            ],
            "context": [
                1,
                0
            ]
        },
        "input": "sent0: β-VAE #REF adds a hyperparameter β to control the regularisation from the KL-term via the following objective function:E q φ (z|x) log p θ (x|z) − βD KL (q φ (z|x), p(z))Reconstructing under β-VAE (with the right value of β) framework encourages encoding data points on a set of representational axes on which nearby points along those dimensions are also close in original data space #TARGET_REF .\n sent1: #REF extends β-VAE via constraint optimisation:\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "translator",
                    "perception",
                    "only."
                ],
                [
                    "The",
                    "mismatch",
                    "between",
                    "the",
                    "translators'",
                    "perception",
                    "of",
                    "productivity",
                    "and",
                    "their",
                    "actual",
                    "productivity",
                    "has",
                    "been",
                    "previously",
                    "reported",
                    "by",
                    "Autodesk,",
                    "specifically",
                    "on",
                    "the",
                    "company's",
                    "follow",
                    "up",
                    "work",
                    "on",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "8",
                    "To",
                    "check",
                    "for",
                    "this",
                    "effect,",
                    "we",
                    "asked",
                    "participants",
                    "to",
                    "fill",
                    "in",
                    "a",
                    "short",
                    "questionnaire",
                    "after",
                    "completing",
                    "the",
                    "tasks."
                ],
                [
                    "One",
                    "of",
                    "the",
                    "questions",
                    "asked",
                    "was",
                    "whether",
                    "they",
                    "thought",
                    "having",
                    "the",
                    "MT",
                    "output",
                    "helped",
                    "them",
                    "complete",
                    "the",
                    "translation."
                ],
                [
                    "They",
                    "were",
                    "asked",
                    "to",
                    "mark",
                    "this",
                    "on",
                    "a",
                    "scale",
                    "from",
                    "-5",
                    "to",
                    "5",
                    "where",
                    "-5",
                    "meant",
                    "that",
                    "the",
                    "MT",
                    "output",
                    "had",
                    "greatly",
                    "hindered",
                    "their",
                    "work",
                    "and",
                    "5",
                    "meant",
                    "that",
                    "the",
                    "MT",
                    "output",
                    "had",
                    "greatly",
                    "helped",
                    "their",
                    "work",
                    "(see",
                    "Table",
                    "5)."
                ],
                [
                    "Overall,",
                    "users",
                    "were",
                    "more",
                    "positive",
                    "about",
                    "having",
                    "the",
                    "MT",
                    "output",
                    "displayed",
                    "when",
                    "translating,",
                    "with",
                    "only",
                    "one",
                    "out",
                    "of",
                    "six",
                    "claiming",
                    "that",
                    "it",
                    "hindered",
                    "the",
                    "process."
                ],
                [
                    "In",
                    "the",
                    "case",
                    "of",
                    "translators,",
                    "however,",
                    "three",
                    "out",
                    "of",
                    "six",
                    "heavily",
                    "penalized",
                    "its",
                    "use,",
                    "one",
                    "reported",
                    "that",
                    "it",
                    "was",
                    "better",
                    "than",
                    "not",
                    "having",
                    "it,",
                    "and",
                    "two",
                    "reported",
                    "some",
                    "benefit."
                ],
                [
                    "T1",
                    "commented",
                    "that",
                    "translation",
                    "and",
                    "post-editing",
                    "required",
                    "different",
                    "skills",
                    "and",
                    "that",
                    "should",
                    "the",
                    "same",
                    "time",
                    "be",
                    "spent",
                    "in",
                    "post-editing",
                    "and",
                    "translating,",
                    "the",
                    "translation",
                    "would",
                    "most",
                    "probably",
                    "be",
                    "of",
                    "better",
                    "quality."
                ],
                [
                    "T6",
                    "was",
                    "the",
                    "most",
                    "positive",
                    "of",
                    "all",
                    "with",
                    "regards",
                    "to",
                    "MT",
                    "and",
                    "admitted",
                    "that",
                    "the",
                    "output",
                    "helped",
                    "in",
                    "acquiring",
                    "the",
                    "terminology",
                    "but",
                    "was",
                    "hopeless",
                    "with",
                    "syntax,",
                    "which",
                    "needed",
                    "a",
                    "complete",
                    "rework."
                ],
                [
                    "T2,",
                    "T3",
                    "and",
                    "T4",
                    "indicated",
                    "that",
                    "the",
                    "MT",
                    "output",
                    "had",
                    "clearly",
                    "interfered",
                    "in",
                    "their",
                    "job."
                ],
                [
                    "T2",
                    "reported",
                    "that",
                    "MT",
                    "output",
                    "slowed",
                    "down",
                    "the",
                    "process",
                    "considerably",
                    "because",
                    "reading,",
                    "understanding",
                    "and",
                    "considering",
                    "what",
                    "to",
                    "reuse",
                    "from",
                    "it",
                    "was",
                    "very",
                    "time-consuming."
                ],
                [
                    "T3",
                    "commented",
                    "that",
                    "translating",
                    "from",
                    "scratch",
                    "was",
                    "easier",
                    "and",
                    "faster,",
                    "and",
                    "that",
                    "even",
                    "checking",
                    "the",
                    "MT",
                    "output",
                    "for",
                    "terminology",
                    "would",
                    "most",
                    "often",
                    "not",
                    "help."
                ],
                [
                    "T4",
                    "claimed",
                    "that",
                    "the",
                    "MT",
                    "system",
                    "did",
                    "not",
                    "translate",
                    "the",
                    "order",
                    "of",
                    "the",
                    "phrases",
                    "properly,",
                    "which",
                    "rendered",
                    "the",
                    "translation",
                    "incomprehensible."
                ],
                [
                    "Interestingly,",
                    "T3",
                    "and",
                    "T4",
                    "did",
                    "benefit",
                    "from",
                    "postediting."
                ]
            ],
            "context": [
                0,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: translator perception only.\n sent1: The mismatch between the translators' perception of productivity and their actual productivity has been previously reported by Autodesk, specifically on the company's follow up work on #TARGET_REF .\n sent2: 8 To check for this effect, we asked participants to fill in a short questionnaire after completing the tasks.\n sent3: One of the questions asked was whether they thought having the MT output helped them complete the translation.\n sent4: They were asked to mark this on a scale from -5 to 5 where -5 meant that the MT output had greatly hindered their work and 5 meant that the MT output had greatly helped their work (see Table 5).\n sent5: Overall, users were more positive about having the MT output displayed when translating, with only one out of six claiming that it hindered the process.\n sent6: In the case of translators, however, three out of six heavily penalized its use, one reported that it was better than not having it, and two reported some benefit.\n sent7: T1 commented that translation and post-editing required different skills and that should the same time be spent in post-editing and translating, the translation would most probably be of better quality.\n sent8: T6 was the most positive of all with regards to MT and admitted that the output helped in acquiring the terminology but was hopeless with syntax, which needed a complete rework.\n sent9: T2, T3 and T4 indicated that the MT output had clearly interfered in their job.\n sent10: T2 reported that MT output slowed down the process considerably because reading, understanding and considering what to reuse from it was very time-consuming.\n sent11: T3 commented that translating from scratch was easier and faster, and that even checking the MT output for terminology would most often not help.\n sent12: T4 claimed that the MT system did not translate the order of the phrases properly, which rendered the translation incomprehensible.\n sent13: Interestingly, T3 and T4 did benefit from postediting.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "UFET",
                    "This",
                    "ultra-fine",
                    "entity",
                    "typing",
                    "dataset",
                    "is",
                    "created",
                    "by",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "This",
                    "dataset",
                    "consists",
                    "of",
                    "6k",
                    "manually",
                    "annotated",
                    "examples."
                ],
                [
                    "The",
                    "entity",
                    "mention",
                    "spans",
                    "could",
                    "be",
                    "named",
                    "entities,",
                    "nominal",
                    "expressions,",
                    "and",
                    "pronouns",
                    "while",
                    "Wiki-based",
                    "datasets",
                    "mostly",
                    "provide",
                    "named",
                    "entity",
                    "mention",
                    "spans."
                ],
                [
                    "We",
                    "use",
                    "5.5k",
                    "examples",
                    "for",
                    "training",
                    "and",
                    "500",
                    "examples",
                    "for",
                    "validation."
                ],
                [
                    "Note",
                    "that",
                    "because",
                    "our",
                    "goal",
                    "in",
                    "this",
                    "work",
                    "is",
                    "downstream",
                    "task",
                    "performance,",
                    "we",
                    "deviate",
                    "from",
                    "the",
                    "standard",
                    "train/dev/test",
                    "splits",
                    "of",
                    "2k/2k/2k",
                    "in",
                    "favor",
                    "of",
                    "higher",
                    "performance."
                ]
            ],
            "context": [
                1,
                1,
                1,
                2,
                2
            ]
        },
        "input": "sent0: UFET This ultra-fine entity typing dataset is created by #TARGET_REF .\n sent1: This dataset consists of 6k manually annotated examples.\n sent2: The entity mention spans could be named entities, nominal expressions, and pronouns while Wiki-based datasets mostly provide named entity mention spans.\n sent3: We use 5.5k examples for training and 500 examples for validation.\n sent4: Note that because our goal in this work is downstream task performance, we deviate from the standard train/dev/test splits of 2k/2k/2k in favor of higher performance.\n",
        "output": "{\"INFORMATION\": [\"sent0\", \"sent1\", \"sent2\"], \"PERCEPTION\": [\"sent3\", \"sent4\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Neural",
                    "methods",
                    "for",
                    "abstractive",
                    "summarization",
                    "#REF",
                    "formulate",
                    "summarization",
                    "as",
                    "a",
                    "sequenceto-sequence",
                    "(Seq2Seq)",
                    "problem",
                    "#TARGET_REF",
                    ",",
                    "learning",
                    "to",
                    "generate",
                    "the",
                    "summary",
                    "in",
                    "an",
                    "autoregressive",
                    "manner."
                ],
                [
                    "Such",
                    "models",
                    "are",
                    "commonly",
                    "trained",
                    "with",
                    "maximum",
                    "likelihood",
                    "estimation",
                    "(MLE),",
                    "maximizing",
                    "predictive",
                    "probability",
                    "of",
                    "the",
                    "reference",
                    "output",
                    "given",
                    "the",
                    "gold",
                    "sub-sequence",
                    "before",
                    "it."
                ],
                [
                    "However,",
                    "during",
                    "inference",
                    "the",
                    "model",
                    "must",
                    "also",
                    "generate",
                    "the",
                    "output",
                    "based",
                    "on",
                    "possibly",
                    "erroneous",
                    "previous",
                    "steps."
                ],
                [
                    "This",
                    "can",
                    "hurt",
                    "model",
                    "performance,",
                    "a",
                    "phenomenon",
                    "often",
                    "called",
                    "exposure",
                    "bias",
                    "#REF",
                    "."
                ],
                [
                    "To",
                    "maintain",
                    "reasonable",
                    "performance",
                    "even",
                    "in",
                    "the",
                    "case",
                    "of",
                    "a",
                    "sub-sequence",
                    "with",
                    "errors,",
                    "we",
                    "argue",
                    "that",
                    "the",
                    "model",
                    "must",
                    "accurately",
                    "estimate",
                    "relative",
                    "quality",
                    "of",
                    "different",
                    "generated",
                    "outputs,",
                    "since",
                    "effective",
                    "inference",
                    "requires",
                    "comparison",
                    "among",
                    "these",
                    "candidates."
                ]
            ],
            "context": [
                3,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Neural methods for abstractive summarization #REF formulate summarization as a sequenceto-sequence (Seq2Seq) problem #TARGET_REF , learning to generate the summary in an autoregressive manner.\n sent1: Such models are commonly trained with maximum likelihood estimation (MLE), maximizing predictive probability of the reference output given the gold sub-sequence before it.\n sent2: However, during inference the model must also generate the output based on possibly erroneous previous steps.\n sent3: This can hurt model performance, a phenomenon often called exposure bias #REF .\n sent4: To maintain reasonable performance even in the case of a sub-sequence with errors, we argue that the model must accurately estimate relative quality of different generated outputs, since effective inference requires comparison among these candidates.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "vast",
                    "majority",
                    "of",
                    "the",
                    "dataset",
                    "was",
                    "annotated",
                    "by",
                    "university",
                    "students",
                    "learning",
                    "about",
                    "sentiment",
                    "analysis",
                    "with",
                    "some",
                    "annotations",
                    "provided",
                    "by",
                    "expert",
                    "annotators",
                    "for",
                    "reliability",
                    "measurements",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "The",
                    "students'",
                    "annotation",
                    "process",
                    "was",
                    "monitored",
                    "and",
                    "evaluated."
                ],
                [
                    "They",
                    "received",
                    "only",
                    "minimal",
                    "instructions."
                ],
                [
                    "These",
                    "instructions",
                    "included",
                    "that",
                    "they",
                    "were",
                    "to",
                    "focus",
                    "on",
                    "the",
                    "quality",
                    "of",
                    "annotations",
                    "rather",
                    "than",
                    "quantity,",
                    "and",
                    "to",
                    "annotate",
                    "from",
                    "the",
                    "point",
                    "of",
                    "view",
                    "of",
                    "the",
                    "speaker."
                ],
                [
                    "We",
                    "also",
                    "asked",
                    "for",
                    "feedback",
                    "on",
                    "the",
                    "annotation",
                    "process",
                    "to",
                    "improve",
                    "the",
                    "user-friendliness",
                    "of",
                    "the",
                    "platform",
                    "for",
                    "future",
                    "use."
                ],
                [
                    "In",
                    "tables",
                    "2",
                    "and",
                    "5",
                    "the",
                    "number",
                    "of",
                    "active",
                    "annotators",
                    "have",
                    "been",
                    "included."
                ],
                [
                    "All",
                    "in",
                    "all",
                    "over",
                    "100",
                    "students",
                    "annotated",
                    "at",
                    "least",
                    "some",
                    "sentences",
                    "with",
                    "around",
                    "60",
                    "active",
                    "annotators,",
                    "meaning",
                    "students",
                    "who",
                    "annotated",
                    "more",
                    "than",
                    "300",
                    "sentences",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                3,
                3,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: The vast majority of the dataset was annotated by university students learning about sentiment analysis with some annotations provided by expert annotators for reliability measurements #TARGET_REF .\n sent1: The students' annotation process was monitored and evaluated.\n sent2: They received only minimal instructions.\n sent3: These instructions included that they were to focus on the quality of annotations rather than quantity, and to annotate from the point of view of the speaker.\n sent4: We also asked for feedback on the annotation process to improve the user-friendliness of the platform for future use.\n sent5: In tables 2 and 5 the number of active annotators have been included.\n sent6: All in all over 100 students annotated at least some sentences with around 60 active annotators, meaning students who annotated more than 300 sentences #REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "evaluate",
                    "our",
                    "embedding",
                    "approach",
                    "on",
                    "benchmark",
                    "tasks",
                    "for",
                    "entity",
                    "representations."
                ],
                [
                    "We",
                    "use",
                    "coreference",
                    "arc",
                    "prediction",
                    "(CAP)",
                    "and",
                    "named",
                    "entity",
                    "disambiguation",
                    "on",
                    "CoNLL-YAGO,",
                    "two",
                    "tasks",
                    "in",
                    "the",
                    "EntEval",
                    "suite",
                    "#REF",
                    ",",
                    "as",
                    "well",
                    "as",
                    "entity",
                    "linking",
                    "on",
                    "WikilinksNED",
                    "#TARGET_REF",
                    ",",
                    "which",
                    "covers",
                    "broader",
                    "entities",
                    "and",
                    "writing",
                    "styles."
                ],
                [
                    "We",
                    "compare",
                    "our",
                    "approach",
                    "against",
                    "entity",
                    "representations",
                    "produced",
                    "directly",
                    "by",
                    "pre-trained",
                    "models."
                ]
            ],
            "context": [
                2,
                2,
                0
            ]
        },
        "input": "sent0: We evaluate our embedding approach on benchmark tasks for entity representations.\n sent1: We use coreference arc prediction (CAP) and named entity disambiguation on CoNLL-YAGO, two tasks in the EntEval suite #REF , as well as entity linking on WikilinksNED #TARGET_REF , which covers broader entities and writing styles.\n sent2: We compare our approach against entity representations produced directly by pre-trained models.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\", \"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "A2C",
                    "Training."
                ],
                [
                    "Each",
                    "parallel",
                    "A2C",
                    "agent",
                    "samples",
                    "from",
                    "the",
                    "the",
                    "current",
                    "pool",
                    "of",
                    "available",
                    "questsi.e."
                ],
                [
                    "the",
                    "curriculum-for",
                    "a",
                    "fixed",
                    "number",
                    "of",
                    "steps",
                    "k",
                    "before",
                    "switching",
                    "to",
                    "the",
                    "quest",
                    "pool",
                    "corresponding",
                    "to",
                    "the",
                    "next",
                    "higher",
                    "level",
                    "difficulty",
                    "curriculum."
                ],
                [
                    "The",
                    "initial",
                    "pool",
                    "of",
                    "quests",
                    "is",
                    "the",
                    "training",
                    "set",
                    "of",
                    "LIGHT-Quests",
                    "as",
                    "seen",
                    "in",
                    "#TARGET_REF",
                    "and",
                    "all",
                    "pools",
                    "after",
                    "that",
                    "correspond",
                    "to",
                    "decreasing",
                    "values",
                    "of",
                    "n",
                    "used",
                    "when",
                    "generating",
                    "the",
                    "curriculums",
                    "(as",
                    "seen",
                    "in",
                    "Figure",
                    "6)."
                ]
            ],
            "context": [
                3,
                0,
                0,
                3
            ]
        },
        "input": "sent0: A2C Training.\n sent1: Each parallel A2C agent samples from the the current pool of available questsi.e.\n sent2: the curriculum-for a fixed number of steps k before switching to the quest pool corresponding to the next higher level difficulty curriculum.\n sent3: The initial pool of quests is the training set of LIGHT-Quests as seen in #TARGET_REF and all pools after that correspond to decreasing values of n used when generating the curriculums (as seen in Figure 6).\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "DMSNAP",
                    "complete",
                    "parsing",
                    "in",
                    "the",
                    "order",
                    "of",
                    "milliseconds."
                ],
                [
                    "While",
                    "actual",
                    "SNAP",
                    "hardware",
                    "is",
                    "now",
                    "being",
                    "assembled",
                    "and",
                    "to",
                    "be",
                    "fully",
                    "operational",
                    "by",
                    "May",
                    "1991,",
                    "this",
                    "section",
                    "provides",
                    "performance",
                    "estimation",
                    "with",
                    "precise",
                    "simulation",
                    "of",
                    "the",
                    "SNAP",
                    "machine."
                ],
                [
                    "Simulations",
                    "of",
                    "the",
                    "DMSNAP",
                    "algorithm",
                    "have",
                    "been",
                    "performed",
                    "on",
                    "a",
                    "SUN",
                    "3/280",
                    "using",
                    "the",
                    "SNAP",
                    "simulator",
                    "which",
                    "has",
                    "been",
                    "developed",
                    "at",
                    "USC",
                    "#TARGET_REF",
                    "]."
                ],
                [
                    "The",
                    "simulator",
                    "is",
                    "implemented",
                    "in",
                    "both",
                    "SUN",
                    "Common",
                    "LISP",
                    "and",
                    "C,",
                    "and",
                    "simulates",
                    "the",
                    "SNAP",
                    "machine",
                    "at",
                    "the",
                    "processor",
                    "level."
                ],
                [
                    "The",
                    "LISP",
                    "version",
                    "of",
                    "the",
                    "simulators",
                    "also",
                    "provides",
                    "information",
                    "about",
                    "the",
                    "number",
                    "of",
                    "SNAP",
                    "clock",
                    "cycles",
                    "required",
                    "to",
                    "perform",
                    "the",
                    "simulation."
                ]
            ],
            "context": [
                0,
                0,
                2,
                3,
                0
            ]
        },
        "input": "sent0: DMSNAP complete parsing in the order of milliseconds.\n sent1: While actual SNAP hardware is now being assembled and to be fully operational by May 1991, this section provides performance estimation with precise simulation of the SNAP machine.\n sent2: Simulations of the DMSNAP algorithm have been performed on a SUN 3/280 using the SNAP simulator which has been developed at USC #TARGET_REF ].\n sent3: The simulator is implemented in both SUN Common LISP and C, and simulates the SNAP machine at the processor level.\n sent4: The LISP version of the simulators also provides information about the number of SNAP clock cycles required to perform the simulation.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Our",
                    "model",
                    "f",
                    "θ",
                    "to",
                    "produce",
                    "these",
                    "embeddings",
                    "is",
                    "shown",
                    "in",
                    "Figure",
                    "2:",
                    "it",
                    "takes",
                    "as",
                    "input",
                    "the",
                    "mention",
                    "m",
                    "and",
                    "its",
                    "context",
                    "s",
                    "and",
                    "predicts",
                    "probabilities",
                    "for",
                    "predefined",
                    "entity",
                    "types",
                    "T",
                    "."
                ],
                [
                    "This",
                    "is",
                    "a",
                    "Transformer-based",
                    "typing",
                    "model",
                    "following",
                    "the",
                    "BERT",
                    "model",
                    "presented",
                    "in",
                    "#REF",
                    "."
                ],
                [
                    "First,",
                    "a",
                    "Transformerbased",
                    "encoder",
                    "#TARGET_REF",
                    "maps",
                    "the",
                    "input",
                    "variables,",
                    "m",
                    "and",
                    "s,",
                    "to",
                    "an",
                    "intermediate",
                    "vector",
                    "repre-sentation."
                ],
                [
                    "A",
                    "type",
                    "embedding",
                    "layer",
                    "then",
                    "projects",
                    "the",
                    "intermediate",
                    "representation",
                    "to",
                    "a",
                    "vector",
                    "whose",
                    "dimensions",
                    "correspond",
                    "to",
                    "the",
                    "entity",
                    "types",
                    "T",
                    "."
                ],
                [
                    "Finally,",
                    "we",
                    "apply",
                    "a",
                    "sigmoid",
                    "function",
                    "on",
                    "each",
                    "real-valued",
                    "score",
                    "in",
                    "the",
                    "vector",
                    "to",
                    "obtain",
                    "the",
                    "posterior",
                    "probabilities",
                    "that",
                    "form",
                    "our",
                    "entity",
                    "representation",
                    "t",
                    "(top",
                    "of",
                    "the",
                    "figure)."
                ]
            ],
            "context": [
                0,
                0,
                2,
                0,
                0
            ]
        },
        "input": "sent0: Our model f θ to produce these embeddings is shown in Figure 2: it takes as input the mention m and its context s and predicts probabilities for predefined entity types T .\n sent1: This is a Transformer-based typing model following the BERT model presented in #REF .\n sent2: First, a Transformerbased encoder #TARGET_REF maps the input variables, m and s, to an intermediate vector repre-sentation.\n sent3: A type embedding layer then projects the intermediate representation to a vector whose dimensions correspond to the entity types T .\n sent4: Finally, we apply a sigmoid function on each real-valued score in the vector to obtain the posterior probabilities that form our entity representation t (top of the figure).\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Recognition",
                    "(NER)",
                    "models",
                    "for",
                    "Swahili",
                    "[swh]",
                    "3",
                    "using",
                    "various",
                    "combinations",
                    "of",
                    "Swahili",
                    "text",
                    "data,",
                    "Swahili",
                    "audio",
                    "data,",
                    "Kinyarwanda",
                    "[kin]",
                    "text",
                    "data,",
                    "and",
                    "Kinyarwanda",
                    "audio",
                    "data."
                ],
                [
                    "These",
                    "two",
                    "languages",
                    "both",
                    "originate",
                    "from",
                    "from",
                    "the",
                    "same",
                    "language",
                    "family,",
                    "Bantu,",
                    "and",
                    "are",
                    "spoken",
                    "by",
                    "millions",
                    "of",
                    "people",
                    "in",
                    "Eastern",
                    "Africa,",
                    "often",
                    "within",
                    "the",
                    "same",
                    "country,",
                    "resulting",
                    "in",
                    "some",
                    "overlap",
                    "in",
                    "loan",
                    "words,",
                    "etc."
                ],
                [
                    "4",
                    "However,",
                    "they",
                    "are",
                    "both",
                    "considered",
                    "low-resource",
                    "languages."
                ],
                [
                    "Kinyarwanda",
                    "in",
                    "particular,",
                    "though",
                    "spoken",
                    "by",
                    "approximately",
                    "13-22",
                    "million",
                    "people",
                    "5",
                    ",",
                    "has",
                    "very",
                    "little",
                    "text",
                    "data",
                    "available",
                    "in",
                    "that",
                    "language,",
                    "with",
                    "fewer",
                    "than",
                    "3,000",
                    "articles",
                    "on",
                    "the",
                    "Kinyarwanda-language",
                    "Wikipedia,",
                    "and",
                    "Swahili",
                    "comparatively",
                    "ahead",
                    "but",
                    "still",
                    "poorly",
                    "resourced",
                    "at",
                    "approximately",
                    "68,000",
                    "articles,",
                    "far",
                    "less",
                    "than",
                    "many",
                    "European",
                    "languages."
                ],
                [
                    "6",
                    ",",
                    "though",
                    "some",
                    "datasets",
                    "have",
                    "been",
                    "created",
                    "such",
                    "as",
                    "KINNEWS",
                    "#REF",
                    "."
                ],
                [
                    "On",
                    "the",
                    "other",
                    "hand,",
                    "Kinyarwanda",
                    "is",
                    "uniquely",
                    "placed",
                    "as",
                    "a",
                    "language",
                    "to",
                    "leverage",
                    "speech-based",
                    "technologies,",
                    "due",
                    "to",
                    "well-organized",
                    "efforts",
                    "7",
                    "to",
                    "collect",
                    "voice",
                    "data",
                    "for",
                    "that",
                    "language."
                ],
                [
                    "It",
                    "is",
                    "in",
                    "fact",
                    "one",
                    "of",
                    "the",
                    "largest",
                    "subsets",
                    "available",
                    "on",
                    "the",
                    "Common",
                    "Voice",
                    "Dataset",
                    "#TARGET_REF",
                    ",",
                    "with",
                    "1,183",
                    "hours",
                    "of",
                    "voice",
                    "clips",
                    "collected",
                    "and",
                    "validated."
                ],
                [
                    "Choosing",
                    "these",
                    "two",
                    "languages",
                    "allowed",
                    "us",
                    "to",
                    "test",
                    "the",
                    "use",
                    "of",
                    "the",
                    "technique",
                    "on",
                    "legitimately",
                    "low-resourced",
                    "languages",
                    "that",
                    "could",
                    "benefit",
                    "from",
                    "improved",
                    "NLP",
                    "technology,",
                    "and",
                    "which",
                    "as",
                    "part",
                    "of",
                    "the",
                    "same",
                    "family",
                    "of",
                    "languages",
                    "5",
                    "Sources",
                    "vary:",
                    "Ethnologue",
                    "cites",
                    "\"Total",
                    "users",
                    "in",
                    "all",
                    "countries:",
                    "13,133,980\",",
                    "but",
                    "there",
                    "are",
                    "22",
                    "million",
                    "according",
                    "to",
                    "WorldData.info",
                    "(https://www.worlddata.info/languages/kinyarwanda.php)."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "sent0: Recognition (NER) models for Swahili [swh] 3 using various combinations of Swahili text data, Swahili audio data, Kinyarwanda [kin] text data, and Kinyarwanda audio data.\n sent1: These two languages both originate from from the same language family, Bantu, and are spoken by millions of people in Eastern Africa, often within the same country, resulting in some overlap in loan words, etc.\n sent2: 4 However, they are both considered low-resource languages.\n sent3: Kinyarwanda in particular, though spoken by approximately 13-22 million people 5 , has very little text data available in that language, with fewer than 3,000 articles on the Kinyarwanda-language Wikipedia, and Swahili comparatively ahead but still poorly resourced at approximately 68,000 articles, far less than many European languages.\n sent4: 6 , though some datasets have been created such as KINNEWS #REF .\n sent5: On the other hand, Kinyarwanda is uniquely placed as a language to leverage speech-based technologies, due to well-organized efforts 7 to collect voice data for that language.\n sent6: It is in fact one of the largest subsets available on the Common Voice Dataset #TARGET_REF , with 1,183 hours of voice clips collected and validated.\n sent7: Choosing these two languages allowed us to test the use of the technique on legitimately low-resourced languages that could benefit from improved NLP technology, and which as part of the same family of languages 5 Sources vary: Ethnologue cites \"Total users in all countries: 13,133,980\", but there are 22 million according to WorldData.info (https://www.worlddata.info/languages/kinyarwanda.php).\n",
        "output": "{\"INFORMATION\": [\"sent6\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Commonsense",
                    "explanation",
                    "generation."
                ],
                [
                    "It",
                    "aims",
                    "to",
                    "generate",
                    "an",
                    "explanation",
                    "given",
                    "a",
                    "counterfactual",
                    "statement",
                    "for",
                    "sense-making",
                    "#REF",
                    "."
                ],
                [
                    "We",
                    "use",
                    "the",
                    "benchmark",
                    "dataset",
                    "ComVE",
                    "from",
                    "SemEval-2020",
                    "Task",
                    "4",
                    "#REF",
                    "."
                ],
                [
                    "The",
                    "dataset",
                    "contains",
                    "10,000",
                    "/",
                    "997",
                    "/",
                    "1,000",
                    "examples",
                    "for",
                    "training",
                    "/",
                    "development",
                    "/",
                    "test",
                    "sets,",
                    "respectively."
                ],
                [
                    "The",
                    "average",
                    "input/output",
                    "length",
                    "is",
                    "7.7",
                    "/",
                    "9.0",
                    "words."
                ],
                [
                    "All",
                    "examples",
                    "in",
                    "the",
                    "dataset",
                    "have",
                    "3",
                    "references."
                ],
                [
                    "Abductive",
                    "commonsense",
                    "reasoning."
                ],
                [
                    "It",
                    "is",
                    "also",
                    "referred",
                    "as",
                    "α-NLG."
                ],
                [
                    "It",
                    "is",
                    "the",
                    "task",
                    "of",
                    "generating",
                    "a",
                    "valid",
                    "hypothesis",
                    "about",
                    "the",
                    "likely",
                    "explanations",
                    "to",
                    "partially",
                    "observable",
                    "past",
                    "and",
                    "future."
                ],
                [
                    "We",
                    "use",
                    "the",
                    "ART",
                    "benchmark",
                    "dataset",
                    "#TARGET_REF",
                    "that",
                    "consists",
                    "of",
                    "50,481",
                    "/",
                    "1,779",
                    "/",
                    "3,560",
                    "examples",
                    "for",
                    "training",
                    "/",
                    "development",
                    "/",
                    "test",
                    "sets."
                ],
                [
                    "The",
                    "average",
                    "input/output",
                    "length",
                    "is",
                    "17.4",
                    "/",
                    "10.8",
                    "words."
                ],
                [
                    "Each",
                    "example",
                    "in",
                    "the",
                    "ART",
                    "dataset",
                    "has",
                    "1",
                    "to",
                    "5",
                    "references."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "sent0: Commonsense explanation generation.\n sent1: It aims to generate an explanation given a counterfactual statement for sense-making #REF .\n sent2: We use the benchmark dataset ComVE from SemEval-2020 Task 4 #REF .\n sent3: The dataset contains 10,000 / 997 / 1,000 examples for training / development / test sets, respectively.\n sent4: The average input/output length is 7.7 / 9.0 words.\n sent5: All examples in the dataset have 3 references.\n sent6: Abductive commonsense reasoning.\n sent7: It is also referred as α-NLG.\n sent8: It is the task of generating a valid hypothesis about the likely explanations to partially observable past and future.\n sent9: We use the ART benchmark dataset #TARGET_REF that consists of 50,481 / 1,779 / 3,560 examples for training / development / test sets.\n sent10: The average input/output length is 17.4 / 10.8 words.\n sent11: Each example in the ART dataset has 1 to 5 references.\n",
        "output": "{\"INFORMATION\": [\"sent9\", \"sent10\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "aforementioned",
                    "correlations,",
                    "however,",
                    "are",
                    "byproducts",
                    "rather",
                    "than",
                    "core",
                    "mechanisms",
                    "of",
                    "argumentative",
                    "relations."
                ],
                [
                    "In",
                    "order",
                    "to",
                    "decide",
                    "whether",
                    "a",
                    "statement",
                    "supports",
                    "or",
                    "attacks",
                    "another,",
                    "we",
                    "cannot",
                    "ignore",
                    "the",
                    "logical",
                    "relation",
                    "between",
                    "them."
                ],
                [
                    "Textual",
                    "entailment",
                    "was",
                    "found",
                    "to",
                    "inform",
                    "argumentative",
                    "relations",
                    "#REF",
                    "and",
                    "used",
                    "to",
                    "detect",
                    "arguments",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Similarly,",
                    "there",
                    "is",
                    "evidence",
                    "that",
                    "the",
                    "opinions",
                    "of",
                    "two",
                    "statements",
                    "toward",
                    "the",
                    "same",
                    "concept",
                    "constitute",
                    "their",
                    "argumentative",
                    "relations",
                    "#REF",
                    "."
                ],
                [
                    "Causality",
                    "between",
                    "events",
                    "also",
                    "received",
                    "attention,",
                    "and",
                    "causality",
                    "graph",
                    "construction",
                    "was",
                    "proposed",
                    "for",
                    "argument",
                    "analysis",
                    "#REF",
                    "."
                ],
                [
                    "Additionally,",
                    "in",
                    "argumentation",
                    "theory,",
                    "Walton's",
                    "argumentation",
                    "schemes",
                    "#REF",
                    "specify",
                    "common",
                    "reasoning",
                    "patterns",
                    "people",
                    "use",
                    "to",
                    "form",
                    "an",
                    "argument."
                ],
                [
                    "This",
                    "motivates",
                    "our",
                    "work",
                    "to",
                    "investigate",
                    "logical",
                    "mechanisms",
                    "in",
                    "four",
                    "categories:",
                    "factual",
                    "consistency,",
                    "sentiment",
                    "coherence,",
                    "causal",
                    "relation,",
                    "and",
                    "normative",
                    "relation."
                ]
            ],
            "context": [
                0,
                0,
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: The aforementioned correlations, however, are byproducts rather than core mechanisms of argumentative relations.\n sent1: In order to decide whether a statement supports or attacks another, we cannot ignore the logical relation between them.\n sent2: Textual entailment was found to inform argumentative relations #REF and used to detect arguments #TARGET_REF .\n sent3: Similarly, there is evidence that the opinions of two statements toward the same concept constitute their argumentative relations #REF .\n sent4: Causality between events also received attention, and causality graph construction was proposed for argument analysis #REF .\n sent5: Additionally, in argumentation theory, Walton's argumentation schemes #REF specify common reasoning patterns people use to form an argument.\n sent6: This motivates our work to investigate logical mechanisms in four categories: factual consistency, sentiment coherence, causal relation, and normative relation.\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "typical",
                    "neural",
                    "NLP",
                    "systems,",
                    "entities",
                    "are",
                    "embedded",
                    "in",
                    "the",
                    "same",
                    "space",
                    "as",
                    "other",
                    "words",
                    "either",
                    "in",
                    "context-independent",
                    "#REF",
                    "or",
                    "in",
                    "context-dependent",
                    "ways",
                    "#REF",
                    "."
                ],
                [
                    "Such",
                    "approaches",
                    "are",
                    "powerful:",
                    "pre-trained",
                    "language",
                    "models",
                    "implicitly",
                    "learn",
                    "factual",
                    "knowledge",
                    "about",
                    "those",
                    "entities",
                    "#REF",
                    "and",
                    "these",
                    "representations",
                    "can",
                    "be",
                    "grounded",
                    "in",
                    "structured",
                    "and",
                    "human-curated",
                    "knowledge",
                    "bases",
                    "#REF",
                    "."
                ],
                [
                    "However,",
                    "these",
                    "embeddings",
                    "do",
                    "not",
                    "explicitly",
                    "maintain",
                    "representations",
                    "of",
                    "this",
                    "knowledge,",
                    "and",
                    "dense",
                    "entity",
                    "representations",
                    "are",
                    "not",
                    "directly",
                    "interpretable."
                ],
                [
                    "Knowledge",
                    "probing",
                    "tasks",
                    "can",
                    "be",
                    "used",
                    "to",
                    "measure",
                    "LMs'",
                    "factual",
                    "knowledge",
                    "#REF",
                    ",",
                    "but",
                    "designing",
                    "the",
                    "right",
                    "probing",
                    "task",
                    "is",
                    "another",
                    "hard",
                    "problem",
                    "#TARGET_REF",
                    ",",
                    "particularly",
                    "if",
                    "the",
                    "probes",
                    "are",
                    "parameter-rich",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                0,
                0,
                0,
                3
            ]
        },
        "input": "sent0: In typical neural NLP systems, entities are embedded in the same space as other words either in context-independent #REF or in context-dependent ways #REF .\n sent1: Such approaches are powerful: pre-trained language models implicitly learn factual knowledge about those entities #REF and these representations can be grounded in structured and human-curated knowledge bases #REF .\n sent2: However, these embeddings do not explicitly maintain representations of this knowledge, and dense entity representations are not directly interpretable.\n sent3: Knowledge probing tasks can be used to measure LMs' factual knowledge #REF , but designing the right probing task is another hard problem #TARGET_REF , particularly if the probes are parameter-rich #REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "MOST",
                    "FREQUENT",
                    "baseline",
                    "chooses",
                    "the",
                    "most",
                    "frequently",
                    "observed",
                    "entity",
                    "for",
                    "a",
                    "given",
                    "mention",
                    "as",
                    "a",
                    "prediction,",
                    "based",
                    "on",
                    "a",
                    "prior",
                    "probability",
                    "p",
                    "prior",
                    "computed",
                    "from",
                    "link",
                    "counts",
                    "on",
                    "Wikipedia."
                ],
                [
                    "All",
                    "baselines",
                    "except",
                    "MOST",
                    "FREQUENT",
                    "combine",
                    "the",
                    "classifier",
                    "output",
                    "and",
                    "the",
                    "prior",
                    "probability",
                    "to",
                    "make",
                    "a",
                    "prediction:",
                    "arg",
                    "max",
                    "c",
                    "p",
                    "prior",
                    "(c)",
                    "+",
                    "p",
                    "classifier",
                    "(c)",
                    "."
                ],
                [
                    "10",
                    "data."
                ],
                [
                    "Our",
                    "approach",
                    "outperforms",
                    "all",
                    "baselines,",
                    "indicating",
                    "that",
                    "our",
                    "entity",
                    "representations",
                    "include",
                    "useful",
                    "information",
                    "about",
                    "entities",
                    "out-of-the-box."
                ],
                [
                    "Such",
                    "a",
                    "performance",
                    "gap",
                    "is",
                    "expected",
                    "since",
                    "our",
                    "entity",
                    "representations",
                    "can",
                    "directly",
                    "encode",
                    "some",
                    "factual",
                    "knowledge",
                    "from",
                    "Wikipedia."
                ],
                [
                    "However,",
                    "these",
                    "results",
                    "also",
                    "imply",
                    "that",
                    "pre-trained",
                    "LMs",
                    "do",
                    "not",
                    "have",
                    "enough",
                    "factual",
                    "information",
                    "out-of-the-box,",
                    "they",
                    "may",
                    "rely",
                    "on",
                    "in-domain",
                    "fine-tuning",
                    "to",
                    "achieve",
                    "high",
                    "performance",
                    "in",
                    "the",
                    "target",
                    "domain,",
                    "and",
                    "often",
                    "fail",
                    "to",
                    "generalize",
                    "to",
                    "new",
                    "settings."
                ],
                [
                    "Note",
                    "that",
                    "while",
                    "these",
                    "accuracies",
                    "are",
                    "significantly",
                    "below",
                    "the",
                    "supervised",
                    "state-of-the-art",
                    "(95%),",
                    "they",
                    "are",
                    "competitive",
                    "with",
                    "the",
                    "\"zero-shot\"",
                    "entity",
                    "results",
                    "from",
                    "recent",
                    "past",
                    "work",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                2
            ]
        },
        "input": "sent0: The MOST FREQUENT baseline chooses the most frequently observed entity for a given mention as a prediction, based on a prior probability p prior computed from link counts on Wikipedia.\n sent1: All baselines except MOST FREQUENT combine the classifier output and the prior probability to make a prediction: arg max c p prior (c) + p classifier (c) .\n sent2: 10 data.\n sent3: Our approach outperforms all baselines, indicating that our entity representations include useful information about entities out-of-the-box.\n sent4: Such a performance gap is expected since our entity representations can directly encode some factual knowledge from Wikipedia.\n sent5: However, these results also imply that pre-trained LMs do not have enough factual information out-of-the-box, they may rely on in-domain fine-tuning to achieve high performance in the target domain, and often fail to generalize to new settings.\n sent6: Note that while these accuracies are significantly below the supervised state-of-the-art (95%), they are competitive with the \"zero-shot\" entity results from recent past work #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent6\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "•",
                    "Contrast",
                    "set:",
                    "the",
                    "evaluation",
                    "set",
                    "by",
                    "#TARGET_REF",
                    "that",
                    "is",
                    "created",
                    "based",
                    "on",
                    "the",
                    "official",
                    "Quoref",
                    "test",
                    "set."
                ],
                [
                    "For",
                    "creating",
                    "this",
                    "evaluation",
                    "set,",
                    "the",
                    "authors",
                    "manually",
                    "performed",
                    "small",
                    "but",
                    "meaningful",
                    "perturbations",
                    "to",
                    "the",
                    "test",
                    "examples",
                    "in",
                    "a",
                    "way",
                    "that",
                    "it",
                    "changes",
                    "the",
                    "gold",
                    "label."
                ],
                [
                    "This",
                    "dataset",
                    "is",
                    "constructed",
                    "to",
                    "evaluate",
                    "whether",
                    "models",
                    "decision",
                    "boundaries",
                    "align",
                    "to",
                    "true",
                    "decision",
                    "boundaries",
                    "when",
                    "they",
                    "are",
                    "measured",
                    "around",
                    "the",
                    "same",
                    "point."
                ]
            ],
            "context": [
                3,
                0,
                0
            ]
        },
        "input": "sent0: • Contrast set: the evaluation set by #TARGET_REF that is created based on the official Quoref test set.\n sent1: For creating this evaluation set, the authors manually performed small but meaningful perturbations to the test examples in a way that it changes the gold label.\n sent2: This dataset is constructed to evaluate whether models decision boundaries align to true decision boundaries when they are measured around the same point.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "data",
                    "forming",
                    "the",
                    "multilingual",
                    "corpus",
                    "of",
                    "examples",
                    "are",
                    "derived",
                    "empirically",
                    "from",
                    "real-life",
                    "examples",
                    "of",
                    "job",
                    "adverts."
                ],
                [
                    "For",
                    "practical",
                    "reasons,",
                    "we",
                    "cannot",
                    "expect",
                    "to",
                    "have",
                    "available",
                    "a",
                    "truly",
                    "parallel",
                    "corpus",
                    "of",
                    "texts",
                    "in",
                    "this",
                    "domain."
                ],
                [
                    "The",
                    "contrastive",
                    "linguistic",
                    "knowledge",
                    "of",
                    "the",
                    "system",
                    "cannot",
                    "therefore",
                    "be",
                    "captured",
                    "by",
                    "paired",
                    "examples",
                    "of",
                    "translational",
                    "equivalents",
                    "as",
                    "in",
                    "the",
                    "IBM",
                    "statistical",
                    "approach",
                    "for",
                    "example",
                    "(",
                    "#REF",
                    "),",
                    "so",
                    "the",
                    "more",
                    "abstract",
                    "intentional",
                    "model",
                    "is",
                    "relied",
                    "on",
                    "as",
                    "a",
                    "kind",
                    "of",
                    "mediator,",
                    "where",
                    "it",
                    "is",
                    "the",
                    "functional",
                    "rather",
                    "than",
                    "formal",
                    "property",
                    "of",
                    "the",
                    "text",
                    "fragment",
                    "that",
                    "gives",
                    "its",
                    "target",
                    "language",
                    "counterpart."
                ],
                [
                    "The",
                    "analysis",
                    "of",
                    "the",
                    "multilingual",
                    "corpora",
                    "(",
                    "#TARGET_REF",
                    ")",
                    "provides",
                    "us",
                    "with",
                    "data",
                    "influencing",
                    "the",
                    "design",
                    "of",
                    "the",
                    "linguistic",
                    "representations",
                    "to",
                    "be",
                    "used",
                    "in",
                    "the",
                    "example",
                    "database,",
                    "as",
                    "well",
                    "as",
                    "determining",
                    "the",
                    "content",
                    "and",
                    "form",
                    "of",
                    "both",
                    "the",
                    "intentional",
                    "model",
                    "-where",
                    "the",
                    "functional",
                    "and",
                    "pragmatic",
                    "aspects",
                    "of",
                    "the",
                    "job",
                    "adverts",
                    "are",
                    "defined",
                    "-and",
                    "providing",
                    "information",
                    "to",
                    "enable",
                    "the",
                    "domain",
                    "knowledge",
                    "of",
                    "the",
                    "system",
                    "to",
                    "be",
                    "defined."
                ],
                [
                    "This",
                    "last",
                    "is",
                    "based",
                    "on",
                    "the",
                    "prepositional",
                    "content",
                    "of",
                    "the",
                    "corpus,",
                    "which",
                    "determines",
                    "not",
                    "only",
                    "what",
                    "are",
                    "the",
                    "commonalities",
                    "of",
                    "the",
                    "language",
                    "of",
                    "the",
                    "domain,",
                    "but",
                    "also",
                    "enabling",
                    "illegal",
                    "phrases",
                    "to",
                    "be",
                    "identified,",
                    "as",
                    "well",
                    "as",
                    "revealing",
                    "problems",
                    "of",
                    "non-equivalences,",
                    "especially",
                    "of",
                    "job",
                    "titles",
                    "and",
                    "qualifications,",
                    "etc."
                ],
                [
                    "An",
                    "additional",
                    "point",
                    "of",
                    "interest",
                    "arising",
                    "especially",
                    "from",
                    "the",
                    "text-type",
                    "and",
                    "domain",
                    "chosen",
                    "is",
                    "the",
                    "likelihood",
                    "of",
                    "cultural",
                    "differences",
                    "being",
                    "reflected",
                    "in",
                    "differences",
                    "in",
                    "the",
                    "examples",
                    "and",
                    "hence",
                    "in",
                    "the",
                    "intentional",
                    "models."
                ],
                [
                    "These",
                    "may",
                    "be",
                    "superficial,",
                    "such",
                    "as",
                    "the",
                    "typical",
                    "order",
                    "of",
                    "presenting",
                    "the",
                    "information,",
                    "or",
                    "may",
                    "pose",
                    "more",
                    "serious",
                    "problems",
                    "(see",
                    "below)."
                ]
            ],
            "context": [
                0,
                0,
                0,
                2,
                0,
                0,
                0
            ]
        },
        "input": "sent0: The data forming the multilingual corpus of examples are derived empirically from real-life examples of job adverts.\n sent1: For practical reasons, we cannot expect to have available a truly parallel corpus of texts in this domain.\n sent2: The contrastive linguistic knowledge of the system cannot therefore be captured by paired examples of translational equivalents as in the IBM statistical approach for example ( #REF ), so the more abstract intentional model is relied on as a kind of mediator, where it is the functional rather than formal property of the text fragment that gives its target language counterpart.\n sent3: The analysis of the multilingual corpora ( #TARGET_REF ) provides us with data influencing the design of the linguistic representations to be used in the example database, as well as determining the content and form of both the intentional model -where the functional and pragmatic aspects of the job adverts are defined -and providing information to enable the domain knowledge of the system to be defined.\n sent4: This last is based on the prepositional content of the corpus, which determines not only what are the commonalities of the language of the domain, but also enabling illegal phrases to be identified, as well as revealing problems of non-equivalences, especially of job titles and qualifications, etc.\n sent5: An additional point of interest arising especially from the text-type and domain chosen is the likelihood of cultural differences being reflected in differences in the examples and hence in the intentional models.\n sent6: These may be superficial, such as the typical order of presenting the information, or may pose more serious problems (see below).\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent3\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Visual",
                    "Entailment",
                    "(VE):",
                    "VE",
                    "task",
                    "aims",
                    "to",
                    "predict",
                    "whether",
                    "an",
                    "image",
                    "semantically",
                    "entails",
                    "the",
                    "text",
                    "and",
                    "requires",
                    "fine-grained",
                    "reasoning",
                    "ability",
                    "in",
                    "a",
                    "model."
                ],
                [
                    "VE",
                    "dataset",
                    "is",
                    "built",
                    "upon",
                    "SNLI",
                    "#TARGET_REF",
                    "and",
                    "Flickr30k."
                ],
                [
                    "Each",
                    "image-text",
                    "pair",
                    "is",
                    "assigned",
                    "with",
                    "one",
                    "of",
                    "three",
                    "classes:",
                    "entailment,",
                    "neutral,",
                    "contradiction."
                ],
                [
                    "As",
                    "in",
                    "UNITER,",
                    "we",
                    "formulate",
                    "it",
                    "as",
                    "3-way",
                    "classification",
                    "problem",
                    "based",
                    "on",
                    "h",
                    "cls",
                    "."
                ],
                [
                    "The",
                    "batch",
                    "size",
                    "is",
                    "32",
                    "per",
                    "GPU",
                    "while",
                    "other",
                    "finetuning",
                    "strategies",
                    "are",
                    "the",
                    "same."
                ]
            ],
            "context": [
                3,
                2,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Visual Entailment (VE): VE task aims to predict whether an image semantically entails the text and requires fine-grained reasoning ability in a model.\n sent1: VE dataset is built upon SNLI #TARGET_REF and Flickr30k.\n sent2: Each image-text pair is assigned with one of three classes: entailment, neutral, contradiction.\n sent3: As in UNITER, we formulate it as 3-way classification problem based on h cls .\n sent4: The batch size is 32 per GPU while other finetuning strategies are the same.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "•",
                    "cosine",
                    "similarity",
                    "in",
                    "Skip-gram",
                    "#TARGET_REF",
                    ","
                ]
            ],
            "context": [
                1
            ]
        },
        "input": "sent0: • cosine similarity in Skip-gram #TARGET_REF ,\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "Russian",
                    "language",
                    "news",
                    "articles",
                    "used",
                    "in",
                    "this",
                    "study",
                    "have",
                    "corresponding",
                    "reference",
                    "translations."
                ],
                [
                    "It",
                    "is",
                    "therefore",
                    "possible",
                    "(although",
                    "given",
                    "current",
                    "machine",
                    "translation",
                    "quality,",
                    "highly",
                    "unlikely)",
                    "that",
                    "machine",
                    "translation",
                    "quality",
                    "for",
                    "any",
                    "given",
                    "segment",
                    "could",
                    "conceivably",
                    "surpass",
                    "the",
                    "quality",
                    "of",
                    "the",
                    "corresponding",
                    "reference",
                    "translation",
                    "(if",
                    "for",
                    "example,",
                    "the",
                    "reference",
                    "translator",
                    "makes",
                    "a",
                    "mistake)."
                ],
                [
                    "For",
                    "assessing",
                    "the",
                    "quality",
                    "of",
                    "the",
                    "Russian-English",
                    "machine",
                    "translations,",
                    "then,",
                    "we",
                    "follow",
                    "the",
                    "12-point",
                    "adequacy",
                    "scale",
                    "of",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "This",
                    "adequacy",
                    "scale",
                    "is",
                    "shown",
                    "in",
                    "Table",
                    "1a",
                    "on",
                    "page",
                    "2,",
                    "this",
                    "scale",
                    "ranges",
                    "from",
                    "a",
                    "low",
                    "of",
                    "2",
                    "(the",
                    "English",
                    "translation",
                    "makes",
                    "no",
                    "sense",
                    "at",
                    "all)",
                    "to",
                    "a",
                    "high",
                    "of",
                    "12",
                    "(the",
                    "translation",
                    "is",
                    "superior",
                    "to",
                    "the",
                    "reference)."
                ]
            ],
            "context": [
                0,
                3,
                2,
                3
            ]
        },
        "input": "sent0: The Russian language news articles used in this study have corresponding reference translations.\n sent1: It is therefore possible (although given current machine translation quality, highly unlikely) that machine translation quality for any given segment could conceivably surpass the quality of the corresponding reference translation (if for example, the reference translator makes a mistake).\n sent2: For assessing the quality of the Russian-English machine translations, then, we follow the 12-point adequacy scale of #TARGET_REF .\n sent3: This adequacy scale is shown in Table 1a on page 2, this scale ranges from a low of 2 (the English translation makes no sense at all) to a high of 12 (the translation is superior to the reference).\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": [\"sent1\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "chitecture",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "The",
                    "cross-encoder",
                    "is",
                    "more",
                    "effective",
                    "and",
                    "robust,",
                    "while",
                    "it",
                    "is",
                    "inefficient",
                    "over",
                    "a",
                    "large",
                    "number",
                    "of",
                    "candidates",
                    "in",
                    "inference."
                ],
                [
                    "Hence,",
                    "we",
                    "first",
                    "train",
                    "a",
                    "cross-encoder",
                    "(following",
                    "the",
                    "architecture",
                    "shown",
                    "in",
                    "Figure",
                    "1b)."
                ],
                [
                    "Then,",
                    "when",
                    "sampling",
                    "hard",
                    "negatives",
                    "from",
                    "the",
                    "top-ranked",
                    "passages",
                    "retrieved",
                    "by",
                    "a",
                    "dense",
                    "retriever,",
                    "we",
                    "select",
                    "only",
                    "the",
                    "passages",
                    "that",
                    "are",
                    "predicted",
                    "as",
                    "negatives",
                    "by",
                    "the",
                    "cross-encoder",
                    "with",
                    "high",
                    "confidence",
                    "scores."
                ],
                [
                    "The",
                    "selected",
                    "top-retrieved",
                    "passages",
                    "can",
                    "be",
                    "considered",
                    "as",
                    "denosied",
                    "samples",
                    "that",
                    "are",
                    "more",
                    "reliable",
                    "to",
                    "be",
                    "used",
                    "as",
                    "hard",
                    "negatives."
                ]
            ],
            "context": [
                1,
                2,
                0,
                0,
                0
            ]
        },
        "input": "sent0: chitecture #TARGET_REF .\n sent1: The cross-encoder is more effective and robust, while it is inefficient over a large number of candidates in inference.\n sent2: Hence, we first train a cross-encoder (following the architecture shown in Figure 1b).\n sent3: Then, when sampling hard negatives from the top-ranked passages retrieved by a dense retriever, we select only the passages that are predicted as negatives by the cross-encoder with high confidence scores.\n sent4: The selected top-retrieved passages can be considered as denosied samples that are more reliable to be used as hard negatives.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Since",
                    "the",
                    "removal",
                    "of",
                    "diacritics",
                    "also",
                    "clearly",
                    "leads",
                    "to",
                    "a",
                    "potential",
                    "ambiguity",
                    "as",
                    "explained",
                    "in",
                    "Section",
                    "(2.1)",
                    "there",
                    "has",
                    "been",
                    "some",
                    "work",
                    "on",
                    "automatic",
                    "diacritization",
                    "of",
                    "partially",
                    "diacritized",
                    "or",
                    "undiacritized",
                    "text",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                1
            ]
        },
        "input": "sent0: Since the removal of diacritics also clearly leads to a potential ambiguity as explained in Section (2.1) there has been some work on automatic diacritization of partially diacritized or undiacritized text #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Our",
                    "baseline",
                    "is",
                    "the",
                    "mBERT",
                    "model",
                    "#TARGET_REF",
                    ",",
                    "which",
                    "is",
                    "pre-trained",
                    "using",
                    "pretext",
                    "tasks",
                    "like",
                    "Masked",
                    "Language",
                    "Modelling",
                    "and",
                    "Next",
                    "Sentence",
                    "Prediction",
                    "on",
                    "a",
                    "multilingual",
                    "text",
                    "corpus",
                    "that",
                    "includes",
                    "our",
                    "target",
                    "languages,",
                    "Hindi",
                    "and",
                    "Tamil."
                ],
                [
                    "The",
                    "default",
                    "output",
                    "head",
                    "of",
                    "mBERT",
                    "is",
                    "replaced",
                    "with",
                    "the",
                    "head",
                    "for",
                    "the",
                    "question-answering",
                    "task."
                ],
                [
                    "This",
                    "is",
                    "done",
                    "by",
                    "adding",
                    "separate",
                    "output",
                    "heads",
                    "for",
                    "classifying",
                    "the",
                    "start",
                    "and",
                    "end",
                    "positions",
                    "as",
                    "shown",
                    "in",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                2,
                2,
                2
            ]
        },
        "input": "sent0: Our baseline is the mBERT model #TARGET_REF , which is pre-trained using pretext tasks like Masked Language Modelling and Next Sentence Prediction on a multilingual text corpus that includes our target languages, Hindi and Tamil.\n sent1: The default output head of mBERT is replaced with the head for the question-answering task.\n sent2: This is done by adding separate output heads for classifying the start and end positions as shown in #REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\", \"sent1\", \"sent2\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Despite",
                    "great",
                    "progress",
                    "has",
                    "been",
                    "made",
                    "over",
                    "improved",
                    "accuracy,",
                    "deep",
                    "learning",
                    "models",
                    "are",
                    "known",
                    "to",
                    "be",
                    "brittle",
                    "to",
                    "out-of-domain",
                    "data",
                    "#TARGET_REF",
                    ",",
                    "adversarial",
                    "attacks",
                    "#REF",
                    ",",
                    "partly",
                    "due",
                    "to",
                    "sometimes",
                    "the",
                    "models",
                    "have",
                    "exploited",
                    "spurious",
                    "correlations",
                    "in",
                    "the",
                    "existing",
                    "training",
                    "data",
                    "#REF",
                    "."
                ],
                [
                    "In",
                    "Figure",
                    "1,",
                    "we",
                    "show",
                    "an",
                    "example",
                    "of",
                    "a",
                    "sentiment",
                    "classification",
                    "model",
                    "making",
                    "spurious",
                    "correlations",
                    "over",
                    "the",
                    "phrases",
                    "\"Spielberg\"",
                    "and",
                    "\"New",
                    "York",
                    "Subway\"",
                    "due",
                    "to",
                    "their",
                    "high",
                    "co-occurrences",
                    "with",
                    "positive",
                    "and",
                    "negative",
                    "labels",
                    "respectively",
                    "in",
                    "the",
                    "training",
                    "data."
                ]
            ],
            "context": [
                3,
                0
            ]
        },
        "input": "sent0: Despite great progress has been made over improved accuracy, deep learning models are known to be brittle to out-of-domain data #TARGET_REF , adversarial attacks #REF , partly due to sometimes the models have exploited spurious correlations in the existing training data #REF .\n sent1: In Figure 1, we show an example of a sentiment classification model making spurious correlations over the phrases \"Spielberg\" and \"New York Subway\" due to their high co-occurrences with positive and negative labels respectively in the training data.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "use",
                    "the",
                    "Transformer-Base",
                    "model",
                    "#REF",
                    "which",
                    "has",
                    "6",
                    "layers",
                    "each",
                    "in",
                    "the",
                    "three",
                    "components:",
                    "encoder",
                    "self-attention",
                    "(ES),",
                    "encoderdecoder",
                    "cross-attention",
                    "(ED),",
                    "and",
                    "decoder",
                    "selfattention",
                    "(DS)."
                ],
                [
                    "In",
                    "each",
                    "layer",
                    "of",
                    "each",
                    "of",
                    "the",
                    "three",
                    "components,",
                    "we",
                    "have",
                    "8",
                    "attention",
                    "heads,",
                    "totalling",
                    "to",
                    "3",
                    "×",
                    "6",
                    "×",
                    "8",
                    "=",
                    "144",
                    "attention",
                    "heads."
                ],
                [
                    "We",
                    "train",
                    "the",
                    "mod-els",
                    "with",
                    "2.5",
                    "million",
                    "sentence",
                    "pairs",
                    "each",
                    "from",
                    "the",
                    "WMT'14",
                    "English-Russian",
                    "(EN-RU)",
                    "and",
                    "English-German",
                    "(EN-DE)",
                    "datasets."
                ],
                [
                    "We",
                    "report",
                    "BLEU",
                    "scores",
                    "on",
                    "WMT's",
                    "newstest2014."
                ],
                [
                    "We",
                    "use",
                    "Adam",
                    "optimizer",
                    "(Kingma",
                    "and",
                    "Ba,",
                    "2014)",
                    "with",
                    "parameters",
                    "β",
                    "1",
                    "=",
                    "0.9,",
                    "β",
                    "2",
                    "=",
                    "0.997,",
                    "and",
                    "=",
                    "10",
                    "−9",
                    "."
                ],
                [
                    "We",
                    "vary",
                    "the",
                    "learning",
                    "rate",
                    "according",
                    "to",
                    "the",
                    "formula",
                    "described",
                    "in",
                    "#REF",
                    "with",
                    "warmup",
                    "steps",
                    "=",
                    "16k."
                ],
                [
                    "We",
                    "use",
                    "large",
                    "batch",
                    "sizes",
                    "of",
                    "32k",
                    "and",
                    "25k",
                    "for",
                    "EN-RU",
                    "and",
                    "EN-DE,",
                    "respectively,",
                    "as",
                    "it",
                    "has",
                    "been",
                    "established",
                    "that",
                    "large",
                    "batch",
                    "sizes",
                    "are",
                    "inherent",
                    "to",
                    "the",
                    "performance",
                    "of",
                    "Transformers",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "We",
                    "achieve",
                    "effectively",
                    "large",
                    "batch",
                    "sizes",
                    "using",
                    "the",
                    "technique",
                    "of",
                    "gradient",
                    "accumulation",
                    "on",
                    "single",
                    "NVIDIA",
                    "V100",
                    "and",
                    "1080Ti",
                    "GPUs."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                0
            ]
        },
        "input": "sent0: We use the Transformer-Base model #REF which has 6 layers each in the three components: encoder self-attention (ES), encoderdecoder cross-attention (ED), and decoder selfattention (DS).\n sent1: In each layer of each of the three components, we have 8 attention heads, totalling to 3 × 6 × 8 = 144 attention heads.\n sent2: We train the mod-els with 2.5 million sentence pairs each from the WMT'14 English-Russian (EN-RU) and English-German (EN-DE) datasets.\n sent3: We report BLEU scores on WMT's newstest2014.\n sent4: We use Adam optimizer (Kingma and Ba, 2014) with parameters β 1 = 0.9, β 2 = 0.997, and = 10 −9 .\n sent5: We vary the learning rate according to the formula described in #REF with warmup steps = 16k.\n sent6: We use large batch sizes of 32k and 25k for EN-RU and EN-DE, respectively, as it has been established that large batch sizes are inherent to the performance of Transformers #TARGET_REF .\n sent7: We achieve effectively large batch sizes using the technique of gradient accumulation on single NVIDIA V100 and 1080Ti GPUs.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent6\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "[EMB]",
                    "0",
                    "0",
                    "CLFR",
                    "0",
                    "0",
                    "CLFR",
                    "1",
                    "0",
                    "CLFR",
                    "0",
                    "1",
                    "CLFR",
                    "0",
                    "0",
                    "S",
                    "E",
                    "S",
                    "E",
                    "S",
                    "E",
                    "S",
                    "E",
                    "CLFR",
                    "NT",
                    "T",
                    "1",
                    "0",
                    "CLFR",
                    "NT",
                    "T",
                    "1",
                    "0",
                    "CLFR",
                    "NT",
                    "T",
                    "0",
                    "1",
                    "CLFR",
                    "NT",
                    "T",
                    "0",
                    "1",
                    "CLFR",
                    "NT",
                    "T",
                    "X",
                    "X",
                    "you",
                    "pathetic",
                    "troll",
                    "[SEP]",
                    "[CLS]",
                    "(b)",
                    "Span+Token",
                    "BERT-based",
                    "Model",
                    "[EMB]",
                    "[EMB]",
                    "[EMB]",
                    "CLFR",
                    "S",
                    "E",
                    "[EMB]",
                    "[EMB]",
                    "0",
                    "0",
                    "CLFR",
                    "0",
                    "0",
                    "CLFR",
                    "1",
                    "0",
                    "CLFR",
                    "0",
                    "1",
                    "CLFR",
                    "0",
                    "0",
                    "S",
                    "E",
                    "S",
                    "E",
                    "S",
                    "E",
                    "S",
                    "E",
                    "you",
                    "pathetic",
                    "troll",
                    "[SEP]",
                    "[CLS]",
                    "(c)",
                    "Span",
                    "Prediction",
                    "BERT-based",
                    "Model",
                    "[EMB]",
                    "CLFR",
                    "1",
                    "0",
                    "S",
                    "E",
                    "[EMB]",
                    "CLFR",
                    "0",
                    "1",
                    "S",
                    "E",
                    "[EMB]",
                    "CLFR",
                    "0",
                    "0",
                    "S",
                    "E",
                    "[EMB]",
                    "CLFR",
                    "1",
                    "0",
                    "S",
                    "E",
                    "[EMB]",
                    "CLFR",
                    "0",
                    "1",
                    "S",
                    "E",
                    "[EMB]",
                    "CLFR",
                    "0",
                    "0",
                    "S",
                    "E",
                    "[EMB]",
                    "CLFR",
                    "0",
                    "0",
                    "S",
                    "E",
                    "[CLS]",
                    "dumb",
                    "ignorant",
                    "boy",
                    "pathetic",
                    "troll",
                    "[SEP]",
                    "(d)",
                    "Multi-Spans",
                    "BERT-based",
                    "Model",
                    "[EMB]",
                    "[EMB]",
                    "[EMB]",
                    "BiLSTM",
                    "NT",
                    "T",
                    "[EMB]",
                    "[EMB]",
                    "0",
                    "0",
                    "BiLSTM",
                    "BiLSTM",
                    "BiLSTM",
                    "BiLSTM",
                    "CRF",
                    "D",
                    "1",
                    "NT",
                    "T",
                    "1",
                    "0",
                    "0",
                    "D",
                    "NT",
                    "T",
                    "0",
                    "1",
                    "0",
                    "D",
                    "NT",
                    "T",
                    "0",
                    "1",
                    "0",
                    "D",
                    "NT",
                    "T",
                    "X",
                    "X",
                    "X",
                    "D",
                    "you",
                    "pathetic",
                    "troll",
                    "[SEP]",
                    "[CLS]",
                    "(e)",
                    "LSTM-CRF"
                ]
            ],
            "context": [
                0
            ]
        },
        "input": "sent0: [EMB] 0 0 CLFR 0 0 CLFR 1 0 CLFR 0 1 CLFR 0 0 S E S E S E S E CLFR NT T 1 0 CLFR NT T 1 0 CLFR NT T 0 1 CLFR NT T 0 1 CLFR NT T X X you pathetic troll [SEP] [CLS] (b) Span+Token BERT-based Model [EMB] [EMB] [EMB] CLFR S E [EMB] [EMB] 0 0 CLFR 0 0 CLFR 1 0 CLFR 0 1 CLFR 0 0 S E S E S E S E you pathetic troll [SEP] [CLS] (c) Span Prediction BERT-based Model [EMB] CLFR 1 0 S E [EMB] CLFR 0 1 S E [EMB] CLFR 0 0 S E [EMB] CLFR 1 0 S E [EMB] CLFR 0 1 S E [EMB] CLFR 0 0 S E [EMB] CLFR 0 0 S E [CLS] dumb ignorant boy pathetic troll [SEP] (d) Multi-Spans BERT-based Model [EMB] [EMB] [EMB] BiLSTM NT T [EMB] [EMB] 0 0 BiLSTM BiLSTM BiLSTM BiLSTM CRF D 1 NT T 1 0 0 D NT T 0 1 0 D NT T 0 1 0 D NT T X X X D you pathetic troll [SEP] [CLS] (e) LSTM-CRF\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Results."
                ],
                [
                    "For",
                    "the",
                    "SVM,",
                    "the",
                    "3-1",
                    "pretraining",
                    "without",
                    "data",
                    "augmentation",
                    "resulted",
                    "in",
                    "the",
                    "highest",
                    "dev",
                    "set",
                    "accuracy",
                    "(32.03%),",
                    "though",
                    "accuracy",
                    "was",
                    "only",
                    "slightly",
                    "better",
                    "than",
                    "for",
                    "direct",
                    "single-trial",
                    "training",
                    "(31.93%)."
                ],
                [
                    "For",
                    "the",
                    "Transformer,",
                    "the",
                    "3-1",
                    "pretraining",
                    "scheme",
                    "with",
                    "250k",
                    "data",
                    "augmentation",
                    "obtained",
                    "the",
                    "highest",
                    "single-trial",
                    "decoding",
                    "accuracy",
                    "(39.41%)",
                    "on",
                    "the",
                    "dev",
                    "set."
                ],
                [
                    "Indeed,",
                    "Wilcoxon",
                    "signed-rank",
                    "test",
                    "#TARGET_REF",
                    "confirmed",
                    "that",
                    "the",
                    "best",
                    "dev",
                    "set",
                    "Transformer",
                    "performed",
                    "significantly",
                    "better",
                    "on",
                    "the",
                    "test",
                    "set",
                    "after",
                    "3-1",
                    "pretraining",
                    "than",
                    "after",
                    "direct",
                    "single-trial",
                    "training",
                    "(p",
                    "&lt,",
                    "0.01)."
                ]
            ],
            "context": [
                0,
                3,
                3,
                2
            ]
        },
        "input": "sent0: Results.\n sent1: For the SVM, the 3-1 pretraining without data augmentation resulted in the highest dev set accuracy (32.03%), though accuracy was only slightly better than for direct single-trial training (31.93%).\n sent2: For the Transformer, the 3-1 pretraining scheme with 250k data augmentation obtained the highest single-trial decoding accuracy (39.41%) on the dev set.\n sent3: Indeed, Wilcoxon signed-rank test #TARGET_REF confirmed that the best dev set Transformer performed significantly better on the test set after 3-1 pretraining than after direct single-trial training (p &lt, 0.01).\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent3\"], \"BACKGROUND\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Large",
                    "explanations",
                    "are",
                    "typically",
                    "evaluated",
                    "on",
                    "two",
                    "dimensions:",
                    "relevance",
                    "and",
                    "completeness."
                ],
                [
                    "Relevance",
                    "refers",
                    "to",
                    "whether",
                    "each",
                    "fact",
                    "in",
                    "an",
                    "explanation",
                    "is",
                    "relevant,",
                    "topical,",
                    "and",
                    "required",
                    "to",
                    "complete",
                    "the",
                    "chain",
                    "of",
                    "inference",
                    "that",
                    "moves",
                    "from",
                    "question",
                    "to",
                    "correct",
                    "answer."
                ],
                [
                    "Conversely,",
                    "completeness",
                    "evaluates",
                    "whether",
                    "the",
                    "entire",
                    "set",
                    "of",
                    "facts",
                    "in",
                    "the",
                    "explanation,",
                    "together,",
                    "composes",
                    "a",
                    "complete",
                    "chain",
                    "of",
                    "inference",
                    "from",
                    "question",
                    "to",
                    "answer,",
                    "without",
                    "significant",
                    "gaps."
                ],
                [
                    "In",
                    "practice,",
                    "both",
                    "of",
                    "these",
                    "are",
                    "challenging",
                    "to",
                    "evaluate",
                    "automatically",
                    "#TARGET_REF",
                    ",",
                    "given",
                    "that",
                    "multi-hop",
                    "datasets",
                    "typically",
                    "include",
                    "a",
                    "single",
                    "example",
                    "of",
                    "a",
                    "complete",
                    "explanation,",
                    "in",
                    "large",
                    "part",
                    "due",
                    "to",
                    "the",
                    "time",
                    "and",
                    "expense",
                    "associated",
                    "with",
                    "generating",
                    "such",
                    "annotation."
                ],
                [
                    "Underscoring",
                    "this",
                    "difficulty,",
                    "post-competition",
                    "manual",
                    "analyses",
                    "on",
                    "participating",
                    "systems",
                    "in",
                    "the",
                    "previous",
                    "two",
                    "iterations",
                    "of",
                    "this",
                    "shared",
                    "task",
                    "showed",
                    "that",
                    "models",
                    "may",
                    "be",
                    "performing",
                    "up",
                    "to",
                    "20%",
                    "better",
                    "at",
                    "retrieving",
                    "correct",
                    "facts",
                    "to",
                    "build",
                    "their",
                    "explanation",
                    "from,",
                    "highlighting",
                    "this",
                    "significant",
                    "methodological",
                    "challenge."
                ]
            ],
            "context": [
                3,
                3,
                3,
                2,
                2
            ]
        },
        "input": "sent0: Large explanations are typically evaluated on two dimensions: relevance and completeness.\n sent1: Relevance refers to whether each fact in an explanation is relevant, topical, and required to complete the chain of inference that moves from question to correct answer.\n sent2: Conversely, completeness evaluates whether the entire set of facts in the explanation, together, composes a complete chain of inference from question to answer, without significant gaps.\n sent3: In practice, both of these are challenging to evaluate automatically #TARGET_REF , given that multi-hop datasets typically include a single example of a complete explanation, in large part due to the time and expense associated with generating such annotation.\n sent4: Underscoring this difficulty, post-competition manual analyses on participating systems in the previous two iterations of this shared task showed that models may be performing up to 20% better at retrieving correct facts to build their explanation from, highlighting this significant methodological challenge.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent3\", \"sent4\"], \"BACKGROUND\": [\"sent0\", \"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Neural",
                    "Machine",
                    "Translation",
                    "(NMT)",
                    "approach",
                    "has",
                    "been",
                    "further",
                    "developed",
                    "in",
                    "the",
                    "last",
                    "years",
                    "#REF",
                    "."
                ],
                [
                    "In",
                    "contrast",
                    "to",
                    "the",
                    "traditional",
                    "phrased-based",
                    "statistical",
                    "machine",
                    "translation",
                    "#REF",
                    "that",
                    "represents",
                    "and",
                    "translates",
                    "the",
                    "input",
                    "sentence",
                    "with",
                    "a",
                    "set",
                    "of",
                    "phrases,",
                    "NMT",
                    "uses",
                    "the",
                    "sequence",
                    "to",
                    "sequence",
                    "learning",
                    "architecture",
                    "and",
                    "the",
                    "whole",
                    "input",
                    "sentence",
                    "is",
                    "considered",
                    "as",
                    "one",
                    "unit",
                    "for",
                    "translation",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Recently,",
                    "NMT",
                    "is",
                    "gaining",
                    "more",
                    "and",
                    "more",
                    "interest",
                    "and",
                    "showing",
                    "better",
                    "accuracy",
                    "than",
                    "phrase-based",
                    "system",
                    "translating",
                    "several",
                    "language",
                    "pairs."
                ],
                [
                    "In",
                    "spite",
                    "of",
                    "these",
                    "recent",
                    "improvements,",
                    "the",
                    "NMT",
                    "systems",
                    "still",
                    "have",
                    "some",
                    "restrictions",
                    "and",
                    "difficulties",
                    "to",
                    "translate."
                ],
                [
                    "One",
                    "of",
                    "them",
                    "is",
                    "the",
                    "high",
                    "computational",
                    "of",
                    "the",
                    "softmax",
                    "function",
                    "which",
                    "requires",
                    "to",
                    "normalize",
                    "all",
                    "the",
                    "output",
                    "vocabulary",
                    "size."
                ]
            ],
            "context": [
                3,
                1,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Neural Machine Translation (NMT) approach has been further developed in the last years #REF .\n sent1: In contrast to the traditional phrased-based statistical machine translation #REF that represents and translates the input sentence with a set of phrases, NMT uses the sequence to sequence learning architecture and the whole input sentence is considered as one unit for translation #TARGET_REF .\n sent2: Recently, NMT is gaining more and more interest and showing better accuracy than phrase-based system translating several language pairs.\n sent3: In spite of these recent improvements, the NMT systems still have some restrictions and difficulties to translate.\n sent4: One of them is the high computational of the softmax function which requires to normalize all the output vocabulary size.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "approach",
                    "this",
                    "promise",
                    "with",
                    "caution,",
                    "however,",
                    "given",
                    "the",
                    "painful",
                    "lessons",
                    "learned",
                    "through",
                    "the",
                    "historical",
                    "difficulty",
                    "of",
                    "making",
                    "syntactic",
                    "and",
                    "semantic",
                    "models",
                    "contribute",
                    "to",
                    "improving",
                    "SMT",
                    "accuracy."
                ],
                [
                    "The",
                    "past",
                    "decade",
                    "has",
                    "at",
                    "last",
                    "seen",
                    "increasing",
                    "amounts",
                    "of",
                    "evidence",
                    "that",
                    "SMT",
                    "accuracy",
                    "can",
                    "indeed",
                    "be",
                    "improved",
                    "via",
                    "tree-structured",
                    "and",
                    "syntactic",
                    "models",
                    "(e.g.,",
                    "#REF",
                    ",",
                    "#REF",
                    ",",
                    "#REF",
                    ")",
                    "despite",
                    "numerous",
                    "disappoint-ing",
                    "attempts",
                    "#REF",
                    "."
                ],
                [
                    "More",
                    "recently,",
                    "lexical",
                    "semantics",
                    "models",
                    "for",
                    "word",
                    "sense",
                    "disambiguation",
                    "have",
                    "also",
                    "finally",
                    "been",
                    "successfully",
                    "applied",
                    "to",
                    "increasing",
                    "SMT",
                    "accuracy",
                    "(e.g.,",
                    "#REF",
                    ",",
                    "#REF",
                    ")",
                    "again",
                    "after",
                    "surprising",
                    "initial",
                    "failures",
                    "(e.g.,",
                    "#TARGET_REF",
                    ")."
                ],
                [
                    "In",
                    "both",
                    "the",
                    "syntactic",
                    "and",
                    "semantic",
                    "cases,",
                    "improving",
                    "SMT",
                    "accuracy",
                    "ultimately",
                    "required",
                    "making",
                    "major",
                    "adaptations",
                    "to",
                    "the",
                    "original",
                    "linguistic",
                    "models."
                ],
                [
                    "We",
                    "can",
                    "reasonably",
                    "expect",
                    "it",
                    "to",
                    "be",
                    "at",
                    "least",
                    "as",
                    "difficult",
                    "to",
                    "successfully",
                    "adapt",
                    "the",
                    "even",
                    "more",
                    "complex",
                    "types",
                    "of",
                    "lexical",
                    "semantics",
                    "modeling",
                    "from",
                    "semantic",
                    "parsing",
                    "and",
                    "role",
                    "labeling."
                ]
            ],
            "context": [
                0,
                3,
                3,
                0,
                0
            ]
        },
        "input": "sent0: We approach this promise with caution, however, given the painful lessons learned through the historical difficulty of making syntactic and semantic models contribute to improving SMT accuracy.\n sent1: The past decade has at last seen increasing amounts of evidence that SMT accuracy can indeed be improved via tree-structured and syntactic models (e.g., #REF , #REF , #REF ) despite numerous disappoint-ing attempts #REF .\n sent2: More recently, lexical semantics models for word sense disambiguation have also finally been successfully applied to increasing SMT accuracy (e.g., #REF , #REF ) again after surprising initial failures (e.g., #TARGET_REF ).\n sent3: In both the syntactic and semantic cases, improving SMT accuracy ultimately required making major adaptations to the original linguistic models.\n sent4: We can reasonably expect it to be at least as difficult to successfully adapt the even more complex types of lexical semantics modeling from semantic parsing and role labeling.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "DMSNAP",
                    "is",
                    "capable",
                    "of",
                    "resolving",
                    "this",
                    "lexical",
                    "ambiguity",
                    "through",
                    "use",
                    "of",
                    "contextual",
                    "priming",
                    "using",
                    "the",
                    "contextual",
                    "marker",
                    "(C-Marker)",
                    "#TARGET_REF",
                    "and",
                    "the",
                    "cost-based",
                    "disambiguation",
                    "#REF",
                    "."
                ],
                [
                    "Sentence",
                    "s3",
                    "contains",
                    "a",
                    "word",
                    "sense",
                    "ambiguity",
                    "in",
                    "the",
                    "interpretation",
                    "of",
                    "the",
                    "word",
                    "'paper''",
                    "as",
                    "either",
                    "a",
                    "technical",
                    "document",
                    "or",
                    "a",
                    "sheet",
                    "of",
                    "paper."
                ],
                [
                    "Upon",
                    "reading",
                    "'paper',",
                    "C-THESIS",
                    "and",
                    "C-PAPER",
                    "are",
                    "activated."
                ],
                [
                    "At",
                    "this",
                    "time,",
                    "C-THESIS",
                    "has",
                    "a",
                    "C-MARKER."
                ],
                [
                    "The",
                    "C-MARKER",
                    "comes",
                    "from",
                    "activation",
                    "of",
                    "C-IJCAI-91",
                    "and",
                    "C-CONFERENCE,",
                    "in",
                    "previous",
                    "sentences,",
                    "which",
                    "has",
                    "contextual",
                    "links",
                    "connecting",
                    "concepts",
                    "relevant",
                    "to",
                    "academic",
                    "conference",
                    "such",
                    "as",
                    "C-THESIS."
                ],
                [
                    "The",
                    "meaning",
                    "hypothesis",
                    "containing",
                    "C-THESIS",
                    "costs",
                    "less",
                    "than",
                    "the",
                    "one",
                    "with",
                    "C-PAPER",
                    "so",
                    "that",
                    "it",
                    "is",
                    "selected",
                    "as",
                    "the",
                    "best",
                    "hypothesis."
                ]
            ],
            "context": [
                2,
                3,
                3,
                3,
                3,
                3
            ]
        },
        "input": "sent0: DMSNAP is capable of resolving this lexical ambiguity through use of contextual priming using the contextual marker (C-Marker) #TARGET_REF and the cost-based disambiguation #REF .\n sent1: Sentence s3 contains a word sense ambiguity in the interpretation of the word 'paper'' as either a technical document or a sheet of paper.\n sent2: Upon reading 'paper', C-THESIS and C-PAPER are activated.\n sent3: At this time, C-THESIS has a C-MARKER.\n sent4: The C-MARKER comes from activation of C-IJCAI-91 and C-CONFERENCE, in previous sentences, which has contextual links connecting concepts relevant to academic conference such as C-THESIS.\n sent5: The meaning hypothesis containing C-THESIS costs less than the one with C-PAPER so that it is selected as the best hypothesis.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": [\"sent1\", \"sent2\", \"sent3\", \"sent4\", \"sent5\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Overall",
                    "training",
                    "is",
                    "done",
                    "via",
                    "A2C",
                    "#TARGET_REF",
                    "a",
                    "policy",
                    "gradient",
                    "algorithm",
                    "that",
                    "maximizes",
                    "long-term",
                    "expected",
                    "reward",
                    "by",
                    "comparing",
                    "the",
                    "advantage",
                    "A(s",
                    "t",
                    ",",
                    "a",
                    "*",
                    "t",
                    ")",
                    "of",
                    "taking",
                    "an",
                    "action",
                    "a",
                    "t",
                    "in",
                    "a",
                    "state",
                    "s",
                    "t",
                    "to",
                    "the",
                    "average",
                    "value",
                    "of",
                    "taking",
                    "any",
                    "valid",
                    "action",
                    "as",
                    "predicted",
                    "by",
                    "the",
                    "critic",
                    "V",
                    "(s",
                    "t",
                    ")."
                ],
                [
                    "The",
                    "setup",
                    "and",
                    "network",
                    "architectures",
                    "used",
                    "are",
                    "similar",
                    "to",
                    "#REF",
                    "and",
                    "are",
                    "summarized",
                    "in",
                    "Figure",
                    "5."
                ],
                [
                    "At",
                    "every",
                    "step,",
                    "the",
                    "LIGHT",
                    "agent",
                    "receives",
                    "as",
                    "input",
                    "the",
                    "text",
                    "describing",
                    "the",
                    "setting,",
                    "the",
                    "character's",
                    "persona",
                    "&amp,",
                    "motivation,",
                    "and",
                    "the",
                    "full",
                    "dialogue",
                    "history."
                ],
                [
                    "This",
                    "is",
                    "then",
                    "encoded",
                    "using",
                    "a",
                    "transformer",
                    "based",
                    "encoder",
                    "and",
                    "sent",
                    "to",
                    "the",
                    "action",
                    "and",
                    "dialogue",
                    "policy",
                    "networks",
                    "which",
                    "output",
                    "an",
                    "action/dialogue",
                    "utterance."
                ],
                [
                    "These",
                    "are",
                    "then",
                    "passed",
                    "into",
                    "the",
                    "LIGHT",
                    "environment",
                    "which",
                    "process",
                    "them",
                    "and",
                    "returns",
                    "rewards",
                    "to",
                    "be",
                    "used",
                    "by",
                    "the",
                    "agent."
                ]
            ],
            "context": [
                1,
                2,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Overall training is done via A2C #TARGET_REF a policy gradient algorithm that maximizes long-term expected reward by comparing the advantage A(s t , a * t ) of taking an action a t in a state s t to the average value of taking any valid action as predicted by the critic V (s t ).\n sent1: The setup and network architectures used are similar to #REF and are summarized in Figure 5.\n sent2: At every step, the LIGHT agent receives as input the text describing the setting, the character's persona &amp, motivation, and the full dialogue history.\n sent3: This is then encoded using a transformer based encoder and sent to the action and dialogue policy networks which output an action/dialogue utterance.\n sent4: These are then passed into the LIGHT environment which process them and returns rewards to be used by the agent.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "There",
                    "are",
                    "two",
                    "approaches",
                    "to",
                    "assessing",
                    "the",
                    "productivity",
                    "of",
                    "a",
                    "word",
                    "formation",
                    "rule:",
                    "the",
                    "qualitative",
                    "and",
                    "the",
                    "quantitative",
                    "approach."
                ],
                [
                    "For",
                    "a",
                    "full",
                    "understanding,",
                    "a",
                    "combination",
                    "of",
                    "both",
                    "perspectives",
                    "is",
                    "necessary."
                ],
                [
                    "In",
                    "the",
                    "qualitative",
                    "approach,",
                    "all",
                    "the",
                    "restrictions",
                    "and",
                    "linguistic",
                    "properties",
                    "of",
                    "the",
                    "rule",
                    "are",
                    "described",
                    "(see",
                    "the",
                    "next",
                    "paragraph",
                    "for",
                    "some",
                    "examples)."
                ],
                [
                    "The",
                    "qualitative",
                    "description",
                    "does",
                    "not",
                    "exhaust",
                    "our",
                    "intuition",
                    "of",
                    "productivity,",
                    "there",
                    "is",
                    "also",
                    "a",
                    "quantitative",
                    "element",
                    "involved:",
                    "intuitively",
                    "one",
                    "could",
                    "say",
                    "that",
                    "some",
                    "word",
                    "formation",
                    "rules",
                    "seem",
                    "to",
                    "produce",
                    "new",
                    "words",
                    "more",
                    "readily",
                    "than",
                    "others",
                    "-an",
                    "intuition",
                    "which",
                    "cannot",
                    "be",
                    "formalized."
                ],
                [
                    "In",
                    "quantitative",
                    "studies",
                    "this",
                    "intuition",
                    "is",
                    "approximated",
                    "by",
                    "the",
                    "question:",
                    "how",
                    "probable",
                    "is",
                    "it",
                    "that",
                    "we",
                    "will",
                    "see",
                    "a",
                    "new",
                    "type",
                    "(lexeme)",
                    "produced",
                    "by",
                    "word",
                    "formation",
                    "process",
                    "X",
                    "after",
                    "we",
                    "have",
                    "sampled",
                    "a",
                    "certain",
                    "amount",
                    "of",
                    "text?"
                ],
                [
                    "Quantitative",
                    "studies",
                    "of",
                    "the",
                    "productivity",
                    "of",
                    "word",
                    "formation",
                    "processes",
                    "are",
                    "important",
                    "for",
                    "the",
                    "design",
                    "of",
                    "word",
                    "formation",
                    "systems",
                    "if",
                    "the",
                    "resources",
                    "are",
                    "limited",
                    "and",
                    "one",
                    "has",
                    "to",
                    "concentrate",
                    "on",
                    "the",
                    "most",
                    "productive",
                    "word",
                    "formation",
                    "processes",
                    "(on",
                    "the",
                    "quantitative",
                    "aspects",
                    "of",
                    "productivity",
                    "see",
                    "for",
                    "example",
                    "#REF",
                    ",",
                    "for",
                    "a",
                    "discussion",
                    "of",
                    "some",
                    "corpus",
                    "related",
                    "problems",
                    "in",
                    "calculating",
                    "productivity",
                    "indices",
                    "see",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                3
            ]
        },
        "input": "sent0: There are two approaches to assessing the productivity of a word formation rule: the qualitative and the quantitative approach.\n sent1: For a full understanding, a combination of both perspectives is necessary.\n sent2: In the qualitative approach, all the restrictions and linguistic properties of the rule are described (see the next paragraph for some examples).\n sent3: The qualitative description does not exhaust our intuition of productivity, there is also a quantitative element involved: intuitively one could say that some word formation rules seem to produce new words more readily than others -an intuition which cannot be formalized.\n sent4: In quantitative studies this intuition is approximated by the question: how probable is it that we will see a new type (lexeme) produced by word formation process X after we have sampled a certain amount of text?\n sent5: Quantitative studies of the productivity of word formation processes are important for the design of word formation systems if the resources are limited and one has to concentrate on the most productive word formation processes (on the quantitative aspects of productivity see for example #REF , for a discussion of some corpus related problems in calculating productivity indices see #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent5\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Giménez",
                    "and",
                    "Màrquez",
                    "(2007b)",
                    "and",
                    "#TARGET_REF",
                    "introduced",
                    "and",
                    "refined",
                    "a",
                    "set",
                    "of",
                    "new",
                    "MT",
                    "evaluation",
                    "metrics",
                    "employing",
                    "rich",
                    "assortments",
                    "of",
                    "features",
                    "reflecting",
                    "various",
                    "kinds",
                    "of",
                    "similarity",
                    "at",
                    "lexical,",
                    "shallow",
                    "syntactic,",
                    "deep",
                    "syntactic,",
                    "shallow",
                    "semantic,",
                    "and",
                    "deep",
                    "semantic",
                    "levels."
                ]
            ],
            "context": [
                1
            ]
        },
        "input": "sent0: Giménez and Màrquez (2007b) and #TARGET_REF introduced and refined a set of new MT evaluation metrics employing rich assortments of features reflecting various kinds of similarity at lexical, shallow syntactic, deep syntactic, shallow semantic, and deep semantic levels.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "Yamamoto",
                    "and",
                    "Church's",
                    "work,",
                    "they",
                    "demonstrate",
                    "how",
                    "interesting",
                    "multiword",
                    "phrases",
                    "can",
                    "be",
                    "discovered,",
                    "by",
                    "interesting",
                    "they",
                    "mean",
                    "phrases",
                    "that",
                    "may",
                    "be",
                    "beneficial",
                    "for",
                    "information",
                    "retrieval",
                    "or",
                    "computational",
                    "lexicography",
                    "as",
                    "determined",
                    "by",
                    "Mutual",
                    "Information",
                    "(MI)",
                    "or",
                    "Residual",
                    "Inverse",
                    "Document",
                    "Frequency",
                    "(RIDF)",
                    "#TARGET_REF",
                    "scores."
                ]
            ],
            "context": [
                3
            ]
        },
        "input": "sent0: In Yamamoto and Church's work, they demonstrate how interesting multiword phrases can be discovered, by interesting they mean phrases that may be beneficial for information retrieval or computational lexicography as determined by Mutual Information (MI) or Residual Inverse Document Frequency (RIDF) #TARGET_REF scores.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "•",
                    "First,",
                    "each",
                    "target-side",
                    "sentence",
                    "from",
                    "the",
                    "parallel",
                    "corpus",
                    "is",
                    "supertagged",
                    "by",
                    "assigning",
                    "the",
                    "best",
                    "sequence",
                    "of",
                    "CCG",
                    "supertags",
                    "to",
                    "its",
                    "words."
                ],
                [
                    "•",
                    "Next,",
                    "phrase",
                    "pairs",
                    "are",
                    "extracted",
                    "from",
                    "the",
                    "parallel",
                    "corpus",
                    "according",
                    "to",
                    "the",
                    "PBSMT",
                    "phrase",
                    "extraction",
                    "method",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "•",
                    "Then,",
                    "each",
                    "phrase",
                    "pair",
                    "is",
                    "assigned",
                    "a",
                    "CCG",
                    "category",
                    "that",
                    "results",
                    "from",
                    "combining",
                    "the",
                    "supertags",
                    "of",
                    "the",
                    "words",
                    "of",
                    "the",
                    "target-side",
                    "phrase",
                    "using",
                    "CCG",
                    "combinatory",
                    "operators."
                ],
                [
                    "In",
                    "case",
                    "phrase",
                    "parsing",
                    "fails",
                    "to",
                    "find",
                    "a",
                    "single",
                    "CCG",
                    "category",
                    "for",
                    "the",
                    "phrase,",
                    "a",
                    "general",
                    "X",
                    "label",
                    "is",
                    "assigned",
                    "to",
                    "the",
                    "phrase."
                ],
                [
                    "•",
                    "Finally,",
                    "hierarchical",
                    "rules",
                    "are",
                    "extracted",
                    "from",
                    "sentencepairs",
                    "according",
                    "to",
                    "the",
                    "same",
                    "basic",
                    "HPB",
                    "SMT",
                    "rule",
                    "extraction",
                    "method",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                3,
                2,
                0,
                0,
                0
            ]
        },
        "input": "sent0: • First, each target-side sentence from the parallel corpus is supertagged by assigning the best sequence of CCG supertags to its words.\n sent1: • Next, phrase pairs are extracted from the parallel corpus according to the PBSMT phrase extraction method #TARGET_REF .\n sent2: • Then, each phrase pair is assigned a CCG category that results from combining the supertags of the words of the target-side phrase using CCG combinatory operators.\n sent3: In case phrase parsing fails to find a single CCG category for the phrase, a general X label is assigned to the phrase.\n sent4: • Finally, hierarchical rules are extracted from sentencepairs according to the same basic HPB SMT rule extraction method #REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Apart",
                    "from",
                    "designing",
                    "classification",
                    "tasks",
                    "for",
                    "disentanglement",
                    "evaluation,",
                    "another",
                    "method",
                    "is",
                    "based",
                    "on",
                    "estimating",
                    "the",
                    "mutual",
                    "information",
                    "(MI)",
                    "between",
                    "a",
                    "single",
                    "dimension",
                    "of",
                    "the",
                    "latent",
                    "variable",
                    "and",
                    "a",
                    "single",
                    "generative",
                    "factor."
                ],
                [
                    "#REF",
                    "propose",
                    "to",
                    "use",
                    "the",
                    "average",
                    "of",
                    "the",
                    "gap",
                    "(difference)",
                    "between",
                    "the",
                    "largest",
                    "normalised",
                    "MI",
                    "(by",
                    "the",
                    "information",
                    "entropy",
                    "of",
                    "the",
                    "generative",
                    "factor)",
                    "and",
                    "the",
                    "second",
                    "largest",
                    "normalised",
                    "MI",
                    "over",
                    "all",
                    "generative",
                    "factors",
                    "as",
                    "the",
                    "disentanglement",
                    "score,",
                    "whereas",
                    "the",
                    "modularity",
                    "metric",
                    "of",
                    "#TARGET_REF",
                    "measures",
                    "whether",
                    "a",
                    "single",
                    "latent",
                    "variable",
                    "has",
                    "the",
                    "highest",
                    "MI",
                    "with",
                    "only",
                    "one",
                    "generative",
                    "factor",
                    "and",
                    "none",
                    "with",
                    "others."
                ]
            ],
            "context": [
                0,
                2
            ]
        },
        "input": "sent0: Apart from designing classification tasks for disentanglement evaluation, another method is based on estimating the mutual information (MI) between a single dimension of the latent variable and a single generative factor.\n sent1: #REF propose to use the average of the gap (difference) between the largest normalised MI (by the information entropy of the generative factor) and the second largest normalised MI over all generative factors as the disentanglement score, whereas the modularity metric of #TARGET_REF measures whether a single latent variable has the highest MI with only one generative factor and none with others.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "have",
                    "started",
                    "working",
                    "on",
                    "these",
                    "issues,",
                    "but",
                    "none",
                    "of",
                    "it",
                    "was",
                    "finally",
                    "used",
                    "in",
                    "our",
                    "system,",
                    "mainly",
                    "due",
                    "to",
                    "the",
                    "fact",
                    "that",
                    "no",
                    "native",
                    "speaker",
                    "of",
                    "the",
                    "Arabic",
                    "language",
                    "was",
                    "available."
                ],
                [
                    "The",
                    "submitted",
                    "system",
                    "was",
                    "only",
                    "retuned",
                    "on",
                    "the",
                    "ASR",
                    "1-best",
                    "development",
                    "data."
                ],
                [
                    "Table",
                    "5",
                    "compares",
                    "the",
                    "BLEU",
                    "score",
                    "on",
                    "various",
                    "data",
                    "sets",
                    "of",
                    "the",
                    "text",
                    "and",
                    "ASR",
                    "condition."
                ],
                [
                    "We",
                    "observe",
                    "a",
                    "degradation",
                    "of",
                    "about",
                    "11%",
                    "relative",
                    "when",
                    "translating",
                    "the",
                    "ASR",
                    "output",
                    "of",
                    "Dev5",
                    "and",
                    "of",
                    "16%",
                    "for",
                    "Dev6",
                    "respectively."
                ],
                [
                    "Unfortunately,",
                    "translation",
                    "of",
                    "the",
                    "ASR",
                    "output",
                    "did",
                    "not",
                    "work",
                    "very",
                    "well",
                    "on",
                    "this",
                    "year's",
                    "test",
                    "data."
                ],
                [
                    "High",
                    "word",
                    "error",
                    "rates",
                    "of",
                    "the",
                    "speech",
                    "recognition",
                    "module",
                    "favor",
                    "the",
                    "translation",
                    "of",
                    "consensus",
                    "networks",
                    "#TARGET_REF",
                    "since",
                    "the",
                    "oracle",
                    "error",
                    "rate",
                    "of",
                    "such",
                    "data",
                    "structures",
                    "is",
                    "usually",
                    "two",
                    "to",
                    "three",
                    "times",
                    "smaller."
                ],
                [
                    "However,",
                    "this",
                    "data",
                    "structure",
                    "is",
                    "incompatible",
                    "with",
                    "SYSTRAN's",
                    "tokenization",
                    "that",
                    "operates",
                    "at",
                    "the",
                    "sentence",
                    "level."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                3,
                0
            ]
        },
        "input": "sent0: We have started working on these issues, but none of it was finally used in our system, mainly due to the fact that no native speaker of the Arabic language was available.\n sent1: The submitted system was only retuned on the ASR 1-best development data.\n sent2: Table 5 compares the BLEU score on various data sets of the text and ASR condition.\n sent3: We observe a degradation of about 11% relative when translating the ASR output of Dev5 and of 16% for Dev6 respectively.\n sent4: Unfortunately, translation of the ASR output did not work very well on this year's test data.\n sent5: High word error rates of the speech recognition module favor the translation of consensus networks #TARGET_REF since the oracle error rate of such data structures is usually two to three times smaller.\n sent6: However, this data structure is incompatible with SYSTRAN's tokenization that operates at the sentence level.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent5\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "examine",
                    "the",
                    "effect",
                    "that",
                    "alignment",
                    "link",
                    "visualization",
                    "has",
                    "on",
                    "each",
                    "bilingual",
                    "post-editor",
                    "in",
                    "Figure",
                    "5",
                    "on",
                    "the",
                    "next",
                    "page."
                ],
                [
                    "In",
                    "the",
                    "Russian-English",
                    "condition,",
                    "where",
                    "overall",
                    "MT",
                    "quality",
                    "is",
                    "poor,",
                    "we",
                    "observe",
                    "that",
                    "post-editing",
                    "quality",
                    "varies",
                    "widely",
                    "between",
                    "post-editors",
                    "(with",
                    "PE2",
                    "and",
                    "PE3",
                    "performing",
                    "best)."
                ],
                [
                    "For",
                    "all",
                    "six",
                    "bilingual",
                    "post-editors,",
                    "we",
                    "observe",
                    "higher",
                    "mean",
                    "adequacy",
                    "scores",
                    "when",
                    "alignment",
                    "links",
                    "were",
                    "presented",
                    "than",
                    "when",
                    "they",
                    "were",
                    "omitted",
                    "from",
                    "the",
                    "post-editing",
                    "tool."
                ],
                [
                    "We",
                    "also",
                    "note",
                    "that",
                    "when",
                    "alignment",
                    "links",
                    "were",
                    "absent,",
                    "one",
                    "bilingual",
                    "post-editor",
                    "(PE5)",
                    "performed",
                    "worse",
                    "than",
                    "the",
                    "monolingual",
                    "post-editor",
                    "(PE0)",
                    "from",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "On",
                    "the",
                    "other",
                    "hand,",
                    "in",
                    "the",
                    "Spanish-English",
                    "condition,",
                    "where",
                    "overall",
                    "MT",
                    "quality",
                    "is",
                    "good,",
                    "we",
                    "observe",
                    "relatively",
                    "little",
                    "variation",
                    "in",
                    "quality",
                    "between",
                    "the",
                    "ten",
                    "post-editors."
                ],
                [
                    "When",
                    "compared",
                    "to",
                    "the",
                    "unedited",
                    "machine",
                    "trans-PE0",
                    "PE1",
                    "PE2",
                    "PE3",
                    "PE4",
                    "PE5",
                    "PE6",
                    "lations,",
                    "post-editing",
                    "resulted",
                    "in",
                    "improved",
                    "mean",
                    "adequacy",
                    "for",
                    "all",
                    "post-editors,",
                    "both",
                    "bilingual",
                    "and",
                    "monolingual."
                ]
            ],
            "context": [
                0,
                0,
                0,
                2,
                0,
                0
            ]
        },
        "input": "sent0: We examine the effect that alignment link visualization has on each bilingual post-editor in Figure 5 on the next page.\n sent1: In the Russian-English condition, where overall MT quality is poor, we observe that post-editing quality varies widely between post-editors (with PE2 and PE3 performing best).\n sent2: For all six bilingual post-editors, we observe higher mean adequacy scores when alignment links were presented than when they were omitted from the post-editing tool.\n sent3: We also note that when alignment links were absent, one bilingual post-editor (PE5) performed worse than the monolingual post-editor (PE0) from #TARGET_REF .\n sent4: On the other hand, in the Spanish-English condition, where overall MT quality is good, we observe relatively little variation in quality between the ten post-editors.\n sent5: When compared to the unedited machine trans-PE0 PE1 PE2 PE3 PE4 PE5 PE6 lations, post-editing resulted in improved mean adequacy for all post-editors, both bilingual and monolingual.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent3\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "A",
                    "simple",
                    "yet",
                    "effective",
                    "method",
                    "for",
                    "improving",
                    "translation",
                    "quality",
                    "on",
                    "the",
                    "downstream",
                    "task",
                    "is",
                    "fine-tuning",
                    "with",
                    "domain",
                    "data,which",
                    "is",
                    "known",
                    "as",
                    "domain",
                    "adaption",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "We",
                    "train",
                    "for",
                    "another",
                    "2",
                    "epochs",
                    "on",
                    "the",
                    "BSTC",
                    "dataset",
                    "with",
                    "pretrained",
                    "model."
                ],
                [
                    "Furthermore,",
                    "we",
                    "obverse",
                    "that",
                    "finetuning",
                    "on",
                    "limited",
                    "spoken",
                    "corpus",
                    "lead",
                    "to",
                    "overfit",
                    "quickly,",
                    "as",
                    "evidenced",
                    "by",
                    "the",
                    "significant",
                    "improvement",
                    "on",
                    "the",
                    "BSTC",
                    "development",
                    "set",
                    "while",
                    "degrades",
                    "rapidly",
                    "on",
                    "the",
                    "CWMT",
                    "development",
                    "set."
                ]
            ],
            "context": [
                2,
                2,
                2
            ]
        },
        "input": "sent0: A simple yet effective method for improving translation quality on the downstream task is fine-tuning with domain data,which is known as domain adaption #TARGET_REF .\n sent1: We train for another 2 epochs on the BSTC dataset with pretrained model.\n sent2: Furthermore, we obverse that finetuning on limited spoken corpus lead to overfit quickly, as evidenced by the significant improvement on the BSTC development set while degrades rapidly on the CWMT development set.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\", \"sent1\", \"sent2\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Constructing",
                    "long",
                    "inference",
                    "chains",
                    "can",
                    "be",
                    "extremely",
                    "challenging",
                    "for",
                    "existing",
                    "models,",
                    "which",
                    "generally",
                    "exhibit",
                    "a",
                    "large",
                    "drop",
                    "in",
                    "performance",
                    "when",
                    "composing",
                    "explanations",
                    "and",
                    "inference",
                    "chains",
                    "requiring",
                    "more",
                    "than",
                    "2",
                    "inference",
                    "steps",
                    "#REF",
                    "."
                ],
                [
                    "To",
                    "this",
                    "end,",
                    "this",
                    "Shared",
                    "Task",
                    "on",
                    "Multi-hop",
                    "Inference",
                    "for",
                    "Explanation",
                    "Regeneration",
                    "#REF",
                    "has",
                    "focused",
                    "on",
                    "expanding",
                    "the",
                    "capacity",
                    "of",
                    "models",
                    "to",
                    "compose",
                    "long",
                    "inference",
                    "chains,",
                    "where",
                    "participants",
                    "are",
                    "asked",
                    "to",
                    "develop",
                    "systems",
                    "capable",
                    "of",
                    "reconstructing",
                    "detailed",
                    "explanations",
                    "for",
                    "science",
                    "exam",
                    "questions",
                    "drawn",
                    "from",
                    "the",
                    "WorldTree",
                    "explanation",
                    "corpus",
                    "#TARGET_REF",
                    ",",
                    "which",
                    "range",
                    "in",
                    "compositional",
                    "complexity",
                    "from",
                    "1",
                    "to",
                    "16",
                    "facts",
                    "(with",
                    "the",
                    "average",
                    "explanation",
                    "including",
                    "6",
                    "facts)."
                ]
            ],
            "context": [
                0,
                3
            ]
        },
        "input": "sent0: Constructing long inference chains can be extremely challenging for existing models, which generally exhibit a large drop in performance when composing explanations and inference chains requiring more than 2 inference steps #REF .\n sent1: To this end, this Shared Task on Multi-hop Inference for Explanation Regeneration #REF has focused on expanding the capacity of models to compose long inference chains, where participants are asked to develop systems capable of reconstructing detailed explanations for science exam questions drawn from the WorldTree explanation corpus #TARGET_REF , which range in compositional complexity from 1 to 16 facts (with the average explanation including 6 facts).\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "As",
                    "a",
                    "first",
                    "step,",
                    "we",
                    "consider",
                    "only",
                    "cases",
                    "where",
                    "at",
                    "any",
                    "node",
                    "during",
                    "the",
                    "tree",
                    "traversal",
                    "in",
                    "the",
                    "BURS,",
                    "there",
                    "is",
                    "only",
                    "potentially",
                    "one",
                    "gNCN",
                    "at",
                    "a",
                    "time:",
                    "that",
                    "is,",
                    "it",
                    "is",
                    "not",
                    "possible",
                    "to",
                    "embed",
                    "or",
                    "overlap",
                    "these",
                    "gNCNs."
                ],
                [
                    "In",
                    "order",
                    "to",
                    "explain",
                    "this,",
                    "consider",
                    "first",
                    "the",
                    "example",
                    "below."
                ],
                [
                    "The",
                    "input",
                    "AST",
                    "(ignoring",
                    "the",
                    "annotations",
                    "on",
                    "the",
                    "nodes)",
                    "is",
                    "in",
                    "Figure",
                    "7,",
                    "pattern",
                    "trees,",
                    "in",
                    "the",
                    "form",
                    "of",
                    "a",
                    "TAG",
                    "grammar",
                    "(with",
                    "associated",
                    "costs",
                    "still",
                    "indicated",
                    "by",
                    "),",
                    "are",
                    "also",
                    "in",
                    "Figure",
                    "7."
                ],
                [
                    "The",
                    "algorithm",
                    "we",
                    "use",
                    "for",
                    "bottom-up",
                    "pattern",
                    "matching,",
                    "adapted",
                    "from",
                    "that",
                    "of",
                    "#TARGET_REF",
                    ",",
                    "is",
                    "in",
                    "Figure",
                    "8."
                ]
            ],
            "context": [
                0,
                0,
                0,
                2
            ]
        },
        "input": "sent0: As a first step, we consider only cases where at any node during the tree traversal in the BURS, there is only potentially one gNCN at a time: that is, it is not possible to embed or overlap these gNCNs.\n sent1: In order to explain this, consider first the example below.\n sent2: The input AST (ignoring the annotations on the nodes) is in Figure 7, pattern trees, in the form of a TAG grammar (with associated costs still indicated by ), are also in Figure 7.\n sent3: The algorithm we use for bottom-up pattern matching, adapted from that of #TARGET_REF , is in Figure 8.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent3\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "1",
                    "https://commoncrawl.org/",
                    "Local",
                    "language",
                    "communities",
                    "that",
                    "are",
                    "working",
                    "to",
                    "develop",
                    "and",
                    "preserve",
                    "their",
                    "languages",
                    "are",
                    "producing",
                    "diverse",
                    "sets",
                    "of",
                    "data",
                    "beyond",
                    "pure",
                    "text."
                ],
                [
                    "The",
                    "Bloom",
                    "Library",
                    "project,",
                    "2",
                    "for",
                    "example,",
                    "is",
                    "being",
                    "used",
                    "by",
                    "local",
                    "language",
                    "communities",
                    "to",
                    "create",
                    "and",
                    "translate",
                    "\"shell\"",
                    "or",
                    "\"template\"",
                    "books",
                    "into",
                    "many",
                    "languages",
                    "(426",
                    "languages",
                    "at",
                    "the",
                    "time",
                    "this",
                    "paper",
                    "is",
                    "being",
                    "written)."
                ],
                [
                    "However,",
                    "Bloom",
                    "allows",
                    "users",
                    "to",
                    "do",
                    "more",
                    "than",
                    "just",
                    "translate",
                    "text."
                ],
                [
                    "Users",
                    "are",
                    "also",
                    "recording",
                    "audio",
                    "tracks",
                    "and",
                    "sign",
                    "language",
                    "videos,",
                    "which",
                    "has",
                    "resulted",
                    "in",
                    "1600+",
                    "oral",
                    "translations."
                ],
                [
                    "Other",
                    "examples",
                    "showing",
                    "the",
                    "multi-modal",
                    "nature",
                    "of",
                    "data",
                    "in",
                    "local",
                    "languages",
                    "include:",
                    "(i)",
                    "the",
                    "creation",
                    "of",
                    "ChoCo:",
                    "a",
                    "multimodal",
                    "corpus",
                    "of",
                    "the",
                    "Choctaw",
                    "language",
                    "#TARGET_REF",
                    ",",
                    "(ii)",
                    "SIL",
                    "International's",
                    "50+",
                    "year",
                    "effort",
                    "to",
                    "document",
                    "endangered",
                    "Austronesian",
                    "languages",
                    "via",
                    "text,",
                    "audio,",
                    "and",
                    "video",
                    "#REF",
                    ",",
                    "(iii)",
                    "the",
                    "grassroots",
                    "Masakhane",
                    "effort",
                    "catalyzing",
                    "the",
                    "creation",
                    "and",
                    "use",
                    "of",
                    "diverse",
                    "sets",
                    "of",
                    "African",
                    "language",
                    "data",
                    "#REF",
                    ",",
                    "and",
                    "(iv)",
                    "work",
                    "with",
                    "the",
                    "Me'phaa",
                    "language",
                    "of",
                    "western",
                    "Mexico",
                    "that",
                    "is",
                    "producing",
                    "digital",
                    "recordings",
                    "(video",
                    "and",
                    "audio)",
                    "along",
                    "with",
                    "vocabulary,",
                    "grammar",
                    "and",
                    "texts",
                    "#REF",
                    "."
                ],
                [
                    "These",
                    "diverse",
                    "data",
                    "sources",
                    "are",
                    "effectively",
                    "unusable",
                    "by",
                    "traditional",
                    "text-based",
                    "NLP",
                    "techniques."
                ],
                [
                    "In",
                    "the",
                    "light",
                    "of",
                    "data",
                    "scarcity",
                    "on",
                    "these",
                    "languages,",
                    "they",
                    "offer",
                    "significant",
                    "untapped",
                    "potential",
                    "to",
                    "unlock",
                    "improved",
                    "NLP",
                    "technology,",
                    "if",
                    "text",
                    "data",
                    "can",
                    "be",
                    "leveraged",
                    "along",
                    "with",
                    "audio,",
                    "image",
                    "and",
                    "video",
                    "data."
                ],
                [
                    "Furthermore,",
                    "flexible",
                    "multi-modal",
                    "technology",
                    "such",
                    "as",
                    "this",
                    "will",
                    "make",
                    "it",
                    "easier",
                    "to",
                    "include",
                    "diverse",
                    "people",
                    "and",
                    "communities",
                    "such",
                    "as",
                    "those",
                    "described",
                    "above",
                    "within",
                    "the",
                    "NLP",
                    "technology",
                    "development",
                    "process",
                    "-audio-based",
                    "technology",
                    "reducing",
                    "the",
                    "need",
                    "for",
                    "literacy,",
                    "for",
                    "example."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                3,
                0,
                0,
                0
            ]
        },
        "input": "sent0: 1 https://commoncrawl.org/ Local language communities that are working to develop and preserve their languages are producing diverse sets of data beyond pure text.\n sent1: The Bloom Library project, 2 for example, is being used by local language communities to create and translate \"shell\" or \"template\" books into many languages (426 languages at the time this paper is being written).\n sent2: However, Bloom allows users to do more than just translate text.\n sent3: Users are also recording audio tracks and sign language videos, which has resulted in 1600+ oral translations.\n sent4: Other examples showing the multi-modal nature of data in local languages include: (i) the creation of ChoCo: a multimodal corpus of the Choctaw language #TARGET_REF , (ii) SIL International's 50+ year effort to document endangered Austronesian languages via text, audio, and video #REF , (iii) the grassroots Masakhane effort catalyzing the creation and use of diverse sets of African language data #REF , and (iv) work with the Me'phaa language of western Mexico that is producing digital recordings (video and audio) along with vocabulary, grammar and texts #REF .\n sent5: These diverse data sources are effectively unusable by traditional text-based NLP techniques.\n sent6: In the light of data scarcity on these languages, they offer significant untapped potential to unlock improved NLP technology, if text data can be leveraged along with audio, image and video data.\n sent7: Furthermore, flexible multi-modal technology such as this will make it easier to include diverse people and communities such as those described above within the NLP technology development process -audio-based technology reducing the need for literacy, for example.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Similar",
                    "to",
                    "#TARGET_REF",
                    ",",
                    "we",
                    "preprocess",
                    "the",
                    "data",
                    "as",
                    "follows:"
                ]
            ],
            "context": [
                2
            ]
        },
        "input": "sent0: Similar to #TARGET_REF , we preprocess the data as follows:\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "collect",
                    "the",
                    "data",
                    "for",
                    "two",
                    "months",
                    "from",
                    "August",
                    "to",
                    "October",
                    "2020."
                ],
                [
                    "There",
                    "are",
                    "two",
                    "main",
                    "sources",
                    "of",
                    "the",
                    "data:",
                    "SNSs",
                    "and",
                    "Vietnamese",
                    "newspapers."
                ],
                [
                    "As",
                    "for",
                    "the",
                    "former",
                    "source,",
                    "public",
                    "social",
                    "media",
                    "posts",
                    "are",
                    "retrieved",
                    "from",
                    "news",
                    "groups",
                    "and",
                    "key",
                    "opinion",
                    "leaders",
                    "(KOLs)."
                ],
                [
                    "Many",
                    "fake",
                    "news,",
                    "however,",
                    "has",
                    "been",
                    "flagged",
                    "and",
                    "removed",
                    "from",
                    "the",
                    "social",
                    "networking",
                    "sites",
                    "since",
                    "the",
                    "enforcement",
                    "of",
                    "Vietnamese",
                    "cybersecurity",
                    "law",
                    "in",
                    "2019",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Therefore,",
                    "to",
                    "include",
                    "the",
                    "deleted",
                    "fake",
                    "news,",
                    "we",
                    "gather",
                    "newspaper",
                    "articles",
                    "reporting",
                    "these",
                    "posts",
                    "and",
                    "recreate",
                    "their",
                    "content."
                ]
            ],
            "context": [
                2,
                2,
                2,
                1,
                2
            ]
        },
        "input": "sent0: We collect the data for two months from August to October 2020.\n sent1: There are two main sources of the data: SNSs and Vietnamese newspapers.\n sent2: As for the former source, public social media posts are retrieved from news groups and key opinion leaders (KOLs).\n sent3: Many fake news, however, has been flagged and removed from the social networking sites since the enforcement of Vietnamese cybersecurity law in 2019 #TARGET_REF .\n sent4: Therefore, to include the deleted fake news, we gather newspaper articles reporting these posts and recreate their content.\n",
        "output": "{\"INFORMATION\": [\"sent3\"], \"PERCEPTION\": [\"sent0\", \"sent1\", \"sent2\", \"sent4\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "As",
                    "shown",
                    "in",
                    "Table",
                    "1,",
                    "the",
                    "size",
                    "of",
                    "the",
                    "'in-domain'",
                    "TED",
                    "training",
                    "data",
                    "is",
                    "much",
                    "smaller",
                    "than",
                    "the",
                    "'out-of-domain'",
                    "Multi-UN",
                    "training",
                    "data."
                ],
                [
                    "Since",
                    "adding",
                    "a",
                    "significant",
                    "amount",
                    "of",
                    "out-ofdomain",
                    "data",
                    "to",
                    "an",
                    "in-domain",
                    "corpus",
                    "reduces",
                    "the",
                    "quality",
                    "of",
                    "translation",
                    "for",
                    "in-domain",
                    "sentences",
                    "#TARGET_REF",
                    ",",
                    "we",
                    "decided",
                    "to",
                    "use",
                    "only",
                    "a",
                    "part",
                    "of",
                    "the",
                    "out-of-domain",
                    "data",
                    "to",
                    "enhance",
                    "the",
                    "translation",
                    "quality."
                ],
                [
                    "In",
                    "order",
                    "to",
                    "achieve",
                    "this,",
                    "we",
                    "constructed",
                    "a",
                    "language",
                    "model",
                    "on",
                    "the",
                    "TED",
                    "monolingual",
                    "data",
                    "and",
                    "computed",
                    "sentence-level",
                    "perplexity",
                    "score",
                    "for",
                    "all",
                    "the",
                    "sentences",
                    "in",
                    "Multi-UN,",
                    "with",
                    "respect",
                    "to",
                    "the",
                    "TED",
                    "language",
                    "model."
                ],
                [
                    "After",
                    "sorting",
                    "the",
                    "sentences",
                    "in",
                    "the",
                    "ascending",
                    "order",
                    "of",
                    "the",
                    "perplexity",
                    "values,",
                    "only",
                    "sentences",
                    "below",
                    "a",
                    "specific",
                    "threshold",
                    "were",
                    "selected."
                ],
                [
                    "This",
                    "method",
                    "provided",
                    "us",
                    "with",
                    "the",
                    "most",
                    "'TED-like'",
                    "sentences",
                    "from",
                    "the",
                    "Multi-UN",
                    "corpora."
                ]
            ],
            "context": [
                3,
                1,
                0,
                0,
                0
            ]
        },
        "input": "sent0: As shown in Table 1, the size of the 'in-domain' TED training data is much smaller than the 'out-of-domain' Multi-UN training data.\n sent1: Since adding a significant amount of out-ofdomain data to an in-domain corpus reduces the quality of translation for in-domain sentences #TARGET_REF , we decided to use only a part of the out-of-domain data to enhance the translation quality.\n sent2: In order to achieve this, we constructed a language model on the TED monolingual data and computed sentence-level perplexity score for all the sentences in Multi-UN, with respect to the TED language model.\n sent3: After sorting the sentences in the ascending order of the perplexity values, only sentences below a specific threshold were selected.\n sent4: This method provided us with the most 'TED-like' sentences from the Multi-UN corpora.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Therefore,",
                    "neural-based",
                    "diversity",
                    "metrics",
                    "are",
                    "highly",
                    "demanded."
                ],
                [
                    "Intuitively,",
                    "the",
                    "metrics",
                    "should",
                    "include",
                    "computational",
                    "comparisons",
                    "of",
                    "multiple",
                    "references",
                    "and",
                    "hypotheses",
                    "by",
                    "projecting",
                    "them",
                    "into",
                    "the",
                    "same",
                    "semantic",
                    "space,",
                    "unlike",
                    "metrics",
                    "for",
                    "evaluating",
                    "the",
                    "generation",
                    "quality,",
                    "e.g.,",
                    "BERTScore",
                    "#REF",
                    "and",
                    "BLEURT",
                    "#TARGET_REF",
                    ",",
                    "which",
                    "only",
                    "measures",
                    "the",
                    "correlation",
                    "between",
                    "a",
                    "pair",
                    "of",
                    "reference",
                    "and",
                    "hypothesis."
                ]
            ],
            "context": [
                0,
                1
            ]
        },
        "input": "sent0: Therefore, neural-based diversity metrics are highly demanded.\n sent1: Intuitively, the metrics should include computational comparisons of multiple references and hypotheses by projecting them into the same semantic space, unlike metrics for evaluating the generation quality, e.g., BERTScore #REF and BLEURT #TARGET_REF , which only measures the correlation between a pair of reference and hypothesis.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Our",
                    "proposed",
                    "SimpDefiner",
                    "also",
                    "takes",
                    "the",
                    "given",
                    "word",
                    "and",
                    "context",
                    "as",
                    "input."
                ],
                [
                    "Differently,",
                    "our",
                    "main",
                    "focus",
                    "is",
                    "to",
                    "generate",
                    "definitions",
                    "with",
                    "appropriate",
                    "complexity",
                    "to",
                    "better",
                    "help",
                    "language",
                    "learners."
                ],
                [
                    "Besides,",
                    "our",
                    "model",
                    "is",
                    "based",
                    "on",
                    "MASS",
                    "#TARGET_REF",
                    ",",
                    "which",
                    "is",
                    "a",
                    "pre-trained",
                    "encoder-decoder",
                    "model",
                    "and",
                    "is",
                    "suitable",
                    "for",
                    "generation",
                    "tasks."
                ]
            ],
            "context": [
                0,
                2,
                2
            ]
        },
        "input": "sent0: Our proposed SimpDefiner also takes the given word and context as input.\n sent1: Differently, our main focus is to generate definitions with appropriate complexity to better help language learners.\n sent2: Besides, our model is based on MASS #TARGET_REF , which is a pre-trained encoder-decoder model and is suitable for generation tasks.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\", \"sent2\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "also",
                    "compare",
                    "with",
                    "the",
                    "semi-rule-based",
                    "system",
                    "used",
                    "for",
                    "pre-annotating",
                    "the",
                    "Parallel",
                    "Meaning",
                    "Bank",
                    "#REF",
                    "."
                ],
                [
                    "#TARGET_REF",
                    "call",
                    "this",
                    "system",
                    "\"Pro",
                    "Boxer\"."
                ],
                [
                    "In",
                    "a",
                    "sense,",
                    "Pro",
                    "Boxer",
                    "is",
                    "closest",
                    "in",
                    "approach",
                    "to",
                    "ours",
                    "because",
                    "it",
                    "makes",
                    "use",
                    "of",
                    "neural",
                    "taggers",
                    "for",
                    "making",
                    "token-level",
                    "tagging",
                    "predictions."
                ],
                [
                    "It",
                    "differs",
                    "from",
                    "ours",
                    "and",
                    "all",
                    "other",
                    "systems",
                    "however",
                    "in",
                    "that",
                    "it",
                    "is",
                    "not",
                    "fully",
                    "trainable",
                    "from",
                    "examples,",
                    "the",
                    "translation",
                    "from",
                    "tags",
                    "to",
                    "DRSs",
                    "is",
                    "done",
                    "via",
                    "hand-crafted",
                    "rules."
                ],
                [
                    "Moreover,",
                    "it",
                    "relies",
                    "on",
                    "a",
                    "CCG",
                    "parser",
                    "that",
                    "creates",
                    "explicit",
                    "syntactic",
                    "representation",
                    "which",
                    "is",
                    "perhaps",
                    "more",
                    "complexity",
                    "than",
                    "needed."
                ],
                [
                    "As",
                    "van",
                    "Noord",
                    "et",
                    "al."
                ],
                [
                    "also",
                    "point",
                    "out,",
                    "the",
                    "comparison",
                    "with",
                    "Pro",
                    "Boxer",
                    "is",
                    "not",
                    "quite",
                    "fair",
                    "because",
                    "it",
                    "is",
                    "the",
                    "system",
                    "that",
                    "produced",
                    "the",
                    "PMB",
                    "pre-annotations",
                    "and",
                    "thus",
                    "profits",
                    "from",
                    "anchoring",
                    "bias."
                ]
            ],
            "context": [
                3,
                1,
                2,
                2,
                1,
                0,
                0
            ]
        },
        "input": "sent0: We also compare with the semi-rule-based system used for pre-annotating the Parallel Meaning Bank #REF .\n sent1: #TARGET_REF call this system \"Pro Boxer\".\n sent2: In a sense, Pro Boxer is closest in approach to ours because it makes use of neural taggers for making token-level tagging predictions.\n sent3: It differs from ours and all other systems however in that it is not fully trainable from examples, the translation from tags to DRSs is done via hand-crafted rules.\n sent4: Moreover, it relies on a CCG parser that creates explicit syntactic representation which is perhaps more complexity than needed.\n sent5: As van Noord et al.\n sent6: also point out, the comparison with Pro Boxer is not quite fair because it is the system that produced the PMB pre-annotations and thus profits from anchoring bias.\n",
        "output": "{\"INFORMATION\": [\"sent1\", \"sent4\"], \"PERCEPTION\": [\"sent2\", \"sent3\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "are",
                    "currently",
                    "incorporating",
                    "the",
                    "parameterized",
                    "parser",
                    "into",
                    "an",
                    "interlingual",
                    "MT",
                    "system",
                    "called",
                    "PRINCITRAN."
                ],
                [
                    "The",
                    "current",
                    "framework",
                    "is",
                    "well-suited",
                    "to",
                    "an",
                    "interlingual",
                    "design",
                    "since",
                    "the",
                    "linking",
                    "rules",
                    "between",
                    "the",
                    "syntactic",
                    "representations",
                    "given",
                    "above",
                    "and",
                    "the",
                    "underlying",
                    "lexical-semantic",
                    "representation",
                    "are",
                    "well-defined",
                    "(see",
                    "#REF",
                    ")."
                ],
                [
                    "We",
                    "adopt",
                    "the",
                    "Lexical",
                    "Conceptual",
                    "Structure",
                    "(LCS)",
                    "of",
                    "Dorr's",
                    "work",
                    "and",
                    "use",
                    "a",
                    "parameter-setting",
                    "approach",
                    "to",
                    "account",
                    "for",
                    "the",
                    "divergences",
                    "presented",
                    "in",
                    "the",
                    "last",
                    "section."
                ],
                [
                    "#TARGET_REF",
                    "describes",
                    "a",
                    "parametric",
                    "approach",
                    "to",
                    "mapping",
                    "between",
                    "the",
                    "interlingua",
                    "and",
                    "the",
                    "syntactic",
                    "structure.)"
                ]
            ],
            "context": [
                0,
                0,
                2,
                1
            ]
        },
        "input": "sent0: We are currently incorporating the parameterized parser into an interlingual MT system called PRINCITRAN.\n sent1: The current framework is well-suited to an interlingual design since the linking rules between the syntactic representations given above and the underlying lexical-semantic representation are well-defined (see #REF ).\n sent2: We adopt the Lexical Conceptual Structure (LCS) of Dorr's work and use a parameter-setting approach to account for the divergences presented in the last section.\n sent3: #TARGET_REF describes a parametric approach to mapping between the interlingua and the syntactic structure.)\n",
        "output": "{\"INFORMATION\": [\"sent3\"], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Accurate",
                    "enough",
                    "parse",
                    "selection",
                    "for",
                    "practical",
                    "applications",
                    "will",
                    "require",
                    "a",
                    "more",
                    "lexic."
                ],
                [
                    "alised",
                    "system."
                ],
                [
                    "#REF",
                    "parser",
                    "is",
                    "an",
                    "extension",
                    "of",
                    "the",
                    "history-based",
                    "parsing",
                    "approach",
                    "devel",
                    "oped",
                    "at",
                    "IBM",
                    "(e.g."
                ],
                [
                    "#TARGET_REF",
                    "in",
                    "which",
                    "rules",
                    "are",
                    "conditioned",
                    "on",
                    "lexical",
                    "and",
                    "other",
                    "(essentially",
                    "arbitrary)",
                    "information",
                    "available",
                    "in",
                    "the",
                    "parse",
                    "history."
                ],
                [
                    "In",
                    "future",
                    "work,",
                    "we",
                    "intend",
                    "to",
                    "explore",
                    "a",
                    "more",
                    "restricted",
                    "and",
                    "semantically-driven",
                    "version",
                    "of",
                    "this",
                    "approach",
                    "in",
                    "which,",
                    "firstly,",
                    "probabilities",
                    "are",
                    "associated",
                    "with",
                    "different",
                    "subcategorisation",
                    "possibilities,",
                    "and",
                    "secondly,",
                    "alternative",
                    "predicate",
                    "argument",
                    "structures",
                    "derived",
                    "from",
                    "the",
                    "grammar",
                    "are",
                    "ranked",
                    "probabilistically."
                ],
                [
                    "However,",
                    "the",
                    "mas",
                    "sively",
                    "increased",
                    "coverage",
                    "obtained",
                    "here",
                    "by",
                    "relaxing",
                    "subcategorisation",
                    "constraints",
                    "underlines",
                    "the",
                    "need",
                    "to",
                    "acquire",
                    "accurate",
                    "and",
                    "complet�",
                    "subcategorisation",
                    "frames",
                    "in",
                    "a",
                    "corpus-driven",
                    "fas",
                    "hion,",
                    "before",
                    "such",
                    "constraints",
                    "can",
                    "be",
                    "exploited",
                    "robustly",
                    "and",
                    "effectively",
                    "with",
                    "free",
                    "text."
                ]
            ],
            "context": [
                0,
                0,
                2,
                1,
                0,
                0
            ]
        },
        "input": "sent0: Accurate enough parse selection for practical applications will require a more lexic.\n sent1: alised system.\n sent2: #REF parser is an extension of the history-based parsing approach devel oped at IBM (e.g.\n sent3: #TARGET_REF in which rules are conditioned on lexical and other (essentially arbitrary) information available in the parse history.\n sent4: In future work, we intend to explore a more restricted and semantically-driven version of this approach in which, firstly, probabilities are associated with different subcategorisation possibilities, and secondly, alternative predicate argument structures derived from the grammar are ranked probabilistically.\n sent5: However, the mas sively increased coverage obtained here by relaxing subcategorisation constraints underlines the need to acquire accurate and complet� subcategorisation frames in a corpus-driven fas hion, before such constraints can be exploited robustly and effectively with free text.\n",
        "output": "{\"INFORMATION\": [\"sent3\"], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "First,",
                    "we",
                    "apply",
                    "our",
                    "framework",
                    "to",
                    "the",
                    "TQ-AutoTest",
                    "dataset",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Originally",
                    "used",
                    "for",
                    "targeted",
                    "evaluation",
                    "of",
                    "MT",
                    "systems,",
                    "it",
                    "includes",
                    "German",
                    "source",
                    "sentences",
                    "that",
                    "each",
                    "exhibit",
                    "one",
                    "of",
                    "14",
                    "different",
                    "linguistic",
                    "phenomena,",
                    "such",
                    "as",
                    "ambiguity,",
                    "composition,",
                    "and",
                    "subordination."
                ]
            ],
            "context": [
                2,
                3
            ]
        },
        "input": "sent0: First, we apply our framework to the TQ-AutoTest dataset #TARGET_REF .\n sent1: Originally used for targeted evaluation of MT systems, it includes German source sentences that each exhibit one of 14 different linguistic phenomena, such as ambiguity, composition, and subordination.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "A",
                    "semantic",
                    "analysis",
                    "of",
                    "the",
                    "selected",
                    "keywords",
                    "was",
                    "done",
                    "using",
                    "WordNet",
                    "(3.1)."
                ],
                [
                    "We",
                    "used",
                    "semantic",
                    "relations",
                    "such",
                    "as",
                    "hypernymy,",
                    "troponymy",
                    "and",
                    "entailment",
                    "#TARGET_REF",
                    "to",
                    "find",
                    "the",
                    "implications",
                    "that",
                    "the",
                    "keywords",
                    "may",
                    "have,",
                    "as",
                    "far",
                    "as",
                    "their",
                    "communicative",
                    "goals",
                    "are",
                    "concerned."
                ]
            ],
            "context": [
                0,
                2
            ]
        },
        "input": "sent0: A semantic analysis of the selected keywords was done using WordNet (3.1).\n sent1: We used semantic relations such as hypernymy, troponymy and entailment #TARGET_REF to find the implications that the keywords may have, as far as their communicative goals are concerned.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Proximity-based",
                    "features",
                    "Traditional",
                    "retrieval",
                    "models",
                    "assume",
                    "terms",
                    "are",
                    "independent",
                    "and",
                    "ignore",
                    "their",
                    "relationships,",
                    "but",
                    "the",
                    "proximity",
                    "among",
                    "query",
                    "terms",
                    "often",
                    "serves",
                    "as",
                    "an",
                    "important",
                    "relevance",
                    "signal."
                ],
                [
                    "Thus,",
                    "we",
                    "include",
                    "features",
                    "that",
                    "directly",
                    "capture",
                    "the",
                    "proximity",
                    "of",
                    "query",
                    "terms,",
                    "such",
                    "as",
                    "the",
                    "counts",
                    "of",
                    "ordered",
                    "and",
                    "unordered",
                    "co-occurrence",
                    "of",
                    "bigrams",
                    "within",
                    "different",
                    "window",
                    "sizes."
                ],
                [
                    "We",
                    "compute",
                    "the",
                    "scores",
                    "of",
                    "proximity-based",
                    "retrieval",
                    "functions,",
                    "such",
                    "as",
                    "SDM",
                    "#REF",
                    "and",
                    "BM25-TP",
                    "#TARGET_REF",
                    ",",
                    "as",
                    "our",
                    "features."
                ]
            ],
            "context": [
                0,
                0,
                2
            ]
        },
        "input": "sent0: Proximity-based features Traditional retrieval models assume terms are independent and ignore their relationships, but the proximity among query terms often serves as an important relevance signal.\n sent1: Thus, we include features that directly capture the proximity of query terms, such as the counts of ordered and unordered co-occurrence of bigrams within different window sizes.\n sent2: We compute the scores of proximity-based retrieval functions, such as SDM #REF and BM25-TP #TARGET_REF , as our features.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Mitigation",
                    "Multiple",
                    "approaches",
                    "have",
                    "been",
                    "proposed",
                    "to",
                    "mitigate",
                    "shortcut",
                    "learning",
                    "and",
                    "data",
                    "biases",
                    "#REF",
                    ",",
                    "through",
                    "data",
                    "augmentation",
                    "#REF",
                    ",",
                    "domain",
                    "adaptation",
                    "#REF",
                    ",",
                    "and",
                    "multi-task",
                    "learning",
                    "#REF",
                    "."
                ],
                [
                    "#TARGET_REF",
                    "proposes",
                    "to",
                    "mitigate",
                    "shortcuts",
                    "by",
                    "suppressing",
                    "model's",
                    "prediction",
                    "on",
                    "examples",
                    "with",
                    "a",
                    "large",
                    "shortcut",
                    "degree."
                ],
                [
                    "Recent",
                    "study",
                    "has",
                    "also",
                    "shown",
                    "removing",
                    "spurious",
                    "correlations",
                    "can",
                    "sometimes",
                    "hurt",
                    "model's",
                    "accuracy",
                    "#REF",
                    "."
                ],
                [
                    "Orthogonal",
                    "to",
                    "existing",
                    "works,",
                    "we",
                    "propose",
                    "to",
                    "first",
                    "identify",
                    "unrobust",
                    "correlations",
                    "in",
                    "an",
                    "NLP",
                    "model",
                    "and",
                    "then",
                    "propose",
                    "a",
                    "targeted",
                    "mitigation",
                    "to",
                    "encourage",
                    "the",
                    "model",
                    "to",
                    "rely",
                    "less",
                    "on",
                    "those",
                    "unrobust",
                    "correlations."
                ]
            ],
            "context": [
                3,
                1,
                0,
                0
            ]
        },
        "input": "sent0: Mitigation Multiple approaches have been proposed to mitigate shortcut learning and data biases #REF , through data augmentation #REF , domain adaptation #REF , and multi-task learning #REF .\n sent1: #TARGET_REF proposes to mitigate shortcuts by suppressing model's prediction on examples with a large shortcut degree.\n sent2: Recent study has also shown removing spurious correlations can sometimes hurt model's accuracy #REF .\n sent3: Orthogonal to existing works, we propose to first identify unrobust correlations in an NLP model and then propose a targeted mitigation to encourage the model to rely less on those unrobust correlations.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Other",
                    "Monolingual",
                    "Since",
                    "there",
                    "were",
                    "less",
                    "training",
                    "data",
                    "available",
                    "for",
                    "non-English",
                    "monolingual",
                    "datasets,",
                    "we",
                    "followed",
                    "a",
                    "few-shot",
                    "learning",
                    "approach",
                    "mentioned",
                    "in",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "When",
                    "we",
                    "are",
                    "starting",
                    "the",
                    "training",
                    "for",
                    "non-English",
                    "monolingual",
                    "language",
                    "pairs,",
                    "rather",
                    "than",
                    "training",
                    "a",
                    "model",
                    "from",
                    "scratch,",
                    "we",
                    "initialised",
                    "the",
                    "weights",
                    "saved",
                    "from",
                    "the",
                    "English-English",
                    "experiment."
                ],
                [
                    "Then",
                    "we",
                    "performed",
                    "training",
                    "on",
                    "the",
                    "dev",
                    "data",
                    "for",
                    "each",
                    "language",
                    "pair",
                    "separately."
                ],
                [
                    "Similar",
                    "to",
                    "English-English",
                    "experiments,",
                    "during",
                    "the",
                    "training",
                    "process,",
                    "the",
                    "parameters",
                    "of",
                    "the",
                    "transformer",
                    "model,",
                    "as",
                    "well",
                    "as",
                    "the",
                    "parameters",
                    "of",
                    "the",
                    "subsequent",
                    "layers,",
                    "were",
                    "updated."
                ]
            ],
            "context": [
                3,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Other Monolingual Since there were less training data available for non-English monolingual datasets, we followed a few-shot learning approach mentioned in #TARGET_REF .\n sent1: When we are starting the training for non-English monolingual language pairs, rather than training a model from scratch, we initialised the weights saved from the English-English experiment.\n sent2: Then we performed training on the dev data for each language pair separately.\n sent3: Similar to English-English experiments, during the training process, the parameters of the transformer model, as well as the parameters of the subsequent layers, were updated.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Compositionality",
                    "and",
                    "Its",
                    "Benefits",
                    "Is",
                    "our",
                    "semantic",
                    "parser",
                    "compositional?"
                ],
                [
                    "#TARGET_REF",
                    "provide",
                    "a",
                    "definition",
                    "of",
                    "compositionality",
                    "in",
                    "meaning",
                    "systems,",
                    "which",
                    "we",
                    "summarize",
                    "as",
                    "follows:",
                    "(1)",
                    "there",
                    "is",
                    "a",
                    "finite",
                    "set",
                    "of",
                    "atomic",
                    "word-meaning",
                    "pairings,",
                    "(2)",
                    "there",
                    "is",
                    "a",
                    "finite",
                    "number",
                    "of",
                    "rules",
                    "combining",
                    "constituent-meaning",
                    "pairings",
                    "into",
                    "larger",
                    "constituent-meaning",
                    "pairings,",
                    "and",
                    "any",
                    "non-atomic",
                    "constituent-meaning",
                    "pairing",
                    "is",
                    "a",
                    "function",
                    "of",
                    "the",
                    "constituent-meaning",
                    "pairings",
                    "from",
                    "which",
                    "it",
                    "is",
                    "created",
                    "and",
                    "of",
                    "the",
                    "rule",
                    "that",
                    "creates",
                    "it,",
                    "(3)",
                    "meaning",
                    "representations",
                    "are",
                    "not",
                    "changed",
                    "destructively."
                ],
                [
                    "They",
                    "argue",
                    "that",
                    "compositional",
                    "aspects",
                    "of",
                    "meaning",
                    "such",
                    "as",
                    "predicate-argument",
                    "structure",
                    "should",
                    "be",
                    "processed",
                    "by",
                    "compositional",
                    "systems,",
                    "whereas",
                    "noncompositional",
                    "aspects",
                    "such",
                    "as",
                    "anaphora",
                    "or",
                    "word",
                    "senses",
                    "should",
                    "be",
                    "handled",
                    "by",
                    "different",
                    "mechanisms."
                ]
            ],
            "context": [
                0,
                2,
                3
            ]
        },
        "input": "sent0: Compositionality and Its Benefits Is our semantic parser compositional?\n sent1: #TARGET_REF provide a definition of compositionality in meaning systems, which we summarize as follows: (1) there is a finite set of atomic word-meaning pairings, (2) there is a finite number of rules combining constituent-meaning pairings into larger constituent-meaning pairings, and any non-atomic constituent-meaning pairing is a function of the constituent-meaning pairings from which it is created and of the rule that creates it, (3) meaning representations are not changed destructively.\n sent2: They argue that compositional aspects of meaning such as predicate-argument structure should be processed by compositional systems, whereas noncompositional aspects such as anaphora or word senses should be handled by different mechanisms.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "This",
                    "paper",
                    "reports",
                    "on",
                    "a",
                    "project",
                    "whose",
                    "aims",
                    "are",
                    "to",
                    "investigate",
                    "the",
                    "usability",
                    "of",
                    "raw",
                    "machine",
                    "translated",
                    "technical",
                    "support",
                    "documentation",
                    "for",
                    "a",
                    "commercial",
                    "online",
                    "service."
                ],
                [
                    "It",
                    "builds",
                    "on",
                    "previous",
                    "work",
                    "which",
                    "investigates",
                    "the",
                    "use",
                    "of",
                    "eye",
                    "tracking",
                    "as",
                    "a",
                    "machine",
                    "translation",
                    "evaluation",
                    "mechanism",
                    "#TARGET_REF",
                    ",",
                    "which",
                    "focused",
                    "on",
                    "the",
                    "readability",
                    "and",
                    "comprehension",
                    "of",
                    "machine-translated",
                    "technical",
                    "support",
                    "documentation",
                    "#REF",
                    ",",
                    "and",
                    "on",
                    "the",
                    "impact",
                    "of",
                    "controlled",
                    "authoring",
                    "on",
                    "the",
                    "readability",
                    "of",
                    "MT",
                    "output",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                3,
                1
            ]
        },
        "input": "sent0: This paper reports on a project whose aims are to investigate the usability of raw machine translated technical support documentation for a commercial online service.\n sent1: It builds on previous work which investigates the use of eye tracking as a machine translation evaluation mechanism #TARGET_REF , which focused on the readability and comprehension of machine-translated technical support documentation #REF , and on the impact of controlled authoring on the readability of MT output #REF .\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Trained",
                    "on",
                    "20GB",
                    "texts",
                    "of",
                    "both",
                    "Vietnamese",
                    "news",
                    "and",
                    "Vietnamese",
                    "Wikipedia",
                    "Bert4News",
                    "#REF",
                    "x",
                    "Trained",
                    "on",
                    "more",
                    "than",
                    "20GB",
                    "texts",
                    "of",
                    "Vietnamese",
                    "news",
                    "vElectra",
                    "and",
                    "ViBERT",
                    "#REF",
                    "x",
                    "vElectra",
                    "was",
                    "trained",
                    "on",
                    "10GB",
                    "texts,",
                    "whereas",
                    "ViBERT",
                    "was",
                    "trained",
                    "on",
                    "60GB",
                    "texts",
                    "of",
                    "Vietnamese",
                    "news",
                    "VGG16",
                    "#TARGET_REF",
                    "x"
                ]
            ],
            "context": [
                0
            ]
        },
        "input": "sent0: Trained on 20GB texts of both Vietnamese news and Vietnamese Wikipedia Bert4News #REF x Trained on more than 20GB texts of Vietnamese news vElectra and ViBERT #REF x vElectra was trained on 10GB texts, whereas ViBERT was trained on 60GB texts of Vietnamese news VGG16 #TARGET_REF x\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "•",
                    "cosine",
                    "similarity",
                    "in",
                    "GloVe",
                    "#TARGET_REF",
                    ","
                ]
            ],
            "context": [
                2
            ]
        },
        "input": "sent0: • cosine similarity in GloVe #TARGET_REF ,\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Evaluation",
                    "For",
                    "this",
                    "task,",
                    "we",
                    "have",
                    "classified",
                    "the",
                    "generated",
                    "sentences",
                    "in",
                    "terms",
                    "of",
                    "their",
                    "sentiment",
                    "using",
                    "a",
                    "Bidirectional-LSTM",
                    "as",
                    "classifier."
                ],
                [
                    "In",
                    "addition,",
                    "we",
                    "have",
                    "evaluated",
                    "two",
                    "quality",
                    "metrics:",
                    "1)",
                    "the",
                    "novelty",
                    "of",
                    "each",
                    "generated",
                    "sentence",
                    "(Eq."
                ],
                [
                    "5)",
                    "using",
                    "the",
                    "definition",
                    "from",
                    "#TARGET_REF",
                    ",",
                    "where",
                    "JS",
                    "is",
                    "the",
                    "Jaccard",
                    "similarity",
                    "and",
                    "C",
                    "j",
                    "are",
                    "the",
                    "training",
                    "set",
                    "sentences."
                ],
                [
                    "The",
                    "novelty",
                    "measures",
                    "the",
                    "diversity",
                    "between",
                    "the",
                    "generated",
                    "data",
                    "and",
                    "the",
                    "training",
                    "corpus,",
                    "and",
                    "2)",
                    "the",
                    "diversity",
                    "metric",
                    "(Eq."
                ],
                [
                    "6),",
                    "a",
                    "measure",
                    "of",
                    "the",
                    "model's",
                    "ability",
                    "to",
                    "generate",
                    "diverse",
                    "sentences",
                    "and",
                    "avoid",
                    "mode",
                    "collapse."
                ]
            ],
            "context": [
                0,
                2,
                1,
                3,
                0
            ]
        },
        "input": "sent0: Evaluation For this task, we have classified the generated sentences in terms of their sentiment using a Bidirectional-LSTM as classifier.\n sent1: In addition, we have evaluated two quality metrics: 1) the novelty of each generated sentence (Eq.\n sent2: 5) using the definition from #TARGET_REF , where JS is the Jaccard similarity and C j are the training set sentences.\n sent3: The novelty measures the diversity between the generated data and the training corpus, and 2) the diversity metric (Eq.\n sent4: 6), a measure of the model's ability to generate diverse sentences and avoid mode collapse.\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Obtaining",
                    "salient",
                    "words",
                    "by",
                    "applying",
                    "existing",
                    "saliency",
                    "identification",
                    "approaches",
                    "#TARGET_REF",
                    "is,",
                    "however,",
                    "unable",
                    "to",
                    "produce",
                    "unified",
                    "action",
                    "representations."
                ],
                [
                    "Specifically,",
                    "system",
                    "utterances",
                    "with",
                    "the",
                    "same",
                    "intention",
                    "might",
                    "not",
                    "share",
                    "similar",
                    "wordings,",
                    "and",
                    "existing",
                    "attribution",
                    "approaches",
                    "can",
                    "only",
                    "identify",
                    "salient",
                    "words",
                    "within",
                    "utterances."
                ],
                [
                    "We",
                    "tackle",
                    "this",
                    "challenge",
                    "by",
                    "proposing",
                    "a",
                    "memoryaugmented",
                    "saliency",
                    "approach",
                    "that",
                    "identifies",
                    "salient",
                    "words",
                    "from",
                    "a",
                    "broader",
                    "vocabulary."
                ],
                [
                    "The",
                    "vocabulary",
                    "consists",
                    "of",
                    "all",
                    "the",
                    "words",
                    "that",
                    "could",
                    "compose",
                    "natural",
                    "language",
                    "actions,",
                    "1",
                    "and",
                    "each",
                    "word",
                    "is",
                    "stored",
                    "as",
                    "a",
                    "slot",
                    "in",
                    "the",
                    "memory",
                    "component."
                ],
                [
                    "By",
                    "incorporating",
                    "the",
                    "memory",
                    "component",
                    "into",
                    "a",
                    "dialogue",
                    "state",
                    "tracking",
                    "model,",
                    "we",
                    "use",
                    "each",
                    "system",
                    "utterance",
                    "as",
                    "a",
                    "query",
                    "to",
                    "perform",
                    "memory",
                    "retrieval,",
                    "and",
                    "the",
                    "retrieval",
                    "results",
                    "are",
                    "considered",
                    "as",
                    "salient",
                    "words."
                ],
                [
                    "The",
                    "retrieval",
                    "results",
                    "might",
                    "contain",
                    "words",
                    "that",
                    "are",
                    "redundant",
                    "since",
                    "we",
                    "do",
                    "not",
                    "have",
                    "direct",
                    "supervision",
                    "for",
                    "the",
                    "retrieval",
                    "operations."
                ],
                [
                    "For",
                    "example,",
                    "the",
                    "resulting",
                    "salient",
                    "words",
                    "might",
                    "be",
                    "\"but",
                    "turn",
                    "bland\"",
                    "in",
                    "the",
                    "example",
                    "shown",
                    "earlier,",
                    "which",
                    "include",
                    "unnecessary",
                    "words",
                    "and",
                    "may",
                    "lead",
                    "to",
                    "degenerated",
                    "action",
                    "results."
                ],
                [
                    "To",
                    "obtain",
                    "compact",
                    "action",
                    "representations,",
                    "we",
                    "propose",
                    "an",
                    "auxiliary",
                    "task",
                    "based",
                    "on",
                    "pseudo",
                    "parallel",
                    "corpus,",
                    "i.e.,",
                    "dialogue",
                    "context",
                    "and",
                    "state",
                    "annotation",
                    "pairs."
                ],
                [
                    "We",
                    "observe",
                    "that",
                    "dialogue",
                    "states",
                    "serve",
                    "as",
                    "good",
                    "examples",
                    "of",
                    "how",
                    "compact",
                    "representation",
                    "should",
                    "be."
                ],
                [
                    "Therefore,",
                    "we",
                    "use",
                    "the",
                    "encoded",
                    "dialogue",
                    "context",
                    "as",
                    "query",
                    "and",
                    "ask",
                    "the",
                    "memory",
                    "component",
                    "to",
                    "reconstruct",
                    "its",
                    "text-based",
                    "dialogue",
                    "states."
                ],
                [
                    "In",
                    "this",
                    "way,",
                    "the",
                    "obtained",
                    "concise",
                    "actions",
                    "generalize",
                    "better",
                    "and",
                    "can",
                    "be",
                    "easily",
                    "interpreted."
                ]
            ],
            "context": [
                1,
                3,
                2,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Obtaining salient words by applying existing saliency identification approaches #TARGET_REF is, however, unable to produce unified action representations.\n sent1: Specifically, system utterances with the same intention might not share similar wordings, and existing attribution approaches can only identify salient words within utterances.\n sent2: We tackle this challenge by proposing a memoryaugmented saliency approach that identifies salient words from a broader vocabulary.\n sent3: The vocabulary consists of all the words that could compose natural language actions, 1 and each word is stored as a slot in the memory component.\n sent4: By incorporating the memory component into a dialogue state tracking model, we use each system utterance as a query to perform memory retrieval, and the retrieval results are considered as salient words.\n sent5: The retrieval results might contain words that are redundant since we do not have direct supervision for the retrieval operations.\n sent6: For example, the resulting salient words might be \"but turn bland\" in the example shown earlier, which include unnecessary words and may lead to degenerated action results.\n sent7: To obtain compact action representations, we propose an auxiliary task based on pseudo parallel corpus, i.e., dialogue context and state annotation pairs.\n sent8: We observe that dialogue states serve as good examples of how compact representation should be.\n sent9: Therefore, we use the encoded dialogue context as query and ask the memory component to reconstruct its text-based dialogue states.\n sent10: In this way, the obtained concise actions generalize better and can be easily interpreted.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": [\"sent1\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "note",
                    "that",
                    "as",
                    "we",
                    "targeted",
                    "at",
                    "the",
                    "one-to-many",
                    "generation",
                    "problem,",
                    "we",
                    "excluded",
                    "those",
                    "baseline",
                    "methods",
                    "mentioned",
                    "in",
                    "the",
                    "related",
                    "work",
                    "that",
                    "cannot",
                    "produce",
                    "multiple",
                    "outputs,",
                    "e.g.,",
                    "#REF",
                    ",",
                    "#REF",
                    ",",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Different",
                    "from",
                    "aforementioned",
                    "methods,",
                    "our",
                    "MoKGE",
                    "can",
                    "seek",
                    "diverse",
                    "reasoning",
                    "on",
                    "KG",
                    "to",
                    "encourage",
                    "various",
                    "generation",
                    "outputs",
                    "without",
                    "any",
                    "additional",
                    "conditions."
                ]
            ],
            "context": [
                2,
                2
            ]
        },
        "input": "sent0: We note that as we targeted at the one-to-many generation problem, we excluded those baseline methods mentioned in the related work that cannot produce multiple outputs, e.g., #REF , #REF , #TARGET_REF .\n sent1: Different from aforementioned methods, our MoKGE can seek diverse reasoning on KG to encourage various generation outputs without any additional conditions.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\", \"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "this",
                    "section",
                    "we",
                    "provide",
                    "a",
                    "short",
                    "overview",
                    "of",
                    "six",
                    "widely",
                    "used",
                    "disentanglement",
                    "metrics,",
                    "highlighting",
                    "their",
                    "key",
                    "differences",
                    "and",
                    "commonalities,",
                    "and",
                    "refer",
                    "the",
                    "readers",
                    "to",
                    "the",
                    "corresponding",
                    "papers",
                    "for",
                    "exact",
                    "details",
                    "of",
                    "computations."
                ],
                [
                    "#TARGET_REF",
                    "define",
                    "three",
                    "criteria",
                    "for",
                    "disentangled",
                    "representations:",
                    "disentanglement,",
                    "which",
                    "measures",
                    "the",
                    "degree",
                    "of",
                    "one",
                    "dimension",
                    "only",
                    "encoding",
                    "information",
                    "about",
                    "no",
                    "more",
                    "than",
                    "one",
                    "generative",
                    "factor,",
                    "completeness,",
                    "which",
                    "measures",
                    "whether",
                    "a",
                    "generative",
                    "factor",
                    "is",
                    "only",
                    "captured",
                    "by",
                    "one",
                    "latent",
                    "variable,",
                    "informativeness,",
                    "which",
                    "measures",
                    "the",
                    "degree",
                    "by",
                    "which",
                    "representations",
                    "capture",
                    "exact",
                    "values",
                    "of",
                    "the",
                    "generative",
                    "factors."
                ],
                [
                    "2",
                    "They",
                    "design",
                    "a",
                    "series",
                    "of",
                    "classification",
                    "tasks",
                    "to",
                    "predict",
                    "the",
                    "value",
                    "of",
                    "a",
                    "generative",
                    "factor",
                    "based",
                    "on",
                    "the",
                    "latent",
                    "code,",
                    "and",
                    "extract",
                    "the",
                    "relative",
                    "importance",
                    "of",
                    "each",
                    "latent",
                    "code",
                    "for",
                    "each",
                    "task",
                    "to",
                    "calculate",
                    "disentanglement",
                    "and",
                    "completeness",
                    "scores."
                ],
                [
                    "Informativeness",
                    "score",
                    "is",
                    "measured",
                    "by",
                    "the",
                    "accuracy",
                    "of",
                    "the",
                    "classifier",
                    "directly."
                ],
                [
                    "Other",
                    "existing",
                    "metrics",
                    "reflect",
                    "at",
                    "least",
                    "one",
                    "of",
                    "these",
                    "three",
                    "criteria,",
                    "as",
                    "summarised",
                    "in",
                    "Table",
                    "1",
                    "#REF",
                    "focus",
                    "on",
                    "disentanglement",
                    "and",
                    "propose",
                    "to",
                    "use",
                    "the",
                    "absolute",
                    "difference",
                    "of",
                    "two",
                    "groups",
                    "of",
                    "representations",
                    "with",
                    "the",
                    "same",
                    "value",
                    "on",
                    "one",
                    "generative",
                    "factor",
                    "to",
                    "predict",
                    "this",
                    "generative",
                    "factor."
                ],
                [
                    "For",
                    "perfectly",
                    "disentangled",
                    "representations,",
                    "latent",
                    "dimensions",
                    "not",
                    "encoding",
                    "information",
                    "about",
                    "this",
                    "generative",
                    "factor",
                    "would",
                    "have",
                    "zero",
                    "difference."
                ],
                [
                    "Hence,",
                    "even",
                    "simple",
                    "linear",
                    "classifiers",
                    "could",
                    "easily",
                    "identify",
                    "the",
                    "generative",
                    "factors",
                    "based",
                    "on",
                    "the",
                    "changes",
                    "of",
                    "values."
                ],
                [
                    "#REF",
                    "consider",
                    "both",
                    "disentanglement",
                    "and",
                    "completeness",
                    "by",
                    "first",
                    "finding",
                    "the",
                    "dimension",
                    "which",
                    "has",
                    "the",
                    "largest",
                    "variance",
                    "when",
                    "fixing",
                    "the",
                    "value",
                    "on",
                    "one",
                    "generative",
                    "factor,",
                    "and",
                    "then",
                    "using",
                    "the",
                    "found",
                    "dimension",
                    "to",
                    "predict",
                    "that",
                    "generative",
                    "factor."
                ],
                [
                    "#REF",
                    "propose",
                    "a",
                    "series",
                    "of",
                    "classification",
                    "tasks",
                    "each",
                    "of",
                    "which",
                    "uses",
                    "a",
                    "single",
                    "latent",
                    "variable",
                    "to",
                    "predict",
                    "the",
                    "value",
                    "of",
                    "a",
                    "generative",
                    "factor",
                    "and",
                    "treat",
                    "the",
                    "average",
                    "of",
                    "the",
                    "difference",
                    "between",
                    "the",
                    "top",
                    "two",
                    "accuracy",
                    "scores",
                    "for",
                    "each",
                    "generative",
                    "factor",
                    "as",
                    "the",
                    "final",
                    "disentanglement",
                    "score."
                ]
            ],
            "context": [
                2,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: In this section we provide a short overview of six widely used disentanglement metrics, highlighting their key differences and commonalities, and refer the readers to the corresponding papers for exact details of computations.\n sent1: #TARGET_REF define three criteria for disentangled representations: disentanglement, which measures the degree of one dimension only encoding information about no more than one generative factor, completeness, which measures whether a generative factor is only captured by one latent variable, informativeness, which measures the degree by which representations capture exact values of the generative factors.\n sent2: 2 They design a series of classification tasks to predict the value of a generative factor based on the latent code, and extract the relative importance of each latent code for each task to calculate disentanglement and completeness scores.\n sent3: Informativeness score is measured by the accuracy of the classifier directly.\n sent4: Other existing metrics reflect at least one of these three criteria, as summarised in Table 1 #REF focus on disentanglement and propose to use the absolute difference of two groups of representations with the same value on one generative factor to predict this generative factor.\n sent5: For perfectly disentangled representations, latent dimensions not encoding information about this generative factor would have zero difference.\n sent6: Hence, even simple linear classifiers could easily identify the generative factors based on the changes of values.\n sent7: #REF consider both disentanglement and completeness by first finding the dimension which has the largest variance when fixing the value on one generative factor, and then using the found dimension to predict that generative factor.\n sent8: #REF propose a series of classification tasks each of which uses a single latent variable to predict the value of a generative factor and treat the average of the difference between the top two accuracy scores for each generative factor as the final disentanglement score.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "this",
                    "paper,",
                    "we",
                    "focus",
                    "on",
                    "addressing",
                    "these",
                    "challenges",
                    "so",
                    "as",
                    "to",
                    "effectively",
                    "train",
                    "a",
                    "dual-encoder",
                    "retriever",
                    "for",
                    "open-domain",
                    "QA."
                ],
                [
                    "We",
                    "propose",
                    "an",
                    "optimized",
                    "training",
                    "approach,",
                    "called",
                    "RocketQA,",
                    "to",
                    "improving",
                    "dense",
                    "passage",
                    "retrieval."
                ],
                [
                    "Considering",
                    "the",
                    "above",
                    "challenges,",
                    "we",
                    "make",
                    "three",
                    "major",
                    "technical",
                    "contributions",
                    "in",
                    "RocketQA."
                ],
                [
                    "First,",
                    "RocketQA",
                    "introduces",
                    "cross-batch",
                    "negatives."
                ],
                [
                    "Comparing",
                    "to",
                    "inbatch",
                    "negatives,",
                    "it",
                    "increases",
                    "the",
                    "number",
                    "of",
                    "available",
                    "negatives",
                    "for",
                    "each",
                    "question",
                    "during",
                    "training,",
                    "and",
                    "alleviates",
                    "the",
                    "discrepancy",
                    "between",
                    "training",
                    "and",
                    "inference."
                ],
                [
                    "Second,",
                    "RocketQA",
                    "introduces",
                    "denoised",
                    "hard",
                    "negatives."
                ],
                [
                    "It",
                    "aims",
                    "to",
                    "remove",
                    "false",
                    "negatives",
                    "from",
                    "the",
                    "top-ranked",
                    "results",
                    "retrieved",
                    "by",
                    "a",
                    "retriever,",
                    "and",
                    "derive",
                    "more",
                    "reliable",
                    "hard",
                    "negatives."
                ],
                [
                    "Third,",
                    "RocketQA",
                    "leverages",
                    "large-scale",
                    "unsupervised",
                    "data",
                    "\"labeled\"",
                    "by",
                    "a",
                    "cross-encoder",
                    "(as",
                    "shown",
                    "in",
                    "Figure",
                    "1b)",
                    "for",
                    "data",
                    "augmentation."
                ],
                [
                    "Though",
                    "inefficient,",
                    "the",
                    "cross-encoder",
                    "architecture",
                    "has",
                    "been",
                    "found",
                    "to",
                    "be",
                    "more",
                    "capable",
                    "than",
                    "the",
                    "dual-encoder",
                    "architecture",
                    "in",
                    "both",
                    "theory",
                    "and",
                    "practice",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Therefore,",
                    "we",
                    "utilize",
                    "a",
                    "cross-encoder",
                    "to",
                    "generate",
                    "highquality",
                    "pseudo",
                    "labels",
                    "for",
                    "unlabeled",
                    "data",
                    "which",
                    "are",
                    "used",
                    "to",
                    "train",
                    "the",
                    "dual-encoder",
                    "retriever."
                ],
                [
                    "The",
                    "contributions",
                    "of",
                    "this",
                    "paper",
                    "are",
                    "as",
                    "follows:"
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                1,
                2,
                0
            ]
        },
        "input": "sent0: In this paper, we focus on addressing these challenges so as to effectively train a dual-encoder retriever for open-domain QA.\n sent1: We propose an optimized training approach, called RocketQA, to improving dense passage retrieval.\n sent2: Considering the above challenges, we make three major technical contributions in RocketQA.\n sent3: First, RocketQA introduces cross-batch negatives.\n sent4: Comparing to inbatch negatives, it increases the number of available negatives for each question during training, and alleviates the discrepancy between training and inference.\n sent5: Second, RocketQA introduces denoised hard negatives.\n sent6: It aims to remove false negatives from the top-ranked results retrieved by a retriever, and derive more reliable hard negatives.\n sent7: Third, RocketQA leverages large-scale unsupervised data \"labeled\" by a cross-encoder (as shown in Figure 1b) for data augmentation.\n sent8: Though inefficient, the cross-encoder architecture has been found to be more capable than the dual-encoder architecture in both theory and practice #TARGET_REF .\n sent9: Therefore, we utilize a cross-encoder to generate highquality pseudo labels for unlabeled data which are used to train the dual-encoder retriever.\n sent10: The contributions of this paper are as follows:\n",
        "output": "{\"INFORMATION\": [\"sent8\"], \"PERCEPTION\": [\"sent9\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Specifically,",
                    "for",
                    "the",
                    "document",
                    "retrieval",
                    "step,",
                    "we",
                    "narrow",
                    "the",
                    "search",
                    "space",
                    "with",
                    "an",
                    "information",
                    "retrieval",
                    "model",
                    "DRQA",
                    "#TARGET_REF",
                    "and",
                    "then",
                    "rerank",
                    "the",
                    "retrieved",
                    "pages."
                ],
                [
                    "For",
                    "the",
                    "evidence",
                    "retrieval",
                    "step,",
                    "we",
                    "design",
                    "a",
                    "multi-turn",
                    "cell",
                    "selector",
                    "to",
                    "extract",
                    "sentence",
                    "evidence",
                    "and",
                    "table",
                    "evidence",
                    "respectively,",
                    "and",
                    "select",
                    "evidence",
                    "cells",
                    "from",
                    "tables."
                ],
                [
                    "Finally,",
                    "we",
                    "propose",
                    "a",
                    "Dual",
                    "Channel",
                    "Unified",
                    "Format",
                    "verification",
                    "model",
                    "(DCUF,",
                    "shown",
                    "in",
                    "Figure",
                    "2)",
                    "for",
                    "the",
                    "verification",
                    "step."
                ],
                [
                    "DCUF",
                    "converts",
                    "evidence",
                    "to",
                    "a",
                    "unified",
                    "table/sentence",
                    "format",
                    "with",
                    "carefully-designed",
                    "evidence",
                    "conversion",
                    "and",
                    "re-organization",
                    "methods",
                    "in",
                    "each",
                    "channel,",
                    "and",
                    "combine",
                    "dual-channel",
                    "encodings",
                    "to",
                    "make",
                    "the",
                    "final",
                    "prediction."
                ]
            ],
            "context": [
                2,
                2,
                0,
                0
            ]
        },
        "input": "sent0: Specifically, for the document retrieval step, we narrow the search space with an information retrieval model DRQA #TARGET_REF and then rerank the retrieved pages.\n sent1: For the evidence retrieval step, we design a multi-turn cell selector to extract sentence evidence and table evidence respectively, and select evidence cells from tables.\n sent2: Finally, we propose a Dual Channel Unified Format verification model (DCUF, shown in Figure 2) for the verification step.\n sent3: DCUF converts evidence to a unified table/sentence format with carefully-designed evidence conversion and re-organization methods in each channel, and combine dual-channel encodings to make the final prediction.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\", \"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "As",
                    "shown",
                    "in",
                    "previous",
                    "work",
                    "#TARGET_REF",
                    ",",
                    "increasing",
                    "the",
                    "depth",
                    "of",
                    "the",
                    "Transformer",
                    "encoder",
                    "can",
                    "substantially",
                    "improve",
                    "model",
                    "performance,",
                    "therefore",
                    "we",
                    "train",
                    "the",
                    "Transformer",
                    "with",
                    "deep",
                    "encoder",
                    "to",
                    "obtain",
                    "a",
                    "better",
                    "source",
                    "representation."
                ]
            ],
            "context": [
                1
            ]
        },
        "input": "sent0: As shown in previous work #TARGET_REF , increasing the depth of the Transformer encoder can substantially improve model performance, therefore we train the Transformer with deep encoder to obtain a better source representation.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "design",
                    "a",
                    "memory",
                    "component",
                    "to",
                    "identify",
                    "the",
                    "salient",
                    "words",
                    "of",
                    "system",
                    "utterances",
                    "in",
                    "terms",
                    "of",
                    "modeling",
                    "state",
                    "transitions",
                    "(Sec."
                ],
                [
                    "3.2)."
                ],
                [
                    "To",
                    "further",
                    "boost",
                    "the",
                    "memory's",
                    "capability",
                    "in",
                    "learning",
                    "compact",
                    "natural",
                    "language",
                    "actions,",
                    "we",
                    "propose",
                    "a",
                    "novel",
                    "auxiliary",
                    "task",
                    "to",
                    "identify",
                    "salient",
                    "words",
                    "of",
                    "dialogue",
                    "context",
                    "in",
                    "a",
                    "supervised",
                    "setting",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Furthermore,we",
                    "propose",
                    "to",
                    "take",
                    "more",
                    "advantage",
                    "from",
                    "the",
                    "action",
                    "learning",
                    "phase",
                    "by",
                    "reusing",
                    "the",
                    "memory",
                    "component",
                    "for",
                    "conditioned",
                    "response",
                    "generation",
                    "(Sec."
                ],
                [
                    "3.4)."
                ]
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "sent0: We design a memory component to identify the salient words of system utterances in terms of modeling state transitions (Sec.\n sent1: 3.2).\n sent2: To further boost the memory's capability in learning compact natural language actions, we propose a novel auxiliary task to identify salient words of dialogue context in a supervised setting #TARGET_REF .\n sent3: Furthermore,we propose to take more advantage from the action learning phase by reusing the memory component for conditioned response generation (Sec.\n sent4: 3.4).\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Even",
                    "in",
                    "the",
                    "presence",
                    "of",
                    "such",
                    "an",
                    "agreement,",
                    "learning",
                    "to",
                    "disentangle",
                    "the",
                    "surface",
                    "realization",
                    "of",
                    "the",
                    "underlying",
                    "factors",
                    "of",
                    "data",
                    "(e.g.,",
                    "semantics,",
                    "syntactic,",
                    "lexical)",
                    "in",
                    "the",
                    "representation",
                    "space",
                    "is",
                    "a",
                    "nontrivial",
                    "task."
                ],
                [
                    "Additionally,",
                    "there",
                    "is",
                    "no",
                    "established",
                    "study",
                    "for",
                    "evaluating",
                    "such",
                    "models",
                    "in",
                    "NLP."
                ],
                [
                    "A",
                    "handful",
                    "of",
                    "recent",
                    "works",
                    "have",
                    "looked",
                    "into",
                    "disentanglement",
                    "for",
                    "text",
                    "by",
                    "splitting",
                    "the",
                    "representation",
                    "space",
                    "into",
                    "predefined",
                    "disentangled",
                    "subspaces",
                    "such",
                    "as",
                    "style",
                    "and",
                    "content",
                    "#TARGET_REF",
                    ",",
                    "or",
                    "syntax",
                    "and",
                    "semantics",
                    "#REF",
                    ",",
                    "and",
                    "rely",
                    "on",
                    "supervision",
                    "during",
                    "training."
                ],
                [
                    "However,",
                    "a",
                    "generalizable",
                    "and",
                    "realistic",
                    "approach",
                    "needs",
                    "to",
                    "be",
                    "unsupervised",
                    "and",
                    "capable",
                    "of",
                    "identifying",
                    "the",
                    "underlying",
                    "factors",
                    "solely",
                    "via",
                    "the",
                    "regularities",
                    "presented",
                    "in",
                    "data."
                ]
            ],
            "context": [
                0,
                0,
                1,
                0
            ]
        },
        "input": "sent0: Even in the presence of such an agreement, learning to disentangle the surface realization of the underlying factors of data (e.g., semantics, syntactic, lexical) in the representation space is a nontrivial task.\n sent1: Additionally, there is no established study for evaluating such models in NLP.\n sent2: A handful of recent works have looked into disentanglement for text by splitting the representation space into predefined disentangled subspaces such as style and content #TARGET_REF , or syntax and semantics #REF , and rely on supervision during training.\n sent3: However, a generalizable and realistic approach needs to be unsupervised and capable of identifying the underlying factors solely via the regularities presented in data.\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "To",
                    "tackle",
                    "those",
                    "challenges,",
                    "most",
                    "existing",
                    "approaches",
                    "#REF",
                    "adopt",
                    "a",
                    "two-step",
                    "pretraining",
                    "strategy",
                    "that",
                    "firstly",
                    "utilizes",
                    "off-the-shelf",
                    "detectors",
                    "to",
                    "parse",
                    "images",
                    "into",
                    "a",
                    "set",
                    "of",
                    "object",
                    "tokens,",
                    "and",
                    "then",
                    "builds",
                    "a",
                    "multi-layer",
                    "Transformer",
                    "to",
                    "learn",
                    "visual",
                    "and",
                    "language",
                    "embeddings",
                    "jointly."
                ],
                [
                    "In",
                    "order",
                    "to",
                    "facilitate",
                    "the",
                    "multi-modal",
                    "learning,",
                    "those",
                    "networks",
                    "are",
                    "typically",
                    "trained",
                    "via",
                    "a",
                    "set",
                    "of",
                    "carefully",
                    "designed",
                    "BERT-like",
                    "objectives",
                    "(e.g."
                ],
                [
                    "Image-Text",
                    "Matching)."
                ],
                [
                    "Despite",
                    "its",
                    "promising",
                    "performance,",
                    "the",
                    "two-step",
                    "strategy",
                    "suffers",
                    "from",
                    "several",
                    "limitations:",
                    "1)",
                    "limited",
                    "visual",
                    "object",
                    "concepts",
                    "as",
                    "the",
                    "external",
                    "detectors",
                    "are",
                    "trained",
                    "on",
                    "a",
                    "predefined",
                    "set",
                    "of",
                    "object",
                    "categories,",
                    "2)",
                    "lack",
                    "of",
                    "context",
                    "cues",
                    "outside",
                    "of",
                    "the",
                    "object",
                    "regions,",
                    "which",
                    "are",
                    "crucial",
                    "for",
                    "complex",
                    "reasoning",
                    "tasks,",
                    "3)",
                    "sub-optimal",
                    "visual",
                    "representation",
                    "due",
                    "to",
                    "stage-wise",
                    "training,",
                    "and",
                    "4)",
                    "computational",
                    "inefficiency",
                    "caused",
                    "by",
                    "additional",
                    "detection",
                    "modules."
                ],
                [
                    "To",
                    "overcome",
                    "those",
                    "limitations,",
                    "recent",
                    "works",
                    "attempt",
                    "to",
                    "learn",
                    "a",
                    "joint",
                    "visual-linguistic",
                    "representations",
                    "in",
                    "an",
                    "end-to-end",
                    "manner",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "These",
                    "methods",
                    "directly",
                    "take",
                    "dense",
                    "visual",
                    "features",
                    "from",
                    "image",
                    "grids",
                    "as",
                    "inputs",
                    "to",
                    "a",
                    "multi-modal",
                    "Transformer",
                    "network,",
                    "and",
                    "hence",
                    "do",
                    "not",
                    "rely",
                    "on",
                    "external",
                    "object",
                    "detectors",
                    "in",
                    "both",
                    "pretraining",
                    "and",
                    "finetuning",
                    "stages."
                ],
                [
                    "Such",
                    "model",
                    "design",
                    "significantly",
                    "simplifies",
                    "overall",
                    "network",
                    "architecture",
                    "and",
                    "allows",
                    "deeper",
                    "integration",
                    "between",
                    "visual",
                    "and",
                    "language",
                    "features."
                ],
                [
                    "However,",
                    "using",
                    "grid-level",
                    "features",
                    "makes",
                    "it",
                    "difficult",
                    "to",
                    "capture",
                    "object-level",
                    "visual",
                    "concepts,",
                    "which",
                    "often",
                    "results",
                    "in",
                    "less",
                    "expressive",
                    "multi-modal",
                    "representations",
                    "and",
                    "inferior",
                    "performances",
                    "in",
                    "downstream",
                    "tasks."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                1,
                1,
                2,
                2
            ]
        },
        "input": "sent0: To tackle those challenges, most existing approaches #REF adopt a two-step pretraining strategy that firstly utilizes off-the-shelf detectors to parse images into a set of object tokens, and then builds a multi-layer Transformer to learn visual and language embeddings jointly.\n sent1: In order to facilitate the multi-modal learning, those networks are typically trained via a set of carefully designed BERT-like objectives (e.g.\n sent2: Image-Text Matching).\n sent3: Despite its promising performance, the two-step strategy suffers from several limitations: 1) limited visual object concepts as the external detectors are trained on a predefined set of object categories, 2) lack of context cues outside of the object regions, which are crucial for complex reasoning tasks, 3) sub-optimal visual representation due to stage-wise training, and 4) computational inefficiency caused by additional detection modules.\n sent4: To overcome those limitations, recent works attempt to learn a joint visual-linguistic representations in an end-to-end manner #TARGET_REF .\n sent5: These methods directly take dense visual features from image grids as inputs to a multi-modal Transformer network, and hence do not rely on external object detectors in both pretraining and finetuning stages.\n sent6: Such model design significantly simplifies overall network architecture and allows deeper integration between visual and language features.\n sent7: However, using grid-level features makes it difficult to capture object-level visual concepts, which often results in less expressive multi-modal representations and inferior performances in downstream tasks.\n",
        "output": "{\"INFORMATION\": [\"sent4\", \"sent5\"], \"PERCEPTION\": [\"sent6\", \"sent7\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "As",
                    "shown",
                    "in",
                    "Figure",
                    "3,",
                    "we",
                    "organize",
                    "the",
                    "above",
                    "three",
                    "training",
                    "strategies",
                    "into",
                    "an",
                    "effective",
                    "training",
                    "pipeline",
                    "for",
                    "the",
                    "dual-encoder."
                ],
                [
                    "It",
                    "makes",
                    "an",
                    "analogy",
                    "to",
                    "a",
                    "multi-stage",
                    "rocket,",
                    "where",
                    "the",
                    "performance",
                    "of",
                    "the",
                    "dual-encoder",
                    "is",
                    "consecutively",
                    "improved",
                    "at",
                    "three",
                    "steps",
                    "(STEP",
                    "1,",
                    "3",
                    "and",
                    "4)."
                ],
                [
                    "That",
                    "is",
                    "why",
                    "we",
                    "call",
                    "our",
                    "approach",
                    "RocketQA."
                ],
                [
                    "Next,",
                    "we",
                    "will",
                    "describe",
                    "the",
                    "details",
                    "of",
                    "the",
                    "whole",
                    "training",
                    "procedure",
                    "of",
                    "RocketQA."
                ],
                [
                    "D",
                    "from",
                    "C",
                    "for",
                    "each",
                    "question",
                    "q",
                    "∈",
                    "Q",
                    "L",
                    "."
                ],
                [
                    "This",
                    "design",
                    "is",
                    "to",
                    "let",
                    "the",
                    "cross-encoder",
                    "adjust",
                    "to",
                    "the",
                    "distribution",
                    "of",
                    "the",
                    "results",
                    "retrieved",
                    "by",
                    "the",
                    "dualencoder,",
                    "since",
                    "the",
                    "cross-encoder",
                    "will",
                    "be",
                    "used",
                    "in",
                    "the",
                    "following",
                    "two",
                    "steps",
                    "for",
                    "optimizing",
                    "the",
                    "dualencoder."
                ],
                [
                    "This",
                    "design",
                    "is",
                    "important,",
                    "and",
                    "there",
                    "is",
                    "similar",
                    "observation",
                    "in",
                    "Facebook",
                    "Search",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                3,
                3,
                0,
                0,
                0,
                0,
                2
            ]
        },
        "input": "sent0: As shown in Figure 3, we organize the above three training strategies into an effective training pipeline for the dual-encoder.\n sent1: It makes an analogy to a multi-stage rocket, where the performance of the dual-encoder is consecutively improved at three steps (STEP 1, 3 and 4).\n sent2: That is why we call our approach RocketQA.\n sent3: Next, we will describe the details of the whole training procedure of RocketQA.\n sent4: D from C for each question q ∈ Q L .\n sent5: This design is to let the cross-encoder adjust to the distribution of the results retrieved by the dualencoder, since the cross-encoder will be used in the following two steps for optimizing the dualencoder.\n sent6: This design is important, and there is similar observation in Facebook Search #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent6\"], \"BACKGROUND\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Equations",
                    "7",
                    "and",
                    "8",
                    "attempt",
                    "to",
                    "account",
                    "for",
                    "this",
                    "by",
                    "using",
                    "a",
                    "shared",
                    "weight",
                    "concatenation",
                    "and",
                    "a",
                    "weighted",
                    "concatenation",
                    "respectively."
                ],
                [
                    "In",
                    "equation",
                    "7,",
                    "W",
                    "∈",
                    "R",
                    "k×k",
                    "is",
                    "a",
                    "weight",
                    "matrix,",
                    "where",
                    "the",
                    "values",
                    "are",
                    "scaled",
                    "down",
                    "to",
                    "1,",
                    "in",
                    "order",
                    "to",
                    "capture",
                    "the",
                    "relative",
                    "importance",
                    "of",
                    "each",
                    "h",
                    "c",
                    "i",
                    "and",
                    "h",
                    "w",
                    "i",
                    "∀h",
                    "c",
                    "i",
                    "∈",
                    "h",
                    "c",
                    ",",
                    "h",
                    "w",
                    "i",
                    "∈",
                    "h",
                    "w",
                    "."
                ],
                [
                    "This",
                    "shared",
                    "weighting",
                    "is",
                    "a",
                    "modification",
                    "of",
                    "the",
                    "concept",
                    "of",
                    "leaky",
                    "integration",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "On",
                    "the",
                    "other",
                    "hand,",
                    "equation",
                    "8",
                    "uses",
                    "two",
                    "independent",
                    "weight",
                    "matrices,",
                    "W",
                    "c",
                    ",",
                    "W",
                    "w",
                    "∈",
                    "R",
                    "k×k",
                    ",",
                    "which",
                    "does",
                    "not",
                    "constrain",
                    "the",
                    "network",
                    "to",
                    "use",
                    "on",
                    "other",
                    "the",
                    "other",
                    "hidden",
                    "representation."
                ],
                [
                    "However,",
                    "the",
                    "gradients",
                    "are",
                    "still",
                    "clipped",
                    "at",
                    "a",
                    "low",
                    "value",
                    "(≈",
                    "1)",
                    "to",
                    "avoid",
                    "explosion."
                ]
            ],
            "context": [
                3,
                0,
                2,
                0,
                0
            ]
        },
        "input": "sent0: Equations 7 and 8 attempt to account for this by using a shared weight concatenation and a weighted concatenation respectively.\n sent1: In equation 7, W ∈ R k×k is a weight matrix, where the values are scaled down to 1, in order to capture the relative importance of each h c i and h w i ∀h c i ∈ h c , h w i ∈ h w .\n sent2: This shared weighting is a modification of the concept of leaky integration #TARGET_REF .\n sent3: On the other hand, equation 8 uses two independent weight matrices, W c , W w ∈ R k×k , which does not constrain the network to use on other the other hidden representation.\n sent4: However, the gradients are still clipped at a low value (≈ 1) to avoid explosion.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "There",
                    "are",
                    "relatively",
                    "few",
                    "studies",
                    "of",
                    "the",
                    "usability",
                    "of",
                    "raw",
                    "machine",
                    "translated",
                    "documentation",
                    "by",
                    "real",
                    "end-users."
                ],
                [
                    "For",
                    "example,",
                    "Tomita's",
                    "work",
                    "#REF",
                    "focused",
                    "on",
                    "the",
                    "concept",
                    "of",
                    "content",
                    "comprehension."
                ],
                [
                    "#REF",
                    "evaluated",
                    "the",
                    "informativeness,",
                    "comprehension,",
                    "and",
                    "fluency",
                    "of",
                    "MT",
                    "output,",
                    "where",
                    "participants",
                    "had",
                    "no",
                    "reference",
                    "to",
                    "the",
                    "source",
                    "text,",
                    "while",
                    "#REF",
                    "measured",
                    "the",
                    "concept",
                    "of",
                    "usefulness."
                ],
                [
                    "#TARGET_REF",
                    "measure",
                    "the",
                    "readability",
                    "of",
                    "MT",
                    "output."
                ],
                [
                    "While",
                    "comprehensibility",
                    "and",
                    "readability",
                    "are",
                    "frequently",
                    "considered",
                    "to",
                    "be",
                    "components",
                    "of",
                    "usability,",
                    "these",
                    "studies",
                    "address",
                    "only",
                    "specific",
                    "aspects",
                    "of",
                    "the",
                    "concept",
                    "of",
                    "usability."
                ],
                [
                    "#REF",
                    "in",
                    "which",
                    "real",
                    "end",
                    "users'",
                    "needs",
                    "are",
                    "evaluated",
                    "in",
                    "the",
                    "context",
                    "of",
                    "web",
                    "usability",
                    "comes",
                    "closest",
                    "to",
                    "the",
                    "study",
                    "presented",
                    "here."
                ],
                [
                    "However,",
                    "Gaspari's",
                    "focus",
                    "is",
                    "on",
                    "the",
                    "usability",
                    "of",
                    "online",
                    "MT",
                    "systems,",
                    "as",
                    "opposed",
                    "to",
                    "the",
                    "text",
                    "they",
                    "generate."
                ]
            ],
            "context": [
                2,
                0,
                3,
                1,
                2,
                0,
                0
            ]
        },
        "input": "sent0: There are relatively few studies of the usability of raw machine translated documentation by real end-users.\n sent1: For example, Tomita's work #REF focused on the concept of content comprehension.\n sent2: #REF evaluated the informativeness, comprehension, and fluency of MT output, where participants had no reference to the source text, while #REF measured the concept of usefulness.\n sent3: #TARGET_REF measure the readability of MT output.\n sent4: While comprehensibility and readability are frequently considered to be components of usability, these studies address only specific aspects of the concept of usability.\n sent5: #REF in which real end users' needs are evaluated in the context of web usability comes closest to the study presented here.\n sent6: However, Gaspari's focus is on the usability of online MT systems, as opposed to the text they generate.\n",
        "output": "{\"INFORMATION\": [\"sent3\"], \"PERCEPTION\": [\"sent0\", \"sent4\"], \"BACKGROUND\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "To",
                    "the",
                    "best",
                    "of",
                    "our",
                    "knowledge,",
                    "the",
                    "first",
                    "algorithms",
                    "similar",
                    "to",
                    "Codenames",
                    "agents",
                    "have",
                    "been",
                    "created",
                    "by",
                    "#TARGET_REF",
                    "specifically",
                    "to",
                    "model",
                    "human",
                    "associations."
                ],
                [
                    "In",
                    "their",
                    "simplified",
                    "game,",
                    "the",
                    "board",
                    "always",
                    "consists",
                    "of",
                    "three",
                    "nouns,",
                    "and",
                    "the",
                    "agent",
                    "gives",
                    "a",
                    "clue",
                    "that",
                    "must",
                    "be",
                    "one",
                    "of",
                    "three",
                    "adjectives,",
                    "and",
                    "refers",
                    "to",
                    "exactly",
                    "two",
                    "of",
                    "the",
                    "board",
                    "words."
                ],
                [
                    "Their",
                    "clues",
                    "were",
                    "generated",
                    "based",
                    "on",
                    "the",
                    "following",
                    "five",
                    "similarity",
                    "functions:"
                ]
            ],
            "context": [
                2,
                3,
                0
            ]
        },
        "input": "sent0: To the best of our knowledge, the first algorithms similar to Codenames agents have been created by #TARGET_REF specifically to model human associations.\n sent1: In their simplified game, the board always consists of three nouns, and the agent gives a clue that must be one of three adjectives, and refers to exactly two of the board words.\n sent2: Their clues were generated based on the following five similarity functions:\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Self-training",
                    "#TARGET_REF",
                    ")",
                    "uses",
                    "a",
                    "source-to-target",
                    "model",
                    "to",
                    "generate",
                    "synthetic",
                    "pairs",
                    "from",
                    "source-side",
                    "monolingual",
                    "data",
                    "to",
                    "augment",
                    "the",
                    "original",
                    "parallel",
                    "corpus."
                ],
                [
                    "We",
                    "combined",
                    "2M",
                    "Chinese",
                    "monolingual",
                    "data",
                    "with",
                    "2M",
                    "Chinese",
                    "sentences",
                    "randomly",
                    "sampled",
                    "from",
                    "the",
                    "CWMT",
                    "parallel",
                    "corpus,",
                    "yielding",
                    "a",
                    "total",
                    "of",
                    "4M",
                    "monolingual",
                    "for",
                    "forward",
                    "translation."
                ]
            ],
            "context": [
                1,
                0
            ]
        },
        "input": "sent0: Self-training #TARGET_REF ) uses a source-to-target model to generate synthetic pairs from source-side monolingual data to augment the original parallel corpus.\n sent1: We combined 2M Chinese monolingual data with 2M Chinese sentences randomly sampled from the CWMT parallel corpus, yielding a total of 4M monolingual for forward translation.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "last",
                    "problem",
                    "is",
                    "not",
                    "necessarily",
                    "linked",
                    "to",
                    "finite-state",
                    "approaches,",
                    "but",
                    "in",
                    "two-level",
                    "morphology,",
                    "cf."
                ],
                [
                    "#TARGET_REF",
                    ",",
                    "#REF",
                    ",",
                    "the",
                    "way",
                    "features",
                    "are",
                    "handled",
                    "is",
                    "specifically",
                    "geared",
                    "towards",
                    "inflection."
                ],
                [
                    "The",
                    "first",
                    "two",
                    "problems",
                    "are",
                    "inherent",
                    "in",
                    "finite-state",
                    "technology",
                    "and",
                    "can",
                    "only",
                    "be",
                    "handled",
                    "by",
                    "one",
                    "of",
                    "the",
                    "following",
                    "strategies:"
                ]
            ],
            "context": [
                2,
                3,
                3
            ]
        },
        "input": "sent0: The last problem is not necessarily linked to finite-state approaches, but in two-level morphology, cf.\n sent1: #TARGET_REF , #REF , the way features are handled is specifically geared towards inflection.\n sent2: The first two problems are inherent in finite-state technology and can only be handled by one of the following strategies:\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "choice",
                    "of",
                    "languages",
                    "used",
                    "for",
                    "translation",
                    "and",
                    "transliteration",
                    "is",
                    "critical."
                ],
                [
                    "#REF",
                    "showed",
                    "that",
                    "languages",
                    "under",
                    "the",
                    "same",
                    "family",
                    "have",
                    "similar",
                    "representations",
                    "in",
                    "multilingual",
                    "models."
                ],
                [
                    "Hence,",
                    "we",
                    "put",
                    "together",
                    "translations",
                    "and",
                    "transliterations",
                    "from",
                    "related",
                    "languages",
                    "within",
                    "the",
                    "same",
                    "language",
                    "family",
                    "to",
                    "achieve",
                    "better",
                    "performance."
                ],
                [
                    "This",
                    "will",
                    "also",
                    "help",
                    "with",
                    "better",
                    "use",
                    "of",
                    "the",
                    "vo-cabulary",
                    "corpora",
                    "from",
                    "the",
                    "low-resource",
                    "languages."
                ],
                [
                    "We",
                    "also",
                    "study",
                    "the",
                    "impact",
                    "of",
                    "translation",
                    "and",
                    "transliteration",
                    "on",
                    "languages",
                    "outside",
                    "the",
                    "family",
                    "of",
                    "the",
                    "target",
                    "language."
                ],
                [
                    "Since",
                    "the",
                    "cross-family",
                    "language",
                    "transfer",
                    "degraded",
                    "the",
                    "QA",
                    "performance,",
                    "we",
                    "introduce",
                    "a",
                    "contrastive",
                    "loss",
                    "#TARGET_REF",
                    "between",
                    "the",
                    "translated",
                    "pairs",
                    "to",
                    "help",
                    "retain",
                    "or",
                    "improve",
                    "the",
                    "original",
                    "performance",
                    "by",
                    "encouraging",
                    "the",
                    "embeddings",
                    "from",
                    "all",
                    "languages",
                    "to",
                    "be",
                    "similar",
                    "regardless",
                    "of",
                    "the",
                    "family",
                    "group."
                ],
                [
                    "Thus,",
                    "the",
                    "contributions",
                    "of",
                    "the",
                    "paper",
                    "are",
                    "three-fold:"
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "sent0: The choice of languages used for translation and transliteration is critical.\n sent1: #REF showed that languages under the same family have similar representations in multilingual models.\n sent2: Hence, we put together translations and transliterations from related languages within the same language family to achieve better performance.\n sent3: This will also help with better use of the vo-cabulary corpora from the low-resource languages.\n sent4: We also study the impact of translation and transliteration on languages outside the family of the target language.\n sent5: Since the cross-family language transfer degraded the QA performance, we introduce a contrastive loss #TARGET_REF between the translated pairs to help retain or improve the original performance by encouraging the embeddings from all languages to be similar regardless of the family group.\n sent6: Thus, the contributions of the paper are three-fold:\n",
        "output": "{\"INFORMATION\": [\"sent5\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Pairwise",
                    "diversity",
                    "(⇓)."
                ],
                [
                    "Referred",
                    "as",
                    "\"self-\"",
                    "(e.g.,",
                    "self-BLEU)",
                    "#TARGET_REF",
                    ",",
                    "it",
                    "measures",
                    "the",
                    "within-distribution",
                    "similarity."
                ],
                [
                    "This",
                    "metric",
                    "computes",
                    "the",
                    "average",
                    "of",
                    "sentence-level",
                    "metrics",
                    "between",
                    "all",
                    "pairwise",
                    "combinations",
                    "of",
                    "hypotheses",
                    "{Y",
                    "(1)",
                    ",",
                    "•",
                    "•",
                    "•",
                    ",",
                    "Y",
                    "(K)",
                    "}",
                    "generated",
                    "from",
                    "each",
                    "source",
                    "sequence",
                    "X."
                ],
                [
                    "Lower",
                    "pairwise",
                    "metric",
                    "indicates",
                    "high",
                    "diversity",
                    "between",
                    "generated",
                    "hypotheses."
                ]
            ],
            "context": [
                3,
                1,
                0,
                0
            ]
        },
        "input": "sent0: Pairwise diversity (⇓).\n sent1: Referred as \"self-\" (e.g., self-BLEU) #TARGET_REF , it measures the within-distribution similarity.\n sent2: This metric computes the average of sentence-level metrics between all pairwise combinations of hypotheses {Y (1) , • • • , Y (K) } generated from each source sequence X.\n sent3: Lower pairwise metric indicates high diversity between generated hypotheses.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Third,",
                    "it",
                    "is",
                    "expensive",
                    "to",
                    "acquire",
                    "large-scale",
                    "training",
                    "data",
                    "for",
                    "open-domain",
                    "QA."
                ],
                [
                    "MSMARCO",
                    "and",
                    "Natural",
                    "Questions",
                    "#TARGET_REF",
                    "are",
                    "two",
                    "largest",
                    "datasets",
                    "for",
                    "open-domain",
                    "QA."
                ],
                [
                    "They",
                    "are",
                    "created",
                    "from",
                    "commercial",
                    "search",
                    "engines,",
                    "and",
                    "have",
                    "516K",
                    "and",
                    "300K",
                    "annotated",
                    "questions,",
                    "respectively."
                ],
                [
                    "However,",
                    "it",
                    "is",
                    "still",
                    "insufficient",
                    "to",
                    "cover",
                    "all",
                    "the",
                    "topics",
                    "of",
                    "questions",
                    "issued",
                    "by",
                    "users",
                    "to",
                    "search",
                    "engines."
                ]
            ],
            "context": [
                0,
                1,
                3,
                0
            ]
        },
        "input": "sent0: Third, it is expensive to acquire large-scale training data for open-domain QA.\n sent1: MSMARCO and Natural Questions #TARGET_REF are two largest datasets for open-domain QA.\n sent2: They are created from commercial search engines, and have 516K and 300K annotated questions, respectively.\n sent3: However, it is still insufficient to cover all the topics of questions issued by users to search engines.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "goal",
                    "of",
                    "SDG",
                    "task",
                    "is",
                    "to",
                    "generate",
                    "simple",
                    "definitions",
                    "for",
                    "languages",
                    "that",
                    "lack",
                    "learner's",
                    "dictionary."
                ],
                [
                    "For",
                    "example,",
                    "Chinese",
                    "as",
                    "Second",
                    "Language",
                    "(CSL)",
                    "learners",
                    "do",
                    "not",
                    "have",
                    "suitable",
                    "dictionaries."
                ],
                [
                    "As",
                    "#TARGET_REF",
                    "pointed",
                    "out,",
                    "since",
                    "the",
                    "difficulty",
                    "of",
                    "definitions",
                    "is",
                    "not",
                    "considered,",
                    "the",
                    "existing",
                    "dictionary",
                    "cannot",
                    "meet",
                    "CSL",
                    "learner's",
                    "needs."
                ]
            ],
            "context": [
                3,
                3,
                1
            ]
        },
        "input": "sent0: The goal of SDG task is to generate simple definitions for languages that lack learner's dictionary.\n sent1: For example, Chinese as Second Language (CSL) learners do not have suitable dictionaries.\n sent2: As #TARGET_REF pointed out, since the difficulty of definitions is not considered, the existing dictionary cannot meet CSL learner's needs.\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Turning",
                    "our",
                    "attention",
                    "now",
                    "to",
                    "Arabic",
                    "POS",
                    "tagging,",
                    "many",
                    "approaches",
                    "have",
                    "also",
                    "been",
                    "adopted",
                    "over",
                    "the",
                    "years",
                    "including",
                    "rule-based",
                    "methods",
                    "#TARGET_REF",
                    ",",
                    "statistical",
                    "models",
                    "#REF",
                    ",",
                    "hybrid",
                    "models",
                    "#REF",
                    "and",
                    "neural",
                    "networks",
                    "#REF",
                    "."
                ],
                [
                    "Performance",
                    "is",
                    "usually",
                    "much",
                    "higher",
                    "for",
                    "POS",
                    "tagging",
                    "than",
                    "NER."
                ],
                [
                    "#REF",
                    "introduced",
                    "a",
                    "hybrid",
                    "POS",
                    "tagger",
                    "(with",
                    "33",
                    "tags)",
                    "which",
                    "combined",
                    "HMM",
                    "with",
                    "a",
                    "rule-based",
                    "tagger."
                ],
                [
                    "They",
                    "used",
                    "the",
                    "Holy",
                    "Quran",
                    "Corpus",
                    "and",
                    "achieved",
                    "an",
                    "accuracy",
                    "rate",
                    "of",
                    "97.6%",
                    "and",
                    "96.8%",
                    "respectively."
                ],
                [
                    "Yousif",
                    "and",
                    "Sembok",
                    "(2008)",
                    "used",
                    "the",
                    "SVM",
                    "approach",
                    "and",
                    "a",
                    "corpus",
                    "of",
                    "177",
                    "tagged",
                    "words."
                ],
                [
                    "#REF",
                    "presented",
                    "a",
                    "probabilistic",
                    "POS",
                    "tagger",
                    "for",
                    "Arabic",
                    "text",
                    "based",
                    "on",
                    "HMM",
                    "called",
                    "Tree",
                    "Tagger."
                ],
                [
                    "The",
                    "proposed",
                    "tagger",
                    "obtained",
                    "accuracy",
                    "rates",
                    "of",
                    "99.4%",
                    "using",
                    "Al-Mus'haf",
                    "corpus."
                ]
            ],
            "context": [
                2,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Turning our attention now to Arabic POS tagging, many approaches have also been adopted over the years including rule-based methods #TARGET_REF , statistical models #REF , hybrid models #REF and neural networks #REF .\n sent1: Performance is usually much higher for POS tagging than NER.\n sent2: #REF introduced a hybrid POS tagger (with 33 tags) which combined HMM with a rule-based tagger.\n sent3: They used the Holy Quran Corpus and achieved an accuracy rate of 97.6% and 96.8% respectively.\n sent4: Yousif and Sembok (2008) used the SVM approach and a corpus of 177 tagged words.\n sent5: #REF presented a probabilistic POS tagger for Arabic text based on HMM called Tree Tagger.\n sent6: The proposed tagger obtained accuracy rates of 99.4% using Al-Mus'haf corpus.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Various",
                    "studies",
                    "attempted",
                    "to",
                    "generate",
                    "multiple",
                    "different",
                    "definitions",
                    "for",
                    "polysemous",
                    "words."
                ],
                [
                    "#REF",
                    "tackled",
                    "this",
                    "problem",
                    "by",
                    "computing",
                    "the",
                    "AdaGram",
                    "vectors",
                    "#REF",
                    "of",
                    "input",
                    "words,",
                    "which",
                    "are",
                    "capable",
                    "of",
                    "learning",
                    "different",
                    "representations",
                    "at",
                    "desired",
                    "semantic",
                    "resolutions."
                ],
                [
                    "However,",
                    "generating",
                    "different",
                    "definitions",
                    "based",
                    "on",
                    "contexts,",
                    "i.e.,",
                    "example",
                    "sentences,",
                    "became",
                    "the",
                    "mainstream",
                    "method",
                    "#REF",
                    ")."
                ],
                [
                    "Among",
                    "them,",
                    "some",
                    "studies",
                    "used",
                    "pre-trained",
                    "language",
                    "models",
                    "to",
                    "obtain",
                    "contextualized",
                    "embeddings."
                ],
                [
                    "#REF",
                    "initialized",
                    "encoders",
                    "with",
                    "BERT",
                    "#TARGET_REF",
                    "and",
                    "employed",
                    "variational",
                    "inference",
                    "for",
                    "estimation",
                    "and",
                    "leveraged",
                    "contextualized",
                    "word",
                    "embeddings",
                    "for",
                    "improved",
                    "performance."
                ],
                [
                    "#REF",
                    "employed",
                    "a",
                    "novel",
                    "spanbased",
                    "encoding",
                    "scheme",
                    "to",
                    "fine-tune",
                    "a",
                    "pre-trained",
                    "English",
                    "encoder-decoder",
                    "system",
                    "to",
                    "generate",
                    "definitions."
                ],
                [
                    "#REF",
                    "leveraged",
                    "the",
                    "T5",
                    "#REF",
                    "model",
                    "for",
                    "this",
                    "task",
                    "and",
                    "introduced",
                    "a",
                    "re-ranking",
                    "mechanism",
                    "to",
                    "model",
                    "specificity",
                    "in",
                    "definitions."
                ]
            ],
            "context": [
                0,
                0,
                3,
                3,
                3,
                0,
                0
            ]
        },
        "input": "sent0: Various studies attempted to generate multiple different definitions for polysemous words.\n sent1: #REF tackled this problem by computing the AdaGram vectors #REF of input words, which are capable of learning different representations at desired semantic resolutions.\n sent2: However, generating different definitions based on contexts, i.e., example sentences, became the mainstream method #REF ).\n sent3: Among them, some studies used pre-trained language models to obtain contextualized embeddings.\n sent4: #REF initialized encoders with BERT #TARGET_REF and employed variational inference for estimation and leveraged contextualized word embeddings for improved performance.\n sent5: #REF employed a novel spanbased encoding scheme to fine-tune a pre-trained English encoder-decoder system to generate definitions.\n sent6: #REF leveraged the T5 #REF model for this task and introduced a re-ranking mechanism to model specificity in definitions.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent2\", \"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "For",
                    "the",
                    "Transformer",
                    "#TARGET_REF",
                    ",",
                    "we",
                    "conducted",
                    "a",
                    "model",
                    "architecture",
                    "and",
                    "hyperparameter",
                    "search",
                    "(layers,",
                    "learning",
                    "rate,",
                    "MLP",
                    "dimensions,",
                    "dropout",
                    "rate,",
                    "Encoder",
                    "vs.",
                    "Encoder-Decoder)",
                    "on",
                    "the",
                    "dev",
                    "set."
                ],
                [
                    "The",
                    "selected",
                    "model",
                    "was",
                    "composed",
                    "of",
                    "four",
                    "encoder-blocks",
                    "and",
                    "a",
                    "final",
                    "dense",
                    "layer",
                    "that",
                    "projects",
                    "the",
                    "output",
                    "of",
                    "the",
                    "last",
                    "encoder-block",
                    "onto",
                    "the",
                    "PoS",
                    "tags",
                    "via",
                    "a",
                    "softmax",
                    "function."
                ],
                [
                    "We",
                    "used",
                    "the",
                    "Adam",
                    "optimiser",
                    "and",
                    "early",
                    "stopping."
                ],
                [
                    "The",
                    "implementation",
                    "is",
                    "based",
                    "on",
                    "the",
                    "WMT",
                    "example",
                    "2",
                    "of",
                    "Google's",
                    "novel",
                    "ML",
                    "frameworks",
                    "Flax/Jax."
                ],
                [
                    "Table",
                    "2",
                    "lists",
                    "the",
                    "selected",
                    "hyperparameters."
                ],
                [
                    "The",
                    "Transformer",
                    "received",
                    "EEG",
                    "channels",
                    "x",
                    "time",
                    "points",
                    "as",
                    "inputs",
                    "and",
                    "provided",
                    "a",
                    "classification",
                    "response",
                    "for",
                    "the",
                    "entire",
                    "time",
                    "window."
                ]
            ],
            "context": [
                2,
                1,
                2,
                0,
                0,
                3
            ]
        },
        "input": "sent0: For the Transformer #TARGET_REF , we conducted a model architecture and hyperparameter search (layers, learning rate, MLP dimensions, dropout rate, Encoder vs. Encoder-Decoder) on the dev set.\n sent1: The selected model was composed of four encoder-blocks and a final dense layer that projects the output of the last encoder-block onto the PoS tags via a softmax function.\n sent2: We used the Adam optimiser and early stopping.\n sent3: The implementation is based on the WMT example 2 of Google's novel ML frameworks Flax/Jax.\n sent4: Table 2 lists the selected hyperparameters.\n sent5: The Transformer received EEG channels x time points as inputs and provided a classification response for the entire time window.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [\"sent0\", \"sent2\"], \"BACKGROUND\": [\"sent5\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Augmented",
                    "Training",
                    "Dataset:",
                    "Another",
                    "explored",
                    "solution",
                    "for",
                    "the",
                    "unbalance",
                    "in",
                    "subtask",
                    "1a",
                    "consists",
                    "in",
                    "augmenting",
                    "the",
                    "poorly",
                    "represented",
                    "class",
                    "(the",
                    "positive",
                    "class)."
                ],
                [
                    "We",
                    "leverage",
                    "the",
                    "predefined",
                    "augmentation",
                    "approaches",
                    "integrated",
                    "into",
                    "the",
                    "Tex-tAttack",
                    "library",
                    "#REF",
                    "."
                ],
                [
                    "New",
                    "positive",
                    "examples",
                    "are",
                    "generated",
                    "by",
                    "char",
                    "swapping,",
                    "by",
                    "replacing",
                    "words",
                    "with",
                    "synonyms",
                    "from",
                    "the",
                    "Word-Net",
                    "thesaurus",
                    "#REF",
                    ",",
                    "and",
                    "by",
                    "using",
                    "methods",
                    "from",
                    "the",
                    "CheckList",
                    "testing",
                    "-i.e.,",
                    "transformations",
                    "like",
                    "location",
                    "replacement",
                    "or",
                    "number",
                    "alteration",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Five",
                    "positive",
                    "examples",
                    "are",
                    "automatically",
                    "added",
                    "for",
                    "each",
                    "initial",
                    "positive",
                    "sample,",
                    "thus",
                    "increasing",
                    "the",
                    "proportion",
                    "of",
                    "the",
                    "poorly",
                    "represented",
                    "class",
                    "from",
                    "7%",
                    "to",
                    "almost",
                    "45%."
                ]
            ],
            "context": [
                0,
                0,
                3,
                0
            ]
        },
        "input": "sent0: Augmented Training Dataset: Another explored solution for the unbalance in subtask 1a consists in augmenting the poorly represented class (the positive class).\n sent1: We leverage the predefined augmentation approaches integrated into the Tex-tAttack library #REF .\n sent2: New positive examples are generated by char swapping, by replacing words with synonyms from the Word-Net thesaurus #REF , and by using methods from the CheckList testing -i.e., transformations like location replacement or number alteration #TARGET_REF .\n sent3: Five positive examples are automatically added for each initial positive sample, thus increasing the proportion of the poorly represented class from 7% to almost 45%.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "As",
                    "the",
                    "popularity",
                    "of",
                    "multi-player",
                    "online",
                    "games",
                    "has",
                    "grown,",
                    "the",
                    "phenomenon",
                    "of",
                    "in-game",
                    "toxic",
                    "behavior",
                    "has",
                    "taken",
                    "root",
                    "within",
                    "them."
                ],
                [
                    "Toxic",
                    "behavior",
                    "is",
                    "strongly",
                    "present",
                    "in",
                    "recent",
                    "online",
                    "games",
                    "and",
                    "is",
                    "problematic",
                    "to",
                    "the",
                    "gaming",
                    "industry",
                    "#REF",
                    "."
                ],
                [
                    "For",
                    "instance,",
                    "74%",
                    "of",
                    "US",
                    "players",
                    "of",
                    "such",
                    "games",
                    "report",
                    "harassment",
                    "with",
                    "65%",
                    "experiencing",
                    "severe",
                    "harassment."
                ],
                [
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                3,
                2,
                1,
                1
            ]
        },
        "input": "sent0: As the popularity of multi-player online games has grown, the phenomenon of in-game toxic behavior has taken root within them.\n sent1: Toxic behavior is strongly present in recent online games and is problematic to the gaming industry #REF .\n sent2: For instance, 74% of US players of such games report harassment with 65% experiencing severe harassment.\n sent3: #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [\"sent2\", \"sent3\"], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "the",
                    "past",
                    "few",
                    "years,",
                    "Natural",
                    "Language",
                    "Processing",
                    "(NLP)",
                    "researchers",
                    "have",
                    "proposed",
                    "several",
                    "online",
                    "game/community",
                    "toxicity",
                    "analysis",
                    "frameworks",
                    "Figure",
                    "1:",
                    "An",
                    "example",
                    "intent/slot",
                    "annotation",
                    "from",
                    "the",
                    "CONDA",
                    "(CONtextual",
                    "Dual-Annotated)",
                    "dataset."
                ],
                [
                    "#TARGET_REF",
                    "and",
                    "datasets",
                    "#REF",
                    "."
                ],
                [
                    "However,",
                    "existing",
                    "datasets",
                    "(1)",
                    "focus",
                    "only",
                    "on",
                    "the",
                    "single",
                    "utterance",
                    "level",
                    "without",
                    "deeper",
                    "understanding",
                    "of",
                    "context",
                    "in",
                    "the",
                    "whole",
                    "conversation/chat,",
                    "and",
                    "(2)",
                    "do",
                    "not",
                    "explicitly",
                    "use",
                    "semantic",
                    "clues",
                    "from",
                    "the",
                    "words",
                    "within",
                    "the",
                    "utterance."
                ]
            ],
            "context": [
                3,
                0,
                0
            ]
        },
        "input": "sent0: In the past few years, Natural Language Processing (NLP) researchers have proposed several online game/community toxicity analysis frameworks Figure 1: An example intent/slot annotation from the CONDA (CONtextual Dual-Annotated) dataset.\n sent1: #TARGET_REF and datasets #REF .\n sent2: However, existing datasets (1) focus only on the single utterance level without deeper understanding of context in the whole conversation/chat, and (2) do not explicitly use semantic clues from the words within the utterance.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "A",
                    "bilingual",
                    "parallel",
                    "corpus",
                    "#REF",
                    "was",
                    "used",
                    "for",
                    "automatic",
                    "evaluation",
                    "using",
                    "BLEU",
                    "metric",
                    "#REF",
                    "."
                ],
                [
                    "The",
                    "results",
                    "are",
                    "presented",
                    "in",
                    "Table",
                    "2."
                ],
                [
                    "The",
                    "values",
                    "are",
                    "quite",
                    "low,",
                    "partly",
                    "due",
                    "to",
                    "reasons",
                    "explained",
                    "in",
                    "(Callison-Burch",
                    "et",
                    "al.,",
                    "partly",
                    "due",
                    "to",
                    "unknown",
                    "words",
                    "in",
                    "test",
                    "corpus."
                ],
                [
                    "Figure",
                    "6",
                    "shows",
                    "results",
                    "of",
                    "evaluation",
                    "of",
                    "translation",
                    "quality",
                    "using",
                    "subjective",
                    "measures",
                    "using",
                    "methodology",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "The",
                    "methodology",
                    "is",
                    "explained",
                    "in",
                    "chapter",
                    "4.1."
                ],
                [
                    "Four",
                    "independent",
                    "evaluators",
                    "(two",
                    "native",
                    "speakers)",
                    "evaluated",
                    "sets",
                    "of",
                    "100",
                    "sentences",
                    "using",
                    "this",
                    "methodology."
                ]
            ],
            "context": [
                3,
                0,
                0,
                1,
                2,
                3
            ]
        },
        "input": "sent0: A bilingual parallel corpus #REF was used for automatic evaluation using BLEU metric #REF .\n sent1: The results are presented in Table 2.\n sent2: The values are quite low, partly due to reasons explained in (Callison-Burch et al., partly due to unknown words in test corpus.\n sent3: Figure 6 shows results of evaluation of translation quality using subjective measures using methodology #TARGET_REF .\n sent4: The methodology is explained in chapter 4.1.\n sent5: Four independent evaluators (two native speakers) evaluated sets of 100 sentences using this methodology.\n",
        "output": "{\"INFORMATION\": [\"sent3\"], \"PERCEPTION\": [\"sent4\"], \"BACKGROUND\": [\"sent0\", \"sent5\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "i.e."
                ],
                [
                    "we",
                    "prune",
                    "all",
                    "the",
                    "attention",
                    "heads",
                    "of",
                    "particular",
                    "layers."
                ],
                [
                    "When",
                    "all",
                    "the",
                    "self-attention",
                    "heads",
                    "of",
                    "a",
                    "layer",
                    "l",
                    "are",
                    "pruned,",
                    "only",
                    "the",
                    "feed-forward",
                    "network",
                    "of",
                    "that",
                    "layer",
                    "will",
                    "be",
                    "active",
                    "whose",
                    "input",
                    "will",
                    "just",
                    "be",
                    "the",
                    "output",
                    "from",
                    "the",
                    "previous",
                    "layer",
                    "l-1."
                ],
                [
                    "Bottom",
                    "layers",
                    "of",
                    "BERT",
                    "have",
                    "been",
                    "identified",
                    "to",
                    "model",
                    "word",
                    "morphology",
                    "#REF",
                    "and",
                    "are",
                    "considered",
                    "to",
                    "be",
                    "important",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Further,",
                    "recent",
                    "work",
                    "has",
                    "identified",
                    "high",
                    "cosine-similarity",
                    "between",
                    "output",
                    "vectors",
                    "of",
                    "the",
                    "top",
                    "layers,",
                    "indicating",
                    "reduced",
                    "importance",
                    "of",
                    "top",
                    "layers",
                    "#REF",
                    "."
                ],
                [
                    "We",
                    "relate",
                    "these",
                    "studies",
                    "to",
                    "pruning",
                    "by",
                    "comparing",
                    "the",
                    "pruning",
                    "of",
                    "the",
                    "same",
                    "number",
                    "of",
                    "top",
                    "and",
                    "bottom",
                    "layers",
                    "(rows",
                    "2-9",
                    "in",
                    "Table",
                    "5)."
                ],
                [
                    "Amongst",
                    "the",
                    "four",
                    "cases,",
                    "two",
                    "cases",
                    "each",
                    "favor",
                    "pruning",
                    "top",
                    "layers",
                    "and",
                    "bottom",
                    "layers,",
                    "revealing",
                    "no",
                    "preference",
                    "in",
                    "pruning."
                ]
            ],
            "context": [
                0,
                2,
                3,
                1,
                0,
                2,
                2
            ]
        },
        "input": "sent0: i.e.\n sent1: we prune all the attention heads of particular layers.\n sent2: When all the self-attention heads of a layer l are pruned, only the feed-forward network of that layer will be active whose input will just be the output from the previous layer l-1.\n sent3: Bottom layers of BERT have been identified to model word morphology #REF and are considered to be important #TARGET_REF .\n sent4: Further, recent work has identified high cosine-similarity between output vectors of the top layers, indicating reduced importance of top layers #REF .\n sent5: We relate these studies to pruning by comparing the pruning of the same number of top and bottom layers (rows 2-9 in Table 5).\n sent6: Amongst the four cases, two cases each favor pruning top layers and bottom layers, revealing no preference in pruning.\n",
        "output": "{\"INFORMATION\": [\"sent3\"], \"PERCEPTION\": [\"sent1\", \"sent5\", \"sent6\"], \"BACKGROUND\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "To",
                    "take",
                    "advantage",
                    "of",
                    "the",
                    "potential",
                    "lexical",
                    "sharing",
                    "of",
                    "the",
                    "languages",
                    "(e.g."
                ],
                [
                    "loanwords)",
                    "and",
                    "address",
                    "the",
                    "polysynthetic",
                    "nature",
                    "of",
                    "the",
                    "indigenous",
                    "languages,",
                    "we",
                    "trained",
                    "a",
                    "unique",
                    "multilingual",
                    "segmentation",
                    "model",
                    "by",
                    "sampling",
                    "all",
                    "languages",
                    "with",
                    "a",
                    "uniform",
                    "distribution."
                ],
                [
                    "We",
                    "used",
                    "the",
                    "unigram",
                    "model",
                    "implementation",
                    "in",
                    "SentencePiece",
                    "#TARGET_REF",
                    "with",
                    "a",
                    "vocabulary",
                    "size",
                    "of",
                    "32,000."
                ]
            ],
            "context": [
                0,
                0,
                1
            ]
        },
        "input": "sent0: To take advantage of the potential lexical sharing of the languages (e.g.\n sent1: loanwords) and address the polysynthetic nature of the indigenous languages, we trained a unique multilingual segmentation model by sampling all languages with a uniform distribution.\n sent2: We used the unigram model implementation in SentencePiece #TARGET_REF with a vocabulary size of 32,000.\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "A",
                    "related",
                    "literature",
                    "in",
                    "corpus",
                    "linguistics",
                    "and",
                    "NLP",
                    "has",
                    "explored",
                    "the",
                    "nature",
                    "of",
                    "restricted",
                    "binary",
                    "word",
                    "co-occurrences,",
                    "called",
                    "collocations",
                    "(for",
                    "recent",
                    "examples,",
                    "see",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "This",
                    "work",
                    "focuses",
                    "narrowly",
                    "on",
                    "the",
                    "estimation",
                    "of",
                    "bilexical",
                    "conditional",
                    "probabilities,",
                    "which",
                    "are",
                    "often",
                    "inputs",
                    "to",
                    "models",
                    "for",
                    "collocation",
                    "detection."
                ]
            ],
            "context": [
                3,
                0
            ]
        },
        "input": "sent0: A related literature in corpus linguistics and NLP has explored the nature of restricted binary word co-occurrences, called collocations (for recent examples, see #TARGET_REF .\n sent1: This work focuses narrowly on the estimation of bilexical conditional probabilities, which are often inputs to models for collocation detection.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "An",
                    "already",
                    "trained",
                    "and",
                    "tested",
                    "POS",
                    "tagger",
                    "#REF",
                    "was",
                    "available",
                    "for",
                    "Slovenian",
                    "language."
                ],
                [
                    "Words",
                    "were",
                    "tagged",
                    "using",
                    "full",
                    "MSD",
                    "descriptions",
                    "#TARGET_REF",
                    "and",
                    "grouped",
                    "into",
                    "classes",
                    "with",
                    "same",
                    "descriptions",
                    "(words",
                    "that",
                    "had",
                    "the",
                    "same",
                    "POS",
                    "tag",
                    "were",
                    "grouped",
                    "together)."
                ],
                [
                    "This",
                    "process",
                    "produced",
                    "312",
                    "classes",
                    "in",
                    "Slovene",
                    "and",
                    "274",
                    "classes",
                    "in",
                    "Serbian",
                    "language,",
                    "see",
                    "Table",
                    "1",
                    "for",
                    "details."
                ],
                [
                    "A",
                    "linguist",
                    "manually",
                    "tagged",
                    "the",
                    "classes",
                    "to",
                    "paradigms."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: An already trained and tested POS tagger #REF was available for Slovenian language.\n sent1: Words were tagged using full MSD descriptions #TARGET_REF and grouped into classes with same descriptions (words that had the same POS tag were grouped together).\n sent2: This process produced 312 classes in Slovene and 274 classes in Serbian language, see Table 1 for details.\n sent3: A linguist manually tagged the classes to paradigms.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Levin",
                    "verb",
                    "classes",
                    "are",
                    "based",
                    "on",
                    "the",
                    "ability",
                    "of",
                    "a",
                    "verb",
                    "to",
                    "occur",
                    "or",
                    "not",
                    "occur",
                    "in",
                    "pairs",
                    "of",
                    "syntactic",
                    "frames",
                    "that",
                    "are",
                    "in",
                    "some",
                    "sense",
                    "meaning",
                    "preserving,",
                    "hence",
                    "the",
                    "term",
                    "diathesis",
                    "alternations",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "The",
                    "distribution",
                    "of",
                    "syntactic",
                    "frames",
                    "a",
                    "verb",
                    "can",
                    "appear",
                    "in",
                    "determines",
                    "its",
                    "class",
                    "membership."
                ],
                [
                    "The",
                    "fundamental",
                    "assumption",
                    "is",
                    "that",
                    "the",
                    "syntactic",
                    "frames",
                    "are",
                    "a",
                    "direct",
                    "reflection",
                    "of",
                    "the",
                    "underlying",
                    "semantics."
                ],
                [
                    "Levin",
                    "classes",
                    "are",
                    "supposed",
                    "to",
                    "provide",
                    "very",
                    "specific",
                    "sets",
                    "of",
                    "syntactic",
                    "frames",
                    "that",
                    "are",
                    "associated",
                    "with",
                    "the",
                    "individual",
                    "classes."
                ]
            ],
            "context": [
                3,
                2,
                2,
                2
            ]
        },
        "input": "sent0: Levin verb classes are based on the ability of a verb to occur or not occur in pairs of syntactic frames that are in some sense meaning preserving, hence the term diathesis alternations #TARGET_REF .\n sent1: The distribution of syntactic frames a verb can appear in determines its class membership.\n sent2: The fundamental assumption is that the syntactic frames are a direct reflection of the underlying semantics.\n sent3: Levin classes are supposed to provide very specific sets of syntactic frames that are associated with the individual classes.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\", \"sent2\", \"sent3\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "All",
                    "methods",
                    "and",
                    "materials",
                    "discussed",
                    "in",
                    "this",
                    "paper",
                    "were",
                    "tested",
                    "on",
                    "a",
                    "fully",
                    "functional",
                    "machine",
                    "translation",
                    "system",
                    "based",
                    "on",
                    "Apertium",
                    "#REF",
                    "and",
                    "#TARGET_REF",
                    ",",
                    "an",
                    "opensource",
                    "RBMT",
                    "toolkit."
                ]
            ],
            "context": [
                2
            ]
        },
        "input": "sent0: All methods and materials discussed in this paper were tested on a fully functional machine translation system based on Apertium #REF and #TARGET_REF , an opensource RBMT toolkit.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "amongst",
                    "the",
                    "connected",
                    "users."
                ],
                [
                    "#REF",
                    "further",
                    "expanded",
                    "on",
                    "this",
                    "work",
                    "by",
                    "adding",
                    "tweet",
                    "nodes",
                    "to",
                    "the",
                    "social",
                    "graph",
                    "of",
                    "#REF",
                    "alongside",
                    "user",
                    "nodes."
                ],
                [
                    "They",
                    "connected",
                    "every",
                    "tweet",
                    "node",
                    "to",
                    "the",
                    "corresponding",
                    "user",
                    "who",
                    "posted",
                    "the",
                    "tweet."
                ],
                [
                    "They",
                    "then",
                    "used",
                    "a",
                    "graph",
                    "convolutional",
                    "network",
                    "#REF",
                    "to",
                    "create",
                    "profiles",
                    "of",
                    "users",
                    "that",
                    "now",
                    "captured",
                    "their",
                    "linguistic",
                    "behavior",
                    "too."
                ],
                [
                    "When",
                    "they",
                    "used",
                    "these",
                    "profiles",
                    "together",
                    "with",
                    "the",
                    "linguistic",
                    "representations",
                    "of",
                    "tweets,",
                    "F",
                    "1",
                    "scores",
                    "on",
                    "the",
                    "racism",
                    "and",
                    "sexism",
                    "classes",
                    "further",
                    "improved",
                    "to",
                    "79.49%",
                    "and",
                    "84.44%",
                    "respectively."
                ],
                [
                    "#TARGET_REF",
                    "also",
                    "applied",
                    "graph",
                    "neural",
                    "networks,",
                    "Graph-Sage",
                    "#REF",
                    ",",
                    "to",
                    "their",
                    "social",
                    "graph",
                    "of",
                    "approximately",
                    "100k",
                    "Twitter",
                    "users",
                    "to",
                    "generate",
                    "profiles",
                    "that",
                    "they",
                    "used",
                    "to",
                    "classify",
                    "the",
                    "users",
                    "as",
                    "hate-ful",
                    "or",
                    "normal."
                ],
                [
                    "They",
                    "noted",
                    "that",
                    "their",
                    "social",
                    "graph",
                    "based",
                    "method",
                    "outperformed",
                    "traditional",
                    "gradientboosted",
                    "decision",
                    "tree",
                    "classifiers",
                    "by",
                    "15",
                    "F",
                    "1",
                    "points",
                    "on",
                    "the",
                    "same",
                    "task."
                ],
                [
                    "Tredici",
                    "et",
                    "al."
                ],
                [
                    "(",
                    "2019)",
                    "constructed",
                    "a",
                    "graph",
                    "of",
                    "users",
                    "whose",
                    "tweets",
                    "are",
                    "in",
                    "the",
                    "hate-speech",
                    "dataset",
                    "of",
                    "#REF",
                    "."
                ],
                [
                    "Nodes",
                    "were",
                    "uses",
                    "and",
                    "edges",
                    "between",
                    "them",
                    "signified",
                    "that",
                    "one",
                    "user",
                    "retweeted",
                    "the",
                    "other."
                ],
                [
                    "They",
                    "used",
                    "Graph",
                    "Attention",
                    "Networks",
                    "#REF",
                    "to",
                    "generate",
                    "representations",
                    "of",
                    "users",
                    "from",
                    "this",
                    "graph,",
                    "which",
                    "when",
                    "used",
                    "alongside",
                    "linguistic",
                    "representations,",
                    "provided",
                    "a",
                    "gain",
                    "of",
                    "5",
                    "F",
                    "1",
                    "points."
                ],
                [
                    "Cecillon",
                    "et",
                    "al."
                ],
                [
                    "(",
                    "2021)",
                    "worked",
                    "with",
                    "a",
                    "social",
                    "graph",
                    "of",
                    "users",
                    "from",
                    "a",
                    "French",
                    "gaming",
                    "website",
                    "where",
                    "weighted",
                    "edges",
                    "represented",
                    "the",
                    "intensity",
                    "of",
                    "communication",
                    "between",
                    "the",
                    "users."
                ],
                [
                    "Then",
                    "for",
                    "each",
                    "comment",
                    "to",
                    "be",
                    "classified,",
                    "the",
                    "researchers",
                    "extracted",
                    "the",
                    "ego-graph",
                    "of",
                    "its",
                    "author",
                    "and",
                    "created",
                    "a",
                    "feature",
                    "vector",
                    "for",
                    "the",
                    "comment",
                    "from",
                    "the",
                    "ego-graph",
                    "using",
                    "node2vec",
                    "along",
                    "with",
                    "measures",
                    "like",
                    "degree",
                    "centrality."
                ],
                [
                    "An",
                    "SVM",
                    "trained",
                    "with",
                    "these",
                    "graph-based",
                    "feature",
                    "vectors",
                    "reached",
                    "89",
                    "F",
                    "1",
                    "points",
                    "as",
                    "opposed",
                    "to",
                    "81",
                    "F",
                    "1",
                    "points",
                    "when",
                    "trained",
                    "with",
                    "content",
                    "features."
                ]
            ],
            "context": [
                0,
                3,
                3,
                3,
                0,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: amongst the connected users.\n sent1: #REF further expanded on this work by adding tweet nodes to the social graph of #REF alongside user nodes.\n sent2: They connected every tweet node to the corresponding user who posted the tweet.\n sent3: They then used a graph convolutional network #REF to create profiles of users that now captured their linguistic behavior too.\n sent4: When they used these profiles together with the linguistic representations of tweets, F 1 scores on the racism and sexism classes further improved to 79.49% and 84.44% respectively.\n sent5: #TARGET_REF also applied graph neural networks, Graph-Sage #REF , to their social graph of approximately 100k Twitter users to generate profiles that they used to classify the users as hate-ful or normal.\n sent6: They noted that their social graph based method outperformed traditional gradientboosted decision tree classifiers by 15 F 1 points on the same task.\n sent7: Tredici et al.\n sent8: ( 2019) constructed a graph of users whose tweets are in the hate-speech dataset of #REF .\n sent9: Nodes were uses and edges between them signified that one user retweeted the other.\n sent10: They used Graph Attention Networks #REF to generate representations of users from this graph, which when used alongside linguistic representations, provided a gain of 5 F 1 points.\n sent11: Cecillon et al.\n sent12: ( 2021) worked with a social graph of users from a French gaming website where weighted edges represented the intensity of communication between the users.\n sent13: Then for each comment to be classified, the researchers extracted the ego-graph of its author and created a feature vector for the comment from the ego-graph using node2vec along with measures like degree centrality.\n sent14: An SVM trained with these graph-based feature vectors reached 89 F 1 points as opposed to 81 F 1 points when trained with content features.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent5\"], \"BACKGROUND\": [\"sent1\", \"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "To",
                    "solve",
                    "this",
                    "dilemma",
                    "and",
                    "bridge",
                    "the",
                    "gap",
                    "between",
                    "practical",
                    "needs",
                    "for",
                    "simple",
                    "definitions",
                    "and",
                    "current",
                    "trivial",
                    "definition",
                    "generation",
                    "systems,",
                    "we",
                    "present",
                    "a",
                    "novel",
                    "method",
                    "for",
                    "the",
                    "SDG",
                    "task."
                ],
                [
                    "As",
                    "illustrated",
                    "in",
                    "Figure",
                    "2,",
                    "our",
                    "method",
                    "leverages",
                    "a",
                    "multitasking",
                    "framework",
                    "SimpDefiner",
                    "to",
                    "generate",
                    "simple",
                    "definitions",
                    "by",
                    "performing",
                    "three",
                    "sub-tasks",
                    "at",
                    "the",
                    "same",
                    "time,",
                    "which",
                    "are",
                    "definition",
                    "generation,",
                    "text",
                    "reconstruction,",
                    "and",
                    "language",
                    "modeling",
                    "tasks."
                ],
                [
                    "The",
                    "framework",
                    "consists",
                    "of",
                    "a",
                    "fully",
                    "shared",
                    "encoder",
                    "and",
                    "two",
                    "partially",
                    "shared",
                    "decoders."
                ],
                [
                    "We",
                    "disentangle",
                    "the",
                    "complexity",
                    "factors",
                    "from",
                    "the",
                    "text",
                    "by",
                    "designing",
                    "a",
                    "parameter",
                    "sharing",
                    "scheme."
                ],
                [
                    "Particularly,",
                    "we",
                    "share",
                    "parameters",
                    "in",
                    "Complexity-Dependent",
                    "Layer",
                    "Normalization",
                    "and",
                    "Complexity-Dependent",
                    "Query",
                    "Projection",
                    "of",
                    "the",
                    "transformer",
                    "architecture",
                    "#TARGET_REF",
                    "to",
                    "control",
                    "the",
                    "complexity",
                    "(Section",
                    "3.3)."
                ],
                [
                    "Through",
                    "joint",
                    "learning",
                    "and",
                    "sharing",
                    "parameters",
                    "between",
                    "the",
                    "decoders,",
                    "the",
                    "SimpDefiner",
                    "is",
                    "able",
                    "to",
                    "generate",
                    "complex",
                    "and",
                    "simple",
                    "definitions",
                    "simultaneously."
                ]
            ],
            "context": [
                0,
                0,
                0,
                2,
                2,
                0
            ]
        },
        "input": "sent0: To solve this dilemma and bridge the gap between practical needs for simple definitions and current trivial definition generation systems, we present a novel method for the SDG task.\n sent1: As illustrated in Figure 2, our method leverages a multitasking framework SimpDefiner to generate simple definitions by performing three sub-tasks at the same time, which are definition generation, text reconstruction, and language modeling tasks.\n sent2: The framework consists of a fully shared encoder and two partially shared decoders.\n sent3: We disentangle the complexity factors from the text by designing a parameter sharing scheme.\n sent4: Particularly, we share parameters in Complexity-Dependent Layer Normalization and Complexity-Dependent Query Projection of the transformer architecture #TARGET_REF to control the complexity (Section 3.3).\n sent5: Through joint learning and sharing parameters between the decoders, the SimpDefiner is able to generate complex and simple definitions simultaneously.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent3\", \"sent4\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "(1a)",
                    "is",
                    "the",
                    "combination",
                    "of",
                    "a",
                    "stem",
                    "clear",
                    "and",
                    "a",
                    "prefix",
                    "un."
                ],
                [
                    "This",
                    "combination",
                    "results",
                    "in",
                    "the",
                    "concatenation",
                    "of",
                    "the",
                    "two",
                    "forms",
                    "and",
                    "the",
                    "compositional",
                    "combination",
                    "of",
                    "syntactic",
                    "and",
                    "semantic",
                    "properties."
                ],
                [
                    "In",
                    "(1a)",
                    "the",
                    "result",
                    "is",
                    "a",
                    "form",
                    "unclear",
                    "with",
                    "the",
                    "syntactic",
                    "category",
                    "Adjective",
                    "and",
                    "the",
                    "meaning",
                    "of",
                    "an",
                    "antonym",
                    "of",
                    "clear."
                ],
                [
                    "The",
                    "examples",
                    "in",
                    "(2)",
                    "require",
                    "additional",
                    "rules",
                    "to",
                    "modify",
                    "the",
                    "stem."
                ],
                [
                    "In",
                    "(2a),",
                    "stem",
                    "vowel",
                    "change",
                    "is",
                    "the",
                    "only",
                    "thing",
                    "marking",
                    "the",
                    "difference",
                    "between",
                    "the",
                    "two",
                    "words."
                ],
                [
                    "The",
                    "alternative",
                    "model",
                    "is",
                    "called",
                    "Item",
                    "&amp,",
                    "Process",
                    "(IP)."
                ],
                [
                    "In",
                    "this",
                    "model,",
                    "word",
                    "formation",
                    "rules",
                    "are",
                    "processes",
                    "applying",
                    "to",
                    "a",
                    "base."
                ],
                [
                    "In",
                    "(1a),",
                    "the",
                    "process",
                    "adds",
                    "un",
                    "to",
                    "the",
                    "left",
                    "of",
                    "the",
                    "stem",
                    "clear."
                ],
                [
                    "In",
                    "(2a),",
                    "the",
                    "process",
                    "changes",
                    "the",
                    "stem",
                    "vowel",
                    "of",
                    "sing."
                ],
                [
                    "In",
                    "IP",
                    "there",
                    "are",
                    "no",
                    "morphemes",
                    "but",
                    "only",
                    "lexemes",
                    "and",
                    "processes."
                ],
                [
                    "In",
                    "modern",
                    "morphological",
                    "theories",
                    "both",
                    "are",
                    "represented,",
                    "e.g."
                ],
                [
                    "#REF",
                    "for",
                    "IA",
                    "and",
                    "#REF",
                    "for",
                    "IP."
                ],
                [
                    "An",
                    "important",
                    "difference",
                    "for",
                    "our",
                    "purposes",
                    "is",
                    "that",
                    "in",
                    "IA",
                    "we",
                    "have",
                    "a",
                    "tree",
                    "structure",
                    "whereas",
                    "in",
                    "IP",
                    "we",
                    "have",
                    "a",
                    "derivation",
                    "history."
                ],
                [
                    "A",
                    "tree",
                    "structure",
                    "represents",
                    "the",
                    "relationship",
                    "between",
                    "morphemes,",
                    "e.g."
                ],
                [
                    "(3)."
                ],
                [
                    "A",
                    "derivation",
                    "history",
                    "lists",
                    "the",
                    "different",
                    "stages",
                    "rules",
                    "applying,",
                    "e.g."
                ],
                [
                    "(",
                    "4)."
                ],
                [
                    "It",
                    "should",
                    "be",
                    "noted",
                    "that",
                    "there",
                    "are",
                    "many",
                    "variants",
                    "of",
                    "IA",
                    "and",
                    "IP."
                ],
                [
                    "The",
                    "reason",
                    "we",
                    "are",
                    "interested",
                    "in",
                    "word",
                    "formation",
                    "rules",
                    "is",
                    "their",
                    "productivity."
                ],
                [
                    "Productivity",
                    "is",
                    "a",
                    "difficult",
                    "and",
                    "controversial",
                    "concept,",
                    "cf."
                ],
                [
                    "#REF",
                    "."
                ],
                [
                    "Basically,",
                    "a",
                    "productive",
                    "word",
                    "formation",
                    "rule",
                    "can",
                    "be",
                    "used",
                    "to",
                    "produce",
                    "new",
                    "lexical",
                    "items."
                ],
                [
                    "When",
                    "a",
                    "speaker",
                    "has",
                    "a",
                    "productive",
                    "word",
                    "formation",
                    "rule",
                    "at",
                    "her",
                    "disposal,",
                    "she",
                    "can",
                    "use",
                    "a",
                    "word",
                    "not",
                    "in",
                    "her",
                    "mental",
                    "lexicon",
                    "and",
                    "be",
                    "understood",
                    "as",
                    "far",
                    "as",
                    "other",
                    "speakers",
                    "have",
                    "the",
                    "same",
                    "word",
                    "formation",
                    "rule",
                    "available."
                ],
                [
                    "The",
                    "productivity",
                    "of",
                    "word",
                    "formation",
                    "makes",
                    "it",
                    "impossible",
                    "to",
                    "cover",
                    "the",
                    "entire",
                    "lexicon",
                    "in",
                    "a",
                    "finite",
                    "list."
                ]
            ],
            "context": [
                2,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: (1a) is the combination of a stem clear and a prefix un.\n sent1: This combination results in the concatenation of the two forms and the compositional combination of syntactic and semantic properties.\n sent2: In (1a) the result is a form unclear with the syntactic category Adjective and the meaning of an antonym of clear.\n sent3: The examples in (2) require additional rules to modify the stem.\n sent4: In (2a), stem vowel change is the only thing marking the difference between the two words.\n sent5: The alternative model is called Item &amp, Process (IP).\n sent6: In this model, word formation rules are processes applying to a base.\n sent7: In (1a), the process adds un to the left of the stem clear.\n sent8: In (2a), the process changes the stem vowel of sing.\n sent9: In IP there are no morphemes but only lexemes and processes.\n sent10: In modern morphological theories both are represented, e.g.\n sent11: #REF for IA and #REF for IP.\n sent12: An important difference for our purposes is that in IA we have a tree structure whereas in IP we have a derivation history.\n sent13: A tree structure represents the relationship between morphemes, e.g.\n sent14: (3).\n sent15: A derivation history lists the different stages rules applying, e.g.\n sent16: ( 4).\n sent17: It should be noted that there are many variants of IA and IP.\n sent18: The reason we are interested in word formation rules is their productivity.\n sent19: Productivity is a difficult and controversial concept, cf.\n sent20: #REF .\n sent21: Basically, a productive word formation rule can be used to produce new lexical items.\n sent22: When a speaker has a productive word formation rule at her disposal, she can use a word not in her mental lexicon and be understood as far as other speakers have the same word formation rule available.\n sent23: The productivity of word formation makes it impossible to cover the entire lexicon in a finite list.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\", \"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "language",
                    "features,",
                    "which",
                    "can",
                    "be",
                    "fine-tuned",
                    "for",
                    "applications",
                    "like",
                    "semantic",
                    "analysis",
                    "and",
                    "question",
                    "answering."
                ],
                [
                    "Multi-lingual-BERT",
                    "(mBERT)",
                    "is",
                    "a",
                    "BERT",
                    "model",
                    "pre-trained",
                    "using",
                    "the",
                    "Wikipedia",
                    "text",
                    "corpus",
                    "#TARGET_REF",
                    "in",
                    "more",
                    "than",
                    "100",
                    "languages",
                    "around",
                    "the",
                    "world."
                ],
                [
                    "XLM-RoBERTa",
                    "#REF",
                    "scaled",
                    "this",
                    "idea",
                    "with",
                    "more",
                    "than",
                    "2",
                    "terabytes",
                    "of",
                    "common",
                    "crawl",
                    "data."
                ]
            ],
            "context": [
                0,
                1,
                0
            ]
        },
        "input": "sent0: language features, which can be fine-tuned for applications like semantic analysis and question answering.\n sent1: Multi-lingual-BERT (mBERT) is a BERT model pre-trained using the Wikipedia text corpus #TARGET_REF in more than 100 languages around the world.\n sent2: XLM-RoBERTa #REF scaled this idea with more than 2 terabytes of common crawl data.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Construction",
                    "with",
                    "masked",
                    "language",
                    "model."
                ],
                [
                    "We",
                    "construct",
                    "neighborhood",
                    "sentences",
                    "from",
                    "x",
                    "0",
                    "by",
                    "substituting",
                    "at",
                    "most",
                    "k",
                    "tokens."
                ],
                [
                    "As",
                    "shown",
                    "in",
                    "Algorithm",
                    "1,",
                    "the",
                    "construction",
                    "employs",
                    "a",
                    "recursive",
                    "approach",
                    "and",
                    "replaces",
                    "one",
                    "token",
                    "at",
                    "a",
                    "time."
                ],
                [
                    "For",
                    "each",
                    "recursion,",
                    "the",
                    "algorithm",
                    "first",
                    "masks",
                    "each",
                    "token",
                    "of",
                    "the",
                    "input",
                    "sentence",
                    "(may",
                    "be",
                    "the",
                    "original",
                    "x",
                    "0",
                    "or",
                    "the",
                    "x",
                    "from",
                    "last",
                    "recursion)",
                    "separately",
                    "and",
                    "predicts",
                    "likely",
                    "replacements",
                    "with",
                    "a",
                    "masked",
                    "language",
                    "model",
                    "(e.g.,",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "To",
                    "ensure",
                    "the",
                    "naturalness,",
                    "we",
                    "keep",
                    "the",
                    "top",
                    "20",
                    "tokens",
                    "for",
                    "each",
                    "mask",
                    "with",
                    "the",
                    "largest",
                    "logit",
                    "(subject",
                    "to",
                    "a",
                    "threshold,",
                    "Line",
                    "9)."
                ],
                [
                    "Then,",
                    "the",
                    "algorithm",
                    "constructs",
                    "neighborhood",
                    "sentences",
                    "by",
                    "replacing",
                    "the",
                    "mask",
                    "with",
                    "found",
                    "tokens."
                ],
                [
                    "We",
                    "use",
                    "the",
                    "notation",
                    "x",
                    "in",
                    "the",
                    "following",
                    "sections",
                    "to",
                    "denote",
                    "the",
                    "constructed",
                    "sentences",
                    "within",
                    "the",
                    "neighborhood."
                ],
                [
                    "L",
                    "←",
                    "SortDecreasing(L),",
                    "9",
                    "lmin",
                    "←",
                    "max{L",
                    "(κ)",
                    ",",
                    "L",
                    "(0)",
                    "−",
                    "δ},"
                ]
            ],
            "context": [
                0,
                0,
                0,
                3,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Construction with masked language model.\n sent1: We construct neighborhood sentences from x 0 by substituting at most k tokens.\n sent2: As shown in Algorithm 1, the construction employs a recursive approach and replaces one token at a time.\n sent3: For each recursion, the algorithm first masks each token of the input sentence (may be the original x 0 or the x from last recursion) separately and predicts likely replacements with a masked language model (e.g., #TARGET_REF .\n sent4: To ensure the naturalness, we keep the top 20 tokens for each mask with the largest logit (subject to a threshold, Line 9).\n sent5: Then, the algorithm constructs neighborhood sentences by replacing the mask with found tokens.\n sent6: We use the notation x in the following sections to denote the constructed sentences within the neighborhood.\n sent7: L ← SortDecreasing(L), 9 lmin ← max{L (κ) , L (0) − δ},\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "modeled",
                    "toxic",
                    "comments",
                    "detection",
                    "as",
                    "a",
                    "heterogeneous",
                    "network",
                    "since",
                    "this",
                    "network",
                    "type",
                    "contains",
                    "abundant",
                    "information",
                    "with",
                    "structural",
                    "relations",
                    "(edges)",
                    "among",
                    "multi-typed",
                    "nodes",
                    "as",
                    "well",
                    "as",
                    "unstructured",
                    "content",
                    "associated",
                    "with",
                    "each",
                    "node",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Graph",
                    "structures",
                    "have",
                    "been",
                    "used",
                    "for",
                    "several",
                    "tasks,",
                    "such",
                    "as:",
                    "topic",
                    "model,",
                    "name",
                    "disambiguation,",
                    "scientific",
                    "impact",
                    "measurement,",
                    "and",
                    "others,",
                    "obtaining",
                    "good",
                    "results",
                    "#REF",
                    "."
                ],
                [
                    "We",
                    "defined",
                    "a",
                    "undirected",
                    "and",
                    "weighted",
                    "graph",
                    "as",
                    "G",
                    "=",
                    "(V,",
                    "E,",
                    "W",
                    "),",
                    "where",
                    "V",
                    "is",
                    "a",
                    "set",
                    "of",
                    "vertices",
                    "V",
                    "=",
                    "{v",
                    "1",
                    ",",
                    "...,",
                    "v",
                    "n",
                    "},",
                    "E",
                    "indicates",
                    "a",
                    "set",
                    "of",
                    "edges",
                    "E",
                    "=",
                    "{e",
                    "1",
                    ",",
                    "...,",
                    "e",
                    "n",
                    "},",
                    "and",
                    "W",
                    "is",
                    "a",
                    "weighted",
                    "adjacency",
                    "matrix,",
                    "in",
                    "which",
                    "W",
                    "i,j",
                    "denotes",
                    "the",
                    "weight",
                    "of",
                    "an",
                    "edge",
                    "between",
                    "nodes",
                    "i",
                    "and",
                    "j."
                ],
                [
                    "We",
                    "defined",
                    "two",
                    "node",
                    "types:",
                    "token",
                    "and",
                    "sentence",
                    "and",
                    "two",
                    "constraints",
                    "not",
                    "allowing",
                    "link",
                    "among",
                    "tokens",
                    "nodes",
                    "or",
                    "among",
                    "sentences",
                    "nodes."
                ]
            ],
            "context": [
                1,
                0,
                0,
                0
            ]
        },
        "input": "sent0: We modeled toxic comments detection as a heterogeneous network since this network type contains abundant information with structural relations (edges) among multi-typed nodes as well as unstructured content associated with each node #TARGET_REF .\n sent1: Graph structures have been used for several tasks, such as: topic model, name disambiguation, scientific impact measurement, and others, obtaining good results #REF .\n sent2: We defined a undirected and weighted graph as G = (V, E, W ), where V is a set of vertices V = {v 1 , ..., v n }, E indicates a set of edges E = {e 1 , ..., e n }, and W is a weighted adjacency matrix, in which W i,j denotes the weight of an edge between nodes i and j.\n sent3: We defined two node types: token and sentence and two constraints not allowing link among tokens nodes or among sentences nodes.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "backbone",
                    "of",
                    "our",
                    "PyTorch",
                    "#TARGET_REF",
                    "implementation",
                    "is",
                    "the",
                    "Transformer",
                    "and",
                    "WordpieceTokenizer",
                    "classes",
                    "offered",
                    "by",
                    "Hugging",
                    "Face",
                    "#REF",
                    "."
                ],
                [
                    "We",
                    "use",
                    "pre-trained",
                    "BERT",
                    "models",
                    "provided",
                    "on",
                    "huggingface.co:",
                    "bert-base-cased,",
                    "dbmz/bert-base-german-cased,",
                    "dbmz/bert-base-italian-cased,",
                    "and",
                    "Geotrend/BERT-base-nl-cased",
                    "#REF",
                    ",",
                    "keeping",
                    "their",
                    "default",
                    "configuration."
                ],
                [
                    "The",
                    "only",
                    "hyperparameters",
                    "we",
                    "choose",
                    "ourselves",
                    "are",
                    "the",
                    "batch",
                    "size",
                    "(",
                    "24),",
                    "the",
                    "learning",
                    "rate,",
                    "and",
                    "the",
                    "number",
                    "of",
                    "epochs."
                ],
                [
                    "We",
                    "used",
                    "the",
                    "Adam",
                    "optimizer",
                    "to",
                    "train",
                    "all",
                    "the",
                    "parameters",
                    "in",
                    "our",
                    "model",
                    "including",
                    "the",
                    "pretrained",
                    "BERT."
                ],
                [
                    "To",
                    "ensure",
                    "stability",
                    "and",
                    "avoid",
                    "overfitting,",
                    "we",
                    "used",
                    "a",
                    "linear",
                    "scheduler",
                    "with",
                    "no",
                    "warm-up",
                    "step,",
                    "which",
                    "gradually",
                    "reduces",
                    "the",
                    "learning",
                    "rate",
                    "from",
                    "0.0015",
                    "to",
                    "0",
                    "for",
                    "each",
                    "training",
                    "iteration."
                ],
                [
                    "During",
                    "preliminary",
                    "experiments",
                    "on",
                    "the",
                    "development",
                    "set,",
                    "we",
                    "found",
                    "that",
                    "training",
                    "loss",
                    "barely",
                    "changed",
                    "after",
                    "five",
                    "epochs."
                ]
            ],
            "context": [
                2,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: The backbone of our PyTorch #TARGET_REF implementation is the Transformer and WordpieceTokenizer classes offered by Hugging Face #REF .\n sent1: We use pre-trained BERT models provided on huggingface.co: bert-base-cased, dbmz/bert-base-german-cased, dbmz/bert-base-italian-cased, and Geotrend/BERT-base-nl-cased #REF , keeping their default configuration.\n sent2: The only hyperparameters we choose ourselves are the batch size ( 24), the learning rate, and the number of epochs.\n sent3: We used the Adam optimizer to train all the parameters in our model including the pretrained BERT.\n sent4: To ensure stability and avoid overfitting, we used a linear scheduler with no warm-up step, which gradually reduces the learning rate from 0.0015 to 0 for each training iteration.\n sent5: During preliminary experiments on the development set, we found that training loss barely changed after five epochs.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "original",
                    "paper",
                    "introducing",
                    "BERTScore",
                    "#TARGET_REF",
                    "naturally",
                    "compared",
                    "BERTScore's",
                    "correlations",
                    "with",
                    "human",
                    "judgments",
                    "to",
                    "that",
                    "of",
                    "other",
                    "metrics."
                ],
                [
                    "However,",
                    "various",
                    "other",
                    "surveys",
                    "of",
                    "MT",
                    "metrics,",
                    "as",
                    "well",
                    "as",
                    "datasets",
                    "and",
                    "methodologies",
                    "have",
                    "been",
                    "conducted,",
                    "offering",
                    "insights",
                    "into",
                    "how",
                    "MT",
                    "system",
                    "and",
                    "metric",
                    "performance",
                    "should",
                    "be",
                    "measured."
                ]
            ],
            "context": [
                1,
                0
            ]
        },
        "input": "sent0: The original paper introducing BERTScore #TARGET_REF naturally compared BERTScore's correlations with human judgments to that of other metrics.\n sent1: However, various other surveys of MT metrics, as well as datasets and methodologies have been conducted, offering insights into how MT system and metric performance should be measured.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Words'",
                    "semantics",
                    "have",
                    "a",
                    "dynamic",
                    "nature",
                    "which",
                    "depends",
                    "on",
                    "the",
                    "surrounding",
                    "context",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Therefore,",
                    "the",
                    "majority",
                    "of",
                    "words",
                    "tends",
                    "to",
                    "be",
                    "polysemous",
                    "(i.e."
                ],
                [
                    "have",
                    "multiple",
                    "senses)."
                ],
                [
                    "For",
                    "few",
                    "examples,",
                    "words",
                    "such",
                    "as",
                    "\"cell\",",
                    "\"bank\"",
                    "and",
                    "\"report\"",
                    "can",
                    "be",
                    "mentioned."
                ],
                [
                    "Due",
                    "to",
                    "this",
                    "nature",
                    "in",
                    "natural",
                    "language,",
                    "it",
                    "is",
                    "important",
                    "to",
                    "focus",
                    "on",
                    "word-in-context",
                    "sense",
                    "while",
                    "extracting",
                    "the",
                    "meaning",
                    "of",
                    "a",
                    "word",
                    "which",
                    "appeared",
                    "in",
                    "a",
                    "text",
                    "segment."
                ],
                [
                    "Also,",
                    "this",
                    "is",
                    "a",
                    "critical",
                    "requirement",
                    "to",
                    "many",
                    "applications",
                    "such",
                    "as",
                    "question",
                    "answering,",
                    "document",
                    "summarisation,",
                    "information",
                    "retrieval",
                    "and",
                    "information",
                    "extraction."
                ]
            ],
            "context": [
                1,
                3,
                3,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Words' semantics have a dynamic nature which depends on the surrounding context #TARGET_REF .\n sent1: Therefore, the majority of words tends to be polysemous (i.e.\n sent2: have multiple senses).\n sent3: For few examples, words such as \"cell\", \"bank\" and \"report\" can be mentioned.\n sent4: Due to this nature in natural language, it is important to focus on word-in-context sense while extracting the meaning of a word which appeared in a text segment.\n sent5: Also, this is a critical requirement to many applications such as question answering, document summarisation, information retrieval and information extraction.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "At",
                    "the",
                    "answer",
                    "level",
                    "the",
                    "top",
                    "candidate",
                    "sentences",
                    "(up",
                    "to",
                    "10)",
                    "returned",
                    "by",
                    "our",
                    "system",
                    "were",
                    "compared",
                    "against",
                    "the",
                    "review",
                    "snippets",
                    "as",
                    "the",
                    "gold",
                    "standard."
                ],
                [
                    "The",
                    "review",
                    "snippets",
                    "were",
                    "top",
                    "review",
                    "sentences",
                    "returned",
                    "by",
                    "the",
                    "system",
                    "used",
                    "by",
                    "#TARGET_REF",
                    "#REF",
                    "Average",
                    "ROUGE",
                    "scores",
                    "are",
                    "reported",
                    "in",
                    "Table",
                    "2."
                ],
                [
                    "Both",
                    "systems",
                    "aim",
                    "at",
                    "providing",
                    "the",
                    "best",
                    "candidate",
                    "sentences."
                ],
                [
                    "Looking",
                    "at",
                    "the",
                    "precision",
                    "scores,",
                    "it",
                    "is",
                    "clear",
                    "that",
                    "our",
                    "system",
                    "performance",
                    "is",
                    "good",
                    "in",
                    "terms",
                    "of",
                    "returning",
                    "relevant",
                    "sentences,",
                    "similar",
                    "in",
                    "content",
                    "to",
                    "the",
                    "gold",
                    "standard."
                ],
                [
                    "The",
                    "sim",
                    "method",
                    "still",
                    "is",
                    "the",
                    "best",
                    "performing",
                    "method."
                ],
                [
                    "We",
                    "say",
                    "this",
                    "because,",
                    "ROUGE-L",
                    "looks",
                    "for",
                    "the",
                    "longest",
                    "common",
                    "sub",
                    "se-",
                    "Looking",
                    "at",
                    "the",
                    "similarity",
                    "scores,",
                    "it",
                    "is",
                    "clear",
                    "that",
                    "the",
                    "candidate",
                    "sentences",
                    "returned",
                    "by",
                    "our",
                    "system",
                    "is",
                    "almost",
                    "exactly",
                    "similar",
                    "to",
                    "the",
                    "sentences",
                    "returned",
                    "by",
                    "#REF",
                    "."
                ],
                [
                    "Once",
                    "again",
                    "our",
                    "system",
                    "is",
                    "able",
                    "to",
                    "perform",
                    "on",
                    "par",
                    "with",
                    "a",
                    "more",
                    "complicated",
                    "system."
                ]
            ],
            "context": [
                2,
                3,
                2,
                2,
                0,
                0,
                0
            ]
        },
        "input": "sent0: At the answer level the top candidate sentences (up to 10) returned by our system were compared against the review snippets as the gold standard.\n sent1: The review snippets were top review sentences returned by the system used by #TARGET_REF #REF Average ROUGE scores are reported in Table 2.\n sent2: Both systems aim at providing the best candidate sentences.\n sent3: Looking at the precision scores, it is clear that our system performance is good in terms of returning relevant sentences, similar in content to the gold standard.\n sent4: The sim method still is the best performing method.\n sent5: We say this because, ROUGE-L looks for the longest common sub se- Looking at the similarity scores, it is clear that the candidate sentences returned by our system is almost exactly similar to the sentences returned by #REF .\n sent6: Once again our system is able to perform on par with a more complicated system.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\", \"sent2\", \"sent3\"], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "addition,",
                    "in",
                    "order",
                    "to",
                    "have",
                    "both",
                    "the",
                    "high",
                    "performance",
                    "of",
                    "post-norm",
                    "and",
                    "the",
                    "stable",
                    "training",
                    "of",
                    "pre-norm",
                    "(Nguyen",
                    "and",
                    "Salazar,",
                    "2019),",
                    "we",
                    "use",
                    "the",
                    "methods",
                    "mentioned",
                    "in",
                    "DeepNet",
                    "#TARGET_REF",
                    "For",
                    "training",
                    "the",
                    "full-sentence",
                    "translation",
                    "model,",
                    "given",
                    "the",
                    "source",
                    "sentence",
                    "x,",
                    "the",
                    "probability",
                    "of",
                    "predicting",
                    "the",
                    "target",
                    "sentence",
                    "y",
                    "is",
                    "as",
                    "shown",
                    "in",
                    "Eq."
                ],
                [
                    "1,",
                    "and",
                    "the",
                    "training",
                    "objective",
                    "is",
                    "to",
                    "minimize",
                    "the",
                    "negative",
                    "log-likelihood",
                    "as",
                    "shown",
                    "in",
                    "Eq."
                ],
                [
                    "2.p(y|x)",
                    "=",
                    "|y|",
                    "t=1",
                    "p(y",
                    "t",
                    "|x,",
                    "y",
                    "&lt,t",
                    ",",
                    "θ)(1)loss",
                    "f",
                    "ull",
                    "(θ)",
                    "=",
                    "−",
                    "(x,y)∈D",
                    "logp",
                    "g",
                    "(y|x,",
                    "θ)",
                    "(2)The",
                    "batch",
                    "size",
                    "for",
                    "training",
                    "is",
                    "4,096",
                    "tokens",
                    "per",
                    "GPU,",
                    "and",
                    "we",
                    "trained",
                    "our",
                    "model",
                    "for",
                    "7",
                    "epochs",
                    "on",
                    "4",
                    "NVIDIA",
                    "V100",
                    "GPUs",
                    "for",
                    "about",
                    "10",
                    "hours."
                ]
            ],
            "context": [
                0,
                0,
                0
            ]
        },
        "input": "sent0: In addition, in order to have both the high performance of post-norm and the stable training of pre-norm (Nguyen and Salazar, 2019), we use the methods mentioned in DeepNet #TARGET_REF For training the full-sentence translation model, given the source sentence x, the probability of predicting the target sentence y is as shown in Eq.\n sent1: 1, and the training objective is to minimize the negative log-likelihood as shown in Eq.\n sent2: 2.p(y|x) = |y| t=1 p(y t |x, y &lt,t , θ)(1)loss f ull (θ) = − (x,y)∈D logp g (y|x, θ) (2)The batch size for training is 4,096 tokens per GPU, and we trained our model for 7 epochs on 4 NVIDIA V100 GPUs for about 10 hours.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Upon",
                    "processing",
                    "the",
                    "first",
                    "word",
                    "'John'",
                    "in",
                    "the",
                    "sentence",
                    "s1,",
                    "C-JOHN",
                    "is",
                    "activated",
                    "so",
                    "that",
                    "C-JOHN",
                    "gets",
                    "an",
                    "A-MARKER",
                    "and",
                    "a",
                    "CI",
                    "JOHN#l",
                    "is",
                    "created",
                    "under",
                    "C-JOHN."
                ],
                [
                    "At",
                    "this",
                    "point,",
                    "the",
                    "corresponding",
                    "Japanese",
                    "lexical",
                    "item",
                    "is",
                    "searched",
                    "for,",
                    "and",
                    "JON",
                    "is",
                    "found."
                ],
                [
                    "A",
                    "G-MARKER",
                    "is",
                    "created",
                    "on",
                    "JON."
                ],
                [
                    "The",
                    "A-MARKER",
                    "and",
                    "G-MARKER",
                    "propagate",
                    "up",
                    "through",
                    "ISA",
                    "links",
                    "(activating",
                    "C-MALE-PERSON",
                    "and",
                    "C-PERSON",
                    "in",
                    "sequence)",
                    "and,",
                    "then,",
                    "ROLE",
                    "links."
                ],
                [
                    "When",
                    "an",
                    "A-MARKER",
                    "collides",
                    "with",
                    "a",
                    "P-MARKER",
                    "at",
                    "a",
                    "CSE,",
                    "the",
                    "associated",
                    "case",
                    "role",
                    "is",
                    "bound",
                    "with",
                    "the",
                    "source",
                    "of",
                    "the",
                    "A-MARKER",
                    "and",
                    "the",
                    "prediction",
                    "is",
                    "updated",
                    "by",
                    "passing",
                    "P-MARKER",
                    "to",
                    "the",
                    "next",
                    "CSE."
                ],
                [
                    "This",
                    "P-MARKER",
                    "is",
                    "passed",
                    "down",
                    "ISA",
                    "links."
                ],
                [
                    "In",
                    "this",
                    "memory",
                    "network,",
                    "the",
                    "ACTOR",
                    "roles",
                    "of",
                    "concept",
                    "sequences",
                    "WANT-CIRCUM-E",
                    "is",
                    "bound",
                    "to",
                    "JOHN#",
                    "1",
                    "pointed",
                    "by",
                    "the",
                    "A-MARKER."
                ],
                [
                    "This",
                    "is",
                    "made",
                    "possible",
                    "in",
                    "the",
                    "SNAP",
                    "architecture",
                    "which",
                    "allows",
                    "markers",
                    "to",
                    "carry",
                    "address",
                    "as",
                    "well",
                    "as",
                    "bit-vectors",
                    "and",
                    "values,",
                    "where",
                    "many",
                    "other",
                    "marker-passing",
                    "machines",
                    "such",
                    "as",
                    "NETL",
                    "#TARGET_REF",
                    "and",
                    "IXM2",
                    "#REF",
                    "only",
                    "allow",
                    "bit-vectors",
                    "to",
                    "be",
                    "passed",
                    "around,",
                    "Also,",
                    "G-MARKERs",
                    "are",
                    "placed",
                    "on",
                    "the",
                    "ACTOR",
                    "role",
                    "CSE",
                    "of",
                    "WANT-CIRCUM-J."
                ],
                [
                    "The",
                    "G-MARKER",
                    "points",
                    "to",
                    "the",
                    "Japanese",
                    "lexical",
                    "item",
                    "'jon'."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                0
            ]
        },
        "input": "sent0: Upon processing the first word 'John' in the sentence s1, C-JOHN is activated so that C-JOHN gets an A-MARKER and a CI JOHN#l is created under C-JOHN.\n sent1: At this point, the corresponding Japanese lexical item is searched for, and JON is found.\n sent2: A G-MARKER is created on JON.\n sent3: The A-MARKER and G-MARKER propagate up through ISA links (activating C-MALE-PERSON and C-PERSON in sequence) and, then, ROLE links.\n sent4: When an A-MARKER collides with a P-MARKER at a CSE, the associated case role is bound with the source of the A-MARKER and the prediction is updated by passing P-MARKER to the next CSE.\n sent5: This P-MARKER is passed down ISA links.\n sent6: In this memory network, the ACTOR roles of concept sequences WANT-CIRCUM-E is bound to JOHN# 1 pointed by the A-MARKER.\n sent7: This is made possible in the SNAP architecture which allows markers to carry address as well as bit-vectors and values, where many other marker-passing machines such as NETL #TARGET_REF and IXM2 #REF only allow bit-vectors to be passed around, Also, G-MARKERs are placed on the ACTOR role CSE of WANT-CIRCUM-J.\n sent8: The G-MARKER points to the Japanese lexical item 'jon'.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent7\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "addition",
                    "to",
                    "Levin",
                    "classes",
                    "like",
                    "cut",
                    "whose",
                    "members",
                    "have",
                    "core",
                    "senses",
                    "that",
                    "are",
                    "closely",
                    "and",
                    "systematically",
                    "related",
                    "in",
                    "the",
                    "WordNet",
                    "hierarchy,",
                    "other",
                    "Levin",
                    "classes",
                    "are",
                    "composed",
                    "of",
                    "verbs",
                    "that",
                    "exhibit",
                    "a",
                    "wider",
                    "range",
                    "of",
                    "possible",
                    "semantic",
                    "components."
                ],
                [
                    "The",
                    "split",
                    "verbs",
                    "(blow,",
                    "break,",
                    "cut,",
                    "draw,",
                    "hack,",
                    "hew,",
                    "kick,",
                    "knock,",
                    "pry,",
                    "pull,",
                    "push,",
                    "rip,",
                    "roll,",
                    "saw,",
                    "shove,",
                    "slip,",
                    "split,",
                    "tear,",
                    "tug,",
                    "yank)",
                    "do",
                    "not",
                    "obviously",
                    "form",
                    "a",
                    "tight",
                    "semantic",
                    "class."
                ],
                [
                    "Instead,",
                    "in",
                    "their",
                    "use",
                    "as",
                    "split",
                    "verbs,",
                    "each",
                    "verb",
                    "manifests",
                    "an",
                    "extended",
                    "sense",
                    "that",
                    "can",
                    "be",
                    "paraphrased",
                    "as",
                    "\"separate",
                    "by",
                    "V-ing,\"",
                    "where",
                    "\"V\"",
                    "is",
                    "the",
                    "basic",
                    "meaning",
                    "of",
                    "that",
                    "verb",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Many",
                    "of",
                    "the",
                    "verbs",
                    "(e.g.,",
                    "draw,",
                    "pull,",
                    "push,",
                    "shove,",
                    "tug,",
                    "yank)",
                    "that",
                    "do",
                    "not",
                    "have",
                    "an",
                    "inherent",
                    "semantic",
                    "component",
                    "of",
                    "\"separating\"",
                    "belong",
                    "to",
                    "this",
                    "class",
                    "because",
                    "of",
                    "the",
                    "component",
                    "of",
                    "force",
                    "in",
                    "their",
                    "meaning."
                ],
                [
                    "They",
                    "are",
                    "interpretable",
                    "as",
                    "verbs",
                    "of",
                    "splitting",
                    "or",
                    "separating",
                    "only",
                    "in",
                    "particular",
                    "syntactic",
                    "frames."
                ],
                [
                    "The",
                    "adjunction",
                    "of",
                    "the",
                    "apart",
                    "adverb",
                    "adds",
                    "a",
                    "change",
                    "of",
                    "state",
                    "semantic",
                    "component",
                    "with",
                    "respect",
                    "to",
                    "the",
                    "object",
                    "which",
                    "is",
                    "not",
                    "present",
                    "otherwise."
                ]
            ],
            "context": [
                0,
                3,
                1,
                0,
                0,
                0
            ]
        },
        "input": "sent0: In addition to Levin classes like cut whose members have core senses that are closely and systematically related in the WordNet hierarchy, other Levin classes are composed of verbs that exhibit a wider range of possible semantic components.\n sent1: The split verbs (blow, break, cut, draw, hack, hew, kick, knock, pry, pull, push, rip, roll, saw, shove, slip, split, tear, tug, yank) do not obviously form a tight semantic class.\n sent2: Instead, in their use as split verbs, each verb manifests an extended sense that can be paraphrased as \"separate by V-ing,\" where \"V\" is the basic meaning of that verb #TARGET_REF .\n sent3: Many of the verbs (e.g., draw, pull, push, shove, tug, yank) that do not have an inherent semantic component of \"separating\" belong to this class because of the component of force in their meaning.\n sent4: They are interpretable as verbs of splitting or separating only in particular syntactic frames.\n sent5: The adjunction of the apart adverb adds a change of state semantic component with respect to the object which is not present otherwise.\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "more",
                    "negatives,",
                    "we",
                    "propose",
                    "to",
                    "use",
                    "cross-batch",
                    "negatives",
                    "when",
                    "training",
                    "on",
                    "multiple",
                    "GPUs,",
                    "as",
                    "illustrated",
                    "at",
                    "the",
                    "bottom",
                    "of",
                    "Figure",
                    "2."
                ],
                [
                    "Specifically,",
                    "we",
                    "first",
                    "compute",
                    "the",
                    "passage",
                    "embeddings",
                    "within",
                    "each",
                    "single",
                    "GPU,",
                    "and",
                    "then",
                    "share",
                    "these",
                    "passage",
                    "embeddings",
                    "among",
                    "all",
                    "the",
                    "GPUs."
                ],
                [
                    "Besides",
                    "the",
                    "in-batch",
                    "negatives,",
                    "we",
                    "collect",
                    "all",
                    "passages",
                    "(i.e.,",
                    "their",
                    "dense",
                    "representations)",
                    "from",
                    "other",
                    "GPUs",
                    "as",
                    "the",
                    "additional",
                    "negatives",
                    "for",
                    "each",
                    "question."
                ],
                [
                    "Hence,",
                    "with",
                    "A",
                    "GPUs",
                    "(or",
                    "mini-batches)",
                    "2",
                    ",",
                    "we",
                    "can",
                    "indeed",
                    "obtain",
                    "A×B",
                    "−1",
                    "negatives",
                    "for",
                    "a",
                    "given",
                    "question,",
                    "which",
                    "is",
                    "approximately",
                    "A",
                    "times",
                    "as",
                    "many",
                    "as",
                    "the",
                    "original",
                    "number",
                    "of",
                    "in-batch",
                    "negatives."
                ],
                [
                    "In",
                    "this",
                    "way,",
                    "we",
                    "can",
                    "use",
                    "more",
                    "negatives",
                    "in",
                    "the",
                    "training",
                    "objective",
                    "of",
                    "Equation",
                    "2,",
                    "so",
                    "that",
                    "the",
                    "results",
                    "are",
                    "expected",
                    "to",
                    "be",
                    "improved."
                ],
                [
                    "Denoised",
                    "Hard",
                    "Negatives",
                    "Although",
                    "the",
                    "above",
                    "strategy",
                    "can",
                    "increase",
                    "the",
                    "number",
                    "of",
                    "negatives,",
                    "most",
                    "of",
                    "negatives",
                    "are",
                    "easy",
                    "ones,",
                    "which",
                    "can",
                    "be",
                    "easily",
                    "discriminated."
                ],
                [
                    "While,",
                    "hard",
                    "negatives",
                    "are",
                    "shown",
                    "to",
                    "be",
                    "important",
                    "to",
                    "train",
                    "a",
                    "dual-encoder",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "To",
                    "obtain",
                    "hard",
                    "negatives,",
                    "a",
                    "straightforward",
                    "method",
                    "is",
                    "to",
                    "select",
                    "the",
                    "top-ranked",
                    "passages",
                    "(excluding",
                    "the",
                    "labeled",
                    "positive",
                    "passages)",
                    "as",
                    "negative",
                    "samples."
                ],
                [
                    "However,",
                    "it",
                    "is",
                    "likely",
                    "to",
                    "bring",
                    "false",
                    "negatives",
                    "(i.e.,",
                    "unlabeled",
                    "positives),",
                    "since",
                    "the",
                    "annotators",
                    "can",
                    "only",
                    "annotate",
                    "a",
                    "few",
                    "top-retrieved",
                    "passages",
                    "(as",
                    "discussed",
                    "in",
                    "Section",
                    "1)."
                ],
                [
                    "Another",
                    "note",
                    "is",
                    "that",
                    "previous",
                    "work",
                    "mainly",
                    "focuses",
                    "on",
                    "factoid",
                    "questions,",
                    "to",
                    "which",
                    "the",
                    "answers",
                    "are",
                    "short",
                    "and",
                    "concise."
                ],
                [
                    "Hence,",
                    "it",
                    "is",
                    "not",
                    "challenging",
                    "to",
                    "filter",
                    "false",
                    "negatives",
                    "by",
                    "using",
                    "the",
                    "short",
                    "answers",
                    "#REF",
                    "."
                ],
                [
                    "However,",
                    "it",
                    "cannot",
                    "apply",
                    "to",
                    "non-factoid",
                    "questions."
                ],
                [
                    "In",
                    "this",
                    "paper,",
                    "we",
                    "aim",
                    "to",
                    "learn",
                    "dense",
                    "passage",
                    "retrieval",
                    "for",
                    "both",
                    "factoid",
                    "questions",
                    "and",
                    "non-factoid",
                    "questions,",
                    "which",
                    "needs",
                    "a",
                    "more",
                    "effective",
                    "way",
                    "for",
                    "denoising",
                    "hard",
                    "negatives."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                1,
                2,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: more negatives, we propose to use cross-batch negatives when training on multiple GPUs, as illustrated at the bottom of Figure 2.\n sent1: Specifically, we first compute the passage embeddings within each single GPU, and then share these passage embeddings among all the GPUs.\n sent2: Besides the in-batch negatives, we collect all passages (i.e., their dense representations) from other GPUs as the additional negatives for each question.\n sent3: Hence, with A GPUs (or mini-batches) 2 , we can indeed obtain A×B −1 negatives for a given question, which is approximately A times as many as the original number of in-batch negatives.\n sent4: In this way, we can use more negatives in the training objective of Equation 2, so that the results are expected to be improved.\n sent5: Denoised Hard Negatives Although the above strategy can increase the number of negatives, most of negatives are easy ones, which can be easily discriminated.\n sent6: While, hard negatives are shown to be important to train a dual-encoder #TARGET_REF .\n sent7: To obtain hard negatives, a straightforward method is to select the top-ranked passages (excluding the labeled positive passages) as negative samples.\n sent8: However, it is likely to bring false negatives (i.e., unlabeled positives), since the annotators can only annotate a few top-retrieved passages (as discussed in Section 1).\n sent9: Another note is that previous work mainly focuses on factoid questions, to which the answers are short and concise.\n sent10: Hence, it is not challenging to filter false negatives by using the short answers #REF .\n sent11: However, it cannot apply to non-factoid questions.\n sent12: In this paper, we aim to learn dense passage retrieval for both factoid questions and non-factoid questions, which needs a more effective way for denoising hard negatives.\n",
        "output": "{\"INFORMATION\": [\"sent6\"], \"PERCEPTION\": [\"sent7\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "•",
                    "MultiRC:",
                    "Multi-Sentence",
                    "Reading",
                    "Comprehension",
                    "set",
                    "#TARGET_REF",
                    "is",
                    "created",
                    "in",
                    "a",
                    "way",
                    "that",
                    "answering",
                    "questions",
                    "requires",
                    "a",
                    "more",
                    "complex",
                    "understanding",
                    "from",
                    "multiple",
                    "sentences."
                ],
                [
                    "Therefore,",
                    "coreference",
                    "reasoning",
                    "can",
                    "be",
                    "one",
                    "of",
                    "the",
                    "sources",
                    "for",
                    "improving",
                    "the",
                    "performance",
                    "on",
                    "this",
                    "dataset."
                ],
                [
                    "The",
                    "Contrast",
                    "set",
                    "and",
                    "MultiRC",
                    "datasets",
                    "are",
                    "not",
                    "designed",
                    "to",
                    "explicitly",
                    "evaluate",
                    "coreference",
                    "reasoning."
                ],
                [
                    "However,",
                    "we",
                    "include",
                    "them",
                    "among",
                    "our",
                    "evaluation",
                    "sets",
                    "to",
                    "have",
                    "a",
                    "broader",
                    "view",
                    "about",
                    "the",
                    "impact",
                    "of",
                    "using",
                    "our",
                    "coreference",
                    "data",
                    "in",
                    "QA."
                ],
                [
                    "6",
                    "as",
                    "we",
                    "use",
                    "it",
                    "for",
                    "investigating",
                    "whether",
                    "the",
                    "resulting",
                    "performance",
                    "changes",
                    "are",
                    "due",
                    "to",
                    "using",
                    "more",
                    "training",
                    "data",
                    "or",
                    "using",
                    "coreference-aware",
                    "additional",
                    "data."
                ]
            ],
            "context": [
                2,
                3,
                0,
                0,
                0
            ]
        },
        "input": "sent0: • MultiRC: Multi-Sentence Reading Comprehension set #TARGET_REF is created in a way that answering questions requires a more complex understanding from multiple sentences.\n sent1: Therefore, coreference reasoning can be one of the sources for improving the performance on this dataset.\n sent2: The Contrast set and MultiRC datasets are not designed to explicitly evaluate coreference reasoning.\n sent3: However, we include them among our evaluation sets to have a broader view about the impact of using our coreference data in QA.\n sent4: 6 as we use it for investigating whether the resulting performance changes are due to using more training data or using coreference-aware additional data.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Objective",
                    "and",
                    "subjective",
                    "evaluation",
                    "methods",
                    "were",
                    "used",
                    "in",
                    "final",
                    "testing",
                    "as",
                    "only",
                    "a",
                    "correct",
                    "mixture",
                    "of",
                    "methods",
                    "minimizes",
                    "evaluation",
                    "bias."
                ],
                [
                    "Translation",
                    "quality",
                    "evaluation",
                    "was",
                    "conducted",
                    "using",
                    "subjective",
                    "evaluation",
                    "methods,",
                    "where",
                    "a",
                    "group",
                    "of",
                    "native",
                    "and",
                    "near-native",
                    "speakers",
                    "scored",
                    "translations."
                ],
                [
                    "Automatic",
                    "objective",
                    "measures",
                    "#REF",
                    "were",
                    "used",
                    "to",
                    "ensure",
                    "wider",
                    "coverage."
                ],
                [
                    "Bilingual",
                    "corpus",
                    "#TARGET_REF",
                    "was",
                    "use",
                    "d",
                    "in",
                    "all",
                    "evaluation",
                    "processes."
                ]
            ],
            "context": [
                0,
                0,
                0,
                2
            ]
        },
        "input": "sent0: Objective and subjective evaluation methods were used in final testing as only a correct mixture of methods minimizes evaluation bias.\n sent1: Translation quality evaluation was conducted using subjective evaluation methods, where a group of native and near-native speakers scored translations.\n sent2: Automatic objective measures #REF were used to ensure wider coverage.\n sent3: Bilingual corpus #TARGET_REF was use d in all evaluation processes.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent3\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "This",
                    "paper",
                    "describes",
                    "the",
                    "system",
                    "developed",
                    "by",
                    "the",
                    "LIUM",
                    "laboratory",
                    "for",
                    "the",
                    "2008",
                    "IWSLT",
                    "evaluation."
                ],
                [
                    "We",
                    "only",
                    "participated",
                    "in",
                    "the",
                    "Arabic/English",
                    "BTEC",
                    "task."
                ],
                [
                    "The",
                    "architecture",
                    "of",
                    "the",
                    "system",
                    "is",
                    "very",
                    "similar",
                    "to",
                    "a",
                    "large",
                    "system",
                    "built",
                    "for",
                    "the",
                    "NIST",
                    "Arabic/English",
                    "task",
                    "#REF",
                    "or",
                    "a",
                    "system",
                    "built",
                    "for",
                    "the",
                    "translation",
                    "between",
                    "French",
                    "and",
                    "English",
                    "#REF",
                    "."
                ],
                [
                    "All",
                    "three",
                    "are",
                    "statistical",
                    "phrase-based",
                    "machine",
                    "translation",
                    "systems",
                    "based",
                    "on",
                    "the",
                    "freely",
                    "available",
                    "Moses",
                    "decoder",
                    "#TARGET_REF",
                    ",",
                    "with",
                    "extensions",
                    "for",
                    "rescoring",
                    "nbest",
                    "lists",
                    "with",
                    "a",
                    "continuous",
                    "space",
                    "language",
                    "model",
                    "in",
                    "a",
                    "second",
                    "pass."
                ],
                [
                    "No",
                    "system",
                    "combination",
                    "is",
                    "used."
                ]
            ],
            "context": [
                3,
                0,
                3,
                2,
                0
            ]
        },
        "input": "sent0: This paper describes the system developed by the LIUM laboratory for the 2008 IWSLT evaluation.\n sent1: We only participated in the Arabic/English BTEC task.\n sent2: The architecture of the system is very similar to a large system built for the NIST Arabic/English task #REF or a system built for the translation between French and English #REF .\n sent3: All three are statistical phrase-based machine translation systems based on the freely available Moses decoder #TARGET_REF , with extensions for rescoring nbest lists with a continuous space language model in a second pass.\n sent4: No system combination is used.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent3\"], \"BACKGROUND\": [\"sent0\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "decoding",
                    "analyses",
                    "used",
                    "linear",
                    "support",
                    "vector",
                    "machines",
                    "(SVM)",
                    "#TARGET_REF",
                    "and",
                    "Transformers,",
                    "which",
                    "can",
                    "capture",
                    "complex",
                    "interactions",
                    "of",
                    "EEG",
                    "data",
                    "across",
                    "time",
                    "points."
                ],
                [
                    "All",
                    "classifiers",
                    "were",
                    "trained",
                    "on",
                    "the",
                    "EEG",
                    "training",
                    "data,",
                    "assessed",
                    "on",
                    "the",
                    "dev",
                    "set",
                    "and",
                    "scored",
                    "on",
                    "the",
                    "independent",
                    "test",
                    "set."
                ],
                [
                    "Hyperparameters",
                    "and",
                    "early",
                    "stopping",
                    "were",
                    "selected",
                    "based",
                    "on",
                    "the",
                    "dev",
                    "set."
                ],
                [
                    "We",
                    "assessed",
                    "linear",
                    "SVMs",
                    "and",
                    "Transformers",
                    "on",
                    "the",
                    "dev",
                    "set",
                    "using",
                    "10",
                    "different",
                    "random",
                    "seed",
                    "points."
                ],
                [
                    "We",
                    "show",
                    "mean",
                    "classification",
                    "accuracy",
                    "with",
                    "68%",
                    "confidence",
                    "intervals",
                    "(CI)",
                    "over",
                    "those",
                    "10",
                    "replications",
                    "on",
                    "the",
                    "dev",
                    "(Table",
                    "4,",
                    "Figure",
                    "3)",
                    "resp."
                ],
                [
                    "test",
                    "set",
                    "(Figure",
                    "2,",
                    "4,",
                    "5)."
                ],
                [
                    "We",
                    "compute",
                    "statistics",
                    "on",
                    "test",
                    "set",
                    "classification",
                    "responses",
                    "from",
                    "the",
                    "model",
                    "that",
                    "scored",
                    "the",
                    "highest",
                    "on",
                    "the",
                    "dev",
                    "set",
                    "(e.g."
                ],
                [
                    "binomial",
                    "or",
                    "Wilcoxon",
                    "signed",
                    "rank",
                    "tests)."
                ]
            ],
            "context": [
                0,
                0,
                0,
                2,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: The decoding analyses used linear support vector machines (SVM) #TARGET_REF and Transformers, which can capture complex interactions of EEG data across time points.\n sent1: All classifiers were trained on the EEG training data, assessed on the dev set and scored on the independent test set.\n sent2: Hyperparameters and early stopping were selected based on the dev set.\n sent3: We assessed linear SVMs and Transformers on the dev set using 10 different random seed points.\n sent4: We show mean classification accuracy with 68% confidence intervals (CI) over those 10 replications on the dev (Table 4, Figure 3) resp.\n sent5: test set (Figure 2, 4, 5).\n sent6: We compute statistics on test set classification responses from the model that scored the highest on the dev set (e.g.\n sent7: binomial or Wilcoxon signed rank tests).\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent3\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "type",
                    "of",
                    "information",
                    "recovered",
                    "by",
                    "the",
                    "NP",
                    "Enrichment",
                    "task",
                    "complements",
                    "well-established",
                    "core",
                    "NLP",
                    "tasks",
                    "such",
                    "as",
                    "entity",
                    "typing,",
                    "entity",
                    "linking,",
                    "coreference",
                    "resolution,",
                    "and",
                    "semantic-role",
                    "labeling",
                    "#REF",
                    "."
                ],
                [
                    "We",
                    "believe",
                    "it",
                    "serves",
                    "as",
                    "an",
                    "important",
                    "and",
                    "much-needed",
                    "building",
                    "block",
                    "for",
                    "downstream",
                    "applications",
                    "that",
                    "require",
                    "text",
                    "understanding,",
                    "including",
                    "information",
                    "retrieval,",
                    "relation",
                    "extraction",
                    "and",
                    "event",
                    "extraction,",
                    "question",
                    "answering,",
                    "and",
                    "so",
                    "on."
                ],
                [
                    "In",
                    "particular,",
                    "the",
                    "NP",
                    "Enrichment",
                    "task",
                    "neatly",
                    "encapsulates",
                    "much",
                    "of",
                    "the",
                    "long-range",
                    "information",
                    "that",
                    "is",
                    "often",
                    "required",
                    "by",
                    "such",
                    "applications."
                ],
                [
                    "Take",
                    "for",
                    "example",
                    "a",
                    "system",
                    "that",
                    "attempts",
                    "to",
                    "extract",
                    "reports",
                    "on",
                    "police",
                    "shooting",
                    "incidents",
                    "#TARGET_REF",
                    ",",
                    "with",
                    "the",
                    "following",
                    "challenging,",
                    "but",
                    "not",
                    "uncommon,",
                    "passage:",
                    "2"
                ]
            ],
            "context": [
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: The type of information recovered by the NP Enrichment task complements well-established core NLP tasks such as entity typing, entity linking, coreference resolution, and semantic-role labeling #REF .\n sent1: We believe it serves as an important and much-needed building block for downstream applications that require text understanding, including information retrieval, relation extraction and event extraction, question answering, and so on.\n sent2: In particular, the NP Enrichment task neatly encapsulates much of the long-range information that is often required by such applications.\n sent3: Take for example a system that attempts to extract reports on police shooting incidents #TARGET_REF , with the following challenging, but not uncommon, passage: 2\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "order",
                    "to",
                    "assess",
                    "the",
                    "contribution",
                    "of",
                    "punctuation",
                    "to",
                    "the",
                    "selection",
                    "of",
                    "the",
                    "correct",
                    "analysis,",
                    "w�",
                    "applied",
                    "the",
                    "same",
                    "trained",
                    "version",
                    "of",
                    "the",
                    "integrated",
                    "grammar",
                    "to",
                    "the",
                    "106",
                    "sentences",
                    "from",
                    "the",
                    "test",
                    "set",
                    "which",
                    "contain",
                    "internal",
                    "punctuation,",
                    "both",
                    "with",
                    "and",
                    "without",
                    "the",
                    "punctuation",
                    "marks",
                    "in",
                    "the",
                    "input."
                ],
                [
                    "A",
                    "comparison",
                    "of",
                    "the",
                    "GEIG",
                    "evaluation",
                    "metrics",
                    "for",
                    "this",
                    "set",
                    "of",
                    "sentences",
                    "punctuated",
                    "and",
                    "unpunctuated",
                    "gives",
                    "a",
                    "measure",
                    "of",
                    "the",
                    "contribution",
                    "of",
                    "punctuation",
                    "to",
                    "parse",
                    "selection",
                    "on",
                    "this",
                    "data."
                ],
                [
                    "(The",
                    "results",
                    "for",
                    "the",
                    "unpunctuated",
                    "set",
                    "were",
                    "computed",
                    "against",
                    "a",
                    "version",
                    "of",
                    "the",
                    "Susanne",
                    "tree",
                    "bank",
                    "from",
                    "which",
                    "punctuation",
                    "had",
                    "also",
                    "been",
                    "�",
                    "emoved.)"
                ],
                [
                    "As",
                    "table",
                    "3",
                    "shows,",
                    "recall",
                    "declines",
                    "by",
                    "10%,",
                    "precision",
                    "by",
                    "5%",
                    "and",
                    "there",
                    "are",
                    "an",
                    "average",
                    "of",
                    "1.27",
                    "more",
                    "crossing",
                    "brackets",
                    "per",
                    "sentence."
                ],
                [
                    "6",
                    "Conclusions",
                    "#TARGET_REF",
                    "and",
                    "#REF",
                    "showed",
                    "that",
                    "the",
                    "LR",
                    "model,",
                    "combined",
                    "with",
                    "a",
                    "gram",
                    "mar",
                    "exploiting",
                    "subcategorisation",
                    "constraints,",
                    "could",
                    "achieve",
                    "good",
                    "parse",
                    "selection",
                    "accuracy",
                    "bu�",
                    "at",
                    "the",
                    "expense",
                    "of",
                    "poor",
                    "coverage",
                    "of",
                    "free",
                    "text."
                ],
                [
                    "The",
                    "results",
                    "reported",
                    "here",
                    "suggest",
                    "that",
                    "improved",
                    "coverage",
                    "of",
                    "heterogeneous",
                    "text",
                    "can",
                    "be",
                    "achieved",
                    "by",
                    "exploiting",
                    "textual",
                    "and",
                    "grammatical",
                    "con",
                    "straints",
                    "on",
                    "PoS",
                    "and",
                    "punctuation",
                    "sequences."
                ],
                [
                    "The",
                    "experiments",
                    "show",
                    "that",
                    "grammatical",
                    "coverage",
                    "can",
                    "be",
                    "greatly",
                    "increased",
                    "by",
                    "relaxing",
                    "subcategorisation",
                    "constraints,",
                    "and",
                    "that",
                    "text",
                    "grammatical",
                    "or",
                    "punctuation-cued",
                    "constraints",
                    "can",
                    "reduce",
                    "ambiguity",
                    "and",
                    "increase",
                    "coverage",
                    "during",
                    "parsing."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                1,
                1,
                1
            ]
        },
        "input": "sent0: In order to assess the contribution of punctuation to the selection of the correct analysis, w� applied the same trained version of the integrated grammar to the 106 sentences from the test set which contain internal punctuation, both with and without the punctuation marks in the input.\n sent1: A comparison of the GEIG evaluation metrics for this set of sentences punctuated and unpunctuated gives a measure of the contribution of punctuation to parse selection on this data.\n sent2: (The results for the unpunctuated set were computed against a version of the Susanne tree bank from which punctuation had also been � emoved.)\n sent3: As table 3 shows, recall declines by 10%, precision by 5% and there are an average of 1.27 more crossing brackets per sentence.\n sent4: 6 Conclusions #TARGET_REF and #REF showed that the LR model, combined with a gram mar exploiting subcategorisation constraints, could achieve good parse selection accuracy bu� at the expense of poor coverage of free text.\n sent5: The results reported here suggest that improved coverage of heterogeneous text can be achieved by exploiting textual and grammatical con straints on PoS and punctuation sequences.\n sent6: The experiments show that grammatical coverage can be greatly increased by relaxing subcategorisation constraints, and that text grammatical or punctuation-cued constraints can reduce ambiguity and increase coverage during parsing.\n",
        "output": "{\"INFORMATION\": [\"sent4\", \"sent5\", \"sent6\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "evaluate",
                    "these",
                    "experiments",
                    "both",
                    "on",
                    "the",
                    "Transformer",
                    "and",
                    "BERT",
                    "models."
                ],
                [
                    "Our",
                    "results",
                    "show",
                    "that",
                    "a",
                    "large",
                    "fraction",
                    "of",
                    "attention",
                    "heads",
                    "can",
                    "be",
                    "pruned",
                    "randomly:",
                    "75%",
                    "of",
                    "the",
                    "attention",
                    "heads",
                    "of",
                    "Transformer",
                    "can",
                    "be",
                    "randomly",
                    "pruned",
                    "with",
                    "a",
                    "drop",
                    "of",
                    "less",
                    "than",
                    "1",
                    "BLEU",
                    "point",
                    "on",
                    "NMT",
                    "tasks."
                ],
                [
                    "Similarly,",
                    "half",
                    "of",
                    "the",
                    "attention",
                    "heads",
                    "of",
                    "BERT",
                    "can",
                    "be",
                    "randomly",
                    "pruned",
                    "with",
                    "an",
                    "average",
                    "drop",
                    "in",
                    "accuracy",
                    "of",
                    "less",
                    "than",
                    "1%",
                    "across",
                    "a",
                    "chosen",
                    "set",
                    "of",
                    "GLUE",
                    "tasks",
                    "1",
                    "."
                ],
                [
                    "Significantly",
                    "for",
                    "Transformers,",
                    "we",
                    "find",
                    "no",
                    "evidence",
                    "for",
                    "pruning",
                    "methods",
                    "preferring",
                    "specific",
                    "attention",
                    "heads",
                    "based",
                    "on",
                    "their",
                    "location,",
                    "even",
                    "when",
                    "the",
                    "locations",
                    "are",
                    "chosen",
                    "to",
                    "match",
                    "attention",
                    "heads",
                    "identified",
                    "to",
                    "be",
                    "more",
                    "important",
                    "in",
                    "existing",
                    "studies."
                ],
                [
                    "Similarly",
                    "on",
                    "the",
                    "BERT",
                    "model,",
                    "pruning",
                    "top",
                    "and",
                    "bottom",
                    "layers",
                    "do",
                    "not",
                    "show",
                    "significant",
                    "difference,",
                    "even",
                    "though",
                    "existing",
                    "studies",
                    "attribute",
                    "higher",
                    "importance",
                    "to",
                    "the",
                    "latter",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "However,",
                    "we",
                    "identify",
                    "a",
                    "preference",
                    "to",
                    "avoid",
                    "pruning",
                    "the",
                    "middle",
                    "layers",
                    "and",
                    "consecutive",
                    "layers."
                ],
                [
                    "Lastly,",
                    "we",
                    "check",
                    "if",
                    "during",
                    "fine-tuning",
                    "certain",
                    "heads",
                    "compensate",
                    "more",
                    "for",
                    "the",
                    "pruned",
                    "heads."
                ],
                [
                    "If",
                    "so,",
                    "such",
                    "heads",
                    "would",
                    "perhaps",
                    "be",
                    "more",
                    "important."
                ],
                [
                    "However,",
                    "we",
                    "find",
                    "no",
                    "such",
                    "evidence."
                ],
                [
                    "In",
                    "particular,",
                    "during",
                    "fine-tuning,",
                    "the",
                    "un-pruned",
                    "heads",
                    "change",
                    "similarly",
                    "across",
                    "most",
                    "pruning",
                    "configurations."
                ],
                [
                    "Overall,",
                    "our",
                    "experiments",
                    "suggest",
                    "that",
                    "interpretation",
                    "of",
                    "attention",
                    "heads",
                    "does",
                    "not",
                    "strongly",
                    "inform",
                    "pruning."
                ],
                [
                    "The",
                    "rest",
                    "of",
                    "the",
                    "paper",
                    "is",
                    "organized",
                    "as",
                    "follows:",
                    "Section",
                    "2",
                    "mentions",
                    "about",
                    "the",
                    "models",
                    "and",
                    "the",
                    "datasets",
                    "used",
                    "for",
                    "this",
                    "work",
                    "followed",
                    "by",
                    "Section",
                    "3",
                    "which",
                    "provides",
                    "details",
                    "of",
                    "the",
                    "experimental",
                    "process."
                ],
                [
                    "This",
                    "section",
                    "reports",
                    "results",
                    "on",
                    "both",
                    "Transformer",
                    "and",
                    "BERT",
                    "models."
                ],
                [
                    "We",
                    "summarize",
                    "our",
                    "work",
                    "in",
                    "Section",
                    "4."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: We evaluate these experiments both on the Transformer and BERT models.\n sent1: Our results show that a large fraction of attention heads can be pruned randomly: 75% of the attention heads of Transformer can be randomly pruned with a drop of less than 1 BLEU point on NMT tasks.\n sent2: Similarly, half of the attention heads of BERT can be randomly pruned with an average drop in accuracy of less than 1% across a chosen set of GLUE tasks 1 .\n sent3: Significantly for Transformers, we find no evidence for pruning methods preferring specific attention heads based on their location, even when the locations are chosen to match attention heads identified to be more important in existing studies.\n sent4: Similarly on the BERT model, pruning top and bottom layers do not show significant difference, even though existing studies attribute higher importance to the latter #TARGET_REF .\n sent5: However, we identify a preference to avoid pruning the middle layers and consecutive layers.\n sent6: Lastly, we check if during fine-tuning certain heads compensate more for the pruned heads.\n sent7: If so, such heads would perhaps be more important.\n sent8: However, we find no such evidence.\n sent9: In particular, during fine-tuning, the un-pruned heads change similarly across most pruning configurations.\n sent10: Overall, our experiments suggest that interpretation of attention heads does not strongly inform pruning.\n sent11: The rest of the paper is organized as follows: Section 2 mentions about the models and the datasets used for this work followed by Section 3 which provides details of the experimental process.\n sent12: This section reports results on both Transformer and BERT models.\n sent13: We summarize our work in Section 4.\n",
        "output": "{\"INFORMATION\": [\"sent4\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Crosslingual",
                    "Since",
                    "there",
                    "were",
                    "no",
                    "training",
                    "data",
                    "available",
                    "for",
                    "cross-lingual",
                    "datasets,",
                    "we",
                    "followed",
                    "a",
                    "zero-shot",
                    "approach",
                    "for",
                    "them."
                ],
                [
                    "Multilingual",
                    "and",
                    "cross-lingual",
                    "transformer",
                    "models",
                    "like",
                    "multilingual",
                    "BERT",
                    "and",
                    "XLM-R",
                    "show",
                    "strong",
                    "cross-lingual",
                    "transfer",
                    "learning",
                    "performance."
                ],
                [
                    "They",
                    "can",
                    "be",
                    "trained",
                    "on",
                    "one",
                    "language,",
                    "typically",
                    "a",
                    "resource-rich",
                    "language",
                    "and",
                    "can",
                    "be",
                    "used",
                    "to",
                    "perform",
                    "inference",
                    "on",
                    "another",
                    "language."
                ],
                [
                    "The",
                    "cross-lingual",
                    "nature",
                    "of",
                    "the",
                    "transformer",
                    "models",
                    "has",
                    "provided",
                    "the",
                    "ability",
                    "to",
                    "do",
                    "this",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Therefore,",
                    "we",
                    "used",
                    "the",
                    "models",
                    "trained",
                    "on",
                    "the",
                    "English-English",
                    "dataset",
                    "to",
                    "get",
                    "predictions",
                    "for",
                    "cross-lingual",
                    "datasets."
                ]
            ],
            "context": [
                0,
                3,
                3,
                1,
                2
            ]
        },
        "input": "sent0: Crosslingual Since there were no training data available for cross-lingual datasets, we followed a zero-shot approach for them.\n sent1: Multilingual and cross-lingual transformer models like multilingual BERT and XLM-R show strong cross-lingual transfer learning performance.\n sent2: They can be trained on one language, typically a resource-rich language and can be used to perform inference on another language.\n sent3: The cross-lingual nature of the transformer models has provided the ability to do this #TARGET_REF .\n sent4: Therefore, we used the models trained on the English-English dataset to get predictions for cross-lingual datasets.\n",
        "output": "{\"INFORMATION\": [\"sent3\"], \"PERCEPTION\": [\"sent4\"], \"BACKGROUND\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Natural",
                    "Question",
                    "(NQ)",
                    "Kwiatkowski",
                    "et",
                    "al."
                ],
                [
                    "(2019)",
                    "introduces",
                    "a",
                    "large",
                    "dataset",
                    "for",
                    "open-domain",
                    "QA."
                ],
                [
                    "The",
                    "original",
                    "dataset",
                    "contains",
                    "more",
                    "than",
                    "300,",
                    "000",
                    "questions",
                    "collected",
                    "from",
                    "Google",
                    "search",
                    "logs."
                ],
                [
                    "In",
                    "#TARGET_REF",
                    ",",
                    "around",
                    "62,",
                    "000",
                    "factoid",
                    "questions",
                    "are",
                    "selected,",
                    "and",
                    "all",
                    "the",
                    "Wikipedia",
                    "articles",
                    "are",
                    "processed",
                    "as",
                    "the",
                    "collection",
                    "of",
                    "passages."
                ],
                [
                    "There",
                    "are",
                    "more",
                    "than",
                    "21",
                    "million",
                    "passages",
                    "in",
                    "the",
                    "corpus."
                ],
                [
                    "In",
                    "our",
                    "experiments,",
                    "we",
                    "reuse",
                    "the",
                    "version",
                    "of",
                    "NQ",
                    "created",
                    "by",
                    "#REF",
                    "."
                ],
                [
                    "Note",
                    "that",
                    "the",
                    "dataset",
                    "used",
                    "in",
                    "DPR",
                    "contains",
                    "empty",
                    "negatives,",
                    "and",
                    "we",
                    "discarded",
                    "the",
                    "empty",
                    "ones."
                ]
            ],
            "context": [
                0,
                0,
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "sent0: Natural Question (NQ) Kwiatkowski et al.\n sent1: (2019) introduces a large dataset for open-domain QA.\n sent2: The original dataset contains more than 300, 000 questions collected from Google search logs.\n sent3: In #TARGET_REF , around 62, 000 factoid questions are selected, and all the Wikipedia articles are processed as the collection of passages.\n sent4: There are more than 21 million passages in the corpus.\n sent5: In our experiments, we reuse the version of NQ created by #REF .\n sent6: Note that the dataset used in DPR contains empty negatives, and we discarded the empty ones.\n",
        "output": "{\"INFORMATION\": [\"sent3\", \"sent4\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Coreference",
                    "We",
                    "follow",
                    "#TARGET_REF",
                    "and",
                    "evaluate",
                    "the",
                    "coreference",
                    "agreement",
                    "scores",
                    "after",
                    "filtering",
                    "singleton",
                    "clusters."
                ],
                [
                    "We",
                    "report",
                    "the",
                    "standard",
                    "CoNLL-2012",
                    "score",
                    "#REF",
                    "that",
                    "combines",
                    "three",
                    "coreference",
                    "metric",
                    "scores."
                ],
                [
                    "The",
                    "in-domain",
                    "test",
                    "score",
                    "13",
                    "is",
                    "82.1,",
                    "while",
                    "in",
                    "the",
                    "OOD",
                    "the",
                    "score",
                    "is",
                    "77.1."
                ],
                [
                    "For",
                    "comparison",
                    "with",
                    "the",
                    "most",
                    "dominant",
                    "coreference",
                    "dataset,",
                    "OntoNotes",
                    "#REF",
                    ",",
                    "which",
                    "only",
                    "reported",
                    "the",
                    "MUC",
                    "agreement",
                    "score",
                    "#REF",
                    ",",
                    "we",
                    "also",
                    "measure",
                    "the",
                    "MUC",
                    "score",
                    "on",
                    "our",
                    "dataset."
                ],
                [
                    "The",
                    "MUC",
                    "score",
                    "on",
                    "our",
                    "dataset",
                    "is",
                    "83.6,",
                    "compared",
                    "to",
                    "78.4-89.4",
                    "in",
                    "OntoNotes,",
                    "depending",
                    "on",
                    "the",
                    "domain",
                    "#REF",
                    "."
                ],
                [
                    "It",
                    "is",
                    "worth",
                    "noting",
                    "that",
                    "on",
                    "the",
                    "Newswire",
                    "domain",
                    "of",
                    "Onto-Notes",
                    "#REF",
                    "(the",
                    "domain",
                    "that",
                    "is",
                    "most",
                    "similar",
                    "to",
                    "ours)",
                    "the",
                    "score",
                    "is",
                    "80.9,",
                    "which",
                    "indicates",
                    "a",
                    "high",
                    "quality",
                    "of",
                    "annotation",
                    "in",
                    "our",
                    "corpus."
                ],
                [
                    "We",
                    "expect",
                    "the",
                    "quality",
                    "of",
                    "our",
                    "final",
                    "coreference",
                    "data",
                    "to",
                    "be",
                    "even",
                    "higher",
                    "due",
                    "to",
                    "the",
                    "consolidation",
                    "step",
                    "that",
                    "was",
                    "done",
                    "by",
                    "an",
                    "expert",
                    "on",
                    "the",
                    "test",
                    "set",
                    "and",
                    "OOD",
                    "splits."
                ]
            ],
            "context": [
                2,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Coreference We follow #TARGET_REF and evaluate the coreference agreement scores after filtering singleton clusters.\n sent1: We report the standard CoNLL-2012 score #REF that combines three coreference metric scores.\n sent2: The in-domain test score 13 is 82.1, while in the OOD the score is 77.1.\n sent3: For comparison with the most dominant coreference dataset, OntoNotes #REF , which only reported the MUC agreement score #REF , we also measure the MUC score on our dataset.\n sent4: The MUC score on our dataset is 83.6, compared to 78.4-89.4 in OntoNotes, depending on the domain #REF .\n sent5: It is worth noting that on the Newswire domain of Onto-Notes #REF (the domain that is most similar to ours) the score is 80.9, which indicates a high quality of annotation in our corpus.\n sent6: We expect the quality of our final coreference data to be even higher due to the consolidation step that was done by an expert on the test set and OOD splits.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "sequential",
                    "decision",
                    "making",
                    "problems",
                    "in",
                    "particular,",
                    "this",
                    "generalization",
                    "gap",
                    "is",
                    "the",
                    "result",
                    "of",
                    "an",
                    "agent",
                    "simply",
                    "memorizing",
                    "trajectories,",
                    "e.g."
                ],
                [
                    "the",
                    "sequence",
                    "of",
                    "actions",
                    "and",
                    "dialogues",
                    "required",
                    "to",
                    "finish",
                    "a",
                    "game,",
                    "and",
                    "thus",
                    "being",
                    "unable",
                    "to",
                    "react",
                    "in",
                    "novel",
                    "scenarios-i.e."
                ],
                [
                    "the",
                    "agent",
                    "learns",
                    "from",
                    "the",
                    "head",
                    "the",
                    "training",
                    "data",
                    "and",
                    "simply",
                    "memorizes",
                    "the",
                    "long",
                    "tail."
                ],
                [
                    "One",
                    "way",
                    "of",
                    "decreasing",
                    "this",
                    "generalization",
                    "gap",
                    "is",
                    "by",
                    "training",
                    "agents",
                    "on",
                    "procedurally",
                    "generated",
                    "environments-wherein",
                    "the",
                    "agent",
                    "learns",
                    "a",
                    "family",
                    "of",
                    "parametrized",
                    "tasks",
                    "with",
                    "a",
                    "significantly",
                    "larger",
                    "state-action",
                    "spaces",
                    "than",
                    "singular",
                    "environments,",
                    "thus",
                    "effectively",
                    "making",
                    "the",
                    "memorization",
                    "of",
                    "trajectories",
                    "impossible",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Drawing",
                    "inspiration",
                    "from",
                    "all",
                    "of",
                    "these",
                    "ideas,",
                    "we",
                    "create",
                    "a",
                    "method",
                    "that",
                    "learns",
                    "to",
                    "create",
                    "a",
                    "training",
                    "curriculum",
                    "of",
                    "increasingly",
                    "more",
                    "difficult",
                    "novel",
                    "procedurally",
                    "generated",
                    "environments."
                ]
            ],
            "context": [
                3,
                0,
                0,
                1,
                0
            ]
        },
        "input": "sent0: In sequential decision making problems in particular, this generalization gap is the result of an agent simply memorizing trajectories, e.g.\n sent1: the sequence of actions and dialogues required to finish a game, and thus being unable to react in novel scenarios-i.e.\n sent2: the agent learns from the head the training data and simply memorizes the long tail.\n sent3: One way of decreasing this generalization gap is by training agents on procedurally generated environments-wherein the agent learns a family of parametrized tasks with a significantly larger state-action spaces than singular environments, thus effectively making the memorization of trajectories impossible #TARGET_REF .\n sent4: Drawing inspiration from all of these ideas, we create a method that learns to create a training curriculum of increasingly more difficult novel procedurally generated environments.\n",
        "output": "{\"INFORMATION\": [\"sent3\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "If",
                    "there",
                    "is",
                    "no",
                    "source",
                    "text,",
                    "the",
                    "focus",
                    "obviously",
                    "falls",
                    "upon",
                    "the",
                    "generation",
                    "of",
                    "the",
                    "target",
                    "text(s),",
                    "a",
                    "problem",
                    "in",
                    "MT",
                    "which",
                    "was",
                    "for",
                    "a",
                    "long",
                    "time",
                    "seriously",
                    "underestimated",
                    "(cf."
                ],
                [
                    "#REF",
                    ")."
                ],
                [
                    "Our",
                    "present",
                    "approach",
                    "has",
                    "been",
                    "influenced",
                    "by",
                    "the",
                    "'phrasebook'",
                    "approach",
                    "to",
                    "speech",
                    "translation",
                    "#TARGET_REF",
                    ",",
                    "in",
                    "which",
                    "set",
                    "phrases",
                    "are",
                    "stored,",
                    "as",
                    "in",
                    "a",
                    "holidaymaker's",
                    "phrasebook,",
                    "and",
                    "retrieved",
                    "by",
                    "the",
                    "fairly",
                    "crude,",
                    "though",
                    "effective,",
                    "technique",
                    "of",
                    "recognising",
                    "keywords",
                    "in",
                    "a",
                    "particular",
                    "order",
                    "in",
                    "the",
                    "input",
                    "speech",
                    "signal."
                ],
                [
                    "It",
                    "also",
                    "builds",
                    "on",
                    "research",
                    "on",
                    "interactive",
                    "generation",
                    "of",
                    "stereotypical",
                    "texts",
                    "(",
                    "#REF",
                    "),",
                    "where",
                    "texts",
                    "in",
                    "certain",
                    "restricted",
                    "domains",
                    "are",
                    "stored",
                    "and",
                    "retrieved",
                    "as",
                    "appropriate",
                    "through",
                    "interaction",
                    "with",
                    "users,",
                    "and",
                    "are",
                    "reformulated",
                    "to",
                    "fulfill",
                    "the",
                    "specific",
                    "requirements",
                    "expressed",
                    "by",
                    "them."
                ]
            ],
            "context": [
                3,
                3,
                1,
                1
            ]
        },
        "input": "sent0: If there is no source text, the focus obviously falls upon the generation of the target text(s), a problem in MT which was for a long time seriously underestimated (cf.\n sent1: #REF ).\n sent2: Our present approach has been influenced by the 'phrasebook' approach to speech translation #TARGET_REF , in which set phrases are stored, as in a holidaymaker's phrasebook, and retrieved by the fairly crude, though effective, technique of recognising keywords in a particular order in the input speech signal.\n sent3: It also builds on research on interactive generation of stereotypical texts ( #REF ), where texts in certain restricted domains are stored and retrieved as appropriate through interaction with users, and are reformulated to fulfill the specific requirements expressed by them.\n",
        "output": "{\"INFORMATION\": [\"sent2\", \"sent3\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "used",
                    "the",
                    "learning",
                    "with",
                    "Local",
                    "and",
                    "Global",
                    "Consistence",
                    "(LGC)",
                    "#REF",
                    "as",
                    "a",
                    "regularization",
                    "method."
                ],
                [
                    "The",
                    "algorithm",
                    "designs",
                    "a",
                    "classi-fying",
                    "function",
                    "that",
                    "is",
                    "sufficiently",
                    "smooth",
                    "concerning",
                    "the",
                    "intrinsic",
                    "structure",
                    "collectively",
                    "revealed",
                    "by",
                    "known",
                    "labeled",
                    "and",
                    "unlabeled",
                    "points."
                ],
                [
                    "Thus,",
                    "the",
                    "LGC",
                    "lets",
                    "every",
                    "point",
                    "iteratively",
                    "spread",
                    "its",
                    "label",
                    "information",
                    "to",
                    "its",
                    "neighbors",
                    "until",
                    "a",
                    "global",
                    "stable",
                    "state",
                    "is",
                    "achieved",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Also,",
                    "it",
                    "allows",
                    "the",
                    "class",
                    "information",
                    "of",
                    "the",
                    "labeled",
                    "objects",
                    "to",
                    "be",
                    "changed",
                    "during",
                    "the",
                    "classification",
                    "as",
                    "objects",
                    "may",
                    "be",
                    "erroneously",
                    "labeled",
                    "and,",
                    "consequently,",
                    "decrease",
                    "the",
                    "performance",
                    "of",
                    "the",
                    "classification."
                ],
                [
                    "More",
                    "than",
                    "that,",
                    "the",
                    "algorithm",
                    "diminished",
                    "the",
                    "influence",
                    "of",
                    "objects",
                    "with",
                    "a",
                    "high",
                    "degree",
                    "(many",
                    "neighboring",
                    "objects),",
                    "therefore,",
                    "these",
                    "objects",
                    "do",
                    "not",
                    "have",
                    "excessive",
                    "influence",
                    "in",
                    "the",
                    "classification."
                ]
            ],
            "context": [
                3,
                0,
                1,
                0,
                0
            ]
        },
        "input": "sent0: We used the learning with Local and Global Consistence (LGC) #REF as a regularization method.\n sent1: The algorithm designs a classi-fying function that is sufficiently smooth concerning the intrinsic structure collectively revealed by known labeled and unlabeled points.\n sent2: Thus, the LGC lets every point iteratively spread its label information to its neighbors until a global stable state is achieved #TARGET_REF .\n sent3: Also, it allows the class information of the labeled objects to be changed during the classification as objects may be erroneously labeled and, consequently, decrease the performance of the classification.\n sent4: More than that, the algorithm diminished the influence of objects with a high degree (many neighboring objects), therefore, these objects do not have excessive influence in the classification.\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "have",
                    "established",
                    "that",
                    "the",
                    "performance",
                    "of",
                    "top-k",
                    "attention",
                    "is",
                    "comparable",
                    "to",
                    "vanilla",
                    "attention",
                    "when",
                    "training",
                    "the",
                    "model",
                    "from",
                    "scratch."
                ],
                [
                    "In",
                    "this",
                    "set-up,",
                    "several",
                    "recently-proposed",
                    "approaches",
                    "have",
                    "also",
                    "reported",
                    "competitive",
                    "performances",
                    "#REF",
                    "."
                ],
                [
                    "Now,",
                    "we",
                    "consider",
                    "a",
                    "different",
                    "and",
                    "more",
                    "practical",
                    "setup,",
                    "where",
                    "the",
                    "starting",
                    "point",
                    "is",
                    "using",
                    "an",
                    "already",
                    "pre-trained",
                    "language",
                    "model",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "As",
                    "such",
                    "models",
                    "were",
                    "trained",
                    "using",
                    "vanilla",
                    "attention,",
                    "replacing",
                    "it",
                    "with",
                    "a",
                    "new",
                    "attention",
                    "variant",
                    "typically",
                    "requires",
                    "a",
                    "corrective",
                    "pretraining",
                    "stage",
                    "to",
                    "allow",
                    "the",
                    "model",
                    "weights",
                    "to",
                    "adjust",
                    "to",
                    "the",
                    "new",
                    "variant,",
                    "which",
                    "can",
                    "be",
                    "expensive",
                    "for",
                    "large",
                    "models."
                ],
                [
                    "For",
                    "example,",
                    "#REF",
                    "have",
                    "shown",
                    "that",
                    "using",
                    "random",
                    "features",
                    "without",
                    "corrective",
                    "pre-training",
                    "leads",
                    "to",
                    "high",
                    "error",
                    "rates",
                    "in",
                    "a",
                    "language",
                    "modeling",
                    "task."
                ],
                [
                    "Moreover,",
                    "as",
                    "explained",
                    "in",
                    "§2.1,",
                    "most",
                    "past",
                    "methods",
                    "are",
                    "incompatible",
                    "with",
                    "feed-forward",
                    "layers."
                ],
                [
                    "In",
                    "the",
                    "subsequent",
                    "experiments",
                    "we",
                    "show",
                    "that",
                    "it",
                    "is",
                    "possible",
                    "to",
                    "replace",
                    "vanilla",
                    "with",
                    "top-k",
                    "attention,",
                    "at",
                    "multi-head",
                    "attention",
                    "and",
                    "feed-forward",
                    "layers,",
                    "and",
                    "perform",
                    "inference",
                    "and",
                    "fine-tuning",
                    "without",
                    "any",
                    "need",
                    "for",
                    "such",
                    "correction."
                ]
            ],
            "context": [
                0,
                0,
                1,
                3,
                0,
                0,
                0
            ]
        },
        "input": "sent0: We have established that the performance of top-k attention is comparable to vanilla attention when training the model from scratch.\n sent1: In this set-up, several recently-proposed approaches have also reported competitive performances #REF .\n sent2: Now, we consider a different and more practical setup, where the starting point is using an already pre-trained language model #TARGET_REF .\n sent3: As such models were trained using vanilla attention, replacing it with a new attention variant typically requires a corrective pretraining stage to allow the model weights to adjust to the new variant, which can be expensive for large models.\n sent4: For example, #REF have shown that using random features without corrective pre-training leads to high error rates in a language modeling task.\n sent5: Moreover, as explained in §2.1, most past methods are incompatible with feed-forward layers.\n sent6: In the subsequent experiments we show that it is possible to replace vanilla with top-k attention, at multi-head attention and feed-forward layers, and perform inference and fine-tuning without any need for such correction.\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "For",
                    "the",
                    "Portuguese",
                    "language,",
                    "most",
                    "of",
                    "the",
                    "works",
                    "follow",
                    "the",
                    "trend",
                    "of",
                    "supervised",
                    "approaches."
                ],
                [
                    "de",
                    "Pelle",
                    "and",
                    "Moreira",
                    "(2017)",
                    "created",
                    "a",
                    "dataset",
                    "consist",
                    "of",
                    "1,",
                    "250",
                    "offensive",
                    "comments",
                    "and",
                    "developed",
                    "a",
                    "baseline",
                    "method",
                    "based",
                    "on",
                    "n-gram",
                    "features",
                    "to",
                    "classify",
                    "offensive",
                    "comments",
                    "in",
                    "their",
                    "dataset."
                ],
                [
                    "#TARGET_REF",
                    "created",
                    "a",
                    "hate",
                    "speech",
                    "dataset",
                    "composed",
                    "of",
                    "5,",
                    "668",
                    "tweets",
                    "and",
                    "developed",
                    "a",
                    "baseline",
                    "classification",
                    "using",
                    "pre-trained",
                    "word",
                    "embeddings",
                    "and",
                    "LSTM",
                    "in",
                    "their",
                    "dataset."
                ],
                [
                    "Coutinho",
                    "and",
                    "Malheiros",
                    "(2020)",
                    "trained",
                    "a",
                    "logistic",
                    "regression",
                    "using",
                    "superficial",
                    "features",
                    "for",
                    "sentiment",
                    "analysis."
                ],
                [
                    "Then,",
                    "they",
                    "evaluated",
                    "that",
                    "model",
                    "into",
                    "a",
                    "homophobia",
                    "corpus",
                    "to",
                    "detect",
                    "homophobic",
                    "posts."
                ]
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "sent0: For the Portuguese language, most of the works follow the trend of supervised approaches.\n sent1: de Pelle and Moreira (2017) created a dataset consist of 1, 250 offensive comments and developed a baseline method based on n-gram features to classify offensive comments in their dataset.\n sent2: #TARGET_REF created a hate speech dataset composed of 5, 668 tweets and developed a baseline classification using pre-trained word embeddings and LSTM in their dataset.\n sent3: Coutinho and Malheiros (2020) trained a logistic regression using superficial features for sentiment analysis.\n sent4: Then, they evaluated that model into a homophobia corpus to detect homophobic posts.\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "End-to-End",
                    "(E2E)",
                    "Pretraining",
                    "directly",
                    "feed",
                    "dense",
                    "features",
                    "on",
                    "image",
                    "grids",
                    "from",
                    "a",
                    "visual",
                    "backbone",
                    "network",
                    "into",
                    "a",
                    "Transformer",
                    "network",
                    "along",
                    "with",
                    "text",
                    "tokens."
                ],
                [
                    "As",
                    "such,",
                    "both",
                    "the",
                    "visual",
                    "and",
                    "Transformer",
                    "networks",
                    "are",
                    "optimized",
                    "jointly",
                    "in",
                    "an",
                    "end-to-end",
                    "manner",
                    "in",
                    "the",
                    "pretraining",
                    "&amp,",
                    "finetuning",
                    "stage."
                ],
                [
                    "Pixel-Bert",
                    "and",
                    "SOHO",
                    "#TARGET_REF",
                    "pioneer",
                    "the",
                    "use",
                    "of",
                    "the",
                    "E2E",
                    "pretraining",
                    "architecture",
                    "and",
                    "propose",
                    "a",
                    "novel",
                    "visual-dictionary",
                    "masked",
                    "vision",
                    "modeling",
                    "task."
                ],
                [
                    "E2E-VLP",
                    "#REF",
                    "presents",
                    "a",
                    "pretraining",
                    "framework",
                    "supervised",
                    "with",
                    "additional",
                    "object",
                    "detection",
                    "and",
                    "image",
                    "captioning",
                    "tasks",
                    "to",
                    "enhance",
                    "visual",
                    "semantics",
                    "learning."
                ],
                [
                    "It",
                    "is",
                    "worth",
                    "noting",
                    "that",
                    "their",
                    "object",
                    "detection",
                    "pretext",
                    "task",
                    "requires",
                    "millions",
                    "of",
                    "bounding",
                    "boxes",
                    "annotation,",
                    "unable",
                    "to",
                    "generalize",
                    "to",
                    "large-scale",
                    "image-text",
                    "corpus."
                ],
                [
                    "ViLT",
                    "#REF",
                    "is",
                    "the",
                    "first",
                    "to",
                    "unify",
                    "vision",
                    "and",
                    "language",
                    "with",
                    "a",
                    "pure",
                    "Transformer",
                    "network,",
                    "which",
                    "has",
                    "a",
                    "simpler",
                    "structure",
                    "and",
                    "enjoys",
                    "faster",
                    "inference."
                ],
                [
                    "However,",
                    "compared",
                    "to",
                    "the",
                    "twostep",
                    "methods,",
                    "they",
                    "are",
                    "typically",
                    "less",
                    "expressive",
                    "in",
                    "terms",
                    "of",
                    "object-level",
                    "concepts",
                    "and",
                    "thus",
                    "suffer",
                    "from",
                    "weaker",
                    "performances",
                    "on",
                    "challenging",
                    "visual",
                    "reasoning",
                    "tasks."
                ],
                [
                    "Our",
                    "method",
                    "is",
                    "in",
                    "line",
                    "with",
                    "the",
                    "E2E",
                    "pretraining",
                    "framework."
                ],
                [
                    "The",
                    "key",
                    "difference",
                    "is",
                    "that",
                    "we",
                    "propose",
                    "to",
                    "facilitate",
                    "learning",
                    "object-aware",
                    "multi-modal",
                    "representations",
                    "by",
                    "performing",
                    "object",
                    "semantic",
                    "knowledge",
                    "distillation."
                ]
            ],
            "context": [
                0,
                0,
                1,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: End-to-End (E2E) Pretraining directly feed dense features on image grids from a visual backbone network into a Transformer network along with text tokens.\n sent1: As such, both the visual and Transformer networks are optimized jointly in an end-to-end manner in the pretraining &amp, finetuning stage.\n sent2: Pixel-Bert and SOHO #TARGET_REF pioneer the use of the E2E pretraining architecture and propose a novel visual-dictionary masked vision modeling task.\n sent3: E2E-VLP #REF presents a pretraining framework supervised with additional object detection and image captioning tasks to enhance visual semantics learning.\n sent4: It is worth noting that their object detection pretext task requires millions of bounding boxes annotation, unable to generalize to large-scale image-text corpus.\n sent5: ViLT #REF is the first to unify vision and language with a pure Transformer network, which has a simpler structure and enjoys faster inference.\n sent6: However, compared to the twostep methods, they are typically less expressive in terms of object-level concepts and thus suffer from weaker performances on challenging visual reasoning tasks.\n sent7: Our method is in line with the E2E pretraining framework.\n sent8: The key difference is that we propose to facilitate learning object-aware multi-modal representations by performing object semantic knowledge distillation.\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Language",
                    "Models:",
                    "We",
                    "experimented",
                    "with",
                    "BERTbase",
                    "#REF",
                    "and",
                    "with",
                    "the",
                    "domainspecific",
                    "Transformers,",
                    "namely",
                    "BioBERT",
                    "and",
                    "Bio-ClinicalBERT",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "After",
                    "a",
                    "preliminary",
                    "fine-tuning",
                    "on",
                    "the",
                    "subtask",
                    "1a,",
                    "the",
                    "most",
                    "promising",
                    "results",
                    "were",
                    "obtained",
                    "by",
                    "BioBERT."
                ]
            ],
            "context": [
                3,
                2
            ]
        },
        "input": "sent0: Language Models: We experimented with BERTbase #REF and with the domainspecific Transformers, namely BioBERT and Bio-ClinicalBERT #TARGET_REF .\n sent1: After a preliminary fine-tuning on the subtask 1a, the most promising results were obtained by BioBERT.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "anticipate",
                    "exploring",
                    "various",
                    "extensions",
                    "to",
                    "and",
                    "validations",
                    "of",
                    "this",
                    "method",
                    "in",
                    "the",
                    "future."
                ],
                [
                    "Specifically,",
                    "we",
                    "would",
                    "like",
                    "to",
                    "explore",
                    "methods",
                    "that",
                    "might",
                    "mitigate",
                    "performance",
                    "degradation",
                    "due",
                    "to",
                    "a",
                    "lack",
                    "of",
                    "word",
                    "boundaries",
                    "in",
                    "our",
                    "method."
                ],
                [
                    "Subword",
                    "tokenization",
                    "techniques,",
                    "such",
                    "as",
                    "Byte-Pair",
                    "Encodings",
                    "(BPE)",
                    "#REF",
                    ",",
                    "or",
                    "character-based",
                    "word",
                    "segmentation",
                    "techniques",
                    "might",
                    "help",
                    "in",
                    "detecting",
                    "and",
                    "exploiting",
                    "repeating",
                    "patterns",
                    "within",
                    "the",
                    "phonetic",
                    "representation."
                ],
                [
                    "Furthermore,",
                    "the",
                    "word",
                    "embedding",
                    "techniques",
                    "used",
                    "by",
                    "#REF",
                    "or",
                    "#TARGET_REF",
                    "have",
                    "been",
                    "shown",
                    "to",
                    "work",
                    "well,",
                    "and",
                    "would",
                    "be",
                    "worth",
                    "investigating",
                    "how",
                    "the",
                    "removal",
                    "of",
                    "spacedelimited",
                    "word",
                    "boundaries",
                    "would",
                    "affect",
                    "this."
                ]
            ],
            "context": [
                0,
                0,
                0,
                2
            ]
        },
        "input": "sent0: We anticipate exploring various extensions to and validations of this method in the future.\n sent1: Specifically, we would like to explore methods that might mitigate performance degradation due to a lack of word boundaries in our method.\n sent2: Subword tokenization techniques, such as Byte-Pair Encodings (BPE) #REF , or character-based word segmentation techniques might help in detecting and exploiting repeating patterns within the phonetic representation.\n sent3: Furthermore, the word embedding techniques used by #REF or #TARGET_REF have been shown to work well, and would be worth investigating how the removal of spacedelimited word boundaries would affect this.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent3\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "SARI",
                    "SARI",
                    "#TARGET_REF",
                    ")",
                    "is",
                    "a",
                    "lexical",
                    "simplicity",
                    "metric",
                    "that",
                    "measures",
                    "how",
                    "good",
                    "are",
                    "the",
                    "words",
                    "added,",
                    "deleted",
                    "and",
                    "kept",
                    "by",
                    "a",
                    "simplification",
                    "model."
                ],
                [
                    "This",
                    "metric",
                    "compares",
                    "the",
                    "model",
                    "output",
                    "to",
                    "simplification",
                    "references",
                    "and",
                    "the",
                    "original",
                    "sentence."
                ],
                [
                    "We",
                    "use",
                    "the",
                    "SARI",
                    "implementation",
                    "in",
                    "the",
                    "EASSE",
                    "toolkit",
                    "4",
                    "."
                ]
            ],
            "context": [
                1,
                1,
                2
            ]
        },
        "input": "sent0: SARI SARI #TARGET_REF ) is a lexical simplicity metric that measures how good are the words added, deleted and kept by a simplification model.\n sent1: This metric compares the model output to simplification references and the original sentence.\n sent2: We use the SARI implementation in the EASSE toolkit 4 .\n",
        "output": "{\"INFORMATION\": [\"sent0\", \"sent1\"], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Technical",
                    "Details",
                    "All",
                    "neural",
                    "models",
                    "are",
                    "trained",
                    "using",
                    "cross-entropy",
                    "loss",
                    "and",
                    "optimized",
                    "with",
                    "Adam",
                    "(Kingma",
                    "and",
                    "Ba,",
                    "2015),",
                    "using",
                    "the",
                    "AllenNLP",
                    "library",
                    "#REF",
                    "."
                ],
                [
                    "We",
                    "train",
                    "using",
                    "a",
                    "1e",
                    "−",
                    "5",
                    "learning",
                    "rate",
                    "for",
                    "40",
                    "epochs,",
                    "with",
                    "early",
                    "stopping",
                    "based",
                    "the",
                    "F1",
                    "metric",
                    "on",
                    "the",
                    "development",
                    "set."
                ],
                [
                    "We",
                    "use",
                    "SpanBERT",
                    "#TARGET_REF",
                    "as",
                    "the",
                    "pretrained",
                    "MLM,",
                    "as",
                    "it",
                    "was",
                    "found",
                    "to",
                    "work",
                    "well",
                    "on",
                    "span-based",
                    "tasks",
                    "with",
                    "its",
                    "base",
                    "and",
                    "the",
                    "large",
                    "variants."
                ],
                [
                    "The",
                    "anchor",
                    "and",
                    "complement",
                    "encoding",
                    "MLPs",
                    "have",
                    "one",
                    "500-dim",
                    "hidden",
                    "layer",
                    "and",
                    "output",
                    "500dim",
                    "representations."
                ],
                [
                    "The",
                    "prediction",
                    "MLPs",
                    "have",
                    "one",
                    "100-dim",
                    "hidden",
                    "layer."
                ],
                [
                    "All",
                    "MLPs",
                    "use",
                    "the",
                    "ReLU",
                    "activation."
                ],
                [
                    "We",
                    "used",
                    "the",
                    "same",
                    "hyperparameters",
                    "for",
                    "all",
                    "baselines",
                    "and",
                    "did",
                    "not",
                    "tune",
                    "them."
                ],
                [
                    "18"
                ]
            ],
            "context": [
                3,
                3,
                1,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Technical Details All neural models are trained using cross-entropy loss and optimized with Adam (Kingma and Ba, 2015), using the AllenNLP library #REF .\n sent1: We train using a 1e − 5 learning rate for 40 epochs, with early stopping based the F1 metric on the development set.\n sent2: We use SpanBERT #TARGET_REF as the pretrained MLM, as it was found to work well on span-based tasks with its base and the large variants.\n sent3: The anchor and complement encoding MLPs have one 500-dim hidden layer and output 500dim representations.\n sent4: The prediction MLPs have one 100-dim hidden layer.\n sent5: All MLPs use the ReLU activation.\n sent6: We used the same hyperparameters for all baselines and did not tune them.\n sent7: 18\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Reasoning",
                    "based",
                    "on",
                    "causal",
                    "relation",
                    "between",
                    "events",
                    "is",
                    "used",
                    "in",
                    "two",
                    "types",
                    "of",
                    "argumentation:",
                    "argument",
                    "from",
                    "cause",
                    "to",
                    "effect",
                    "and",
                    "argument",
                    "from",
                    "effect",
                    "to",
                    "cause",
                    "#TARGET_REF",
                    "Effect-to-cause",
                    "(E2C)",
                    "reasoning",
                    "has",
                    "the",
                    "reversed",
                    "direction,",
                    "S",
                    "describes",
                    "an",
                    "observation",
                    "and",
                    "C",
                    "is",
                    "a",
                    "reasonable",
                    "explanation",
                    "that",
                    "may",
                    "have",
                    "caused",
                    "it."
                ],
                [
                    "If",
                    "C",
                    "causes",
                    "(obstructs)",
                    "S,",
                    "then",
                    "S",
                    "is",
                    "likely",
                    "to",
                    "support",
                    "(attack)",
                    "C,",
                    "as",
                    "in:",
                    "The",
                    "probabilities",
                    "are",
                    "computed",
                    "by",
                    "a",
                    "causality",
                    "module",
                    "(",
                    "§4.3).Claim:",
                    "St.",
                    "Andrew"
                ]
            ],
            "context": [
                0,
                0
            ]
        },
        "input": "sent0: Reasoning based on causal relation between events is used in two types of argumentation: argument from cause to effect and argument from effect to cause #TARGET_REF Effect-to-cause (E2C) reasoning has the reversed direction, S describes an observation and C is a reasonable explanation that may have caused it.\n sent1: If C causes (obstructs) S, then S is likely to support (attack) C, as in: The probabilities are computed by a causality module ( §4.3).Claim: St. Andrew\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "called",
                    "DMSNAP",
                    "using",
                    "a",
                    "parallel",
                    "marker-passing",
                    "scheme."
                ],
                [
                    "DM-SNAP",
                    "is",
                    "a",
                    "SNAP",
                    "implementation",
                    "of",
                    "the",
                    "ΦDMDIALOG",
                    "speechto-speech",
                    "dialogue",
                    "translation",
                    "system",
                    "#TARGET_REF",
                    ",",
                    "but",
                    "with",
                    "some",
                    "modifications",
                    "to",
                    "meet",
                    "hardware",
                    "constraints."
                ],
                [
                    "Despite",
                    "its",
                    "high",
                    "performance,",
                    "our",
                    "system",
                    "carries",
                    "out",
                    "sound",
                    "syntactic",
                    "and",
                    "semantic",
                    "analysis",
                    "including",
                    "lexical",
                    "ambiguity,",
                    "structural",
                    "ambiguity,",
                    "pronoun",
                    "reference,",
                    "control,",
                    "unbounded",
                    "dependency,",
                    "and",
                    "others."
                ]
            ],
            "context": [
                0,
                3,
                2
            ]
        },
        "input": "sent0: called DMSNAP using a parallel marker-passing scheme.\n sent1: DM-SNAP is a SNAP implementation of the ΦDMDIALOG speechto-speech dialogue translation system #TARGET_REF , but with some modifications to meet hardware constraints.\n sent2: Despite its high performance, our system carries out sound syntactic and semantic analysis including lexical ambiguity, structural ambiguity, pronoun reference, control, unbounded dependency, and others.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "These",
                    "methods",
                    "directly",
                    "incorporate",
                    "handengineered",
                    "features",
                    "and",
                    "personal",
                    "traits",
                    "of",
                    "users",
                    "or",
                    "their",
                    "communities",
                    "in",
                    "order",
                    "to",
                    "model",
                    "the",
                    "likelihood",
                    "of",
                    "abusive",
                    "language",
                    "in",
                    "the",
                    "users'",
                    "comments,",
                    "a",
                    "process",
                    "known",
                    "as",
                    "profiling",
                    "#REF",
                    "."
                ],
                [
                    "#TARGET_REF",
                    "included",
                    "the",
                    "age",
                    "of",
                    "users",
                    "alongside",
                    "other",
                    "traditional",
                    "lexicon-based",
                    "features",
                    "to",
                    "detect",
                    "cyber-bullying,",
                    "while",
                    "Galán-García",
                    "et",
                    "al."
                ],
                [
                    "(",
                    "2016)",
                    "utilized",
                    "the",
                    "time",
                    "of",
                    "publication,",
                    "geo-position,",
                    "and",
                    "language",
                    "in",
                    "the",
                    "profile",
                    "of",
                    "Twitter",
                    "users."
                ],
                [
                    "#REF",
                    "exploited",
                    "gender",
                    "of",
                    "Twitter",
                    "users",
                    "on",
                    "top",
                    "of",
                    "character",
                    "n-gram",
                    "counts",
                    "to",
                    "improve",
                    "detection",
                    "of",
                    "sexism",
                    "and",
                    "racism",
                    "in",
                    "a",
                    "dataset",
                    "comprising",
                    "racist,",
                    "sexist",
                    "and",
                    "benign",
                    "tweets",
                    "-they",
                    "noted",
                    "that",
                    "the",
                    "F",
                    "1",
                    "increased",
                    "slightly",
                    "from",
                    "73.89%",
                    "to",
                    "73.93%",
                    "when",
                    "the",
                    "gender",
                    "feature",
                    "was",
                    "included."
                ],
                [
                    "Using",
                    "the",
                    "same",
                    "setup,",
                    "Unsvåg",
                    "and",
                    "Gambäck",
                    "(2018)",
                    "showed",
                    "that",
                    "the",
                    "inclusion",
                    "of",
                    "social",
                    "community",
                    "(i.e.,",
                    "number",
                    "of",
                    "followers",
                    "and",
                    "friends)",
                    "and",
                    "activity",
                    "(i.e.,",
                    "number",
                    "of",
                    "status",
                    "updates",
                    "and",
                    "favorites)",
                    "features",
                    "of",
                    "users",
                    "alongside",
                    "their",
                    "gender",
                    "further",
                    "enhanced",
                    "performance",
                    "by",
                    "3",
                    "F",
                    "1",
                    "points",
                    "over",
                    "the",
                    "n-gram",
                    "baseline."
                ]
            ],
            "context": [
                0,
                1,
                3,
                0,
                0
            ]
        },
        "input": "sent0: These methods directly incorporate handengineered features and personal traits of users or their communities in order to model the likelihood of abusive language in the users' comments, a process known as profiling #REF .\n sent1: #TARGET_REF included the age of users alongside other traditional lexicon-based features to detect cyber-bullying, while Galán-García et al.\n sent2: ( 2016) utilized the time of publication, geo-position, and language in the profile of Twitter users.\n sent3: #REF exploited gender of Twitter users on top of character n-gram counts to improve detection of sexism and racism in a dataset comprising racist, sexist and benign tweets -they noted that the F 1 increased slightly from 73.89% to 73.93% when the gender feature was included.\n sent4: Using the same setup, Unsvåg and Gambäck (2018) showed that the inclusion of social community (i.e., number of followers and friends) and activity (i.e., number of status updates and favorites) features of users alongside their gender further enhanced performance by 3 F 1 points over the n-gram baseline.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "BoW",
                    "+",
                    "BERT",
                    "As",
                    "a",
                    "baseline,",
                    "we",
                    "consider",
                    "the",
                    "retrieve-and-rerank",
                    "approach",
                    "originally",
                    "proposed",
                    "by",
                    "#TARGET_REF",
                    ",",
                    "which",
                    "has",
                    "emerged",
                    "as",
                    "the",
                    "standard",
                    "architecture",
                    "for",
                    "applying",
                    "pretrained",
                    "transformers",
                    "to",
                    "ranking."
                ],
                [
                    "We",
                    "notate",
                    "a",
                    "specific",
                    "configuration",
                    "of",
                    "this",
                    "design",
                    "as",
                    "BoW(k",
                    "0",
                    ")",
                    "+",
                    "BERT,",
                    "where",
                    "k",
                    "0",
                    "denotes",
                    "the",
                    "number",
                    "of",
                    "candidates",
                    "from",
                    "bag-of-words",
                    "retrieval",
                    "that",
                    "are",
                    "then",
                    "reranked",
                    "by",
                    "BERT."
                ],
                [
                    "A",
                    "commonly",
                    "used",
                    "default",
                    "is",
                    "BoW(1000)",
                    "+",
                    "BERT",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                2,
                2,
                0
            ]
        },
        "input": "sent0: BoW + BERT As a baseline, we consider the retrieve-and-rerank approach originally proposed by #TARGET_REF , which has emerged as the standard architecture for applying pretrained transformers to ranking.\n sent1: We notate a specific configuration of this design as BoW(k 0 ) + BERT, where k 0 denotes the number of candidates from bag-of-words retrieval that are then reranked by BERT.\n sent2: A commonly used default is BoW(1000) + BERT #REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\", \"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "As",
                    "other",
                    "current",
                    "actions",
                    "in",
                    "the",
                    "field",
                    "of",
                    "LR",
                    "(e.g."
                ],
                [
                    "EAGLES,",
                    "which",
                    "is",
                    "a",
                    "direct",
                    "descendant",
                    "of",
                    "the",
                    "\"Pisa",
                    "Group\",",
                    "set-up",
                    "at",
                    "the",
                    "Grosseto",
                    "Workshop",
                    "by",
                    "the",
                    "Istituto",
                    "di",
                    "Linguistica",
                    "Computazionale",
                    "(ILC)",
                    "of",
                    "Pisa",
                    "and",
                    "sponsored",
                    "by",
                    "the",
                    "Association",
                    "for",
                    "Computational",
                    "Linguistics",
                    "#REF",
                    "(ACL)",
                    "to",
                    "explore",
                    "the",
                    "feasibility",
                    "of",
                    "\"polytheoretical",
                    "lexicons\"",
                    "#REF",
                    ")),",
                    "the",
                    "PAROLE",
                    "and",
                    "SIMPLE",
                    "projects,",
                    "building",
                    "large",
                    "corpora",
                    "and",
                    "lexicons",
                    "for",
                    "many",
                    "European",
                    "languages,",
                    "are",
                    "the",
                    "follow-up",
                    "of",
                    "some",
                    "initiatives",
                    "promoted",
                    "at",
                    "the",
                    "Grosseto",
                    "Workshop."
                ],
                [
                    "The",
                    "Council",
                    "of",
                    "Europe,",
                    "which",
                    "had",
                    "co-sponsored",
                    "the",
                    "workshop,",
                    "formed",
                    "a",
                    "group",
                    "of",
                    "experts,",
                    "representing",
                    "European",
                    "institutes",
                    "with",
                    "a",
                    "well",
                    "established",
                    "tradition",
                    "in",
                    "the",
                    "field",
                    "of",
                    "lexical",
                    "and",
                    "corpus",
                    "studies,",
                    "to",
                    "explore",
                    "the",
                    "feasibility",
                    "of",
                    "harmonising",
                    "their",
                    "activities,",
                    "in",
                    "order",
                    "to",
                    "establish",
                    "a",
                    "Network",
                    "of",
                    "European",
                    "Reference",
                    "Corpora",
                    "(NERC,",
                    "for",
                    "which",
                    "see",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "This",
                    "group,",
                    "gradually",
                    "enlarged",
                    "to",
                    "include",
                    "members",
                    "of",
                    "all",
                    "the",
                    "European",
                    "Union",
                    "(EU)",
                    "languages,",
                    "constituted",
                    "the",
                    "PAROLE",
                    "Consortium",
                    "which",
                    "has",
                    "executed",
                    "the",
                    "LE",
                    "(Language",
                    "Engineering)",
                    "PAROLE",
                    "project",
                    "now",
                    "followed",
                    "by",
                    "the",
                    "LE",
                    "SIMPLE",
                    "project",
                    "carried",
                    "on",
                    "by",
                    "a",
                    "similar",
                    "Consortium",
                    "1",
                    "."
                ]
            ],
            "context": [
                0,
                0,
                3,
                2
            ]
        },
        "input": "sent0: As other current actions in the field of LR (e.g.\n sent1: EAGLES, which is a direct descendant of the \"Pisa Group\", set-up at the Grosseto Workshop by the Istituto di Linguistica Computazionale (ILC) of Pisa and sponsored by the Association for Computational Linguistics #REF (ACL) to explore the feasibility of \"polytheoretical lexicons\" #REF )), the PAROLE and SIMPLE projects, building large corpora and lexicons for many European languages, are the follow-up of some initiatives promoted at the Grosseto Workshop.\n sent2: The Council of Europe, which had co-sponsored the workshop, formed a group of experts, representing European institutes with a well established tradition in the field of lexical and corpus studies, to explore the feasibility of harmonising their activities, in order to establish a Network of European Reference Corpora (NERC, for which see #TARGET_REF .\n sent3: This group, gradually enlarged to include members of all the European Union (EU) languages, constituted the PAROLE Consortium which has executed the LE (Language Engineering) PAROLE project now followed by the LE SIMPLE project carried on by a similar Consortium 1 .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent3\"], \"BACKGROUND\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Effect",
                    "of",
                    "hyperparameter",
                    "tuning",
                    "A",
                    "generally",
                    "unstated",
                    "assumption",
                    "is",
                    "that",
                    "pre-trained",
                    "linguistic",
                    "models",
                    "are",
                    "under-optimized",
                    "and",
                    "that",
                    "practices",
                    "commonly",
                    "adopted",
                    "for",
                    "the",
                    "fine-tuning",
                    "stage",
                    "can",
                    "be",
                    "detrimental",
                    "to",
                    "performance",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "This",
                    "is",
                    "quite",
                    "apparent",
                    "in",
                    "all",
                    "settings,",
                    "with",
                    "better",
                    "gains",
                    "through",
                    "hyperparameter",
                    "optimization",
                    "stages."
                ],
                [
                    "Fine-tuning",
                    "CamemBERT",
                    "large",
                    "on",
                    "the",
                    "French",
                    "dataset",
                    "yields",
                    "90.2",
                    "/",
                    "75.5",
                    "F1",
                    "/",
                    "EM",
                    "on",
                    "the",
                    "FQuAD",
                    "dev",
                    "set."
                ],
                [
                    "By",
                    "means",
                    "of",
                    "comparison,",
                    "CamemBERT",
                    "large",
                    "scores",
                    "were",
                    "81.2",
                    "/",
                    "55.9",
                    "F1",
                    "/",
                    "EM",
                    "on",
                    "the",
                    "same",
                    "set",
                    "with",
                    "no",
                    "hyperparameter",
                    "tuning."
                ]
            ],
            "context": [
                1,
                2,
                3,
                0
            ]
        },
        "input": "sent0: Effect of hyperparameter tuning A generally unstated assumption is that pre-trained linguistic models are under-optimized and that practices commonly adopted for the fine-tuning stage can be detrimental to performance #TARGET_REF .\n sent1: This is quite apparent in all settings, with better gains through hyperparameter optimization stages.\n sent2: Fine-tuning CamemBERT large on the French dataset yields 90.2 / 75.5 F1 / EM on the FQuAD dev set.\n sent3: By means of comparison, CamemBERT large scores were 81.2 / 55.9 F1 / EM on the same set with no hyperparameter tuning.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "For",
                    "natural",
                    "language",
                    "processing",
                    "tasks",
                    "in",
                    "Vietnamese,",
                    "there",
                    "have",
                    "been",
                    "many",
                    "pre-trained",
                    "language",
                    "models",
                    "are",
                    "available."
                ],
                [
                    "In",
                    "2016,",
                    "#REF",
                    "introduced",
                    "the",
                    "first",
                    "monolingual",
                    "pre-trained",
                    "models",
                    "for",
                    "Vietnamese",
                    "based",
                    "on",
                    "Word2Vec",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "The",
                    "use",
                    "of",
                    "pre-trained",
                    "Word2VecVN",
                    "models",
                    "was",
                    "proved",
                    "to",
                    "be",
                    "useful",
                    "in",
                    "various",
                    "tasks,",
                    "such",
                    "as",
                    "the",
                    "name",
                    "entity",
                    "recognition",
                    "task",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                3,
                3,
                3
            ]
        },
        "input": "sent0: For natural language processing tasks in Vietnamese, there have been many pre-trained language models are available.\n sent1: In 2016, #REF introduced the first monolingual pre-trained models for Vietnamese based on Word2Vec #TARGET_REF .\n sent2: The use of pre-trained Word2VecVN models was proved to be useful in various tasks, such as the name entity recognition task #REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\", \"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Across",
                    "the",
                    "three",
                    "categories",
                    "of",
                    "methods,",
                    "we",
                    "note",
                    "that",
                    "the",
                    "general",
                    "setup",
                    "is",
                    "to",
                    "create",
                    "representations,",
                    "called",
                    "profiles,",
                    "for",
                    "users",
                    "or",
                    "communities",
                    "and",
                    "utilize",
                    "them",
                    "alongside",
                    "linguistic",
                    "features."
                ],
                [
                    "In",
                    "social",
                    "feature",
                    "engineering",
                    "based",
                    "methods,",
                    "these",
                    "profiles",
                    "are",
                    "manually",
                    "constructed",
                    "vectors",
                    "of",
                    "features",
                    "that",
                    "capture",
                    "the",
                    "relevant",
                    "traits,",
                    "such",
                    "as",
                    "age",
                    "in",
                    "the",
                    "case",
                    "of",
                    "cyber-bullying",
                    "and",
                    "gender",
                    "in",
                    "the",
                    "case",
                    "of",
                    "sexism."
                ],
                [
                    "In",
                    "user",
                    "embeddings",
                    "and",
                    "social",
                    "graph",
                    "based",
                    "methods,",
                    "the",
                    "profiles",
                    "are",
                    "instead",
                    "generated",
                    "by",
                    "neural",
                    "network",
                    "architectures",
                    "to",
                    "capture",
                    "the",
                    "linguistic",
                    "behavior",
                    "or",
                    "community",
                    "traits",
                    "of",
                    "users."
                ],
                [
                    "That",
                    "said,",
                    "across",
                    "all",
                    "three",
                    "categories,",
                    "the",
                    "profiles",
                    "essentially",
                    "provide",
                    "a",
                    "wider",
                    "context",
                    "to",
                    "the",
                    "comment",
                    "being",
                    "classified",
                    "for",
                    "abuse."
                ],
                [
                    "For",
                    "example,",
                    "having",
                    "the",
                    "gender",
                    "of",
                    "the",
                    "user",
                    "who",
                    "produces",
                    "a",
                    "comment",
                    "such",
                    "as",
                    "\"Had",
                    "an",
                    "accident,",
                    "women",
                    "can't",
                    "drive",
                    "it",
                    "seems!\""
                ],
                [
                    "can",
                    "help",
                    "to",
                    "classify",
                    "the",
                    "comment",
                    "as",
                    "sexist",
                    "or",
                    "not",
                    "by",
                    "differentiating",
                    "benign",
                    "self-deprecating",
                    "humor",
                    "from",
                    "intent",
                    "to",
                    "degrade."
                ],
                [
                    "The",
                    "context",
                    "that",
                    "the",
                    "profiles",
                    "encode",
                    "increases",
                    "as",
                    "we",
                    "go",
                    "from",
                    "social",
                    "feature",
                    "engineering",
                    "based",
                    "methods",
                    "to",
                    "user",
                    "embeddings",
                    "based",
                    "methods",
                    "and",
                    "further",
                    "to",
                    "social",
                    "graph",
                    "based",
                    "methods."
                ],
                [
                    "This",
                    "is",
                    "also",
                    "evident",
                    "from",
                    "the",
                    "magnitude",
                    "of",
                    "gains",
                    "that",
                    "the",
                    "profiles",
                    "provide",
                    "on",
                    "top",
                    "of",
                    "linguistic",
                    "features."
                ],
                [
                    "For",
                    "example,",
                    "the",
                    "gender",
                    "feature",
                    "only",
                    "increases",
                    "the",
                    "F",
                    "1",
                    "from",
                    "73.89%",
                    "to",
                    "73.93%",
                    "over",
                    "character",
                    "n-gram",
                    "counts",
                    "on",
                    "the",
                    "dataset",
                    "by",
                    "#REF",
                    ",",
                    "while",
                    "the",
                    "social",
                    "graph",
                    "based",
                    "method",
                    "of",
                    "#TARGET_REF",
                    "increases",
                    "the",
                    "F",
                    "1",
                    "to",
                    "above",
                    "80%."
                ],
                [
                    "The",
                    "example",
                    "aside,",
                    "it",
                    "makes",
                    "intuitive",
                    "sense",
                    "that",
                    "profiles",
                    "from",
                    "social",
                    "graph",
                    "based",
                    "methods",
                    "encode",
                    "the",
                    "most",
                    "amount",
                    "of",
                    "context,",
                    "since",
                    "these",
                    "profiles",
                    "are",
                    "able",
                    "to",
                    "capture",
                    "the",
                    "various",
                    "phenomena",
                    "that",
                    "occur",
                    "in",
                    "social",
                    "networks,",
                    "the",
                    "most",
                    "prominent",
                    "ones",
                    "of",
                    "which",
                    "are:"
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Across the three categories of methods, we note that the general setup is to create representations, called profiles, for users or communities and utilize them alongside linguistic features.\n sent1: In social feature engineering based methods, these profiles are manually constructed vectors of features that capture the relevant traits, such as age in the case of cyber-bullying and gender in the case of sexism.\n sent2: In user embeddings and social graph based methods, the profiles are instead generated by neural network architectures to capture the linguistic behavior or community traits of users.\n sent3: That said, across all three categories, the profiles essentially provide a wider context to the comment being classified for abuse.\n sent4: For example, having the gender of the user who produces a comment such as \"Had an accident, women can't drive it seems!\"\n sent5: can help to classify the comment as sexist or not by differentiating benign self-deprecating humor from intent to degrade.\n sent6: The context that the profiles encode increases as we go from social feature engineering based methods to user embeddings based methods and further to social graph based methods.\n sent7: This is also evident from the magnitude of gains that the profiles provide on top of linguistic features.\n sent8: For example, the gender feature only increases the F 1 from 73.89% to 73.93% over character n-gram counts on the dataset by #REF , while the social graph based method of #TARGET_REF increases the F 1 to above 80%.\n sent9: The example aside, it makes intuitive sense that profiles from social graph based methods encode the most amount of context, since these profiles are able to capture the various phenomena that occur in social networks, the most prominent ones of which are:\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Using",
                    "CCG",
                    "categories",
                    "to",
                    "label",
                    "non-terminals",
                    "in",
                    "HPB",
                    "rules",
                    "can",
                    "produce",
                    "better",
                    "translation",
                    "quality",
                    "and",
                    "smaller",
                    "trans-lation",
                    "models",
                    "in",
                    "comparison",
                    "with",
                    "SAMT",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "CCG",
                    "nonterminal",
                    "labels",
                    "are",
                    "less",
                    "sparse",
                    "and",
                    "represent",
                    "richer",
                    "and",
                    "more",
                    "accurate",
                    "syntactic",
                    "constraints",
                    "compared",
                    "to",
                    "SAMT",
                    "nonterminal",
                    "labels",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                1,
                2
            ]
        },
        "input": "sent0: Using CCG categories to label non-terminals in HPB rules can produce better translation quality and smaller trans-lation models in comparison with SAMT #TARGET_REF .\n sent1: CCG nonterminal labels are less sparse and represent richer and more accurate syntactic constraints compared to SAMT nonterminal labels #REF .\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "•",
                    "Sampled",
                    "Curriculums."
                ],
                [
                    "Inspired",
                    "by",
                    "#TARGET_REF",
                    ",",
                    "Graves",
                    "et",
                    "al."
                ],
                [
                    "(",
                    "2017),",
                    "we",
                    "explore",
                    "an",
                    "alternate",
                    "method",
                    "of",
                    "creating",
                    "curriculums",
                    "by",
                    "simply",
                    "oversampling",
                    "the",
                    "same",
                    "rare",
                    "quests",
                    "found",
                    "in",
                    "the",
                    "tails",
                    "of",
                    "the",
                    "distributions."
                ]
            ],
            "context": [
                0,
                2,
                2
            ]
        },
        "input": "sent0: • Sampled Curriculums.\n sent1: Inspired by #TARGET_REF , Graves et al.\n sent2: ( 2017), we explore an alternate method of creating curriculums by simply oversampling the same rare quests found in the tails of the distributions.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\", \"sent2\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "where",
                    "X",
                    "∈",
                    "{A,",
                    "B,",
                    "C,",
                    "D})."
                ],
                [
                    "We",
                    "consider",
                    "two",
                    "settings",
                    "where",
                    "each",
                    "generative",
                    "factor",
                    "is",
                    "embedded",
                    "in",
                    "a",
                    "single",
                    "dimension",
                    "(denoted",
                    "by",
                    "Ex.1),",
                    "or",
                    "two",
                    "dimensions",
                    "(denoted",
                    "by",
                    "Ex.2)."
                ],
                [
                    "In",
                    "each",
                    "setting",
                    "we",
                    "uniformly",
                    "sample",
                    "20",
                    "values",
                    "from",
                    "-1",
                    "to",
                    "1",
                    "to",
                    "represent",
                    "20",
                    "assignments",
                    "per",
                    "factor",
                    "and",
                    "use",
                    "them",
                    "to",
                    "allocate",
                    "the",
                    "assignments",
                    "into",
                    "distinctive",
                    "bins",
                    "per",
                    "each",
                    "corresponding",
                    "dimension."
                ],
                [
                    "By",
                    "concatenating",
                    "dimensions",
                    "for",
                    "each",
                    "generative",
                    "factor,",
                    "we",
                    "construct",
                    "two",
                    "ideal",
                    "disentangled",
                    "representations",
                    "for",
                    "data",
                    "points",
                    "in",
                    "this",
                    "toy",
                    "dataset,",
                    "amounting",
                    "to",
                    "4",
                    "and",
                    "8",
                    "dimensional",
                    "representations,",
                    "respectively."
                ],
                [
                    "Using",
                    "these",
                    "representations",
                    "(skipping",
                    "the",
                    "encoding",
                    "step),",
                    "we",
                    "measured",
                    "the",
                    "above",
                    "metrics."
                ],
                [
                    "Table",
                    "1",
                    "(Ex.1",
                    "and",
                    "Ex.2",
                    "columns)",
                    "summarises",
                    "the",
                    "results,",
                    "illustrating",
                    "that",
                    "out",
                    "of",
                    "the",
                    "6",
                    "metrics,",
                    "#REF",
                    ",",
                    "Ridgeway",
                    "and",
                    "Mozer",
                    "(2018),",
                    "#REF",
                    "are",
                    "the",
                    "only",
                    "ones",
                    "that",
                    "reach",
                    "the",
                    "potential",
                    "maximum",
                    "(i.e.,",
                    "100),",
                    "while",
                    "#TARGET_REF",
                    "exhibits",
                    "its",
                    "sensitivity",
                    "towards",
                    "completeness",
                    "when",
                    "we",
                    "allocate",
                    "two",
                    "dimensions",
                    "per",
                    "factors."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                3
            ]
        },
        "input": "sent0: where X ∈ {A, B, C, D}).\n sent1: We consider two settings where each generative factor is embedded in a single dimension (denoted by Ex.1), or two dimensions (denoted by Ex.2).\n sent2: In each setting we uniformly sample 20 values from -1 to 1 to represent 20 assignments per factor and use them to allocate the assignments into distinctive bins per each corresponding dimension.\n sent3: By concatenating dimensions for each generative factor, we construct two ideal disentangled representations for data points in this toy dataset, amounting to 4 and 8 dimensional representations, respectively.\n sent4: Using these representations (skipping the encoding step), we measured the above metrics.\n sent5: Table 1 (Ex.1 and Ex.2 columns) summarises the results, illustrating that out of the 6 metrics, #REF , Ridgeway and Mozer (2018), #REF are the only ones that reach the potential maximum (i.e., 100), while #TARGET_REF exhibits its sensitivity towards completeness when we allocate two dimensions per factors.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent5\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "There",
                    "is",
                    "a",
                    "recent",
                    "explosion",
                    "of",
                    "explanation-centred",
                    "datasets",
                    "for",
                    "multi-hop",
                    "question",
                    "answering",
                    "#REF",
                    "."
                ],
                [
                    "However,",
                    "most",
                    "of",
                    "these",
                    "datasets",
                    "require",
                    "the",
                    "aggregation",
                    "of",
                    "only",
                    "two",
                    "sentences",
                    "or",
                    "paragraphs,",
                    "making",
                    "it",
                    "hard",
                    "to",
                    "evaluate",
                    "the",
                    "robustness",
                    "of",
                    "the",
                    "models",
                    "in",
                    "terms",
                    "of",
                    "semantic",
                    "drift."
                ],
                [
                    "On",
                    "the",
                    "other",
                    "hand,",
                    "the",
                    "WorldTree",
                    "corpus",
                    "#TARGET_REF",
                    "used",
                    "in",
                    "this",
                    "shared",
                    "task",
                    "is",
                    "explicitly",
                    "designed",
                    "to",
                    "test",
                    "multi-hop",
                    "inference",
                    "models",
                    "on",
                    "the",
                    "reconstruction",
                    "of",
                    "long",
                    "inference",
                    "chains",
                    "requiring",
                    "the",
                    "aggregation",
                    "of",
                    "an",
                    "average",
                    "of",
                    "6",
                    "facts,",
                    "and",
                    "as",
                    "many",
                    "as",
                    "16",
                    "facts."
                ]
            ],
            "context": [
                0,
                0,
                2
            ]
        },
        "input": "sent0: There is a recent explosion of explanation-centred datasets for multi-hop question answering #REF .\n sent1: However, most of these datasets require the aggregation of only two sentences or paragraphs, making it hard to evaluate the robustness of the models in terms of semantic drift.\n sent2: On the other hand, the WorldTree corpus #TARGET_REF used in this shared task is explicitly designed to test multi-hop inference models on the reconstruction of long inference chains requiring the aggregation of an average of 6 facts, and as many as 16 facts.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "collect",
                    "a",
                    "set",
                    "of",
                    "occurrences",
                    "of",
                    "typed",
                    "entity",
                    "mentions",
                    "using",
                    "hyperlinks",
                    "in",
                    "Wikipedia."
                ],
                [
                    "Given",
                    "a",
                    "sentence",
                    "with",
                    "a",
                    "hyperlink,",
                    "we",
                    "use",
                    "the",
                    "hyperlink",
                    "as",
                    "an",
                    "entity",
                    "mention",
                    "m,",
                    "the",
                    "sentence",
                    "as",
                    "a",
                    "context",
                    "sentence",
                    "s,",
                    "and",
                    "the",
                    "Wiki",
                    "categories",
                    "of",
                    "the",
                    "destination",
                    "page",
                    "as",
                    "the",
                    "gold",
                    "entity",
                    "types",
                    "t",
                    "*",
                    "."
                ],
                [
                    "We",
                    "use",
                    "the",
                    "preprocessing",
                    "of",
                    "#TARGET_REF",
                    "to",
                    "modify",
                    "the",
                    "type",
                    "set:",
                    "they",
                    "introduce",
                    "more",
                    "general",
                    "categories",
                    "into",
                    "the",
                    "Wikipedia",
                    "category",
                    "set",
                    "by",
                    "splitting",
                    "existing",
                    "complex",
                    "categories."
                ],
                [
                    "Following",
                    "their",
                    "work,",
                    "we",
                    "filter",
                    "the",
                    "resulting",
                    "set",
                    "to",
                    "keep",
                    "the",
                    "60,000",
                    "most",
                    "frequent",
                    "types."
                ],
                [
                    "Scraping",
                    "Wikipedia",
                    "yields",
                    "6M",
                    "training",
                    "examples",
                    "that",
                    "cover",
                    "a",
                    "wide",
                    "range",
                    "of",
                    "entities",
                    "and",
                    "entity",
                    "types."
                ]
            ],
            "context": [
                0,
                0,
                2,
                2,
                0
            ]
        },
        "input": "sent0: We collect a set of occurrences of typed entity mentions using hyperlinks in Wikipedia.\n sent1: Given a sentence with a hyperlink, we use the hyperlink as an entity mention m, the sentence as a context sentence s, and the Wiki categories of the destination page as the gold entity types t * .\n sent2: We use the preprocessing of #TARGET_REF to modify the type set: they introduce more general categories into the Wikipedia category set by splitting existing complex categories.\n sent3: Following their work, we filter the resulting set to keep the 60,000 most frequent types.\n sent4: Scraping Wikipedia yields 6M training examples that cover a wide range of entities and entity types.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent2\", \"sent3\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "As",
                    "future",
                    "work,",
                    "we",
                    "would",
                    "be",
                    "looking",
                    "to",
                    "improve",
                    "our",
                    "results",
                    "more",
                    "with",
                    "new",
                    "strategies."
                ],
                [
                    "We",
                    "would",
                    "like",
                    "to",
                    "experiment",
                    "with",
                    "whether",
                    "adding",
                    "languagespecific",
                    "processing",
                    "and",
                    "resources",
                    "would",
                    "improve",
                    "the",
                    "results."
                ],
                [
                    "We",
                    "are",
                    "keen",
                    "to",
                    "add",
                    "different",
                    "neural",
                    "network",
                    "architectures",
                    "like",
                    "Siamese",
                    "transformer",
                    "networks",
                    "#REF",
                    "that",
                    "perform",
                    "well",
                    "in",
                    "sentence",
                    "pair",
                    "classification",
                    "tasks",
                    "#TARGET_REF",
                    "to",
                    "the",
                    "TransWiC",
                    "framework."
                ],
                [
                    "Furthermore,",
                    "we",
                    "are",
                    "hoping",
                    "to",
                    "work",
                    "in",
                    "a",
                    "multi-task",
                    "environment",
                    "and",
                    "experiment",
                    "whether",
                    "transfer",
                    "learning",
                    "from",
                    "a",
                    "similar",
                    "task",
                    "like",
                    "semantic",
                    "textual",
                    "similarity",
                    "#REF",
                    "would",
                    "improve",
                    "the",
                    "results",
                    "for",
                    "this",
                    "task."
                ]
            ],
            "context": [
                0,
                3,
                2,
                0
            ]
        },
        "input": "sent0: As future work, we would be looking to improve our results more with new strategies.\n sent1: We would like to experiment with whether adding languagespecific processing and resources would improve the results.\n sent2: We are keen to add different neural network architectures like Siamese transformer networks #REF that perform well in sentence pair classification tasks #TARGET_REF to the TransWiC framework.\n sent3: Furthermore, we are hoping to work in a multi-task environment and experiment whether transfer learning from a similar task like semantic textual similarity #REF would improve the results for this task.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "this",
                    "work,",
                    "we",
                    "explore",
                    "a",
                    "set",
                    "of",
                    "interpretable",
                    "entity",
                    "representations",
                    "that",
                    "are",
                    "simultaneously",
                    "human",
                    "and",
                    "machine",
                    "readable."
                ],
                [
                    "The",
                    "key",
                    "idea",
                    "of",
                    "this",
                    "approach",
                    "is",
                    "to",
                    "use",
                    "fine-grained",
                    "entity",
                    "typing",
                    "models",
                    "with",
                    "large",
                    "type",
                    "inventories",
                    "#REF",
                    "."
                ],
                [
                    "Given",
                    "an",
                    "entity",
                    "mention",
                    "and",
                    "context",
                    "words,",
                    "our",
                    "typing",
                    "model",
                    "outputs",
                    "a",
                    "highdimensional",
                    "vector",
                    "whose",
                    "values",
                    "are",
                    "associated",
                    "with",
                    "predefined",
                    "fine-grained",
                    "entity",
                    "types."
                ],
                [
                    "Each",
                    "value",
                    "ranges",
                    "between",
                    "0",
                    "and",
                    "1,",
                    "corresponding",
                    "to",
                    "the",
                    "confidence",
                    "of",
                    "the",
                    "model's",
                    "decision",
                    "that",
                    "the",
                    "entity",
                    "has",
                    "the",
                    "property",
                    "given",
                    "by",
                    "the",
                    "corresponding",
                    "type."
                ],
                [
                    "We",
                    "use",
                    "pre-trained",
                    "Transformer-based",
                    "entity",
                    "typing",
                    "models,",
                    "trained",
                    "either",
                    "on",
                    "a",
                    "supervised",
                    "entity",
                    "typing",
                    "dataset",
                    "#TARGET_REF",
                    "or",
                    "on",
                    "a",
                    "distantlysupervised",
                    "dataset",
                    "derived",
                    "from",
                    "Wikipedia",
                    "categories",
                    "#REF",
                    "."
                ],
                [
                    "The",
                    "type",
                    "vectors",
                    "from",
                    "these",
                    "models,",
                    "which",
                    "contain",
                    "tens",
                    "of",
                    "thousands",
                    "of",
                    "types,",
                    "are",
                    "then",
                    "used",
                    "as",
                    "contextualized",
                    "entity",
                    "embeddings",
                    "in",
                    "downstream",
                    "tasks."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                3,
                0
            ]
        },
        "input": "sent0: In this work, we explore a set of interpretable entity representations that are simultaneously human and machine readable.\n sent1: The key idea of this approach is to use fine-grained entity typing models with large type inventories #REF .\n sent2: Given an entity mention and context words, our typing model outputs a highdimensional vector whose values are associated with predefined fine-grained entity types.\n sent3: Each value ranges between 0 and 1, corresponding to the confidence of the model's decision that the entity has the property given by the corresponding type.\n sent4: We use pre-trained Transformer-based entity typing models, trained either on a supervised entity typing dataset #TARGET_REF or on a distantlysupervised dataset derived from Wikipedia categories #REF .\n sent5: The type vectors from these models, which contain tens of thousands of types, are then used as contextualized entity embeddings in downstream tasks.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "(2)",
                    "The",
                    "BAE",
                    "attack",
                    "#TARGET_REF",
                    "generates",
                    "coherent",
                    "adversarial",
                    "examples",
                    "by",
                    "masking",
                    "and",
                    "replacing",
                    "words",
                    "using",
                    "BERT."
                ],
                [
                    "For",
                    "both",
                    "methods",
                    "we",
                    "use",
                    "the",
                    "implementation",
                    "provided",
                    "by",
                    "TextAttack",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                1,
                2
            ]
        },
        "input": "sent0: (2) The BAE attack #TARGET_REF generates coherent adversarial examples by masking and replacing words using BERT.\n sent1: For both methods we use the implementation provided by TextAttack #REF .\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Researchers",
                    "usually",
                    "regard",
                    "the",
                    "sentence",
                    "simplification",
                    "task",
                    "as",
                    "a",
                    "monolingual",
                    "variant",
                    "of",
                    "machine",
                    "translation",
                    "(MT)",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Benefiting",
                    "from",
                    "the",
                    "advancement",
                    "of",
                    "neural",
                    "machine",
                    "translation,",
                    "this",
                    "task",
                    "has",
                    "also",
                    "made",
                    "great",
                    "progress",
                    "in",
                    "recent",
                    "years."
                ]
            ],
            "context": [
                3,
                3
            ]
        },
        "input": "sent0: Researchers usually regard the sentence simplification task as a monolingual variant of machine translation (MT) #TARGET_REF .\n sent1: Benefiting from the advancement of neural machine translation, this task has also made great progress in recent years.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Prior",
                    "to",
                    "this",
                    "shared",
                    "task,",
                    "some",
                    "studies",
                    "have",
                    "created",
                    "datasets",
                    "for",
                    "similar",
                    "tasks",
                    "#REF",
                    "."
                ],
                [
                    "However,",
                    "one",
                    "of",
                    "them",
                    "is",
                    "created",
                    "for",
                    "publications",
                    "written",
                    "in",
                    "Japanese",
                    "#TARGET_REF",
                    ",",
                    "making",
                    "it",
                    "nearly",
                    "impossible",
                    "to",
                    "transfer",
                    "to",
                    "English",
                    "literature."
                ],
                [
                    "While",
                    "two",
                    "other",
                    "datasets",
                    "#REF",
                    "only",
                    "annotate",
                    "small-scale",
                    "golden",
                    "datasets",
                    "for",
                    "evaluation",
                    "purposes."
                ],
                [
                    "As",
                    "the",
                    "result,",
                    "no",
                    "training",
                    "data",
                    "is",
                    "available",
                    "for",
                    "training",
                    "deep",
                    "neural",
                    "network",
                    "models."
                ],
                [
                    "In",
                    "this",
                    "shared",
                    "task,",
                    "we",
                    "provide",
                    "a",
                    "large-scale",
                    "dataset",
                    "for",
                    "English",
                    "literature",
                    "that",
                    "we",
                    "believe",
                    "will",
                    "provide",
                    "enough",
                    "supervision",
                    "for",
                    "the",
                    "promising",
                    "deep",
                    "neural",
                    "network-based",
                    "models."
                ]
            ],
            "context": [
                0,
                2,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Prior to this shared task, some studies have created datasets for similar tasks #REF .\n sent1: However, one of them is created for publications written in Japanese #TARGET_REF , making it nearly impossible to transfer to English literature.\n sent2: While two other datasets #REF only annotate small-scale golden datasets for evaluation purposes.\n sent3: As the result, no training data is available for training deep neural network models.\n sent4: In this shared task, we provide a large-scale dataset for English literature that we believe will provide enough supervision for the promising deep neural network-based models.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Multi-Task",
                    "Learning",
                    "represents",
                    "a",
                    "training",
                    "strategy",
                    "where",
                    "a",
                    "shared",
                    "model",
                    "is",
                    "simultaneously",
                    "learning",
                    "multiple",
                    "tasks."
                ],
                [
                    "#REF",
                    "analysed",
                    "the",
                    "techniques",
                    "applied",
                    "in",
                    "MTL",
                    "and",
                    "compared",
                    "the",
                    "hard",
                    "parameter",
                    "sharing",
                    "and",
                    "soft",
                    "parameter",
                    "sharing",
                    "paradigms,",
                    "concluding",
                    "that",
                    "the",
                    "former",
                    "is",
                    "still",
                    "pervasive",
                    "in",
                    "nowadays",
                    "approaches."
                ],
                [
                    "MTL",
                    "proved",
                    "to",
                    "fasten",
                    "the",
                    "convergence",
                    "and",
                    "to",
                    "improve",
                    "the",
                    "model",
                    "performance",
                    "in",
                    "a",
                    "variety",
                    "of",
                    "NLP",
                    "applications,",
                    "including",
                    "named",
                    "entity",
                    "recognition",
                    "#REF",
                    ",",
                    "fake",
                    "news",
                    "detection",
                    "#REF",
                    ",",
                    "multilingual",
                    "offensive",
                    "language",
                    "identification",
                    "#REF",
                    ",",
                    "sentiment",
                    "analysis",
                    "#REF",
                    ",",
                    "humor",
                    "classification",
                    "#REF",
                    ",",
                    "recommender",
                    "systems",
                    "#REF",
                    ",",
                    "and",
                    "even",
                    "question",
                    "answering",
                    "#REF",
                    "."
                ],
                [
                    "MTL",
                    "also",
                    "increases",
                    "performance",
                    "in",
                    "conjunction",
                    "with",
                    "semi-supervised",
                    "learning",
                    "#REF",
                    ",",
                    "curriculum",
                    "learning",
                    "#REF",
                    ",",
                    "sequence-tosequence",
                    "#REF",
                    ",",
                    "reinforcement",
                    "learning",
                    "#REF",
                    ",",
                    "and",
                    "adversarial",
                    "learning",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                3,
                0,
                0,
                3
            ]
        },
        "input": "sent0: Multi-Task Learning represents a training strategy where a shared model is simultaneously learning multiple tasks.\n sent1: #REF analysed the techniques applied in MTL and compared the hard parameter sharing and soft parameter sharing paradigms, concluding that the former is still pervasive in nowadays approaches.\n sent2: MTL proved to fasten the convergence and to improve the model performance in a variety of NLP applications, including named entity recognition #REF , fake news detection #REF , multilingual offensive language identification #REF , sentiment analysis #REF , humor classification #REF , recommender systems #REF , and even question answering #REF .\n sent3: MTL also increases performance in conjunction with semi-supervised learning #REF , curriculum learning #REF , sequence-tosequence #REF , reinforcement learning #REF , and adversarial learning #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Interpretability",
                    "There",
                    "has",
                    "been",
                    "a",
                    "lot",
                    "of",
                    "work",
                    "on",
                    "better",
                    "interpreting",
                    "models'",
                    "decision",
                    "process,",
                    "e.g.,",
                    "understanding",
                    "BERT",
                    "#REF",
                    "and",
                    "attention",
                    "in",
                    "transformers",
                    "#REF",
                    ",",
                    "or",
                    "through",
                    "text",
                    "generation",
                    "models",
                    "#REF",
                    "."
                ],
                [
                    "In",
                    "this",
                    "paper",
                    "we",
                    "utilize",
                    "the",
                    "attention",
                    "scores",
                    "as",
                    "a",
                    "generic",
                    "way",
                    "to",
                    "understand",
                    "what",
                    "features",
                    "a",
                    "model",
                    "relies",
                    "on",
                    "for",
                    "making",
                    "its",
                    "predictions."
                ],
                [
                    "Other",
                    "common",
                    "model",
                    "interpretation",
                    "techniques",
                    "#REF",
                    ",",
                    "or",
                    "more",
                    "recent",
                    "work",
                    "on",
                    "hierarchical",
                    "attentions",
                    "#REF",
                    "and",
                    "contrastive",
                    "explanations",
                    "#REF",
                    ",",
                    "can",
                    "be",
                    "used",
                    "as",
                    "well."
                ],
                [
                    "In",
                    "#REF",
                    ",",
                    "the",
                    "authors",
                    "found",
                    "that",
                    "attention",
                    "scores",
                    "can",
                    "be",
                    "manipulated",
                    "to",
                    "deceive",
                    "human",
                    "decision",
                    "makers."
                ],
                [
                    "The",
                    "reliability",
                    "of",
                    "existing",
                    "interpretation",
                    "methods",
                    "is",
                    "a",
                    "research",
                    "topic",
                    "by",
                    "itself,",
                    "and",
                    "extra",
                    "care",
                    "needs",
                    "to",
                    "be",
                    "taken",
                    "when",
                    "using",
                    "attention",
                    "for",
                    "auditing",
                    "models",
                    "on",
                    "fairness",
                    "and",
                    "accountability",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Interpretability There has been a lot of work on better interpreting models' decision process, e.g., understanding BERT #REF and attention in transformers #REF , or through text generation models #REF .\n sent1: In this paper we utilize the attention scores as a generic way to understand what features a model relies on for making its predictions.\n sent2: Other common model interpretation techniques #REF , or more recent work on hierarchical attentions #REF and contrastive explanations #REF , can be used as well.\n sent3: In #REF , the authors found that attention scores can be manipulated to deceive human decision makers.\n sent4: The reliability of existing interpretation methods is a research topic by itself, and extra care needs to be taken when using attention for auditing models on fairness and accountability #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "of",
                    "regret",
                    "at",
                    "having",
                    "caused",
                    "trouble",
                    "for",
                    "someone."
                ],
                [
                    "It",
                    "has",
                    "acknowledgement",
                    "as",
                    "its",
                    "direct",
                    "hypernymy,",
                    "which",
                    "is",
                    "defined",
                    "as",
                    "a",
                    "statement",
                    "acknowledging",
                    "something",
                    "or",
                    "someone."
                ],
                [
                    "From",
                    "the",
                    "communicative",
                    "perspective",
                    "this",
                    "acknowledgment",
                    "is",
                    "a",
                    "precursor",
                    "to",
                    "the",
                    "expectation",
                    "of",
                    "some",
                    "sort",
                    "of",
                    "reparation",
                    "or",
                    "compensation",
                    "on",
                    "the",
                    "part",
                    "of",
                    "the",
                    "offended."
                ],
                [
                    "In",
                    "the",
                    "corpus,",
                    "the",
                    "apology",
                    "number",
                    "7,",
                    "has",
                    "the",
                    "sentence,",
                    "We",
                    "would",
                    "like",
                    "to",
                    "tender",
                    "an",
                    "unconditional",
                    "apology",
                    "to",
                    "the",
                    "society",
                    "at",
                    "large",
                    "and",
                    "especially",
                    "to",
                    "the",
                    "affected",
                    "families",
                    "and",
                    "to",
                    "everyone",
                    "whom",
                    "we",
                    "have",
                    "offended."
                ],
                [
                    "This",
                    "is",
                    "an",
                    "unequivocal",
                    "expression",
                    "of",
                    "apology",
                    "and",
                    "shows",
                    "that",
                    "tenderers",
                    "do",
                    "not",
                    "want",
                    "to",
                    "make",
                    "any",
                    "excuses",
                    "for",
                    "their",
                    "wrongdoing."
                ],
                [
                    "The",
                    "gloss",
                    "of",
                    "selected",
                    "sense",
                    "of",
                    "the",
                    "noun",
                    "regret",
                    "is",
                    "sadness",
                    "associated",
                    "with",
                    "some",
                    "wrong",
                    "done",
                    "or",
                    "some",
                    "disappointment."
                ],
                [
                    "The",
                    "direct",
                    "hypernymy",
                    "of",
                    "this",
                    "is",
                    "the",
                    "concept",
                    "of",
                    "sadness",
                    "which",
                    "is",
                    "emotions",
                    "experienced",
                    "when",
                    "not",
                    "in",
                    "a",
                    "state",
                    "of",
                    "well-being."
                ],
                [
                    "This",
                    "is",
                    "followed",
                    "by",
                    "the",
                    "concept",
                    "of",
                    "feeling",
                    "or",
                    "the",
                    "experiencing",
                    "of",
                    "affective",
                    "and",
                    "emotional",
                    "states."
                ],
                [
                    "Thus",
                    "the",
                    "hypernymy",
                    "relation",
                    "makes",
                    "it",
                    "clear",
                    "that",
                    "regret",
                    "is",
                    "a",
                    "kind",
                    "of",
                    "feeling",
                    "associated",
                    "with",
                    "sadness."
                ],
                [
                    "From",
                    "a",
                    "communicative",
                    "point",
                    "of",
                    "view,",
                    "it",
                    "is",
                    "simply",
                    "an",
                    "expression",
                    "of",
                    "an",
                    "emotion",
                    "on",
                    "the",
                    "part",
                    "of",
                    "the",
                    "tenderer",
                    "of",
                    "the",
                    "apology",
                    "and",
                    "not",
                    "necessarily",
                    "expression",
                    "of",
                    "remorse",
                    "or",
                    "liability."
                ],
                [
                    "For",
                    "example,",
                    "in",
                    "apology",
                    "number",
                    "13,",
                    "the",
                    "Member",
                    "of",
                    "Parliament",
                    "states,",
                    "I",
                    "write",
                    "to",
                    "convey",
                    "my",
                    "regrets",
                    "for",
                    "the",
                    "unfortunate",
                    "incident",
                    "that",
                    "took",
                    "place",
                    "on",
                    "23rd",
                    "March",
                    "2017",
                    "in",
                    "the",
                    "Air",
                    "India",
                    "flight",
                    "No."
                ],
                [
                    "AI",
                    "852,",
                    "seat",
                    "No.1F."
                ],
                [
                    "Given",
                    "that",
                    "the",
                    "writer",
                    "only",
                    "uses",
                    "the",
                    "noun",
                    "regret,",
                    "it",
                    "can",
                    "be",
                    "implied",
                    "that",
                    "the",
                    "writer",
                    "feels",
                    "sad",
                    "about",
                    "the",
                    "incident",
                    "but",
                    "not",
                    "necessarily",
                    "repentant."
                ],
                [
                    "However,",
                    "it",
                    "is",
                    "important",
                    "to",
                    "look",
                    "at",
                    "the",
                    "results",
                    "of",
                    "SentiWordNet",
                    "and",
                    "WordNet-Affect",
                    "to",
                    "understand",
                    "the",
                    "implications",
                    "and",
                    "underlying",
                    "emotions",
                    "and",
                    "sentiments",
                    "before",
                    "arriving",
                    "at",
                    "any",
                    "further",
                    "conclusions."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: of regret at having caused trouble for someone.\n sent1: It has acknowledgement as its direct hypernymy, which is defined as a statement acknowledging something or someone.\n sent2: From the communicative perspective this acknowledgment is a precursor to the expectation of some sort of reparation or compensation on the part of the offended.\n sent3: In the corpus, the apology number 7, has the sentence, We would like to tender an unconditional apology to the society at large and especially to the affected families and to everyone whom we have offended.\n sent4: This is an unequivocal expression of apology and shows that tenderers do not want to make any excuses for their wrongdoing.\n sent5: The gloss of selected sense of the noun regret is sadness associated with some wrong done or some disappointment.\n sent6: The direct hypernymy of this is the concept of sadness which is emotions experienced when not in a state of well-being.\n sent7: This is followed by the concept of feeling or the experiencing of affective and emotional states.\n sent8: Thus the hypernymy relation makes it clear that regret is a kind of feeling associated with sadness.\n sent9: From a communicative point of view, it is simply an expression of an emotion on the part of the tenderer of the apology and not necessarily expression of remorse or liability.\n sent10: For example, in apology number 13, the Member of Parliament states, I write to convey my regrets for the unfortunate incident that took place on 23rd March 2017 in the Air India flight No.\n sent11: AI 852, seat No.1F.\n sent12: Given that the writer only uses the noun regret, it can be implied that the writer feels sad about the incident but not necessarily repentant.\n sent13: However, it is important to look at the results of SentiWordNet and WordNet-Affect to understand the implications and underlying emotions and sentiments before arriving at any further conclusions.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "For",
                    "semantics,",
                    "the",
                    "situation",
                    "is",
                    "even",
                    "more",
                    "complicated."
                ],
                [
                    "While",
                    "BERT's",
                    "performance",
                    "on",
                    "natural",
                    "language",
                    "understanding",
                    "tasks",
                    "set",
                    "a",
                    "new",
                    "state",
                    "of",
                    "the",
                    "art,",
                    "more",
                    "targeted",
                    "tests",
                    "of",
                    "its",
                    "semantic",
                    "abilities",
                    "have",
                    "yielded",
                    "less",
                    "positive",
                    "results."
                ],
                [
                    "BERT",
                    "has",
                    "limited",
                    "knowledge",
                    "of",
                    "lexical",
                    "semantic",
                    "relations",
                    "such",
                    "as",
                    "hypernymy",
                    "#TARGET_REF",
                    "and",
                    "antonymy",
                    "#REF",
                    "."
                ],
                [
                    "Moreover,",
                    "it",
                    "has",
                    "fragile",
                    "representations",
                    "of",
                    "named",
                    "entities",
                    "#REF",
                    ",",
                    "and",
                    "imprecise",
                    "representations",
                    "of",
                    "numbers",
                    "#REF",
                    "."
                ],
                [
                    "These",
                    "flaws",
                    "comprise",
                    "specific",
                    "linguistic",
                    "phenomena",
                    "that",
                    "BERTScore,",
                    "due",
                    "to",
                    "its",
                    "use",
                    "of",
                    "BERT,",
                    "might",
                    "be",
                    "unable",
                    "to",
                    "handle,",
                    "and",
                    "thus",
                    "merit",
                    "investigation."
                ]
            ],
            "context": [
                0,
                0,
                1,
                3,
                3
            ]
        },
        "input": "sent0: For semantics, the situation is even more complicated.\n sent1: While BERT's performance on natural language understanding tasks set a new state of the art, more targeted tests of its semantic abilities have yielded less positive results.\n sent2: BERT has limited knowledge of lexical semantic relations such as hypernymy #TARGET_REF and antonymy #REF .\n sent3: Moreover, it has fragile representations of named entities #REF , and imprecise representations of numbers #REF .\n sent4: These flaws comprise specific linguistic phenomena that BERTScore, due to its use of BERT, might be unable to handle, and thus merit investigation.\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "then",
                    "extract",
                    "attention",
                    "scores",
                    "{a",
                    "1",
                    "i",
                    ",",
                    "a",
                    "2",
                    "i",
                    ",",
                    "."
                ],
                [
                    "."
                ],
                [
                    "."
                ],
                [
                    ",",
                    "a",
                    "m",
                    "i",
                    "}",
                    "for",
                    "tokens",
                    "{t",
                    "1",
                    "i",
                    ",",
                    "t",
                    "2",
                    "i",
                    ",",
                    "."
                ],
                [
                    "."
                ],
                [
                    "."
                ],
                [
                    ",",
                    "t",
                    "m",
                    "i",
                    "}",
                    "in",
                    "sentence",
                    "s",
                    "i",
                    ",",
                    "where",
                    "m",
                    "is",
                    "the",
                    "length",
                    "of",
                    "the",
                    "sentence."
                ],
                [
                    "In",
                    "BERT-based",
                    "classification",
                    "models,",
                    "the",
                    "embedding",
                    "of",
                    "#TARGET_REF",
                    "token",
                    "in",
                    "the",
                    "final",
                    "layer",
                    "is",
                    "fed",
                    "to",
                    "a",
                    "classification",
                    "layer."
                ],
                [
                    "We",
                    "thus",
                    "extract",
                    "the",
                    "attention",
                    "scores",
                    "of",
                    "each",
                    "token",
                    "t",
                    "used",
                    "for",
                    "computing",
                    "the",
                    "embedding",
                    "of",
                    "the",
                    "[CLS]",
                    "token",
                    "and",
                    "average",
                    "them",
                    "across",
                    "different",
                    "heads."
                ],
                [
                    "If",
                    "p",
                    "pos",
                    "i",
                    "&gt,",
                    "p",
                    "neg",
                    "i",
                    ",",
                    "we",
                    "obtain",
                    "the",
                    "updated",
                    "attention",
                    "score",
                    "ãj",
                    "i",
                    "=",
                    "a",
                    "j",
                    "i",
                    "*",
                    "p",
                    "pos",
                    "i",
                    ",",
                    "otherwiseãj",
                    "i",
                    "=",
                    "−a",
                    "j",
                    "i",
                    "*",
                    "p",
                    "neg",
                    "i",
                    ".For",
                    "each",
                    "token",
                    "t",
                    "in",
                    "the",
                    "vocabulary",
                    "V,",
                    "we",
                    "compute",
                    "the",
                    "average",
                    "attention",
                    "score:āt",
                    "=",
                    "1",
                    "mn",
                    "•",
                    "Σ",
                    "n",
                    "i=1",
                    "Σ",
                    "m",
                    "j=1",
                    "[ã",
                    "j",
                    "i",
                    "•",
                    "1(t",
                    "j",
                    "i",
                    "=",
                    "t)],where",
                    "we",
                    "aggregate",
                    "the",
                    "attention",
                    "scores",
                    "ãj",
                    "i",
                    "for",
                    "token",
                    "t,",
                    "across",
                    "all",
                    "n",
                    "sentences",
                    "in",
                    "the",
                    "corpus."
                ],
                [
                    "We",
                    "then",
                    "normalize",
                    "the",
                    "attention",
                    "scores",
                    "across",
                    "the",
                    "vocabulary",
                    "to",
                    "obtain",
                    "the",
                    "importance",
                    "score",
                    "for",
                    "each",
                    "token",
                    "t:",
                    "I",
                    "t",
                    "=",
                    "āt",
                    "/Σ",
                    "t∈V",
                    "āt",
                    "."
                ],
                [
                    "This",
                    "can",
                    "lead",
                    "to",
                    "very",
                    "small",
                    "I",
                    "t",
                    "for",
                    "certain",
                    "tokens,",
                    "thus",
                    "we",
                    "take",
                    "the",
                    "log",
                    "of",
                    "all",
                    "importance",
                    "scores",
                    "to",
                    "avoid",
                    "underflow,",
                    "I",
                    "′",
                    "t",
                    "=",
                    "log(I",
                    "t",
                    ")."
                ],
                [
                    "So",
                    "far,",
                    "we",
                    "have",
                    "computed",
                    "the",
                    "importance",
                    "score",
                    "for",
                    "each",
                    "token."
                ],
                [
                    "However,",
                    "we",
                    "observe",
                    "that",
                    "some",
                    "tokens",
                    "appearing",
                    "only",
                    "very",
                    "a",
                    "few",
                    "times",
                    "could",
                    "accidentally",
                    "have",
                    "very",
                    "high",
                    "importance",
                    "scores."
                ],
                [
                    "Thus,",
                    "we",
                    "propose",
                    "to",
                    "penalize",
                    "the",
                    "tokens",
                    "with",
                    "low",
                    "frequencies:Ît",
                    "=",
                    "I",
                    "′",
                    "t",
                    "−",
                    "λ/",
                    "log(1",
                    "+",
                    "c",
                    "t",
                    "),",
                    "where",
                    "c",
                    "t",
                    "is",
                    "the",
                    "frequency",
                    "of",
                    "token",
                    "t",
                    "and",
                    "λ",
                    "is",
                    "a",
                    "temperature",
                    "parameter",
                    "to",
                    "adjust",
                    "the",
                    "degree",
                    "that",
                    "we",
                    "want",
                    "to",
                    "penalize",
                    "over",
                    "the",
                    "frequency."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: then extract attention scores {a 1 i , a 2 i , .\n sent1: .\n sent2: .\n sent3: , a m i } for tokens {t 1 i , t 2 i , .\n sent4: .\n sent5: .\n sent6: , t m i } in sentence s i , where m is the length of the sentence.\n sent7: In BERT-based classification models, the embedding of #TARGET_REF token in the final layer is fed to a classification layer.\n sent8: We thus extract the attention scores of each token t used for computing the embedding of the [CLS] token and average them across different heads.\n sent9: If p pos i &gt, p neg i , we obtain the updated attention score ãj i = a j i * p pos i , otherwiseãj i = −a j i * p neg i .For each token t in the vocabulary V, we compute the average attention score:āt = 1 mn • Σ n i=1 Σ m j=1 [ã j i • 1(t j i = t)],where we aggregate the attention scores ãj i for token t, across all n sentences in the corpus.\n sent10: We then normalize the attention scores across the vocabulary to obtain the importance score for each token t: I t = āt /Σ t∈V āt .\n sent11: This can lead to very small I t for certain tokens, thus we take the log of all importance scores to avoid underflow, I ′ t = log(I t ).\n sent12: So far, we have computed the importance score for each token.\n sent13: However, we observe that some tokens appearing only very a few times could accidentally have very high importance scores.\n sent14: Thus, we propose to penalize the tokens with low frequencies:Ît = I ′ t − λ/ log(1 + c t ), where c t is the frequency of token t and λ is a temperature parameter to adjust the degree that we want to penalize over the frequency.\n",
        "output": "{\"INFORMATION\": [\"sent7\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "utilised",
                    "the",
                    "ANLT",
                    "metagrammatical",
                    "formalism",
                    "to",
                    "develop",
                    "a",
                    "feature-based,",
                    "declara",
                    "tive",
                    "description",
                    "of",
                    "PoS",
                    "label",
                    "sequences",
                    "for",
                    "English."
                ],
                [
                    "This",
                    "grammar",
                    "compiles",
                    "into",
                    "a",
                    "DCG-like",
                    "grammar",
                    "of",
                    "approximately",
                    "400",
                    "rules."
                ],
                [
                    "It",
                    "has",
                    "been",
                    "designed",
                    "to",
                    "enumerate",
                    "possible",
                    "valencies",
                    "for",
                    "predicates",
                    "(verbs,",
                    "adj",
                    "ectives",
                    "and",
                    "nouns)",
                    "by",
                    "including",
                    "separate",
                    "rules",
                    "for",
                    "each",
                    "pattern",
                    "of",
                    "possible",
                    "complementation",
                    "in",
                    "English."
                ],
                [
                    "The",
                    "distinction",
                    "between",
                    "arguments",
                    "and",
                    "adj",
                    "uncts",
                    "is",
                    "expressed,",
                    "following",
                    "X-bar",
                    "theory",
                    "(e.g."
                ],
                [
                    "#TARGET_REF",
                    ",",
                    "by",
                    "Chomsky-adjunction",
                    "of",
                    "adjuncts",
                    "to",
                    "maximal",
                    "projections",
                    "{XP",
                    "�",
                    "XP",
                    "Adjunct)",
                    "as",
                    "opposed",
                    "to",
                    "government",
                    "of",
                    "arguments",
                    "(i.e."
                ],
                [
                    "arguments",
                    "are",
                    "sisters",
                    "within",
                    "Xl",
                    "projections,",
                    "XI",
                    "�",
                    "XO",
                    "Argl."
                ],
                [
                    "..",
                    "ArgN)",
                    "."
                ],
                [
                    "Although",
                    "the",
                    "grammar",
                    "enumerates",
                    "complementation",
                    "possibilities",
                    "and",
                    "checks",
                    "for",
                    "global",
                    "sentential",
                    "well-formedness,",
                    "it",
                    "is",
                    "best",
                    "de",
                    "scribed",
                    "as",
                    "'intermediate'",
                    "as",
                    "it",
                    "does",
                    "not",
                    "attempt",
                    "to",
                    "associate",
                    "'displaced'",
                    "constituents",
                    "with",
                    "their",
                    "canonical",
                    "position",
                    "/",
                    "grammatical",
                    "role."
                ]
            ],
            "context": [
                0,
                0,
                0,
                3,
                3,
                3,
                3,
                0
            ]
        },
        "input": "sent0: We utilised the ANLT metagrammatical formalism to develop a feature-based, declara tive description of PoS label sequences for English.\n sent1: This grammar compiles into a DCG-like grammar of approximately 400 rules.\n sent2: It has been designed to enumerate possible valencies for predicates (verbs, adj ectives and nouns) by including separate rules for each pattern of possible complementation in English.\n sent3: The distinction between arguments and adj uncts is expressed, following X-bar theory (e.g.\n sent4: #TARGET_REF , by Chomsky-adjunction of adjuncts to maximal projections {XP � XP Adjunct) as opposed to government of arguments (i.e.\n sent5: arguments are sisters within Xl projections, XI � XO Argl.\n sent6: .. ArgN) .\n sent7: Although the grammar enumerates complementation possibilities and checks for global sentential well-formedness, it is best de scribed as 'intermediate' as it does not attempt to associate 'displaced' constituents with their canonical position / grammatical role.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent3\", \"sent4\", \"sent5\", \"sent6\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "interaction",
                    "layer",
                    "is",
                    "the",
                    "core",
                    "element",
                    "of",
                    "the",
                    "architecture",
                    "for",
                    "which",
                    "several",
                    "kinds",
                    "attention",
                    "mechanisms",
                    "has",
                    "been",
                    "developed",
                    "to",
                    "improve",
                    "the",
                    "QA",
                    "matching",
                    "process",
                    "such",
                    "as",
                    "bi-attention",
                    "#REF",
                    ",",
                    "co-attention",
                    "#REF",
                    "or",
                    "re-attention",
                    "#TARGET_REF",
                    ",",
                    "to",
                    "name",
                    "just",
                    "a",
                    "few."
                ]
            ],
            "context": [
                2
            ]
        },
        "input": "sent0: The interaction layer is the core element of the architecture for which several kinds attention mechanisms has been developed to improve the QA matching process such as bi-attention #REF , co-attention #REF or re-attention #TARGET_REF , to name just a few.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Our",
                    "model",
                    "achieved",
                    "the",
                    "highest",
                    "score",
                    "for",
                    "subtask",
                    "1b",
                    "(i.e.,",
                    "adverse",
                    "effect",
                    "span",
                    "detection)",
                    "with",
                    "an",
                    "F",
                    "1",
                    "-score",
                    "of",
                    "51%,",
                    "arguing",
                    "that",
                    "MTL",
                    "can",
                    "enhance",
                    "adverse",
                    "effect",
                    "extraction",
                    "from",
                    "social",
                    "media",
                    "posts."
                ],
                [
                    "In",
                    "terms",
                    "of",
                    "future",
                    "work,",
                    "adversarial",
                    "training",
                    "#TARGET_REF",
                    "will",
                    "be",
                    "considered",
                    "to",
                    "improve",
                    "the",
                    "robustness",
                    "of",
                    "our",
                    "approach."
                ]
            ],
            "context": [
                3,
                2
            ]
        },
        "input": "sent0: Our model achieved the highest score for subtask 1b (i.e., adverse effect span detection) with an F 1 -score of 51%, arguing that MTL can enhance adverse effect extraction from social media posts.\n sent1: In terms of future work, adversarial training #TARGET_REF will be considered to improve the robustness of our approach.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Bidirectional",
                    "Encoder",
                    "Representations",
                    "from",
                    "Transformers",
                    "(BERT)",
                    "#TARGET_REF",
                    "is",
                    "a",
                    "deep",
                    "learning",
                    "model",
                    "for",
                    "general-purpose",
                    "language",
                    "representations."
                ],
                [
                    "BERT",
                    "is",
                    "often",
                    "used",
                    "as",
                    "the",
                    "backbone",
                    "model",
                    "for",
                    "several",
                    "NLP",
                    "tasks",
                    "like",
                    "semantic",
                    "analysis,",
                    "question",
                    "answering,",
                    "and",
                    "named",
                    "entity",
                    "recognition."
                ],
                [
                    "The",
                    "bidirectional",
                    "transformer",
                    "used",
                    "in",
                    "BERT",
                    "has",
                    "a",
                    "deeper",
                    "sense",
                    "of",
                    "language",
                    "context",
                    "and",
                    "generates",
                    "intricate",
                    "semantic",
                    "feature",
                    "representations."
                ],
                [
                    "These",
                    "representations",
                    "are",
                    "learned",
                    "through",
                    "a",
                    "pre-training",
                    "step",
                    "using",
                    "Next",
                    "Sentence",
                    "Prediction",
                    "(NSP)",
                    "and",
                    "Masked",
                    "Language",
                    "Modelling",
                    "(MLM)",
                    "as",
                    "pretext",
                    "tasks",
                    "and",
                    "transferred",
                    "to",
                    "the",
                    "downstream",
                    "NLP",
                    "tasks."
                ],
                [
                    "The",
                    "goal",
                    "of",
                    "the",
                    "Next",
                    "Sentence",
                    "Prediction",
                    "task",
                    "is",
                    "to",
                    "identify",
                    "whether",
                    "the",
                    "two",
                    "input",
                    "sentences",
                    "are",
                    "consecutive",
                    "or",
                    "not."
                ],
                [
                    "In",
                    "Masked",
                    "Language",
                    "Modelling,",
                    "BERT",
                    "is",
                    "trained",
                    "to",
                    "predict",
                    "randomly",
                    "masked",
                    "words",
                    "in",
                    "a",
                    "sentence."
                ],
                [
                    "The",
                    "Transformer",
                    "network",
                    "receives",
                    "a",
                    "sequence",
                    "of",
                    "tokens",
                    "as",
                    "input",
                    "and",
                    "utilizes",
                    "the",
                    "attention",
                    "mechanism",
                    "to",
                    "learn",
                    "the",
                    "contextual",
                    "relationships",
                    "between",
                    "words",
                    "in",
                    "a",
                    "text."
                ],
                [
                    "These",
                    "relationships",
                    "can",
                    "then",
                    "be",
                    "used",
                    "to",
                    "extract",
                    "high-quality"
                ]
            ],
            "context": [
                1,
                3,
                3,
                0,
                0,
                3,
                0,
                0
            ]
        },
        "input": "sent0: Bidirectional Encoder Representations from Transformers (BERT) #TARGET_REF is a deep learning model for general-purpose language representations.\n sent1: BERT is often used as the backbone model for several NLP tasks like semantic analysis, question answering, and named entity recognition.\n sent2: The bidirectional transformer used in BERT has a deeper sense of language context and generates intricate semantic feature representations.\n sent3: These representations are learned through a pre-training step using Next Sentence Prediction (NSP) and Masked Language Modelling (MLM) as pretext tasks and transferred to the downstream NLP tasks.\n sent4: The goal of the Next Sentence Prediction task is to identify whether the two input sentences are consecutive or not.\n sent5: In Masked Language Modelling, BERT is trained to predict randomly masked words in a sentence.\n sent6: The Transformer network receives a sequence of tokens as input and utilizes the attention mechanism to learn the contextual relationships between words in a text.\n sent7: These relationships can then be used to extract high-quality\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\", \"sent2\", \"sent5\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Although",
                    "the",
                    "canonical",
                    "way",
                    "to",
                    "represent",
                    "words",
                    "is",
                    "to",
                    "assign",
                    "them",
                    "to",
                    "vectors,",
                    "if",
                    "the",
                    "goal",
                    "is",
                    "to",
                    "model",
                    "connections",
                    "between",
                    "words,",
                    "a",
                    "graph",
                    "structure",
                    "is",
                    "at",
                    "least",
                    "as",
                    "suitable."
                ],
                [
                    "When",
                    "each",
                    "word",
                    "is",
                    "represented",
                    "by",
                    "a",
                    "vector,",
                    "the",
                    "similarity",
                    "between",
                    "them",
                    "is",
                    "most",
                    "often",
                    "calculated",
                    "as",
                    "the",
                    "cosine",
                    "of",
                    "the",
                    "angle",
                    "of",
                    "the",
                    "two",
                    "vectors."
                ],
                [
                    "In",
                    "the",
                    "case",
                    "of",
                    "graph",
                    "representations,",
                    "all",
                    "words",
                    "in",
                    "the",
                    "dictionary",
                    "correspond",
                    "to",
                    "the",
                    "vertices",
                    "of",
                    "a",
                    "large",
                    "graph,",
                    "and",
                    "the",
                    "distance",
                    "between",
                    "them",
                    "can",
                    "be",
                    "defined",
                    "in",
                    "many",
                    "ways",
                    "depending",
                    "on",
                    "the",
                    "graph."
                ],
                [
                    "One",
                    "option",
                    "is",
                    "the",
                    "length",
                    "or",
                    "weight",
                    "of",
                    "the",
                    "shortest",
                    "path",
                    "between",
                    "the",
                    "two",
                    "vertices."
                ],
                [
                    "Knowledge",
                    "graphs",
                    "#TARGET_REF",
                    "were",
                    "already",
                    "used",
                    "to",
                    "model",
                    "word",
                    "connections",
                    "in",
                    "previous",
                    "Codenames",
                    "agents,",
                    "but",
                    "other",
                    "types",
                    "of",
                    "language",
                    "graphs",
                    "also",
                    "exist,",
                    "which",
                    "could",
                    "be",
                    "utilized",
                    "for",
                    "this",
                    "task",
                    "as",
                    "well."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                3
            ]
        },
        "input": "sent0: Although the canonical way to represent words is to assign them to vectors, if the goal is to model connections between words, a graph structure is at least as suitable.\n sent1: When each word is represented by a vector, the similarity between them is most often calculated as the cosine of the angle of the two vectors.\n sent2: In the case of graph representations, all words in the dictionary correspond to the vertices of a large graph, and the distance between them can be defined in many ways depending on the graph.\n sent3: One option is the length or weight of the shortest path between the two vertices.\n sent4: Knowledge graphs #TARGET_REF were already used to model word connections in previous Codenames agents, but other types of language graphs also exist, which could be utilized for this task as well.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "all",
                    "configurations,",
                    "the",
                    "performance",
                    "in",
                    "terms",
                    "of",
                    "EM",
                    "and",
                    "F1",
                    "on",
                    "PIAF",
                    "remains",
                    "significantly",
                    "lower",
                    "than",
                    "that",
                    "obtained",
                    "on",
                    "FQuAD",
                    "since",
                    "the",
                    "PIAF",
                    "corpus",
                    "does",
                    "not",
                    "include",
                    "multiple",
                    "responses",
                    "as",
                    "pointed",
                    "out",
                    "by",
                    "d",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Unsurprisingly,",
                    "PIAF",
                    "dev",
                    "offer",
                    "a",
                    "more",
                    "challenging",
                    "evaluation",
                    "set,",
                    "where",
                    "the",
                    "answer",
                    "extraction",
                    "performance",
                    "are",
                    "lower."
                ],
                [
                    "Indeed,",
                    "the",
                    "corpus",
                    "is",
                    "more",
                    "diversified",
                    "with",
                    "questions",
                    "on",
                    "191",
                    "different",
                    "Wikipedia",
                    "articles,",
                    "whereas",
                    "on",
                    "FQuAD",
                    "dev",
                    "it",
                    "only",
                    "covers",
                    "18."
                ]
            ],
            "context": [
                3,
                0,
                0
            ]
        },
        "input": "sent0: In all configurations, the performance in terms of EM and F1 on PIAF remains significantly lower than that obtained on FQuAD since the PIAF corpus does not include multiple responses as pointed out by d #TARGET_REF .\n sent1: Unsurprisingly, PIAF dev offer a more challenging evaluation set, where the answer extraction performance are lower.\n sent2: Indeed, the corpus is more diversified with questions on 191 different Wikipedia articles, whereas on FQuAD dev it only covers 18.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "require",
                    "aligned",
                    "parallel",
                    "text",
                    "to",
                    "fuel",
                    "our",
                    "methods",
                    "and",
                    "we",
                    "relied",
                    "on",
                    "the",
                    "Europarl",
                    "corpus",
                    "#TARGET_REF",
                    "which",
                    "is",
                    "comprised",
                    "of",
                    "EU",
                    "parliamentary",
                    "oration",
                    "that",
                    "has",
                    "been",
                    "manually",
                    "translated",
                    "into",
                    "other",
                    "EU",
                    "languages."
                ],
                [
                    "There",
                    "is",
                    "approximately",
                    "160",
                    "MB",
                    "of",
                    "text",
                    "(about",
                    "24",
                    "million",
                    "words)",
                    "per",
                    "language."
                ]
            ],
            "context": [
                1,
                1
            ]
        },
        "input": "sent0: We require aligned parallel text to fuel our methods and we relied on the Europarl corpus #TARGET_REF which is comprised of EU parliamentary oration that has been manually translated into other EU languages.\n sent1: There is approximately 160 MB of text (about 24 million words) per language.\n",
        "output": "{\"INFORMATION\": [\"sent0\", \"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Pre-training."
                ],
                [
                    "We",
                    "test",
                    "two",
                    "model",
                    "types,",
                    "drawing",
                    "from",
                    "#TARGET_REF",
                    ",",
                    "to",
                    "determine",
                    "if",
                    "pre-training",
                    "effects",
                    "curriculums",
                    "learning."
                ]
            ],
            "context": [
                0,
                2
            ]
        },
        "input": "sent0: Pre-training.\n sent1: We test two model types, drawing from #TARGET_REF , to determine if pre-training effects curriculums learning.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "these",
                    "experiments",
                    "we",
                    "have",
                    "compared",
                    "the",
                    "sentiment-conditioned",
                    "text",
                    "generation",
                    "of",
                    "CTERM-GAN",
                    "with",
                    "that",
                    "of",
                    "SeqGAN",
                    "#REF",
                    ",",
                    "SentiGAN",
                    "#REF",
                    "and",
                    "an",
                    "RNNLM",
                    "baseline",
                    "#REF",
                    "."
                ],
                [
                    "Following",
                    "the",
                    "experiments",
                    "carried",
                    "out",
                    "in",
                    "#TARGET_REF",
                    ",",
                    "the",
                    "conditioning",
                    "has",
                    "been",
                    "performed",
                    "based",
                    "on",
                    "only",
                    "two",
                    "sentiments,",
                    "positive",
                    "or",
                    "negative."
                ],
                [
                    "In",
                    "this",
                    "case,",
                    "the",
                    "conditioning",
                    "vector,",
                    "c,",
                    "taken",
                    "as",
                    "input",
                    "by",
                    "CTERM-GAN",
                    "is",
                    "a",
                    "one-hot",
                    "binary",
                    "variable",
                    "representing",
                    "the",
                    "desired",
                    "sentiment."
                ],
                [
                    "The",
                    "RNNLM",
                    "and",
                    "SeqGAN",
                    "models",
                    "have",
                    "been",
                    "trained",
                    "separately",
                    "on",
                    "the",
                    "two",
                    "sentiments",
                    "by",
                    "treating",
                    "positive",
                    "and",
                    "negative",
                    "sentences",
                    "as",
                    "two",
                    "separate",
                    "datasets,",
                    "while",
                    "SentiGAN",
                    "and",
                    "the",
                    "proposed",
                    "model",
                    "have",
                    "been",
                    "trained",
                    "jointly."
                ],
                [
                    "This",
                    "procedure",
                    "makes",
                    "the",
                    "results",
                    "comparable",
                    "although",
                    "it",
                    "is",
                    "clear",
                    "how",
                    "the",
                    "flexibility",
                    "of",
                    "SentiGAN",
                    "and",
                    "CTERM-GAN",
                    "makes",
                    "these",
                    "models",
                    "more",
                    "general."
                ],
                [
                    "Dataset",
                    "We",
                    "have",
                    "used",
                    "two",
                    "datasets,",
                    "Movie",
                    "Reviews",
                    "(MR)",
                    "#REF",
                    "and",
                    "Customer",
                    "Reviews",
                    "(CR)",
                    "#REF",
                    ",",
                    "where",
                    "individual",
                    "sentences",
                    "are",
                    "annotated",
                    "as",
                    "either",
                    "positive",
                    "or",
                    "negative."
                ],
                [
                    "The",
                    "Movie",
                    "Reviews",
                    "dataset",
                    "consists",
                    "of",
                    "user",
                    "reviews",
                    "of",
                    "movies,",
                    "with",
                    "2,",
                    "133",
                    "positive",
                    "and",
                    "2,",
                    "370",
                    "negative",
                    "sentences."
                ],
                [
                    "The",
                    "Customer",
                    "Reviews",
                    "dataset",
                    "consists",
                    "of",
                    "1,",
                    "500",
                    "reviews",
                    "of",
                    "products",
                    "sold",
                    "online,",
                    "with",
                    "positive/negative",
                    "annotation",
                    "at",
                    "sentence",
                    "level."
                ],
                [
                    "For",
                    "this",
                    "task,",
                    "only",
                    "sentences",
                    "of",
                    "length",
                    "shorter",
                    "than",
                    "15",
                    "words",
                    "have",
                    "been",
                    "retained,",
                    "to",
                    "be",
                    "able",
                    "to",
                    "use",
                    "the",
                    "same",
                    "preprocessing",
                    "as",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                0,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: In these experiments we have compared the sentiment-conditioned text generation of CTERM-GAN with that of SeqGAN #REF , SentiGAN #REF and an RNNLM baseline #REF .\n sent1: Following the experiments carried out in #TARGET_REF , the conditioning has been performed based on only two sentiments, positive or negative.\n sent2: In this case, the conditioning vector, c, taken as input by CTERM-GAN is a one-hot binary variable representing the desired sentiment.\n sent3: The RNNLM and SeqGAN models have been trained separately on the two sentiments by treating positive and negative sentences as two separate datasets, while SentiGAN and the proposed model have been trained jointly.\n sent4: This procedure makes the results comparable although it is clear how the flexibility of SentiGAN and CTERM-GAN makes these models more general.\n sent5: Dataset We have used two datasets, Movie Reviews (MR) #REF and Customer Reviews (CR) #REF , where individual sentences are annotated as either positive or negative.\n sent6: The Movie Reviews dataset consists of user reviews of movies, with 2, 133 positive and 2, 370 negative sentences.\n sent7: The Customer Reviews dataset consists of 1, 500 reviews of products sold online, with positive/negative annotation at sentence level.\n sent8: For this task, only sentences of length shorter than 15 words have been retained, to be able to use the same preprocessing as #REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Another",
                    "approach",
                    "consists",
                    "of",
                    "translating",
                    "the",
                    "QA",
                    "triples",
                    "of",
                    "the",
                    "target",
                    "domain",
                    "into",
                    "the",
                    "source",
                    "domain,",
                    "so",
                    "the",
                    "model",
                    "trained",
                    "on",
                    "the",
                    "source",
                    "language",
                    "can",
                    "be",
                    "directly",
                    "applied",
                    "on",
                    "the",
                    "translated",
                    "target",
                    "language",
                    "testing",
                    "data."
                ],
                [
                    "As",
                    "an",
                    "exemple,",
                    "#TARGET_REF",
                    "'s",
                    "method",
                    "consisted",
                    "of",
                    "combining",
                    "the",
                    "alignment",
                    "attention",
                    "scores",
                    "from",
                    "a",
                    "MT",
                    "model",
                    "with",
                    "an",
                    "English",
                    "QA",
                    "model",
                    "to",
                    "guide",
                    "the",
                    "answer",
                    "extraction",
                    "process."
                ]
            ],
            "context": [
                0,
                1
            ]
        },
        "input": "sent0: Another approach consists of translating the QA triples of the target domain into the source domain, so the model trained on the source language can be directly applied on the translated target language testing data.\n sent1: As an exemple, #TARGET_REF 's method consisted of combining the alignment attention scores from a MT model with an English QA model to guide the answer extraction process.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Two",
                    "current",
                    "approaches",
                    "to",
                    "English",
                    "verb",
                    "classifications",
                    "are",
                    "WordNet",
                    "synonym",
                    "sets",
                    "#TARGET_REF",
                    "and",
                    "Levin",
                    "classes",
                    "#REF",
                    "."
                ],
                [
                    "WordNet",
                    "is",
                    "an",
                    "on-line",
                    "lexical",
                    "database",
                    "of",
                    "English",
                    "that",
                    "currently",
                    "contains",
                    "about",
                    "120,000",
                    "sets",
                    "of",
                    "noun,",
                    "verb",
                    "adjective,",
                    "and",
                    "adverb",
                    "synonyms,",
                    "each",
                    "representing",
                    "a",
                    "lexicalized",
                    "concept."
                ],
                [
                    "A",
                    "synset",
                    "(synonym",
                    "set)",
                    "contains",
                    "besides",
                    "all",
                    "the",
                    "word",
                    "forms",
                    "that",
                    "can",
                    "refer",
                    "to",
                    "a",
                    "given",
                    "concept,",
                    "a",
                    "definitional",
                    "gloss",
                    "and",
                    "-in",
                    "most",
                    "cases",
                    "-an",
                    "example",
                    "sentence."
                ],
                [
                    "Words",
                    "and",
                    "synsets",
                    "are",
                    "interrelated",
                    "by",
                    "means",
                    "of",
                    "lexical",
                    "and",
                    "semantic-conceptual",
                    "links,",
                    "respectively."
                ],
                [
                    "Antonymy",
                    "or",
                    "semantic",
                    "opposition",
                    "links",
                    "individual",
                    "words,",
                    "while",
                    "the",
                    "super-/subordinate",
                    "relation",
                    "links",
                    "entire",
                    "synsets."
                ],
                [
                    "WordNet",
                    "was",
                    "designed",
                    "principally",
                    "as",
                    "a",
                    "semantic",
                    "network,",
                    "and",
                    "contains",
                    "little",
                    "syntactic",
                    "information."
                ]
            ],
            "context": [
                2,
                3,
                0,
                0,
                0,
                3
            ]
        },
        "input": "sent0: Two current approaches to English verb classifications are WordNet synonym sets #TARGET_REF and Levin classes #REF .\n sent1: WordNet is an on-line lexical database of English that currently contains about 120,000 sets of noun, verb adjective, and adverb synonyms, each representing a lexicalized concept.\n sent2: A synset (synonym set) contains besides all the word forms that can refer to a given concept, a definitional gloss and -in most cases -an example sentence.\n sent3: Words and synsets are interrelated by means of lexical and semantic-conceptual links, respectively.\n sent4: Antonymy or semantic opposition links individual words, while the super-/subordinate relation links entire synsets.\n sent5: WordNet was designed principally as a semantic network, and contains little syntactic information.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": [\"sent1\", \"sent5\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Therefore,",
                    "we",
                    "additionally",
                    "use",
                    "two",
                    "distantly",
                    "labeled",
                    "entity",
                    "typing",
                    "datasets",
                    "derived",
                    "from",
                    "Wikipedia."
                ],
                [
                    "We",
                    "leverage",
                    "past",
                    "work",
                    "in",
                    "using",
                    "types",
                    "derived",
                    "from",
                    "Wikipedia",
                    "categories",
                    "#TARGET_REF",
                    ",",
                    "which",
                    "contain",
                    "type",
                    "information",
                    "and",
                    "are",
                    "widely",
                    "annotated",
                    "across",
                    "Wikipedia",
                    "articles."
                ],
                [
                    "We",
                    "select",
                    "the",
                    "appropriate",
                    "dataset",
                    "for",
                    "each",
                    "setting",
                    "depending",
                    "on",
                    "task-specific",
                    "requirements",
                    "(see",
                    "Section",
                    "6)."
                ],
                [
                    "For",
                    "all",
                    "datasets,",
                    "we",
                    "compute",
                    "entity",
                    "typing",
                    "macro",
                    "F1",
                    "using",
                    "development",
                    "examples",
                    "(1k)",
                    "to",
                    "check",
                    "model",
                    "convergence."
                ]
            ],
            "context": [
                3,
                1,
                3,
                0
            ]
        },
        "input": "sent0: Therefore, we additionally use two distantly labeled entity typing datasets derived from Wikipedia.\n sent1: We leverage past work in using types derived from Wikipedia categories #TARGET_REF , which contain type information and are widely annotated across Wikipedia articles.\n sent2: We select the appropriate dataset for each setting depending on task-specific requirements (see Section 6).\n sent3: For all datasets, we compute entity typing macro F1 using development examples (1k) to check model convergence.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "further",
                    "ascertain",
                    "the",
                    "findings",
                    "of",
                    "§4.1",
                    "via",
                    "language",
                    "modeling",
                    "on",
                    "WikiText-103",
                    "#TARGET_REF",
                    "using",
                    "a",
                    "6-layer",
                    "Transformer",
                    "decoder",
                    "with",
                    "156M",
                    "parameters."
                ],
                [
                    "Using",
                    "an",
                    "input",
                    "length",
                    "of",
                    "1024,",
                    "we",
                    "trained",
                    "two",
                    "models",
                    "with",
                    "vanilla",
                    "and",
                    "top-64",
                    "attentions",
                    "at",
                    "the",
                    "self-attention",
                    "layers,",
                    "obtaining",
                    "test",
                    "perplexity",
                    "scores",
                    "of",
                    "30.96",
                    "and",
                    "30.51",
                    "respectively,",
                    "slightly",
                    "better",
                    "in",
                    "case",
                    "of",
                    "top-64",
                    "(details",
                    "in",
                    "§A.3)."
                ]
            ],
            "context": [
                2,
                0
            ]
        },
        "input": "sent0: We further ascertain the findings of §4.1 via language modeling on WikiText-103 #TARGET_REF using a 6-layer Transformer decoder with 156M parameters.\n sent1: Using an input length of 1024, we trained two models with vanilla and top-64 attentions at the self-attention layers, obtaining test perplexity scores of 30.96 and 30.51 respectively, slightly better in case of top-64 (details in §A.3).\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "2",
                    "As",
                    "pointed",
                    "out",
                    "by",
                    "a",
                    "reviewer,",
                    "an",
                    "even",
                    "better",
                    "factorization",
                    "of",
                    "fragments",
                    "could",
                    "potentially",
                    "be",
                    "achieved",
                    "by",
                    "indexing",
                    "not",
                    "with",
                    "respect",
                    "to",
                    "linear",
                    "position",
                    "but",
                    "with",
                    "respect",
                    "to",
                    "the",
                    "syntactic",
                    "head",
                    "word."
                ],
                [
                    "This",
                    "would",
                    "require",
                    "introducing",
                    "a",
                    "dependency",
                    "parsing",
                    "component."
                ],
                [
                    "We",
                    "leave",
                    "this",
                    "for",
                    "future",
                    "work."
                ],
                [
                    "Sequence",
                    "Labeling",
                    "Transformer",
                    "Model",
                    "Our",
                    "model",
                    "is",
                    "schematically",
                    "depicted",
                    "in",
                    "Figure",
                    "4."
                ],
                [
                    "It",
                    "takes",
                    "an",
                    "input",
                    "sequence",
                    "of",
                    "tokens",
                    "X",
                    "=",
                    "w",
                    "1",
                    "."
                ],
                [
                    "."
                ],
                [
                    "."
                ],
                [
                    "w",
                    "n",
                    "and",
                    "produces",
                    "aligned",
                    "output",
                    "sequences",
                    "Y",
                    "s",
                    ",",
                    "Y",
                    "f",
                    ",",
                    "Y",
                    "i",
                    ",",
                    "which",
                    "are",
                    "word",
                    "senses,",
                    "fragments,",
                    "and",
                    "integration",
                    "labels."
                ],
                [
                    "Our",
                    "model",
                    "simply",
                    "consists",
                    "of",
                    "a",
                    "pre-trained",
                    "BERT",
                    "model",
                    "#TARGET_REF",
                    "and",
                    "three",
                    "linear",
                    "classifiers."
                ],
                [
                    "Each",
                    "classifier",
                    "can",
                    "be",
                    "seen",
                    "as",
                    "a",
                    "sub-system",
                    "of",
                    "the",
                    "semantic",
                    "parser",
                    "that",
                    "produces",
                    "one",
                    "of",
                    "the",
                    "three",
                    "labels",
                    "(word",
                    "sense,",
                    "fragment,",
                    "and",
                    "integration",
                    "label)."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: 2 As pointed out by a reviewer, an even better factorization of fragments could potentially be achieved by indexing not with respect to linear position but with respect to the syntactic head word.\n sent1: This would require introducing a dependency parsing component.\n sent2: We leave this for future work.\n sent3: Sequence Labeling Transformer Model Our model is schematically depicted in Figure 4.\n sent4: It takes an input sequence of tokens X = w 1 .\n sent5: .\n sent6: .\n sent7: w n and produces aligned output sequences Y s , Y f , Y i , which are word senses, fragments, and integration labels.\n sent8: Our model simply consists of a pre-trained BERT model #TARGET_REF and three linear classifiers.\n sent9: Each classifier can be seen as a sub-system of the semantic parser that produces one of the three labels (word sense, fragment, and integration label).\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Following",
                    "the",
                    "SAMT",
                    "approach,",
                    "CCG-augmented",
                    "HPB",
                    "SMT",
                    "#TARGET_REF",
                    "uses",
                    "CCG",
                    "#REF",
                    "to",
                    "label",
                    "non-terminals."
                ],
                [
                    "CCG",
                    "has",
                    "distinct",
                    "advantages",
                    "over",
                    "phrase-structure",
                    "grammar",
                    "in",
                    "the",
                    "general",
                    "SMT",
                    "context,",
                    "particularly",
                    "in",
                    "extracting",
                    "non-terminal",
                    "labels",
                    "in",
                    "HPB",
                    "SMT."
                ],
                [
                    "This",
                    "section",
                    "gives",
                    "a",
                    "brief",
                    "introduction",
                    "to",
                    "CCG",
                    "followed",
                    "by",
                    "a",
                    "description",
                    "of",
                    "the",
                    "approach",
                    "of",
                    "extracting",
                    "non-terminal",
                    "labels",
                    "using",
                    "the",
                    "same."
                ]
            ],
            "context": [
                1,
                2,
                0
            ]
        },
        "input": "sent0: Following the SAMT approach, CCG-augmented HPB SMT #TARGET_REF uses CCG #REF to label non-terminals.\n sent1: CCG has distinct advantages over phrase-structure grammar in the general SMT context, particularly in extracting non-terminal labels in HPB SMT.\n sent2: This section gives a brief introduction to CCG followed by a description of the approach of extracting non-terminal labels using the same.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "order",
                    "to",
                    "see",
                    "the",
                    "effectiveness",
                    "of",
                    "the",
                    "representation",
                    "learning",
                    "method,",
                    "the",
                    "next",
                    "two",
                    "baselines",
                    "incorporate",
                    "logical",
                    "mechanisms",
                    "in",
                    "different",
                    "ways."
                ],
                [
                    "BERT+LX",
                    "uses",
                    "latent",
                    "cross",
                    "#TARGET_REF",
                    "to",
                    "directly",
                    "incorporate",
                    "predicate",
                    "values",
                    "in",
                    "R1-R13",
                    "as",
                    "features,",
                    "we",
                    "use",
                    "an",
                    "MLP",
                    "to",
                    "encode",
                    "the",
                    "predicate",
                    "values,",
                    "exploring",
                    "(i)",
                    "one",
                    "hidden",
                    "layer",
                    "with",
                    "D=768",
                    "and",
                    "(ii)",
                    "no",
                    "hidden",
                    "layers."
                ],
                [
                    "BERT+LX",
                    "consistently",
                    "outperforms",
                    "a",
                    "simple",
                    "MLP",
                    "without",
                    "latent",
                    "cross."
                ],
                [
                    "BERT+MT",
                    "uses",
                    "multitask",
                    "learning",
                    "to",
                    "train",
                    "the",
                    "main",
                    "and",
                    "logic",
                    "tasks",
                    "simultaneously."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: In order to see the effectiveness of the representation learning method, the next two baselines incorporate logical mechanisms in different ways.\n sent1: BERT+LX uses latent cross #TARGET_REF to directly incorporate predicate values in R1-R13 as features, we use an MLP to encode the predicate values, exploring (i) one hidden layer with D=768 and (ii) no hidden layers.\n sent2: BERT+LX consistently outperforms a simple MLP without latent cross.\n sent3: BERT+MT uses multitask learning to train the main and logic tasks simultaneously.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "A",
                    "line",
                    "of",
                    "work",
                    "has",
                    "been",
                    "proposed",
                    "to",
                    "study",
                    "the",
                    "vulnerability",
                    "of",
                    "natural",
                    "language",
                    "models,",
                    "through",
                    "transformations",
                    "such",
                    "as",
                    "character-level",
                    "perturbations",
                    "#TARGET_REF",
                    ",",
                    "word-level",
                    "perturbations",
                    "#REF",
                    ",",
                    "prepending",
                    "or",
                    "appending",
                    "a",
                    "sequence",
                    "#REF",
                    ",",
                    "and",
                    "generative",
                    "models",
                    "#REF",
                    "."
                ],
                [
                    "They",
                    "focus",
                    "on",
                    "constructing",
                    "adversarial",
                    "examples",
                    "from",
                    "the",
                    "test",
                    "set",
                    "that",
                    "alter",
                    "the",
                    "prediction,",
                    "whereas",
                    "our",
                    "methods",
                    "focus",
                    "on",
                    "finding",
                    "vulnerable",
                    "examples",
                    "beyond",
                    "the",
                    "test",
                    "set",
                    "whose",
                    "prediction",
                    "can",
                    "be",
                    "altered."
                ],
                [
                    "Robustness",
                    "beyond",
                    "the",
                    "test",
                    "set."
                ],
                [
                    "Several",
                    "works",
                    "have",
                    "studied",
                    "model",
                    "robustness",
                    "beyond",
                    "test",
                    "sets",
                    "but",
                    "mostly",
                    "focused",
                    "on",
                    "computer",
                    "vision",
                    "tasks."
                ],
                [
                    "#REF",
                    "demonstrate",
                    "that",
                    "a",
                    "robustly",
                    "trained",
                    "model",
                    "could",
                    "still",
                    "be",
                    "vulnerable",
                    "to",
                    "small",
                    "perturbations",
                    "if",
                    "the",
                    "input",
                    "comes",
                    "from",
                    "a",
                    "distribution",
                    "only",
                    "slightly",
                    "different",
                    "than",
                    "a",
                    "normal",
                    "test",
                    "set",
                    "(e.g.,",
                    "images",
                    "with",
                    "slightly",
                    "different",
                    "contrasts)."
                ],
                [
                    "#REF",
                    "study",
                    "more",
                    "sources",
                    "of",
                    "common",
                    "corruptions",
                    "such",
                    "as",
                    "brightness,",
                    "motion",
                    "blur",
                    "and",
                    "fog."
                ],
                [
                    "Unlike",
                    "in",
                    "computer",
                    "vision",
                    "where",
                    "simple",
                    "image",
                    "transformations",
                    "can",
                    "be",
                    "used,",
                    "in",
                    "our",
                    "natural",
                    "language",
                    "setting,",
                    "generating",
                    "a",
                    "valid",
                    "example",
                    "beyond",
                    "test",
                    "set",
                    "is",
                    "more",
                    "challenging",
                    "because",
                    "language",
                    "semantics",
                    "and",
                    "grammar",
                    "must",
                    "be",
                    "maintained."
                ],
                [
                    "Counterfactual",
                    "fairness."
                ],
                [
                    "#REF",
                    "propose",
                    "counterfactual",
                    "fairness",
                    "and",
                    "consider",
                    "a",
                    "model",
                    "fair",
                    "if",
                    "changing",
                    "the",
                    "protected",
                    "attributes",
                    "does",
                    "not",
                    "affect",
                    "the",
                    "distribution",
                    "of",
                    "prediction."
                ],
                [
                    "We",
                    "follow",
                    "the",
                    "definition",
                    "and",
                    "focus",
                    "on",
                    "evaluating",
                    "the",
                    "counterfactual",
                    "bias",
                    "between",
                    "pairs",
                    "of",
                    "protected",
                    "tokens."
                ],
                [
                    "Existing",
                    "literature",
                    "quantifies",
                    "fairness",
                    "on",
                    "a",
                    "test",
                    "dataset",
                    "or",
                    "through",
                    "templates",
                    "#REF",
                    "."
                ],
                [
                    "For",
                    "instance,",
                    "#REF",
                    "quantify",
                    "the",
                    "absolute",
                    "counterfactual",
                    "token",
                    "fairness",
                    "gap",
                    "on",
                    "the",
                    "test",
                    "set,",
                    "Prabhakaran",
                    "et",
                    "al."
                ],
                [
                    "(",
                    "2019)",
                    "study",
                    "perturbation",
                    "sensitivity",
                    "for",
                    "named",
                    "entities",
                    "on",
                    "a",
                    "given",
                    "set",
                    "of",
                    "corpus."
                ],
                [
                    "#REF",
                    ",",
                    "#REF",
                    "study",
                    "how",
                    "language",
                    "generation",
                    "models",
                    "respond",
                    "differently",
                    "to",
                    "prompt",
                    "sentences",
                    "containing",
                    "mentions",
                    "of",
                    "different",
                    "demographic",
                    "groups."
                ],
                [
                    "In",
                    "contrast,",
                    "our",
                    "method",
                    "quantifies",
                    "the",
                    "bias",
                    "on",
                    "the",
                    "constructed",
                    "neighborhood."
                ]
            ],
            "context": [
                2,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: A line of work has been proposed to study the vulnerability of natural language models, through transformations such as character-level perturbations #TARGET_REF , word-level perturbations #REF , prepending or appending a sequence #REF , and generative models #REF .\n sent1: They focus on constructing adversarial examples from the test set that alter the prediction, whereas our methods focus on finding vulnerable examples beyond the test set whose prediction can be altered.\n sent2: Robustness beyond the test set.\n sent3: Several works have studied model robustness beyond test sets but mostly focused on computer vision tasks.\n sent4: #REF demonstrate that a robustly trained model could still be vulnerable to small perturbations if the input comes from a distribution only slightly different than a normal test set (e.g., images with slightly different contrasts).\n sent5: #REF study more sources of common corruptions such as brightness, motion blur and fog.\n sent6: Unlike in computer vision where simple image transformations can be used, in our natural language setting, generating a valid example beyond test set is more challenging because language semantics and grammar must be maintained.\n sent7: Counterfactual fairness.\n sent8: #REF propose counterfactual fairness and consider a model fair if changing the protected attributes does not affect the distribution of prediction.\n sent9: We follow the definition and focus on evaluating the counterfactual bias between pairs of protected tokens.\n sent10: Existing literature quantifies fairness on a test dataset or through templates #REF .\n sent11: For instance, #REF quantify the absolute counterfactual token fairness gap on the test set, Prabhakaran et al.\n sent12: ( 2019) study perturbation sensitivity for named entities on a given set of corpus.\n sent13: #REF , #REF study how language generation models respond differently to prompt sentences containing mentions of different demographic groups.\n sent14: In contrast, our method quantifies the bias on the constructed neighborhood.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\", \"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "However,",
                    "sentence",
                    "diversity",
                    "is",
                    "based",
                    "not",
                    "only",
                    "on",
                    "individual",
                    "tokens,",
                    "but",
                    "also",
                    "on",
                    "the",
                    "token",
                    "sequence."
                ],
                [
                    "We",
                    "are",
                    "able",
                    "to",
                    "compute",
                    "weights",
                    "for",
                    "loss",
                    "functions",
                    "dynamically,",
                    "depending",
                    "on",
                    "the",
                    "context,",
                    "while",
                    "retaining",
                    "the",
                    "fluency",
                    "of",
                    "generated",
                    "sentences."
                ],
                [
                    "We",
                    "propose",
                    "such",
                    "a",
                    "loss",
                    "function,",
                    "Inverse",
                    "N-gram",
                    "Frequency",
                    "(INF)",
                    "loss,",
                    "which",
                    "uses",
                    "the",
                    "inverse",
                    "of",
                    "the",
                    "frequency",
                    "of",
                    "the",
                    "n-gram",
                    "of",
                    "the",
                    "tokens,",
                    "rather",
                    "than",
                    "the",
                    "token",
                    "frequency."
                ],
                [
                    "We",
                    "built",
                    "a",
                    "neural",
                    "dialogue",
                    "system",
                    "trained",
                    "by",
                    "INF",
                    "loss",
                    "using",
                    "huge",
                    "amounts",
                    "of",
                    "dialogue",
                    "data",
                    "extracted",
                    "from",
                    "Twitter."
                ],
                [
                    "After",
                    "comparing",
                    "models",
                    "using",
                    "the",
                    "SCE",
                    "loss,",
                    "the",
                    "ITF",
                    "loss,",
                    "and",
                    "the",
                    "INF",
                    "loss,",
                    "we",
                    "evaluated",
                    "their",
                    "diversity",
                    "and",
                    "fluency."
                ],
                [
                    "Results",
                    "show",
                    "that",
                    "our",
                    "proposed",
                    "INF",
                    "loss",
                    "model",
                    "outperformed",
                    "the",
                    "SCE",
                    "loss",
                    "and",
                    "ITF",
                    "loss",
                    "models",
                    "for",
                    "most",
                    "automatic",
                    "assessment",
                    "measures",
                    "such",
                    "as",
                    "DIST-N",
                    "#TARGET_REF",
                    "and",
                    "ROUGE",
                    "#REF",
                    "."
                ],
                [
                    "Our",
                    "INF",
                    "loss",
                    "model",
                    "also",
                    "achieved",
                    "higher",
                    "scores",
                    "on",
                    "our",
                    "human",
                    "evaluations",
                    "of",
                    "coherence",
                    "and",
                    "richness."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                3,
                2,
                0
            ]
        },
        "input": "sent0: However, sentence diversity is based not only on individual tokens, but also on the token sequence.\n sent1: We are able to compute weights for loss functions dynamically, depending on the context, while retaining the fluency of generated sentences.\n sent2: We propose such a loss function, Inverse N-gram Frequency (INF) loss, which uses the inverse of the frequency of the n-gram of the tokens, rather than the token frequency.\n sent3: We built a neural dialogue system trained by INF loss using huge amounts of dialogue data extracted from Twitter.\n sent4: After comparing models using the SCE loss, the ITF loss, and the INF loss, we evaluated their diversity and fluency.\n sent5: Results show that our proposed INF loss model outperformed the SCE loss and ITF loss models for most automatic assessment measures such as DIST-N #TARGET_REF and ROUGE #REF .\n sent6: Our INF loss model also achieved higher scores on our human evaluations of coherence and richness.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent5\"], \"BACKGROUND\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Control",
                    "is",
                    "handled",
                    "using",
                    "the",
                    "syntactic",
                    "constraint",
                    "network."
                ],
                [
                    "Sentence",
                    "s7",
                    "is",
                    "an",
                    "example",
                    "of",
                    "sentence",
                    "involving",
                    "functional",
                    "control",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "In",
                    "s7,",
                    "both",
                    "subject",
                    "control",
                    "and",
                    "object",
                    "control",
                    "exist",
                    "-the",
                    "subject",
                    "of",
                    "'persuade'",
                    "should",
                    "be",
                    "the",
                    "subject",
                    "of",
                    "'tried'",
                    "(subject",
                    "control),",
                    "and",
                    "the",
                    "subject",
                    "of",
                    "'help'",
                    "should",
                    "be",
                    "the",
                    "object",
                    "of",
                    "'persuade'",
                    "(object",
                    "control)."
                ],
                [
                    "In",
                    "this",
                    "case,",
                    "CSCS",
                    "for",
                    "infinitival",
                    "complement",
                    "has",
                    "CSE",
                    "without",
                    "NEXT",
                    "link."
                ],
                [
                    "Such",
                    "an",
                    "CSE",
                    "represents",
                    "missing",
                    "subject."
                ],
                [
                    "There",
                    "are",
                    "SUBJ,",
                    "OBJ,",
                    "and",
                    "OBJ2",
                    "nodes",
                    "(these",
                    "are",
                    "functional",
                    "controller)",
                    "in",
                    "the",
                    "syntactic",
                    "constraints",
                    "network",
                    "each",
                    "of",
                    "which",
                    "store",
                    "pointer",
                    "to",
                    "the",
                    "CI",
                    "node",
                    "for",
                    "possible",
                    "controllee."
                ],
                [
                    "Syntactic",
                    "constraint",
                    "links",
                    "from",
                    "each",
                    "lexical",
                    "items",
                    "of",
                    "the",
                    "verb",
                    "determine",
                    "which",
                    "functional",
                    "controller",
                    "is",
                    "active."
                ],
                [
                    "Activated",
                    "functional",
                    "controller",
                    "propagate",
                    "a",
                    "pointer",
                    "to",
                    "the",
                    "CI",
                    "node",
                    "to",
                    "unbound",
                    "subject",
                    "nodes",
                    "of",
                    "CSCs",
                    "for",
                    "infinitival",
                    "complements."
                ],
                [
                    "Basically,",
                    "one",
                    "set",
                    "of",
                    "nodes",
                    "for",
                    "functional",
                    "controller",
                    "handles",
                    "deeply",
                    "nested",
                    "cases",
                    "due",
                    "to",
                    "functional",
                    "locality."
                ]
            ],
            "context": [
                3,
                2,
                3,
                0,
                0,
                3,
                3,
                3,
                3
            ]
        },
        "input": "sent0: Control is handled using the syntactic constraint network.\n sent1: Sentence s7 is an example of sentence involving functional control #TARGET_REF .\n sent2: In s7, both subject control and object control exist -the subject of 'persuade' should be the subject of 'tried' (subject control), and the subject of 'help' should be the object of 'persuade' (object control).\n sent3: In this case, CSCS for infinitival complement has CSE without NEXT link.\n sent4: Such an CSE represents missing subject.\n sent5: There are SUBJ, OBJ, and OBJ2 nodes (these are functional controller) in the syntactic constraints network each of which store pointer to the CI node for possible controllee.\n sent6: Syntactic constraint links from each lexical items of the verb determine which functional controller is active.\n sent7: Activated functional controller propagate a pointer to the CI node to unbound subject nodes of CSCs for infinitival complements.\n sent8: Basically, one set of nodes for functional controller handles deeply nested cases due to functional locality.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent0\", \"sent2\", \"sent5\", \"sent6\", \"sent7\", \"sent8\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "marginal",
                    "position",
                    "of",
                    "word",
                    "formation",
                    "is",
                    "illustrated",
                    "by",
                    "the",
                    "treatment",
                    "in",
                    "general",
                    "surveys",
                    "of",
                    "computational",
                    "linguistics."
                ],
                [
                    "Surveys",
                    "such",
                    "as",
                    "#REF",
                    "and",
                    "9",
                    "Stem",
                    "changes",
                    "have",
                    "been",
                    "analysed",
                    "in",
                    "a",
                    "number",
                    "of",
                    "different",
                    "ways",
                    "in",
                    "the",
                    "literature."
                ],
                [
                    "Since",
                    "many",
                    "forms",
                    "look",
                    "like",
                    "inflected",
                    "forms",
                    "(the",
                    "so-called",
                    "paradigmic",
                    "forms)",
                    "it",
                    "is",
                    "sometimes",
                    "argued",
                    "that",
                    "these",
                    "are",
                    "inflected",
                    "forms",
                    "in",
                    "word",
                    "formation."
                ],
                [
                    "There",
                    "are",
                    "good",
                    "arguments",
                    "against",
                    "this",
                    "view:",
                    "(a)",
                    "there",
                    "are",
                    "many",
                    "stems",
                    "that",
                    "do",
                    "not",
                    "change",
                    "in",
                    "word",
                    "formation,",
                    "(b)",
                    "changed",
                    "stems",
                    "do",
                    "not",
                    "necessarily",
                    "have",
                    "the",
                    "semantics",
                    "of",
                    "the",
                    "corresponding",
                    "plural",
                    "form",
                    "and",
                    "unchanged",
                    "stems",
                    "do",
                    "not",
                    "necessarily",
                    "have",
                    "the",
                    "semantics",
                    "of",
                    "the",
                    "singular,",
                    "and",
                    "(c)",
                    "there",
                    "are",
                    "also",
                    "non-paradigmic",
                    "forms."
                ],
                [
                    "See",
                    "the",
                    "discussions",
                    "in",
                    "#REF",
                    "and",
                    "#REF",
                    "."
                ],
                [
                    "The",
                    "existence",
                    "of",
                    "stem",
                    "changes",
                    "is",
                    "often",
                    "used",
                    "as",
                    "an",
                    "argument",
                    "against",
                    "the",
                    "IA",
                    "model,",
                    "e.g."
                ],
                [
                    "by",
                    "#REF",
                    "."
                ],
                [
                    "An",
                    "analysis",
                    "in",
                    "terms",
                    "of",
                    "\"stem",
                    "formation\"",
                    "is",
                    "elaborated",
                    "by",
                    "ten",
                    "Hacken",
                    "(1994)."
                ],
                [
                    "#TARGET_REF",
                    "do",
                    "not",
                    "even",
                    "mention",
                    "inflection",
                    "and",
                    "word",
                    "formation",
                    "as",
                    "terms,",
                    "let",
                    "alone",
                    "make",
                    "the",
                    "conceptual",
                    "distinction."
                ],
                [
                    "The",
                    "starting",
                    "point",
                    "of",
                    "the",
                    "approaches",
                    "they",
                    "describe",
                    "is",
                    "clearly",
                    "inflection."
                ],
                [
                    "As",
                    "far",
                    "as",
                    "word",
                    "formation",
                    "phenomena",
                    "are",
                    "treated,",
                    "e.g."
                ],
                [
                    "#REF",
                    ",",
                    "they",
                    "are",
                    "not",
                    "considered",
                    "from",
                    "the",
                    "perspective",
                    "of",
                    "the",
                    "creation",
                    "of",
                    "new",
                    "lexemes,",
                    "but",
                    "as",
                    "examples",
                    "of",
                    "more",
                    "difficult",
                    "combinations",
                    "of",
                    "formatives."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                0,
                0,
                0
            ]
        },
        "input": "sent0: The marginal position of word formation is illustrated by the treatment in general surveys of computational linguistics.\n sent1: Surveys such as #REF and 9 Stem changes have been analysed in a number of different ways in the literature.\n sent2: Since many forms look like inflected forms (the so-called paradigmic forms) it is sometimes argued that these are inflected forms in word formation.\n sent3: There are good arguments against this view: (a) there are many stems that do not change in word formation, (b) changed stems do not necessarily have the semantics of the corresponding plural form and unchanged stems do not necessarily have the semantics of the singular, and (c) there are also non-paradigmic forms.\n sent4: See the discussions in #REF and #REF .\n sent5: The existence of stem changes is often used as an argument against the IA model, e.g.\n sent6: by #REF .\n sent7: An analysis in terms of \"stem formation\" is elaborated by ten Hacken (1994).\n sent8: #TARGET_REF do not even mention inflection and word formation as terms, let alone make the conceptual distinction.\n sent9: The starting point of the approaches they describe is clearly inflection.\n sent10: As far as word formation phenomena are treated, e.g.\n sent11: #REF , they are not considered from the perspective of the creation of new lexemes, but as examples of more difficult combinations of formatives.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent8\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Protected",
                    "Tokens",
                    "Fig."
                ],
                [
                    "7",
                    "presents",
                    "the",
                    "experimental",
                    "results",
                    "with",
                    "additional",
                    "protected",
                    "tokens",
                    "such",
                    "as",
                    "nationality,",
                    "religion,",
                    "and",
                    "sexual",
                    "orientation",
                    "(from",
                    "Ribeiro",
                    "et",
                    "al."
                ],
                [
                    "(",
                    "2020))."
                ],
                [
                    "We",
                    "use",
                    "the",
                    "same",
                    "base",
                    "LSTM",
                    "as",
                    "described",
                    "in",
                    "§4.2."
                ],
                [
                    "One",
                    "interesting",
                    "observation",
                    "is",
                    "when",
                    "p",
                    "=",
                    "(gay,",
                    "straight)",
                    "where",
                    "the",
                    "bias",
                    "is",
                    "negative,",
                    "indicating",
                    "that",
                    "the",
                    "sentiment",
                    "classifier",
                    "tends",
                    "to",
                    "give",
                    "more",
                    "negative",
                    "prediction",
                    "when",
                    "substituting",
                    "gay",
                    "→",
                    "straight",
                    "in",
                    "the",
                    "input."
                ],
                [
                    "This",
                    "phenomenon",
                    "is",
                    "opposite",
                    "to",
                    "the",
                    "behavior",
                    "of",
                    "toxicity",
                    "classifiers",
                    "#TARGET_REF",
                    ",",
                    "and",
                    "we",
                    "hypothesize",
                    "that",
                    "it",
                    "may",
                    "be",
                    "caused",
                    "by",
                    "the",
                    "different",
                    "distribution",
                    "of",
                    "training",
                    "data."
                ],
                [
                    "To",
                    "verify",
                    "the",
                    "hypothesis,",
                    "we",
                    "count",
                    "the",
                    "number",
                    "of",
                    "training",
                    "examples",
                    "containing",
                    "each",
                    "word,",
                    "and",
                    "observe",
                    "that",
                    "we",
                    "have",
                    "far",
                    "more",
                    "negative",
                    "examples",
                    "than",
                    "positive",
                    "examples",
                    "among",
                    "those",
                    "containing",
                    "straight",
                    "(Table",
                    "7)."
                ],
                [
                    "After",
                    "looking",
                    "into",
                    "the",
                    "training",
                    "set,",
                    "it",
                    "turns",
                    "out",
                    "that",
                    "straight",
                    "to",
                    "video",
                    "is",
                    "a",
                    "common",
                    "phrase",
                    "to",
                    "criticize",
                    "a",
                    "film,",
                    "thus",
                    "the",
                    "classifier",
                    "incorrectly",
                    "correlates",
                    "straight",
                    "with",
                    "negative",
                    "sentiment."
                ],
                [
                    "This",
                    "also",
                    "reveals",
                    "the",
                    "limitation",
                    "of",
                    "our",
                    "method",
                    "on",
                    "polysemous",
                    "words."
                ],
                [
                    "In",
                    "Fig."
                ],
                [
                    "8,",
                    "we",
                    "measure",
                    "the",
                    "bias",
                    "on",
                    "X",
                    "test",
                    "and",
                    "observe",
                    "positive",
                    "bias",
                    "on",
                    "most",
                    "tokens",
                    "for",
                    "both",
                    "k",
                    "=",
                    "0",
                    "and",
                    "k",
                    "=",
                    "3,",
                    "which",
                    "indicates",
                    "that",
                    "the",
                    "model",
                    "\"tends\"",
                    "to",
                    "make",
                    "more",
                    "positive",
                    "predictions",
                    "for",
                    "examples",
                    "containing",
                    "certain",
                    "female",
                    "pronouns",
                    "than",
                    "male",
                    "pro-",
                    "nouns."
                ],
                [
                    "Notice",
                    "that",
                    "even",
                    "though",
                    "gender",
                    "swap",
                    "mitigates",
                    "the",
                    "bias",
                    "to",
                    "some",
                    "extent,",
                    "it",
                    "is",
                    "still",
                    "difficult",
                    "to",
                    "fully",
                    "eliminate",
                    "the",
                    "bias."
                ],
                [
                    "This",
                    "is",
                    "probably",
                    "caused",
                    "by",
                    "tuples",
                    "like",
                    "(him,",
                    "his,",
                    "her)",
                    "which",
                    "cannot",
                    "be",
                    "swapped",
                    "perfectly,",
                    "and",
                    "requires",
                    "additional",
                    "processing",
                    "such",
                    "as",
                    "part-of-speech",
                    "resolving",
                    "#REF",
                    "."
                ],
                [
                    "To",
                    "help",
                    "evaluate",
                    "the",
                    "naturalness",
                    "of",
                    "our",
                    "constructed",
                    "examples",
                    "used",
                    "in",
                    "§4,",
                    "we",
                    "provide",
                    "sample",
                    "sentences",
                    "in",
                    "Table",
                    "9",
                    "and",
                    "Table",
                    "10."
                ],
                [
                    "Bold",
                    "words",
                    "are",
                    "the",
                    "corresponding",
                    "patch",
                    "words",
                    "p,",
                    "taken",
                    "from",
                    "the",
                    "predefined",
                    "list",
                    "of",
                    "gendered",
                    "pronouns."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                3,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Protected Tokens Fig.\n sent1: 7 presents the experimental results with additional protected tokens such as nationality, religion, and sexual orientation (from Ribeiro et al.\n sent2: ( 2020)).\n sent3: We use the same base LSTM as described in §4.2.\n sent4: One interesting observation is when p = (gay, straight) where the bias is negative, indicating that the sentiment classifier tends to give more negative prediction when substituting gay → straight in the input.\n sent5: This phenomenon is opposite to the behavior of toxicity classifiers #TARGET_REF , and we hypothesize that it may be caused by the different distribution of training data.\n sent6: To verify the hypothesis, we count the number of training examples containing each word, and observe that we have far more negative examples than positive examples among those containing straight (Table 7).\n sent7: After looking into the training set, it turns out that straight to video is a common phrase to criticize a film, thus the classifier incorrectly correlates straight with negative sentiment.\n sent8: This also reveals the limitation of our method on polysemous words.\n sent9: In Fig.\n sent10: 8, we measure the bias on X test and observe positive bias on most tokens for both k = 0 and k = 3, which indicates that the model \"tends\" to make more positive predictions for examples containing certain female pronouns than male pro- nouns.\n sent11: Notice that even though gender swap mitigates the bias to some extent, it is still difficult to fully eliminate the bias.\n sent12: This is probably caused by tuples like (him, his, her) which cannot be swapped perfectly, and requires additional processing such as part-of-speech resolving #REF .\n sent13: To help evaluate the naturalness of our constructed examples used in §4, we provide sample sentences in Table 9 and Table 10.\n sent14: Bold words are the corresponding patch words p, taken from the predefined list of gendered pronouns.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent5\"], \"BACKGROUND\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Phrase-based",
                    "SMT",
                    "systems",
                    "#REF",
                    "are",
                    "the",
                    "most",
                    "commonly",
                    "used",
                    "technique",
                    "in",
                    "statistical",
                    "machine",
                    "translation",
                    "nowadays."
                ],
                [
                    "In",
                    "this",
                    "approach,",
                    "source",
                    "and",
                    "target",
                    "phrase",
                    "pairs",
                    "consistent",
                    "with",
                    "the",
                    "word",
                    "alignment",
                    "are",
                    "extracted",
                    "from",
                    "the",
                    "parallel",
                    "training",
                    "data."
                ],
                [
                    "Phrases",
                    "in",
                    "PBSMT",
                    "are",
                    "just",
                    "contiguous",
                    "chunks",
                    "of",
                    "text,",
                    "and",
                    "are",
                    "not",
                    "linguistically",
                    "motivated."
                ],
                [
                    "The",
                    "extracted",
                    "source-target",
                    "phrase",
                    "pairs",
                    "along",
                    "with",
                    "their",
                    "translation",
                    "probabilities",
                    "(computed",
                    "from",
                    "the",
                    "same",
                    "training",
                    "data)",
                    "are",
                    "stored",
                    "in",
                    "a",
                    "structure",
                    "known",
                    "as",
                    "the",
                    "'phrase",
                    "table'."
                ],
                [
                    "During",
                    "translation,",
                    "an",
                    "input",
                    "sentence",
                    "is",
                    "split",
                    "up",
                    "into",
                    "phrases",
                    "and",
                    "their",
                    "corresponding",
                    "translations",
                    "are",
                    "looked",
                    "up",
                    "from",
                    "the",
                    "phrase",
                    "table",
                    "to",
                    "create",
                    "a",
                    "set",
                    "of",
                    "translated",
                    "sentences",
                    "in",
                    "the",
                    "target",
                    "language."
                ],
                [
                    "The",
                    "target",
                    "phrases",
                    "in",
                    "each",
                    "such",
                    "translation",
                    "are",
                    "subsequently",
                    "reordered",
                    "using",
                    "a",
                    "statistical",
                    "re-ordering",
                    "model",
                    "that",
                    "assigns",
                    "a",
                    "probability",
                    "based",
                    "on",
                    "the",
                    "orientation",
                    "between",
                    "a",
                    "phrase",
                    "and",
                    "the",
                    "previously",
                    "translated",
                    "phrase."
                ],
                [
                    "A",
                    "language",
                    "model",
                    "is",
                    "further",
                    "used",
                    "for",
                    "better",
                    "fluency",
                    "and",
                    "grammaticality",
                    "of",
                    "the",
                    "translation."
                ],
                [
                    "The",
                    "phrase",
                    "translation",
                    "probabilities",
                    "along",
                    "with",
                    "reordering",
                    "and",
                    "language",
                    "model",
                    "probabilities",
                    "are",
                    "combined",
                    "in",
                    "a",
                    "log-linear",
                    "fashion",
                    "to",
                    "assign",
                    "a",
                    "score",
                    "to",
                    "each",
                    "possible",
                    "translation",
                    "of",
                    "an",
                    "input",
                    "sentence."
                ],
                [
                    "Finally",
                    "the",
                    "best",
                    "scoring",
                    "translation",
                    "is",
                    "searched",
                    "for",
                    "by",
                    "the",
                    "decoding",
                    "algorithm",
                    "and",
                    "is",
                    "presented",
                    "as",
                    "the",
                    "best",
                    "translation",
                    "for",
                    "the",
                    "corresponding",
                    "input",
                    "sentence."
                ],
                [
                    "Formally",
                    "this",
                    "task",
                    "can",
                    "be",
                    "expressed",
                    "as",
                    "in",
                    "#TARGET_REF",
                    ":"
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "sent0: Phrase-based SMT systems #REF are the most commonly used technique in statistical machine translation nowadays.\n sent1: In this approach, source and target phrase pairs consistent with the word alignment are extracted from the parallel training data.\n sent2: Phrases in PBSMT are just contiguous chunks of text, and are not linguistically motivated.\n sent3: The extracted source-target phrase pairs along with their translation probabilities (computed from the same training data) are stored in a structure known as the 'phrase table'.\n sent4: During translation, an input sentence is split up into phrases and their corresponding translations are looked up from the phrase table to create a set of translated sentences in the target language.\n sent5: The target phrases in each such translation are subsequently reordered using a statistical re-ordering model that assigns a probability based on the orientation between a phrase and the previously translated phrase.\n sent6: A language model is further used for better fluency and grammaticality of the translation.\n sent7: The phrase translation probabilities along with reordering and language model probabilities are combined in a log-linear fashion to assign a score to each possible translation of an input sentence.\n sent8: Finally the best scoring translation is searched for by the decoding algorithm and is presented as the best translation for the corresponding input sentence.\n sent9: Formally this task can be expressed as in #TARGET_REF :\n",
        "output": "{\"INFORMATION\": [\"sent9\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "For",
                    "Kinyarwanda",
                    "pre-training",
                    "data,",
                    "we",
                    "use",
                    "the",
                    "Common",
                    "Voice",
                    "(CV)",
                    "Kinyarwanda",
                    "6.1",
                    "subset",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Again,",
                    "we",
                    "utilize",
                    "both",
                    "the",
                    "audio",
                    "files",
                    "and",
                    "transcriptions."
                ],
                [
                    "Due",
                    "to",
                    "the",
                    "large",
                    "size",
                    "of",
                    "the",
                    "CV",
                    "6.1",
                    "Kinyarwanda",
                    "subset,",
                    "we",
                    "processed",
                    "only",
                    "about",
                    "80%",
                    "of",
                    "the",
                    "audio",
                    "files."
                ]
            ],
            "context": [
                1,
                3,
                2
            ]
        },
        "input": "sent0: For Kinyarwanda pre-training data, we use the Common Voice (CV) Kinyarwanda 6.1 subset #TARGET_REF .\n sent1: Again, we utilize both the audio files and transcriptions.\n sent2: Due to the large size of the CV 6.1 Kinyarwanda subset, we processed only about 80% of the audio files.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "attention",
                    "function",
                    "(Eq."
                ],
                [
                    "1)",
                    "requires",
                    "the",
                    "computation",
                    "of",
                    "QK",
                    "containing",
                    "L",
                    "Q",
                    "•",
                    "L",
                    "K",
                    "entries",
                    "and",
                    "can",
                    "be",
                    "expensive",
                    "for",
                    "long",
                    "sequences",
                    "(L",
                    "Q",
                    "and",
                    "L",
                    "K",
                    "are",
                    "typically",
                    "the",
                    "sequence",
                    "length)."
                ],
                [
                    "To",
                    "alleviate",
                    "this",
                    "issue,",
                    "sparse",
                    "attention",
                    "variants",
                    "#TARGET_REF",
                    "relax",
                    "this",
                    "requirement",
                    "and",
                    "compute",
                    "only",
                    "a",
                    "few",
                    "entries",
                    "of",
                    "QK",
                    ",",
                    "masking",
                    "out",
                    "the",
                    "rest."
                ],
                [
                    "For",
                    "a",
                    "binary",
                    "mask"
                ]
            ],
            "context": [
                2,
                2,
                2,
                0
            ]
        },
        "input": "sent0: The attention function (Eq.\n sent1: 1) requires the computation of QK containing L Q • L K entries and can be expensive for long sequences (L Q and L K are typically the sequence length).\n sent2: To alleviate this issue, sparse attention variants #TARGET_REF relax this requirement and compute only a few entries of QK , masking out the rest.\n sent3: For a binary mask\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\", \"sent1\", \"sent2\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "translation",
                    "experiments",
                    "we",
                    "used",
                    "OpenMaTrEx",
                    "#REF",
                    ",",
                    "an",
                    "open",
                    "source",
                    "SMT",
                    "system",
                    "which",
                    "provides",
                    "a",
                    "wrapper",
                    "around",
                    "the",
                    "standard",
                    "log-linear",
                    "phrase-based",
                    "SMT",
                    "system",
                    "Moses",
                    "#REF",
                    "."
                ],
                [
                    "Word",
                    "alignment",
                    "was",
                    "performed",
                    "using",
                    "Giza++",
                    "#REF",
                    "."
                ],
                [
                    "The",
                    "phrase",
                    "and",
                    "the",
                    "reordering",
                    "tables",
                    "were",
                    "built",
                    "on",
                    "the",
                    "word",
                    "alignments",
                    "using",
                    "the",
                    "Moses",
                    "training",
                    "script."
                ],
                [
                    "The",
                    "feature",
                    "weights",
                    "for",
                    "the",
                    "log-linear",
                    "combination",
                    "of",
                    "the",
                    "feature",
                    "functions",
                    "were",
                    "tuned",
                    "using",
                    "Minimum",
                    "Error",
                    "Rate",
                    "Training",
                    "(MERT)",
                    "#REF",
                    "on",
                    "the",
                    "devset",
                    "with",
                    "respect",
                    "to",
                    "BLEU",
                    "#REF",
                    "."
                ],
                [
                    "We",
                    "used",
                    "5-gram",
                    "language",
                    "models",
                    "in",
                    "all",
                    "our",
                    "experiments",
                    "created",
                    "using",
                    "the",
                    "IRSTLM",
                    "language",
                    "modelling",
                    "toolkit",
                    "#REF",
                    "using",
                    "Modified",
                    "Kneser-Ney",
                    "smoothing",
                    "#REF",
                    "."
                ],
                [
                    "Mixture",
                    "adaptation",
                    "of",
                    "language",
                    "models",
                    "mentioned",
                    "in",
                    "Section",
                    "2.2",
                    "was",
                    "also",
                    "performed",
                    "using",
                    "the",
                    "features",
                    "of",
                    "the",
                    "IRSTLM",
                    "toolkit."
                ],
                [
                    "Results",
                    "of",
                    "translations",
                    "in",
                    "every",
                    "phase",
                    "of",
                    "our",
                    "experiments",
                    "were",
                    "evaluated",
                    "using",
                    "BLEU,",
                    "METEOR",
                    "#TARGET_REF",
                    "and",
                    "TER",
                    "#REF",
                    "metrics."
                ],
                [
                    "The",
                    "datasets",
                    "used",
                    "for",
                    "the",
                    "experiments",
                    "included",
                    "the",
                    "specific",
                    "datasets",
                    "released",
                    "by",
                    "the",
                    "IWSLT",
                    "2011",
                    "evaluation",
                    "campaign."
                ],
                [
                    "The",
                    "primary",
                    "bi-lingual",
                    "training",
                    "data",
                    "comprised",
                    "of",
                    "a",
                    "collection",
                    "of",
                    "public",
                    "speech",
                    "transcriptions",
                    "on",
                    "a",
                    "variety",
                    "of",
                    "topics",
                    "from",
                    "TED",
                    "Talks."
                ],
                [
                    "The",
                    "development",
                    "data",
                    "released",
                    "for",
                    "the",
                    "task,",
                    "comprised",
                    "of",
                    "both",
                    "the",
                    "IWSLT-2010",
                    "4",
                    "development",
                    "and",
                    "test",
                    "sets."
                ],
                [
                    "However,",
                    "for",
                    "experiments",
                    "reported",
                    "in",
                    "this",
                    "paper,",
                    "the",
                    "IWSLT-2010",
                    "development",
                    "set",
                    "and",
                    "test",
                    "sets",
                    "were",
                    "used",
                    "for",
                    "tuning",
                    "and",
                    "testing",
                    "respectively."
                ],
                [
                    "As",
                    "an",
                    "auxiliary",
                    "out-of-domain",
                    "source",
                    "of",
                    "bi-lingual",
                    "training",
                    "data,",
                    "the",
                    "Multi-UN",
                    "corpus",
                    "was",
                    "also",
                    "released."
                ],
                [
                    "The",
                    "monolingual",
                    "data",
                    "required",
                    "to",
                    "train",
                    "lan-guage",
                    "models",
                    "also",
                    "comprised",
                    "of",
                    "data",
                    "from",
                    "both",
                    "Multi-UN",
                    "and",
                    "TED",
                    "Talks."
                ],
                [
                    "Table",
                    "1",
                    "shows",
                    "the",
                    "exact",
                    "sentence",
                    "counts",
                    "of",
                    "the",
                    "different",
                    "datasets",
                    "used",
                    "in",
                    "the",
                    "experiments."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: translation experiments we used OpenMaTrEx #REF , an open source SMT system which provides a wrapper around the standard log-linear phrase-based SMT system Moses #REF .\n sent1: Word alignment was performed using Giza++ #REF .\n sent2: The phrase and the reordering tables were built on the word alignments using the Moses training script.\n sent3: The feature weights for the log-linear combination of the feature functions were tuned using Minimum Error Rate Training (MERT) #REF on the devset with respect to BLEU #REF .\n sent4: We used 5-gram language models in all our experiments created using the IRSTLM language modelling toolkit #REF using Modified Kneser-Ney smoothing #REF .\n sent5: Mixture adaptation of language models mentioned in Section 2.2 was also performed using the features of the IRSTLM toolkit.\n sent6: Results of translations in every phase of our experiments were evaluated using BLEU, METEOR #TARGET_REF and TER #REF metrics.\n sent7: The datasets used for the experiments included the specific datasets released by the IWSLT 2011 evaluation campaign.\n sent8: The primary bi-lingual training data comprised of a collection of public speech transcriptions on a variety of topics from TED Talks.\n sent9: The development data released for the task, comprised of both the IWSLT-2010 4 development and test sets.\n sent10: However, for experiments reported in this paper, the IWSLT-2010 development set and test sets were used for tuning and testing respectively.\n sent11: As an auxiliary out-of-domain source of bi-lingual training data, the Multi-UN corpus was also released.\n sent12: The monolingual data required to train lan-guage models also comprised of data from both Multi-UN and TED Talks.\n sent13: Table 1 shows the exact sentence counts of the different datasets used in the experiments.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent6\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Different",
                    "from",
                    "the",
                    "above",
                    "term-based",
                    "approaches,",
                    "dense",
                    "passage",
                    "retrieval",
                    "has",
                    "been",
                    "proposed",
                    "to",
                    "represent",
                    "both",
                    "questions",
                    "and",
                    "documents",
                    "as",
                    "dense",
                    "vectors",
                    "(i.e.,",
                    "embeddings),",
                    "typically",
                    "in",
                    "a",
                    "dual-encoder",
                    "architecture",
                    "(as",
                    "shown",
                    "in",
                    "Figure",
                    "1a)."
                ],
                [
                    "Existing",
                    "approaches",
                    "can",
                    "be",
                    "divided",
                    "into",
                    "two",
                    "categories:",
                    "(1)",
                    "self-supervised",
                    "pre-training",
                    "for",
                    "retrieval",
                    "#TARGET_REF",
                    "and",
                    "(2)",
                    "fine-tuning",
                    "pre-trained",
                    "language",
                    "models",
                    "on",
                    "labeled",
                    "data."
                ],
                [
                    "Our",
                    "work",
                    "follows",
                    "the",
                    "second",
                    "class",
                    "of",
                    "approaches,",
                    "which",
                    "show",
                    "better",
                    "performance",
                    "with",
                    "less",
                    "cost."
                ],
                [
                    "Although",
                    "the",
                    "dual-encoder",
                    "architecture",
                    "enables",
                    "the",
                    "appealing",
                    "paradigm",
                    "of",
                    "dense",
                    "retrieval,",
                    "it",
                    "is",
                    "difficult",
                    "to",
                    "effectively",
                    "train",
                    "a",
                    "retriever",
                    "with",
                    "such",
                    "an",
                    "architecture."
                ],
                [
                    "As",
                    "discussed",
                    "in",
                    "Section",
                    "1,",
                    "it",
                    "suffers",
                    "from",
                    "a",
                    "number",
                    "of",
                    "challenges,",
                    "including",
                    "the",
                    "training",
                    "and",
                    "inference",
                    "discrepancy,",
                    "a",
                    "large",
                    "number",
                    "of",
                    "unlabeled",
                    "positives",
                    "and",
                    "limited",
                    "training",
                    "data."
                ],
                [
                    "Several",
                    "recent",
                    "studies",
                    "#REF",
                    "tried",
                    "to",
                    "address",
                    "the",
                    "first",
                    "challenge",
                    "by",
                    "designing",
                    "complicated",
                    "sampling",
                    "mechanism",
                    "to",
                    "generate",
                    "hard",
                    "negatives."
                ],
                [
                    "However,",
                    "it",
                    "still",
                    "suffers",
                    "from",
                    "the",
                    "issue",
                    "of",
                    "false",
                    "negatives."
                ],
                [
                    "The",
                    "later",
                    "two",
                    "challenges",
                    "have",
                    "seldom",
                    "been",
                    "considered",
                    "for",
                    "open-domain",
                    "QA."
                ]
            ],
            "context": [
                0,
                2,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Different from the above term-based approaches, dense passage retrieval has been proposed to represent both questions and documents as dense vectors (i.e., embeddings), typically in a dual-encoder architecture (as shown in Figure 1a).\n sent1: Existing approaches can be divided into two categories: (1) self-supervised pre-training for retrieval #TARGET_REF and (2) fine-tuning pre-trained language models on labeled data.\n sent2: Our work follows the second class of approaches, which show better performance with less cost.\n sent3: Although the dual-encoder architecture enables the appealing paradigm of dense retrieval, it is difficult to effectively train a retriever with such an architecture.\n sent4: As discussed in Section 1, it suffers from a number of challenges, including the training and inference discrepancy, a large number of unlabeled positives and limited training data.\n sent5: Several recent studies #REF tried to address the first challenge by designing complicated sampling mechanism to generate hard negatives.\n sent6: However, it still suffers from the issue of false negatives.\n sent7: The later two challenges have seldom been considered for open-domain QA.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "For",
                    "model",
                    "training,",
                    "we",
                    "used",
                    "Adam",
                    "with",
                    "batch",
                    "size",
                    "of",
                    "60,",
                    "learning",
                    "rate",
                    "of",
                    "3e-5,",
                    "L2",
                    "weight",
                    "decay",
                    "of",
                    "0.01,",
                    "learning",
                    "rate",
                    "warm",
                    "up",
                    "over",
                    "the",
                    "first",
                    "10,000",
                    "steps,",
                    "and",
                    "linear",
                    "decay",
                    "of",
                    "learning",
                    "rate."
                ],
                [
                    "Our",
                    "models",
                    "were",
                    "trained",
                    "by",
                    "one",
                    "Tesla",
                    "V100",
                    "GPU",
                    "card",
                    "with",
                    "32GB",
                    "memory,",
                    "and",
                    "implemented",
                    "on",
                    "PyTorch",
                    "with",
                    "the",
                    "Huggingface's",
                    "Transformer",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "All",
                    "Transformer-based",
                    "methods",
                    "were",
                    "trained",
                    "with",
                    "30",
                    "epochs,",
                    "taken",
                    "about",
                    "4-5",
                    "hours",
                    "on",
                    "the",
                    "ComVE",
                    "dataset",
                    "and",
                    "7-9",
                    "hours",
                    "on",
                    "the",
                    "α-NLG",
                    "dataset."
                ]
            ],
            "context": [
                0,
                2,
                3
            ]
        },
        "input": "sent0: For model training, we used Adam with batch size of 60, learning rate of 3e-5, L2 weight decay of 0.01, learning rate warm up over the first 10,000 steps, and linear decay of learning rate.\n sent1: Our models were trained by one Tesla V100 GPU card with 32GB memory, and implemented on PyTorch with the Huggingface's Transformer #TARGET_REF .\n sent2: All Transformer-based methods were trained with 30 epochs, taken about 4-5 hours on the ComVE dataset and 7-9 hours on the α-NLG dataset.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Model",
                    "stacking",
                    "is",
                    "an",
                    "efficient",
                    "ensemble",
                    "method",
                    "to",
                    "improve",
                    "model",
                    "accuracy."
                ],
                [
                    "The",
                    "main",
                    "procedure",
                    "of",
                    "stacking",
                    "trained",
                    "models",
                    "in",
                    "our",
                    "method",
                    "including",
                    "five",
                    "steps."
                ],
                [
                    "First,",
                    "we",
                    "use",
                    "heterogeneous",
                    "PLMs",
                    "including",
                    "BERT,",
                    "RoBERTa,",
                    "ALBERT,",
                    "and",
                    "ERNIE",
                    "as",
                    "base",
                    "models."
                ],
                [
                    "Second,",
                    "we",
                    "generate",
                    "multiple",
                    "hyperparameter",
                    "sets",
                    "by",
                    "setting",
                    "different",
                    "values",
                    "of",
                    "dropout,",
                    "selecting",
                    "different",
                    "numbers",
                    "of",
                    "last",
                    "hidden",
                    "layers,",
                    "and",
                    "using",
                    "different",
                    "loss",
                    "functions."
                ],
                [
                    "Since",
                    "our",
                    "purpose",
                    "here",
                    "is",
                    "not",
                    "only",
                    "to",
                    "find",
                    "the",
                    "best",
                    "hyperparameter",
                    "sets",
                    "but",
                    "also",
                    "to",
                    "collect",
                    "diverse",
                    "sets",
                    "with",
                    "reasonable",
                    "performances,",
                    "we",
                    "keep",
                    "all",
                    "the",
                    "training",
                    "results",
                    "from",
                    "different",
                    "sets."
                ],
                [
                    "Third,",
                    "we",
                    "perform",
                    "7fold",
                    "cross-validation",
                    "during",
                    "the",
                    "whole",
                    "training",
                    "process",
                    "to",
                    "avoid",
                    "overfitting",
                    "or",
                    "selection",
                    "bias."
                ],
                [
                    "Fourth,",
                    "we",
                    "adopt",
                    "several",
                    "training",
                    "strategies",
                    "including",
                    "using",
                    "pseudo-labelling",
                    "#TARGET_REF",
                    "and",
                    "data",
                    "augmentation",
                    "to",
                    "further",
                    "improve",
                    "the",
                    "diversity",
                    "of",
                    "trained",
                    "models."
                ]
            ],
            "context": [
                3,
                2,
                0,
                0,
                0,
                0,
                2
            ]
        },
        "input": "sent0: Model stacking is an efficient ensemble method to improve model accuracy.\n sent1: The main procedure of stacking trained models in our method including five steps.\n sent2: First, we use heterogeneous PLMs including BERT, RoBERTa, ALBERT, and ERNIE as base models.\n sent3: Second, we generate multiple hyperparameter sets by setting different values of dropout, selecting different numbers of last hidden layers, and using different loss functions.\n sent4: Since our purpose here is not only to find the best hyperparameter sets but also to collect diverse sets with reasonable performances, we keep all the training results from different sets.\n sent5: Third, we perform 7fold cross-validation during the whole training process to avoid overfitting or selection bias.\n sent6: Fourth, we adopt several training strategies including using pseudo-labelling #TARGET_REF and data augmentation to further improve the diversity of trained models.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\", \"sent6\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "E",
                    "q",
                    "φ",
                    "(z|x)",
                    "log",
                    "p",
                    "θ",
                    "(x|z)",
                    "−",
                    "β",
                    "|D",
                    "KL",
                    "(q",
                    "φ",
                    "(z|x),",
                    "p(z))",
                    "−",
                    "C|where",
                    "C",
                    "is",
                    "a",
                    "positive",
                    "real",
                    "value",
                    "which",
                    "represents",
                    "the",
                    "target",
                    "KL-divergence",
                    "term",
                    "value."
                ],
                [
                    "This",
                    "has",
                    "an",
                    "information-theoretic",
                    "interpretation,",
                    "where",
                    "the",
                    "placed",
                    "constraint",
                    "C",
                    "on",
                    "the",
                    "KL",
                    "term",
                    "is",
                    "seen",
                    "as",
                    "the",
                    "amount",
                    "of",
                    "information",
                    "transmitted",
                    "from",
                    "a",
                    "sender",
                    "(encoder)",
                    "to",
                    "a",
                    "receiver",
                    "(decoder)",
                    "via",
                    "the",
                    "message",
                    "(z)",
                    "#REF",
                    ",",
                    "and",
                    "impacts",
                    "the",
                    "sharpness",
                    "of",
                    "the",
                    "posterior",
                    "distribution",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "This",
                    "constraint",
                    "allows",
                    "the",
                    "model",
                    "to",
                    "prioritize",
                    "underlying",
                    "factors",
                    "of",
                    "data",
                    "according",
                    "to",
                    "the",
                    "availability",
                    "of",
                    "channel",
                    "capacity",
                    "and",
                    "their",
                    "contributions",
                    "to",
                    "the",
                    "reconstruction",
                    "loss",
                    "improvement."
                ],
                [
                    "#REF",
                    "introduces",
                    "an",
                    "additional",
                    "term",
                    "to",
                    "β-VAE,",
                    "D",
                    "M",
                    "M",
                    "D",
                    "(q",
                    "φ",
                    "(z),",
                    "p",
                    "θ",
                    "(z)),"
                ]
            ],
            "context": [
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: E q φ (z|x) log p θ (x|z) − β |D KL (q φ (z|x), p(z)) − C|where C is a positive real value which represents the target KL-divergence term value.\n sent1: This has an information-theoretic interpretation, where the placed constraint C on the KL term is seen as the amount of information transmitted from a sender (encoder) to a receiver (decoder) via the message (z) #REF , and impacts the sharpness of the posterior distribution #TARGET_REF .\n sent2: This constraint allows the model to prioritize underlying factors of data according to the availability of channel capacity and their contributions to the reconstruction loss improvement.\n sent3: #REF introduces an additional term to β-VAE, D M M D (q φ (z), p θ (z)),\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "memory-based",
                    "approach",
                    "is",
                    "expected",
                    "to",
                    "offer",
                    "solutions",
                    "to",
                    "these",
                    "problems",
                    "by",
                    "allowing",
                    "large",
                    "numbers",
                    "of",
                    "cases",
                    "to",
                    "be",
                    "stored",
                    "in",
                    "the",
                    "memory,",
                    "and",
                    "make",
                    "translation",
                    "by",
                    "using",
                    "these",
                    "cases."
                ],
                [
                    "The",
                    "memory-based",
                    "translation",
                    "essentially",
                    "convert",
                    "lime-complexity",
                    "of",
                    "rule",
                    "application",
                    "into",
                    "space-complexity",
                    "by",
                    "preparing",
                    "large",
                    "examples",
                    "of",
                    "translation",
                    "pairs."
                ],
                [
                    "Since",
                    "each",
                    "case",
                    "can",
                    "be",
                    "represented",
                    "in",
                    "a",
                    "fairly",
                    "context-sensitive",
                    "manner",
                    "with",
                    "full",
                    "semantic",
                    "restrictions",
                    "incorporated,",
                    "the",
                    "memorybased",
                    "translation",
                    "avoids",
                    "expensive",
                    "computations",
                    "generally",
                    "takes",
                    "place",
                    "in",
                    "the",
                    "rule-based",
                    "translation",
                    "system."
                ],
                [
                    "In",
                    "addition,",
                    "the",
                    "memory-based",
                    "approach",
                    "is",
                    "expected",
                    "to",
                    "produce",
                    "high",
                    "quality",
                    "translation",
                    "due",
                    "to",
                    "its",
                    "capability",
                    "to",
                    "reuse",
                    "stylistic",
                    "translation",
                    "in",
                    "the",
                    "past."
                ],
                [
                    "Since,",
                    "detailed",
                    "mechanisms",
                    "and",
                    "rationale",
                    "for",
                    "the",
                    "memory-based",
                    "translation",
                    "approach",
                    "has",
                    "been",
                    "discussed",
                    "by",
                    "relevant",
                    "literatures",
                    "(see",
                    "#REF",
                    ",",
                    "#TARGET_REF",
                    ",",
                    "#REF",
                    ",",
                    "#REF",
                    ",",
                    "#REF",
                    ",",
                    "and",
                    "#REF",
                    "),",
                    "we",
                    "will",
                    "simply",
                    "focus",
                    "on",
                    "its",
                    "massively",
                    "parallel",
                    "implementation",
                    "and",
                    "its",
                    "performance."
                ]
            ],
            "context": [
                0,
                0,
                3,
                3,
                2
            ]
        },
        "input": "sent0: The memory-based approach is expected to offer solutions to these problems by allowing large numbers of cases to be stored in the memory, and make translation by using these cases.\n sent1: The memory-based translation essentially convert lime-complexity of rule application into space-complexity by preparing large examples of translation pairs.\n sent2: Since each case can be represented in a fairly context-sensitive manner with full semantic restrictions incorporated, the memorybased translation avoids expensive computations generally takes place in the rule-based translation system.\n sent3: In addition, the memory-based approach is expected to produce high quality translation due to its capability to reuse stylistic translation in the past.\n sent4: Since, detailed mechanisms and rationale for the memory-based translation approach has been discussed by relevant literatures (see #REF , #TARGET_REF , #REF , #REF , #REF , and #REF ), we will simply focus on its massively parallel implementation and its performance.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent4\"], \"BACKGROUND\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Many",
                    "of",
                    "the",
                    "lexical",
                    "items",
                    "classified",
                    "into",
                    "Levin's",
                    "verb",
                    "classes",
                    "are",
                    "listed",
                    "as",
                    "members",
                    "of",
                    "more",
                    "than",
                    "one",
                    "semantic",
                    "class",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "There",
                    "are",
                    "in",
                    "fact",
                    "3104",
                    "verbs,",
                    "but",
                    "4194",
                    "verb/class",
                    "pairings,",
                    "or",
                    "verb",
                    "senses,",
                    "for",
                    "an",
                    "average",
                    "of",
                    "1.35",
                    "senses",
                    "per",
                    "verb."
                ],
                [
                    "Levin",
                    "gives",
                    "only",
                    "a",
                    "few",
                    "informal",
                    "indications",
                    "about",
                    "how",
                    "to",
                    "interpret",
                    "a",
                    "multiple",
                    "listing",
                    "for",
                    "a",
                    "verb."
                ],
                [
                    "Sometimes",
                    "the",
                    "verb",
                    "is",
                    "listed",
                    "in",
                    "several",
                    "classes",
                    "because",
                    "there",
                    "is",
                    "a",
                    "systematic",
                    "meaning",
                    "relationship",
                    "among",
                    "them."
                ],
                [
                    "Other",
                    "times,",
                    "the",
                    "multiple",
                    "categorization",
                    "seems",
                    "to",
                    "be",
                    "an",
                    "idiosyncrasy",
                    "involving",
                    "two",
                    "verbs",
                    "that",
                    "happen",
                    "to",
                    "have",
                    "the",
                    "same",
                    "spelling,",
                    "i.e.,",
                    "homonyms."
                ],
                [
                    "For",
                    "example,",
                    "the",
                    "verb",
                    "draw",
                    "is",
                    "listed",
                    "as",
                    "a",
                    "remove",
                    "verb",
                    "(class",
                    "10.1),",
                    "as",
                    "a",
                    "scribble",
                    "verb",
                    "(class",
                    "25.2)",
                    "and",
                    "as",
                    "a",
                    "performance",
                    "verb",
                    "(class",
                    "26.7)."
                ],
                [
                    "While",
                    "the",
                    "latter",
                    "two",
                    "senses",
                    "seem",
                    "systematically",
                    "related",
                    "(both",
                    "seem",
                    "to",
                    "be",
                    "involved,",
                    "for",
                    "example,",
                    "in",
                    "a",
                    "usage",
                    "like",
                    "draw",
                    "a",
                    "portrait)",
                    "the",
                    "remove",
                    "sense",
                    "(as",
                    "in",
                    "draw",
                    "water",
                    "from",
                    "the",
                    "well)",
                    "is",
                    "clearly",
                    "distinct."
                ]
            ],
            "context": [
                1,
                1,
                2,
                2,
                2,
                3,
                3
            ]
        },
        "input": "sent0: Many of the lexical items classified into Levin's verb classes are listed as members of more than one semantic class #TARGET_REF .\n sent1: There are in fact 3104 verbs, but 4194 verb/class pairings, or verb senses, for an average of 1.35 senses per verb.\n sent2: Levin gives only a few informal indications about how to interpret a multiple listing for a verb.\n sent3: Sometimes the verb is listed in several classes because there is a systematic meaning relationship among them.\n sent4: Other times, the multiple categorization seems to be an idiosyncrasy involving two verbs that happen to have the same spelling, i.e., homonyms.\n sent5: For example, the verb draw is listed as a remove verb (class 10.1), as a scribble verb (class 25.2) and as a performance verb (class 26.7).\n sent6: While the latter two senses seem systematically related (both seem to be involved, for example, in a usage like draw a portrait) the remove sense (as in draw water from the well) is clearly distinct.\n",
        "output": "{\"INFORMATION\": [\"sent0\", \"sent1\"], \"PERCEPTION\": [\"sent2\", \"sent3\", \"sent4\"], \"BACKGROUND\": [\"sent5\", \"sent6\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Besides,",
                    "concept",
                    "instance",
                    "nodes",
                    "(CI)",
                    "and",
                    "concept",
                    "sequence",
                    "instance",
                    "structures",
                    "(CSI)",
                    "are",
                    "dynamically",
                    "created",
                    "during",
                    "parsing."
                ],
                [
                    "Each",
                    "CI",
                    "or",
                    "CSI",
                    "is",
                    "connected",
                    "to",
                    "the",
                    "associated",
                    "CC",
                    "or",
                    "CSC",
                    "by",
                    "INST",
                    "link."
                ],
                [
                    "CIs",
                    "correspond",
                    "to",
                    "discourse",
                    "entities",
                    "proposed",
                    "in",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Three",
                    "additional",
                    "links",
                    "are",
                    "used",
                    "to",
                    "facilitate",
                    "pragmatic",
                    "inferences."
                ],
                [
                    "They",
                    "are",
                    "CONTEXT",
                    "links."
                ],
                [
                    "CONSTRAINT",
                    "links",
                    "and",
                    "EQROLE",
                    "links."
                ],
                [
                    "A",
                    "CONTEXT",
                    "link",
                    "is",
                    "a",
                    "path",
                    "of",
                    "contextual",
                    "priming",
                    "which",
                    "is",
                    "crucial",
                    "in",
                    "word",
                    "sense",
                    "disambiguation."
                ],
                [
                    "When",
                    "a",
                    "word",
                    "is",
                    "activated",
                    "during",
                    "processing,",
                    "the",
                    "activation",
                    "spreads",
                    "through",
                    "CONTEXT",
                    "links",
                    "and",
                    "impose",
                    "contextual",
                    "priming",
                    "to",
                    "relevant",
                    "concepts."
                ],
                [
                    "A",
                    "CONSTRAINT",
                    "link",
                    "denotes",
                    "an",
                    "antecedent/consequence",
                    "relationship",
                    "between",
                    "two",
                    "events",
                    "or",
                    "states,",
                    "which",
                    "is",
                    "created",
                    "between",
                    "two",
                    "CSRs."
                ],
                [
                    "An",
                    "EQROLE",
                    "link",
                    "denotes",
                    "the",
                    "necessary",
                    "argument",
                    "matching",
                    "condition",
                    "for",
                    "testing",
                    "an",
                    "antecedent/consequence",
                    "relationship,",
                    "which",
                    "is",
                    "created",
                    "between",
                    "two",
                    "CSEs",
                    "in",
                    "different",
                    "CSCs."
                ]
            ],
            "context": [
                0,
                0,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Besides, concept instance nodes (CI) and concept sequence instance structures (CSI) are dynamically created during parsing.\n sent1: Each CI or CSI is connected to the associated CC or CSC by INST link.\n sent2: CIs correspond to discourse entities proposed in #TARGET_REF .\n sent3: Three additional links are used to facilitate pragmatic inferences.\n sent4: They are CONTEXT links.\n sent5: CONSTRAINT links and EQROLE links.\n sent6: A CONTEXT link is a path of contextual priming which is crucial in word sense disambiguation.\n sent7: When a word is activated during processing, the activation spreads through CONTEXT links and impose contextual priming to relevant concepts.\n sent8: A CONSTRAINT link denotes an antecedent/consequence relationship between two events or states, which is created between two CSRs.\n sent9: An EQROLE link denotes the necessary argument matching condition for testing an antecedent/consequence relationship, which is created between two CSEs in different CSCs.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "training",
                    "paradigm",
                    "proposed",
                    "in",
                    "this",
                    "paper",
                    "may",
                    "be",
                    "extended",
                    "to",
                    "any",
                    "Seq2Seq",
                    "model."
                ],
                [
                    "However,",
                    "it",
                    "can",
                    "be",
                    "a",
                    "non-trivial",
                    "overhead",
                    "to",
                    "generate",
                    "the",
                    "candidate",
                    "summaries",
                    "using",
                    "large",
                    "neural",
                    "models",
                    "on",
                    "the",
                    "entire",
                    "training",
                    "set."
                ],
                [
                    "On",
                    "the",
                    "other",
                    "hand,",
                    "recent",
                    "work",
                    "#TARGET_REF",
                    "Reference",
                    "chelsea",
                    "forward",
                    "tammy",
                    "abraham",
                    "nets",
                    "first-half",
                    "double",
                    "for",
                    "chelsea."
                ],
                [
                    "dominic",
                    "solanke",
                    "adds",
                    "a",
                    "third",
                    "late",
                    "on",
                    "as",
                    "chelsea",
                    "look",
                    "set",
                    "to",
                    "win",
                    "trophy."
                ],
                [
                    "manchester",
                    "city",
                    "struggle",
                    "without",
                    "injured",
                    "star",
                    "thierry",
                    "ambrose."
                ],
                [
                    "read:",
                    "mourinho",
                    "warns",
                    "his",
                    "young",
                    "chelsea",
                    "players",
                    "he",
                    "can",
                    "not",
                    "play",
                    "them",
                    "all."
                ],
                [
                    "click",
                    "here",
                    "to",
                    "read",
                    "our",
                    "match",
                    "report",
                    "from",
                    "man",
                    "city",
                    "'s",
                    "academy",
                    "stadium."
                ]
            ],
            "context": [
                0,
                0,
                2,
                2,
                2,
                0,
                0
            ]
        },
        "input": "sent0: The training paradigm proposed in this paper may be extended to any Seq2Seq model.\n sent1: However, it can be a non-trivial overhead to generate the candidate summaries using large neural models on the entire training set.\n sent2: On the other hand, recent work #TARGET_REF Reference chelsea forward tammy abraham nets first-half double for chelsea.\n sent3: dominic solanke adds a third late on as chelsea look set to win trophy.\n sent4: manchester city struggle without injured star thierry ambrose.\n sent5: read: mourinho warns his young chelsea players he can not play them all.\n sent6: click here to read our match report from man city 's academy stadium.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent2\", \"sent3\", \"sent4\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Early",
                    "studies",
                    "for",
                    "scientific",
                    "literature",
                    "link",
                    "formulae",
                    "to",
                    "Wikipedia",
                    "page",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Even",
                    "though",
                    "this",
                    "can",
                    "provide",
                    "additional",
                    "information",
                    "regarding",
                    "the",
                    "mathematical",
                    "expression,",
                    "a",
                    "reader",
                    "might",
                    "find",
                    "it",
                    "harder",
                    "to",
                    "understand",
                    "the",
                    "Wikipedia",
                    "page",
                    "as",
                    "it",
                    "is",
                    "presented",
                    "in",
                    "many",
                    "unrelated",
                    "forms."
                ],
                [
                    "Linking",
                    "to",
                    "the",
                    "description",
                    "in",
                    "the",
                    "same",
                    "document",
                    "is",
                    "more",
                    "practical",
                    "#REF",
                    "as",
                    "the",
                    "descriptions",
                    "are",
                    "dedicated",
                    "to",
                    "the",
                    "symbols",
                    "and",
                    "the",
                    "context",
                    "presented",
                    "in",
                    "the",
                    "document."
                ]
            ],
            "context": [
                1,
                2,
                0
            ]
        },
        "input": "sent0: Early studies for scientific literature link formulae to Wikipedia page #TARGET_REF .\n sent1: Even though this can provide additional information regarding the mathematical expression, a reader might find it harder to understand the Wikipedia page as it is presented in many unrelated forms.\n sent2: Linking to the description in the same document is more practical #REF as the descriptions are dedicated to the symbols and the context presented in the document.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "E",
                    "q",
                    "φ",
                    "(z|x)",
                    "[log",
                    "p",
                    "θ",
                    "(x|z)]",
                    "−",
                    "βD",
                    "KL",
                    "(q",
                    "φ",
                    "(z|x),",
                    "p(z))",
                    "−λD",
                    "M",
                    "M",
                    "D",
                    "(q",
                    "φ",
                    "(z),",
                    "p(z))where",
                    "D",
                    "M",
                    "M",
                    "D",
                    "is",
                    "computed",
                    "using",
                    "maximum",
                    "mean",
                    "discrepancy",
                    "#REF",
                    ",",
                    "MMD)",
                    "and",
                    "λ",
                    "is",
                    "the",
                    "scalar",
                    "weight."
                ],
                [
                    "This",
                    "term",
                    "regularises",
                    "the",
                    "aggregated",
                    "posterior",
                    "q",
                    "φ",
                    "(z)",
                    "with",
                    "a",
                    "factorised",
                    "spikeand-slab",
                    "prior",
                    "#TARGET_REF",
                    ",",
                    "which",
                    "aims",
                    "for",
                    "disentanglement",
                    "via",
                    "clustering",
                    "and",
                    "sparsifying",
                    "the",
                    "representations",
                    "of",
                    "z."
                ]
            ],
            "context": [
                0,
                1
            ]
        },
        "input": "sent0: E q φ (z|x) [log p θ (x|z)] − βD KL (q φ (z|x), p(z)) −λD M M D (q φ (z), p(z))where D M M D is computed using maximum mean discrepancy #REF , MMD) and λ is the scalar weight.\n sent1: This term regularises the aggregated posterior q φ (z) with a factorised spikeand-slab prior #TARGET_REF , which aims for disentanglement via clustering and sparsifying the representations of z.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Syntactic",
                    "constraint",
                    "network",
                    "(SCN)",
                    "is",
                    "a",
                    "new",
                    "feature",
                    "which",
                    "has",
                    "not",
                    "been",
                    "used",
                    "in",
                    "the",
                    "previous",
                    "works",
                    "in",
                    "memory-based",
                    "NLP",
                    "SCN",
                    "is",
                    "used",
                    "to",
                    "handle",
                    "syntactic",
                    "phenomena",
                    "without",
                    "undermining",
                    "benefits",
                    "of",
                    "memory-based",
                    "approach."
                ],
                [
                    "Although,",
                    "unification",
                    "has",
                    "been",
                    "the",
                    "central",
                    "operation",
                    "in",
                    "the",
                    "recent",
                    "syntactic",
                    "theories",
                    "such",
                    "as",
                    "LFG",
                    "#REF",
                    "and",
                    "HPSG",
                    "#REF",
                    ",",
                    "we",
                    "prefer",
                    "SCN",
                    "over",
                    "unificationbased",
                    "approach",
                    "because",
                    "unification",
                    "is",
                    "computationally",
                    "expensive",
                    "and",
                    "it",
                    "is",
                    "not",
                    "suitable",
                    "for",
                    "massively",
                    "parallel",
                    "implementation."
                ],
                [
                    "Although",
                    "there",
                    "is",
                    "a",
                    "report",
                    "on",
                    "an",
                    "unification",
                    "algorithm",
                    "on",
                    "massively",
                    "parallel",
                    "machines",
                    "#REF",
                    ",",
                    "still",
                    "it",
                    "is",
                    "computationally",
                    "expensive,",
                    "and",
                    "takes",
                    "up",
                    "major",
                    "part",
                    "of",
                    "computing",
                    "lime",
                    "even",
                    "on",
                    "SNAP."
                ],
                [
                    "In",
                    "addition."
                ],
                [
                    "there",
                    "is",
                    "a",
                    "report",
                    "that",
                    "unification",
                    "is",
                    "not",
                    "necessary",
                    "the",
                    "correct",
                    "mechanism",
                    "of",
                    "enforcing",
                    "Figure",
                    "2:",
                    "Concept",
                    "Sequence",
                    "on",
                    "SNAP",
                    "agreement",
                    "#REF",
                    "."
                ],
                [
                    "Also,",
                    "the",
                    "classification-based",
                    "approach",
                    "#TARGET_REF",
                    ",",
                    "which",
                    "pre-compiles",
                    "a",
                    "hierarchy",
                    "of",
                    "feature",
                    "structures",
                    "in",
                    "the",
                    "form",
                    "of",
                    "a",
                    "semantic",
                    "network,",
                    "can",
                    "carry",
                    "out",
                    "similar",
                    "task",
                    "with",
                    "less",
                    "computational",
                    "cost",
                    "#REF",
                    "."
                ],
                [
                    "Finally,",
                    "current",
                    "unification",
                    "hard-rejects",
                    "failure",
                    "which",
                    "is",
                    "not",
                    "desirable",
                    "from",
                    "our",
                    "point."
                ],
                [
                    "We",
                    "want",
                    "the",
                    "system",
                    "to",
                    "be",
                    "robust",
                    "enough",
                    "that",
                    "while",
                    "recognizing",
                    "minor",
                    "syntactic",
                    "violation,",
                    "it",
                    "keep",
                    "processing",
                    "to",
                    "get",
                    "meaning",
                    "of",
                    "the",
                    "sentence."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "sent0: Syntactic constraint network (SCN) is a new feature which has not been used in the previous works in memory-based NLP SCN is used to handle syntactic phenomena without undermining benefits of memory-based approach.\n sent1: Although, unification has been the central operation in the recent syntactic theories such as LFG #REF and HPSG #REF , we prefer SCN over unificationbased approach because unification is computationally expensive and it is not suitable for massively parallel implementation.\n sent2: Although there is a report on an unification algorithm on massively parallel machines #REF , still it is computationally expensive, and takes up major part of computing lime even on SNAP.\n sent3: In addition.\n sent4: there is a report that unification is not necessary the correct mechanism of enforcing Figure 2: Concept Sequence on SNAP agreement #REF .\n sent5: Also, the classification-based approach #TARGET_REF , which pre-compiles a hierarchy of feature structures in the form of a semantic network, can carry out similar task with less computational cost #REF .\n sent6: Finally, current unification hard-rejects failure which is not desirable from our point.\n sent7: We want the system to be robust enough that while recognizing minor syntactic violation, it keep processing to get meaning of the sentence.\n",
        "output": "{\"INFORMATION\": [\"sent5\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "order",
                    "to",
                    "generate",
                    "character",
                    "embeddings",
                    "from",
                    "the",
                    "input",
                    "sentence,",
                    "we",
                    "first",
                    "use",
                    "a",
                    "CharCNN",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Let",
                    "C",
                    "be",
                    "the",
                    "dictionary",
                    "of",
                    "all",
                    "the",
                    "characters",
                    "in",
                    "the",
                    "language",
                    "and",
                    "V",
                    "be",
                    "all",
                    "the",
                    "words",
                    "in",
                    "the",
                    "language."
                ],
                [
                    "We",
                    "first",
                    "define",
                    "the",
                    "character",
                    "embeddings",
                    "matrix",
                    "E",
                    "∈",
                    "R",
                    "d×|C",
                    "|",
                    ",",
                    "where",
                    "d",
                    "is",
                    "the",
                    "dimensionality",
                    "of",
                    "the",
                    "character",
                    "embeddings,",
                    "with",
                    "the",
                    "constraint",
                    "that",
                    "d",
                    "&lt,",
                    "|C",
                    "|."
                ],
                [
                    "Let",
                    "word",
                    "w",
                    "i",
                    "∈",
                    "V",
                    "c",
                    "w",
                    "i",
                    "=",
                    "[c",
                    "w",
                    "i",
                    "1",
                    ",",
                    "c",
                    "w",
                    "i",
                    "2",
                    ",",
                    "."
                ],
                [
                    "."
                ],
                [
                    "."
                ],
                [
                    ",",
                    "c",
                    "w",
                    "i",
                    "n",
                    "]."
                ],
                [
                    "The",
                    "character",
                    "representation",
                    "of",
                    "w",
                    "i",
                    "is",
                    "therefore",
                    "given",
                    "by",
                    "E",
                    "w",
                    "i",
                    "∈",
                    "R",
                    "d×n",
                    "."
                ]
            ],
            "context": [
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: In order to generate character embeddings from the input sentence, we first use a CharCNN #TARGET_REF .\n sent1: Let C be the dictionary of all the characters in the language and V be all the words in the language.\n sent2: We first define the character embeddings matrix E ∈ R d×|C | , where d is the dimensionality of the character embeddings, with the constraint that d &lt, |C |.\n sent3: Let word w i ∈ V c w i = [c w i 1 , c w i 2 , .\n sent4: .\n sent5: .\n sent6: , c w i n ].\n sent7: The character representation of w i is therefore given by E w i ∈ R d×n .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Structural",
                    "ambiguity",
                    "is",
                    "resolved",
                    "by",
                    "the",
                    "cost-based",
                    "ambiguity",
                    "resolution",
                    "method",
                    "#REF",
                    "]."
                ],
                [
                    "The",
                    "cost-based",
                    "ambiguity",
                    "resolution",
                    "takes",
                    "into",
                    "account",
                    "various",
                    "psycholinguistic",
                    "studies",
                    "such",
                    "as",
                    "#TARGET_REF",
                    "and",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                3,
                3
            ]
        },
        "input": "sent0: Structural ambiguity is resolved by the cost-based ambiguity resolution method #REF ].\n sent1: The cost-based ambiguity resolution takes into account various psycholinguistic studies such as #TARGET_REF and #REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "•",
                    "MS",
                    "COCO",
                    "#TARGET_REF",
                    ")",
                    ":",
                    "an",
                    "object",
                    "detection",
                    "and",
                    "captioning",
                    "dataset",
                    "with",
                    "&gt,200",
                    "K",
                    "labeled",
                    "images",
                    "and",
                    "5",
                    "captions",
                    "in",
                    "a",
                    "sentence",
                    "form",
                    "for",
                    "each",
                    "image",
                    "•",
                    "Flicker30k",
                    "#REF",
                    ":",
                    "31",
                    "K",
                    "images",
                    "collected",
                    "from",
                    "Flickr,",
                    "together",
                    "with",
                    "5",
                    "reference",
                    "sentences",
                    "•",
                    "ImageNET",
                    "#REF",
                    ":",
                    "14",
                    "M",
                    "annotated",
                    "images,",
                    "hierarchically",
                    "organized",
                    "(w.r.t."
                ],
                [
                    "WordNet)",
                    "•",
                    "MVSO",
                    "#REF",
                    ":",
                    "15",
                    "K",
                    "visual",
                    "concepts",
                    "across",
                    "12",
                    "languages,",
                    "7.36",
                    "M",
                    "images",
                    "Additionally,",
                    "there",
                    "are",
                    "multimodal",
                    "datasets",
                    "that",
                    "were",
                    "created",
                    "for",
                    "a",
                    "specific",
                    "task:"
                ]
            ],
            "context": [
                0,
                0
            ]
        },
        "input": "sent0: • MS COCO #TARGET_REF ) : an object detection and captioning dataset with &gt,200 K labeled images and 5 captions in a sentence form for each image • Flicker30k #REF : 31 K images collected from Flickr, together with 5 reference sentences • ImageNET #REF : 14 M annotated images, hierarchically organized (w.r.t.\n sent1: WordNet) • MVSO #REF : 15 K visual concepts across 12 languages, 7.36 M images Additionally, there are multimodal datasets that were created for a specific task:\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Moreover,",
                    "these",
                    "LLMs",
                    "are",
                    "known",
                    "to",
                    "have",
                    "flaws."
                ],
                [
                    "BERT",
                    "in",
                    "particular",
                    "has",
                    "been",
                    "shown",
                    "to",
                    "be,",
                    "in",
                    "certain",
                    "scenarios,",
                    "insensitive",
                    "to",
                    "negation",
                    "#REF",
                    "and",
                    "word",
                    "order",
                    "#REF",
                    "."
                ],
                [
                    "BERT",
                    "also",
                    "has",
                    "inexact",
                    "representations",
                    "of",
                    "numbers",
                    "#REF",
                    "and",
                    "fails",
                    "to",
                    "be",
                    "robust",
                    "to",
                    "named",
                    "entities",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "All",
                    "of",
                    "these",
                    "phenomena",
                    "could",
                    "result",
                    "in",
                    "poor-quality",
                    "scores",
                    "from",
                    "BERTScore."
                ],
                [
                    "However,",
                    "it",
                    "is",
                    "difficult",
                    "to",
                    "say",
                    "for",
                    "certain",
                    "how",
                    "these",
                    "issues",
                    "might",
                    "manifest",
                    "in",
                    "BERTScore,",
                    "as",
                    "it",
                    "employs",
                    "BERT",
                    "in",
                    "an",
                    "unsupervised",
                    "scenario",
                    "distinct",
                    "from",
                    "that",
                    "of",
                    "these",
                    "analyses."
                ]
            ],
            "context": [
                3,
                0,
                1,
                2,
                2
            ]
        },
        "input": "sent0: Moreover, these LLMs are known to have flaws.\n sent1: BERT in particular has been shown to be, in certain scenarios, insensitive to negation #REF and word order #REF .\n sent2: BERT also has inexact representations of numbers #REF and fails to be robust to named entities #TARGET_REF .\n sent3: All of these phenomena could result in poor-quality scores from BERTScore.\n sent4: However, it is difficult to say for certain how these issues might manifest in BERTScore, as it employs BERT in an unsupervised scenario distinct from that of these analyses.\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [\"sent3\", \"sent4\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "SPARKLE",
                    "project",
                    "has",
                    "shown",
                    "(see",
                    "#TARGET_REF",
                    "how",
                    "far",
                    "simple",
                    "robust",
                    "phrasal",
                    "parsing",
                    "combined",
                    "with",
                    "classification",
                    "techniques",
                    "utilising",
                    "limited",
                    "and",
                    "manageable",
                    "linguistic",
                    "knowledge",
                    "and",
                    "statistical",
                    "data",
                    "from",
                    "substantial",
                    "corpora",
                    "can",
                    "ameliorate",
                    "this",
                    "problem",
                    "in",
                    "the",
                    "area",
                    "of",
                    "predicate",
                    "subcategorisation,",
                    "argument",
                    "structure",
                    "and",
                    "semantic",
                    "preferences,",
                    "an",
                    "area",
                    "in",
                    "which",
                    "most",
                    "extant",
                    "conventional",
                    "dictionaries,",
                    "lexical",
                    "databases",
                    "and",
                    "realistic",
                    "lexicons",
                    "are",
                    "demonstrably",
                    "weak",
                    "or",
                    "-when",
                    "available",
                    "-by",
                    "necessity",
                    "never",
                    "complete."
                ]
            ],
            "context": [
                3
            ]
        },
        "input": "sent0: The SPARKLE project has shown (see #TARGET_REF how far simple robust phrasal parsing combined with classification techniques utilising limited and manageable linguistic knowledge and statistical data from substantial corpora can ameliorate this problem in the area of predicate subcategorisation, argument structure and semantic preferences, an area in which most extant conventional dictionaries, lexical databases and realistic lexicons are demonstrably weak or -when available -by necessity never complete.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "Transformer",
                    "architecture",
                    "#REF",
                    "has",
                    "been",
                    "successful",
                    "in",
                    "a",
                    "wide",
                    "range",
                    "of",
                    "natural",
                    "language",
                    "processing",
                    "tasks,",
                    "including",
                    "machine",
                    "translation",
                    "#REF",
                    ",",
                    "language",
                    "modeling",
                    "#REF",
                    ",",
                    "question-answering",
                    "#REF",
                    ",",
                    "and",
                    "many",
                    "more."
                ],
                [
                    "Transformers",
                    "pre-trained",
                    "on",
                    "large",
                    "amounts",
                    "of",
                    "text",
                    "with",
                    "a",
                    "language",
                    "modeling",
                    "(LM)",
                    "objective,",
                    "have",
                    "become",
                    "the",
                    "standard",
                    "in",
                    "NLP,",
                    "exhibiting",
                    "surprising",
                    "amounts",
                    "of",
                    "linguistic",
                    "and",
                    "world",
                    "knowledge",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                3,
                1
            ]
        },
        "input": "sent0: The Transformer architecture #REF has been successful in a wide range of natural language processing tasks, including machine translation #REF , language modeling #REF , question-answering #REF , and many more.\n sent1: Transformers pre-trained on large amounts of text with a language modeling (LM) objective, have become the standard in NLP, exhibiting surprising amounts of linguistic and world knowledge #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "To",
                    "the",
                    "best",
                    "of",
                    "our",
                    "knowledge,",
                    "we",
                    "are",
                    "the",
                    "first",
                    "work",
                    "to",
                    "explore",
                    "diverse",
                    "knowledge",
                    "reasoning",
                    "on",
                    "commonsense",
                    "KG",
                    "to",
                    "generate",
                    "multiple",
                    "diverse",
                    "output",
                    "sequences."
                ],
                [
                    "Therefore,",
                    "we",
                    "only",
                    "compared",
                    "our",
                    "MoKGE",
                    "with",
                    "existing",
                    "diversity-promoting",
                    "baselines",
                    "without",
                    "using",
                    "knowledge",
                    "graph."
                ],
                [
                    "VAE-based",
                    "method."
                ],
                [
                    "The",
                    "variational",
                    "auto-encoder",
                    "(VAE)",
                    "(Kingma",
                    "and",
                    "Welling,",
                    "2014)",
                    "is",
                    "a",
                    "deep",
                    "generative",
                    "latent",
                    "variable",
                    "model."
                ],
                [
                    "VAE-based",
                    "methods",
                    "produce",
                    "diverse",
                    "outputs",
                    "by",
                    "sampling",
                    "different",
                    "latent",
                    "variables",
                    "from",
                    "an",
                    "approximate",
                    "posterior",
                    "distribution."
                ],
                [
                    "CVAE-SVG",
                    "(SVG",
                    "is",
                    "short",
                    "for",
                    "sentence",
                    "variant",
                    "generation)",
                    "#REF",
                    ")",
                    "is",
                    "a",
                    "conditional",
                    "VAE",
                    "model",
                    "that",
                    "can",
                    "produce",
                    "multiple",
                    "outputs",
                    "based",
                    "an",
                    "original",
                    "sentence",
                    "as",
                    "input."
                ],
                [
                    "MoE-based",
                    "method."
                ],
                [
                    "Mixture",
                    "models",
                    "provide",
                    "an",
                    "alternative",
                    "approach",
                    "to",
                    "generate",
                    "diverse",
                    "outputs",
                    "by",
                    "sampling",
                    "different",
                    "mixture",
                    "components."
                ],
                [
                    "We",
                    "compare",
                    "against",
                    "two",
                    "mixture",
                    "of",
                    "experts",
                    "(MoE)",
                    "implementations",
                    "by",
                    "#REF",
                    "and",
                    "#REF",
                    "."
                ],
                [
                    "We",
                    "refer",
                    "them",
                    "as",
                    "MoE-prompt",
                    "#REF",
                    "and",
                    "MoE-embed",
                    "#REF",
                    "."
                ],
                [
                    "Sampling-based",
                    "method."
                ],
                [
                    "Sampling",
                    "methods",
                    "create",
                    "diverse",
                    "outputs",
                    "by",
                    "sampling",
                    "next",
                    "token",
                    "widely",
                    "from",
                    "the",
                    "vocabulary."
                ],
                [
                    "We",
                    "compare",
                    "against",
                    "two",
                    "sampling",
                    "algorithms",
                    "for",
                    "decoding,",
                    "including",
                    "truncated",
                    "sampling",
                    "#TARGET_REF",
                    "and",
                    "nucleus",
                    "sampling",
                    "#REF",
                    "."
                ],
                [
                    "Truncated",
                    "sampling",
                    "#REF",
                    "randomly",
                    "samples",
                    "words",
                    "from",
                    "top-k",
                    "probability",
                    "candidates",
                    "of",
                    "the",
                    "predicted",
                    "distribution",
                    "at",
                    "each",
                    "decoding",
                    "step."
                ],
                [
                    "Nucleus",
                    "sampling",
                    "#REF",
                    ")",
                    "avoids",
                    "text",
                    "degeneration",
                    "by",
                    "truncating",
                    "the",
                    "unreliable",
                    "tails",
                    "and",
                    "sampling",
                    "from",
                    "the",
                    "dynamic",
                    "nucleus",
                    "of",
                    "tokens",
                    "containing",
                    "the",
                    "vast",
                    "majority",
                    "of",
                    "the",
                    "probability",
                    "mass."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                3,
                0
            ]
        },
        "input": "sent0: To the best of our knowledge, we are the first work to explore diverse knowledge reasoning on commonsense KG to generate multiple diverse output sequences.\n sent1: Therefore, we only compared our MoKGE with existing diversity-promoting baselines without using knowledge graph.\n sent2: VAE-based method.\n sent3: The variational auto-encoder (VAE) (Kingma and Welling, 2014) is a deep generative latent variable model.\n sent4: VAE-based methods produce diverse outputs by sampling different latent variables from an approximate posterior distribution.\n sent5: CVAE-SVG (SVG is short for sentence variant generation) #REF ) is a conditional VAE model that can produce multiple outputs based an original sentence as input.\n sent6: MoE-based method.\n sent7: Mixture models provide an alternative approach to generate diverse outputs by sampling different mixture components.\n sent8: We compare against two mixture of experts (MoE) implementations by #REF and #REF .\n sent9: We refer them as MoE-prompt #REF and MoE-embed #REF .\n sent10: Sampling-based method.\n sent11: Sampling methods create diverse outputs by sampling next token widely from the vocabulary.\n sent12: We compare against two sampling algorithms for decoding, including truncated sampling #TARGET_REF and nucleus sampling #REF .\n sent13: Truncated sampling #REF randomly samples words from top-k probability candidates of the predicted distribution at each decoding step.\n sent14: Nucleus sampling #REF ) avoids text degeneration by truncating the unreliable tails and sampling from the dynamic nucleus of tokens containing the vast majority of the probability mass.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent12\"], \"BACKGROUND\": [\"sent13\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "use",
                    "a",
                    "three-layer",
                    "transformer",
                    "#TARGET_REF",
                    "with",
                    "a",
                    "hidden",
                    "size",
                    "of",
                    "128",
                    "and",
                    "4",
                    "heads",
                    "as",
                    "our",
                    "base",
                    "model",
                    "for",
                    "content",
                    "planning",
                    "and",
                    "response",
                    "generation,",
                    "i.e.,",
                    "p",
                    "l",
                    "(a|c)",
                    "and",
                    "p",
                    "r",
                    "(a,",
                    "c)",
                    ",",
                    "respectively."
                ],
                [
                    "We",
                    "use",
                    "grid",
                    "search",
                    "to",
                    "find",
                    "the",
                    "best",
                    "hyperparameters",
                    "for",
                    "the",
                    "models",
                    "based",
                    "on",
                    "validation",
                    "performance,",
                    "which",
                    "we",
                    "use",
                    "a",
                    "combination",
                    "of",
                    "Inform,",
                    "Success",
                    "and",
                    "BLEU",
                    "scores",
                    "to",
                    "measure."
                ],
                [
                    "We",
                    "choose",
                    "the",
                    "embedding",
                    "dimensionality",
                    "d",
                    "among",
                    "{50,",
                    "75,",
                    "100,",
                    "150,",
                    "200},",
                    "the",
                    "hyperparameters",
                    "α",
                    "and",
                    "β",
                    "in",
                    "[0.01,",
                    "1.0]."
                ]
            ],
            "context": [
                1,
                0,
                0
            ]
        },
        "input": "sent0: We use a three-layer transformer #TARGET_REF with a hidden size of 128 and 4 heads as our base model for content planning and response generation, i.e., p l (a|c) and p r (a, c) , respectively.\n sent1: We use grid search to find the best hyperparameters for the models based on validation performance, which we use a combination of Inform, Success and BLEU scores to measure.\n sent2: We choose the embedding dimensionality d among {50, 75, 100, 150, 200}, the hyperparameters α and β in [0.01, 1.0].\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Ortega",
                    "et",
                    "al."
                ],
                [
                    "(",
                    "2020b)",
                    "used",
                    "morphological",
                    "information,",
                    "such",
                    "as",
                    "affixes,",
                    "to",
                    "guide",
                    "the",
                    "Byte-Pair-Encoding",
                    "(BPE)",
                    "segmentation",
                    "algorithm",
                    "#REF",
                    "for",
                    "Quechua."
                ],
                [
                    "However,",
                    "their",
                    "improvement",
                    "is",
                    "not",
                    "significant,",
                    "and",
                    "according",
                    "to",
                    "#REF",
                    ",",
                    "BPE",
                    "tends",
                    "to",
                    "oversplit",
                    "roots",
                    "of",
                    "infrequent",
                    "words."
                ],
                [
                    "They",
                    "showed",
                    "that",
                    "a",
                    "unigram",
                    "language",
                    "model",
                    "#TARGET_REF",
                    "seems",
                    "like",
                    "a",
                    "better",
                    "alternative",
                    "to",
                    "split",
                    "affixes",
                    "and",
                    "preserve",
                    "roots",
                    "(in",
                    "English",
                    "and",
                    "Japanese)."
                ]
            ],
            "context": [
                0,
                0,
                3,
                2
            ]
        },
        "input": "sent0: Ortega et al.\n sent1: ( 2020b) used morphological information, such as affixes, to guide the Byte-Pair-Encoding (BPE) segmentation algorithm #REF for Quechua.\n sent2: However, their improvement is not significant, and according to #REF , BPE tends to oversplit roots of infrequent words.\n sent3: They showed that a unigram language model #TARGET_REF seems like a better alternative to split affixes and preserve roots (in English and Japanese).\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent3\"], \"BACKGROUND\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "conduct",
                    "the",
                    "experiments",
                    "on",
                    "two",
                    "popular",
                    "QA",
                    "benchmarks:",
                    "MSMARCO",
                    "Passage",
                    "Ranking",
                    "#TARGET_REF",
                    "and",
                    "Natural",
                    "Questions",
                    "(NQ)",
                    "#REF",
                    "."
                ],
                [
                    "The",
                    "statistics",
                    "of",
                    "the",
                    "datasets",
                    "are",
                    "listed",
                    "in",
                    "Table",
                    "1."
                ],
                [
                    "MSMARCO",
                    "Passage",
                    "Ranking",
                    "MSMARCO",
                    "is",
                    "originally",
                    "designed",
                    "for",
                    "multiple",
                    "passage",
                    "MRC,",
                    "and",
                    "its",
                    "questions",
                    "were",
                    "sampled",
                    "from",
                    "Bing",
                    "search",
                    "logs."
                ],
                [
                    "Based",
                    "on",
                    "the",
                    "questions",
                    "and",
                    "passages",
                    "in",
                    "MS-MARCO",
                    "Question",
                    "Answering,",
                    "a",
                    "dataset",
                    "for",
                    "passage",
                    "ranking",
                    "was",
                    "created,",
                    "namely",
                    "MSMARCO",
                    "Passage",
                    "Ranking,",
                    "consisting",
                    "of",
                    "about",
                    "8.8",
                    "million",
                    "passages."
                ],
                [
                    "The",
                    "goal",
                    "is",
                    "to",
                    "find",
                    "positive",
                    "passages",
                    "that",
                    "answer",
                    "the",
                    "questions."
                ]
            ],
            "context": [
                2,
                3,
                1,
                2,
                3
            ]
        },
        "input": "sent0: We conduct the experiments on two popular QA benchmarks: MSMARCO Passage Ranking #TARGET_REF and Natural Questions (NQ) #REF .\n sent1: The statistics of the datasets are listed in Table 1.\n sent2: MSMARCO Passage Ranking MSMARCO is originally designed for multiple passage MRC, and its questions were sampled from Bing search logs.\n sent3: Based on the questions and passages in MS-MARCO Question Answering, a dataset for passage ranking was created, namely MSMARCO Passage Ranking, consisting of about 8.8 million passages.\n sent4: The goal is to find positive passages that answer the questions.\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [\"sent0\", \"sent3\"], \"BACKGROUND\": [\"sent1\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "1986",
                    "Grosseto",
                    "(Tuscany)",
                    "Workshop",
                    "\"On",
                    "automating",
                    "the",
                    "lexicon\"",
                    "#REF",
                    "is",
                    "usually",
                    "recognised",
                    "as",
                    "the",
                    "event",
                    "marking",
                    "an",
                    "inversion",
                    "of",
                    "tendency",
                    "and",
                    "the",
                    "starting",
                    "point",
                    "of",
                    "the",
                    "process",
                    "which",
                    "gradually",
                    "brought",
                    "the",
                    "major",
                    "actors",
                    "of",
                    "the",
                    "NLP",
                    "sector",
                    "to",
                    "pay",
                    "more",
                    "and",
                    "more",
                    "attention",
                    "to",
                    "reusable",
                    "language",
                    "resources."
                ],
                [
                    "This",
                    "process,",
                    "which",
                    "was",
                    "fostered",
                    "by",
                    "a",
                    "number",
                    "of",
                    "initiatives",
                    "which",
                    "followed",
                    "directly",
                    "from",
                    "the",
                    "Grosseto",
                    "workshop,",
                    "achieved",
                    "a",
                    "crucial",
                    "step",
                    "through",
                    "the",
                    "recognition,",
                    "in",
                    "the",
                    "so-",
                    "#REF",
                    ",",
                    "of",
                    "the",
                    "infrastructural",
                    "role",
                    "of",
                    "LR",
                    "(see",
                    "also",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "This",
                    "was",
                    "very",
                    "influential",
                    "in",
                    "the",
                    "formation",
                    "of",
                    "the",
                    "strategy",
                    "of",
                    "the",
                    "European",
                    "Commission",
                    "(EC)."
                ],
                [
                    "In",
                    "fact,",
                    "the",
                    "issue",
                    "of",
                    "LR",
                    "is",
                    "now",
                    "regularly",
                    "present",
                    "in",
                    "the",
                    "initiatives",
                    "of",
                    "the",
                    "EC",
                    "in",
                    "the",
                    "field",
                    "of",
                    "language",
                    "processing."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: The 1986 Grosseto (Tuscany) Workshop \"On automating the lexicon\" #REF is usually recognised as the event marking an inversion of tendency and the starting point of the process which gradually brought the major actors of the NLP sector to pay more and more attention to reusable language resources.\n sent1: This process, which was fostered by a number of initiatives which followed directly from the Grosseto workshop, achieved a crucial step through the recognition, in the so- #REF , of the infrastructural role of LR (see also #TARGET_REF .\n sent2: This was very influential in the formation of the strategy of the European Commission (EC).\n sent3: In fact, the issue of LR is now regularly present in the initiatives of the EC in the field of language processing.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "design",
                    "of",
                    "the",
                    "system",
                    "is",
                    "in",
                    "response",
                    "to",
                    "perceived",
                    "weaknesses",
                    "in",
                    "the",
                    "classical",
                    "approach",
                    "to",
                    "MT,",
                    "as",
                    "still",
                    "found",
                    "in",
                    "so-called",
                    "state-of-the-art",
                    "developments",
                    "(cf."
                ],
                [
                    "#TARGET_REF",
                    "):",
                    "notably",
                    "these",
                    "include",
                    "reliance",
                    "on",
                    "structure-preserving",
                    "translation",
                    "as",
                    "a",
                    "first",
                    "choice,",
                    "a",
                    "stratificational",
                    "approach",
                    "to",
                    "both",
                    "linguistic",
                    "and",
                    "computing",
                    "aspects,",
                    "leading",
                    "to",
                    "a",
                    "predominantly",
                    "bottom-up",
                    "compositional",
                    "system",
                    "design,",
                    "and",
                    "the",
                    "reliance",
                    "on",
                    "the",
                    "intuitions",
                    "and",
                    "introspection",
                    "of",
                    "linguists",
                    "rather",
                    "than",
                    "'real'",
                    "data."
                ],
                [
                    "The",
                    "system",
                    "design",
                    "is",
                    "influenced",
                    "by",
                    "a",
                    "number",
                    "of",
                    "recent",
                    "research",
                    "directions",
                    "including",
                    "Dialogue-Based",
                    "MT",
                    "with",
                    "a",
                    "monolingual",
                    "user,",
                    "Example-Based",
                    "MT,",
                    "corpus-based",
                    "MT,",
                    "and",
                    "the",
                    "use",
                    "of",
                    "sublanguage."
                ]
            ],
            "context": [
                3,
                3,
                0
            ]
        },
        "input": "sent0: The design of the system is in response to perceived weaknesses in the classical approach to MT, as still found in so-called state-of-the-art developments (cf.\n sent1: #TARGET_REF ): notably these include reliance on structure-preserving translation as a first choice, a stratificational approach to both linguistic and computing aspects, leading to a predominantly bottom-up compositional system design, and the reliance on the intuitions and introspection of linguists rather than 'real' data.\n sent2: The system design is influenced by a number of recent research directions including Dialogue-Based MT with a monolingual user, Example-Based MT, corpus-based MT, and the use of sublanguage.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Electro-/Magnetoencephalography",
                    "(EEG/MEG),",
                    "which",
                    "measures",
                    "neural",
                    "activity",
                    "at",
                    "millisecond",
                    "resolution,",
                    "is",
                    "a",
                    "key",
                    "neuroscientific",
                    "method",
                    "to",
                    "assess",
                    "how",
                    "neural",
                    "representations",
                    "unfold",
                    "dynamically",
                    "in",
                    "language",
                    "processing."
                ],
                [
                    "Early",
                    "event",
                    "related",
                    "potential",
                    "(ERP)",
                    "studies",
                    "that",
                    "rely",
                    "on",
                    "averaging",
                    "EEG",
                    "activity",
                    "across",
                    "multiple",
                    "trials",
                    "have",
                    "shown",
                    "that",
                    "EEG",
                    "signal",
                    "magnitude",
                    "and",
                    "topography",
                    "depend",
                    "on",
                    "word",
                    "length,",
                    "frequency",
                    "and",
                    "open",
                    "vs.",
                    "closed",
                    "class."
                ],
                [
                    "Word",
                    "length",
                    "effects",
                    "arose",
                    "in",
                    "EEG",
                    "at",
                    "about",
                    "150",
                    "ms,",
                    "frequency",
                    "effects",
                    "at",
                    "200",
                    "ms",
                    "and",
                    "word",
                    "class",
                    "effects",
                    "from",
                    "400-700",
                    "ms",
                    "#REF",
                    "."
                ],
                [
                    "Recent",
                    "studies",
                    "were",
                    "able",
                    "to",
                    "predict",
                    "these",
                    "and",
                    "other",
                    "(e.g."
                ],
                [
                    "semantic)",
                    "aspects",
                    "based",
                    "on",
                    "single",
                    "trial",
                    "multi-channel",
                    "EEG/MEG",
                    "activity",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Importantly,",
                    "the",
                    "aim",
                    "of",
                    "cognitive",
                    "neuroscience",
                    "studies",
                    "is",
                    "to",
                    "dissociate",
                    "when",
                    "(i.e."
                ],
                [
                    "latency)",
                    "and",
                    "where",
                    "(i.e."
                ]
            ],
            "context": [
                0,
                0,
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "sent0: Electro-/Magnetoencephalography (EEG/MEG), which measures neural activity at millisecond resolution, is a key neuroscientific method to assess how neural representations unfold dynamically in language processing.\n sent1: Early event related potential (ERP) studies that rely on averaging EEG activity across multiple trials have shown that EEG signal magnitude and topography depend on word length, frequency and open vs. closed class.\n sent2: Word length effects arose in EEG at about 150 ms, frequency effects at 200 ms and word class effects from 400-700 ms #REF .\n sent3: Recent studies were able to predict these and other (e.g.\n sent4: semantic) aspects based on single trial multi-channel EEG/MEG activity #TARGET_REF .\n sent5: Importantly, the aim of cognitive neuroscience studies is to dissociate when (i.e.\n sent6: latency) and where (i.e.\n",
        "output": "{\"INFORMATION\": [\"sent3\", \"sent4\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Although",
                    "there",
                    "are",
                    "some",
                    "efforts",
                    "to",
                    "detect",
                    "nonacceptable",
                    "language",
                    "in",
                    "Portuguese,",
                    "they",
                    "evaluate",
                    "the",
                    "developed",
                    "approach",
                    "in",
                    "their",
                    "own",
                    "corpus,",
                    "making",
                    "a",
                    "fair",
                    "comparison",
                    "among",
                    "the",
                    "models",
                    "difficult."
                ],
                [
                    "Moreover,",
                    "these",
                    "corpora",
                    "are",
                    "much",
                    "smaller",
                    "when",
                    "compared",
                    "to",
                    "corpora",
                    "of",
                    "other",
                    "languages",
                    "#TARGET_REF",
                    "and",
                    "than",
                    "the",
                    "ToLD-Br",
                    "corpus."
                ],
                [
                    "This",
                    "fact",
                    "makes",
                    "the",
                    "development",
                    "of",
                    "robust",
                    "strategies",
                    "to",
                    "handle",
                    "toxic",
                    "comments",
                    "difficult,",
                    "as",
                    "they",
                    "usually",
                    "require",
                    "a",
                    "large",
                    "corpus."
                ],
                [
                    "As",
                    "one",
                    "can",
                    "see",
                    "in",
                    "Table",
                    "1,",
                    "the",
                    "corpus",
                    "has",
                    "a",
                    "little",
                    "more",
                    "non-toxic",
                    "than",
                    "toxic",
                    "tweets."
                ],
                [
                    "In",
                    "this",
                    "paper,",
                    "we",
                    "adopted",
                    "the",
                    "binary",
                    "version",
                    "of",
                    "the",
                    "corpus,",
                    "i.e.,",
                    "our",
                    "objective",
                    "is",
                    "to",
                    "identify",
                    "if",
                    "a",
                    "comment",
                    "is",
                    "toxic",
                    "or",
                    "non-toxic."
                ]
            ],
            "context": [
                0,
                2,
                2,
                0,
                0
            ]
        },
        "input": "sent0: Although there are some efforts to detect nonacceptable language in Portuguese, they evaluate the developed approach in their own corpus, making a fair comparison among the models difficult.\n sent1: Moreover, these corpora are much smaller when compared to corpora of other languages #TARGET_REF and than the ToLD-Br corpus.\n sent2: This fact makes the development of robust strategies to handle toxic comments difficult, as they usually require a large corpus.\n sent3: As one can see in Table 1, the corpus has a little more non-toxic than toxic tweets.\n sent4: In this paper, we adopted the binary version of the corpus, i.e., our objective is to identify if a comment is toxic or non-toxic.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\", \"sent2\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "use",
                    "AI4Bharat's",
                    "IndicTrans",
                    "2",
                    "#REF",
                    "for",
                    "translation,",
                    "which",
                    "is",
                    "a",
                    "Transformer-4X",
                    "model",
                    "trained",
                    "on",
                    "Samanantar",
                    "dataset",
                    "#REF",
                    "."
                ],
                [
                    "In",
                    "IndicTrans,",
                    "translation",
                    "can",
                    "be",
                    "done",
                    "from",
                    "Indian",
                    "languages",
                    "to",
                    "English",
                    "and",
                    "vice",
                    "versa."
                ],
                [
                    "Available",
                    "Indian",
                    "languages",
                    "include",
                    "Assamese,",
                    "Bengali,",
                    "Gujarati,",
                    "Hindi,",
                    "Kannada,",
                    "Malayalam,",
                    "Marathi,",
                    "Oriya,",
                    "Punjabi,",
                    "Tamil,",
                    "and",
                    "Telugu."
                ],
                [
                    "At",
                    "first,",
                    "we",
                    "translate",
                    "the",
                    "ChAII",
                    "dataset",
                    "from",
                    "Hindi",
                    "and",
                    "Tamil",
                    "to",
                    "English",
                    "and",
                    "then",
                    "to",
                    "Bengali,",
                    "Marathi,",
                    "Malayalam,",
                    "and",
                    "Telugu."
                ],
                [
                    "In",
                    "the",
                    "FLORES",
                    "devset",
                    "benchmark",
                    "#REF",
                    ",",
                    "the",
                    "BLEU",
                    "scores",
                    "of",
                    "IndicTrans",
                    "for",
                    "translating",
                    "Hindi",
                    "and",
                    "Tamil",
                    "to",
                    "English",
                    "are",
                    "37.9",
                    "and",
                    "28.6,",
                    "respectively."
                ],
                [
                    "The",
                    "scores",
                    "#TARGET_REF",
                    "for",
                    "translating",
                    "English",
                    "to",
                    "Bengali,",
                    "Marathi,",
                    "#REF",
                    ".0,",
                    "respectively."
                ],
                [
                    "We",
                    "were",
                    "not",
                    "able",
                    "to",
                    "translate",
                    "nearly",
                    "500",
                    "of",
                    "the",
                    "ChAII",
                    "instances",
                    "to",
                    "English",
                    "as",
                    "the",
                    "automatic",
                    "search",
                    "for",
                    "the",
                    "translated",
                    "answers",
                    "in",
                    "the",
                    "translated",
                    "contexts",
                    "failed."
                ],
                [
                    "This",
                    "happened",
                    "because",
                    "the",
                    "same",
                    "word",
                    "got",
                    "translated",
                    "differently",
                    "in",
                    "the",
                    "context",
                    "and",
                    "the",
                    "answer."
                ],
                [
                    "For",
                    "the",
                    "same",
                    "reason,",
                    "we",
                    "lost",
                    "nearly",
                    "another",
                    "200",
                    "instances",
                    "when",
                    "translating",
                    "from",
                    "English",
                    "to",
                    "other",
                    "Indian",
                    "languages."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                3,
                0,
                0,
                0
            ]
        },
        "input": "sent0: We use AI4Bharat's IndicTrans 2 #REF for translation, which is a Transformer-4X model trained on Samanantar dataset #REF .\n sent1: In IndicTrans, translation can be done from Indian languages to English and vice versa.\n sent2: Available Indian languages include Assamese, Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Oriya, Punjabi, Tamil, and Telugu.\n sent3: At first, we translate the ChAII dataset from Hindi and Tamil to English and then to Bengali, Marathi, Malayalam, and Telugu.\n sent4: In the FLORES devset benchmark #REF , the BLEU scores of IndicTrans for translating Hindi and Tamil to English are 37.9 and 28.6, respectively.\n sent5: The scores #TARGET_REF for translating English to Bengali, Marathi, #REF .0, respectively.\n sent6: We were not able to translate nearly 500 of the ChAII instances to English as the automatic search for the translated answers in the translated contexts failed.\n sent7: This happened because the same word got translated differently in the context and the answer.\n sent8: For the same reason, we lost nearly another 200 instances when translating from English to other Indian languages.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent5\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Token-level",
                    "Slot",
                    "Annotation",
                    "With",
                    "the",
                    "processed",
                    "token-level",
                    "data,",
                    "an",
                    "automated",
                    "slot",
                    "labelling",
                    "is",
                    "performed."
                ],
                [
                    "Initially,",
                    "we",
                    "create",
                    "six",
                    "distinct",
                    "slot",
                    "labels:",
                    "T",
                    "(Toxicity),",
                    "C",
                    "(Character),",
                    "D",
                    "(Dotaspecific),",
                    "S",
                    "(game",
                    "Slang),",
                    "P",
                    "(Pronoun)",
                    "and",
                    "O",
                    "(Other)."
                ],
                [
                    "To",
                    "construct",
                    "the",
                    "T",
                    "lexicon,",
                    "we",
                    "combine",
                    "several",
                    "toxicity",
                    "lexicons",
                    "(see",
                    "Section",
                    "8",
                    "Ethics)",
                    "and",
                    "remove",
                    "overlaps."
                ],
                [
                    "We",
                    "also",
                    "use",
                    "the",
                    "supplemental",
                    "data",
                    "sourced",
                    "by",
                    "#TARGET_REF",
                    "for",
                    "the",
                    "gamerelated",
                    "lexicons",
                    "(C,",
                    "D",
                    "and",
                    "S)",
                    "and",
                    "carefully",
                    "modify",
                    "it."
                ],
                [
                    "The",
                    "P",
                    "lexicon",
                    "(e.g."
                ],
                [
                    "'u',",
                    "'ur')",
                    "is",
                    "constructed",
                    "by",
                    "this",
                    "research",
                    "because",
                    "in-game",
                    "chat",
                    "is",
                    "extremely",
                    "abbreviated."
                ],
                [
                    "Then,",
                    "we",
                    "perform",
                    "lexicon-based",
                    "automation",
                    "by",
                    "exact",
                    "matching",
                    "each",
                    "lower-cased",
                    "token",
                    "against",
                    "the",
                    "lexicons."
                ],
                [
                    "Anything",
                    "not",
                    "matching",
                    "a",
                    "lexicon",
                    "is",
                    "labelled",
                    "O."
                ],
                [
                    "We",
                    "contrast",
                    "this",
                    "with",
                    "typical",
                    "NLU",
                    "slot",
                    "labelling",
                    "where",
                    "a",
                    "semantic",
                    "concept",
                    "can",
                    "stretch",
                    "over",
                    "a",
                    "span",
                    "of",
                    "words."
                ],
                [
                    "In",
                    "comparison",
                    "to",
                    "other",
                    "toxicity",
                    "datasets,",
                    "our",
                    "lexicon-based",
                    "slot",
                    "labelling",
                    "enables",
                    "deeper",
                    "understanding",
                    "of",
                    "game",
                    "context."
                ]
            ],
            "context": [
                0,
                0,
                2,
                2,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Token-level Slot Annotation With the processed token-level data, an automated slot labelling is performed.\n sent1: Initially, we create six distinct slot labels: T (Toxicity), C (Character), D (Dotaspecific), S (game Slang), P (Pronoun) and O (Other).\n sent2: To construct the T lexicon, we combine several toxicity lexicons (see Section 8 Ethics) and remove overlaps.\n sent3: We also use the supplemental data sourced by #TARGET_REF for the gamerelated lexicons (C, D and S) and carefully modify it.\n sent4: The P lexicon (e.g.\n sent5: 'u', 'ur') is constructed by this research because in-game chat is extremely abbreviated.\n sent6: Then, we perform lexicon-based automation by exact matching each lower-cased token against the lexicons.\n sent7: Anything not matching a lexicon is labelled O.\n sent8: We contrast this with typical NLU slot labelling where a semantic concept can stretch over a span of words.\n sent9: In comparison to other toxicity datasets, our lexicon-based slot labelling enables deeper understanding of game context.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent2\", \"sent3\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Word",
                    "co-occurrence",
                    "probabilities",
                    "are",
                    "hard",
                    "to",
                    "estimate",
                    "accurately",
                    "from",
                    "text",
                    "data",
                    "because",
                    "empirical",
                    "counts",
                    "of",
                    "a",
                    "particular",
                    "pair",
                    "of",
                    "words",
                    "in",
                    "a",
                    "particular",
                    "relation",
                    "are",
                    "often",
                    "sparse."
                ],
                [
                    "This",
                    "limitation",
                    "makes",
                    "it",
                    "hard",
                    "to",
                    "evaluate",
                    "cognitive",
                    "theories",
                    "that",
                    "operate",
                    "on",
                    "co-occurrence",
                    "probabilities."
                ],
                [
                    "Although",
                    "high-performance",
                    "pretrained",
                    "language",
                    "models",
                    "now",
                    "exist",
                    "#TARGET_REF",
                    ",",
                    "the",
                    "probabilities",
                    "of",
                    "interest",
                    "often",
                    "cannot",
                    "be",
                    "read",
                    "off",
                    "of",
                    "these",
                    "models",
                    "directly,",
                    "because",
                    "w",
                    "and",
                    "c",
                    "might",
                    "be",
                    "defined",
                    "by",
                    "relations",
                    "that",
                    "cannot",
                    "be",
                    "straightforwardly",
                    "detected",
                    "in",
                    "terms",
                    "of",
                    "linear",
                    "word",
                    "order",
                    "or",
                    "templates."
                ],
                [
                    "For",
                    "example,",
                    "suppose",
                    "we",
                    "are",
                    "interested",
                    "in",
                    "the",
                    "distribution",
                    "of",
                    "adjectives",
                    "attributively",
                    "modifying",
                    "a",
                    "noun",
                    "in",
                    "English."
                ],
                [
                    "It",
                    "would",
                    "not",
                    "do",
                    "to",
                    "ask",
                    "a",
                    "language",
                    "model",
                    "for",
                    "the",
                    "distribution",
                    "of",
                    "words",
                    "immediately",
                    "preceding",
                    "a",
                    "noun,",
                    "because",
                    "some",
                    "of",
                    "these",
                    "words",
                    "will",
                    "not",
                    "be",
                    "attributive",
                    "adjectives."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Word co-occurrence probabilities are hard to estimate accurately from text data because empirical counts of a particular pair of words in a particular relation are often sparse.\n sent1: This limitation makes it hard to evaluate cognitive theories that operate on co-occurrence probabilities.\n sent2: Although high-performance pretrained language models now exist #TARGET_REF , the probabilities of interest often cannot be read off of these models directly, because w and c might be defined by relations that cannot be straightforwardly detected in terms of linear word order or templates.\n sent3: For example, suppose we are interested in the distribution of adjectives attributively modifying a noun in English.\n sent4: It would not do to ask a language model for the distribution of words immediately preceding a noun, because some of these words will not be attributive adjectives.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "They",
                    "found",
                    "that",
                    "the",
                    "behavior",
                    "of",
                    "human",
                    "players",
                    "is",
                    "best",
                    "modeled",
                    "on",
                    "the",
                    "probabilities",
                    "of",
                    "bigrams,",
                    "which",
                    "is",
                    "in",
                    "line",
                    "with",
                    "the",
                    "results",
                    "of",
                    "(Spence",
                    "and",
                    "Owens,",
                    "1990)",
                    "(although",
                    "the",
                    "latter",
                    "calculated",
                    "cooccurrences",
                    "with",
                    "much",
                    "larger",
                    "window",
                    "size)."
                ],
                [
                    "#TARGET_REF",
                    "were",
                    "the",
                    "first",
                    "to",
                    "build",
                    "agents",
                    "designed",
                    "explicitly",
                    "to",
                    "play",
                    "the",
                    "game."
                ],
                [
                    "As",
                    "a",
                    "background",
                    "to",
                    "their",
                    "relatedness",
                    "measure,",
                    "they",
                    "used",
                    "•",
                    "CBOW,",
                    "Skip-gram",
                    "and",
                    "GloVe",
                    "word",
                    "embeddings",
                    "(in",
                    "multiple",
                    "configurations),"
                ]
            ],
            "context": [
                0,
                1,
                1
            ]
        },
        "input": "sent0: They found that the behavior of human players is best modeled on the probabilities of bigrams, which is in line with the results of (Spence and Owens, 1990) (although the latter calculated cooccurrences with much larger window size).\n sent1: #TARGET_REF were the first to build agents designed explicitly to play the game.\n sent2: As a background to their relatedness measure, they used • CBOW, Skip-gram and GloVe word embeddings (in multiple configurations),\n",
        "output": "{\"INFORMATION\": [\"sent1\", \"sent2\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "this",
                    "paper,",
                    "we",
                    "introduce",
                    "our",
                    "system",
                    "for",
                    "the",
                    "lexical",
                    "complexity",
                    "prediction",
                    "task",
                    "of",
                    "the",
                    "SemEval-2021",
                    "#REF",
                    "."
                ],
                [
                    "We",
                    "fulfill",
                    "this",
                    "task",
                    "by",
                    "leveraging",
                    "multiple",
                    "pre-trained",
                    "language",
                    "models",
                    "(PLM)",
                    "with",
                    "different",
                    "training",
                    "strategies."
                ],
                [
                    "There",
                    "are",
                    "two",
                    "main",
                    "steps",
                    "for",
                    "our",
                    "system:",
                    "(i)",
                    "fine-tuning",
                    "numbers",
                    "of",
                    "heterogeneous",
                    "PLMs,",
                    "including",
                    "BERT",
                    "#REF",
                    ",",
                    "ALBERT",
                    "#TARGET_REF",
                    ",",
                    "RoBERTa",
                    "#REF",
                    "and",
                    "ERNIE",
                    "#REF",
                    ",",
                    "with",
                    "various",
                    "hyperparameters",
                    "and",
                    "training",
                    "strategies,",
                    "obtaining",
                    "diverse",
                    "models,",
                    "(ii)",
                    "applying",
                    "an",
                    "effective",
                    "stacking",
                    "mechanism",
                    "on",
                    "top",
                    "of",
                    "these",
                    "PLMs",
                    "to",
                    "predict",
                    "the",
                    "final",
                    "complexity",
                    "scores."
                ]
            ],
            "context": [
                0,
                3,
                3
            ]
        },
        "input": "sent0: In this paper, we introduce our system for the lexical complexity prediction task of the SemEval-2021 #REF .\n sent1: We fulfill this task by leveraging multiple pre-trained language models (PLM) with different training strategies.\n sent2: There are two main steps for our system: (i) fine-tuning numbers of heterogeneous PLMs, including BERT #REF , ALBERT #TARGET_REF , RoBERTa #REF and ERNIE #REF , with various hyperparameters and training strategies, obtaining diverse models, (ii) applying an effective stacking mechanism on top of these PLMs to predict the final complexity scores.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "As",
                    "seen",
                    "in",
                    "Figure",
                    "1,",
                    "we",
                    "focus",
                    "on",
                    "creating",
                    "agents",
                    "in",
                    "#TARGET_REF",
                    ",",
                    "a",
                    "large-scale",
                    "crowdsourced",
                    "fantasy",
                    "text-adventure",
                    "game,",
                    "consisting",
                    "of",
                    "rich",
                    "textual",
                    "worlds-locations,",
                    "objects,",
                    "and",
                    "characters",
                    "with",
                    "personas,",
                    "and",
                    "quests-motivations",
                    "for",
                    "each",
                    "character."
                ],
                [
                    "To",
                    "complete",
                    "these",
                    "quests,",
                    "an",
                    "agent",
                    "must:",
                    "(1)",
                    "maintain",
                    "character",
                    "via",
                    "its",
                    "persona,",
                    "and",
                    "(2)",
                    "reason",
                    "in",
                    "a",
                    "partially",
                    "observable",
                    "world",
                    "about",
                    "potential",
                    "actions",
                    "and",
                    "utterances",
                    "based",
                    "on",
                    "incomplete",
                    "descriptions",
                    "of",
                    "the",
                    "locations,",
                    "objects,",
                    "and",
                    "other",
                    "characters."
                ],
                [
                    "This",
                    "requires",
                    "several",
                    "human",
                    "like",
                    "competencies",
                    "such",
                    "as",
                    "commonsense",
                    "reasoning,",
                    "dynamic",
                    "natural",
                    "language",
                    "understanding,",
                    "and",
                    "operating",
                    "in",
                    "combinatorially",
                    "sized",
                    "language-based",
                    "stateaction",
                    "spaces."
                ],
                [
                    "Although",
                    "recent",
                    "work",
                    "has",
                    "provided",
                    "evidence",
                    "showing",
                    "that",
                    "interactive",
                    "language",
                    "learning",
                    "via",
                    "reinforcement",
                    "learning",
                    "(RL)",
                    "in",
                    "text",
                    "games",
                    "can",
                    "be",
                    "significantly",
                    "more",
                    "sample",
                    "efficient",
                    "than",
                    "static",
                    "supervised",
                    "learning",
                    "#REF",
                    "when",
                    "creating",
                    "goal-driven",
                    "natural",
                    "language",
                    "agents,",
                    "their",
                    "ability",
                    "to",
                    "robustly",
                    "generalize",
                    "to",
                    "novel",
                    "scenarios",
                    "is",
                    "limited."
                ]
            ],
            "context": [
                1,
                3,
                3,
                0
            ]
        },
        "input": "sent0: As seen in Figure 1, we focus on creating agents in #TARGET_REF , a large-scale crowdsourced fantasy text-adventure game, consisting of rich textual worlds-locations, objects, and characters with personas, and quests-motivations for each character.\n sent1: To complete these quests, an agent must: (1) maintain character via its persona, and (2) reason in a partially observable world about potential actions and utterances based on incomplete descriptions of the locations, objects, and other characters.\n sent2: This requires several human like competencies such as commonsense reasoning, dynamic natural language understanding, and operating in combinatorially sized language-based stateaction spaces.\n sent3: Although recent work has provided evidence showing that interactive language learning via reinforcement learning (RL) in text games can be significantly more sample efficient than static supervised learning #REF when creating goal-driven natural language agents, their ability to robustly generalize to novel scenarios is limited.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Parametrizing",
                    "Curriculum",
                    "Difficulty."
                ],
                [
                    "Given",
                    "the",
                    "relative",
                    "imbalance",
                    "of",
                    "this",
                    "multinomial",
                    "distribution,",
                    "as",
                    "seen",
                    "in",
                    "Figure",
                    "3,",
                    "we",
                    "hypothesize",
                    "that",
                    "a",
                    "LIGHT",
                    "agent",
                    "only",
                    "learns",
                    "to",
                    "do",
                    "well",
                    "on",
                    "certain",
                    "types",
                    "of",
                    "objectives",
                    "and",
                    "not",
                    "others-memorizing",
                    "trajectories",
                    "for",
                    "less",
                    "seen",
                    "quest",
                    "types,",
                    "i.e."
                ],
                [
                    "those",
                    "found",
                    "in",
                    "the",
                    "tail",
                    "of",
                    "the",
                    "distribution."
                ],
                [
                    "Preliminary",
                    "evidence",
                    "for",
                    "this",
                    "hypothesis",
                    "is",
                    "also",
                    "seen",
                    "in",
                    "#TARGET_REF",
                    ",",
                    "where",
                    "they",
                    "show",
                    "a",
                    "positive",
                    "correlation",
                    "between",
                    "the",
                    "number",
                    "of",
                    "instances",
                    "of",
                    "a",
                    "particular",
                    "type",
                    "of",
                    "quest",
                    "during",
                    "training",
                    "and",
                    "the",
                    "final",
                    "test",
                    "goal-achievement",
                    "performance."
                ],
                [
                    "Based",
                    "on",
                    "these",
                    "observations",
                    "and",
                    "our",
                    "initial",
                    "hypothesis,",
                    "we",
                    "use",
                    "this",
                    "particular",
                    "dimension",
                    "to",
                    "parametrize",
                    "curriculum",
                    "difficulty",
                    "for",
                    "training",
                    "LIGHT",
                    "agents-quest",
                    "types",
                    "that",
                    "are",
                    "rarer",
                    "in",
                    "the",
                    "initial",
                    "training",
                    "data",
                    "will",
                    "be",
                    "harder",
                    "for",
                    "the",
                    "agent",
                    "to",
                    "generalize",
                    "to",
                    "in",
                    "a",
                    "zero-shot",
                    "setting."
                ]
            ],
            "context": [
                0,
                2,
                2,
                1,
                2
            ]
        },
        "input": "sent0: Parametrizing Curriculum Difficulty.\n sent1: Given the relative imbalance of this multinomial distribution, as seen in Figure 3, we hypothesize that a LIGHT agent only learns to do well on certain types of objectives and not others-memorizing trajectories for less seen quest types, i.e.\n sent2: those found in the tail of the distribution.\n sent3: Preliminary evidence for this hypothesis is also seen in #TARGET_REF , where they show a positive correlation between the number of instances of a particular type of quest during training and the final test goal-achievement performance.\n sent4: Based on these observations and our initial hypothesis, we use this particular dimension to parametrize curriculum difficulty for training LIGHT agents-quest types that are rarer in the initial training data will be harder for the agent to generalize to in a zero-shot setting.\n",
        "output": "{\"INFORMATION\": [\"sent3\"], \"PERCEPTION\": [\"sent1\", \"sent2\", \"sent4\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Recent",
                    "advances",
                    "in",
                    "the",
                    "field",
                    "of",
                    "Natural",
                    "Language",
                    "Processing",
                    "(NLP)",
                    "have",
                    "been",
                    "made",
                    "with",
                    "the",
                    "development",
                    "of",
                    "transfer",
                    "learning",
                    "and",
                    "the",
                    "availability",
                    "of",
                    "pre-trained",
                    "language",
                    "models",
                    "based",
                    "on",
                    "Transformer",
                    "architectures",
                    "#REF",
                    ",",
                    "such",
                    "as",
                    "BERT",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "As",
                    "they",
                    "provide",
                    "contextualized",
                    "semantic",
                    "representation",
                    "they",
                    "contribute",
                    "both",
                    "to",
                    "advance",
                    "the",
                    "state-of-the-art",
                    "on",
                    "several",
                    "NLP",
                    "tasks",
                    "and",
                    "also",
                    "to",
                    "evolve",
                    "training",
                    "practices",
                    "through",
                    "the",
                    "use",
                    "of",
                    "fine-tuning."
                ]
            ],
            "context": [
                2,
                2
            ]
        },
        "input": "sent0: Recent advances in the field of Natural Language Processing (NLP) have been made with the development of transfer learning and the availability of pre-trained language models based on Transformer architectures #REF , such as BERT #TARGET_REF .\n sent1: As they provide contextualized semantic representation they contribute both to advance the state-of-the-art on several NLP tasks and also to evolve training practices through the use of fine-tuning.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\", \"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Previous",
                    "experiments",
                    "have",
                    "shown",
                    "the",
                    "effectiveness",
                    "of",
                    "RocketQA",
                    "on",
                    "passage",
                    "retrieval."
                ],
                [
                    "Next,",
                    "we",
                    "verify",
                    "whether",
                    "the",
                    "retrieval",
                    "results",
                    "of",
                    "RocketQA",
                    "can",
                    "improve",
                    "the",
                    "performance",
                    "of",
                    "passage",
                    "reading",
                    "for",
                    "extracting",
                    "correct",
                    "answers."
                ],
                [
                    "We",
                    "implement",
                    "an",
                    "end-to-end",
                    "QA",
                    "system",
                    "in",
                    "which",
                    "we",
                    "have",
                    "an",
                    "extractive",
                    "reader",
                    "stacked",
                    "on",
                    "our",
                    "RocketQA",
                    "retriever."
                ],
                [
                    "For",
                    "a",
                    "fair",
                    "comparison,",
                    "we",
                    "first",
                    "re-use",
                    "the",
                    "released",
                    "model",
                    "6",
                    "of",
                    "the",
                    "extractive",
                    "reader",
                    "in",
                    "DPR",
                    "#REF",
                    ",",
                    "and",
                    "take",
                    "100",
                    "retrieved",
                    "passages",
                    "during",
                    "inference",
                    "(the",
                    "same",
                    "setting",
                    "used",
                    "in",
                    "DPR)."
                ],
                [
                    "Besides,",
                    "6",
                    "https://github.com/facebookresearch/",
                    "DPR",
                    "Model",
                    "EM",
                    "BM25+BERT",
                    "#REF",
                    "26.5",
                    "HardEM",
                    "#REF",
                    "28.1",
                    "GraphRetriever",
                    "#REF",
                    "PathRetriever",
                    "#REF",
                    "32.6",
                    "ORQA",
                    "#REF",
                    "33.3",
                    "REALM",
                    "#TARGET_REF",
                    "40.4",
                    "DPR",
                    "#REF",
                    "41.5",
                    "GAR",
                    "#REF",
                    "41.6",
                    "RocketQA",
                    "+",
                    "DPR",
                    "reader",
                    "42.0",
                    "RocketQA",
                    "+",
                    "re-trained",
                    "DPR",
                    "reader",
                    "42.8"
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                3
            ]
        },
        "input": "sent0: Previous experiments have shown the effectiveness of RocketQA on passage retrieval.\n sent1: Next, we verify whether the retrieval results of RocketQA can improve the performance of passage reading for extracting correct answers.\n sent2: We implement an end-to-end QA system in which we have an extractive reader stacked on our RocketQA retriever.\n sent3: For a fair comparison, we first re-use the released model 6 of the extractive reader in DPR #REF , and take 100 retrieved passages during inference (the same setting used in DPR).\n sent4: Besides, 6 https://github.com/facebookresearch/ DPR Model EM BM25+BERT #REF 26.5 HardEM #REF 28.1 GraphRetriever #REF PathRetriever #REF 32.6 ORQA #REF 33.3 REALM #TARGET_REF 40.4 DPR #REF 41.5 GAR #REF 41.6 RocketQA + DPR reader 42.0 RocketQA + re-trained DPR reader 42.8\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Explicit",
                    "vs."
                ],
                [
                    "Implicit",
                    "NP",
                    "Relations",
                    "Next,",
                    "we",
                    "analyze",
                    "the",
                    "composition",
                    "of",
                    "the",
                    "relations",
                    "in",
                    "the",
                    "dataset,",
                    "as",
                    "to",
                    "whether",
                    "these",
                    "relations",
                    "are",
                    "implicit",
                    "or",
                    "explicit."
                ],
                [
                    "While",
                    "there",
                    "is",
                    "no",
                    "accepted",
                    "definition",
                    "of",
                    "explicit-implicit",
                    "distinction",
                    "in",
                    "the",
                    "literature",
                    "#REF",
                    ",",
                    "here",
                    "we",
                    "adapt",
                    "a",
                    "definition",
                    "originally",
                    "used",
                    "by",
                    "#TARGET_REF",
                    "for",
                    "another",
                    "phenomenon,",
                    "implicit",
                    "arguments:",
                    "14",
                    "In",
                    "an",
                    "implicit",
                    "relation",
                    "the",
                    "anchor",
                    "and",
                    "the",
                    "complement",
                    "are",
                    "not",
                    "syntactically",
                    "connected",
                    "to",
                    "each",
                    "other",
                    "and",
                    "might",
                    "not",
                    "even",
                    "appear",
                    "in",
                    "the",
                    "same",
                    "sentence."
                ],
                [
                    "This",
                    "implies,",
                    "for",
                    "example,",
                    "that",
                    "any",
                    "inter-sentential",
                    "relations",
                    "are",
                    "implicit",
                    "15",
                    ",",
                    "while",
                    "relations",
                    "within",
                    "one",
                    "sentence",
                    "can",
                    "be",
                    "either",
                    "implicit",
                    "or",
                    "explicit."
                ],
                [
                    "We",
                    "sample",
                    "three",
                    "documents",
                    "from",
                    "the",
                    "test-set,",
                    "containing",
                    "590",
                    "links",
                    "in",
                    "total,",
                    "and",
                    "count",
                    "the",
                    "number",
                    "of",
                    "relations",
                    "of",
                    "each",
                    "type."
                ],
                [
                    "Our",
                    "manual",
                    "analysis",
                    "reveals",
                    "that",
                    "89.8%",
                    "of",
                    "the",
                    "relations",
                    "are",
                    "implicit."
                ]
            ],
            "context": [
                0,
                0,
                2,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Explicit vs.\n sent1: Implicit NP Relations Next, we analyze the composition of the relations in the dataset, as to whether these relations are implicit or explicit.\n sent2: While there is no accepted definition of explicit-implicit distinction in the literature #REF , here we adapt a definition originally used by #TARGET_REF for another phenomenon, implicit arguments: 14 In an implicit relation the anchor and the complement are not syntactically connected to each other and might not even appear in the same sentence.\n sent3: This implies, for example, that any inter-sentential relations are implicit 15 , while relations within one sentence can be either implicit or explicit.\n sent4: We sample three documents from the test-set, containing 590 links in total, and count the number of relations of each type.\n sent5: Our manual analysis reveals that 89.8% of the relations are implicit.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "A",
                    "dataset",
                    "for",
                    "classification",
                    "tasks",
                    "is",
                    "useful",
                    "only",
                    "if",
                    "the",
                    "accuracy",
                    "of",
                    "its",
                    "annotations",
                    "can",
                    "be",
                    "confirmed."
                ],
                [
                    "To",
                    "this",
                    "end",
                    "we",
                    "use",
                    "BERT",
                    "to",
                    "evaluate",
                    "our",
                    "annotations",
                    "as",
                    "it",
                    "has",
                    "consistently",
                    "outperformed",
                    "other",
                    "models",
                    "in",
                    "recent",
                    "classification",
                    "tasks",
                    "(see",
                    "e.g",
                    "#TARGET_REF",
                    "),",
                    "and",
                    "Support",
                    "Vector",
                    "Machines",
                    "for",
                    "its",
                    "simplicity",
                    "and",
                    "effectiveness."
                ],
                [
                    "We",
                    "use",
                    "a",
                    "stratified",
                    "split",
                    "of",
                    "70:20:10",
                    "for",
                    "training,",
                    "dev,",
                    "and",
                    "test",
                    "data."
                ]
            ],
            "context": [
                2,
                1,
                3
            ]
        },
        "input": "sent0: A dataset for classification tasks is useful only if the accuracy of its annotations can be confirmed.\n sent1: To this end we use BERT to evaluate our annotations as it has consistently outperformed other models in recent classification tasks (see e.g #TARGET_REF ), and Support Vector Machines for its simplicity and effectiveness.\n sent2: We use a stratified split of 70:20:10 for training, dev, and test data.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "A",
                    "confounding",
                    "factor",
                    "which",
                    "could",
                    "pollute",
                    "this",
                    "analysis",
                    "is",
                    "the",
                    "role",
                    "of",
                    "strong",
                    "auto-regressive",
                    "decoding",
                    "of",
                    "VAEs",
                    "and",
                    "the",
                    "type",
                    "of",
                    "information",
                    "captured",
                    "by",
                    "the",
                    "decoder",
                    "in",
                    "such",
                    "scenario."
                ],
                [
                    "While",
                    "a",
                    "preliminary",
                    "analysis",
                    "has",
                    "been",
                    "provided",
                    "recently",
                    "#TARGET_REF",
                    ",",
                    "this",
                    "has",
                    "been",
                    "vastly",
                    "underexplored",
                    "and",
                    "requires",
                    "more",
                    "explicit",
                    "attempts."
                ],
                [
                    "We",
                    "leave",
                    "deeper",
                    "investigation",
                    "of",
                    "this",
                    "to",
                    "future",
                    "work."
                ]
            ],
            "context": [
                3,
                1,
                0
            ]
        },
        "input": "sent0: A confounding factor which could pollute this analysis is the role of strong auto-regressive decoding of VAEs and the type of information captured by the decoder in such scenario.\n sent1: While a preliminary analysis has been provided recently #TARGET_REF , this has been vastly underexplored and requires more explicit attempts.\n sent2: We leave deeper investigation of this to future work.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "To",
                    "conclude,",
                    "we",
                    "argue",
                    "that",
                    "despite",
                    "the",
                    "ambiguities",
                    "of",
                    "prepositions,",
                    "they",
                    "allow",
                    "us",
                    "to",
                    "obtain",
                    "a",
                    "meaningful",
                    "set",
                    "of",
                    "typed",
                    "semantic",
                    "links",
                    "between",
                    "NPs,",
                    "which",
                    "are",
                    "well",
                    "understood",
                    "by",
                    "people",
                    "and",
                    "can",
                    "be",
                    "effectively",
                    "processed",
                    "by",
                    "NLP",
                    "models."
                ],
                [
                    "While",
                    "the",
                    "annotation",
                    "can",
                    "be",
                    "refined",
                    "to",
                    "include",
                    "a",
                    "fine-grained",
                    "sense",
                    "annotation",
                    "for",
                    "each",
                    "link,",
                    "for",
                    "example,",
                    "via",
                    "a",
                    "scheme",
                    "as",
                    "that",
                    "of",
                    "#TARGET_REF",
                    ",",
                    "we",
                    "leave",
                    "such",
                    "an",
                    "extension",
                    "to",
                    "future",
                    "work."
                ]
            ],
            "context": [
                2,
                3
            ]
        },
        "input": "sent0: To conclude, we argue that despite the ambiguities of prepositions, they allow us to obtain a meaningful set of typed semantic links between NPs, which are well understood by people and can be effectively processed by NLP models.\n sent1: While the annotation can be refined to include a fine-grained sense annotation for each link, for example, via a scheme as that of #TARGET_REF , we leave such an extension to future work.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "With",
                    "the",
                    "success",
                    "of",
                    "BERT",
                    "#REF",
                    "in",
                    "language",
                    "modeling,",
                    "self-supervised",
                    "Vision-and-Language",
                    "Pretraining",
                    "(VLP)",
                    "has",
                    "attracted",
                    "much",
                    "interest",
                    "from",
                    "AI",
                    "community,",
                    "which",
                    "aims",
                    "to",
                    "learn",
                    "generalizable",
                    "multi-modal",
                    "representations",
                    "from",
                    "largescale",
                    "image-text",
                    "data."
                ],
                [
                    "Combined",
                    "with",
                    "a",
                    "pretrainthen-transfer",
                    "strategy,",
                    "it",
                    "shows",
                    "great",
                    "potential",
                    "in",
                    "tackling",
                    "vision",
                    "and",
                    "language",
                    "reasoning",
                    "tasks,",
                    "such",
                    "as",
                    "image-text",
                    "retrieval,",
                    "visual",
                    "question",
                    "answering",
                    "(VQA)",
                    "and",
                    "visual",
                    "entailment",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "A",
                    "critical",
                    "step",
                    "in",
                    "such",
                    "representation",
                    "learning",
                    "is",
                    "to",
                    "jointly",
                    "model",
                    "linguistic",
                    "entities",
                    "and",
                    "visual",
                    "semantic",
                    "concepts",
                    "(e.g.,",
                    "attributes,",
                    "objects,",
                    "and",
                    "relations),",
                    "as",
                    "well",
                    "as",
                    "their",
                    "alignment."
                ],
                [
                    "However,",
                    "this",
                    "is",
                    "particularly",
                    "challenging",
                    "due",
                    "to",
                    "large",
                    "discrepancy",
                    "in",
                    "visual",
                    "and",
                    "language",
                    "representations",
                    "(pixels",
                    "vs",
                    "words)",
                    "and",
                    "lack",
                    "of",
                    "entity-level",
                    "cross-modal",
                    "correspondence",
                    "in",
                    "supervision."
                ]
            ],
            "context": [
                0,
                2,
                3,
                0
            ]
        },
        "input": "sent0: With the success of BERT #REF in language modeling, self-supervised Vision-and-Language Pretraining (VLP) has attracted much interest from AI community, which aims to learn generalizable multi-modal representations from largescale image-text data.\n sent1: Combined with a pretrainthen-transfer strategy, it shows great potential in tackling vision and language reasoning tasks, such as image-text retrieval, visual question answering (VQA) and visual entailment #TARGET_REF .\n sent2: A critical step in such representation learning is to jointly model linguistic entities and visual semantic concepts (e.g., attributes, objects, and relations), as well as their alignment.\n sent3: However, this is particularly challenging due to large discrepancy in visual and language representations (pixels vs words) and lack of entity-level cross-modal correspondence in supervision.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "These",
                    "items",
                    "are",
                    "like",
                    "those",
                    "proposed",
                    "for",
                    "the",
                    "tabulation",
                    "of",
                    "linear",
                    "indexed",
                    "automata",
                    "#TARGET_REF",
                    "LCYK",
                    "=",
                    "{",
                    "[A,",
                    ",y,i,j",
                    "I",
                    "B,",
                    "p,q]",
                    "I",
                    "A,",
                    "B",
                    "E",
                    "VN,",
                    "'Y",
                    "E",
                    "Vi,",
                    "0",
                    "::,",
                    "i::,",
                    "j,",
                    "(p,q)",
                    "::,",
                    "(i,",
                    "j)",
                    "}",
                    "11.cYK",
                    "=",
                    "{",
                    "[",
                    "a,",
                    "i",
                    "-1,",
                    "i]",
                    "I",
                    "a",
                    "=",
                    "ai,",
                    "lJ",
                    "::,",
                    "i",
                    "::,",
                    "n",
                    "}",
                    "v",
                    "scan",
                    "-",
                    "[a,j,",
                    "j",
                    "+",
                    "1]",
                    "A[",
                    "]",
                    "➔",
                    "a",
                    "E",
                    "p",
                    "CYK",
                    "-[A",
                    "."
                ],
                [
                    "."
                ],
                [
                    "1",
                    "1",
                    "]",
                    ",",
                    "-,",
                    "1",
                    ",",
                    "1+",
                    "-,",
                    "-,",
                    "-I",
                    "[B,-,",
                    "i,",
                    "k",
                    "1",
                    "-,",
                    "-,",
                    "-],",
                    "1)[00-y](",
                    "](oo]",
                    "_",
                    "[C,",
                    "17",
                    ",k,j",
                    "I",
                    "D,"
                ]
            ],
            "context": [
                1,
                1,
                1
            ]
        },
        "input": "sent0: These items are like those proposed for the tabulation of linear indexed automata #TARGET_REF LCYK = { [A, ,y,i,j I B, p,q] I A, B E VN, 'Y E Vi, 0 ::, i::, j, (p,q) ::, (i, j) } 11.cYK = { [ a, i -1, i] I a = ai, lJ ::, i ::, n } v scan - [a,j, j + 1] A[ ] ➔ a E p CYK -[A .\n sent1: .\n sent2: 1 1 ] , -, 1 , 1+ -, -, -I [B,-, i, k 1 -, -, -], 1)[00-y]( ](oo] _ [C, 17 ,k,j I D,\n",
        "output": "{\"INFORMATION\": [\"sent0\", \"sent1\", \"sent2\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "BLEU",
                    "Previous",
                    "definition",
                    "generation",
                    "studies",
                    "#REF",
                    "used",
                    "the",
                    "BLEU",
                    "#TARGET_REF",
                    "score",
                    "to",
                    "measure",
                    "the",
                    "closeness",
                    "of",
                    "generated",
                    "results",
                    "to",
                    "the",
                    "standard",
                    "answers,",
                    "and",
                    "to",
                    "evaluate",
                    "the",
                    "accuracy",
                    "of",
                    "results."
                ],
                [
                    "Since",
                    "the",
                    "English",
                    "test",
                    "set",
                    "is",
                    "manually",
                    "annotated,",
                    "we",
                    "calculate",
                    "the",
                    "BLEU",
                    "score",
                    "of",
                    "both",
                    "complex",
                    "and",
                    "simple",
                    "definitions,",
                    "respectively."
                ]
            ],
            "context": [
                3,
                2
            ]
        },
        "input": "sent0: BLEU Previous definition generation studies #REF used the BLEU #TARGET_REF score to measure the closeness of generated results to the standard answers, and to evaluate the accuracy of results.\n sent1: Since the English test set is manually annotated, we calculate the BLEU score of both complex and simple definitions, respectively.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "existing",
                    "approach",
                    "to",
                    "convert",
                    "coreference",
                    "annotations",
                    "into",
                    "(question,",
                    "context,",
                    "answer)",
                    "tuples,",
                    "which",
                    "is",
                    "used",
                    "to",
                    "improve",
                    "coreference",
                    "resolution",
                    "performance",
                    "#TARGET_REF",
                    ",",
                    "is",
                    "to",
                    "use",
                    "the",
                    "sentence",
                    "of",
                    "the",
                    "anaphor",
                    "as",
                    "a",
                    "declarative",
                    "query,",
                    "and",
                    "its",
                    "closest",
                    "antecedent",
                    "as",
                    "the",
                    "answer."
                ],
                [
                    "The",
                    "format",
                    "of",
                    "these",
                    "queries",
                    "is",
                    "not",
                    "compatible",
                    "with",
                    "questions",
                    "in",
                    "MRC",
                    "datasets,",
                    "and",
                    "therefore,",
                    "the",
                    "impact",
                    "of",
                    "this",
                    "data",
                    "on",
                    "MRC",
                    "models",
                    "may",
                    "be",
                    "limited."
                ],
                [
                    "In",
                    "this",
                    "work,",
                    "we",
                    "instead",
                    "generate",
                    "questions",
                    "from",
                    "those",
                    "declarative",
                    "queries",
                    "using",
                    "an",
                    "automatic",
                    "question",
                    "generation",
                    "model."
                ],
                [
                    "We",
                    "use",
                    "the",
                    "BART",
                    "model",
                    "#REF",
                    "that",
                    "is",
                    "one",
                    "of",
                    "the",
                    "state-of-the-art",
                    "text",
                    "generation",
                    "models."
                ],
                [
                    "Below",
                    "we",
                    "explain",
                    "the",
                    "details",
                    "of",
                    "each",
                    "of",
                    "these",
                    "two",
                    "approaches",
                    "for",
                    "creating",
                    "QA",
                    "data",
                    "from",
                    "CoNLL-2012."
                ],
                [
                    "Table",
                    "4",
                    "shows",
                    "examples",
                    "from",
                    "both",
                    "approaches."
                ],
                [
                    "2019)",
                    "choose",
                    "a",
                    "sentence",
                    "that",
                    "contains",
                    "an",
                    "anaphor",
                    "as",
                    "a",
                    "declarative",
                    "query,",
                    "the",
                    "closest",
                    "nonpronominal",
                    "antecedent",
                    "of",
                    "that",
                    "anaphor",
                    "as",
                    "the",
                    "answer,",
                    "and",
                    "the",
                    "corresponding",
                    "document",
                    "of",
                    "the",
                    "expressions",
                    "as",
                    "the",
                    "context."
                ],
                [
                    "10",
                    "We",
                    "remove",
                    "the",
                    "tuples",
                    "in",
                    "which",
                    "the",
                    "anaphor",
                    "and",
                    "its",
                    "antecedent",
                    "are",
                    "identical."
                ],
                [
                    "The",
                    "reason",
                    "is",
                    "that",
                    "(1)",
                    "Quoref",
                    "already",
                    "contains",
                    "many",
                    "examples",
                    "in",
                    "which",
                    "the",
                    "coreference",
                    "relation",
                    "is",
                    "between",
                    "two",
                    "mentions",
                    "with",
                    "the",
                    "same",
                    "string,",
                    "and",
                    "(2)",
                    "even",
                    "after",
                    "removing",
                    "such",
                    "examples,",
                    "CoNLL",
                    "dec",
                    "contains",
                    "around",
                    "four",
                    "times",
                    "more",
                    "QA",
                    "pairs",
                    "than",
                    "the",
                    "Quoref",
                    "training",
                    "data."
                ]
            ],
            "context": [
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: The existing approach to convert coreference annotations into (question, context, answer) tuples, which is used to improve coreference resolution performance #TARGET_REF , is to use the sentence of the anaphor as a declarative query, and its closest antecedent as the answer.\n sent1: The format of these queries is not compatible with questions in MRC datasets, and therefore, the impact of this data on MRC models may be limited.\n sent2: In this work, we instead generate questions from those declarative queries using an automatic question generation model.\n sent3: We use the BART model #REF that is one of the state-of-the-art text generation models.\n sent4: Below we explain the details of each of these two approaches for creating QA data from CoNLL-2012.\n sent5: Table 4 shows examples from both approaches.\n sent6: 2019) choose a sentence that contains an anaphor as a declarative query, the closest nonpronominal antecedent of that anaphor as the answer, and the corresponding document of the expressions as the context.\n sent7: 10 We remove the tuples in which the anaphor and its antecedent are identical.\n sent8: The reason is that (1) Quoref already contains many examples in which the coreference relation is between two mentions with the same string, and (2) even after removing such examples, CoNLL dec contains around four times more QA pairs than the Quoref training data.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "basic",
                    "AI",
                    "argument",
                    "for",
                    "knowledge-based",
                    "processing",
                    "does",
                    "not",
                    "admit",
                    "defeat",
                    "and",
                    "retreat,",
                    "it",
                    "just",
                    "regroups."
                ],
                [
                    "It",
                    "has",
                    "to",
                    "accept",
                    "Bar",
                    "Hillel's",
                    "old",
                    "anti-MT",
                    "argument",
                    "(Bar",
                    "Hillel",
                    "1960)",
                    "on",
                    "its",
                    "own",
                    "side-i.e."
                ],
                [
                    "that",
                    "as",
                    "he",
                    "said,",
                    "good",
                    "MT",
                    "must",
                    "in",
                    "the",
                    "end",
                    "need",
                    "knowledge",
                    "representations."
                ],
                [
                    "One",
                    "version",
                    "of",
                    "this",
                    "argument",
                    "is",
                    "the",
                    "primitive",
                    "psychological",
                    "one:",
                    "humans",
                    "do",
                    "not",
                    "do",
                    "translation",
                    "by",
                    "exposure",
                    "to",
                    "such",
                    "vast",
                    "texts,",
                    "because",
                    "they",
                    "simply",
                    "have",
                    "not",
                    "had",
                    "such",
                    "exposure,",
                    "and",
                    "in",
                    "the",
                    "end",
                    "how",
                    "people",
                    "do",
                    "things",
                    "will",
                    "prove",
                    "important."
                ],
                [
                    "Note",
                    "that",
                    "this",
                    "argument",
                    "makes",
                    "an",
                    "empirical",
                    "claim",
                    "about",
                    "human",
                    "exposure",
                    "to",
                    "text",
                    "that",
                    "might",
                    "be",
                    "hard",
                    "to",
                    "substantiate."
                ],
                [
                    "This",
                    "argument",
                    "will",
                    "cut",
                    "little",
                    "ice",
                    "with",
                    "our",
                    "opponents,",
                    "but",
                    "there",
                    "may",
                    "still",
                    "be",
                    "a",
                    "good",
                    "argument",
                    "that",
                    "we",
                    "do",
                    "need",
                    "representations",
                    "for",
                    "tasks",
                    "in",
                    "NLP",
                    "related",
                    "to",
                    "MT:",
                    "e.g."
                ],
                [
                    "we",
                    "cannot",
                    "really",
                    "imagine",
                    "doing",
                    "summarization",
                    "or",
                    "question",
                    "answering",
                    "by",
                    "purely",
                    "statistical",
                    "methods,",
                    "can",
                    "we?"
                ],
                [
                    "There",
                    "is",
                    "related",
                    "practical",
                    "evidence",
                    "from",
                    "message",
                    "extraction:",
                    "in",
                    "the",
                    "MUC",
                    "competitions",
                    "#TARGET_REF",
                    ",",
                    "the",
                    "systems",
                    "that",
                    "have",
                    "done",
                    "best",
                    "have",
                    "been",
                    "hybrids",
                    "of",
                    "preference",
                    "and",
                    "statistics,",
                    "such",
                    "as",
                    "of",
                    "Grishman",
                    "and",
                    "Lehnert,",
                    "and",
                    "not",
                    "pure",
                    "systems",
                    "of",
                    "either",
                    "type."
                ]
            ],
            "context": [
                0,
                0,
                2,
                2,
                2,
                2,
                3,
                2
            ]
        },
        "input": "sent0: The basic AI argument for knowledge-based processing does not admit defeat and retreat, it just regroups.\n sent1: It has to accept Bar Hillel's old anti-MT argument (Bar Hillel 1960) on its own side-i.e.\n sent2: that as he said, good MT must in the end need knowledge representations.\n sent3: One version of this argument is the primitive psychological one: humans do not do translation by exposure to such vast texts, because they simply have not had such exposure, and in the end how people do things will prove important.\n sent4: Note that this argument makes an empirical claim about human exposure to text that might be hard to substantiate.\n sent5: This argument will cut little ice with our opponents, but there may still be a good argument that we do need representations for tasks in NLP related to MT: e.g.\n sent6: we cannot really imagine doing summarization or question answering by purely statistical methods, can we?\n sent7: There is related practical evidence from message extraction: in the MUC competitions #TARGET_REF , the systems that have done best have been hybrids of preference and statistics, such as of Grishman and Lehnert, and not pure systems of either type.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent2\", \"sent3\", \"sent4\", \"sent5\", \"sent7\"], \"BACKGROUND\": [\"sent6\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Our",
                    "main",
                    "contribution",
                    "is",
                    "to",
                    "change",
                    "the",
                    "target",
                    "distribution",
                    "of",
                    "abstractive",
                    "models",
                    "from",
                    "a",
                    "one-point",
                    "deterministic",
                    "distribution",
                    "assumed",
                    "by",
                    "MLE",
                    "training",
                    "to",
                    "a",
                    "non-deterministic",
                    "distribution",
                    "in",
                    "which",
                    "candidate",
                    "summaries",
                    "are",
                    "also",
                    "assigned",
                    "probability",
                    "mass",
                    "according",
                    "to",
                    "their",
                    "quality."
                ],
                [
                    "The",
                    "new",
                    "SOTA",
                    "performance",
                    "on",
                    "#REF",
                    "and",
                    "XSum",
                    "#TARGET_REF",
                    "datasets",
                    "demonstrated",
                    "the",
                    "effectiveness",
                    "of",
                    "our",
                    "method."
                ],
                [
                    "Our",
                    "in-depth",
                    "analysis",
                    "also",
                    "found",
                    "that",
                    "the",
                    "abstractive",
                    "models",
                    "trained",
                    "using",
                    "our",
                    "method",
                    "can",
                    "estimate",
                    "the",
                    "candidate",
                    "summary",
                    "quality",
                    "more",
                    "accurately,",
                    "in",
                    "concert",
                    "with",
                    "the",
                    "the",
                    "objective",
                    "of",
                    "our",
                    "training",
                    "paradigm."
                ]
            ],
            "context": [
                0,
                2,
                0
            ]
        },
        "input": "sent0: Our main contribution is to change the target distribution of abstractive models from a one-point deterministic distribution assumed by MLE training to a non-deterministic distribution in which candidate summaries are also assigned probability mass according to their quality.\n sent1: The new SOTA performance on #REF and XSum #TARGET_REF datasets demonstrated the effectiveness of our method.\n sent2: Our in-depth analysis also found that the abstractive models trained using our method can estimate the candidate summary quality more accurately, in concert with the the objective of our training paradigm.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Pre-training",
                    "We",
                    "pre-trained",
                    "two",
                    "MT",
                    "models",
                    "with",
                    "the",
                    "Spanish-English",
                    "language-pair",
                    "in",
                    "both",
                    "directions."
                ],
                [
                    "We",
                    "did",
                    "not",
                    "include",
                    "an",
                    "agglutinative",
                    "language",
                    "like",
                    "Finnish",
                    "#TARGET_REF",
                    "for",
                    "two",
                    "reasons:",
                    "it",
                    "is",
                    "not",
                    "a",
                    "must",
                    "to",
                    "consider",
                    "highly",
                    "related",
                    "languages",
                    "for",
                    "effective",
                    "transfer",
                    "learning",
                    "(e.g."
                ],
                [
                    "English-German",
                    "to",
                    "English-Tamil",
                    "#REF",
                    "),",
                    "and",
                    "we",
                    "wanted",
                    "to",
                    "translate",
                    "the",
                    "English",
                    "side",
                    "of",
                    "en-aym,",
                    "en-quy",
                    "and",
                    "en-quz",
                    "to",
                    "augment",
                    "their",
                    "correspondent",
                    "Spanish-paired",
                    "datasets."
                ],
                [
                    "The",
                    "en→es",
                    "and",
                    "es→en",
                    "models",
                    "achieved",
                    "34.4",
                    "and",
                    "32.3",
                    "BLEU",
                    "points,",
                    "respectively,",
                    "in",
                    "the",
                    "newsdev2013",
                    "set."
                ]
            ],
            "context": [
                0,
                3,
                3,
                0
            ]
        },
        "input": "sent0: Pre-training We pre-trained two MT models with the Spanish-English language-pair in both directions.\n sent1: We did not include an agglutinative language like Finnish #TARGET_REF for two reasons: it is not a must to consider highly related languages for effective transfer learning (e.g.\n sent2: English-German to English-Tamil #REF ), and we wanted to translate the English side of en-aym, en-quy and en-quz to augment their correspondent Spanish-paired datasets.\n sent3: The en→es and es→en models achieved 34.4 and 32.3 BLEU points, respectively, in the newsdev2013 set.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Table",
                    "2",
                    "presents",
                    "the",
                    "F1",
                    "scores",
                    "for",
                    "our",
                    "various",
                    "training",
                    "scenarios",
                    "in",
                    "the",
                    "downstream",
                    "NER3",
                    "task,",
                    "which",
                    "should",
                    "be",
                    "the",
                    "most",
                    "challenging",
                    "for",
                    "our",
                    "phonetic",
                    "models."
                ],
                [
                    "The",
                    "influence",
                    "of",
                    "pre-training",
                    "is",
                    "more",
                    "noticeable",
                    "for",
                    "this",
                    "task."
                ],
                [
                    "Further,",
                    "the",
                    "models",
                    "pretrained",
                    "on",
                    "the",
                    "kin",
                    "audio",
                    "and",
                    "text",
                    "data",
                    "have",
                    "the",
                    "best",
                    "performance."
                ],
                [
                    "This",
                    "is",
                    "likely",
                    "due",
                    "to",
                    "the",
                    "fact",
                    "that",
                    "the",
                    "kin",
                    "data",
                    "is",
                    "both",
                    "large",
                    "and",
                    "higher",
                    "quality",
                    "(in",
                    "terms",
                    "of",
                    "sound",
                    "quality)",
                    "as",
                    "compared",
                    "to",
                    "the",
                    "ALFFA",
                    "Swahili",
                    "data."
                ],
                [
                    "This",
                    "benefit",
                    "of",
                    "this",
                    "data",
                    "size",
                    "and",
                    "quality",
                    "appears",
                    "to",
                    "outweigh",
                    "any",
                    "degradation",
                    "due",
                    "to",
                    "the",
                    "pre-training",
                    "occurring",
                    "in",
                    "a",
                    "different",
                    "(although",
                    "related)",
                    "language."
                ],
                [
                    "#REF",
                    "."
                ],
                [
                    "Average",
                    "of",
                    "at",
                    "least",
                    "three",
                    "trials",
                    "per",
                    "experiment,",
                    "scores",
                    "calculated",
                    "with",
                    "seqeval",
                    "library."
                ],
                [
                    "#TARGET_REF",
                    "The",
                    "importance",
                    "(or",
                    "relative",
                    "impact)",
                    "of",
                    "pretraining",
                    "phonetic",
                    "language",
                    "models",
                    "increases",
                    "with",
                    "the",
                    "complexity",
                    "of",
                    "the",
                    "NER",
                    "task."
                ],
                [
                    "Fig."
                ],
                [
                    "4",
                    "shows",
                    "the",
                    "maximum",
                    "percentage",
                    "improvement",
                    "due",
                    "to",
                    "pretraining",
                    "for",
                    "each",
                    "of",
                    "our",
                    "NER",
                    "tasks."
                ],
                [
                    "This",
                    "suggests",
                    "that",
                    "simple",
                    "NLP",
                    "tasks",
                    "with",
                    "a",
                    "small",
                    "number",
                    "of",
                    "output",
                    "classes",
                    "are",
                    "much",
                    "easier",
                    "to",
                    "port",
                    "to",
                    "phonetic",
                    "representations,",
                    "even",
                    "without",
                    "pre-training,",
                    "while",
                    "more",
                    "complicated",
                    "NLP",
                    "tasks",
                    "may",
                    "require",
                    "a",
                    "more",
                    "significant",
                    "amount",
                    "of",
                    "text",
                    "and/or",
                    "audio",
                    "data",
                    "for",
                    "pretraining."
                ],
                [
                    "We",
                    "expect",
                    "this",
                    "trend",
                    "to",
                    "carry",
                    "through",
                    "to",
                    "tasks",
                    "like",
                    "sentiment",
                    "analysis,",
                    "which",
                    "could",
                    "be",
                    "formulated",
                    "as",
                    "a",
                    "simple",
                    "classification",
                    "task",
                    "with",
                    "NEG,",
                    "NEU,",
                    "and",
                    "POS",
                    "sentiment",
                    "labels",
                    "or",
                    "a",
                    "more",
                    "complicated",
                    "aspect",
                    "based",
                    "sentiment",
                    "analysis",
                    "task."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                1,
                3,
                3,
                3,
                0
            ]
        },
        "input": "sent0: Table 2 presents the F1 scores for our various training scenarios in the downstream NER3 task, which should be the most challenging for our phonetic models.\n sent1: The influence of pre-training is more noticeable for this task.\n sent2: Further, the models pretrained on the kin audio and text data have the best performance.\n sent3: This is likely due to the fact that the kin data is both large and higher quality (in terms of sound quality) as compared to the ALFFA Swahili data.\n sent4: This benefit of this data size and quality appears to outweigh any degradation due to the pre-training occurring in a different (although related) language.\n sent5: #REF .\n sent6: Average of at least three trials per experiment, scores calculated with seqeval library.\n sent7: #TARGET_REF The importance (or relative impact) of pretraining phonetic language models increases with the complexity of the NER task.\n sent8: Fig.\n sent9: 4 shows the maximum percentage improvement due to pretraining for each of our NER tasks.\n sent10: This suggests that simple NLP tasks with a small number of output classes are much easier to port to phonetic representations, even without pre-training, while more complicated NLP tasks may require a more significant amount of text and/or audio data for pretraining.\n sent11: We expect this trend to carry through to tasks like sentiment analysis, which could be formulated as a simple classification task with NEG, NEU, and POS sentiment labels or a more complicated aspect based sentiment analysis task.\n",
        "output": "{\"INFORMATION\": [\"sent7\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent8\", \"sent9\", \"sent10\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Semantic",
                    "Drift."
                ],
                [
                    "Multi-hop",
                    "question",
                    "answering",
                    "systems",
                    "suffer",
                    "from",
                    "the",
                    "tendency",
                    "of",
                    "composing",
                    "out-of-context",
                    "inference",
                    "chains",
                    "as",
                    "the",
                    "number",
                    "of",
                    "required",
                    "hops",
                    "(aggregated",
                    "facts)",
                    "increases."
                ],
                [
                    "This",
                    "phenomenon,",
                    "known",
                    "as",
                    "semantic",
                    "drift,",
                    "has",
                    "been",
                    "observed",
                    "in",
                    "a",
                    "number",
                    "of",
                    "works",
                    "#REF",
                    ",",
                    "which",
                    "have",
                    "empirically",
                    "demonstrated",
                    "that",
                    "multi-hop",
                    "inference",
                    "models",
                    "exhibit",
                    "a",
                    "substantial",
                    "drop",
                    "in",
                    "performance",
                    "when",
                    "aggregating",
                    "more",
                    "than",
                    "2",
                    "facts",
                    "or",
                    "paragraphs."
                ],
                [
                    "Semantic",
                    "drift",
                    "has",
                    "been",
                    "observed",
                    "across",
                    "a",
                    "variety",
                    "of",
                    "representations",
                    "and",
                    "traversal",
                    "methods,",
                    "including",
                    "word",
                    "and",
                    "dependency",
                    "level",
                    "#REF",
                    ",",
                    "sentence",
                    "level",
                    "#REF",
                    ",",
                    "and",
                    "paragraph",
                    "level",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "#REF",
                    "have",
                    "demonstrated",
                    "that",
                    "ongoing",
                    "efforts",
                    "on",
                    "\"very",
                    "long\"",
                    "multi-hop",
                    "reasoning",
                    "are",
                    "unlikely",
                    "to",
                    "succeed",
                    "without",
                    "the",
                    "adoption",
                    "of",
                    "a",
                    "richer",
                    "underlying",
                    "representation",
                    "that",
                    "allows",
                    "for",
                    "reasoning",
                    "with",
                    "fewer",
                    "hops."
                ]
            ],
            "context": [
                0,
                0,
                0,
                3,
                0
            ]
        },
        "input": "sent0: Semantic Drift.\n sent1: Multi-hop question answering systems suffer from the tendency of composing out-of-context inference chains as the number of required hops (aggregated facts) increases.\n sent2: This phenomenon, known as semantic drift, has been observed in a number of works #REF , which have empirically demonstrated that multi-hop inference models exhibit a substantial drop in performance when aggregating more than 2 facts or paragraphs.\n sent3: Semantic drift has been observed across a variety of representations and traversal methods, including word and dependency level #REF , sentence level #REF , and paragraph level #TARGET_REF .\n sent4: #REF have demonstrated that ongoing efforts on \"very long\" multi-hop reasoning are unlikely to succeed without the adoption of a richer underlying representation that allows for reasoning with fewer hops.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "exponential",
                    "growth",
                    "of",
                    "published",
                    "articles",
                    "may",
                    "exceeds",
                    "many",
                    "readers'",
                    "ability",
                    "to",
                    "keep",
                    "track",
                    "of",
                    "the",
                    "development",
                    "of",
                    "their",
                    "field",
                    "of",
                    "interest."
                ],
                [
                    "Hence,",
                    "automatic",
                    "reading",
                    "comprehension",
                    "of",
                    "scientific",
                    "documents",
                    "has",
                    "attracted",
                    "the",
                    "attention",
                    "of",
                    "researchers",
                    "across",
                    "various",
                    "domains",
                    "such",
                    "as",
                    "Drug",
                    "Discovery,",
                    "Knowledge",
                    "Base",
                    "Construction,",
                    "and",
                    "Natural",
                    "Language",
                    "Processing."
                ],
                [
                    "A",
                    "crucial",
                    "aspect",
                    "of",
                    "understanding",
                    "scientific",
                    "literature",
                    "is",
                    "understanding",
                    "terminologies",
                    "and",
                    "formulae",
                    "because",
                    "they",
                    "offer",
                    "an",
                    "explicit",
                    "and",
                    "precise",
                    "interface",
                    "to",
                    "present",
                    "the",
                    "relation",
                    "between",
                    "scientific",
                    "concepts",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "As",
                    "such,",
                    "a",
                    "reading",
                    "comprehension",
                    "machine",
                    "needs",
                    "to",
                    "(i)",
                    "identify",
                    "their",
                    "descriptions",
                    "and",
                    "formulae,",
                    "(ii)",
                    "segment",
                    "them",
                    "into",
                    "primitive",
                    "terms",
                    "and",
                    "symbols,",
                    "and",
                    "(iii)",
                    "link",
                    "the",
                    "associated",
                    "terms",
                    "and",
                    "corresponding",
                    "symbols."
                ]
            ],
            "context": [
                0,
                0,
                1,
                3
            ]
        },
        "input": "sent0: The exponential growth of published articles may exceeds many readers' ability to keep track of the development of their field of interest.\n sent1: Hence, automatic reading comprehension of scientific documents has attracted the attention of researchers across various domains such as Drug Discovery, Knowledge Base Construction, and Natural Language Processing.\n sent2: A crucial aspect of understanding scientific literature is understanding terminologies and formulae because they offer an explicit and precise interface to present the relation between scientific concepts #TARGET_REF .\n sent3: As such, a reading comprehension machine needs to (i) identify their descriptions and formulae, (ii) segment them into primitive terms and symbols, and (iii) link the associated terms and corresponding symbols.\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "To",
                    "minimise",
                    "confounds",
                    "arising",
                    "from",
                    "the",
                    "preceding",
                    "word",
                    "in",
                    "the",
                    "sentence,",
                    "we",
                    "balanced",
                    "the",
                    "test",
                    "set",
                    "with",
                    "respect",
                    "to",
                    "the",
                    "open/closed",
                    "class",
                    "status",
                    "of",
                    "the",
                    "previous",
                    "word."
                ],
                [
                    "Similarly,",
                    "we",
                    "controlled",
                    "the",
                    "decoding",
                    "of",
                    "word",
                    "frequency",
                    "for",
                    "word",
                    "length,",
                    "and",
                    "the",
                    "analysis",
                    "of",
                    "word",
                    "length",
                    "for",
                    "word",
                    "frequency,",
                    "and",
                    "both",
                    "analyses",
                    "for",
                    "open/closed",
                    "class",
                    "and",
                    "sentence",
                    "position."
                ],
                [
                    "2",
                    "shows",
                    "the",
                    "mean",
                    "accuracy",
                    "values",
                    "(averaged",
                    "across",
                    "10",
                    "seed",
                    "points)",
                    "from",
                    "the",
                    "test",
                    "set",
                    "(centred",
                    "on",
                    "the",
                    "last",
                    "bin",
                    "of",
                    "each",
                    "time",
                    "window",
                    "#TARGET_REF",
                    ")",
                    "with",
                    "±",
                    "68%",
                    "CI."
                ],
                [
                    "The",
                    "classification",
                    "responses",
                    "for",
                    "the",
                    "test",
                    "set",
                    "from",
                    "the",
                    "model",
                    "that",
                    "performed",
                    "best",
                    "on",
                    "the",
                    "dev",
                    "set",
                    "were",
                    "entered",
                    "into",
                    "a",
                    "two-sided",
                    "binomial",
                    "test,",
                    "separately",
                    "for",
                    "each",
                    "time",
                    "window."
                ],
                [
                    "Solid",
                    "lines",
                    "in",
                    "Figure",
                    "2",
                    "above",
                    "the",
                    "decoding",
                    "accuracy",
                    "time",
                    "courses",
                    "indicate",
                    "time",
                    "points",
                    "that",
                    "were",
                    "significant",
                    "at",
                    "(p",
                    "&lt,",
                    "0.05)",
                    "False",
                    "Discovery",
                    "Rate",
                    "(FDR)",
                    "corrected",
                    "for",
                    "multiple",
                    "comparisons",
                    "#REF",
                    "across",
                    "time",
                    "(i.e."
                ],
                [
                    "160",
                    "tests)."
                ]
            ],
            "context": [
                0,
                0,
                3,
                3,
                0,
                0
            ]
        },
        "input": "sent0: To minimise confounds arising from the preceding word in the sentence, we balanced the test set with respect to the open/closed class status of the previous word.\n sent1: Similarly, we controlled the decoding of word frequency for word length, and the analysis of word length for word frequency, and both analyses for open/closed class and sentence position.\n sent2: 2 shows the mean accuracy values (averaged across 10 seed points) from the test set (centred on the last bin of each time window #TARGET_REF ) with ± 68% CI.\n sent3: The classification responses for the test set from the model that performed best on the dev set were entered into a two-sided binomial test, separately for each time window.\n sent4: Solid lines in Figure 2 above the decoding accuracy time courses indicate time points that were significant at (p &lt, 0.05) False Discovery Rate (FDR) corrected for multiple comparisons #REF across time (i.e.\n sent5: 160 tests).\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "During",
                    "fine-tuning,",
                    "for",
                    "each",
                    "data",
                    "point",
                    "in",
                    "the",
                    "original",
                    "batch",
                    "(B",
                    "o",
                    ")",
                    "of",
                    "size",
                    "n,",
                    "we",
                    "pick",
                    "one",
                    "of",
                    "its",
                    "corresponding",
                    "translations",
                    "uniformly",
                    "at",
                    "random",
                    "and",
                    "form",
                    "a",
                    "translated",
                    "batch",
                    "(B",
                    "p",
                    ")",
                    "of",
                    "the",
                    "same",
                    "size",
                    "n.",
                    "It",
                    "is",
                    "important",
                    "to",
                    "note",
                    "that",
                    "B",
                    "o",
                    "itself",
                    "is",
                    "taken",
                    "from",
                    "the",
                    "combined",
                    "dataset",
                    "of",
                    "source",
                    "instances",
                    "and",
                    "translated",
                    "instances."
                ],
                [
                    "The",
                    "two",
                    "batches",
                    "that",
                    "form",
                    "a",
                    "pair",
                    "are",
                    "denoted",
                    "as",
                    "original",
                    "batch",
                    "and",
                    "pair",
                    "batch,",
                    "respectively,",
                    "in",
                    "Figure",
                    "4."
                ],
                [
                    "We",
                    "use",
                    "the",
                    "same",
                    "mBERT",
                    "network",
                    "up",
                    "to",
                    "a",
                    "specific",
                    "layer",
                    "as",
                    "our",
                    "encoder",
                    "(enc)",
                    "to",
                    "transform",
                    "B",
                    "o",
                    "and",
                    "B",
                    "p",
                    "to",
                    "get",
                    "the",
                    "embeddings,",
                    "E",
                    "o",
                    ",",
                    "E",
                    "p",
                    "∈",
                    "R",
                    "n",
                    "*",
                    "t",
                    "*",
                    "d",
                    ",",
                    "respectively."
                ],
                [
                    "Then,",
                    "we",
                    "apply",
                    "a",
                    "global",
                    "average",
                    "pooling",
                    "(gap)",
                    "operation",
                    "to",
                    "aggregate",
                    "the",
                    "vector",
                    "representations",
                    "of",
                    "t",
                    "tokens",
                    "into",
                    "a",
                    "single",
                    "vector",
                    "representation",
                    "of",
                    "dimension",
                    "d",
                    "for",
                    "each",
                    "instance",
                    "in",
                    "each",
                    "batch."
                ],
                [
                    "This",
                    "will",
                    "result",
                    "in",
                    "the",
                    "aggregated",
                    "embeddings",
                    "O,",
                    "P",
                    "∈",
                    "R",
                    "n",
                    "*",
                    "d",
                    "for",
                    "B",
                    "o",
                    "and",
                    "B",
                    "p",
                    ",",
                    "respectively."
                ],
                [
                    "With",
                    "these",
                    "n",
                    "feature",
                    "vectors",
                    "in",
                    "the",
                    "original",
                    "and",
                    "the",
                    "translated",
                    "batch,",
                    "we",
                    "follow",
                    "the",
                    "CLIP",
                    "#TARGET_REF",
                    "approach",
                    "and",
                    "compute",
                    "the",
                    "contrastive",
                    "loss",
                    "using",
                    "the",
                    "cross-entropy",
                    "loss",
                    "(L",
                    "ce",
                    ")."
                ],
                [
                    "Specifically,",
                    "we",
                    "multiply",
                    "the",
                    "matrices",
                    "O",
                    "and",
                    "P",
                    "T",
                    "to",
                    "get",
                    "the",
                    "logits",
                    "matrix",
                    "Q",
                    "∈",
                    "R",
                    "n",
                    "*",
                    "n",
                    "."
                ],
                [
                    "Then,",
                    "we",
                    "apply",
                    "the",
                    "cross-entropy",
                    "loss",
                    "L",
                    "ce",
                    "row-wise",
                    "and",
                    "column-wise",
                    "to",
                    "the",
                    "logits",
                    "matrix",
                    "Q,",
                    "with",
                    "its",
                    "diagonal",
                    "locations",
                    "as",
                    "original",
                    "classes",
                    "for",
                    "each",
                    "row",
                    "and",
                    "column,",
                    "respectively.O",
                    "=",
                    "gap(enc(B",
                    "o",
                    ")),(2)"
                ]
            ],
            "context": [
                0,
                0,
                2,
                2,
                2,
                2,
                2,
                2
            ]
        },
        "input": "sent0: During fine-tuning, for each data point in the original batch (B o ) of size n, we pick one of its corresponding translations uniformly at random and form a translated batch (B p ) of the same size n. It is important to note that B o itself is taken from the combined dataset of source instances and translated instances.\n sent1: The two batches that form a pair are denoted as original batch and pair batch, respectively, in Figure 4.\n sent2: We use the same mBERT network up to a specific layer as our encoder (enc) to transform B o and B p to get the embeddings, E o , E p ∈ R n * t * d , respectively.\n sent3: Then, we apply a global average pooling (gap) operation to aggregate the vector representations of t tokens into a single vector representation of dimension d for each instance in each batch.\n sent4: This will result in the aggregated embeddings O, P ∈ R n * d for B o and B p , respectively.\n sent5: With these n feature vectors in the original and the translated batch, we follow the CLIP #TARGET_REF approach and compute the contrastive loss using the cross-entropy loss (L ce ).\n sent6: Specifically, we multiply the matrices O and P T to get the logits matrix Q ∈ R n * n .\n sent7: Then, we apply the cross-entropy loss L ce row-wise and column-wise to the logits matrix Q, with its diagonal locations as original classes for each row and column, respectively.O = gap(enc(B o )),(2)\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent2\", \"sent3\", \"sent4\", \"sent5\", \"sent6\", \"sent7\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "To",
                    "verify",
                    "that",
                    "the",
                    "plug-and-play",
                    "property",
                    "of",
                    "topk",
                    "attention",
                    "also",
                    "holds",
                    "at",
                    "self-attention",
                    "layers,",
                    "we",
                    "downloaded",
                    "a",
                    "BERT-large-uncased-whole-wordmasking",
                    "checkpoint",
                    "#TARGET_REF",
                    "already",
                    "fine-tuned",
                    "on",
                    "SQuAD",
                    "v1",
                    "#REF",
                    "and",
                    "evaluated",
                    "its",
                    "performance",
                    "on",
                    "the",
                    "development",
                    "set",
                    "before",
                    "and",
                    "after",
                    "replacing",
                    "its",
                    "self-attention",
                    "layers",
                    "with",
                    "top-k",
                    "attention."
                ],
                [
                    "For",
                    "k",
                    "as",
                    "low",
                    "as",
                    "16",
                    "(4%",
                    "of",
                    "input",
                    "length),",
                    "we",
                    "only",
                    "saw",
                    "a",
                    "minor",
                    "decrease",
                    "in",
                    "the",
                    "exact",
                    "match",
                    "scores",
                    "(86.9",
                    "→",
                    "86.2)."
                ],
                [
                    "Moreover,",
                    "to",
                    "empirically",
                    "verify",
                    "that",
                    "dense",
                    "approximations",
                    "of",
                    "vanilla",
                    "attention",
                    "(Performer,",
                    "RFA,",
                    "etc)",
                    "indeed",
                    "require",
                    "corrective",
                    "pre-training,",
                    "we",
                    "repeated",
                    "the",
                    "measurement",
                    "using",
                    "Performer",
                    "attention",
                    "with",
                    "256",
                    "features,",
                    "obtaining",
                    "a",
                    "score",
                    "of",
                    "0.38."
                ]
            ],
            "context": [
                2,
                0,
                0
            ]
        },
        "input": "sent0: To verify that the plug-and-play property of topk attention also holds at self-attention layers, we downloaded a BERT-large-uncased-whole-wordmasking checkpoint #TARGET_REF already fine-tuned on SQuAD v1 #REF and evaluated its performance on the development set before and after replacing its self-attention layers with top-k attention.\n sent1: For k as low as 16 (4% of input length), we only saw a minor decrease in the exact match scores (86.9 → 86.2).\n sent2: Moreover, to empirically verify that dense approximations of vanilla attention (Performer, RFA, etc) indeed require corrective pre-training, we repeated the measurement using Performer attention with 256 features, obtaining a score of 0.38.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Mixture",
                    "Modelling",
                    "#REF",
                    ",",
                    "a",
                    "well-established",
                    "technique",
                    "for",
                    "combining",
                    "multiple",
                    "models,",
                    "has",
                    "been",
                    "extensively",
                    "used",
                    "for",
                    "language",
                    "model",
                    "adaptation",
                    "in",
                    "SMT",
                    "#REF",
                    "."
                ],
                [
                    "This",
                    "technique",
                    "has",
                    "also",
                    "been",
                    "used",
                    "for",
                    "adapting",
                    "the",
                    "translation",
                    "model",
                    "in",
                    "SMT",
                    "with",
                    "limited",
                    "success",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "For",
                    "the",
                    "given",
                    "task,",
                    "since",
                    "the",
                    "size",
                    "of",
                    "the",
                    "'in-domain'",
                    "data",
                    "was",
                    "not",
                    "significantly",
                    "large,",
                    "we",
                    "used",
                    "'suitable'",
                    "subsets",
                    "of",
                    "data",
                    "from",
                    "the",
                    "other",
                    "available",
                    "'out-ofdomain'",
                    "corpora",
                    "to",
                    "enrich",
                    "the",
                    "models."
                ],
                [
                    "For",
                    "a",
                    "mixture",
                    "adapted",
                    "language",
                    "model,",
                    "the",
                    "probability",
                    "of",
                    "an",
                    "n-gram",
                    "hw",
                    "is",
                    "given",
                    "as",
                    "in",
                    "(",
                    "2):P",
                    "r",
                    "mix",
                    "(w|h)",
                    "=",
                    "f",
                    "*",
                    "mix",
                    "(w|h)",
                    "+",
                    "λ",
                    "mix",
                    "(h)P",
                    "r",
                    "mix",
                    "(w|",
                    "h)",
                    "(2)where",
                    "w",
                    "is",
                    "the",
                    "current",
                    "word,",
                    "h",
                    "is",
                    "the",
                    "corresponding",
                    "history,",
                    "f",
                    "*",
                    "mix",
                    "is",
                    "the",
                    "mixture",
                    "model",
                    "discounted",
                    "relative",
                    "fre-quency,",
                    "λ",
                    "mix",
                    "indicates",
                    "the",
                    "mixture",
                    "model",
                    "zero-frequency",
                    "estimate",
                    "and",
                    "hw",
                    "is",
                    "the",
                    "lower",
                    "order",
                    "n",
                    "−",
                    "1",
                    "gram."
                ],
                [
                    "The",
                    "discounted",
                    "frequency",
                    "and",
                    "zero-frequency",
                    "estimates",
                    "are",
                    "defined",
                    "as",
                    "follows:"
                ]
            ],
            "context": [
                3,
                3,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Mixture Modelling #REF , a well-established technique for combining multiple models, has been extensively used for language model adaptation in SMT #REF .\n sent1: This technique has also been used for adapting the translation model in SMT with limited success #TARGET_REF .\n sent2: For the given task, since the size of the 'in-domain' data was not significantly large, we used 'suitable' subsets of data from the other available 'out-ofdomain' corpora to enrich the models.\n sent3: For a mixture adapted language model, the probability of an n-gram hw is given as in ( 2):P r mix (w|h) = f * mix (w|h) + λ mix (h)P r mix (w| h) (2)where w is the current word, h is the corresponding history, f * mix is the mixture model discounted relative fre-quency, λ mix indicates the mixture model zero-frequency estimate and hw is the lower order n − 1 gram.\n sent4: The discounted frequency and zero-frequency estimates are defined as follows:\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "To",
                    "avoid",
                    "the",
                    "use",
                    "of",
                    "additional",
                    "data",
                    "structures,",
                    "such",
                    "as",
                    "finite",
                    "automata",
                    "or",
                    "precomputed",
                    "relations,",
                    "we",
                    "have",
                    "been",
                    "inspired",
                    "by",
                    "the",
                    "use",
                    "of",
                    "context-free",
                    "grammars",
                    "to",
                    "represent",
                    "the",
                    "parse",
                    "forest",
                    "of",
                    "tree",
                    "adjoining",
                    "grammars",
                    "#TARGET_REF",
                    "in",
                    "order",
                    "to",
                    "capture",
                    "the",
                    "context-freeness",
                    "of",
                    "production",
                    "application",
                    "in",
                    "the",
                    "case",
                    "of",
                    "LIG."
                ]
            ],
            "context": [
                2
            ]
        },
        "input": "sent0: To avoid the use of additional data structures, such as finite automata or precomputed relations, we have been inspired by the use of context-free grammars to represent the parse forest of tree adjoining grammars #TARGET_REF in order to capture the context-freeness of production application in the case of LIG.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Controllability",
                    "Analysis",
                    "on",
                    "Temporal",
                    "Frequency",
                    "Then,",
                    "we",
                    "analyze",
                    "the",
                    "controllability",
                    "of",
                    "the",
                    "temporal",
                    "frequency",
                    "τ",
                    "to",
                    "present",
                    "whether",
                    "the",
                    "coarse-grained",
                    "or",
                    "fine-grained",
                    "tracepoints",
                    "(sampling",
                    "rate,",
                    "in",
                    "other",
                    "words)",
                    "affects",
                    "the",
                    "generation",
                    "performance."
                ],
                [
                    "As",
                    "the",
                    "Table",
                    "4",
                    "shows,",
                    "we",
                    "change",
                    "the",
                    "temporal",
                    "frequency",
                    "τ",
                    "from",
                    "0.4",
                    "to",
                    "1.2."
                ],
                [
                    "A",
                    "performance",
                    "drop",
                    "is",
                    "impressive",
                    "with",
                    "the",
                    "τ",
                    "getting",
                    "larger."
                ],
                [
                    "The",
                    "purpose",
                    "of",
                    "this",
                    "experiment",
                    "for",
                    "various",
                    "τ",
                    "is",
                    "to",
                    "simulate",
                    "the",
                    "trace",
                    "drawing",
                    "speed",
                    "of",
                    "users",
                    "in",
                    "a",
                    "real",
                    "application",
                    "scenario,",
                    "and",
                    "a",
                    "larger",
                    "τ",
                    "is",
                    "equivalent",
                    "to",
                    "a",
                    "faster",
                    "drawing",
                    "speed."
                ],
                [
                    "As",
                    "#TARGET_REF",
                    "has",
                    "demonstrated,",
                    "the",
                    "length",
                    "is",
                    "one",
                    "of",
                    "the",
                    "critical",
                    "facts",
                    "that",
                    "impact",
                    "quantitative",
                    "performance."
                ],
                [
                    "This",
                    "result",
                    "implies",
                    "we",
                    "can",
                    "further",
                    "decide",
                    "to",
                    "generate",
                    "either",
                    "a",
                    "coarse-grained",
                    "or",
                    "fine-grained",
                    "caption",
                    "by",
                    "controlling",
                    "the",
                    "time-frequency",
                    "τ",
                    "."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                1,
                2
            ]
        },
        "input": "sent0: Controllability Analysis on Temporal Frequency Then, we analyze the controllability of the temporal frequency τ to present whether the coarse-grained or fine-grained tracepoints (sampling rate, in other words) affects the generation performance.\n sent1: As the Table 4 shows, we change the temporal frequency τ from 0.4 to 1.2.\n sent2: A performance drop is impressive with the τ getting larger.\n sent3: The purpose of this experiment for various τ is to simulate the trace drawing speed of users in a real application scenario, and a larger τ is equivalent to a faster drawing speed.\n sent4: As #TARGET_REF has demonstrated, the length is one of the critical facts that impact quantitative performance.\n sent5: This result implies we can further decide to generate either a coarse-grained or fine-grained caption by controlling the time-frequency τ .\n",
        "output": "{\"INFORMATION\": [\"sent4\"], \"PERCEPTION\": [\"sent5\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Image-Text",
                    "Retrieval:",
                    "The",
                    "image-text",
                    "retrieval",
                    "typically",
                    "includes",
                    "two",
                    "sub-tasks:",
                    "image-retrieval",
                    "(IR)",
                    "aims",
                    "to",
                    "retrieval",
                    "an",
                    "image",
                    "when",
                    "given",
                    "a",
                    "specific",
                    "caption",
                    "and",
                    "text-retrieval",
                    "(TR)",
                    "is",
                    "on",
                    "the",
                    "contrary."
                ],
                [
                    "We",
                    "perform",
                    "experiments",
                    "on",
                    "both",
                    "Flickr30k",
                    "#TARGET_REF",
                    "and",
                    "MSCOCO",
                    "dataset."
                ],
                [
                    "As",
                    "in",
                    "UNITER,",
                    "we",
                    "construct",
                    "a",
                    "mini-batch",
                    "for",
                    "each",
                    "GPU",
                    "of",
                    "a",
                    "matched",
                    "image-text",
                    "pair,",
                    "t-1",
                    "negative",
                    "images,",
                    "and",
                    "t-1",
                    "negative",
                    "texts",
                    "where",
                    "t",
                    "is",
                    "set",
                    "as",
                    "32."
                ],
                [
                    "Besides,",
                    "we",
                    "take",
                    "a",
                    "fully-connected",
                    "network",
                    "on",
                    "top",
                    "of",
                    "h",
                    "cls",
                    "and",
                    "adopt",
                    "the",
                    "binary",
                    "cross-entropy",
                    "loss",
                    "as",
                    "supervision",
                    "signal."
                ],
                [
                    "The",
                    "finetuning",
                    "iterations",
                    "are",
                    "up",
                    "to",
                    "10K",
                    "by",
                    "following",
                    "linear",
                    "decay",
                    "scheduling",
                    "with",
                    "initial",
                    "lr",
                    "7e-5",
                    "for",
                    "Transformer,",
                    "1e-4",
                    "for",
                    "CNNs."
                ],
                [
                    "Top-K",
                    "(R@K,",
                    "K",
                    "∈",
                    "{1,",
                    "5,",
                    "10})",
                    "recall",
                    "is",
                    "the",
                    "evaluation",
                    "metric."
                ]
            ],
            "context": [
                0,
                2,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Image-Text Retrieval: The image-text retrieval typically includes two sub-tasks: image-retrieval (IR) aims to retrieval an image when given a specific caption and text-retrieval (TR) is on the contrary.\n sent1: We perform experiments on both Flickr30k #TARGET_REF and MSCOCO dataset.\n sent2: As in UNITER, we construct a mini-batch for each GPU of a matched image-text pair, t-1 negative images, and t-1 negative texts where t is set as 32.\n sent3: Besides, we take a fully-connected network on top of h cls and adopt the binary cross-entropy loss as supervision signal.\n sent4: The finetuning iterations are up to 10K by following linear decay scheduling with initial lr 7e-5 for Transformer, 1e-4 for CNNs.\n sent5: Top-K (R@K, K ∈ {1, 5, 10}) recall is the evaluation metric.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Following",
                    "#TARGET_REF",
                    ",",
                    "we",
                    "use",
                    "λ",
                    "=",
                    "0.5",
                    "for",
                    "Koyyalagunta",
                    "and",
                    "KoyyRestrict",
                    "scoring",
                    "functions,",
                    "but",
                    "also",
                    "for",
                    "the",
                    "Harmonic",
                    "function."
                ],
                [
                    "We",
                    "pair",
                    "all",
                    "relatedness",
                    "measures",
                    "to",
                    "all",
                    "scoring",
                    "functions,",
                    "creating",
                    "16",
                    "agents",
                    "in",
                    "total,",
                    "and",
                    "generate",
                    "clues",
                    "for",
                    "n",
                    "=",
                    "2",
                    "and",
                    "3",
                    "targeted",
                    "blue",
                    "words",
                    "using",
                    "all",
                    "of",
                    "them."
                ],
                [
                    "Differently",
                    "from",
                    "#REF",
                    ",",
                    "we",
                    "consider",
                    "all",
                    "of",
                    "our",
                    "vocabulary",
                    "words",
                    "as",
                    "possible",
                    "clue",
                    "words."
                ],
                [
                    "For",
                    "each",
                    "possible",
                    "clue",
                    "word,",
                    "the",
                    "best",
                    "target",
                    "words",
                    "in",
                    "the",
                    "set",
                    "I",
                    "n",
                    "are",
                    "the",
                    "n",
                    "closest",
                    "words",
                    "to",
                    "the",
                    "clue",
                    "word,",
                    "so",
                    "scoring",
                    "a",
                    "possible",
                    "clue",
                    "is",
                    "computationally",
                    "inexpensive."
                ]
            ],
            "context": [
                3,
                3,
                2,
                3
            ]
        },
        "input": "sent0: Following #TARGET_REF , we use λ = 0.5 for Koyyalagunta and KoyyRestrict scoring functions, but also for the Harmonic function.\n sent1: We pair all relatedness measures to all scoring functions, creating 16 agents in total, and generate clues for n = 2 and 3 targeted blue words using all of them.\n sent2: Differently from #REF , we consider all of our vocabulary words as possible clue words.\n sent3: For each possible clue word, the best target words in the set I n are the n closest words to the clue word, so scoring a possible clue is computationally inexpensive.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": [\"sent0\", \"sent1\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "used",
                    "GNU",
                    "Parallel",
                    "for",
                    "much",
                    "of",
                    "the",
                    "dataset",
                    "processing",
                    "#REF",
                    "."
                ],
                [
                    "In",
                    "combination",
                    "with",
                    "#TARGET_REF",
                    "from",
                    "Hugging",
                    "Face,",
                    "GNU",
                    "Parallel",
                    "significantly",
                    "accelerated",
                    "pre-processing",
                    "and",
                    "phone",
                    "transcription."
                ]
            ],
            "context": [
                0,
                2
            ]
        },
        "input": "sent0: We used GNU Parallel for much of the dataset processing #REF .\n sent1: In combination with #TARGET_REF from Hugging Face, GNU Parallel significantly accelerated pre-processing and phone transcription.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "English",
                    "and",
                    "Portuguese",
                    "are",
                    "the",
                    "primary",
                    "languages",
                    "and",
                    "general",
                    "STS",
                    "data,",
                    "from",
                    "STSBenchmark",
                    "#TARGET_REF",
                    "and",
                    "ASSIN2",
                    "#REF",
                    "for",
                    "English",
                    "and",
                    "Portuguese",
                    "respectively,",
                    "and",
                    "idiom",
                    "STS",
                    "data",
                    "for",
                    "both",
                    "languages",
                    "are",
                    "included",
                    "in",
                    "the",
                    "train,",
                    "dev,",
                    "eval",
                    "and",
                    "test",
                    "sets."
                ],
                [
                    "A",
                    "very",
                    "small",
                    "amount",
                    "(50",
                    "examples)",
                    "of",
                    "Galician",
                    "data,",
                    "comprised",
                    "of",
                    "idiom",
                    "STS",
                    "data,",
                    "is",
                    "also",
                    "included",
                    "in",
                    "the",
                    "test",
                    "set."
                ]
            ],
            "context": [
                3,
                3
            ]
        },
        "input": "sent0: English and Portuguese are the primary languages and general STS data, from STSBenchmark #TARGET_REF and ASSIN2 #REF for English and Portuguese respectively, and idiom STS data for both languages are included in the train, dev, eval and test sets.\n sent1: A very small amount (50 examples) of Galician data, comprised of idiom STS data, is also included in the test set.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "All",
                    "models",
                    "are",
                    "implemented",
                    "based",
                    "on",
                    "the",
                    "opensource",
                    "transformers",
                    "library",
                    "of",
                    "hugging",
                    "face",
                    "#TARGET_REF",
                    ",",
                    "which",
                    "provides",
                    "thousands",
                    "of",
                    "pretrained",
                    "models",
                    "that",
                    "can",
                    "be",
                    "quickly",
                    "downloaded",
                    "and",
                    "fine-tuned",
                    "on",
                    "specific",
                    "tasks."
                ],
                [
                    "of",
                    "doing",
                    "this",
                    "in",
                    "Table",
                    "3",
                    "and",
                    "we",
                    "can",
                    "find",
                    "that",
                    "it",
                    "is",
                    "very",
                    "effective",
                    "by",
                    "increasing",
                    "0.02",
                    "from",
                    "base",
                    "models."
                ]
            ],
            "context": [
                1,
                0
            ]
        },
        "input": "sent0: All models are implemented based on the opensource transformers library of hugging face #TARGET_REF , which provides thousands of pretrained models that can be quickly downloaded and fine-tuned on specific tasks.\n sent1: of doing this in Table 3 and we can find that it is very effective by increasing 0.02 from base models.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "To",
                    "this",
                    "aim,",
                    "we",
                    "propose",
                    "to",
                    "use",
                    "a",
                    "memory",
                    "component",
                    "as",
                    "the",
                    "additional",
                    "vocabulary."
                ],
                [
                    "Note",
                    "that",
                    "the",
                    "selection",
                    "of",
                    "words",
                    "to",
                    "build",
                    "the",
                    "vocabulary",
                    "is",
                    "task",
                    "dependent,",
                    "and",
                    "we",
                    "select",
                    "the",
                    "words",
                    "appearing",
                    "in",
                    "state",
                    "annotations",
                    "and",
                    "content",
                    "words",
                    "2",
                    "extracted",
                    "from",
                    "task",
                    "descriptions",
                    "provided",
                    "in",
                    "the",
                    "dataset",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "This",
                    "simple",
                    "strategy",
                    "is",
                    "intuitive",
                    "and",
                    "turns",
                    "out",
                    "to",
                    "be",
                    "empirically",
                    "competitive."
                ]
            ],
            "context": [
                0,
                2,
                0
            ]
        },
        "input": "sent0: To this aim, we propose to use a memory component as the additional vocabulary.\n sent1: Note that the selection of words to build the vocabulary is task dependent, and we select the words appearing in state annotations and content words 2 extracted from task descriptions provided in the dataset #TARGET_REF .\n sent2: This simple strategy is intuitive and turns out to be empirically competitive.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "To",
                    "compare",
                    "the",
                    "linguistic",
                    "annotation",
                    "performance,",
                    "we",
                    "prepared",
                    "a",
                    "pipeline",
                    "system,",
                    "i.e.,",
                    "ASR",
                    "followed",
                    "by",
                    "an",
                    "NLP-based",
                    "linguistic",
                    "annotation."
                ],
                [
                    "In",
                    "the",
                    "pipeline",
                    "system,",
                    "the",
                    "separated",
                    "model",
                    "of",
                    "CTC+Transformer",
                    "first",
                    "predicts",
                    "graphemic",
                    "sequences."
                ],
                [
                    "Then,",
                    "the",
                    "linear",
                    "SVM",
                    "with",
                    "L2",
                    "normalization,",
                    "trained",
                    "using",
                    "KyTea",
                    "#TARGET_REF",
                    ",",
                    "predicts",
                    "word",
                    "boundaries",
                    "and",
                    "linguistic",
                    "annotation",
                    "from",
                    "the",
                    "predicted",
                    "sequences."
                ],
                [
                    "To",
                    "train",
                    "KyTea,",
                    "we",
                    "only",
                    "used",
                    "the",
                    "transcriptions",
                    "in",
                    "the",
                    "ASR",
                    "training",
                    "set",
                    "to",
                    "perform",
                    "a",
                    "fair",
                    "comparison",
                    "to",
                    "the",
                    "proposed",
                    "method."
                ]
            ],
            "context": [
                0,
                0,
                3,
                2
            ]
        },
        "input": "sent0: To compare the linguistic annotation performance, we prepared a pipeline system, i.e., ASR followed by an NLP-based linguistic annotation.\n sent1: In the pipeline system, the separated model of CTC+Transformer first predicts graphemic sequences.\n sent2: Then, the linear SVM with L2 normalization, trained using KyTea #TARGET_REF , predicts word boundaries and linguistic annotation from the predicted sequences.\n sent3: To train KyTea, we only used the transcriptions in the ASR training set to perform a fair comparison to the proposed method.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent3\"], \"BACKGROUND\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "confound-controlled",
                    "analysis",
                    "dissociated",
                    "word",
                    "length,",
                    "frequency",
                    "and",
                    "class",
                    "effects",
                    "in",
                    "EEG."
                ],
                [
                    "This",
                    "replication",
                    "of",
                    "earlier",
                    "ERP",
                    "#REF",
                    "and",
                    "MEG",
                    "decoding",
                    "work",
                    "#REF",
                    "validates",
                    "a",
                    "new",
                    "EEG",
                    "data",
                    "set",
                    "for",
                    "an",
                    "extensive",
                    "morphosyntactically",
                    "gold",
                    "annotated",
                    "corpus,",
                    "c.f."
                ],
                [
                    "#REF",
                    "."
                ],
                [
                    "Transformers",
                    "successfully",
                    "decoded",
                    "6",
                    "PoS",
                    "tags",
                    "from",
                    "single",
                    "trial",
                    "EEG",
                    "with",
                    "data",
                    "augmentation",
                    "and",
                    "3-1",
                    "pretraining",
                    "(≈",
                    "40%",
                    "accuracy),",
                    "raising",
                    "the",
                    "possibility",
                    "to",
                    "boost",
                    "PoS",
                    "induction",
                    "with",
                    "EEG-decoded",
                    "PoS",
                    "tags."
                ],
                [
                    "While",
                    "we",
                    "acknowledge",
                    "that",
                    "our",
                    "results",
                    "are",
                    "limited",
                    "to",
                    "EEG",
                    "data",
                    "from",
                    "a",
                    "single",
                    "subject,",
                    "given",
                    "the",
                    "spatial",
                    "smoothness",
                    "of",
                    "EEG",
                    "scalp",
                    "topographies,",
                    "we",
                    "envision",
                    "pretraining",
                    "on",
                    "EEG",
                    "obtained",
                    "from",
                    "different",
                    "participants."
                ],
                [
                    "Further,",
                    "because",
                    "human",
                    "brains",
                    "generate",
                    "similar",
                    "neural",
                    "signatures",
                    "for",
                    "word",
                    "classes",
                    "across",
                    "different",
                    "languages",
                    "(c.f."
                ],
                [
                    "Yudes",
                    "(2016),",
                    "#REF",
                    ",",
                    "#TARGET_REF",
                    "),",
                    "pretraining",
                    "PoS-EEG",
                    "decoders",
                    "on",
                    "large",
                    "morphosyntactically",
                    "annotated",
                    "EEG",
                    "datasets",
                    "for",
                    "English",
                    "followed",
                    "by",
                    "fine-tuning",
                    "on",
                    "a",
                    "smaller",
                    "annotated",
                    "EEG",
                    "data",
                    "set",
                    "for",
                    "a",
                    "low-resource",
                    "language",
                    "may",
                    "enable",
                    "successful",
                    "generalisation",
                    "to",
                    "EEG",
                    "obtained",
                    "from",
                    "reading",
                    "non-annotated",
                    "texts",
                    "in",
                    "this",
                    "low-resource",
                    "language."
                ],
                [
                    "PoS-induction",
                    "jointly",
                    "based",
                    "on",
                    "annotated",
                    "texts",
                    "and",
                    "EEG",
                    "signals",
                    "could",
                    "thus",
                    "be",
                    "transformative",
                    "for",
                    "corpus",
                    "generation",
                    "of",
                    "low-resource",
                    "languages."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                0
            ]
        },
        "input": "sent0: The confound-controlled analysis dissociated word length, frequency and class effects in EEG.\n sent1: This replication of earlier ERP #REF and MEG decoding work #REF validates a new EEG data set for an extensive morphosyntactically gold annotated corpus, c.f.\n sent2: #REF .\n sent3: Transformers successfully decoded 6 PoS tags from single trial EEG with data augmentation and 3-1 pretraining (≈ 40% accuracy), raising the possibility to boost PoS induction with EEG-decoded PoS tags.\n sent4: While we acknowledge that our results are limited to EEG data from a single subject, given the spatial smoothness of EEG scalp topographies, we envision pretraining on EEG obtained from different participants.\n sent5: Further, because human brains generate similar neural signatures for word classes across different languages (c.f.\n sent6: Yudes (2016), #REF , #TARGET_REF ), pretraining PoS-EEG decoders on large morphosyntactically annotated EEG datasets for English followed by fine-tuning on a smaller annotated EEG data set for a low-resource language may enable successful generalisation to EEG obtained from reading non-annotated texts in this low-resource language.\n sent7: PoS-induction jointly based on annotated texts and EEG signals could thus be transformative for corpus generation of low-resource languages.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent5\", \"sent6\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Curriculum",
                    "Diversity."
                ],
                [
                    "The",
                    "variations",
                    "in",
                    "the",
                    "combinations",
                    "of",
                    "quests",
                    "and",
                    "worlds",
                    "themselves",
                    "seen",
                    "at",
                    "training",
                    "time",
                    "has",
                    "potential",
                    "to",
                    "effect",
                    "zero-shot",
                    "performance",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "We",
                    "introduce",
                    "two",
                    "baselines",
                    "that",
                    "change",
                    "the",
                    "relative",
                    "diversities",
                    "of",
                    "resulting",
                    "quests",
                    "in",
                    "the",
                    "curriculums,",
                    "to",
                    "contrast",
                    "with",
                    "our",
                    "proposed",
                    "procedural",
                    "generation",
                    "pipeline."
                ],
                [
                    "Generated",
                    "quest",
                    "details",
                    "are",
                    "found",
                    "in",
                    "Appendix",
                    "A.5."
                ]
            ],
            "context": [
                0,
                1,
                0,
                0
            ]
        },
        "input": "sent0: Curriculum Diversity.\n sent1: The variations in the combinations of quests and worlds themselves seen at training time has potential to effect zero-shot performance #TARGET_REF .\n sent2: We introduce two baselines that change the relative diversities of resulting quests in the curriculums, to contrast with our proposed procedural generation pipeline.\n sent3: Generated quest details are found in Appendix A.5.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Recently",
                    "#TARGET_REF",
                    "introduced",
                    "a",
                    "neural-Bayesian",
                    "nonparametric",
                    "estimator",
                    "for",
                    "probability",
                    "distributions",
                    "on",
                    "single",
                    "words."
                ],
                [
                    "Their",
                    "setting",
                    "has",
                    "an",
                    "unknown",
                    "and",
                    "generally",
                    "infinite",
                    "vocabulary",
                    "V",
                    ",",
                    "and",
                    "their",
                    "model",
                    "generalizes",
                    "using",
                    "a",
                    "characterlevel",
                    "LSTM."
                ],
                [
                    "In",
                    "contrast,",
                    "the",
                    "current",
                    "model",
                    "assumes",
                    "a",
                    "pre-existing",
                    "known",
                    "vocabulary",
                    "V",
                    "with",
                    "embeddings,",
                    "and",
                    "generalizes",
                    "based",
                    "on",
                    "those",
                    "embeddings."
                ],
                [
                    "A",
                    "hybrid",
                    "model",
                    "may",
                    "be",
                    "possible",
                    "in",
                    "future",
                    "work."
                ]
            ],
            "context": [
                1,
                1,
                0,
                0
            ]
        },
        "input": "sent0: Recently #TARGET_REF introduced a neural-Bayesian nonparametric estimator for probability distributions on single words.\n sent1: Their setting has an unknown and generally infinite vocabulary V , and their model generalizes using a characterlevel LSTM.\n sent2: In contrast, the current model assumes a pre-existing known vocabulary V with embeddings, and generalizes based on those embeddings.\n sent3: A hybrid model may be possible in future work.\n",
        "output": "{\"INFORMATION\": [\"sent0\", \"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "This",
                    "work",
                    "is",
                    "part",
                    "of",
                    "an",
                    "effort",
                    "to",
                    "develop",
                    "a",
                    "robust,",
                    "domain-independent",
                    "syntactic",
                    "parser",
                    "capable",
                    "of",
                    "yielding",
                    "the",
                    "one",
                    "correct",
                    "analysis",
                    "for",
                    "unrestricted",
                    "naturally-occurring",
                    "input."
                ],
                [
                    "Our",
                    "goal",
                    "is",
                    "to",
                    "develop",
                    "a",
                    "system",
                    "with",
                    "performance",
                    "comparable",
                    "to",
                    "extant",
                    "part-of-speech",
                    "taggers,",
                    "returning",
                    "a",
                    "syntactic",
                    "analysis",
                    "from",
                    "which",
                    "predicate-argument",
                    "structure",
                    "can",
                    "be",
                    "recovered,",
                    "and",
                    "which",
                    "can",
                    "support",
                    "semantic",
                    "interpretation."
                ],
                [
                    "The",
                    "requirement",
                    "for",
                    "a",
                    "domain-independent",
                    "analyser",
                    "favours",
                    "statistical",
                    "techniques",
                    "to",
                    "resolve",
                    "ambiguities,",
                    "whilst",
                    "the",
                    "latter",
                    "goal",
                    "favours",
                    "a",
                    "more",
                    "sophisticated",
                    "grammatical",
                    "formalism",
                    "than",
                    "is",
                    "typical",
                    "in",
                    "statistical",
                    "approaches",
                    "to",
                    "robust",
                    "analysis",
                    "of",
                    "corpus",
                    "material."
                ],
                [
                    "#REF",
                    "describe",
                    "a",
                    "probablistic",
                    "parser",
                    "using",
                    "a",
                    "wide-coverage",
                    "unification",
                    "based",
                    "grammar",
                    "of",
                    "English",
                    "written",
                    "in",
                    "the",
                    "Alvey",
                    "Natural",
                    "Language",
                    "To",
                    "ols",
                    "(ANLT)",
                    "meta",
                    "g",
                    "rammat",
                    "ical",
                    "formalism",
                    "#REF",
                    ",",
                    "generating",
                    "around",
                    "800",
                    "rules",
                    "in",
                    "a",
                    "syntactic",
                    "variant",
                    "of",
                    "the",
                    "Definite",
                    "Clause",
                    "Grammar",
                    "formalism",
                    "(DCG,",
                    "#REF",
                    "extended",
                    "with",
                    "iterative",
                    "(Kleene)",
                    "operators."
                ],
                [
                    "The",
                    "ANLT",
                    "grammar",
                    "is",
                    "linked",
                    "to",
                    "a",
                    "lexicon",
                    "containing",
                    "about",
                    "64K",
                    "entries",
                    "for",
                    "40K",
                    "lexemes,",
                    "including",
                    "detailed",
                    "subcategorisation",
                    "information",
                    "appropriate",
                    "for",
                    "the",
                    "grammar,",
                    "built",
                    "semi-automatically",
                    "from",
                    "a",
                    "learners'",
                    "dictionary",
                    "#REF",
                    "."
                ],
                [
                    "The",
                    "resulting",
                    "parser",
                    "is",
                    "efficient,",
                    "capable",
                    "of",
                    "constructing",
                    "a",
                    "parse",
                    "forest",
                    "in",
                    "what",
                    "seems",
                    "to",
                    "be",
                    "roughly",
                    "quadratic",
                    "time,",
                    "and",
                    "efficiently",
                    "returning",
                    "the",
                    "ranked",
                    "n-most",
                    "likely",
                    "analyses",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "The",
                    "probabilistic",
                    "model",
                    "is",
                    "a",
                    "refinement",
                    "of",
                    "probabilistic",
                    "context-free",
                    "grammar",
                    "(PCFG)",
                    "conditioning",
                    "CF",
                    "'backbone'",
                    "rule",
                    "application",
                    "on",
                    "LR",
                    "state",
                    "and",
                    "lookahead",
                    "item."
                ],
                [
                    "Unification",
                    "of",
                    "the",
                    "'residue'"
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                3,
                0,
                0
            ]
        },
        "input": "sent0: This work is part of an effort to develop a robust, domain-independent syntactic parser capable of yielding the one correct analysis for unrestricted naturally-occurring input.\n sent1: Our goal is to develop a system with performance comparable to extant part-of-speech taggers, returning a syntactic analysis from which predicate-argument structure can be recovered, and which can support semantic interpretation.\n sent2: The requirement for a domain-independent analyser favours statistical techniques to resolve ambiguities, whilst the latter goal favours a more sophisticated grammatical formalism than is typical in statistical approaches to robust analysis of corpus material.\n sent3: #REF describe a probablistic parser using a wide-coverage unification based grammar of English written in the Alvey Natural Language To ols (ANLT) meta g rammat ical formalism #REF , generating around 800 rules in a syntactic variant of the Definite Clause Grammar formalism (DCG, #REF extended with iterative (Kleene) operators.\n sent4: The ANLT grammar is linked to a lexicon containing about 64K entries for 40K lexemes, including detailed subcategorisation information appropriate for the grammar, built semi-automatically from a learners' dictionary #REF .\n sent5: The resulting parser is efficient, capable of constructing a parse forest in what seems to be roughly quadratic time, and efficiently returning the ranked n-most likely analyses #TARGET_REF .\n sent6: The probabilistic model is a refinement of probabilistic context-free grammar (PCFG) conditioning CF 'backbone' rule application on LR state and lookahead item.\n sent7: Unification of the 'residue'\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent5\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "To",
                    "observe",
                    "the",
                    "effect",
                    "of",
                    "disentanglement",
                    "in",
                    "homotopy",
                    "#TARGET_REF",
                    "Additionally,",
                    "to",
                    "highlight",
                    "the",
                    "role",
                    "of",
                    "generative",
                    "factor",
                    "in",
                    "generation,",
                    "we",
                    "conduct",
                    "a",
                    "dimensionwise",
                    "homotopy,",
                    "transitioning",
                    "from",
                    "the",
                    "first",
                    "to",
                    "the",
                    "last",
                    "sentence",
                    "by",
                    "interpolating",
                    "between",
                    "the",
                    "dimensions",
                    "one-by-one."
                ],
                [
                    "This",
                    "is",
                    "implemented",
                    "as",
                    "follows:",
                    "(i)",
                    "using",
                    "prior",
                    "distribution",
                    "7",
                    "we",
                    "sample",
                    "two",
                    "latent",
                    "codes",
                    "denoted",
                    "by",
                    "z",
                    "1",
                    "=",
                    "(z",
                    "1,1",
                    ",",
                    "z",
                    "1,2",
                    ",",
                    "."
                ],
                [
                    "."
                ],
                [
                    "."
                ],
                [
                    ",",
                    "z",
                    "1,n",
                    "),",
                    "z",
                    "2",
                    "=",
                    "(z",
                    "2,1",
                    ",",
                    "z",
                    "2,2",
                    ",",
                    "."
                ],
                [
                    "."
                ],
                [
                    "."
                ],
                [
                    ",",
                    "z",
                    "2,n",
                    "),",
                    "(ii)",
                    "for",
                    "i-th",
                    "dimension,",
                    "using",
                    "z",
                    "1,i",
                    "=",
                    "(z",
                    "2,1",
                    ",",
                    "."
                ],
                [
                    "."
                ],
                [
                    "."
                ],
                [
                    ",",
                    "z",
                    "2,i−1",
                    ",",
                    "z",
                    "1,i",
                    ",",
                    "."
                ],
                [
                    "."
                ],
                [
                    "."
                ],
                [
                    ",",
                    "z",
                    "1,n",
                    ")",
                    "as",
                    "the",
                    "start,",
                    "we",
                    "interpolate",
                    "along",
                    "the",
                    "i-th",
                    "dimension",
                    "towards",
                    "z",
                    "2,i",
                    "=",
                    "(z",
                    "2,1",
                    ",",
                    "."
                ],
                [
                    "."
                ],
                [
                    "."
                ],
                [
                    ",",
                    "z",
                    "2,i",
                    ",",
                    "z",
                    "1,i+1",
                    ",",
                    "."
                ],
                [
                    "."
                ],
                [
                    "."
                ],
                [
                    ",",
                    "z",
                    "1,n",
                    ")."
                ],
                [
                    "Table",
                    "6",
                    "illustrates",
                    "this",
                    "for",
                    "a",
                    "3D",
                    "latent",
                    "code",
                    "example."
                ]
            ],
            "context": [
                2,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3
            ]
        },
        "input": "sent0: To observe the effect of disentanglement in homotopy #TARGET_REF Additionally, to highlight the role of generative factor in generation, we conduct a dimensionwise homotopy, transitioning from the first to the last sentence by interpolating between the dimensions one-by-one.\n sent1: This is implemented as follows: (i) using prior distribution 7 we sample two latent codes denoted by z 1 = (z 1,1 , z 1,2 , .\n sent2: .\n sent3: .\n sent4: , z 1,n ), z 2 = (z 2,1 , z 2,2 , .\n sent5: .\n sent6: .\n sent7: , z 2,n ), (ii) for i-th dimension, using z 1,i = (z 2,1 , .\n sent8: .\n sent9: .\n sent10: , z 2,i−1 , z 1,i , .\n sent11: .\n sent12: .\n sent13: , z 1,n ) as the start, we interpolate along the i-th dimension towards z 2,i = (z 2,1 , .\n sent14: .\n sent15: .\n sent16: , z 2,i , z 1,i+1 , .\n sent17: .\n sent18: .\n sent19: , z 1,n ).\n sent20: Table 6 illustrates this for a 3D latent code example.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": [\"sent1\", \"sent2\", \"sent3\", \"sent4\", \"sent5\", \"sent6\", \"sent7\", \"sent8\", \"sent9\", \"sent10\", \"sent11\", \"sent12\", \"sent13\", \"sent14\", \"sent15\", \"sent16\", \"sent17\", \"sent18\", \"sent19\", \"sent20\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "fact,",
                    "homophily",
                    "is",
                    "so",
                    "prominent,",
                    "#REF",
                    "noted",
                    "in",
                    "their",
                    "work",
                    "that",
                    "the",
                    "profiles",
                    "they",
                    "generated",
                    "from",
                    "the",
                    "social",
                    "graph",
                    "of",
                    "users",
                    "and",
                    "tweets",
                    "could",
                    "encode",
                    "patters",
                    "of",
                    "similar",
                    "linguistic",
                    "practices",
                    "amongst",
                    "connected",
                    "users",
                    "in",
                    "the",
                    "Waseem",
                    "and",
                    "Hovy",
                    "(2016)",
                    "dataset,",
                    "hence",
                    "allowing",
                    "for",
                    "comments",
                    "with",
                    "implicit",
                    "and",
                    "generalized",
                    "sexism",
                    "or",
                    "racism",
                    "to",
                    "be",
                    "better",
                    "detected."
                ],
                [
                    "Moreover,",
                    "homophily",
                    "has",
                    "direct",
                    "associations",
                    "with",
                    "all",
                    "the",
                    "four",
                    "aspects",
                    "of",
                    "context",
                    "that",
                    "we",
                    "described",
                    "in",
                    "section",
                    "2,",
                    "i.e.,",
                    "similar",
                    "sociolinguistic",
                    "norms",
                    "and",
                    "shared",
                    "language",
                    "markers",
                    "facilitate",
                    "homophilic",
                    "ties",
                    "in",
                    "social",
                    "networks",
                    "#TARGET_REF",
                    ",",
                    "as",
                    "do",
                    "shared",
                    "beliefs,",
                    "stereotypes,",
                    "and",
                    "demographic",
                    "traits",
                    "#REF",
                    "."
                ],
                [
                    "Therefore,",
                    "capturing",
                    "homophily",
                    "allows",
                    "for",
                    "all",
                    "the",
                    "four",
                    "aspects",
                    "to",
                    "be",
                    "directly",
                    "captured",
                    "together."
                ],
                [
                    "We",
                    "note",
                    "that",
                    "just",
                    "exploiting",
                    "simplistic",
                    "and",
                    "limited",
                    "inductive",
                    "biases",
                    "that",
                    "are",
                    "easy",
                    "to",
                    "extract,",
                    "like",
                    "gender",
                    "of",
                    "the",
                    "user,",
                    "can",
                    "render",
                    "methods",
                    "prone",
                    "to",
                    "making",
                    "faulty",
                    "generalizations",
                    "because",
                    "of",
                    "overfitting",
                    "to",
                    "patterns",
                    "in",
                    "the",
                    "training",
                    "data."
                ],
                [
                    "This",
                    "is",
                    "also",
                    "evident",
                    "from",
                    "the",
                    "observations",
                    "that",
                    "#REF",
                    "made",
                    "in",
                    "their",
                    "work."
                ],
                [
                    "They",
                    "noted",
                    "that",
                    "the",
                    "profiles",
                    "they",
                    "generated",
                    "from",
                    "the",
                    "social",
                    "graph",
                    "consisting",
                    "of",
                    "user",
                    "and",
                    "tweet",
                    "nodes",
                    "improved",
                    "F",
                    "1",
                    "scores",
                    "over",
                    "the",
                    "profiles",
                    "#REF",
                    "generated",
                    "from",
                    "the",
                    "social",
                    "graph",
                    "just",
                    "consisting",
                    "of",
                    "users,",
                    "with",
                    "the",
                    "gains",
                    "mainly",
                    "coming",
                    "from",
                    "increase",
                    "in",
                    "precision."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: In fact, homophily is so prominent, #REF noted in their work that the profiles they generated from the social graph of users and tweets could encode patters of similar linguistic practices amongst connected users in the Waseem and Hovy (2016) dataset, hence allowing for comments with implicit and generalized sexism or racism to be better detected.\n sent1: Moreover, homophily has direct associations with all the four aspects of context that we described in section 2, i.e., similar sociolinguistic norms and shared language markers facilitate homophilic ties in social networks #TARGET_REF , as do shared beliefs, stereotypes, and demographic traits #REF .\n sent2: Therefore, capturing homophily allows for all the four aspects to be directly captured together.\n sent3: We note that just exploiting simplistic and limited inductive biases that are easy to extract, like gender of the user, can render methods prone to making faulty generalizations because of overfitting to patterns in the training data.\n sent4: This is also evident from the observations that #REF made in their work.\n sent5: They noted that the profiles they generated from the social graph consisting of user and tweet nodes improved F 1 scores over the profiles #REF generated from the social graph just consisting of users, with the gains mainly coming from increase in precision.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Leveraging",
                    "sparsity",
                    "of",
                    "matrices",
                    "We",
                    "considered",
                    "the",
                    "option",
                    "of",
                    "performing",
                    "matrix",
                    "products",
                    "involving",
                    "large",
                    "sparse",
                    "matrices",
                    "(Lines",
                    "8,",
                    "15,",
                    "21,",
                    "25",
                    "in",
                    "our",
                    "pseudo-code",
                    "(",
                    "§2.2))",
                    "by",
                    "representing",
                    "them",
                    "in",
                    "PyTorch's",
                    "torch.sparse_coo_tensor",
                    "format",
                    "and",
                    "using",
                    "the",
                    "torch.sparse",
                    "framework",
                    "to",
                    "explicitly",
                    "leverage",
                    "their",
                    "sparsity",
                    "for",
                    "saving",
                    "compute."
                ],
                [
                    "Unfortunately,",
                    "the",
                    "results",
                    "were",
                    "not",
                    "encour-",
                    "aging",
                    "even",
                    "for",
                    "k",
                    "=",
                    "1%",
                    "of",
                    "number",
                    "of",
                    "keys",
                    "(Figure",
                    "4)."
                ],
                [
                    "While",
                    "future",
                    "devices",
                    "might",
                    "allow",
                    "faster",
                    "sparsedense",
                    "products,",
                    "in",
                    "the",
                    "immediate",
                    "future,",
                    "one",
                    "can",
                    "leverage",
                    "block-sparse",
                    "kernels",
                    "#REF",
                    "which",
                    "have",
                    "been",
                    "successfully",
                    "used",
                    "for",
                    "such",
                    "products",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                0,
                0,
                1
            ]
        },
        "input": "sent0: Leveraging sparsity of matrices We considered the option of performing matrix products involving large sparse matrices (Lines 8, 15, 21, 25 in our pseudo-code ( §2.2)) by representing them in PyTorch's torch.sparse_coo_tensor format and using the torch.sparse framework to explicitly leverage their sparsity for saving compute.\n sent1: Unfortunately, the results were not encour- aging even for k = 1% of number of keys (Figure 4).\n sent2: While future devices might allow faster sparsedense products, in the immediate future, one can leverage block-sparse kernels #REF which have been successfully used for such products #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "this",
                    "context,",
                    "the",
                    "main",
                    "question",
                    "then",
                    "arises:",
                    "shouldn't",
                    "machine",
                    "translation",
                    "be",
                    "multilingual",
                    "for",
                    "languages",
                    "spoken",
                    "in",
                    "a",
                    "multilingual",
                    "country",
                    "like",
                    "Peru?"
                ],
                [
                    "By",
                    "taking",
                    "advantage",
                    "of",
                    "few",
                    "resources,",
                    "and",
                    "other",
                    "strategies",
                    "such",
                    "as",
                    "multilingual",
                    "unsupervised",
                    "subword",
                    "segmentation",
                    "models",
                    "#TARGET_REF",
                    ",",
                    "pretraining",
                    "with",
                    "high",
                    "resource",
                    "language-pairs",
                    "#REF",
                    ",",
                    "back-translation",
                    "#REF",
                    ",",
                    "and",
                    "fine-tuning",
                    "#REF",
                    ",",
                    "we",
                    "deployed",
                    "the",
                    "first",
                    "many-to-one",
                    "and",
                    "one-to-many",
                    "multilingual",
                    "NMT",
                    "models",
                    "(paired",
                    "with",
                    "Spanish)",
                    "for",
                    "four",
                    "indigenous",
                    "languages:",
                    "Aymara,",
                    "Ashaninka,",
                    "Quechua",
                    "and",
                    "Shipibo-Konibo."
                ]
            ],
            "context": [
                2,
                2
            ]
        },
        "input": "sent0: In this context, the main question then arises: shouldn't machine translation be multilingual for languages spoken in a multilingual country like Peru?\n sent1: By taking advantage of few resources, and other strategies such as multilingual unsupervised subword segmentation models #TARGET_REF , pretraining with high resource language-pairs #REF , back-translation #REF , and fine-tuning #REF , we deployed the first many-to-one and one-to-many multilingual NMT models (paired with Spanish) for four indigenous languages: Aymara, Ashaninka, Quechua and Shipibo-Konibo.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\", \"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Recently,",
                    "#TARGET_REF",
                    "and",
                    "#REF",
                    "have",
                    "nevertheless",
                    "shown",
                    "that",
                    "the",
                    "commonly",
                    "adopted",
                    "practices",
                    "(the",
                    "number",
                    "of",
                    "iterations,",
                    "the",
                    "choice",
                    "of",
                    "model",
                    "layers)",
                    "when",
                    "fine-tuning",
                    "Transformers-based",
                    "langage",
                    "models",
                    "are",
                    "inappropriate",
                    "under",
                    "resource",
                    "constrained",
                    "conditions",
                    "and",
                    "adversely",
                    "affect",
                    "the",
                    "stability",
                    "of",
                    "models",
                    "performances",
                    "as",
                    "overfitting,",
                    "label",
                    "noise",
                    "memorization",
                    "or",
                    "catastrophic",
                    "forgetting."
                ],
                [
                    "Added",
                    "to",
                    "this,",
                    "because",
                    "the",
                    "pretraining",
                    "process",
                    "is",
                    "particularly",
                    "constraining,",
                    "various",
                    "works",
                    "have",
                    "been",
                    "oriented",
                    "towards",
                    "the",
                    "research",
                    "and",
                    "training",
                    "of",
                    "efficient",
                    "models,",
                    "both",
                    "in",
                    "terms",
                    "of",
                    "available",
                    "capacities",
                    "and",
                    "resources",
                    "and",
                    "in",
                    "terms",
                    "of",
                    "environmental",
                    "footprint."
                ]
            ],
            "context": [
                1,
                3
            ]
        },
        "input": "sent0: Recently, #TARGET_REF and #REF have nevertheless shown that the commonly adopted practices (the number of iterations, the choice of model layers) when fine-tuning Transformers-based langage models are inappropriate under resource constrained conditions and adversely affect the stability of models performances as overfitting, label noise memorization or catastrophic forgetting.\n sent1: Added to this, because the pretraining process is particularly constraining, various works have been oriented towards the research and training of efficient models, both in terms of available capacities and resources and in terms of environmental footprint.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "initialize",
                    "attribute",
                    "embeddings",
                    "with",
                    "Glove",
                    "#TARGET_REF",
                    "word",
                    "embeddings."
                ],
                [
                    "For",
                    "an",
                    "entity,",
                    "we",
                    "take",
                    "all",
                    "the",
                    "known",
                    "attributes",
                    "from",
                    "K",
                    "e",
                    "."
                ],
                [
                    "The",
                    "representation",
                    "of",
                    "each",
                    "entity",
                    "is",
                    "the",
                    "weighted",
                    "sum",
                    "of",
                    "the",
                    "known",
                    "attributes,",
                    "with",
                    "learned",
                    "attention",
                    "weights."
                ],
                [
                    "The",
                    "weights",
                    "are",
                    "shared",
                    "between",
                    "entities",
                    "and",
                    "attributes."
                ]
            ],
            "context": [
                2,
                0,
                0,
                0
            ]
        },
        "input": "sent0: We initialize attribute embeddings with Glove #TARGET_REF word embeddings.\n sent1: For an entity, we take all the known attributes from K e .\n sent2: The representation of each entity is the weighted sum of the known attributes, with learned attention weights.\n sent3: The weights are shared between entities and attributes.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Each",
                    "annotated",
                    "document",
                    "consists",
                    "of",
                    "a",
                    "title",
                    "and",
                    "3",
                    "paragraphs",
                    "of",
                    "text,",
                    "and",
                    "contains",
                    "a",
                    "list",
                    "of",
                    "non-pronominal",
                    "base-NPs",
                    "(most",
                    "identified",
                    "by",
                    "SpaCy",
                    "#TARGET_REF",
                    "9",
                    "but",
                    "some",
                    "added",
                    "manually",
                    "by",
                    "the",
                    "annotators),",
                    "a",
                    "list",
                    "of",
                    "coreference",
                    "clusters",
                    "over",
                    "the",
                    "NPs,",
                    "and",
                    "a",
                    "list",
                    "of",
                    "NP-relations",
                    "that",
                    "hold",
                    "in",
                    "the",
                    "text."
                ],
                [
                    "Each",
                    "relation",
                    "is",
                    "a",
                    "triplet",
                    "consisting",
                    "of",
                    "two",
                    "NPs",
                    "from",
                    "the",
                    "NP",
                    "list,",
                    "and",
                    "a",
                    "connecting",
                    "element",
                    "which",
                    "is",
                    "one",
                    "of",
                    "23",
                    "prepositions",
                    "(displayed",
                    "in",
                    "Table",
                    "1)",
                    "10",
                    "or",
                    "a",
                    "''member(s)",
                    "of''",
                    "relation",
                    "designating",
                    "set-membership."
                ],
                [
                    "The",
                    "list",
                    "of",
                    "NP",
                    "relations",
                    "is",
                    "exhaustive,",
                    "and",
                    "aims",
                    "to",
                    "cover",
                    "all",
                    "and",
                    "only",
                    "valid",
                    "NP-NP",
                    "relations",
                    "in",
                    "the",
                    "document."
                ]
            ],
            "context": [
                0,
                0,
                0
            ]
        },
        "input": "sent0: Each annotated document consists of a title and 3 paragraphs of text, and contains a list of non-pronominal base-NPs (most identified by SpaCy #TARGET_REF 9 but some added manually by the annotators), a list of coreference clusters over the NPs, and a list of NP-relations that hold in the text.\n sent1: Each relation is a triplet consisting of two NPs from the NP list, and a connecting element which is one of 23 prepositions (displayed in Table 1) 10 or a ''member(s) of'' relation designating set-membership.\n sent2: The list of NP relations is exhaustive, and aims to cover all and only valid NP-NP relations in the document.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Following",
                    "the",
                    "widely",
                    "adopted",
                    "fact",
                    "verification",
                    "pipeline",
                    "#TARGET_REF",
                    ",",
                    "we",
                    "take",
                    "three",
                    "steps",
                    "to",
                    "solve",
                    "the",
                    "FEVEROUS",
                    "task",
                    "(i)",
                    "retrieving",
                    "pages",
                    "from",
                    "the",
                    "Wikipedia",
                    "dump,",
                    "(ii)",
                    "extracting",
                    "evidence",
                    "from",
                    "the",
                    "retrieved",
                    "pages,",
                    "and",
                    "(iii)",
                    "verifying",
                    "the",
                    "claim",
                    "according",
                    "to",
                    "extracted",
                    "evidence."
                ]
            ],
            "context": [
                3
            ]
        },
        "input": "sent0: Following the widely adopted fact verification pipeline #TARGET_REF , we take three steps to solve the FEVEROUS task (i) retrieving pages from the Wikipedia dump, (ii) extracting evidence from the retrieved pages, and (iii) verifying the claim according to extracted evidence.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "All",
                    "of",
                    "the",
                    "models",
                    "in",
                    "the",
                    "pipeline",
                    "described",
                    "in",
                    "Section",
                    "2",
                    "are",
                    "trained",
                    "using",
                    "only",
                    "the",
                    "training",
                    "set",
                    "of",
                    "the",
                    "original",
                    "LIGHT",
                    "and",
                    "LIGHT-Quests",
                    "data."
                ],
                [
                    "LIGHT-Quests",
                    "inherits",
                    "characters,",
                    "locations,",
                    "and",
                    "objects",
                    "from",
                    "the",
                    "original",
                    "LIGHT",
                    "dataset",
                    "and",
                    "adds",
                    "on",
                    "motivations",
                    "and",
                    "goals",
                    "in",
                    "the",
                    "form",
                    "of",
                    "quests."
                ],
                [
                    "Thus,",
                    "the",
                    "character,",
                    "location,",
                    "and",
                    "object",
                    "retrieval",
                    "models",
                    "are",
                    "evaluated",
                    "on",
                    "the",
                    "LIGHT",
                    "unseen",
                    "test",
                    "set",
                    "and",
                    "the",
                    "motivation",
                    "and",
                    "goal",
                    "generation",
                    "models",
                    "are",
                    "evaluated",
                    "on",
                    "the",
                    "LIGHT-Quests",
                    "test",
                    "set."
                ],
                [
                    "We",
                    "report",
                    "the",
                    "standard",
                    "array",
                    "of",
                    "metrics:",
                    "hits@10",
                    "and",
                    "F1",
                    "ranking",
                    "prediction",
                    "score",
                    "for",
                    "retrieval",
                    "models,",
                    "and",
                    "F1",
                    "(as",
                    "a",
                    "harmonic",
                    "average",
                    "of",
                    "#REF",
                    "and",
                    "ROUGE-1",
                    "#TARGET_REF",
                    ")",
                    "and",
                    "perplexity",
                    "for",
                    "generative",
                    "models."
                ],
                [
                    "Hyperparameters",
                    "for",
                    "all",
                    "models",
                    "are",
                    "found",
                    "in",
                    "Appendix",
                    "A.6."
                ],
                [
                    "Analysis."
                ],
                [
                    "Table",
                    "1",
                    "presents",
                    "the",
                    "results",
                    "of",
                    "this",
                    "evaluation."
                ],
                [
                    "There",
                    "are",
                    "two",
                    "primary",
                    "trends",
                    "to",
                    "note:",
                    "(1)",
                    "character",
                    "retrieval",
                    "is",
                    "easier",
                    "than",
                    "retrieving",
                    "location",
                    "and",
                    "objects,",
                    "and",
                    "(2)",
                    "goal",
                    "action",
                    "generation",
                    "is",
                    "easier",
                    "than",
                    "motivation",
                    "generation."
                ],
                [
                    "We",
                    "hypothesize",
                    "that",
                    "the",
                    "first",
                    "trend",
                    "is",
                    "a",
                    "direct",
                    "consequence",
                    "of",
                    "the",
                    "fact",
                    "that",
                    "generated",
                    "motivations",
                    "and",
                    "goals",
                    "regularly",
                    "contain",
                    "the",
                    "names",
                    "of",
                    "the",
                    "characters",
                    "involved",
                    "but",
                    "mostly",
                    "leave",
                    "implicit",
                    "information",
                    "such",
                    "as",
                    "the",
                    "objects",
                    "required-e.g."
                ],
                [
                    "the",
                    "action",
                    "hit",
                    "dragon",
                    "as",
                    "a",
                    "knight",
                    "would",
                    "require",
                    "a",
                    "weapon",
                    "such",
                    "as",
                    "a",
                    "sword",
                    "to",
                    "be",
                    "equipped",
                    "first."
                ],
                [
                    "The",
                    "second",
                    "trend",
                    "stems",
                    "from",
                    "the",
                    "fact",
                    "that",
                    "goal",
                    "actions",
                    "can",
                    "often",
                    "be",
                    "thought",
                    "of",
                    "as",
                    "condensed",
                    "version",
                    "of",
                    "the",
                    "short",
                    "motivation-number",
                    "of",
                    "tokens",
                    "required",
                    "to",
                    "generate",
                    "goal",
                    "actions",
                    "is",
                    "far",
                    "less",
                    "than",
                    "short",
                    "motivations."
                ],
                [
                    "This",
                    "implies",
                    "that",
                    "the",
                    "goal",
                    "action",
                    "model",
                    "is",
                    "akin",
                    "to",
                    "a",
                    "summarization",
                    "model",
                    "as",
                    "opposed",
                    "to",
                    "the",
                    "short",
                    "motivation",
                    "model",
                    "which",
                    "has",
                    "the",
                    "more",
                    "difficult",
                    "task",
                    "of",
                    "generating",
                    "the",
                    "motivation",
                    "with",
                    "only",
                    "initial",
                    "persona",
                    "and",
                    "location",
                    "information."
                ]
            ],
            "context": [
                0,
                0,
                0,
                2,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: All of the models in the pipeline described in Section 2 are trained using only the training set of the original LIGHT and LIGHT-Quests data.\n sent1: LIGHT-Quests inherits characters, locations, and objects from the original LIGHT dataset and adds on motivations and goals in the form of quests.\n sent2: Thus, the character, location, and object retrieval models are evaluated on the LIGHT unseen test set and the motivation and goal generation models are evaluated on the LIGHT-Quests test set.\n sent3: We report the standard array of metrics: hits@10 and F1 ranking prediction score for retrieval models, and F1 (as a harmonic average of #REF and ROUGE-1 #TARGET_REF ) and perplexity for generative models.\n sent4: Hyperparameters for all models are found in Appendix A.6.\n sent5: Analysis.\n sent6: Table 1 presents the results of this evaluation.\n sent7: There are two primary trends to note: (1) character retrieval is easier than retrieving location and objects, and (2) goal action generation is easier than motivation generation.\n sent8: We hypothesize that the first trend is a direct consequence of the fact that generated motivations and goals regularly contain the names of the characters involved but mostly leave implicit information such as the objects required-e.g.\n sent9: the action hit dragon as a knight would require a weapon such as a sword to be equipped first.\n sent10: The second trend stems from the fact that goal actions can often be thought of as condensed version of the short motivation-number of tokens required to generate goal actions is far less than short motivations.\n sent11: This implies that the goal action model is akin to a summarization model as opposed to the short motivation model which has the more difficult task of generating the motivation with only initial persona and location information.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent3\"], \"BACKGROUND\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "•",
                    "Southern",
                    "Quechua:",
                    "with",
                    "6+",
                    "millions",
                    "of",
                    "speakers",
                    "and",
                    "several",
                    "variants,",
                    "it",
                    "is",
                    "the",
                    "most",
                    "widespread",
                    "indigenous",
                    "language",
                    "in",
                    "Peru."
                ],
                [
                    "AmericasNLP",
                    "provides",
                    "evaluation",
                    "sets",
                    "in",
                    "the",
                    "standard",
                    "Southern",
                    "Quechua,",
                    "which",
                    "is",
                    "based",
                    "mostly",
                    "on",
                    "the",
                    "Quechua",
                    "Ayacucho",
                    "(quy)",
                    "variant."
                ],
                [
                    "There",
                    "is",
                    "parallel",
                    "data",
                    "from",
                    "dictionaries",
                    "and",
                    "Jehovah",
                    "Witnesses",
                    "#REF",
                    "."
                ],
                [
                    "There",
                    "is",
                    "parallel",
                    "corpus",
                    "aligned",
                    "with",
                    "English",
                    "too."
                ],
                [
                    "We",
                    "also",
                    "include",
                    "the",
                    "close",
                    "variant",
                    "of",
                    "Quechua",
                    "Cusco",
                    "(quz)",
                    "to",
                    "support",
                    "the",
                    "multilingual",
                    "learning."
                ],
                [
                    "•",
                    "Aymara",
                    "(aym):",
                    "with",
                    "1.7",
                    "million",
                    "of",
                    "speakers",
                    "(mostly",
                    "in",
                    "Bolivia)."
                ],
                [
                    "The",
                    "parallel",
                    "and",
                    "monolingual",
                    "data",
                    "is",
                    "extracted",
                    "from",
                    "a",
                    "news",
                    "website",
                    "(Global",
                    "Voices)",
                    "and",
                    "distributed",
                    "by",
                    "OPUS",
                    "#REF",
                    "."
                ],
                [
                    "There",
                    "are",
                    "aligned",
                    "data",
                    "with",
                    "English",
                    "too."
                ],
                [
                    "•",
                    "Shipibo-Konibo",
                    "(shp):",
                    "a",
                    "Panoan",
                    "language",
                    "with",
                    "almost",
                    "30,000",
                    "speakers",
                    "in",
                    "the",
                    "Amazonian",
                    "region."
                ],
                [
                    "There",
                    "are",
                    "parallel",
                    "data",
                    "from",
                    "dictionaries,",
                    "educational",
                    "material",
                    "#REF",
                    ",",
                    "language",
                    "learning",
                    "flashcards",
                    "(Gómez",
                    "#REF",
                    ",",
                    "plus",
                    "monolingual",
                    "data",
                    "from",
                    "educational",
                    "books",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "•",
                    "Ashaninka",
                    "(cni):",
                    "an",
                    "Arawakan",
                    "language",
                    "with",
                    "45,000",
                    "speakers",
                    "in",
                    "the",
                    "Amazon."
                ],
                [
                    "There",
                    "is",
                    "parallel",
                    "data",
                    "from",
                    "dictionaries,",
                    "laws",
                    "and",
                    "books",
                    "#REF",
                    ",",
                    "plus",
                    "monolingual",
                    "corpus",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                0,
                0
            ]
        },
        "input": "sent0: • Southern Quechua: with 6+ millions of speakers and several variants, it is the most widespread indigenous language in Peru.\n sent1: AmericasNLP provides evaluation sets in the standard Southern Quechua, which is based mostly on the Quechua Ayacucho (quy) variant.\n sent2: There is parallel data from dictionaries and Jehovah Witnesses #REF .\n sent3: There is parallel corpus aligned with English too.\n sent4: We also include the close variant of Quechua Cusco (quz) to support the multilingual learning.\n sent5: • Aymara (aym): with 1.7 million of speakers (mostly in Bolivia).\n sent6: The parallel and monolingual data is extracted from a news website (Global Voices) and distributed by OPUS #REF .\n sent7: There are aligned data with English too.\n sent8: • Shipibo-Konibo (shp): a Panoan language with almost 30,000 speakers in the Amazonian region.\n sent9: There are parallel data from dictionaries, educational material #REF , language learning flashcards (Gómez #REF , plus monolingual data from educational books #TARGET_REF .\n sent10: • Ashaninka (cni): an Arawakan language with 45,000 speakers in the Amazon.\n sent11: There is parallel data from dictionaries, laws and books #REF , plus monolingual corpus #REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent9\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "use",
                    "MultiWOZ",
                    "#REF",
                    ",",
                    "a",
                    "multi-domain",
                    "human-human",
                    "conversational",
                    "dataset",
                    "in",
                    "our",
                    "experiments."
                ],
                [
                    "It",
                    "contains",
                    "in",
                    "total",
                    "8438",
                    "dialogues",
                    "spanning",
                    "over",
                    "seven",
                    "domains,",
                    "and",
                    "each",
                    "dialogue",
                    "has",
                    "13.7",
                    "turns",
                    "on",
                    "average."
                ],
                [
                    "We",
                    "use",
                    "the",
                    "separation",
                    "of",
                    "training,",
                    "validation",
                    "and",
                    "testing",
                    "data",
                    "as",
                    "original",
                    "MultiWOZ",
                    "dataset."
                ],
                [
                    "We",
                    "use",
                    "the",
                    "evaluation",
                    "metrics",
                    "as",
                    "#REF",
                    "to",
                    "measure",
                    "dialogue",
                    "task",
                    "completion,",
                    "which",
                    "are",
                    "how",
                    "often",
                    "the",
                    "system",
                    "provides",
                    "a",
                    "correct",
                    "entity",
                    "(Inform)",
                    "and",
                    "answers",
                    "all",
                    "the",
                    "requested",
                    "information",
                    "(Success)."
                ],
                [
                    "We",
                    "use",
                    "BLEU",
                    "#TARGET_REF",
                    "to",
                    "measure",
                    "the",
                    "language",
                    "quality",
                    "of",
                    "generated",
                    "responses."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                2
            ]
        },
        "input": "sent0: We use MultiWOZ #REF , a multi-domain human-human conversational dataset in our experiments.\n sent1: It contains in total 8438 dialogues spanning over seven domains, and each dialogue has 13.7 turns on average.\n sent2: We use the separation of training, validation and testing data as original MultiWOZ dataset.\n sent3: We use the evaluation metrics as #REF to measure dialogue task completion, which are how often the system provides a correct entity (Inform) and answers all the requested information (Success).\n sent4: We use BLEU #TARGET_REF to measure the language quality of generated responses.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent4\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Kinyarwanda",
                    "#TARGET_REF",
                    "data",
                    "is",
                    "used",
                    "in",
                    "our",
                    "experiments",
                    "as",
                    "a",
                    "language",
                    "related",
                    "to",
                    "the",
                    "target",
                    "language",
                    "(swh)",
                    "with",
                    "existing",
                    "text",
                    "and",
                    "audio",
                    "resources",
                    "that,",
                    "in",
                    "some",
                    "ways,",
                    "surpasses",
                    "those",
                    "available",
                    "in",
                    "the",
                    "target",
                    "language."
                ],
                [
                    "Thus,",
                    "we",
                    "pre-train",
                    "some",
                    "models",
                    "on",
                    "kin",
                    "data",
                    "while",
                    "fine-tuning",
                    "for",
                    "the",
                    "downstream",
                    "NER",
                    "task",
                    "using",
                    "swh",
                    "data."
                ]
            ],
            "context": [
                2,
                2
            ]
        },
        "input": "sent0: Kinyarwanda #TARGET_REF data is used in our experiments as a language related to the target language (swh) with existing text and audio resources that, in some ways, surpasses those available in the target language.\n sent1: Thus, we pre-train some models on kin data while fine-tuning for the downstream NER task using swh data.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\", \"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Text-based",
                    "Game",
                    "Playing",
                    "and",
                    "Generation."
                ],
                [
                    "Recent",
                    "text",
                    "game",
                    "playing",
                    "works",
                    "have",
                    "focused",
                    "on",
                    "tackling",
                    "three",
                    "primary",
                    "challenges:",
                    "(1)",
                    "how",
                    "to",
                    "represent",
                    "agent",
                    "knowledge",
                    "to",
                    "effectively",
                    "operate",
                    "in",
                    "partially",
                    "observable",
                    "environments",
                    "#REF",
                    ",",
                    "(2)",
                    "scaling",
                    "RL",
                    "algorithms",
                    "to",
                    "handle",
                    "combinatorial",
                    "natural",
                    "language",
                    "state-action",
                    "spaces",
                    "#TARGET_REF",
                    ",",
                    "and",
                    "(3)",
                    "giving",
                    "agents",
                    "commonsense",
                    "priors",
                    "to",
                    "better",
                    "reason",
                    "about",
                    "the",
                    "world",
                    "#REF",
                    "On",
                    "the",
                    "flip",
                    "side,",
                    "we",
                    "have",
                    "procedural",
                    "generation",
                    "of",
                    "games",
                    "with",
                    "works",
                    "such",
                    "as",
                    "Short",
                    "and",
                    "Adams",
                    "(2017),",
                    "Risi",
                    "and",
                    "Togelius",
                    "(2019),",
                    "Khalifa",
                    "et",
                    "al."
                ]
            ],
            "context": [
                0,
                3
            ]
        },
        "input": "sent0: Text-based Game Playing and Generation.\n sent1: Recent text game playing works have focused on tackling three primary challenges: (1) how to represent agent knowledge to effectively operate in partially observable environments #REF , (2) scaling RL algorithms to handle combinatorial natural language state-action spaces #TARGET_REF , and (3) giving agents commonsense priors to better reason about the world #REF On the flip side, we have procedural generation of games with works such as Short and Adams (2017), Risi and Togelius (2019), Khalifa et al.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Fact",
                    "Verification",
                    "over",
                    "Structured",
                    "and",
                    "Unstructured",
                    "Evidence",
                    "FEVEROUS",
                    "#TARGET_REF",
                    "is",
                    "the",
                    "first",
                    "dataset",
                    "of",
                    "fact",
                    "verification",
                    "on",
                    "structured",
                    "and",
                    "unstructured",
                    "evidence."
                ],
                [
                    "Many",
                    "previous",
                    "works",
                    "follow",
                    "the",
                    "baseline",
                    "settings",
                    "and",
                    "convert",
                    "all",
                    "evidence",
                    "to",
                    "text",
                    "format",
                    "to",
                    "perform",
                    "evidence",
                    "interaction."
                ],
                [
                    "They",
                    "transform",
                    "each",
                    "cell",
                    "to",
                    "header-value",
                    "pairs",
                    "#REF",
                    "or",
                    "in",
                    "a",
                    "cell",
                    "location",
                    "indication",
                    "type",
                    "#REF",
                    "."
                ],
                [
                    "They",
                    "pay",
                    "less",
                    "attention",
                    "to",
                    "making",
                    "the",
                    "converted",
                    "text",
                    "more",
                    "consistent",
                    "with",
                    "natural",
                    "language",
                    "expressions",
                    "or",
                    "identifying",
                    "what",
                    "the",
                    "context",
                    "cells",
                    "represent."
                ],
                [
                    "#REF",
                    "propose",
                    "to",
                    "convert",
                    "all",
                    "evidence",
                    "to",
                    "tables."
                ],
                [
                    "They",
                    "simply",
                    "convert",
                    "each",
                    "sentence",
                    "to",
                    "a",
                    "2-cell",
                    "table",
                    "with",
                    "the",
                    "Wikipedia",
                    "title",
                    "and",
                    "itself",
                    "instead",
                    "of",
                    "packing",
                    "closely-tied",
                    "evidence",
                    "and",
                    "building",
                    "a",
                    "global",
                    "evidence",
                    "table."
                ],
                [
                    "There",
                    "are",
                    "also",
                    "works",
                    "focusing",
                    "on",
                    "the",
                    "first",
                    "two",
                    "steps",
                    "two",
                    "improve",
                    "the",
                    "final",
                    "results."
                ],
                [
                    "#REF",
                    "propose",
                    "to",
                    "add",
                    "a",
                    "document",
                    "re-ranker",
                    "to",
                    "strengthen",
                    "the",
                    "document",
                    "retrieval."
                ],
                [
                    "Multi-hop",
                    "Dense",
                    "Retriever",
                    "#REF",
                    "and",
                    "T5",
                    "generator",
                    "#REF",
                    "are",
                    "introduced",
                    "to",
                    "better",
                    "extract",
                    "multi-hop",
                    "evidence."
                ]
            ],
            "context": [
                2,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Fact Verification over Structured and Unstructured Evidence FEVEROUS #TARGET_REF is the first dataset of fact verification on structured and unstructured evidence.\n sent1: Many previous works follow the baseline settings and convert all evidence to text format to perform evidence interaction.\n sent2: They transform each cell to header-value pairs #REF or in a cell location indication type #REF .\n sent3: They pay less attention to making the converted text more consistent with natural language expressions or identifying what the context cells represent.\n sent4: #REF propose to convert all evidence to tables.\n sent5: They simply convert each sentence to a 2-cell table with the Wikipedia title and itself instead of packing closely-tied evidence and building a global evidence table.\n sent6: There are also works focusing on the first two steps two improve the final results.\n sent7: #REF propose to add a document re-ranker to strengthen the document retrieval.\n sent8: Multi-hop Dense Retriever #REF and T5 generator #REF are introduced to better extract multi-hop evidence.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "train",
                    "data",
                    "have",
                    "been",
                    "extracted",
                    "from",
                    "different",
                    "domains",
                    "and",
                    "sources,",
                    "which",
                    "are",
                    "not",
                    "necessarily",
                    "the",
                    "same",
                    "as",
                    "the",
                    "evaluation",
                    "sets",
                    "provided",
                    "for",
                    "the",
                    "Shared",
                    "Task."
                ],
                [
                    "Therefore,",
                    "the",
                    "official",
                    "development",
                    "set",
                    "(995",
                    "sentences",
                    "per",
                    "language)",
                    "is",
                    "split",
                    "into",
                    "three",
                    "parts:",
                    "25%-25%-50%."
                ],
                [
                    "The",
                    "first",
                    "two",
                    "parts",
                    "are",
                    "our",
                    "custom",
                    "dev",
                    "and",
                    "devtest",
                    "sets",
                    "3",
                    "."
                ],
                [
                    "We",
                    "add",
                    "the",
                    "50%",
                    "section",
                    "to",
                    "the",
                    "training",
                    "set",
                    "with",
                    "a",
                    "sampling",
                    "distribution",
                    "of",
                    "20%,",
                    "to",
                    "reduce",
                    "the",
                    "domain",
                    "gap",
                    "in",
                    "the",
                    "training",
                    "data."
                ],
                [
                    "Likewise,",
                    "we",
                    "extract",
                    "a",
                    "sample",
                    "of",
                    "the",
                    "training",
                    "and",
                    "double",
                    "the",
                    "size",
                    "of",
                    "the",
                    "development",
                    "set."
                ],
                [
                    "The",
                    "mixed",
                    "data",
                    "in",
                    "the",
                    "validation",
                    "set",
                    "is",
                    "relevant,",
                    "as",
                    "it",
                    "allows",
                    "to",
                    "evaluate",
                    "how",
                    "the",
                    "model",
                    "fits",
                    "with",
                    "all",
                    "the",
                    "domains."
                ],
                [
                    "We",
                    "used",
                    "the",
                    "same",
                    "multi-text",
                    "sentences",
                    "for",
                    "evaluation,",
                    "and",
                    "avoid",
                    "any",
                    "overlapping",
                    "of",
                    "the",
                    "Spanish",
                    "side",
                    "with",
                    "the",
                    "training",
                    "set,",
                    "this",
                    "is",
                    "also",
                    "important",
                    "as",
                    "we",
                    "are",
                    "going",
                    "to",
                    "evaluate",
                    "multilingual",
                    "models."
                ],
                [
                    "Evaluation",
                    "for",
                    "all",
                    "the",
                    "models",
                    "used",
                    "BLEU",
                    "#TARGET_REF",
                    "and",
                    "chrF",
                    "#REF",
                    "metrics."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2
            ]
        },
        "input": "sent0: The train data have been extracted from different domains and sources, which are not necessarily the same as the evaluation sets provided for the Shared Task.\n sent1: Therefore, the official development set (995 sentences per language) is split into three parts: 25%-25%-50%.\n sent2: The first two parts are our custom dev and devtest sets 3 .\n sent3: We add the 50% section to the training set with a sampling distribution of 20%, to reduce the domain gap in the training data.\n sent4: Likewise, we extract a sample of the training and double the size of the development set.\n sent5: The mixed data in the validation set is relevant, as it allows to evaluate how the model fits with all the domains.\n sent6: We used the same multi-text sentences for evaluation, and avoid any overlapping of the Spanish side with the training set, this is also important as we are going to evaluate multilingual models.\n sent7: Evaluation for all the models used BLEU #TARGET_REF and chrF #REF metrics.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent7\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Long",
                    "Range",
                    "Arena",
                    "#REF",
                    ")",
                    "is",
                    "a",
                    "recently",
                    "established",
                    "benchmark",
                    "for",
                    "evaluating",
                    "the",
                    "ability",
                    "of",
                    "Transformer",
                    "variants",
                    "to",
                    "handle",
                    "long",
                    "sequences."
                ],
                [
                    "It",
                    "comprises",
                    "of",
                    "multiple",
                    "text",
                    "classification",
                    "tasks",
                    "with",
                    "inputs",
                    "containing",
                    "thousands",
                    "of",
                    "tokens",
                    "(Table",
                    "1)."
                ],
                [
                    "In",
                    "ListOps",
                    "#REF",
                    ",",
                    "given",
                    "a",
                    "sequence",
                    "of",
                    "operations",
                    "on",
                    "single-digit",
                    "integers,",
                    "the",
                    "model",
                    "predicts",
                    "a",
                    "single-digit",
                    "solution",
                    "modeled",
                    "as",
                    "10-way",
                    "classification."
                ],
                [
                    "IMDb",
                    "movie",
                    "reviews",
                    "#TARGET_REF",
                    "is",
                    "a",
                    "character-level",
                    "binary",
                    "sentiment",
                    "classification",
                    "task."
                ],
                [
                    "Lastly,",
                    "in",
                    "the",
                    "ACL",
                    "Anthology",
                    "Network",
                    "(AAN)",
                    "#REF",
                    "task,",
                    "a",
                    "character-level",
                    "model",
                    "classifies",
                    "if",
                    "there",
                    "is",
                    "a",
                    "citation",
                    "between",
                    "a",
                    "pair",
                    "of",
                    "papers."
                ]
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "sent0: Long Range Arena #REF ) is a recently established benchmark for evaluating the ability of Transformer variants to handle long sequences.\n sent1: It comprises of multiple text classification tasks with inputs containing thousands of tokens (Table 1).\n sent2: In ListOps #REF , given a sequence of operations on single-digit integers, the model predicts a single-digit solution modeled as 10-way classification.\n sent3: IMDb movie reviews #TARGET_REF is a character-level binary sentiment classification task.\n sent4: Lastly, in the ACL Anthology Network (AAN) #REF task, a character-level model classifies if there is a citation between a pair of papers.\n",
        "output": "{\"INFORMATION\": [\"sent3\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Corpus",
                    "diversity",
                    "(⇑)."
                ],
                [
                    "Distinct-k",
                    "#TARGET_REF",
                    "measures",
                    "the",
                    "total",
                    "number",
                    "of",
                    "unique",
                    "k-grams",
                    "normalized",
                    "by",
                    "the",
                    "total",
                    "number",
                    "of",
                    "generated",
                    "k-gram",
                    "tokens",
                    "to",
                    "avoid",
                    "favoring",
                    "long",
                    "sentences."
                ],
                [
                    "Entropyk",
                    "#REF",
                    "reflects",
                    "how",
                    "evenly",
                    "the",
                    "empirical",
                    "k-gram",
                    "distribution",
                    "is",
                    "for",
                    "a",
                    "given",
                    "sentence",
                    "when",
                    "word",
                    "frequency",
                    "is",
                    "considered."
                ],
                [
                    "Human",
                    "6.62",
                    "0.0",
                    "12.43",
                    "0.0",
                    "10.36",
                    "0.0",
                    "6.04",
                    "0.0",
                    "53.57",
                    "0.0",
                    "10.84",
                    "0.0",
                    "100.0",
                    "0.0",
                    "100.0",
                    "0.0",
                    "*",
                    "Metrics:",
                    "SB-3/4:",
                    "Self-BLEU-3/4",
                    "(⇓),",
                    "D-2:",
                    "Distinct-2",
                    "(⇑),",
                    "E-4:",
                    "Entropy-4",
                    "(⇑),",
                    "B-4:",
                    "BLEU-4",
                    "(⇑),",
                    "R-L:",
                    "ROUGE-L",
                    "(⇑)#Uni.C(⇑)",
                    "Jaccard",
                    "(⇓)",
                    "SB-3",
                    "(⇓)",
                    "SB-4",
                    "(⇓)",
                    "D-2(⇑)",
                    "E-4(⇑)",
                    "B-4",
                    "(⇑)",
                    "R-L",
                    "(⇑)",
                    "CVAE",
                    "z",
                    "="
                ]
            ],
            "context": [
                0,
                1,
                3,
                0
            ]
        },
        "input": "sent0: Corpus diversity (⇑).\n sent1: Distinct-k #TARGET_REF measures the total number of unique k-grams normalized by the total number of generated k-gram tokens to avoid favoring long sentences.\n sent2: Entropyk #REF reflects how evenly the empirical k-gram distribution is for a given sentence when word frequency is considered.\n sent3: Human 6.62 0.0 12.43 0.0 10.36 0.0 6.04 0.0 53.57 0.0 10.84 0.0 100.0 0.0 100.0 0.0 * Metrics: SB-3/4: Self-BLEU-3/4 (⇓), D-2: Distinct-2 (⇑), E-4: Entropy-4 (⇑), B-4: BLEU-4 (⇑), R-L: ROUGE-L (⇑)#Uni.C(⇑) Jaccard (⇓) SB-3 (⇓) SB-4 (⇓) D-2(⇑) E-4(⇑) B-4 (⇑) R-L (⇑) CVAE z =\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Although",
                    "not",
                    "a",
                    "formalist",
                    "herself,",
                    "and",
                    "considered",
                    "an",
                    "anti-formalist",
                    "by",
                    "many,",
                    "MMB",
                    "nevertheless",
                    "believed",
                    "passionately",
                    "in",
                    "the",
                    "applicability",
                    "of",
                    "mathematical",
                    "techniques",
                    "to",
                    "natural",
                    "language,",
                    "without",
                    "them,",
                    "she",
                    "believed,",
                    "there",
                    "would",
                    "be",
                    "nothing",
                    "worthy",
                    "of",
                    "the",
                    "name",
                    "of",
                    "theory."
                ],
                [
                    "Her",
                    "opposition",
                    "was",
                    "to",
                    "the",
                    "assumption",
                    "that",
                    "formal",
                    "logic,",
                    "in",
                    "particular,",
                    "applied",
                    "directly",
                    "to",
                    "natural",
                    "language,",
                    "and",
                    "she",
                    "would",
                    "not",
                    "concede",
                    "much",
                    "distinction",
                    "between",
                    "that",
                    "and",
                    "the",
                    "methods",
                    "of",
                    "#TARGET_REF",
                    ",",
                    "a",
                    "position",
                    "that",
                    "has",
                    "some",
                    "historical",
                    "justification."
                ]
            ],
            "context": [
                0,
                2
            ]
        },
        "input": "sent0: Although not a formalist herself, and considered an anti-formalist by many, MMB nevertheless believed passionately in the applicability of mathematical techniques to natural language, without them, she believed, there would be nothing worthy of the name of theory.\n sent1: Her opposition was to the assumption that formal logic, in particular, applied directly to natural language, and she would not concede much distinction between that and the methods of #TARGET_REF , a position that has some historical justification.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Data",
                    "I",
                    "use",
                    "Universal",
                    "Dependencies",
                    "(UD)",
                    "2.8",
                    "2",
                    "#TARGET_REF",
                    "and",
                    "the",
                    "automatically-parsed",
                    "Wikipedia",
                    "datasets",
                    "released",
                    "as",
                    "part",
                    "of",
                    "the",
                    "CoNLL",
                    "2017",
                    "Shared",
                    "Task",
                    "#REF",
                    "as",
                    "a",
                    "source",
                    "of",
                    "attributive",
                    "adjective-noun",
                    "pairs."
                ],
                [
                    "I",
                    "extract",
                    "all",
                    "pairs",
                    "of",
                    "words",
                    "linked",
                    "by",
                    "a",
                    "dependency",
                    "of",
                    "type",
                    "amod",
                    "where",
                    "the",
                    "head",
                    "has",
                    "universal",
                    "part-of-speech",
                    "(UPOS)",
                    "NOUN",
                    "and",
                    "the",
                    "dependent",
                    "has",
                    "UPOS",
                    "ADJ."
                ],
                [
                    "I",
                    "represent",
                    "the",
                    "pair",
                    "using",
                    "the",
                    "downcased",
                    "wordforms",
                    "of",
                    "the",
                    "adjective",
                    "and",
                    "noun."
                ]
            ],
            "context": [
                0,
                2,
                0
            ]
        },
        "input": "sent0: Data I use Universal Dependencies (UD) 2.8 2 #TARGET_REF and the automatically-parsed Wikipedia datasets released as part of the CoNLL 2017 Shared Task #REF as a source of attributive adjective-noun pairs.\n sent1: I extract all pairs of words linked by a dependency of type amod where the head has universal part-of-speech (UPOS) NOUN and the dependent has UPOS ADJ.\n sent2: I represent the pair using the downcased wordforms of the adjective and noun.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "MMB",
                    "must",
                    "be",
                    "credited",
                    "with",
                    "helping",
                    "to",
                    "keep",
                    "belief",
                    "in",
                    "MT",
                    "alive",
                    "during",
                    "long",
                    "years",
                    "of",
                    "public",
                    "scepticism,",
                    "and",
                    "above",
                    "all",
                    "with",
                    "the",
                    "belief",
                    "that",
                    "MT",
                    "was",
                    "an",
                    "intellectually",
                    "challenging",
                    "and",
                    "interesting",
                    "task",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "I",
                    "think",
                    "that",
                    "is",
                    "now",
                    "widely",
                    "granted,",
                    "although",
                    "it",
                    "was",
                    "not",
                    "conceded",
                    "within",
                    "artificial",
                    "intelligence,",
                    "for",
                    "example,",
                    "until",
                    "relatively",
                    "recently."
                ],
                [
                    "There",
                    "it",
                    "was",
                    "generally",
                    "thought",
                    "that,",
                    "although",
                    "language",
                    "understanding",
                    "in",
                    "general",
                    "required",
                    "inference",
                    "knowledge",
                    "of",
                    "the",
                    "world",
                    "and",
                    "processing",
                    "of",
                    "almost",
                    "arbitrary",
                    "complexity,",
                    "MT",
                    "did",
                    "not:",
                    "it",
                    "was",
                    "a",
                    "task",
                    "that",
                    "required",
                    "only",
                    "superficial",
                    "processing",
                    "of",
                    "language."
                ],
                [
                    "I",
                    "think",
                    "that",
                    "now",
                    "almost",
                    "everyone",
                    "concedes",
                    "that",
                    "that",
                    "view",
                    "is",
                    "false."
                ]
            ],
            "context": [
                2,
                0,
                0,
                0
            ]
        },
        "input": "sent0: MMB must be credited with helping to keep belief in MT alive during long years of public scepticism, and above all with the belief that MT was an intellectually challenging and interesting task #TARGET_REF .\n sent1: I think that is now widely granted, although it was not conceded within artificial intelligence, for example, until relatively recently.\n sent2: There it was generally thought that, although language understanding in general required inference knowledge of the world and processing of almost arbitrary complexity, MT did not: it was a task that required only superficial processing of language.\n sent3: I think that now almost everyone concedes that that view is false.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Genuine",
                    "tokens",
                    "are",
                    "tokens",
                    "that",
                    "causally",
                    "affect",
                    "a",
                    "task's",
                    "label",
                    "#REF",
                    ",",
                    "and",
                    "thus",
                    "the",
                    "correlations",
                    "between",
                    "those",
                    "tokens",
                    "and",
                    "the",
                    "labels",
                    "are",
                    "what",
                    "we",
                    "expect",
                    "the",
                    "model",
                    "to",
                    "capture",
                    "and",
                    "to",
                    "more",
                    "heavily",
                    "rely",
                    "on."
                ],
                [
                    "On",
                    "the",
                    "other",
                    "hand,",
                    "spurious",
                    "tokens,",
                    "or",
                    "shortcuts",
                    "as",
                    "commonly",
                    "denoted",
                    "in",
                    "prior",
                    "work",
                    "#REF",
                    ",",
                    "are",
                    "features",
                    "that",
                    "correlate",
                    "with",
                    "task",
                    "labels",
                    "but",
                    "are",
                    "not",
                    "genuine,",
                    "and",
                    "thus",
                    "might",
                    "fail",
                    "to",
                    "transfer",
                    "to",
                    "challenging",
                    "test",
                    "conditions",
                    "#REF",
                    "or",
                    "out-of-distribution",
                    "data,",
                    "spurious",
                    "tokens",
                    "do",
                    "not",
                    "causally",
                    "affect",
                    "task",
                    "labels",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                3,
                0
            ]
        },
        "input": "sent0: Genuine tokens are tokens that causally affect a task's label #REF , and thus the correlations between those tokens and the labels are what we expect the model to capture and to more heavily rely on.\n sent1: On the other hand, spurious tokens, or shortcuts as commonly denoted in prior work #REF , are features that correlate with task labels but are not genuine, and thus might fail to transfer to challenging test conditions #REF or out-of-distribution data, spurious tokens do not causally affect task labels #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Sentences",
                    "are",
                    "represented",
                    "with",
                    "their",
                    "shortest",
                    "dependency",
                    "path",
                    "between",
                    "two",
                    "candidates,",
                    "as",
                    "proposed",
                    "in",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "The",
                    "bottom",
                    "of",
                    "Figure",
                    "1",
                    "shows",
                    "an",
                    "example",
                    "of",
                    "the",
                    "shortest",
                    "path",
                    "between",
                    "an",
                    "entity",
                    "and",
                    "attribute",
                    "for",
                    "one",
                    "sentence."
                ],
                [
                    "We",
                    "converts",
                    "each",
                    "sentence",
                    "from",
                    "a",
                    "string",
                    "to",
                    "a",
                    "list",
                    "of",
                    "terms,",
                    "where",
                    "the",
                    "first",
                    "and",
                    "last",
                    "term",
                    "is",
                    "either",
                    "the",
                    "entity",
                    "or",
                    "the",
                    "attribute."
                ],
                [
                    "Each",
                    "term",
                    "in",
                    "the",
                    "dependency",
                    "path",
                    "is",
                    "represented",
                    "by",
                    "the",
                    "lemma",
                    "of",
                    "the",
                    "term,",
                    "the",
                    "part-of-speech",
                    "tag,",
                    "the",
                    "dependency",
                    "label,",
                    "the",
                    "direction",
                    "of",
                    "the",
                    "dependency",
                    "path",
                    "to",
                    "the",
                    "parent",
                    "(left,",
                    "right",
                    "or",
                    "root)."
                ],
                [
                    "Each",
                    "of",
                    "these",
                    "features",
                    "is",
                    "embedded",
                    "and",
                    "concatenated",
                    "to",
                    "produce",
                    "a",
                    "sequence",
                    "of",
                    "vectors",
                    "that",
                    "represents",
                    "the",
                    "dependency",
                    "path."
                ],
                [
                    "The",
                    "concatenation",
                    "is",
                    "the",
                    "edge",
                    "representation−",
                    "→",
                    "v",
                    "edge",
                    "=",
                    "[",
                    "−",
                    "→",
                    "v",
                    "lemma",
                    ",",
                    "−",
                    "→",
                    "v",
                    "pos",
                    ",",
                    "−",
                    "→",
                    "v",
                    "dep",
                    ",",
                    "−",
                    "→",
                    "v",
                    "dir",
                    "]The",
                    "sequence",
                    "of",
                    "terms",
                    "in",
                    "each",
                    "path",
                    "is",
                    "input",
                    "into",
                    "an",
                    "LSTM",
                    "to",
                    "produce",
                    "a",
                    "single",
                    "vector",
                    "representation",
                    "for",
                    "the",
                    "sentence,",
                    "−",
                    "→",
                    "v",
                    "s",
                    "."
                ],
                [
                    "This",
                    "is",
                    "repeated",
                    "for",
                    "each",
                    "sentence",
                    "producing",
                    "one",
                    "vector",
                    "per",
                    "sentence."
                ],
                [
                    "The",
                    "sentences",
                    "are",
                    "aggregated",
                    "with",
                    "a",
                    "weighted",
                    "mean",
                    "of",
                    "the",
                    "sentence",
                    "representations",
                    "to",
                    "form",
                    "a",
                    "representation",
                    "of",
                    "the",
                    "multiset",
                    "of",
                    "sentences,",
                    "−",
                    "→",
                    "v",
                    "sents(e,a)",
                    "."
                ]
            ],
            "context": [
                1,
                3,
                2,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Sentences are represented with their shortest dependency path between two candidates, as proposed in #TARGET_REF .\n sent1: The bottom of Figure 1 shows an example of the shortest path between an entity and attribute for one sentence.\n sent2: We converts each sentence from a string to a list of terms, where the first and last term is either the entity or the attribute.\n sent3: Each term in the dependency path is represented by the lemma of the term, the part-of-speech tag, the dependency label, the direction of the dependency path to the parent (left, right or root).\n sent4: Each of these features is embedded and concatenated to produce a sequence of vectors that represents the dependency path.\n sent5: The concatenation is the edge representation− → v edge = [ − → v lemma , − → v pos , − → v dep , − → v dir ]The sequence of terms in each path is input into an LSTM to produce a single vector representation for the sentence, − → v s .\n sent6: This is repeated for each sentence producing one vector per sentence.\n sent7: The sentences are aggregated with a weighted mean of the sentence representations to form a representation of the multiset of sentences, − → v sents(e,a) .\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Robustness",
                    "and",
                    "Bias",
                    "An",
                    "increasing",
                    "body",
                    "of",
                    "work",
                    "has",
                    "been",
                    "conducted",
                    "on",
                    "understanding",
                    "robustness",
                    "in",
                    "deep",
                    "neural",
                    "networks,",
                    "particularly,",
                    "how",
                    "models",
                    "sometimes",
                    "might",
                    "exploit",
                    "spurious",
                    "correlations",
                    "#REF",
                    "and",
                    "take",
                    "shortcuts",
                    "#TARGET_REF",
                    "Figure",
                    "2:",
                    "Our",
                    "proposed",
                    "pipeline",
                    "to",
                    "identify",
                    "spurious",
                    "correlations",
                    "at",
                    "scale."
                ],
                [
                    "In",
                    "the",
                    "first",
                    "step,",
                    "we",
                    "extract",
                    "important",
                    "tokens",
                    "from",
                    "input",
                    "text."
                ],
                [
                    "In",
                    "the",
                    "second",
                    "step,",
                    "we",
                    "analyze",
                    "extracted",
                    "tokens",
                    "from",
                    "various",
                    "datasets",
                    "to",
                    "identify",
                    "likely",
                    "\"spurious\"",
                    "tokens."
                ],
                [
                    "Finally,",
                    "we",
                    "further",
                    "validate",
                    "the",
                    "output",
                    "from",
                    "the",
                    "second",
                    "step",
                    "through",
                    "knowledge-aware",
                    "perturbation."
                ]
            ],
            "context": [
                2,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Robustness and Bias An increasing body of work has been conducted on understanding robustness in deep neural networks, particularly, how models sometimes might exploit spurious correlations #REF and take shortcuts #TARGET_REF Figure 2: Our proposed pipeline to identify spurious correlations at scale.\n sent1: In the first step, we extract important tokens from input text.\n sent2: In the second step, we analyze extracted tokens from various datasets to identify likely \"spurious\" tokens.\n sent3: Finally, we further validate the output from the second step through knowledge-aware perturbation.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "this",
                    "paper",
                    "we",
                    "have",
                    "presented",
                    "a",
                    "more",
                    "fine-grained",
                    "analysis",
                    "of",
                    "the",
                    "Levin",
                    "classes",
                    "which",
                    "highlights",
                    "the",
                    "semantic",
                    "components",
                    "entailed",
                    "by",
                    "certain",
                    "syntactic",
                    "frames,",
                    "and",
                    "hence",
                    "the",
                    "semantic",
                    "components",
                    "of",
                    "entire",
                    "classes",
                    "of",
                    "verbs."
                ],
                [
                    "We",
                    "hypothesize",
                    "that",
                    "the",
                    "semantic",
                    "components",
                    "we",
                    "are",
                    "identifying",
                    "will",
                    "be",
                    "useful",
                    "for",
                    "cross-linguistic",
                    "generalizations."
                ],
                [
                    "An",
                    "important",
                    "avenue",
                    "of",
                    "future",
                    "research",
                    "which",
                    "we",
                    "intend",
                    "to",
                    "explore",
                    "is",
                    "the",
                    "comparison",
                    "of",
                    "the",
                    "translations",
                    "of",
                    "these",
                    "classes",
                    "to",
                    "independently-defined",
                    "classes",
                    "in",
                    "other",
                    "languages,",
                    "such",
                    "as",
                    "French",
                    "verb",
                    "classes",
                    "#TARGET_REF",
                    "or",
                    "European",
                    "WordNet."
                ],
                [
                    "3",
                    "These",
                    "cross-linguistic",
                    "generalizations",
                    "will",
                    "be",
                    "equally",
                    "valuable",
                    "for",
                    "both",
                    "transfer-based",
                    "and",
                    "interlinguabased",
                    "approaches",
                    "to",
                    "machine",
                    "translation."
                ],
                [
                    "Presumably",
                    "both",
                    "approaches",
                    "need",
                    "to",
                    "be",
                    "augmented",
                    "with",
                    "pragmatic",
                    "information",
                    "about",
                    "tense",
                    "and",
                    "aspect",
                    "and",
                    "information",
                    "structure,",
                    "in",
                    "particular",
                    "coreference,",
                    "in",
                    "order",
                    "to",
                    "provide",
                    "an",
                    "adequate",
                    "basis",
                    "for",
                    "translation",
                    "in",
                    "many",
                    "circumstances."
                ],
                [
                    "It",
                    "could",
                    "be",
                    "argued",
                    "that",
                    "a",
                    "language-specific",
                    "predicate-argument",
                    "structure",
                    "will",
                    "lend",
                    "itself",
                    "more",
                    "readily",
                    "to",
                    "language-specific",
                    "pragmatic",
                    "annotation",
                    "than",
                    "a",
                    "language-independent",
                    "one,",
                    "but",
                    "it",
                    "would",
                    "still",
                    "be",
                    "necessary",
                    "to",
                    "ensure",
                    "that",
                    "the",
                    "pragmatic",
                    "annotation",
                    "was",
                    "meaningful",
                    "in",
                    "the",
                    "target",
                    "languages",
                    "as",
                    "well,",
                    "i.e.,",
                    "cross-linguistic."
                ],
                [
                    "The",
                    "discovery",
                    "of",
                    "cross-linguistic",
                    "pragmatic",
                    "features",
                    "is",
                    "an",
                    "equally",
                    "important",
                    "area",
                    "for",
                    "future",
                    "research."
                ]
            ],
            "context": [
                0,
                0,
                2,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: In this paper we have presented a more fine-grained analysis of the Levin classes which highlights the semantic components entailed by certain syntactic frames, and hence the semantic components of entire classes of verbs.\n sent1: We hypothesize that the semantic components we are identifying will be useful for cross-linguistic generalizations.\n sent2: An important avenue of future research which we intend to explore is the comparison of the translations of these classes to independently-defined classes in other languages, such as French verb classes #TARGET_REF or European WordNet.\n sent3: 3 These cross-linguistic generalizations will be equally valuable for both transfer-based and interlinguabased approaches to machine translation.\n sent4: Presumably both approaches need to be augmented with pragmatic information about tense and aspect and information structure, in particular coreference, in order to provide an adequate basis for translation in many circumstances.\n sent5: It could be argued that a language-specific predicate-argument structure will lend itself more readily to language-specific pragmatic annotation than a language-independent one, but it would still be necessary to ensure that the pragmatic annotation was meaningful in the target languages as well, i.e., cross-linguistic.\n sent6: The discovery of cross-linguistic pragmatic features is an equally important area for future research.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "•",
                    "Top-k",
                    "attention",
                    "is",
                    "a",
                    "highly",
                    "accurate",
                    "approximation",
                    "to",
                    "vanilla",
                    "attention",
                    "and",
                    "is",
                    "a",
                    "plug-andplay",
                    "replacement",
                    "at",
                    "both",
                    "multi-head",
                    "attention",
                    "and",
                    "feed-forward",
                    "layers",
                    "of",
                    "a",
                    "Transformer."
                ],
                [
                    "This",
                    "is",
                    "unlike",
                    "past",
                    "attention",
                    "variants",
                    "#TARGET_REF",
                    "that",
                    "require",
                    "an",
                    "expensive",
                    "corrective",
                    "pretraining",
                    "stage",
                    "to",
                    "adjust",
                    "model",
                    "weights",
                    "to",
                    "the",
                    "new",
                    "variant,",
                    "which",
                    "can",
                    "be",
                    "prohibitive",
                    "for",
                    "large",
                    "models."
                ],
                [
                    "We",
                    "show",
                    "top-k",
                    "attention",
                    "can",
                    "replace",
                    "vanilla",
                    "attention",
                    "in",
                    "a",
                    "zero-shot",
                    "inference",
                    "setup",
                    "and",
                    "at",
                    "finetuning",
                    "time",
                    "without",
                    "any",
                    "corrective",
                    "pre-training."
                ]
            ],
            "context": [
                3,
                1,
                0
            ]
        },
        "input": "sent0: • Top-k attention is a highly accurate approximation to vanilla attention and is a plug-andplay replacement at both multi-head attention and feed-forward layers of a Transformer.\n sent1: This is unlike past attention variants #TARGET_REF that require an expensive corrective pretraining stage to adjust model weights to the new variant, which can be prohibitive for large models.\n sent2: We show top-k attention can replace vanilla attention in a zero-shot inference setup and at finetuning time without any corrective pre-training.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Prevailing",
                    "stereotypes."
                ],
                [
                    "Previous",
                    "research",
                    "has",
                    "shown",
                    "that",
                    "prevailing",
                    "stereotypes",
                    "often",
                    "form",
                    "the",
                    "basis",
                    "and",
                    "justification",
                    "of",
                    "abuse."
                ],
                [
                    "For",
                    "example,",
                    "many",
                    "twitter",
                    "accounts",
                    "were",
                    "open",
                    "about",
                    "their",
                    "anger",
                    "and",
                    "hatred",
                    "for",
                    "Muslims",
                    "in",
                    "the",
                    "wake",
                    "of",
                    "the",
                    "Rochdale",
                    "scandal",
                    "that",
                    "involved",
                    "several",
                    "British-Asian",
                    "men",
                    "getting",
                    "convicted",
                    "for",
                    "child",
                    "grooming",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Stereotypes",
                    "are",
                    "not",
                    "only",
                    "explicit",
                    "but",
                    "implicit",
                    "too",
                    "#REF",
                    ",",
                    "which",
                    "often",
                    "show",
                    "up",
                    "as",
                    "implicit",
                    "and",
                    "subtle",
                    "abuse",
                    "in",
                    "the",
                    "form",
                    "of",
                    "sarcasm,",
                    "racist",
                    "jokes,",
                    "or",
                    "unnecessary",
                    "associations."
                ],
                [
                    "While",
                    "explicit",
                    "stereotypes",
                    "are",
                    "consciously",
                    "endorsed,",
                    "and",
                    "may",
                    "be",
                    "controllable,",
                    "implicit",
                    "stereotypes",
                    "are",
                    "thought",
                    "to",
                    "be",
                    "shaped",
                    "by",
                    "experience",
                    "and",
                    "based",
                    "on",
                    "learned",
                    "associations",
                    "#REF",
                    "."
                ],
                [
                    "User",
                    "and",
                    "community",
                    "information",
                    "plays",
                    "an",
                    "important",
                    "role",
                    "in",
                    "the",
                    "identification",
                    "of",
                    "such",
                    "stereotypes."
                ],
                [
                    "For",
                    "example,",
                    "if",
                    "the",
                    "location",
                    "of",
                    "users",
                    "is",
                    "available",
                    "alongside",
                    "linguistic",
                    "features",
                    "of",
                    "the",
                    "comments",
                    "they",
                    "post,",
                    "one",
                    "can",
                    "quickly",
                    "discover",
                    "the",
                    "presence",
                    "(or",
                    "absence)",
                    "of",
                    "correlations",
                    "between",
                    "specific",
                    "regions",
                    "and",
                    "specific",
                    "kinds",
                    "of",
                    "abuse."
                ],
                [
                    "Moreover,",
                    "shared",
                    "stereotypes",
                    "may",
                    "unconsciously",
                    "bring",
                    "users",
                    "together",
                    "on",
                    "online",
                    "platforms",
                    "to",
                    "form",
                    "communities."
                ],
                [
                    "Hence,",
                    "having",
                    "linguistic",
                    "information",
                    "of",
                    "a",
                    "community,",
                    "such",
                    "as",
                    "the",
                    "topics",
                    "users",
                    "in",
                    "that",
                    "community",
                    "interact",
                    "with",
                    "and",
                    "the",
                    "stance",
                    "of",
                    "users",
                    "towards",
                    "different",
                    "pieces",
                    "of",
                    "news,",
                    "can",
                    "help",
                    "capture",
                    "the",
                    "prevailing",
                    "stereotypes",
                    "that",
                    "form",
                    "the",
                    "motivation",
                    "behind",
                    "abusive",
                    "comments",
                    "from",
                    "such",
                    "users."
                ]
            ],
            "context": [
                0,
                2,
                1,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Prevailing stereotypes.\n sent1: Previous research has shown that prevailing stereotypes often form the basis and justification of abuse.\n sent2: For example, many twitter accounts were open about their anger and hatred for Muslims in the wake of the Rochdale scandal that involved several British-Asian men getting convicted for child grooming #TARGET_REF .\n sent3: Stereotypes are not only explicit but implicit too #REF , which often show up as implicit and subtle abuse in the form of sarcasm, racist jokes, or unnecessary associations.\n sent4: While explicit stereotypes are consciously endorsed, and may be controllable, implicit stereotypes are thought to be shaped by experience and based on learned associations #REF .\n sent5: User and community information plays an important role in the identification of such stereotypes.\n sent6: For example, if the location of users is available alongside linguistic features of the comments they post, one can quickly discover the presence (or absence) of correlations between specific regions and specific kinds of abuse.\n sent7: Moreover, shared stereotypes may unconsciously bring users together on online platforms to form communities.\n sent8: Hence, having linguistic information of a community, such as the topics users in that community interact with and the stance of users towards different pieces of news, can help capture the prevailing stereotypes that form the motivation behind abusive comments from such users.\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "central",
                    "position",
                    "in",
                    "the",
                    "bow-tie",
                    "model",
                    "is",
                    "taken",
                    "by",
                    "the",
                    "lexeme."
                ],
                [
                    "The",
                    "notion",
                    "of",
                    "lexeme",
                    "in",
                    "WM",
                    "is",
                    "similar",
                    "to",
                    "the",
                    "one",
                    "adopted",
                    "by",
                    "#REF",
                    ",",
                    "#TARGET_REF",
                    "and",
                    "others."
                ],
                [
                    "A",
                    "lexeme",
                    "is",
                    "a",
                    "word",
                    "considered",
                    "as",
                    "an",
                    "inflectional",
                    "paradigm."
                ],
                [
                    "Fig."
                ],
                [
                    "1",
                    "highlights",
                    "two",
                    "mappings",
                    "involving",
                    "the",
                    "lexeme:"
                ]
            ],
            "context": [
                0,
                3,
                0,
                0,
                0
            ]
        },
        "input": "sent0: The central position in the bow-tie model is taken by the lexeme.\n sent1: The notion of lexeme in WM is similar to the one adopted by #REF , #TARGET_REF and others.\n sent2: A lexeme is a word considered as an inflectional paradigm.\n sent3: Fig.\n sent4: 1 highlights two mappings involving the lexeme:\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "A",
                    "majority",
                    "of",
                    "recent",
                    "papers",
                    "on",
                    "multilabel",
                    "emotion",
                    "classification",
                    "focus",
                    "on",
                    "the",
                    "SemEval",
                    "2018",
                    "dataset",
                    "which",
                    "is",
                    "based",
                    "on",
                    "tweets."
                ],
                [
                    "Similarly,",
                    "many",
                    "of",
                    "the",
                    "non-multilabel",
                    "classification",
                    "papers",
                    "use",
                    "Twitter",
                    "data."
                ],
                [
                    "Twitter",
                    "is",
                    "a",
                    "good",
                    "base",
                    "for",
                    "emotion",
                    "classification",
                    "as",
                    "tweets",
                    "are",
                    "limited",
                    "in",
                    "length",
                    "and",
                    "generally",
                    "stand-alone,",
                    "i.e."
                ],
                [
                    "the",
                    "reader",
                    "or",
                    "annotator",
                    "does",
                    "not",
                    "need",
                    "to",
                    "guess",
                    "the",
                    "context",
                    "in",
                    "the",
                    "majority",
                    "of",
                    "cases."
                ],
                [
                    "Furthermore,",
                    "hashtags",
                    "and",
                    "emojis",
                    "are",
                    "common,",
                    "which",
                    "further",
                    "makes",
                    "the",
                    "emotion",
                    "recognition",
                    "easier",
                    "for",
                    "both",
                    "human",
                    "annotators",
                    "and",
                    "emotion",
                    "detection",
                    "and",
                    "sentiment",
                    "analysis",
                    "models."
                ],
                [
                    "Reddit",
                    "data,",
                    "as",
                    "used",
                    "by",
                    "#REF",
                    ",",
                    "and",
                    "movie",
                    "subtitles",
                    "used",
                    "by",
                    "this",
                    "paper,",
                    "are",
                    "slightly",
                    "more",
                    "problematic",
                    "as",
                    "they",
                    "are",
                    "not",
                    "\"selfcontained\"."
                ],
                [
                    "Reddit",
                    "comments",
                    "are",
                    "typically",
                    "longer",
                    "than",
                    "one",
                    "line",
                    "and",
                    "therefore",
                    "provide",
                    "some",
                    "context",
                    "for",
                    "annotators",
                    "to",
                    "go",
                    "by,",
                    "but",
                    "often",
                    "lacks",
                    "the",
                    "hashtags",
                    "and",
                    "emojis",
                    "of",
                    "twitter",
                    "and",
                    "can",
                    "be",
                    "quite",
                    "context-dependent",
                    "as",
                    "Reddit",
                    "comments",
                    "are",
                    "by",
                    "definition",
                    "reactions",
                    "to",
                    "a",
                    "post",
                    "or",
                    "another",
                    "comment."
                ],
                [
                    "Movie",
                    "subtitles",
                    "annotated",
                    "out",
                    "of",
                    "sequence",
                    "have",
                    "virtually",
                    "no",
                    "context",
                    "to",
                    "aid",
                    "the",
                    "annotator",
                    "and",
                    "are",
                    "supposed",
                    "to",
                    "be",
                    "accompanied",
                    "by",
                    "visual",
                    "cues",
                    "as",
                    "well."
                ],
                [
                    "However,",
                    "annotating",
                    "with",
                    "context",
                    "can",
                    "reduce",
                    "the",
                    "accuracy",
                    "of",
                    "one's",
                    "model",
                    "by",
                    "doubly",
                    "weighting",
                    "surrounding",
                    "units",
                    "of",
                    "granularity",
                    "(roughly",
                    "'sentences'",
                    "in",
                    "our",
                    "case)",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "On",
                    "the",
                    "other",
                    "hand,",
                    "contextual",
                    "annotations",
                    "are",
                    "less",
                    "frustrating",
                    "for",
                    "the",
                    "annotator",
                    "and",
                    "therefore,",
                    "would",
                    "likely",
                    "provide",
                    "more",
                    "annotations",
                    "in",
                    "the",
                    "same",
                    "amount",
                    "of",
                    "time",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                1,
                3
            ]
        },
        "input": "sent0: A majority of recent papers on multilabel emotion classification focus on the SemEval 2018 dataset which is based on tweets.\n sent1: Similarly, many of the non-multilabel classification papers use Twitter data.\n sent2: Twitter is a good base for emotion classification as tweets are limited in length and generally stand-alone, i.e.\n sent3: the reader or annotator does not need to guess the context in the majority of cases.\n sent4: Furthermore, hashtags and emojis are common, which further makes the emotion recognition easier for both human annotators and emotion detection and sentiment analysis models.\n sent5: Reddit data, as used by #REF , and movie subtitles used by this paper, are slightly more problematic as they are not \"selfcontained\".\n sent6: Reddit comments are typically longer than one line and therefore provide some context for annotators to go by, but often lacks the hashtags and emojis of twitter and can be quite context-dependent as Reddit comments are by definition reactions to a post or another comment.\n sent7: Movie subtitles annotated out of sequence have virtually no context to aid the annotator and are supposed to be accompanied by visual cues as well.\n sent8: However, annotating with context can reduce the accuracy of one's model by doubly weighting surrounding units of granularity (roughly 'sentences' in our case) #TARGET_REF .\n sent9: On the other hand, contextual annotations are less frustrating for the annotator and therefore, would likely provide more annotations in the same amount of time #REF .\n",
        "output": "{\"INFORMATION\": [\"sent8\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent9\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Semantic",
                    "parsers",
                    "analyze",
                    "a",
                    "sentence",
                    "with",
                    "the",
                    "aim",
                    "of",
                    "identifying",
                    "the",
                    "\"who",
                    "did",
                    "what",
                    "to",
                    "whom,",
                    "for",
                    "whom",
                    "or",
                    "what,",
                    "how,",
                    "where,",
                    "when,",
                    "and",
                    "why.\""
                ],
                [
                    "Shallow",
                    "semantic",
                    "parsing",
                    "extracts",
                    "the",
                    "predicateargument",
                    "structure",
                    "of",
                    "verbs",
                    "in",
                    "a",
                    "sentence",
                    "based",
                    "on",
                    "the",
                    "syntactic",
                    "tree",
                    "of",
                    "that",
                    "sentence."
                ],
                [
                    "For",
                    "example,",
                    "the",
                    "predicate",
                    "argument",
                    "structure",
                    "of",
                    "the",
                    "verb",
                    "hold",
                    "in",
                    "Figure",
                    "1",
                    "specifies",
                    "a",
                    "\"holding\"",
                    "relation",
                    "between",
                    "both",
                    "sides",
                    "(who)",
                    "and",
                    "meeting",
                    "(what)",
                    "on",
                    "Sunday",
                    "(when)."
                ],
                [
                    "For",
                    "a",
                    "sentence",
                    "with",
                    "multiple",
                    "verbs,",
                    "there",
                    "can",
                    "be",
                    "multiple",
                    "predicate",
                    "argument",
                    "structures."
                ],
                [
                    "Shallow",
                    "semantic",
                    "parsing",
                    "systems",
                    "are",
                    "mostly",
                    "based",
                    "on",
                    "classifiers",
                    "that",
                    "learn",
                    "from",
                    "a",
                    "manually",
                    "annotated",
                    "semantic",
                    "corpus",
                    "#TARGET_REF",
                    ",",
                    "#REF",
                    ")."
                ],
                [
                    "Following",
                    "the",
                    "publication",
                    "of",
                    "the",
                    "Proposional",
                    "Bank",
                    "(PropBank)",
                    "#REF",
                    "first",
                    "in",
                    "English,",
                    "then",
                    "in",
                    "Chinese,",
                    "it",
                    "has",
                    "been",
                    "possible",
                    "to",
                    "train",
                    "these",
                    "classifiers",
                    "to",
                    "perform",
                    "semantic",
                    "analysis",
                    "on",
                    "news",
                    "wire",
                    "type",
                    "of",
                    "texts."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                2,
                0
            ]
        },
        "input": "sent0: Semantic parsers analyze a sentence with the aim of identifying the \"who did what to whom, for whom or what, how, where, when, and why.\"\n sent1: Shallow semantic parsing extracts the predicateargument structure of verbs in a sentence based on the syntactic tree of that sentence.\n sent2: For example, the predicate argument structure of the verb hold in Figure 1 specifies a \"holding\" relation between both sides (who) and meeting (what) on Sunday (when).\n sent3: For a sentence with multiple verbs, there can be multiple predicate argument structures.\n sent4: Shallow semantic parsing systems are mostly based on classifiers that learn from a manually annotated semantic corpus #TARGET_REF , #REF ).\n sent5: Following the publication of the Proposional Bank (PropBank) #REF first in English, then in Chinese, it has been possible to train these classifiers to perform semantic analysis on news wire type of texts.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent4\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "A",
                    "major",
                    "concern",
                    "of",
                    "MMB's",
                    "was",
                    "always",
                    "how",
                    "to",
                    "parse",
                    "written",
                    "English",
                    "into",
                    "a",
                    "machine",
                    "representation",
                    "for",
                    "MT",
                    "#REF",
                    "."
                ],
                [
                    "She",
                    "believed",
                    "that",
                    "such",
                    "a",
                    "representation",
                    "should",
                    "be",
                    "fundamentally",
                    "semantic",
                    "in",
                    "nature",
                    "(i.e."
                ],
                [
                    "based",
                    "on",
                    "meaning",
                    "rather",
                    "than",
                    "syntax)",
                    "and",
                    "that",
                    "those",
                    "semantic",
                    "structures",
                    "should",
                    "be",
                    "used",
                    "in",
                    "the",
                    "parsing",
                    "process",
                    "itself."
                ],
                [
                    "The",
                    "latter",
                    "view",
                    "was",
                    "highly",
                    "original,",
                    "since",
                    "virtually",
                    "no",
                    "one",
                    "has",
                    "ever",
                    "proposed",
                    "such",
                    "a",
                    "thing",
                    "-the",
                    "doctrine",
                    "is",
                    "now",
                    "known",
                    "as",
                    "semantic",
                    "parsing,",
                    "and",
                    "is",
                    "well",
                    "known",
                    "even",
                    "if",
                    "not",
                    "as",
                    "fashionable",
                    "as",
                    "it",
                    "was",
                    "ten",
                    "years",
                    "ago",
                    "-and",
                    "espousing",
                    "it",
                    "certainly",
                    "set",
                    "MMB",
                    "apart",
                    "from",
                    "the",
                    "prevailing",
                    "syntactic",
                    "approaches",
                    "of",
                    "her",
                    "time."
                ],
                [
                    "Some",
                    "contemporary",
                    "clarification",
                    "will",
                    "be",
                    "needed",
                    "in",
                    "later",
                    "commentary",
                    "on",
                    "this",
                    "point,",
                    "since",
                    "the",
                    "meaning",
                    "of",
                    "the",
                    "word",
                    "'semantics'",
                    "as",
                    "used",
                    "by",
                    "MMB",
                    "in",
                    "this",
                    "connection,",
                    "cannot",
                    "be",
                    "equated",
                    "with",
                    "either",
                    "its",
                    "use",
                    "in",
                    "'semantic",
                    "grammar'",
                    "(e.g."
                ],
                [
                    "#TARGET_REF",
                    "to",
                    "mean",
                    "parsing",
                    "by",
                    "the",
                    "use",
                    "of",
                    "particular",
                    "word-names",
                    "as",
                    "they",
                    "occur",
                    "in",
                    "text",
                    "(e.g."
                ],
                [
                    "as",
                    "in",
                    "a",
                    "program",
                    "that",
                    "knew",
                    "what",
                    "words",
                    "would",
                    "follow",
                    "'electrical'),",
                    "nor",
                    "with",
                    "its",
                    "currently",
                    "dominant",
                    "use",
                    "in",
                    "formal,",
                    "logical",
                    "semantics,",
                    "to",
                    "which",
                    "we",
                    "shall",
                    "return",
                    "in",
                    "a",
                    "moment."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                3,
                2,
                0
            ]
        },
        "input": "sent0: A major concern of MMB's was always how to parse written English into a machine representation for MT #REF .\n sent1: She believed that such a representation should be fundamentally semantic in nature (i.e.\n sent2: based on meaning rather than syntax) and that those semantic structures should be used in the parsing process itself.\n sent3: The latter view was highly original, since virtually no one has ever proposed such a thing -the doctrine is now known as semantic parsing, and is well known even if not as fashionable as it was ten years ago -and espousing it certainly set MMB apart from the prevailing syntactic approaches of her time.\n sent4: Some contemporary clarification will be needed in later commentary on this point, since the meaning of the word 'semantics' as used by MMB in this connection, cannot be equated with either its use in 'semantic grammar' (e.g.\n sent5: #TARGET_REF to mean parsing by the use of particular word-names as they occur in text (e.g.\n sent6: as in a program that knew what words would follow 'electrical'), nor with its currently dominant use in formal, logical semantics, to which we shall return in a moment.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent5\"], \"BACKGROUND\": [\"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Although",
                    "many",
                    "NLP",
                    "methods",
                    "have",
                    "already",
                    "been",
                    "used",
                    "to",
                    "generate",
                    "distance",
                    "matrices,",
                    "others",
                    "are",
                    "worth",
                    "trying."
                ],
                [
                    "Examples",
                    "include",
                    "graph",
                    "embedding",
                    "of",
                    "associations",
                    "#TARGET_REF",
                    "and",
                    "GraphGlove",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                2,
                1
            ]
        },
        "input": "sent0: Although many NLP methods have already been used to generate distance matrices, others are worth trying.\n sent1: Examples include graph embedding of associations #TARGET_REF and GraphGlove #REF .\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "this",
                    "Spanish-English",
                    "example,",
                    "the",
                    "clitic",
                    "can",
                    "climb",
                    "over",
                    "an",
                    "unlimited",
                    "number",
                    "of",
                    "'trigger",
                    "verbs'",
                    "#TARGET_REF",
                    ")",
                    "(indicated",
                    "by",
                    "the",
                    "ellipses",
                    "in",
                    "the",
                    "example),",
                    "and",
                    "for",
                    "certain",
                    "TAG",
                    "grammars",
                    "this",
                    "can",
                    "correspond",
                    "to",
                    "a",
                    "pair",
                    "of",
                    "derivation",
                    "trees",
                    "as",
                    "in",
                    "Figure",
                    "2."
                ],
                [
                    "In",
                    "this",
                    "pair",
                    "of",
                    "trees,",
                    "his",
                    "corresponds",
                    "to",
                    "both",
                    "los",
                    "and",
                    "the",
                    "clitic",
                    "le."
                ],
                [
                    "Both",
                    "his",
                    "and",
                    "los",
                    "are",
                    "fixed",
                    "in",
                    "relation",
                    "to",
                    "the",
                    "root",
                    "of",
                    "the",
                    "tree,",
                    "but",
                    "le",
                    "is",
                    "an",
                    "unbounded",
                    "distance",
                    "from",
                    "it,",
                    "so",
                    "it",
                    "is",
                    "not",
                    "possible",
                    "to",
                    "form",
                    "a",
                    "gCN",
                    "in",
                    "the",
                    "Spanish",
                    "tree",
                    "for",
                    "pairing",
                    "without",
                    "the",
                    "unbounded",
                    "and",
                    "unrelated",
                    "recursively-inserted",
                    "verbs,",
                    "hence",
                    "requiring",
                    "infinitely",
                    "many",
                    "transfer",
                    "rules."
                ]
            ],
            "context": [
                2,
                0,
                0
            ]
        },
        "input": "sent0: In this Spanish-English example, the clitic can climb over an unlimited number of 'trigger verbs' #TARGET_REF ) (indicated by the ellipses in the example), and for certain TAG grammars this can correspond to a pair of derivation trees as in Figure 2.\n sent1: In this pair of trees, his corresponds to both los and the clitic le.\n sent2: Both his and los are fixed in relation to the root of the tree, but le is an unbounded distance from it, so it is not possible to form a gCN in the Spanish tree for pairing without the unbounded and unrelated recursively-inserted verbs, hence requiring infinitely many transfer rules.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Parsing",
                    "schemata",
                    "are",
                    "closely",
                    "related",
                    "to",
                    "grammatical",
                    "deduction",
                    "systems",
                    "#TARGET_REF",
                    ",",
                    "where",
                    "items",
                    "are",
                    "called",
                    "formula",
                    "schemata,",
                    "deduction",
                    "steps",
                    "are",
                    "inference",
                    "rules,",
                    "hypothesis",
                    "are",
                    "axioms",
                    "and",
                    "final",
                    "items",
                    "are",
                    "goal",
                    "formulas."
                ]
            ],
            "context": [
                2
            ]
        },
        "input": "sent0: Parsing schemata are closely related to grammatical deduction systems #TARGET_REF , where items are called formula schemata, deduction steps are inference rules, hypothesis are axioms and final items are goal formulas.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "to",
                    "the",
                    "cost",
                    "of",
                    "the",
                    "number",
                    "of",
                    "translated",
                    "segments,",
                    "which",
                    "is",
                    "marked",
                    "with",
                    "Comtsegm",
                    "instead",
                    "of",
                    "Cosegm",
                    "as",
                    "in",
                    "Table",
                    "1."
                ],
                [
                    "For",
                    "good",
                    "language",
                    "and",
                    "translation",
                    "models,",
                    "when",
                    "overall",
                    "MT",
                    "output",
                    "BLEU",
                    "score",
                    "#TARGET_REF",
                    ")",
                    "is",
                    "higher",
                    "than",
                    "65%,",
                    "a",
                    "MT",
                    "pre-translated",
                    "segment",
                    "translation",
                    "is",
                    "paid",
                    "50%",
                    "of",
                    "the",
                    "human",
                    "translation",
                    "rate",
                    "per",
                    "segment."
                ],
                [
                    "Consequently,",
                    "Comtsegm",
                    "=",
                    "50%",
                    "*Cosegm."
                ],
                [
                    "To",
                    "our",
                    "knowledge,",
                    "our",
                    "internal",
                    "proofing",
                    "costs",
                    "do",
                    "not",
                    "change",
                    "according",
                    "to",
                    "recent",
                    "statistics."
                ],
                [
                    "With",
                    "time,",
                    "these",
                    "statistics",
                    "can",
                    "change",
                    "and",
                    "guide",
                    "better",
                    "pricing",
                    "model",
                    "development."
                ],
                [
                    "Table",
                    "4.",
                    "shows",
                    "the",
                    "translation",
                    "process",
                    "investment",
                    "costs",
                    "when",
                    "MT",
                    "is",
                    "developed",
                    "internally",
                    "on",
                    "LSP",
                    "premises."
                ],
                [
                    "The",
                    "document",
                    "conversion,",
                    "project",
                    "coordination,",
                    "additional",
                    "services",
                    "costs,",
                    "along",
                    "with",
                    "the",
                    "cost",
                    "of",
                    "terminology",
                    "management,",
                    "and",
                    "resources,",
                    "are",
                    "identical",
                    "and",
                    "the",
                    "same",
                    "as",
                    "in",
                    "Scenario",
                    "1."
                ],
                [
                    "It",
                    "is",
                    "interesting",
                    "to",
                    "note",
                    "the",
                    "costs",
                    "that",
                    "accompany",
                    "internal",
                    "MT",
                    "development."
                ],
                [
                    "These",
                    "costs",
                    "include",
                    "but",
                    "are",
                    "not",
                    "restricted",
                    "to",
                    "customization,",
                    "6",
                    "equal",
                    "to",
                    "50%",
                    "*Cosegm",
                    "automation,",
                    "software",
                    "and",
                    "hardware",
                    "costs."
                ],
                [
                    "A",
                    "cost",
                    "for",
                    "trainings",
                    "is",
                    "introduced."
                ],
                [
                    "It",
                    "covers",
                    "project",
                    "coordinators",
                    "MT",
                    "training",
                    "expenses."
                ],
                [
                    "Additionally",
                    "it",
                    "includes",
                    "MT",
                    "specialist",
                    "new",
                    "methodology",
                    "and",
                    "approach",
                    "trainings."
                ],
                [
                    "These",
                    "costs",
                    "can",
                    "be",
                    "roughly",
                    "estimated",
                    "to",
                    "the",
                    "expenses",
                    "incurred",
                    "for",
                    "the",
                    "number",
                    "of",
                    "employees",
                    "required",
                    "per",
                    "day."
                ],
                [
                    "Depending",
                    "of",
                    "the",
                    "scale",
                    "of",
                    "the",
                    "MT",
                    "projects,",
                    "hardware",
                    "costs",
                    "for",
                    "two",
                    "servers",
                    "allowing",
                    "simultaneous",
                    "translation",
                    "requests",
                    "can",
                    "easily",
                    "reach",
                    "60K",
                    "euros."
                ],
                [
                    "The",
                    "hardware",
                    "cost",
                    "is",
                    "repetitive",
                    "every",
                    "year",
                    "as",
                    "more",
                    "projects",
                    "and",
                    "users",
                    "are",
                    "expected",
                    "to",
                    "benefit",
                    "from",
                    "MT."
                ],
                [
                    "The",
                    "cost",
                    "of",
                    "customization",
                    "and",
                    "optimization",
                    "include",
                    "human",
                    "development",
                    "effort",
                    "and",
                    "is",
                    "measured",
                    "in",
                    "man-hours."
                ],
                [
                    "This",
                    "cost",
                    "is",
                    "directly",
                    "linked",
                    "to",
                    "the",
                    "software",
                    "licensing",
                    "expenses",
                    "as",
                    "licensed",
                    "tools",
                    "are",
                    "used",
                    "to",
                    "accelerate",
                    "the",
                    "MT",
                    "specialists'",
                    "development",
                    "effort."
                ],
                [
                    "More",
                    "managerial",
                    "tasks",
                    "by",
                    "human",
                    "resources",
                    "are",
                    "required",
                    "such",
                    "as",
                    "risk",
                    "management",
                    "monitoring",
                    "and",
                    "development",
                    "of",
                    "prevention",
                    "plan",
                    "strategies."
                ]
            ],
            "context": [
                0,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: to the cost of the number of translated segments, which is marked with Comtsegm instead of Cosegm as in Table 1.\n sent1: For good language and translation models, when overall MT output BLEU score #TARGET_REF ) is higher than 65%, a MT pre-translated segment translation is paid 50% of the human translation rate per segment.\n sent2: Consequently, Comtsegm = 50% *Cosegm.\n sent3: To our knowledge, our internal proofing costs do not change according to recent statistics.\n sent4: With time, these statistics can change and guide better pricing model development.\n sent5: Table 4. shows the translation process investment costs when MT is developed internally on LSP premises.\n sent6: The document conversion, project coordination, additional services costs, along with the cost of terminology management, and resources, are identical and the same as in Scenario 1.\n sent7: It is interesting to note the costs that accompany internal MT development.\n sent8: These costs include but are not restricted to customization, 6 equal to 50% *Cosegm automation, software and hardware costs.\n sent9: A cost for trainings is introduced.\n sent10: It covers project coordinators MT training expenses.\n sent11: Additionally it includes MT specialist new methodology and approach trainings.\n sent12: These costs can be roughly estimated to the expenses incurred for the number of employees required per day.\n sent13: Depending of the scale of the MT projects, hardware costs for two servers allowing simultaneous translation requests can easily reach 60K euros.\n sent14: The hardware cost is repetitive every year as more projects and users are expected to benefit from MT.\n sent15: The cost of customization and optimization include human development effort and is measured in man-hours.\n sent16: This cost is directly linked to the software licensing expenses as licensed tools are used to accelerate the MT specialists' development effort.\n sent17: More managerial tasks by human resources are required such as risk management monitoring and development of prevention plan strategies.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Unlabeled",
                    "questions",
                    "We",
                    "collect",
                    "1.7",
                    "million",
                    "unlabeled",
                    "questions",
                    "from",
                    "Yahoo!"
                ],
                [
                    "Answers",
                    "5",
                    ",",
                    "ORCAS",
                    "#TARGET_REF",
                    "and",
                    "MRQA",
                    "#REF",
                    "."
                ],
                [
                    "We",
                    "use",
                    "the",
                    "questions",
                    "from",
                    "Yahoo!"
                ],
                [
                    "Answers,",
                    "ORCAS",
                    "and",
                    "NQ",
                    "as",
                    "new",
                    "questions",
                    "in",
                    "the",
                    "experiments",
                    "of",
                    "MSMARCO."
                ],
                [
                    "We",
                    "only",
                    "use",
                    "the",
                    "questions",
                    "from",
                    "MRQA",
                    "as",
                    "the",
                    "new",
                    "questions",
                    "in",
                    "the",
                    "experiments",
                    "of",
                    "NQ."
                ],
                [
                    "Since",
                    "both",
                    "NQ",
                    "and",
                    "MRQA",
                    "mainly",
                    "contain",
                    "factoid-questions,",
                    "while",
                    "other",
                    "datasets",
                    "contain",
                    "both",
                    "factoid",
                    "and",
                    "non-factoid",
                    "questions."
                ]
            ],
            "context": [
                0,
                3,
                2,
                2,
                0,
                0
            ]
        },
        "input": "sent0: Unlabeled questions We collect 1.7 million unlabeled questions from Yahoo!\n sent1: Answers 5 , ORCAS #TARGET_REF and MRQA #REF .\n sent2: We use the questions from Yahoo!\n sent3: Answers, ORCAS and NQ as new questions in the experiments of MSMARCO.\n sent4: We only use the questions from MRQA as the new questions in the experiments of NQ.\n sent5: Since both NQ and MRQA mainly contain factoid-questions, while other datasets contain both factoid and non-factoid questions.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent2\", \"sent3\"], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "GPT-3",
                    "displays",
                    "strong",
                    "zero-shot",
                    "abilities",
                    "#REF",
                    ",",
                    "i.e.,",
                    "using",
                    "a",
                    "simple",
                    "instruction",
                    "or",
                    "\"prompt\"",
                    "as",
                    "input,",
                    "the",
                    "model",
                    "will",
                    "extend",
                    "or",
                    "complete",
                    "the",
                    "text",
                    "accordingly",
                    "without",
                    "any",
                    "pre-defined",
                    "examples."
                ],
                [
                    "Prompt-engineering",
                    "thus",
                    "refers",
                    "to",
                    "manipulations",
                    "and",
                    "perturbations",
                    "of",
                    "this",
                    "prompt",
                    "to",
                    "context-force",
                    "the",
                    "desired",
                    "output",
                    "behaviour",
                    "#REF",
                    "."
                ],
                [
                    "In",
                    "contrast",
                    "to",
                    "zero-shot,",
                    "GPT-3",
                    "can",
                    "be",
                    "fine-tuned",
                    "over",
                    "a",
                    "dataset",
                    "with",
                    "desired",
                    "inputoutput",
                    "pairs",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "To",
                    "conduct",
                    "the",
                    "experiment",
                    "to",
                    "compare",
                    "neutral",
                    "and",
                    "diversityencouraging",
                    "prompts,",
                    "we",
                    "compile",
                    "a",
                    "list",
                    "of",
                    "18",
                    "prompts."
                ],
                [
                    "Nine",
                    "of",
                    "them",
                    "are",
                    "designated",
                    "\"neutral\"",
                    "and",
                    "used",
                    "as",
                    "our",
                    "\"zero-shot\"",
                    "prompts."
                ],
                [
                    "These",
                    "simply",
                    "specify",
                    "a",
                    "task",
                    "of",
                    "generating",
                    "an",
                    "ad",
                    "for",
                    "a",
                    "given",
                    "job",
                    "but",
                    "are",
                    "syntactically",
                    "varied."
                ],
                [
                    "The",
                    "other",
                    "nine",
                    "prompts",
                    "are",
                    "\"equality",
                    "and",
                    "diversity",
                    "prompts\",",
                    "which",
                    "we",
                    "call",
                    "\"engineered\"",
                    "prompts."
                ],
                [
                    "Tab."
                ],
                [
                    "3",
                    "displays",
                    "all",
                    "18",
                    "prompts",
                    "with",
                    "their",
                    "respective",
                    "bias",
                    "and",
                    "realism",
                    "scores."
                ]
            ],
            "context": [
                0,
                0,
                1,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: GPT-3 displays strong zero-shot abilities #REF , i.e., using a simple instruction or \"prompt\" as input, the model will extend or complete the text accordingly without any pre-defined examples.\n sent1: Prompt-engineering thus refers to manipulations and perturbations of this prompt to context-force the desired output behaviour #REF .\n sent2: In contrast to zero-shot, GPT-3 can be fine-tuned over a dataset with desired inputoutput pairs #TARGET_REF .\n sent3: To conduct the experiment to compare neutral and diversityencouraging prompts, we compile a list of 18 prompts.\n sent4: Nine of them are designated \"neutral\" and used as our \"zero-shot\" prompts.\n sent5: These simply specify a task of generating an ad for a given job but are syntactically varied.\n sent6: The other nine prompts are \"equality and diversity prompts\", which we call \"engineered\" prompts.\n sent7: Tab.\n sent8: 3 displays all 18 prompts with their respective bias and realism scores.\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "For",
                    "the",
                    "training",
                    "of",
                    "Transformer+CTC,",
                    "we",
                    "applied",
                    "joint",
                    "CTC",
                    "training",
                    "to",
                    "improve",
                    "performance",
                    "#REF",
                    "."
                ],
                [
                    "For",
                    "CTC-based",
                    "decoding,",
                    "we",
                    "used",
                    "the",
                    "greedy",
                    "search",
                    "algorithm."
                ],
                [
                    "For",
                    "Transformer",
                    "decoding,",
                    "we",
                    "used",
                    "the",
                    "beam",
                    "search",
                    "algorithm",
                    "and",
                    "tuned",
                    "search",
                    "parameters",
                    "using",
                    "the",
                    "development",
                    "set."
                ],
                [
                    "For",
                    "the",
                    "Transformer+CTC",
                    "model,",
                    "we",
                    "applied",
                    "Transformer/CTC",
                    "joint",
                    "decoding",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "and",
                    "tuned",
                    "the",
                    "weights",
                    "of",
                    "the",
                    "objective",
                    "using",
                    "the",
                    "development",
                    "set."
                ],
                [
                    "Note",
                    "that",
                    "the",
                    "language",
                    "model",
                    "shallow",
                    "fusion",
                    "#REF",
                    "is",
                    "not",
                    "applied",
                    "since",
                    "we",
                    "could",
                    "not",
                    "find",
                    "effectiveness",
                    "in",
                    "our",
                    "preliminary",
                    "experiment."
                ]
            ],
            "context": [
                0,
                0,
                0,
                3,
                0,
                0
            ]
        },
        "input": "sent0: For the training of Transformer+CTC, we applied joint CTC training to improve performance #REF .\n sent1: For CTC-based decoding, we used the greedy search algorithm.\n sent2: For Transformer decoding, we used the beam search algorithm and tuned search parameters using the development set.\n sent3: For the Transformer+CTC model, we applied Transformer/CTC joint decoding #TARGET_REF .\n sent4: and tuned the weights of the objective using the development set.\n sent5: Note that the language model shallow fusion #REF is not applied since we could not find effectiveness in our preliminary experiment.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "analyzed",
                    "the",
                    "results",
                    "related",
                    "to",
                    "the",
                    "keywords",
                    "in",
                    "WordNet-Affect",
                    "#TARGET_REF",
                    "),",
                    "a",
                    "linguistic",
                    "resource",
                    "for",
                    "the",
                    "lexical",
                    "representation",
                    "of",
                    "affective",
                    "knowledge."
                ],
                [
                    "In",
                    "this",
                    "the",
                    "affective",
                    "concepts",
                    "representing",
                    "emotional",
                    "state",
                    "are",
                    "individuated",
                    "by",
                    "synsets",
                    "marked",
                    "with",
                    "the",
                    "alabel",
                    "EMOTION."
                ],
                [
                    "There",
                    "are",
                    "also",
                    "other",
                    "a-labels",
                    "for",
                    "those",
                    "concepts",
                    "representing",
                    "moods,",
                    "situations",
                    "eliciting",
                    "emotions,",
                    "or",
                    "emotional",
                    "responses."
                ]
            ],
            "context": [
                1,
                3,
                3
            ]
        },
        "input": "sent0: We analyzed the results related to the keywords in WordNet-Affect #TARGET_REF ), a linguistic resource for the lexical representation of affective knowledge.\n sent1: In this the affective concepts representing emotional state are individuated by synsets marked with the alabel EMOTION.\n sent2: There are also other a-labels for those concepts representing moods, situations eliciting emotions, or emotional responses.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\", \"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "10",
                    "An",
                    "example",
                    "of",
                    "this",
                    "type",
                    "of",
                    "work",
                    "is",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "In",
                    "Arabic",
                    "a",
                    "root",
                    "such",
                    "as",
                    "ktb",
                    "is",
                    "combined",
                    "with",
                    "a",
                    "vowel",
                    "pattern",
                    "to",
                    "produce",
                    "words",
                    "such",
                    "as",
                    "kitaab",
                    "('book')",
                    "and",
                    "kutub",
                    "('books')."
                ],
                [
                    "It",
                    "is",
                    "interesting",
                    "to",
                    "note",
                    "that",
                    "the",
                    "traditional",
                    "approach",
                    "to",
                    "Arabic",
                    "roots",
                    "results",
                    "in",
                    "approximately",
                    "10,000",
                    "different",
                    "items."
                ],
                [
                    "This",
                    "number",
                    "corresponds",
                    "more",
                    "closely",
                    "to",
                    "the",
                    "number",
                    "of",
                    "simple",
                    "lexemes",
                    "to",
                    "be",
                    "expected",
                    "in",
                    "the",
                    "lexicon",
                    "of",
                    "a",
                    "language",
                    "than",
                    "to",
                    "the",
                    "number",
                    "of",
                    "lexemes."
                ],
                [
                    "It",
                    "is",
                    "then",
                    "not",
                    "surprising",
                    "to",
                    "find",
                    "items",
                    "such",
                    "as",
                    "kaatib",
                    "('writer'),",
                    "kutib",
                    "('be",
                    "written')",
                    "with",
                    "the",
                    "same",
                    "root."
                ],
                [
                    "11",
                    "In",
                    "principle",
                    "we",
                    "could",
                    "of",
                    "course",
                    "reverse",
                    "the",
                    "entire",
                    "system."
                ],
                [
                    "Thus,",
                    "languages",
                    "such",
                    "as",
                    "Navajo,",
                    "which",
                    "use",
                    "only",
                    "prefixation,",
                    "are",
                    "not",
                    "a",
                    "major",
                    "problem."
                ],
                [
                    "12",
                    "There",
                    "is",
                    "of",
                    "course",
                    "a",
                    "different",
                    "prefixation",
                    "process",
                    "attaching",
                    "un-to",
                    "a",
                    "verb",
                    "as",
                    "in",
                    "undo,",
                    "but",
                    "it",
                    "would",
                    "yield",
                    "the",
                    "wrong",
                    "analysis",
                    "for",
                    "unacceptable."
                ],
                [
                    "The",
                    "word",
                    "means",
                    "'which",
                    "cannot",
                    "be",
                    "accepted',",
                    "not",
                    "'which",
                    "can",
                    "be",
                    "unaccepted'."
                ]
            ],
            "context": [
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: 10 An example of this type of work is #TARGET_REF .\n sent1: In Arabic a root such as ktb is combined with a vowel pattern to produce words such as kitaab ('book') and kutub ('books').\n sent2: It is interesting to note that the traditional approach to Arabic roots results in approximately 10,000 different items.\n sent3: This number corresponds more closely to the number of simple lexemes to be expected in the lexicon of a language than to the number of lexemes.\n sent4: It is then not surprising to find items such as kaatib ('writer'), kutib ('be written') with the same root.\n sent5: 11 In principle we could of course reverse the entire system.\n sent6: Thus, languages such as Navajo, which use only prefixation, are not a major problem.\n sent7: 12 There is of course a different prefixation process attaching un-to a verb as in undo, but it would yield the wrong analysis for unacceptable.\n sent8: The word means 'which cannot be accepted', not 'which can be unaccepted'.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "For",
                    "the",
                    "visual",
                    "feature,",
                    "we",
                    "adopt",
                    "Faster-RCNN",
                    "#TARGET_REF",
                    "to",
                    "extract",
                    "100",
                    "bounding",
                    "box",
                    "proposals."
                ]
            ],
            "context": [
                2
            ]
        },
        "input": "sent0: For the visual feature, we adopt Faster-RCNN #TARGET_REF to extract 100 bounding box proposals.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "four",
                    "languages",
                    "are",
                    "highly",
                    "agglutinative",
                    "or",
                    "polysynthetic,",
                    "meaning",
                    "that",
                    "they",
                    "usually",
                    "express",
                    "a",
                    "large",
                    "amount",
                    "of",
                    "information",
                    "in",
                    "just",
                    "one",
                    "word",
                    "with",
                    "several",
                    "joint",
                    "morphemes."
                ],
                [
                    "This",
                    "is",
                    "a",
                    "real",
                    "challenge",
                    "for",
                    "MT",
                    "and",
                    "subword",
                    "segmentation",
                    "methods,",
                    "given",
                    "the",
                    "high",
                    "probability",
                    "of",
                    "addressing",
                    "a",
                    "\"rare",
                    "word\"",
                    "for",
                    "the",
                    "system."
                ],
                [
                    "We",
                    "also",
                    "note",
                    "that",
                    "each",
                    "language",
                    "belongs",
                    "to",
                    "a",
                    "different",
                    "language",
                    "family,",
                    "but",
                    "that",
                    "is",
                    "not",
                    "a",
                    "problem",
                    "for",
                    "multilingual",
                    "models,",
                    "as",
                    "usually",
                    "the",
                    "family-based",
                    "clusters",
                    "are",
                    "not",
                    "the",
                    "most",
                    "effective",
                    "ones",
                    "#TARGET_REF",
                    "Pre-processing",
                    "The",
                    "datasets",
                    "were",
                    "noisy",
                    "and",
                    "not",
                    "cleaned."
                ],
                [
                    "Lines",
                    "are",
                    "reduced",
                    "according",
                    "to",
                    "several",
                    "heuristics:",
                    "Arabic",
                    "numbers",
                    "or",
                    "punctuation",
                    "do",
                    "not",
                    "match",
                    "in",
                    "the",
                    "parallel",
                    "sentences,",
                    "there",
                    "are",
                    "more",
                    "symbols",
                    "or",
                    "numbers",
                    "than",
                    "words",
                    "in",
                    "a",
                    "sentence,",
                    "the",
                    "ratio",
                    "of",
                    "words",
                    "from",
                    "one",
                    "side",
                    "is",
                    "five",
                    "times",
                    "larger",
                    "or",
                    "shorter",
                    "than",
                    "the",
                    "other,",
                    "among",
                    "others."
                ],
                [
                    "Table",
                    "5",
                    "in",
                    "the",
                    "Appendix",
                    "includes",
                    "the",
                    "original",
                    "and",
                    "cleaned",
                    "data",
                    "size",
                    "per",
                    "language-pair,",
                    "whereas",
                    "Table",
                    "1",
                    "presents",
                    "the",
                    "final",
                    "sizes."
                ]
            ],
            "context": [
                0,
                0,
                2,
                0,
                0
            ]
        },
        "input": "sent0: The four languages are highly agglutinative or polysynthetic, meaning that they usually express a large amount of information in just one word with several joint morphemes.\n sent1: This is a real challenge for MT and subword segmentation methods, given the high probability of addressing a \"rare word\" for the system.\n sent2: We also note that each language belongs to a different language family, but that is not a problem for multilingual models, as usually the family-based clusters are not the most effective ones #TARGET_REF Pre-processing The datasets were noisy and not cleaned.\n sent3: Lines are reduced according to several heuristics: Arabic numbers or punctuation do not match in the parallel sentences, there are more symbols or numbers than words in a sentence, the ratio of words from one side is five times larger or shorter than the other, among others.\n sent4: Table 5 in the Appendix includes the original and cleaned data size per language-pair, whereas Table 1 presents the final sizes.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Deep",
                    "models",
                    "such",
                    "as",
                    "Transformers",
                    "rely",
                    "heavily",
                    "on",
                    "the",
                    "availability",
                    "of",
                    "a",
                    "large",
                    "amount",
                    "of",
                    "annotated",
                    "data,",
                    "which",
                    "is",
                    "available",
                    "only",
                    "for",
                    "prominent",
                    "languages",
                    "like",
                    "English,",
                    "Russian,",
                    "German",
                    "or",
                    "Spanish",
                    "#REF",
                    "."
                ],
                [
                    "For",
                    "a",
                    "majority",
                    "of",
                    "other",
                    "languages",
                    "with",
                    "a",
                    "minimal",
                    "number",
                    "of",
                    "annotations,",
                    "cross-lingual",
                    "transfer",
                    "learning",
                    "#TARGET_REF",
                    "has",
                    "been",
                    "proposed",
                    "as",
                    "a",
                    "possible",
                    "solution."
                ],
                [
                    "This",
                    "approach",
                    "can",
                    "transfer",
                    "knowledge",
                    "from",
                    "the",
                    "annotation-rich",
                    "source",
                    "language",
                    "to",
                    "low-resource",
                    "or",
                    "zero-resource",
                    "target",
                    "languages."
                ],
                [
                    "Furthermore,",
                    "multilingual",
                    "models",
                    "#REF",
                    "can",
                    "be",
                    "used",
                    "to",
                    "mitigate",
                    "the",
                    "data",
                    "scarcity",
                    "problem."
                ],
                [
                    "For",
                    "example,",
                    "LASER",
                    "#REF",
                    ")",
                    "used",
                    "a",
                    "bidirectional",
                    "LSTM",
                    "#REF",
                    "encoder",
                    "with",
                    "a",
                    "byte",
                    "pair",
                    "encoding",
                    "vocabulary",
                    "shared",
                    "between",
                    "languages."
                ],
                [
                    "This",
                    "work",
                    "showed",
                    "that",
                    "joint",
                    "training",
                    "of",
                    "multiple",
                    "languages",
                    "helped",
                    "to",
                    "improve",
                    "the",
                    "model",
                    "performance",
                    "for",
                    "low-resource",
                    "languages."
                ],
                [
                    "LaBSE",
                    "#REF",
                    "used",
                    "the",
                    "mBERT",
                    "#REF",
                    "encoder",
                    "pre-trained",
                    "with",
                    "masked",
                    "language",
                    "modelling",
                    "and",
                    "translation",
                    "language",
                    "modelling",
                    "#REF",
                    "tasks."
                ],
                [
                    "It",
                    "attempted",
                    "to",
                    "optimize",
                    "the",
                    "dual",
                    "encoder",
                    "translation",
                    "ranking",
                    "#REF",
                    "loss",
                    "during",
                    "pre-training",
                    "to",
                    "achieve",
                    "similar",
                    "embedding",
                    "for",
                    "the",
                    "same",
                    "text",
                    "in",
                    "different",
                    "languages."
                ]
            ],
            "context": [
                3,
                2,
                1,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Deep models such as Transformers rely heavily on the availability of a large amount of annotated data, which is available only for prominent languages like English, Russian, German or Spanish #REF .\n sent1: For a majority of other languages with a minimal number of annotations, cross-lingual transfer learning #TARGET_REF has been proposed as a possible solution.\n sent2: This approach can transfer knowledge from the annotation-rich source language to low-resource or zero-resource target languages.\n sent3: Furthermore, multilingual models #REF can be used to mitigate the data scarcity problem.\n sent4: For example, LASER #REF ) used a bidirectional LSTM #REF encoder with a byte pair encoding vocabulary shared between languages.\n sent5: This work showed that joint training of multiple languages helped to improve the model performance for low-resource languages.\n sent6: LaBSE #REF used the mBERT #REF encoder pre-trained with masked language modelling and translation language modelling #REF tasks.\n sent7: It attempted to optimize the dual encoder translation ranking #REF loss during pre-training to achieve similar embedding for the same text in different languages.\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Second,",
                    "we",
                    "propose",
                    "to",
                    "directly",
                    "use",
                    "coreference",
                    "resolution",
                    "datasets",
                    "for",
                    "training",
                    "MRC",
                    "models",
                    "to",
                    "improve",
                    "their",
                    "coreference",
                    "reasoning."
                ],
                [
                    "We",
                    "automatically",
                    "create",
                    "a",
                    "question",
                    "whose",
                    "answer",
                    "is",
                    "a",
                    "coreferring",
                    "expression",
                    "m",
                    "1",
                    "using",
                    "the",
                    "BART",
                    "model",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "We",
                    "then",
                    "consider",
                    "this",
                    "question,",
                    "m",
                    "1",
                    "'s",
                    "antecedent,",
                    "and",
                    "the",
                    "corresponding",
                    "document",
                    "as",
                    "a",
                    "new",
                    "#REF",
                    "tuple."
                ],
                [
                    "This",
                    "data",
                    "helps",
                    "the",
                    "model",
                    "learning",
                    "to",
                    "resolve",
                    "the",
                    "coreference",
                    "relation",
                    "between",
                    "m",
                    "1",
                    "and",
                    "its",
                    "antecedent",
                    "to",
                    "answer",
                    "the",
                    "question."
                ],
                [
                    "We",
                    "show",
                    "that",
                    "incorporating",
                    "these",
                    "additional",
                    "data",
                    "improves",
                    "the",
                    "performance",
                    "of",
                    "the",
                    "state-of-the-art",
                    "models",
                    "on",
                    "our",
                    "new",
                    "evaluation",
                    "set."
                ]
            ],
            "context": [
                0,
                2,
                3,
                0,
                0
            ]
        },
        "input": "sent0: Second, we propose to directly use coreference resolution datasets for training MRC models to improve their coreference reasoning.\n sent1: We automatically create a question whose answer is a coreferring expression m 1 using the BART model #TARGET_REF .\n sent2: We then consider this question, m 1 's antecedent, and the corresponding document as a new #REF tuple.\n sent3: This data helps the model learning to resolve the coreference relation between m 1 and its antecedent to answer the question.\n sent4: We show that incorporating these additional data improves the performance of the state-of-the-art models on our new evaluation set.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "To",
                    "extract",
                    "the",
                    "features",
                    "regarding",
                    "the",
                    "network",
                    "object",
                    "classes,",
                    "we",
                    "applied",
                    "a",
                    "regularization",
                    "method",
                    "to",
                    "the",
                    "graph."
                ],
                [
                    "Regularization",
                    "is",
                    "a",
                    "kind",
                    "of",
                    "semi-supervised",
                    "(or",
                    "transductive)",
                    "classification",
                    "method",
                    "that",
                    "aims",
                    "to",
                    "find",
                    "a",
                    "set",
                    "of",
                    "labels,",
                    "minimizing",
                    "a",
                    "cost",
                    "function",
                    "and",
                    "satisfying",
                    "two",
                    "conditions:",
                    "(i)",
                    "the",
                    "method",
                    "needs",
                    "to",
                    "be",
                    "consistent",
                    "with",
                    "the",
                    "set",
                    "of",
                    "labels",
                    "manually",
                    "annotated",
                    "and",
                    "(ii)",
                    "the",
                    "method",
                    "needs",
                    "to",
                    "be",
                    "consistent",
                    "with",
                    "the",
                    "network",
                    "topology,",
                    "considering",
                    "that",
                    "nearest",
                    "neighbors",
                    "tend",
                    "to",
                    "have",
                    "the",
                    "same",
                    "labels",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                0,
                3
            ]
        },
        "input": "sent0: To extract the features regarding the network object classes, we applied a regularization method to the graph.\n sent1: Regularization is a kind of semi-supervised (or transductive) classification method that aims to find a set of labels, minimizing a cost function and satisfying two conditions: (i) the method needs to be consistent with the set of labels manually annotated and (ii) the method needs to be consistent with the network topology, considering that nearest neighbors tend to have the same labels #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "•",
                    "EasyEnsemble",
                    "#TARGET_REF",
                    "is",
                    "used",
                    "to",
                    "represent",
                    "a",
                    "tradition",
                    "approach",
                    "in",
                    "dealing",
                    "with",
                    "im-balanced",
                    "dataset."
                ],
                [
                    "For",
                    "the",
                    "vectorization,",
                    "we",
                    "trained",
                    "a",
                    "Sent2Vec",
                    "#REF",
                    "using",
                    "the",
                    "combined",
                    "1GB",
                    "texts",
                    "of",
                    "Vietnamese",
                    "Wikipedia",
                    "data",
                    "#REF",
                    "and",
                    "19",
                    "GB",
                    "texts",
                    "of",
                    "Vuong",
                    "(2018)."
                ]
            ],
            "context": [
                2,
                0
            ]
        },
        "input": "sent0: • EasyEnsemble #TARGET_REF is used to represent a tradition approach in dealing with im-balanced dataset.\n sent1: For the vectorization, we trained a Sent2Vec #REF using the combined 1GB texts of Vietnamese Wikipedia data #REF and 19 GB texts of Vuong (2018).\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "transliteration",
                    "of",
                    "text",
                    "and",
                    "audio",
                    "data",
                    "into",
                    "phonetic",
                    "representations",
                    "presents",
                    "several",
                    "other",
                    "challenges",
                    "related",
                    "to",
                    "potential",
                    "loss",
                    "of",
                    "information",
                    "or",
                    "injection",
                    "of",
                    "noise:",
                    "1."
                ],
                [
                    "Loss",
                    "of",
                    "suprasegmental",
                    "information:",
                    "In",
                    "some",
                    "languages,",
                    "meaning",
                    "may",
                    "be",
                    "encoded",
                    "through",
                    "tones,",
                    "or",
                    "pitch",
                    "changes",
                    "across",
                    "sounds",
                    "(aka",
                    "across",
                    "segments,",
                    "or",
                    "\"suprasegmental\")."
                ],
                [
                    "Particularly",
                    "for",
                    "tonal",
                    "languages",
                    "such",
                    "as",
                    "Mandarin",
                    "Chinese",
                    "[cmn],",
                    "this",
                    "loss",
                    "can",
                    "represent",
                    "a",
                    "significant",
                    "informational",
                    "loss",
                    "particularly",
                    "for",
                    "homophones",
                    "with",
                    "different",
                    "tones,",
                    "as",
                    "seen",
                    "in",
                    "(Amrhein",
                    "and",
                    "Sennrich,",
                    "2020)."
                ],
                [
                    "While",
                    "IPA",
                    "symbols",
                    "can",
                    "represent",
                    "these",
                    "intricacies,",
                    "it",
                    "adds",
                    "complexity",
                    "2."
                ],
                [
                    "Phone/phoneme",
                    "differences:",
                    "As",
                    "noted",
                    "in",
                    "#TARGET_REF",
                    ",",
                    "speech",
                    "sounds",
                    "which",
                    "are",
                    "physically",
                    "different",
                    "(different",
                    "phones),",
                    "may",
                    "be",
                    "perceived",
                    "as",
                    "the",
                    "same",
                    "(one",
                    "phoneme)",
                    "by",
                    "speakers",
                    "of",
                    "one",
                    "language,",
                    "but",
                    "these",
                    "same",
                    "sounds",
                    "could",
                    "perhaps",
                    "be",
                    "distinguished",
                    "by",
                    "speakers",
                    "of",
                    "another",
                    "language."
                ],
                [
                    "For",
                    "example,",
                    "the",
                    "French",
                    "words",
                    "words",
                    "bouche,",
                    "and",
                    "bûche",
                    "contain",
                    "phones",
                    "(/u/",
                    "vs.",
                    "/y/)",
                    "which",
                    "may",
                    "sound",
                    "\"the",
                    "same\"",
                    "to",
                    "English",
                    "speakers,",
                    "but",
                    "are",
                    "semantically",
                    "different",
                    "to",
                    "French",
                    "speakers."
                ],
                [
                    "In",
                    "other",
                    "words,",
                    "in",
                    "English,",
                    "both",
                    "phones",
                    "map",
                    "to",
                    "the",
                    "same",
                    "phoneme",
                    "perceptually."
                ],
                [
                    "As",
                    "the",
                    "Allosaurus",
                    "phone",
                    "recognizer",
                    "recognizes",
                    "the",
                    "actual",
                    "phones/sounds,",
                    "not",
                    "their",
                    "perceived",
                    "phonemes,",
                    "it",
                    "would",
                    "transcribe",
                    "these",
                    "two",
                    "phones",
                    "to",
                    "different",
                    "representations",
                    "even",
                    "for",
                    "English",
                    "speech."
                ],
                [
                    "This",
                    "can",
                    "be",
                    "mitigated",
                    "to",
                    "an",
                    "extent",
                    "by",
                    "customizing",
                    "the",
                    "output",
                    "of",
                    "Allosaurus",
                    "on",
                    "a",
                    "per-language",
                    "basis,",
                    "see",
                    "Sec."
                ],
                [
                    "4.3."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                1,
                3,
                3,
                0,
                0,
                0
            ]
        },
        "input": "sent0: The transliteration of text and audio data into phonetic representations presents several other challenges related to potential loss of information or injection of noise: 1.\n sent1: Loss of suprasegmental information: In some languages, meaning may be encoded through tones, or pitch changes across sounds (aka across segments, or \"suprasegmental\").\n sent2: Particularly for tonal languages such as Mandarin Chinese [cmn], this loss can represent a significant informational loss particularly for homophones with different tones, as seen in (Amrhein and Sennrich, 2020).\n sent3: While IPA symbols can represent these intricacies, it adds complexity 2.\n sent4: Phone/phoneme differences: As noted in #TARGET_REF , speech sounds which are physically different (different phones), may be perceived as the same (one phoneme) by speakers of one language, but these same sounds could perhaps be distinguished by speakers of another language.\n sent5: For example, the French words words bouche, and bûche contain phones (/u/ vs. /y/) which may sound \"the same\" to English speakers, but are semantically different to French speakers.\n sent6: In other words, in English, both phones map to the same phoneme perceptually.\n sent7: As the Allosaurus phone recognizer recognizes the actual phones/sounds, not their perceived phonemes, it would transcribe these two phones to different representations even for English speech.\n sent8: This can be mitigated to an extent by customizing the output of Allosaurus on a per-language basis, see Sec.\n sent9: 4.3.\n",
        "output": "{\"INFORMATION\": [\"sent4\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent5\", \"sent6\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Unsupervised",
                    "systems",
                    "Majority",
                    "of",
                    "the",
                    "unsupervised",
                    "WSD",
                    "systems",
                    "use",
                    "external",
                    "knowledge",
                    "bases",
                    "like",
                    "WordNet",
                    "#REF",
                    "and",
                    "BabelNet",
                    "#REF",
                    "."
                ],
                [
                    "For",
                    "each",
                    "input",
                    "word,",
                    "its",
                    "correct",
                    "meaning",
                    "according",
                    "to",
                    "the",
                    "context",
                    "can",
                    "be",
                    "found",
                    "using",
                    "graph-based",
                    "techniques",
                    "from",
                    "those",
                    "external",
                    "knowledge",
                    "bases."
                ],
                [
                    "However,",
                    "these",
                    "approaches",
                    "are",
                    "only",
                    "limited",
                    "to",
                    "the",
                    "languages",
                    "supported",
                    "by",
                    "used",
                    "knowledge",
                    "bases."
                ],
                [
                    "More",
                    "recent",
                    "works",
                    "like",
                    "Hettiarachchi",
                    "and",
                    "Ranasinghe",
                    "(2020a),",
                    "#REF",
                    "propose",
                    "to",
                    "use",
                    "stacked",
                    "word",
                    "embeddings",
                    "#REF",
                    "obtained",
                    "by",
                    "general",
                    "purpose",
                    "pretrained",
                    "contextualised",
                    "word",
                    "embedding",
                    "models",
                    "such",
                    "as",
                    "BERT",
                    "#REF",
                    "and",
                    "Flair",
                    "#TARGET_REF",
                    "for",
                    "unsupervised",
                    "WSD."
                ],
                [
                    "Despite",
                    "their",
                    "ability",
                    "to",
                    "scale",
                    "over",
                    "different",
                    "languages,",
                    "unsupervised",
                    "approaches",
                    "fall",
                    "behind",
                    "supervised",
                    "systems",
                    "in",
                    "terms",
                    "of",
                    "accuracy."
                ]
            ],
            "context": [
                0,
                0,
                0,
                3,
                0
            ]
        },
        "input": "sent0: Unsupervised systems Majority of the unsupervised WSD systems use external knowledge bases like WordNet #REF and BabelNet #REF .\n sent1: For each input word, its correct meaning according to the context can be found using graph-based techniques from those external knowledge bases.\n sent2: However, these approaches are only limited to the languages supported by used knowledge bases.\n sent3: More recent works like Hettiarachchi and Ranasinghe (2020a), #REF propose to use stacked word embeddings #REF obtained by general purpose pretrained contextualised word embedding models such as BERT #REF and Flair #TARGET_REF for unsupervised WSD.\n sent4: Despite their ability to scale over different languages, unsupervised approaches fall behind supervised systems in terms of accuracy.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Comparing",
                    "Eq."
                ],
                [
                    "6",
                    "with",
                    "Eq."
                ],
                [
                    "3,",
                    "the",
                    "major",
                    "difference",
                    "is",
                    "that",
                    "during",
                    "inference",
                    "the",
                    "model",
                    "makes",
                    "new",
                    "predictions",
                    "based",
                    "on",
                    "its",
                    "own",
                    "previous",
                    "predictions",
                    "S",
                    "&lt,t",
                    "instead",
                    "of",
                    "the",
                    "reference",
                    "S",
                    "*",
                    "&lt,t",
                    "."
                ],
                [
                    "As",
                    "a",
                    "result,",
                    "even",
                    "if",
                    "the",
                    "generation",
                    "model",
                    "g",
                    "achieves",
                    "very",
                    "high",
                    "accuracy",
                    "w.r.t."
                ],
                [
                    "Eq."
                ],
                [
                    "3,",
                    "once",
                    "S",
                    "&lt,t",
                    "starts",
                    "to",
                    "deviate",
                    "from",
                    "S",
                    "*",
                    ",",
                    "there",
                    "is",
                    "the",
                    "risk",
                    "that",
                    "the",
                    "performance",
                    "of",
                    "g",
                    "will",
                    "significantly",
                    "degrade."
                ],
                [
                    "This",
                    "problem",
                    "has",
                    "been",
                    "identified",
                    "as",
                    "the",
                    "exposure",
                    "bias",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                0,
                0,
                0,
                3,
                3,
                3,
                1
            ]
        },
        "input": "sent0: Comparing Eq.\n sent1: 6 with Eq.\n sent2: 3, the major difference is that during inference the model makes new predictions based on its own previous predictions S &lt,t instead of the reference S * &lt,t .\n sent3: As a result, even if the generation model g achieves very high accuracy w.r.t.\n sent4: Eq.\n sent5: 3, once S &lt,t starts to deviate from S * , there is the risk that the performance of g will significantly degrade.\n sent6: This problem has been identified as the exposure bias #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [\"sent6\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent3\", \"sent4\", \"sent5\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "During",
                    "the",
                    "fine-tuning",
                    "phase,",
                    "we",
                    "first",
                    "apply",
                    "finetuning",
                    "on",
                    "a",
                    "small",
                    "spoken",
                    "corpus."
                ],
                [
                    "For",
                    "better",
                    "domain",
                    "adaptation,",
                    "we",
                    "adopt",
                    "mixed",
                    "fine-tuning",
                    "#REF",
                    ",",
                    "which",
                    "trains",
                    "on",
                    "a",
                    "mixed",
                    "dataset",
                    "that",
                    "includes",
                    "a",
                    "subsampled",
                    "general",
                    "corpus",
                    "and",
                    "an",
                    "upsampled",
                    "spoken",
                    "corpus."
                ],
                [
                    "Thirdly,",
                    "we",
                    "propose",
                    "a",
                    "method",
                    "called",
                    "\"in-domain",
                    "mixed",
                    "fine-tuning\",",
                    "which",
                    "further",
                    "improve",
                    "the",
                    "BLEU",
                    "score",
                    "than",
                    "mixed",
                    "finetuning."
                ],
                [
                    "Specifically,",
                    "inspired",
                    "by",
                    "in-domain",
                    "data",
                    "filtering",
                    "#TARGET_REF",
                    ",",
                    "we",
                    "mixed",
                    "upsampled",
                    "spoken",
                    "data",
                    "with",
                    "selected",
                    "in-domain",
                    "data",
                    "from",
                    "general",
                    "corpus",
                    "rather",
                    "than",
                    "random",
                    "subsampled."
                ]
            ],
            "context": [
                0,
                0,
                3,
                2
            ]
        },
        "input": "sent0: During the fine-tuning phase, we first apply finetuning on a small spoken corpus.\n sent1: For better domain adaptation, we adopt mixed fine-tuning #REF , which trains on a mixed dataset that includes a subsampled general corpus and an upsampled spoken corpus.\n sent2: Thirdly, we propose a method called \"in-domain mixed fine-tuning\", which further improve the BLEU score than mixed finetuning.\n sent3: Specifically, inspired by in-domain data filtering #TARGET_REF , we mixed upsampled spoken data with selected in-domain data from general corpus rather than random subsampled.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent3\"], \"BACKGROUND\": [\"sent2\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Word",
                    "Sense",
                    "Disambiguation",
                    "(WSD)-based",
                    "approaches",
                    "were",
                    "widely",
                    "used",
                    "by",
                    "previous",
                    "research",
                    "to",
                    "tackle",
                    "this",
                    "problem",
                    "#REF",
                    "."
                ],
                [
                    "WSD",
                    "associates",
                    "the",
                    "word",
                    "in",
                    "a",
                    "text",
                    "with",
                    "its",
                    "correct",
                    "meaning",
                    "from",
                    "a",
                    "predefined",
                    "sense",
                    "inventory",
                    "#REF",
                    "."
                ],
                [
                    "As",
                    "such",
                    "inventories,",
                    "WordNet",
                    "#REF",
                    "and",
                    "Babel-Net",
                    "#TARGET_REF",
                    "were",
                    "commonly",
                    "used."
                ],
                [
                    "However,",
                    "these",
                    "approaches",
                    "fail",
                    "to",
                    "generalise",
                    "into",
                    "different",
                    "languages",
                    "as",
                    "these",
                    "inventories",
                    "are",
                    "often",
                    "limited",
                    "to",
                    "high",
                    "resource",
                    "languages."
                ],
                [
                    "Targeting",
                    "this",
                    "gap,",
                    "SemEval-2021",
                    "Task",
                    "2:",
                    "Multilingual",
                    "and",
                    "Cross-lingual",
                    "Word-in-Context",
                    "Disambiguation",
                    "is",
                    "designed",
                    "to",
                    "capture",
                    "the",
                    "word",
                    "sense",
                    "without",
                    "relying",
                    "on",
                    "fixed",
                    "sense",
                    "inventories",
                    "in",
                    "both",
                    "monolingual",
                    "and",
                    "cross-lingual",
                    "setting."
                ],
                [
                    "In",
                    "summary,",
                    "this",
                    "task",
                    "is",
                    "designed",
                    "as",
                    "a",
                    "binary",
                    "classification",
                    "problem",
                    "which",
                    "predicts",
                    "whether",
                    "the",
                    "target",
                    "word",
                    "has",
                    "the",
                    "same",
                    "meaning",
                    "or",
                    "different",
                    "meaning",
                    "in",
                    "different",
                    "contexts",
                    "of",
                    "the",
                    "same",
                    "language",
                    "(monolingual",
                    "setting)",
                    "or",
                    "different",
                    "languages",
                    "(cross-lingual",
                    "setting)."
                ]
            ],
            "context": [
                0,
                3,
                2,
                3,
                0,
                0
            ]
        },
        "input": "sent0: Word Sense Disambiguation (WSD)-based approaches were widely used by previous research to tackle this problem #REF .\n sent1: WSD associates the word in a text with its correct meaning from a predefined sense inventory #REF .\n sent2: As such inventories, WordNet #REF and Babel-Net #TARGET_REF were commonly used.\n sent3: However, these approaches fail to generalise into different languages as these inventories are often limited to high resource languages.\n sent4: Targeting this gap, SemEval-2021 Task 2: Multilingual and Cross-lingual Word-in-Context Disambiguation is designed to capture the word sense without relying on fixed sense inventories in both monolingual and cross-lingual setting.\n sent5: In summary, this task is designed as a binary classification problem which predicts whether the target word has the same meaning or different meaning in different contexts of the same language (monolingual setting) or different languages (cross-lingual setting).\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": [\"sent1\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Goal",
                    "oriented",
                    "Dialogue."
                ],
                [
                    "Sub-tasks",
                    "within",
                    "the",
                    "overall",
                    "task",
                    "of",
                    "goal",
                    "oriented",
                    "dialogue,",
                    "such",
                    "as",
                    "dialogue",
                    "state",
                    "management",
                    "#REF",
                    "and",
                    "response",
                    "generation",
                    "#REF",
                    "have",
                    "used",
                    "RL",
                    "to",
                    "boost",
                    "performance."
                ],
                [
                    "As",
                    "noted",
                    "by",
                    "#REF",
                    ",",
                    "the",
                    "negotiation",
                    "tasks",
                    "of",
                    "(Yarats",
                    "and",
                    "#TARGET_REF",
                    ",",
                    "where",
                    "two",
                    "agents",
                    "are",
                    "trying",
                    "to",
                    "convince",
                    "each",
                    "other",
                    "to",
                    "perform",
                    "certain",
                    "actions,",
                    "are",
                    "related",
                    "to",
                    "the",
                    "tasks",
                    "in",
                    "LIGHT-Quests."
                ],
                [
                    "These",
                    "works",
                    "all",
                    "lack",
                    "environment",
                    "grounding."
                ]
            ],
            "context": [
                0,
                0,
                1,
                2
            ]
        },
        "input": "sent0: Goal oriented Dialogue.\n sent1: Sub-tasks within the overall task of goal oriented dialogue, such as dialogue state management #REF and response generation #REF have used RL to boost performance.\n sent2: As noted by #REF , the negotiation tasks of (Yarats and #TARGET_REF , where two agents are trying to convince each other to perform certain actions, are related to the tasks in LIGHT-Quests.\n sent3: These works all lack environment grounding.\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [\"sent3\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "One",
                    "frequently",
                    "used",
                    "NN",
                    "architecture",
                    "#TARGET_REF",
                    ")",
                    "that",
                    "maximizes",
                    "Eq."
                ],
                [
                    "(",
                    "3)",
                    "is",
                    "the",
                    "O2M",
                    "model",
                    "trained",
                    "with",
                    "multi-task",
                    "learning."
                ],
                [
                    "Fig."
                ],
                [
                    "1(a)",
                    "shows",
                    "the",
                    "architecture",
                    "of",
                    "the",
                    "model."
                ],
                [
                    "The",
                    "O2M",
                    "model",
                    "outputs",
                    "several",
                    "types",
                    "of",
                    "sequences",
                    "independently."
                ],
                [
                    "In",
                    "other",
                    "words,",
                    "multi-task",
                    "learning",
                    "is",
                    "derived",
                    "by",
                    "assuming",
                    "conditional",
                    "independence",
                    "of",
                    "output",
                    "token",
                    "types",
                    "for",
                    "Eq."
                ],
                [
                    "(",
                    "3),",
                    "as",
                    "follows:"
                ]
            ],
            "context": [
                2,
                2,
                3,
                3,
                0,
                0,
                0
            ]
        },
        "input": "sent0: One frequently used NN architecture #TARGET_REF ) that maximizes Eq.\n sent1: ( 3) is the O2M model trained with multi-task learning.\n sent2: Fig.\n sent3: 1(a) shows the architecture of the model.\n sent4: The O2M model outputs several types of sequences independently.\n sent5: In other words, multi-task learning is derived by assuming conditional independence of output token types for Eq.\n sent6: ( 3), as follows:\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\", \"sent1\"], \"BACKGROUND\": [\"sent2\", \"sent3\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "WorldTree."
                ],
                [
                    "A",
                    "number",
                    "of",
                    "approaches",
                    "have",
                    "been",
                    "proposed",
                    "for",
                    "the",
                    "explanation",
                    "regeneration",
                    "task",
                    "on",
                    "WorldTree,",
                    "including",
                    "those",
                    "from",
                    "previous",
                    "iterations",
                    "of",
                    "this",
                    "shared",
                    "task."
                ],
                [
                    "These",
                    "approaches",
                    "adopt",
                    "a",
                    "set",
                    "of",
                    "diverse",
                    "techniques",
                    "ranging",
                    "from",
                    "graph-based",
                    "learning",
                    "#REF",
                    ",",
                    "to",
                    "Transformer-based",
                    "language",
                    "models",
                    "#TARGET_REF",
                    ",",
                    "Integer",
                    "Linear",
                    "Programming",
                    "#REF",
                    ",",
                    "and",
                    "sparse",
                    "retrieval",
                    "models",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                0,
                0,
                2
            ]
        },
        "input": "sent0: WorldTree.\n sent1: A number of approaches have been proposed for the explanation regeneration task on WorldTree, including those from previous iterations of this shared task.\n sent2: These approaches adopt a set of diverse techniques ranging from graph-based learning #REF , to Transformer-based language models #TARGET_REF , Integer Linear Programming #REF , and sparse retrieval models #REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Post-processing",
                    "After",
                    "the",
                    "neural",
                    "model",
                    "predicts",
                    "a",
                    "fragment",
                    "and",
                    "a",
                    "word",
                    "sense",
                    "for",
                    "each",
                    "token,",
                    "we",
                    "assemble",
                    "these",
                    "predictions",
                    "into",
                    "a",
                    "complete",
                    "clause",
                    "list",
                    "by",
                    "choosing",
                    "unique",
                    "new",
                    "names",
                    "for",
                    "discourse",
                    "referents",
                    "with",
                    "index",
                    "0",
                    "and",
                    "unifying",
                    "other",
                    "discourse",
                    "referents",
                    "with",
                    "them",
                    "according",
                    "to",
                    "their",
                    "relative",
                    "indices."
                ],
                [
                    "We",
                    "also",
                    "replace",
                    "DUMMY",
                    "strings",
                    "in",
                    "clauses",
                    "by",
                    "the",
                    "predicted",
                    "word",
                    "senses",
                    "and",
                    "by",
                    "symbols",
                    "for",
                    "names,",
                    "cardinalities,",
                    "and",
                    "date/time",
                    "expressions,",
                    "which",
                    "are",
                    "predicted",
                    "from",
                    "the",
                    "tokens",
                    "by",
                    "a",
                    "rule-based",
                    "system",
                    "similar",
                    "to",
                    "that",
                    "of",
                    "#REF",
                    "."
                ],
                [
                    "For",
                    "example,",
                    "for",
                    "the",
                    "proper",
                    "name",
                    "Tom",
                    "it",
                    "predicts",
                    "the",
                    "symbol",
                    "\"tom\",",
                    "for",
                    "the",
                    "numeral",
                    "two",
                    "it",
                    "predicts",
                    "\"2\",",
                    "and",
                    "for",
                    "the",
                    "time",
                    "expression",
                    "five",
                    "o'clock,",
                    "it",
                    "predicts",
                    "\"17:00\"."
                ],
                [
                    "Special",
                    "clauses",
                    "like",
                    "b1",
                    "\"speaker\"",
                    "x1",
                    "and",
                    "b1",
                    "\"hearer\"",
                    "x1",
                    "are",
                    "removed",
                    "and",
                    "the",
                    "corresponding",
                    "referent",
                    "(x1",
                    "in",
                    "the",
                    "example)",
                    "replaced",
                    "by",
                    "the",
                    "symbols",
                    "\"speaker\"",
                    "and",
                    "\"hearer\"."
                ],
                [
                    "Finally,",
                    "we",
                    "use",
                    "a",
                    "set",
                    "of",
                    "postprocessing",
                    "rules",
                    "similar",
                    "to",
                    "that",
                    "of",
                    "#TARGET_REF",
                    "to",
                    "ensure",
                    "the",
                    "validity",
                    "of",
                    "the",
                    "resulting",
                    "DRS:",
                    "if",
                    "there",
                    "is",
                    "a",
                    "loop",
                    "in",
                    "the",
                    "subordination",
                    "relation",
                    "among",
                    "boxes,",
                    "an",
                    "arbitrary",
                    "box",
                    "in",
                    "the",
                    "loop",
                    "is",
                    "chosen,",
                    "and",
                    "all",
                    "its",
                    "clauses",
                    "are",
                    "removed",
                    "to",
                    "break",
                    "the",
                    "loop",
                    "(cf."
                ],
                [
                    "Figure",
                    "9"
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                3,
                3
            ]
        },
        "input": "sent0: Post-processing After the neural model predicts a fragment and a word sense for each token, we assemble these predictions into a complete clause list by choosing unique new names for discourse referents with index 0 and unifying other discourse referents with them according to their relative indices.\n sent1: We also replace DUMMY strings in clauses by the predicted word senses and by symbols for names, cardinalities, and date/time expressions, which are predicted from the tokens by a rule-based system similar to that of #REF .\n sent2: For example, for the proper name Tom it predicts the symbol \"tom\", for the numeral two it predicts \"2\", and for the time expression five o'clock, it predicts \"17:00\".\n sent3: Special clauses like b1 \"speaker\" x1 and b1 \"hearer\" x1 are removed and the corresponding referent (x1 in the example) replaced by the symbols \"speaker\" and \"hearer\".\n sent4: Finally, we use a set of postprocessing rules similar to that of #TARGET_REF to ensure the validity of the resulting DRS: if there is a loop in the subordination relation among boxes, an arbitrary box in the loop is chosen, and all its clauses are removed to break the loop (cf.\n sent5: Figure 9\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent4\", \"sent5\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Transformer-based",
                    "language",
                    "models",
                    "such",
                    "as",
                    "BERT",
                    "#TARGET_REF",
                    "are",
                    "pre-trained",
                    "on",
                    "largescale",
                    "data",
                    "collections",
                    "sourced",
                    "from",
                    "Wikipedia",
                    "or",
                    "Common",
                    "Crawl",
                    "(CC)",
                    "with",
                    "one",
                    "or",
                    "multiple",
                    "training",
                    "objectives",
                    "(masked",
                    "language",
                    "modeling,",
                    "next",
                    "sentence",
                    "or",
                    "sentence",
                    "order",
                    "prediction)."
                ],
                [
                    "This",
                    "pretraining",
                    "can",
                    "be",
                    "followed",
                    "by",
                    "supervised",
                    "fine-tuning",
                    "according",
                    "to",
                    "the",
                    "tasks,",
                    "whether",
                    "generatives",
                    "(machine",
                    "translation,",
                    "abstractive",
                    "summarization)",
                    "or",
                    "discriminatives",
                    "(classification,",
                    "question-answering)."
                ],
                [
                    "The",
                    "ensuing",
                    "fine-tuning",
                    "phase",
                    "allows",
                    "for",
                    "better",
                    "initialization",
                    "of",
                    "the",
                    "models",
                    "parameters",
                    "while",
                    "requiring",
                    "less",
                    "task-specific",
                    "data",
                    "so",
                    "as",
                    "to",
                    "make",
                    "the",
                    "training",
                    "of",
                    "subsequent",
                    "tasks",
                    "faster."
                ]
            ],
            "context": [
                3,
                2,
                0
            ]
        },
        "input": "sent0: Transformer-based language models such as BERT #TARGET_REF are pre-trained on largescale data collections sourced from Wikipedia or Common Crawl (CC) with one or multiple training objectives (masked language modeling, next sentence or sentence order prediction).\n sent1: This pretraining can be followed by supervised fine-tuning according to the tasks, whether generatives (machine translation, abstractive summarization) or discriminatives (classification, question-answering).\n sent2: The ensuing fine-tuning phase allows for better initialization of the models parameters while requiring less task-specific data so as to make the training of subsequent tasks faster.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Producing",
                    "K",
                    "outputs",
                    "during",
                    "inference."
                ],
                [
                    "In",
                    "order",
                    "to",
                    "generate",
                    "K",
                    "different",
                    "outputs",
                    "on",
                    "test",
                    "set,",
                    "we",
                    "follow",
                    "#TARGET_REF",
                    "to",
                    "enumerate",
                    "all",
                    "latent",
                    "variables",
                    "z",
                    "and",
                    "then",
                    "greedily",
                    "decoding",
                    "each",
                    "token",
                    "by",
                    "ŷt",
                    "=",
                    "arg",
                    "max",
                    "p(y|ŷ",
                    "1:t−1",
                    ",",
                    "z,",
                    "x)."
                ],
                [
                    "In",
                    "other",
                    "words,",
                    "we",
                    "ask",
                    "each",
                    "expert",
                    "to",
                    "seek",
                    "different",
                    "sets",
                    "of",
                    "concepts",
                    "on",
                    "the",
                    "knowledge",
                    "graph,",
                    "and",
                    "use",
                    "the",
                    "selected",
                    "concepts",
                    "to",
                    "generate",
                    "K",
                    "different",
                    "outputs."
                ],
                [
                    "Notably,",
                    "this",
                    "decoding",
                    "procedure",
                    "is",
                    "efficient",
                    "and",
                    "easily",
                    "parallelizable."
                ],
                [
                    "Furthermore,",
                    "to",
                    "make",
                    "fair",
                    "comparisons",
                    "with",
                    "sampling-based",
                    "methods,",
                    "we",
                    "use",
                    "greedy",
                    "decoding",
                    "without",
                    "any",
                    "sampling",
                    "strategy."
                ]
            ],
            "context": [
                0,
                2,
                2,
                2,
                2
            ]
        },
        "input": "sent0: Producing K outputs during inference.\n sent1: In order to generate K different outputs on test set, we follow #TARGET_REF to enumerate all latent variables z and then greedily decoding each token by ŷt = arg max p(y|ŷ 1:t−1 , z, x).\n sent2: In other words, we ask each expert to seek different sets of concepts on the knowledge graph, and use the selected concepts to generate K different outputs.\n sent3: Notably, this decoding procedure is efficient and easily parallelizable.\n sent4: Furthermore, to make fair comparisons with sampling-based methods, we use greedy decoding without any sampling strategy.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\", \"sent2\", \"sent3\", \"sent4\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "approach",
                    "to",
                    "text",
                    "grammar",
                    "taken",
                    "here",
                    "is",
                    "in",
                    "many",
                    "ways",
                    "similar",
                    "to",
                    "that",
                    "of",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "However,",
                    "he",
                    "opts",
                    "to",
                    "treat",
                    "punctuation",
                    "marks",
                    "as",
                    "clitics",
                    "on",
                    "words",
                    "which",
                    "introduce",
                    "additional",
                    "featural",
                    "information",
                    "into",
                    "standard",
                    "syntactic",
                    "rules."
                ],
                [
                    "Thus,",
                    "his",
                    "grammar",
                    "is",
                    "thoroughly",
                    "integrated",
                    "and",
                    "it",
                    "would",
                    "be",
                    "harder",
                    "to",
                    "extract",
                    "an",
                    "independent",
                    "text",
                    "grammar",
                    "or",
                    "build",
                    "a",
                    "modular",
                    "semantics."
                ],
                [
                    "The",
                    "coverage",
                    "of",
                    "the",
                    "integrated",
                    "version",
                    "of",
                    "the",
                    "text",
                    "grammar",
                    "is",
                    "described",
                    "in",
                    "more",
                    "detail",
                    "in",
                    "#REF",
                    "."
                ],
                [
                    "#REF",
                    "trained",
                    "on",
                    "text",
                    "tagged",
                    "with",
                    "a",
                    "slightly",
                    "modified",
                    "version",
                    "of",
                    "CLAWS-II",
                    "labels",
                    "#REF",
                    "."
                ],
                [
                    "#REF",
                    "that",
                    "in",
                    "practice",
                    "NL",
                    "grammars",
                    "do",
                    "not",
                    "evince",
                    "worst-case",
                    "parsing",
                    "complexity."
                ]
            ],
            "context": [
                2,
                2,
                2,
                0,
                0,
                0
            ]
        },
        "input": "sent0: The approach to text grammar taken here is in many ways similar to that of #TARGET_REF .\n sent1: However, he opts to treat punctuation marks as clitics on words which introduce additional featural information into standard syntactic rules.\n sent2: Thus, his grammar is thoroughly integrated and it would be harder to extract an independent text grammar or build a modular semantics.\n sent3: The coverage of the integrated version of the text grammar is described in more detail in #REF .\n sent4: #REF trained on text tagged with a slightly modified version of CLAWS-II labels #REF .\n sent5: #REF that in practice NL grammars do not evince worst-case parsing complexity.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\", \"sent1\", \"sent2\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "Baseline",
                    "method",
                    "only",
                    "takes",
                    "image",
                    "feature",
                    "as",
                    "input",
                    "while",
                    "the",
                    "+Trace",
                    "model",
                    "take",
                    "image",
                    "feature",
                    "and",
                    "trace",
                    "both",
                    "as",
                    "input."
                ],
                [
                    "They",
                    "employ",
                    "the",
                    "architecture",
                    "in",
                    "#TARGET_REF",
                    "with",
                    "a",
                    "few",
                    "minor",
                    "differences."
                ],
                [
                    "First,",
                    "they",
                    "set",
                    "the",
                    "number",
                    "of",
                    "Transformers'",
                    "layers",
                    "for",
                    "both",
                    "the",
                    "encoder",
                    "and",
                    "the",
                    "decoder",
                    "to",
                    "2",
                    "instead",
                    "of",
                    "6."
                ],
                [
                    "Second,",
                    "their",
                    "projection",
                    "layers",
                    "also",
                    "consist",
                    "of",
                    "layer",
                    "normalization",
                    "#REF",
                    "."
                ],
                [
                    "Third,",
                    "they",
                    "set",
                    "the",
                    "maximum",
                    "number",
                    "of",
                    "iterations",
                    "to",
                    "150k."
                ],
                [
                    "Finally,",
                    "they",
                    "allow",
                    "the",
                    "maximum",
                    "number",
                    "of",
                    "target",
                    "captions",
                    "to",
                    "be",
                    "as",
                    "long",
                    "as",
                    "225",
                    "to",
                    "account",
                    "for",
                    "the",
                    "narration's",
                    "longer",
                    "nature."
                ],
                [
                    "LoopCAG",
                    "methods",
                    "Our",
                    "model",
                    "comprises",
                    "of",
                    "four",
                    "components:",
                    "1)",
                    "the",
                    "transformer",
                    "encoderdecoder",
                    "framework,",
                    "2)",
                    "the",
                    "trace",
                    "input,",
                    "3)",
                    "Attention",
                    "Guidance(+AG",
                    "for",
                    "short)",
                    "grounding",
                    "loss,",
                    "4)",
                    "Contrastive",
                    "constraints(+C",
                    "for",
                    "short)."
                ]
            ],
            "context": [
                0,
                2,
                1,
                1,
                2,
                2,
                0
            ]
        },
        "input": "sent0: The Baseline method only takes image feature as input while the +Trace model take image feature and trace both as input.\n sent1: They employ the architecture in #TARGET_REF with a few minor differences.\n sent2: First, they set the number of Transformers' layers for both the encoder and the decoder to 2 instead of 6.\n sent3: Second, their projection layers also consist of layer normalization #REF .\n sent4: Third, they set the maximum number of iterations to 150k.\n sent5: Finally, they allow the maximum number of target captions to be as long as 225 to account for the narration's longer nature.\n sent6: LoopCAG methods Our model comprises of four components: 1) the transformer encoderdecoder framework, 2) the trace input, 3) Attention Guidance(+AG for short) grounding loss, 4) Contrastive constraints(+C for short).\n",
        "output": "{\"INFORMATION\": [\"sent2\", \"sent3\"], \"PERCEPTION\": [\"sent1\", \"sent4\", \"sent5\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "In",
                    "experiments",
                    "carried",
                    "out",
                    "on",
                    "PropBank",
                    "data",
                    "using",
                    "gold",
                    "standard",
                    "syntactic",
                    "parse",
                    "trees,",
                    "extended",
                    "syntactic",
                    "features",
                    "such",
                    "as",
                    "Path",
                    "Trigram",
                    "and",
                    "Path",
                    "Abbreviations",
                    "were",
                    "found",
                    "to",
                    "have",
                    "the",
                    "highest",
                    "contribution",
                    "to",
                    "system",
                    "performance",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Another",
                    "feature,",
                    "Verb",
                    "Cluster,",
                    "was",
                    "also",
                    "found",
                    "to",
                    "be",
                    "most",
                    "useful",
                    "by",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                3,
                0
            ]
        },
        "input": "sent0: In experiments carried out on PropBank data using gold standard syntactic parse trees, extended syntactic features such as Path Trigram and Path Abbreviations were found to have the highest contribution to system performance #TARGET_REF .\n sent1: Another feature, Verb Cluster, was also found to be most useful by #REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Existing",
                    "diversity-promoting",
                    "methods",
                    "only",
                    "varied",
                    "the",
                    "language",
                    "styles",
                    "and",
                    "failed",
                    "to",
                    "perform",
                    "different",
                    "knowledge",
                    "reasoning",
                    "to",
                    "generate",
                    "diverse",
                    "contents",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "Here,",
                    "incorporating",
                    "commonsense",
                    "KG",
                    "is",
                    "essential",
                    "for",
                    "the",
                    "generative",
                    "reasoning",
                    "(GR)",
                    "tasks",
                    "because",
                    "the",
                    "KG",
                    "cannot",
                    "only",
                    "augment",
                    "the",
                    "limited",
                    "information",
                    "in",
                    "the",
                    "input",
                    "text,",
                    "but",
                    "also",
                    "provide",
                    "a",
                    "rich",
                    "searching",
                    "space",
                    "for",
                    "knowledge",
                    "reasoning."
                ],
                [
                    "Therefore,",
                    "we",
                    "propose",
                    "to",
                    "employ",
                    "commonsense",
                    "KG",
                    "to",
                    "play",
                    "the",
                    "central",
                    "role",
                    "of",
                    "performing",
                    "diverse",
                    "knowledge",
                    "reasoning,",
                    "then",
                    "use",
                    "different",
                    "sets",
                    "of",
                    "selected",
                    "concepts",
                    "to",
                    "produce",
                    "diverse",
                    "outputs."
                ]
            ],
            "context": [
                3,
                0,
                0
            ]
        },
        "input": "sent0: Existing diversity-promoting methods only varied the language styles and failed to perform different knowledge reasoning to generate diverse contents #TARGET_REF .\n sent1: Here, incorporating commonsense KG is essential for the generative reasoning (GR) tasks because the KG cannot only augment the limited information in the input text, but also provide a rich searching space for knowledge reasoning.\n sent2: Therefore, we propose to employ commonsense KG to play the central role of performing diverse knowledge reasoning, then use different sets of selected concepts to produce diverse outputs.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "would",
                    "also",
                    "like",
                    "to",
                    "validate",
                    "our",
                    "methods",
                    "on",
                    "a",
                    "variety",
                    "of",
                    "other",
                    "data",
                    "sets",
                    "and",
                    "tasks."
                ],
                [
                    "We",
                    "selected",
                    "the",
                    "MasakhaNER",
                    "dataset",
                    "for",
                    "evaluation",
                    "because",
                    "we",
                    "specifically",
                    "wished",
                    "to",
                    "evaluate",
                    "results",
                    "on",
                    "ac-tual",
                    "low-resource",
                    "languages",
                    "supported",
                    "by",
                    "both",
                    "Allosaurus",
                    "and",
                    "Epitran."
                ],
                [
                    "While",
                    "there",
                    "are",
                    "still,",
                    "we",
                    "argue,",
                    "detectable",
                    "improvements",
                    "in",
                    "downstream",
                    "results",
                    "with",
                    "our",
                    "method,",
                    "further",
                    "work",
                    "would",
                    "benefit",
                    "from",
                    "additional",
                    "evaluations",
                    "on",
                    "other",
                    "data",
                    "sets",
                    "or",
                    "tasks."
                ],
                [
                    "In",
                    "particular,",
                    "the",
                    "Swahili",
                    "News",
                    "Classification",
                    "corpus",
                    "#TARGET_REF",
                    "corpus",
                    "may",
                    "provide",
                    "a",
                    "useful",
                    "evaluation."
                ]
            ],
            "context": [
                3,
                3,
                0,
                1
            ]
        },
        "input": "sent0: We would also like to validate our methods on a variety of other data sets and tasks.\n sent1: We selected the MasakhaNER dataset for evaluation because we specifically wished to evaluate results on ac-tual low-resource languages supported by both Allosaurus and Epitran.\n sent2: While there are still, we argue, detectable improvements in downstream results with our method, further work would benefit from additional evaluations on other data sets or tasks.\n sent3: In particular, the Swahili News Classification corpus #TARGET_REF corpus may provide a useful evaluation.\n",
        "output": "{\"INFORMATION\": [\"sent3\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Binary",
                    "refers",
                    "to",
                    "positive",
                    "and",
                    "negative,",
                    "and",
                    "ternary",
                    "refers",
                    "to",
                    "positive,",
                    "negativeand",
                    "neutral."
                ],
                [
                    "For",
                    "binary",
                    "evaluations",
                    "we",
                    "categorized",
                    "anger,",
                    "disgust,",
                    "fear,",
                    "and",
                    "sadness",
                    "as",
                    "negative,",
                    "and",
                    "anticipation,",
                    "joy,",
                    "and",
                    "trust",
                    "as",
                    "positive."
                ],
                [
                    "Surprise",
                    "was",
                    "either",
                    "discarded",
                    "or",
                    "included",
                    "as",
                    "a",
                    "separate",
                    "category",
                    "(see",
                    "table",
                    "7)."
                ],
                [
                    "For",
                    "this",
                    "classification",
                    "task",
                    "BERT",
                    "achieved",
                    "macro",
                    "f1",
                    "scores",
                    "of",
                    "0.536",
                    "and",
                    "accuracies",
                    "of",
                    "0.544."
                ],
                [
                    "This",
                    "is",
                    "comparable",
                    "to",
                    "other",
                    "similar",
                    "datasets",
                    "when",
                    "classes",
                    "are",
                    "merged",
                    "(e.g."
                ],
                [
                    "#TARGET_REF",
                    ")."
                ]
            ],
            "context": [
                0,
                0,
                0,
                3,
                3,
                1
            ]
        },
        "input": "sent0: Binary refers to positive and negative, and ternary refers to positive, negativeand neutral.\n sent1: For binary evaluations we categorized anger, disgust, fear, and sadness as negative, and anticipation, joy, and trust as positive.\n sent2: Surprise was either discarded or included as a separate category (see table 7).\n sent3: For this classification task BERT achieved macro f1 scores of 0.536 and accuracies of 0.544.\n sent4: This is comparable to other similar datasets when classes are merged (e.g.\n sent5: #TARGET_REF ).\n",
        "output": "{\"INFORMATION\": [\"sent5\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent3\", \"sent4\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "sufficient",
                    "to",
                    "solve",
                    "the",
                    "task,",
                    "as",
                    "was",
                    "also",
                    "argued",
                    "in",
                    "#TARGET_REF",
                    "and",
                    "#REF",
                    "."
                ],
                [
                    "Finally,",
                    "we",
                    "note",
                    "an",
                    "interesting",
                    "trend",
                    "that",
                    "the",
                    "decoupled",
                    "variant",
                    "favors",
                    "recall",
                    "whereas",
                    "the",
                    "coupled",
                    "variant",
                    "favors",
                    "precision,",
                    "across",
                    "all",
                    "models."
                ],
                [
                    "In",
                    "summary,",
                    "all",
                    "models",
                    "perform",
                    "substantially",
                    "below",
                    "human",
                    "agreement,",
                    "leaving",
                    "a",
                    "large",
                    "room",
                    "for",
                    "improvement."
                ]
            ],
            "context": [
                3,
                0,
                0
            ]
        },
        "input": "sent0: sufficient to solve the task, as was also argued in #TARGET_REF and #REF .\n sent1: Finally, we note an interesting trend that the decoupled variant favors recall whereas the coupled variant favors precision, across all models.\n sent2: In summary, all models perform substantially below human agreement, leaving a large room for improvement.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Trained",
                    "on",
                    "Vietnamese",
                    "texts",
                    "of",
                    "the",
                    "CommonCrawl",
                    "corpus",
                    "ETNLP",
                    "#REF",
                    "x",
                    "Trained",
                    "on",
                    "1GB",
                    "texts",
                    "of",
                    "Vietnamese",
                    "Wikipedia",
                    "PhoBERT",
                    "#TARGET_REF",
                    "x"
                ]
            ],
            "context": [
                3
            ]
        },
        "input": "sent0: Trained on Vietnamese texts of the CommonCrawl corpus ETNLP #REF x Trained on 1GB texts of Vietnamese Wikipedia PhoBERT #TARGET_REF x\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "trained",
                    "and",
                    "qualified",
                    "23",
                    "workers",
                    "on",
                    "the",
                    "Amazon",
                    "Mechanical",
                    "Turk",
                    "(AMT)",
                    "platform,",
                    "to",
                    "participate",
                    "in",
                    "the",
                    "coreference,",
                    "NP",
                    "relations,",
                    "and",
                    "consolidation",
                    "tasks."
                ],
                [
                    "We",
                    "follow",
                    "the",
                    "controlled",
                    "crowdsourcing",
                    "protocol",
                    "suggested",
                    "by",
                    "#TARGET_REF",
                    "and",
                    "#REF",
                    ",",
                    "giving",
                    "detailed",
                    "than",
                    "two",
                    "prepositions",
                    "for",
                    "the",
                    "same",
                    "NP",
                    "pairs",
                    "is",
                    "not",
                    "common,",
                    "and",
                    "two",
                    "prepositions",
                    "occur",
                    "in",
                    "11.6%",
                    "of",
                    "the",
                    "test-set."
                ],
                [
                    "For",
                    "simplicity,",
                    "in",
                    "this",
                    "work,",
                    "we",
                    "consider",
                    "a",
                    "single",
                    "preposition",
                    "for",
                    "each",
                    "NP",
                    "pair,",
                    "but",
                    "the",
                    "collected",
                    "data",
                    "may",
                    "contain",
                    "two",
                    "prepositions",
                    "for",
                    "some",
                    "pairs."
                ],
                [
                    "We",
                    "paid",
                    "$1.50,",
                    "$2.50,",
                    "and",
                    "$1.5)",
                    "for",
                    "each",
                    "HIT",
                    "in",
                    "the",
                    "coreference,",
                    "NP-relations,",
                    "and",
                    "consolidation",
                    "tasks,",
                    "respectively."
                ],
                [
                    "The",
                    "price",
                    "for",
                    "the",
                    "NPrelations",
                    "task",
                    "was",
                    "raised",
                    "to",
                    "$2.70",
                    "for",
                    "the",
                    "test",
                    "and",
                    "out-of-domain",
                    "subsets."
                ],
                [
                    "We",
                    "additionally",
                    "paid",
                    "bonus",
                    "payments",
                    "on",
                    "multiple",
                    "occasions."
                ],
                [
                    "Overall,",
                    "we",
                    "aimed",
                    "at",
                    "paying",
                    "at",
                    "least",
                    "the",
                    "minimum",
                    "wage",
                    "in",
                    "the",
                    "United",
                    "States."
                ]
            ],
            "context": [
                0,
                3,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: We trained and qualified 23 workers on the Amazon Mechanical Turk (AMT) platform, to participate in the coreference, NP relations, and consolidation tasks.\n sent1: We follow the controlled crowdsourcing protocol suggested by #TARGET_REF and #REF , giving detailed than two prepositions for the same NP pairs is not common, and two prepositions occur in 11.6% of the test-set.\n sent2: For simplicity, in this work, we consider a single preposition for each NP pair, but the collected data may contain two prepositions for some pairs.\n sent3: We paid $1.50, $2.50, and $1.5) for each HIT in the coreference, NP-relations, and consolidation tasks, respectively.\n sent4: The price for the NPrelations task was raised to $2.70 for the test and out-of-domain subsets.\n sent5: We additionally paid bonus payments on multiple occasions.\n sent6: Overall, we aimed at paying at least the minimum wage in the United States.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Some",
                    "emotions",
                    "are",
                    "also",
                    "harder",
                    "to",
                    "detect,",
                    "even",
                    "for",
                    "humans."
                ],
                [
                    "#REF",
                    "show",
                    "that",
                    "the",
                    "emotions",
                    "of",
                    "admiration,",
                    "approval,",
                    "annoyance,",
                    "gratitude",
                    "had",
                    "the",
                    "highest",
                    "interrater",
                    "correlations",
                    "at",
                    "around",
                    "0.6,",
                    "and",
                    "grief,",
                    "relief,",
                    "pride,",
                    "nervousness,",
                    "embarrassment",
                    "had",
                    "the",
                    "lowest",
                    "interrater",
                    "correlations",
                    "between",
                    "0-0.2,",
                    "with",
                    "a",
                    "vast",
                    "majority",
                    "of",
                    "emotions",
                    "falling",
                    "in",
                    "the",
                    "range",
                    "of",
                    "0.3-0.5",
                    "for",
                    "interrater",
                    "correlation."
                ],
                [
                    "Emotions",
                    "are",
                    "also",
                    "expressed",
                    "differently",
                    "in",
                    "text",
                    "with",
                    "anger",
                    "and",
                    "disgust",
                    "expressed",
                    "explicitly,",
                    "and",
                    "surprise",
                    "in",
                    "context",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                2,
                3,
                1
            ]
        },
        "input": "sent0: Some emotions are also harder to detect, even for humans.\n sent1: #REF show that the emotions of admiration, approval, annoyance, gratitude had the highest interrater correlations at around 0.6, and grief, relief, pride, nervousness, embarrassment had the lowest interrater correlations between 0-0.2, with a vast majority of emotions falling in the range of 0.3-0.5 for interrater correlation.\n sent2: Emotions are also expressed differently in text with anger and disgust expressed explicitly, and surprise in context #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [\"sent2\"], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "The",
                    "key",
                    "insight",
                    "of",
                    "this",
                    "paper",
                    "is",
                    "that",
                    "entities",
                    "that",
                    "share",
                    "many",
                    "attributes",
                    "are",
                    "often",
                    "similar."
                ],
                [
                    "This",
                    "is",
                    "an",
                    "extension",
                    "of",
                    "the",
                    "distributional",
                    "hypothesis,",
                    "#TARGET_REF",
                    ",",
                    "which",
                    "states",
                    "that",
                    "words",
                    "with",
                    "similar",
                    "semantic",
                    "meanings",
                    "tend",
                    "to",
                    "appear",
                    "in",
                    "similar",
                    "contexts,",
                    "and",
                    "builds",
                    "on",
                    "work",
                    "that",
                    "use",
                    "referential",
                    "attributes",
                    "to",
                    "estimate",
                    "semantic",
                    "relatedness",
                    "#REF",
                    "."
                ],
                [
                    "For",
                    "the",
                    "attribute-aware",
                    "embeddings,",
                    "we",
                    "argue",
                    "that",
                    "a",
                    "good",
                    "representation",
                    "for",
                    "an",
                    "entity",
                    "can",
                    "be",
                    "inferred",
                    "from",
                    "its",
                    "most",
                    "common",
                    "attributes,",
                    "which",
                    "we",
                    "may",
                    "have",
                    "access",
                    "to",
                    "from",
                    "an",
                    "external",
                    "source",
                    "of",
                    "knowledge."
                ],
                [
                    "In",
                    "Figure",
                    "1,",
                    "we",
                    "want",
                    "to",
                    "classify",
                    "two",
                    "candidate",
                    "relations",
                    "given",
                    "the",
                    "other",
                    "known",
                    "relations."
                ]
            ],
            "context": [
                0,
                1,
                0,
                0
            ]
        },
        "input": "sent0: The key insight of this paper is that entities that share many attributes are often similar.\n sent1: This is an extension of the distributional hypothesis, #TARGET_REF , which states that words with similar semantic meanings tend to appear in similar contexts, and builds on work that use referential attributes to estimate semantic relatedness #REF .\n sent2: For the attribute-aware embeddings, we argue that a good representation for an entity can be inferred from its most common attributes, which we may have access to from an external source of knowledge.\n sent3: In Figure 1, we want to classify two candidate relations given the other known relations.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "define",
                    "text-level",
                    "bias",
                    "as",
                    "the",
                    "frequency",
                    "of",
                    "certain",
                    "words",
                    "which",
                    "are",
                    "recognised",
                    "as",
                    "favouring",
                    "one",
                    "gender",
                    "over",
                    "another."
                ],
                [
                    "The",
                    "problem",
                    "is",
                    "then",
                    "in",
                    "defining",
                    "this",
                    "list",
                    "of",
                    "words."
                ],
                [
                    "To",
                    "avoid",
                    "overfitting",
                    "to",
                    "one",
                    "axis",
                    "of",
                    "gender",
                    "bias,",
                    "we",
                    "construct",
                    "a",
                    "composite",
                    "score",
                    "based",
                    "on",
                    "pre-existing",
                    "lists",
                    "which",
                    "have",
                    "in",
                    "turn",
                    "been",
                    "defined",
                    "through",
                    "experiments",
                    "and",
                    "empirical",
                    "assessments",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "The",
                    "presence",
                    "of",
                    "words",
                    "which",
                    "are",
                    "more",
                    "likely",
                    "to",
                    "be",
                    "associated",
                    "with",
                    "one",
                    "gender",
                    "does",
                    "not",
                    "directly",
                    "result",
                    "in",
                    "biased",
                    "outcomes."
                ],
                [
                    "Bias",
                    "may",
                    "be",
                    "more",
                    "accurately",
                    "measured",
                    "as",
                    "the",
                    "relative",
                    "gender",
                    "distribution",
                    "of",
                    "applicants",
                    "who",
                    "apply",
                    "to",
                    "a",
                    "given",
                    "ad."
                ],
                [
                    "In",
                    "this",
                    "work,",
                    "we",
                    "focus",
                    "on",
                    "gendered",
                    "word",
                    "lists",
                    "as",
                    "one",
                    "overt",
                    "presentation",
                    "of",
                    "gender",
                    "bias",
                    "but",
                    "encourage",
                    "further",
                    "research",
                    "to",
                    "empirically",
                    "measure",
                    "allocational",
                    "harm,",
                    "so",
                    "long",
                    "as",
                    "any",
                    "experiments",
                    "consider",
                    "the",
                    "ethical",
                    "issues",
                    "of",
                    "posting",
                    "\"fake\"",
                    "ads",
                    "online."
                ]
            ],
            "context": [
                3,
                3,
                2,
                0,
                0,
                0
            ]
        },
        "input": "sent0: We define text-level bias as the frequency of certain words which are recognised as favouring one gender over another.\n sent1: The problem is then in defining this list of words.\n sent2: To avoid overfitting to one axis of gender bias, we construct a composite score based on pre-existing lists which have in turn been defined through experiments and empirical assessments #TARGET_REF .\n sent3: The presence of words which are more likely to be associated with one gender does not directly result in biased outcomes.\n sent4: Bias may be more accurately measured as the relative gender distribution of applicants who apply to a given ad.\n sent5: In this work, we focus on gendered word lists as one overt presentation of gender bias but encourage further research to empirically measure allocational harm, so long as any experiments consider the ethical issues of posting \"fake\" ads online.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent2\"], \"BACKGROUND\": [\"sent0\", \"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Varying",
                    "Pruning",
                    "Percentage."
                ],
                [
                    "We",
                    "randomly",
                    "prune",
                    "attention",
                    "heads",
                    "across",
                    "all",
                    "components",
                    "and",
                    "layers",
                    "varying",
                    "the",
                    "percentage",
                    "of",
                    "pruning",
                    "from",
                    "25%",
                    "to",
                    "87%",
                    "(Table",
                    "1)."
                ],
                [
                    "We",
                    "observed",
                    "that",
                    "in",
                    "the",
                    "case",
                    "of",
                    "extreme",
                    "pruning,",
                    "i.e.,",
                    "keeping",
                    "just",
                    "one",
                    "head",
                    "in",
                    "each",
                    "layer",
                    "of",
                    "each",
                    "of",
                    "the",
                    "three",
                    "components",
                    "(which",
                    "corresponds",
                    "to",
                    "a",
                    "pruning",
                    "percentage",
                    "of",
                    "87%),",
                    "the",
                    "drop",
                    "in",
                    "BLEU",
                    "was",
                    "1.62",
                    "(EN-RU)",
                    "and",
                    "1.03",
                    "(EN-DE)",
                    "as",
                    "can",
                    "be",
                    "seen",
                    "from",
                    "Table",
                    "1."
                ],
                [
                    "Across",
                    "both",
                    "EN-RU",
                    "and",
                    "EN-DE",
                    "tasks,",
                    "60%",
                    "of",
                    "the",
                    "attention",
                    "heads",
                    "can",
                    "be",
                    "pruned",
                    "with",
                    "a",
                    "maximum",
                    "drop",
                    "in",
                    "BLEU",
                    "score",
                    "by",
                    "only",
                    "0.15."
                ],
                [
                    "As",
                    "can",
                    "be",
                    "observed",
                    "from",
                    "Figure",
                    "1,",
                    "the",
                    "drop",
                    "is",
                    "sharper",
                    "as",
                    "we",
                    "increase",
                    "the",
                    "pruning",
                    "percentage",
                    "beyond",
                    "60%."
                ],
                [
                    "three",
                    "layers,",
                    "3",
                    "in",
                    "the",
                    "fourth",
                    "layer",
                    "and",
                    "2",
                    "each",
                    "in",
                    "the",
                    "last",
                    "two",
                    "layers."
                ],
                [
                    "For",
                    "each",
                    "pruning",
                    "percentage,",
                    "the",
                    "first",
                    "row",
                    "corresponds",
                    "to",
                    "the",
                    "configuration",
                    "in",
                    "which",
                    "heads",
                    "considered",
                    "important",
                    "#TARGET_REF",
                    "were",
                    "retained",
                    "and",
                    "the",
                    "second",
                    "row",
                    "corresponds",
                    "to",
                    "the",
                    "adversarial",
                    "configuration",
                    "in",
                    "which",
                    "heads",
                    "considered",
                    "important",
                    "were",
                    "pruned."
                ],
                [
                    "We",
                    "identify",
                    "no",
                    "preference",
                    "in",
                    "pruning",
                    "as",
                    "for",
                    "each",
                    "pruning",
                    "percentage",
                    "the",
                    "performance",
                    "of",
                    "both",
                    "configurations",
                    "is",
                    "very",
                    "similar."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                0
            ]
        },
        "input": "sent0: Varying Pruning Percentage.\n sent1: We randomly prune attention heads across all components and layers varying the percentage of pruning from 25% to 87% (Table 1).\n sent2: We observed that in the case of extreme pruning, i.e., keeping just one head in each layer of each of the three components (which corresponds to a pruning percentage of 87%), the drop in BLEU was 1.62 (EN-RU) and 1.03 (EN-DE) as can be seen from Table 1.\n sent3: Across both EN-RU and EN-DE tasks, 60% of the attention heads can be pruned with a maximum drop in BLEU score by only 0.15.\n sent4: As can be observed from Figure 1, the drop is sharper as we increase the pruning percentage beyond 60%.\n sent5: three layers, 3 in the fourth layer and 2 each in the last two layers.\n sent6: For each pruning percentage, the first row corresponds to the configuration in which heads considered important #TARGET_REF were retained and the second row corresponds to the adversarial configuration in which heads considered important were pruned.\n sent7: We identify no preference in pruning as for each pruning percentage the performance of both configurations is very similar.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent6\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "We",
                    "perform",
                    "evaluations",
                    "with",
                    "fine-tuned",
                    "cased",
                    "multilingual",
                    "and",
                    "language",
                    "specific",
                    "BERT",
                    "(Bidirectional",
                    "Encoder",
                    "Representations",
                    "from",
                    "Transformers)",
                    "models",
                    "#TARGET_REF",
                    ",",
                    "as",
                    "well",
                    "as",
                    "Suport",
                    "Vector",
                    "Machines",
                    "(SVMs)."
                ],
                [
                    "Our",
                    "evaluations",
                    "show",
                    "that",
                    "the",
                    "human-annotated",
                    "datasets",
                    "behave",
                    "on",
                    "par",
                    "with",
                    "comparable",
                    "state-of-the-art",
                    "datasets",
                    "such",
                    "as",
                    "the",
                    "GoEmotions",
                    "dataset",
                    "#REF",
                    "."
                ],
                [
                    "Furthermore,",
                    "the",
                    "projected",
                    "datasets",
                    "have",
                    "accuracies",
                    "that",
                    "closely",
                    "resemble",
                    "human-annotated",
                    "data",
                    "with",
                    "macro",
                    "f1",
                    "scores",
                    "of",
                    "0.51",
                    "for",
                    "the",
                    "human",
                    "annotated",
                    "Finnish",
                    "data",
                    "and",
                    "0.45",
                    "for",
                    "the",
                    "projected",
                    "Finnish",
                    "data",
                    "when",
                    "evaluating",
                    "with",
                    "FinBERT",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                2,
                0,
                0
            ]
        },
        "input": "sent0: We perform evaluations with fine-tuned cased multilingual and language specific BERT (Bidirectional Encoder Representations from Transformers) models #TARGET_REF , as well as Suport Vector Machines (SVMs).\n sent1: Our evaluations show that the human-annotated datasets behave on par with comparable state-of-the-art datasets such as the GoEmotions dataset #REF .\n sent2: Furthermore, the projected datasets have accuracies that closely resemble human-annotated data with macro f1 scores of 0.51 for the human annotated Finnish data and 0.45 for the projected Finnish data when evaluating with FinBERT #REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "To",
                    "apply",
                    "RDS",
                    "#TARGET_REF",
                    "for",
                    "the",
                    "data",
                    "splitting",
                    "process,",
                    "it",
                    "requires",
                    "to",
                    "have",
                    "baseline",
                    "learners",
                    "to",
                    "obtain",
                    "rewards",
                    "for",
                    "the",
                    "reinforced",
                    "process."
                ],
                [
                    "It",
                    "is",
                    "recommended",
                    "to",
                    "choose",
                    "representative",
                    "baseline",
                    "learners,",
                    "to",
                    "let",
                    "the",
                    "reinforced",
                    "learner",
                    "better",
                    "capture",
                    "different",
                    "learning",
                    "behaviors."
                ],
                [
                    "The",
                    "use",
                    "of",
                    "these",
                    "baseline",
                    "learners",
                    "is",
                    "important",
                    "since",
                    "each",
                    "learner",
                    "will",
                    "behave",
                    "differently",
                    "depending",
                    "on",
                    "the",
                    "patterns",
                    "contained",
                    "in",
                    "the",
                    "target",
                    "data."
                ],
                [
                    "As",
                    "a",
                    "result,",
                    "RDS",
                    "helps",
                    "to",
                    "increase",
                    "the",
                    "diversity",
                    "of",
                    "the",
                    "data",
                    "samples",
                    "in",
                    "different",
                    "sets."
                ],
                [
                    "Here",
                    "we",
                    "employ",
                    "three",
                    "models",
                    "to",
                    "classify",
                    "reliable",
                    "news",
                    "using",
                    "textual",
                    "features",
                    "as",
                    "follows:",
                    "Bi-LSTM",
                    "network",
                    "is",
                    "a",
                    "standard",
                    "baseline",
                    "for",
                    "most",
                    "of",
                    "text",
                    "classification",
                    "tasks."
                ]
            ],
            "context": [
                2,
                3,
                0,
                2,
                0
            ]
        },
        "input": "sent0: To apply RDS #TARGET_REF for the data splitting process, it requires to have baseline learners to obtain rewards for the reinforced process.\n sent1: It is recommended to choose representative baseline learners, to let the reinforced learner better capture different learning behaviors.\n sent2: The use of these baseline learners is important since each learner will behave differently depending on the patterns contained in the target data.\n sent3: As a result, RDS helps to increase the diversity of the data samples in different sets.\n sent4: Here we employ three models to classify reliable news using textual features as follows: Bi-LSTM network is a standard baseline for most of text classification tasks.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent0\", \"sent3\"], \"BACKGROUND\": [\"sent1\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "To",
                    "ensure",
                    "the",
                    "task",
                    "was",
                    "as",
                    "realistic",
                    "as",
                    "possible,",
                    "we",
                    "selected",
                    "English",
                    "documentation",
                    "for",
                    "a",
                    "well-known",
                    "online",
                    "file",
                    "storage",
                    "and",
                    "sharing",
                    "service."
                ],
                [
                    "We",
                    "made",
                    "an",
                    "initial",
                    "assumption",
                    "that",
                    "the",
                    "original",
                    "English",
                    "instructions",
                    "published",
                    "by",
                    "the",
                    "developer",
                    "were",
                    "reasonably",
                    "usable,",
                    "given",
                    "that",
                    "the",
                    "service",
                    "has",
                    "over",
                    "50",
                    "million",
                    "users",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "As",
                    "native",
                    "speakers",
                    "of",
                    "English,",
                    "both",
                    "authors",
                    "judged",
                    "the",
                    "documentation",
                    "to",
                    "be",
                    "of",
                    "reasonable",
                    "quality",
                    "and",
                    "well-formed."
                ],
                [
                    "These",
                    "were",
                    "initial",
                    "assumptions",
                    "which",
                    "would",
                    "be",
                    "tested",
                    "in",
                    "the",
                    "project."
                ]
            ],
            "context": [
                0,
                2,
                0,
                0
            ]
        },
        "input": "sent0: To ensure the task was as realistic as possible, we selected English documentation for a well-known online file storage and sharing service.\n sent1: We made an initial assumption that the original English instructions published by the developer were reasonably usable, given that the service has over 50 million users #TARGET_REF .\n sent2: As native speakers of English, both authors judged the documentation to be of reasonable quality and well-formed.\n sent3: These were initial assumptions which would be tested in the project.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Among",
                    "the",
                    "EC",
                    "projects",
                    "working",
                    "in",
                    "this",
                    "direction",
                    "we",
                    "mention",
                    "LE",
                    "SPARKLE",
                    "(Shallow",
                    "PARsing",
                    "and",
                    "Knowledge",
                    "extraction",
                    "for",
                    "Language",
                    "Engineering",
                    "2",
                    "),",
                    "combining",
                    "shallow",
                    "parsing",
                    "and",
                    "lexical",
                    "acquisition",
                    "techniques",
                    "capable",
                    "of",
                    "learning",
                    "(from",
                    "large",
                    "corpora)",
                    "aspects",
                    "of",
                    "word",
                    "knowledge",
                    "required",
                    "for",
                    "LE",
                    "applications",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "The",
                    "project",
                    "(http://www.ilc.pi.cnr.it/sparkle.html)",
                    "is",
                    "positioned",
                    "as",
                    "research",
                    "on",
                    "the",
                    "development",
                    "of",
                    "methodologies",
                    "and",
                    "techniques",
                    "for",
                    "application-or",
                    "domaindependent",
                    "lexical",
                    "resources",
                    "to",
                    "be",
                    "acquired",
                    "(semi)automatically",
                    "from",
                    "texts,",
                    "an",
                    "area",
                    "which",
                    "is",
                    "crucial",
                    "to",
                    "most",
                    "NLP",
                    "applications."
                ],
                [
                    "Economically",
                    "feasible",
                    "development",
                    "of",
                    "language",
                    "models",
                    "and",
                    "of",
                    "substantial",
                    "lexical",
                    "resources",
                    "for",
                    "real-world",
                    "NLP",
                    "applications",
                    "needs",
                    "to",
                    "be",
                    "based",
                    "on",
                    "substantially",
                    "(semi)-automated",
                    "techniques",
                    "and",
                    "flexible",
                    "tools",
                    "for",
                    "analysing",
                    "and",
                    "extracting",
                    "lexical",
                    "information",
                    "from",
                    "textual",
                    "corpora,",
                    "otherwise",
                    "coverage",
                    "and/or",
                    "accuracy",
                    "will",
                    "remain",
                    "inadequate."
                ]
            ],
            "context": [
                1,
                0,
                0
            ]
        },
        "input": "sent0: Among the EC projects working in this direction we mention LE SPARKLE (Shallow PARsing and Knowledge extraction for Language Engineering 2 ), combining shallow parsing and lexical acquisition techniques capable of learning (from large corpora) aspects of word knowledge required for LE applications #TARGET_REF .\n sent1: The project (http://www.ilc.pi.cnr.it/sparkle.html) is positioned as research on the development of methodologies and techniques for application-or domaindependent lexical resources to be acquired (semi)automatically from texts, an area which is crucial to most NLP applications.\n sent2: Economically feasible development of language models and of substantial lexical resources for real-world NLP applications needs to be based on substantially (semi)-automated techniques and flexible tools for analysing and extracting lexical information from textual corpora, otherwise coverage and/or accuracy will remain inadequate.\n",
        "output": "{\"INFORMATION\": [\"sent0\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Caption",
                    "Decoder",
                    "Caption",
                    "decoder",
                    "combines",
                    "vision",
                    "and",
                    "trace",
                    "information",
                    "using",
                    "cross",
                    "attention",
                    "connected",
                    "to",
                    "the",
                    "hidden",
                    "states",
                    "of",
                    "Vision-Trace",
                    "Encoder's",
                    "last",
                    "layer."
                ],
                [
                    "Using",
                    "a",
                    "casual",
                    "mask",
                    "to",
                    "encode",
                    "generated",
                    "token",
                    "progressively,",
                    "the",
                    "transformer",
                    "decoder",
                    "ensures",
                    "that",
                    "the",
                    "predictions",
                    "for",
                    "position",
                    "i",
                    "can",
                    "depend",
                    "only",
                    "on",
                    "the",
                    "known",
                    "outputs",
                    "at",
                    "positions",
                    "less",
                    "than",
                    "i."
                ],
                [
                    "During",
                    "training,",
                    "the",
                    "ground",
                    "truth",
                    "caption",
                    "tokens",
                    "are",
                    "shifted",
                    "right,",
                    "and",
                    "a",
                    "special",
                    "token",
                    "BOS",
                    "(begin",
                    "of",
                    "the",
                    "sentence)",
                    "is",
                    "inserted",
                    "into",
                    "the",
                    "first",
                    "position."
                ],
                [
                    "A",
                    "cross-entropy",
                    "generation",
                    "loss",
                    "L",
                    "gen",
                    "is",
                    "then",
                    "computed",
                    "with",
                    "the",
                    "logits",
                    "transformed",
                    "from",
                    "the",
                    "last",
                    "decoder",
                    "layer's",
                    "hidden",
                    "states",
                    "and",
                    "un-shifted",
                    "ground",
                    "truth",
                    "caption",
                    "token",
                    "ids",
                    "with",
                    "a",
                    "special",
                    "token",
                    "EOS",
                    "(end",
                    "of",
                    "the",
                    "sentence)",
                    "appended.L",
                    "gen",
                    "=",
                    "−",
                    "E",
                    "ŷi",
                    "∼ŷ",
                    "log",
                    "p",
                    "ŷi",
                    "|",
                    "ŷ&lt,i",
                    ",",
                    "T",
                    ",",
                    "Ṽ",
                    ",",
                    "θ",
                    "."
                ],
                [
                    "(3)It",
                    "is",
                    "noted",
                    "that",
                    "ŷ",
                    "is",
                    "the",
                    "masked",
                    "version",
                    "of",
                    "the",
                    "ground-truth",
                    "caption",
                    "y."
                ],
                [
                    "To",
                    "make",
                    "a",
                    "fair",
                    "comparison",
                    "with",
                    "the",
                    "baseline",
                    "(Pont-Tuset",
                    "et",
                    "al.,",
                    "2020),",
                    "we",
                    "apply",
                    "the",
                    "same",
                    "setting",
                    "and",
                    "do",
                    "not",
                    "employ",
                    "common",
                    "techniques",
                    "such",
                    "as",
                    "label",
                    "smoothing",
                    "#TARGET_REF",
                    "or",
                    "self-critical",
                    "training",
                    "#REF",
                    "."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                2
            ]
        },
        "input": "sent0: Caption Decoder Caption decoder combines vision and trace information using cross attention connected to the hidden states of Vision-Trace Encoder's last layer.\n sent1: Using a casual mask to encode generated token progressively, the transformer decoder ensures that the predictions for position i can depend only on the known outputs at positions less than i.\n sent2: During training, the ground truth caption tokens are shifted right, and a special token BOS (begin of the sentence) is inserted into the first position.\n sent3: A cross-entropy generation loss L gen is then computed with the logits transformed from the last decoder layer's hidden states and un-shifted ground truth caption token ids with a special token EOS (end of the sentence) appended.L gen = − E ŷi ∼ŷ log p ŷi | ŷ&lt,i , T , Ṽ , θ .\n sent4: (3)It is noted that ŷ is the masked version of the ground-truth caption y.\n sent5: To make a fair comparison with the baseline (Pont-Tuset et al., 2020), we apply the same setting and do not employ common techniques such as label smoothing #TARGET_REF or self-critical training #REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent5\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Specifically,",
                    "we",
                    "use",
                    "a",
                    "retrieval-based",
                    "ranker",
                    "model",
                    "that",
                    "checks",
                    "for",
                    "similarity",
                    "of",
                    "#REF",
                    "embeddings."
                ],
                [
                    "Our",
                    "choice",
                    "of",
                    "model",
                    "is",
                    "influenced",
                    "by",
                    "#TARGET_REF",
                    "who",
                    "report",
                    "stateof-the-art",
                    "retrieval",
                    "performance",
                    "for",
                    "locations",
                    "in",
                    "LIGHT",
                    "using",
                    "this",
                    "model."
                ],
                [
                    "The",
                    "overall",
                    "ranker",
                    "model",
                    "first",
                    "trains",
                    "a",
                    "randomly",
                    "initialized",
                    "StarSpace",
                    "embedding",
                    "model",
                    "that",
                    "is",
                    "designed",
                    "to",
                    "correlate",
                    "characters",
                    "with",
                    "the",
                    "locations",
                    "they",
                    "are",
                    "found",
                    "in."
                ],
                [
                    "It",
                    "learns",
                    "a",
                    "single",
                    "bag-of-words",
                    "embedding",
                    "that",
                    "takes",
                    "into",
                    "account",
                    "all",
                    "the",
                    "individual",
                    "words",
                    "contained",
                    "within",
                    "the",
                    "input-encoding",
                    "character",
                    "and",
                    "location",
                    "information",
                    "as",
                    "well",
                    "as",
                    "the",
                    "previously",
                    "mentioned",
                    "negative",
                    "Quest",
                    "Generation."
                ],
                [
                    "The",
                    "quest",
                    "is",
                    "now",
                    "generated",
                    "using",
                    "the",
                    "existing",
                    "character",
                    "and",
                    "location",
                    "information."
                ],
                [
                    "The",
                    "generation-based",
                    "models",
                    "used",
                    "in",
                    "this",
                    "pipeline",
                    "are",
                    "trained",
                    "to",
                    "return",
                    "the",
                    "most",
                    "likely",
                    "output",
                    "sequence",
                    "given",
                    "an",
                    "input",
                    "sequence."
                ],
                [
                    "Given",
                    "a",
                    "target",
                    "sequence",
                    "Y",
                    "=",
                    "{y",
                    "1",
                    ",",
                    "...,",
                    "y",
                    "M",
                    "}",
                    "and",
                    "some",
                    "input",
                    "context",
                    "vector",
                    "via",
                    "the",
                    "encoders",
                    "X."
                ],
                [
                    "These",
                    "models",
                    "use",
                    "autoregressive",
                    "decoding",
                    "techniques",
                    "that",
                    "factor",
                    "the",
                    "distribution",
                    "over",
                    "the",
                    "target",
                    "sequence",
                    "into",
                    "a",
                    "chain",
                    "of",
                    "conditional",
                    "probabilities",
                    "with",
                    "a",
                    "causal",
                    "left",
                    "to",
                    "right",
                    "structure",
                    "as",
                    "P",
                    "(Y",
                    "|X,",
                    "θ)",
                    "=",
                    "M",
                    "+1",
                    "i=1",
                    "p(y",
                    "i",
                    "|y",
                    "0:i−1",
                    ",",
                    "X,",
                    "θ)",
                    "where",
                    "θ",
                    "represents",
                    "the",
                    "current",
                    "network",
                    "parameters."
                ],
                [
                    "At",
                    "test",
                    "time,",
                    "a",
                    "special",
                    "start-of-sequence",
                    "token",
                    "is",
                    "provided",
                    "to",
                    "the",
                    "model",
                    "which",
                    "then",
                    "proceeds",
                    "to",
                    "decode",
                    "the",
                    "rest",
                    "of",
                    "the",
                    "output",
                    "sequence",
                    "using",
                    "beam",
                    "search."
                ]
            ],
            "context": [
                3,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: Specifically, we use a retrieval-based ranker model that checks for similarity of #REF embeddings.\n sent1: Our choice of model is influenced by #TARGET_REF who report stateof-the-art retrieval performance for locations in LIGHT using this model.\n sent2: The overall ranker model first trains a randomly initialized StarSpace embedding model that is designed to correlate characters with the locations they are found in.\n sent3: It learns a single bag-of-words embedding that takes into account all the individual words contained within the input-encoding character and location information as well as the previously mentioned negative Quest Generation.\n sent4: The quest is now generated using the existing character and location information.\n sent5: The generation-based models used in this pipeline are trained to return the most likely output sequence given an input sequence.\n sent6: Given a target sequence Y = {y 1 , ..., y M } and some input context vector via the encoders X.\n sent7: These models use autoregressive decoding techniques that factor the distribution over the target sequence into a chain of conditional probabilities with a causal left to right structure as P (Y |X, θ) = M +1 i=1 p(y i |y 0:i−1 , X, θ) where θ represents the current network parameters.\n sent8: At test time, a special start-of-sequence token is provided to the model which then proceeds to decode the rest of the output sequence using beam search.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": [\"sent0\"]}"
    },
    {
        "gold": {
            "text": [
                [
                    "Arabic",
                    "being",
                    "a",
                    "morphologically",
                    "rich",
                    "language,",
                    "has",
                    "many",
                    "different",
                    "surface",
                    "forms",
                    "of",
                    "words",
                    "with",
                    "same",
                    "root."
                ],
                [
                    "This",
                    "phenomenon",
                    "poses",
                    "a",
                    "data",
                    "sparsity",
                    "problem",
                    "for",
                    "SMT",
                    "systems."
                ],
                [
                    "In",
                    "order",
                    "to",
                    "reduce",
                    "data",
                    "sparsity,",
                    "we",
                    "segment",
                    "the",
                    "Arabic",
                    "data",
                    "morphologically",
                    "before",
                    "training."
                ],
                [
                    "The",
                    "Arabic",
                    "data",
                    "is",
                    "segmented",
                    "according",
                    "to",
                    "the",
                    "D3",
                    "segmentation",
                    "scheme",
                    "using",
                    "MADA",
                    "(Morphological",
                    "Analysis",
                    "and",
                    "Disambiguation",
                    "for",
                    "Arabic)."
                ],
                [
                    "5",
                    "For",
                    "all",
                    "the",
                    "available",
                    "Chinese",
                    "data,",
                    "we",
                    "segment",
                    "the",
                    "sentences",
                    "to",
                    "words",
                    "using",
                    "the",
                    "Stanford",
                    "Chinese",
                    "Word",
                    "Segmenter",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "English",
                    "data",
                    "is",
                    "lower-cased",
                    "and",
                    "tokenized",
                    "in",
                    "the",
                    "preprocessing",
                    "step."
                ]
            ],
            "context": [
                0,
                0,
                0,
                0,
                2,
                0
            ]
        },
        "input": "sent0: Arabic being a morphologically rich language, has many different surface forms of words with same root.\n sent1: This phenomenon poses a data sparsity problem for SMT systems.\n sent2: In order to reduce data sparsity, we segment the Arabic data morphologically before training.\n sent3: The Arabic data is segmented according to the D3 segmentation scheme using MADA (Morphological Analysis and Disambiguation for Arabic).\n sent4: 5 For all the available Chinese data, we segment the sentences to words using the Stanford Chinese Word Segmenter #TARGET_REF .\n sent5: English data is lower-cased and tokenized in the preprocessing step.\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent4\"], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "race",
                    "are",
                    "always",
                    "labeled",
                    "abusive/benign."
                ],
                [
                    "Moreover,",
                    "relying",
                    "solely",
                    "on",
                    "personal",
                    "traits",
                    "of",
                    "users",
                    "also",
                    "comes",
                    "with",
                    "the",
                    "risk",
                    "that",
                    "such",
                    "information",
                    "may",
                    "not",
                    "always",
                    "be",
                    "present",
                    "or",
                    "may",
                    "not",
                    "be",
                    "accurate",
                    "even",
                    "when",
                    "present",
                    "#TARGET_REF",
                    "."
                ],
                [
                    "On",
                    "the",
                    "other",
                    "hand,",
                    "more",
                    "complex",
                    "inductive",
                    "biases",
                    "learned",
                    "from",
                    "data,",
                    "as",
                    "in",
                    "the",
                    "case",
                    "of",
                    "social",
                    "graph",
                    "based",
                    "methods,",
                    "provide",
                    "a",
                    "safer",
                    "and",
                    "more",
                    "reliable",
                    "generalization",
                    "from",
                    "personal",
                    "behaviors",
                    "of",
                    "users",
                    "or",
                    "communities",
                    "to",
                    "population",
                    "level",
                    "trends."
                ],
                [
                    "Bias",
                    "in",
                    "datasets."
                ],
                [
                    "An",
                    "obvious",
                    "pitfall",
                    "in",
                    "working",
                    "with",
                    "methods",
                    "that",
                    "incorporate",
                    "user",
                    "and",
                    "community",
                    "information",
                    "is",
                    "having",
                    "datasets",
                    "where",
                    "comments",
                    "come",
                    "from",
                    "users",
                    "belonging",
                    "to",
                    "some",
                    "limited",
                    "demographics",
                    "only."
                ],
                [
                    "We",
                    "refer",
                    "to",
                    "this",
                    "as",
                    "demographic",
                    "bias."
                ],
                [
                    "Datasets",
                    "with",
                    "demographic",
                    "bias",
                    "will",
                    "cause",
                    "the",
                    "methods",
                    "to",
                    "overfit",
                    "to",
                    "linguistic",
                    "practices",
                    "and",
                    "dialects",
                    "of",
                    "users",
                    "and",
                    "communities",
                    "belonging",
                    "to",
                    "specific",
                    "demographics",
                    "#REF",
                    ",",
                    "hence",
                    "diminishing",
                    "the",
                    "power",
                    "of",
                    "the",
                    "methods",
                    "to",
                    "generalize."
                ],
                [
                    "In",
                    "fact,",
                    "this",
                    "bias",
                    "is",
                    "not",
                    "only",
                    "a",
                    "problem",
                    "for",
                    "methods",
                    "we",
                    "discussed,",
                    "but",
                    "for",
                    "any",
                    "NLP",
                    "method",
                    "in",
                    "general."
                ],
                [
                    "When",
                    "it",
                    "comes",
                    "to",
                    "methods",
                    "that",
                    "incorporate",
                    "user",
                    "or",
                    "community",
                    "information",
                    "specifically,",
                    "there",
                    "are",
                    "two",
                    "other",
                    "biases",
                    "that",
                    "must",
                    "be",
                    "kept",
                    "in",
                    "mind",
                    "when",
                    "constructing",
                    "datasets,",
                    "we",
                    "refer",
                    "to",
                    "them",
                    "as",
                    "comment",
                    "distribution",
                    "bias",
                    "and",
                    "label",
                    "distribution",
                    "bias."
                ],
                [
                    "Comment",
                    "distribution",
                    "bias",
                    "occurs",
                    "when",
                    "the",
                    "majority",
                    "of",
                    "comments",
                    "in",
                    "the",
                    "dataset",
                    "come",
                    "from",
                    "a",
                    "small",
                    "number",
                    "of",
                    "unique",
                    "users."
                ],
                [
                    "Such",
                    "datasets",
                    "allow",
                    "the",
                    "methods",
                    "to",
                    "simply",
                    "overfit",
                    "to",
                    "the",
                    "linguistic",
                    "or",
                    "social",
                    "behaviors",
                    "and",
                    "community",
                    "roles",
                    "of",
                    "specific",
                    "users",
                    "#REF",
                    "."
                ],
                [
                    "Label",
                    "distribution",
                    "bias",
                    "occurs",
                    "when",
                    "only",
                    "the",
                    "abusive",
                    "comments",
                    "of",
                    "a",
                    "user",
                    "are",
                    "included",
                    "in",
                    "the",
                    "dataset."
                ],
                [
                    "Abuse",
                    "is",
                    "a",
                    "relatively",
                    "infrequent",
                    "phenomenon,",
                    "even",
                    "at",
                    "an",
                    "individual",
                    "level",
                    "#REF",
                    "."
                ],
                [
                    "Only",
                    "getting",
                    "abusive",
                    "comments",
                    "of",
                    "a",
                    "user",
                    "can",
                    "make",
                    "the",
                    "methods",
                    "simply",
                    "associate",
                    "the",
                    "identity",
                    "of",
                    "the",
                    "user",
                    "to",
                    "abusiveness",
                    "when",
                    "including",
                    "user",
                    "information."
                ],
                [
                    "Moreover,",
                    "datasets",
                    "with",
                    "this",
                    "bias",
                    "can",
                    "also",
                    "make",
                    "phenomena",
                    "like",
                    "homophily",
                    "appear",
                    "overly",
                    "effective",
                    "in",
                    "the",
                    "detection",
                    "of",
                    "abuse",
                    "by",
                    "sampling",
                    "only",
                    "abusive",
                    "comments",
                    "from",
                    "users",
                    "who",
                    "are",
                    "close",
                    "in",
                    "the",
                    "social",
                    "network."
                ]
            ],
            "context": [
                0,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sent0: race are always labeled abusive/benign.\n sent1: Moreover, relying solely on personal traits of users also comes with the risk that such information may not always be present or may not be accurate even when present #TARGET_REF .\n sent2: On the other hand, more complex inductive biases learned from data, as in the case of social graph based methods, provide a safer and more reliable generalization from personal behaviors of users or communities to population level trends.\n sent3: Bias in datasets.\n sent4: An obvious pitfall in working with methods that incorporate user and community information is having datasets where comments come from users belonging to some limited demographics only.\n sent5: We refer to this as demographic bias.\n sent6: Datasets with demographic bias will cause the methods to overfit to linguistic practices and dialects of users and communities belonging to specific demographics #REF , hence diminishing the power of the methods to generalize.\n sent7: In fact, this bias is not only a problem for methods we discussed, but for any NLP method in general.\n sent8: When it comes to methods that incorporate user or community information specifically, there are two other biases that must be kept in mind when constructing datasets, we refer to them as comment distribution bias and label distribution bias.\n sent9: Comment distribution bias occurs when the majority of comments in the dataset come from a small number of unique users.\n sent10: Such datasets allow the methods to simply overfit to the linguistic or social behaviors and community roles of specific users #REF .\n sent11: Label distribution bias occurs when only the abusive comments of a user are included in the dataset.\n sent12: Abuse is a relatively infrequent phenomenon, even at an individual level #REF .\n sent13: Only getting abusive comments of a user can make the methods simply associate the identity of the user to abusiveness when including user information.\n sent14: Moreover, datasets with this bias can also make phenomena like homophily appear overly effective in the detection of abuse by sampling only abusive comments from users who are close in the social network.\n",
        "output": "{\"INFORMATION\": [\"sent1\"], \"PERCEPTION\": [], \"BACKGROUND\": []}"
    },
    {
        "gold": {
            "text": [
                [
                    "Following",
                    "the",
                    "previous",
                    "editions",
                    "of",
                    "the",
                    "shared",
                    "task,",
                    "we",
                    "frame",
                    "explanation",
                    "generation",
                    "as",
                    "a",
                    "ranking",
                    "problem."
                ],
                [
                    "Specifically,",
                    "for",
                    "a",
                    "given",
                    "science",
                    "question,",
                    "a",
                    "model",
                    "is",
                    "supplied",
                    "both",
                    "the",
                    "question",
                    "and",
                    "correct",
                    "answer",
                    "text,",
                    "and",
                    "must",
                    "then",
                    "selectively",
                    "rank",
                    "all",
                    "the",
                    "atomic",
                    "scientific",
                    "and",
                    "world",
                    "knowledge",
                    "facts",
                    "in",
                    "the",
                    "knowledge",
                    "base",
                    "such",
                    "that",
                    "those",
                    "that",
                    "were",
                    "labelled",
                    "as",
                    "most",
                    "relevant",
                    "to",
                    "building",
                    "an",
                    "explanation",
                    "by",
                    "a",
                    "human",
                    "annotator",
                    "are",
                    "ranked",
                    "the",
                    "highest."
                ],
                [
                    "Additional",
                    "details",
                    "on",
                    "the",
                    "ranking",
                    "problem",
                    "are",
                    "described",
                    "in",
                    "the",
                    "2019",
                    "shared",
                    "task",
                    "summary",
                    "paper",
                    "#TARGET_REF",
                    "."
                ]
            ],
            "context": [
                0,
                2,
                3
            ]
        },
        "input": "sent0: Following the previous editions of the shared task, we frame explanation generation as a ranking problem.\n sent1: Specifically, for a given science question, a model is supplied both the question and correct answer text, and must then selectively rank all the atomic scientific and world knowledge facts in the knowledge base such that those that were labelled as most relevant to building an explanation by a human annotator are ranked the highest.\n sent2: Additional details on the ranking problem are described in the 2019 shared task summary paper #TARGET_REF .\n",
        "output": "{\"INFORMATION\": [], \"PERCEPTION\": [\"sent1\"], \"BACKGROUND\": [\"sent2\"]}"
    }
]