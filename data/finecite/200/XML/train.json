[
    {
        "gold": {
            "text": [
                "The",
                "benefits",
                "of",
                "a",
                "New",
                "York",
                "Subway",
                "system",
                "is",
                "that",
                "a",
                "person",
                "can",
                "get",
                "from",
                "A",
                "to",
                "B",
                "without",
                "being",
                "stuck",
                "in",
                "traffic",
                "and",
                "subway",
                "trains",
                "are",
                "faster",
                "than",
                "buses.",
                "(Prediction:",
                "Negative)",
                "Natural",
                "Language",
                "Inference",
                "#REF",
                ",",
                "synonym",
                "substitutions",
                "#TARGET_REF",
                ",",
                "or",
                "adding",
                "adversarial",
                "sentences",
                "for",
                "QA",
                "#REF",
                ".",
                "More",
                "recent",
                "work",
                "on",
                "testing",
                "models'",
                "behaviour",
                "using",
                "#REF",
                ")",
                "also",
                "used",
                "a",
                "pre-defined",
                "series",
                "of",
                "test",
                "types,",
                "e.g.,",
                "adding",
                "negation,",
                "temporal",
                "change,",
                "and",
                "switching",
                "locations/person",
                "names.",
                "However,",
                "for",
                "safe",
                "deployment",
                "of",
                "NLP",
                "models",
                "in",
                "the",
                "real",
                "world,",
                "in",
                "addition",
                "to",
                "predefining",
                "a",
                "small",
                "or",
                "limited",
                "set",
                "of",
                "patterns",
                "which",
                "the",
                "model",
                "could",
                "be",
                "vulnerable",
                "to,",
                "it",
                "is",
                "also",
                "important",
                "to",
                "proactively",
                "discover",
                "and",
                "identify",
                "models'",
                "unrobust",
                "regions",
                "automatically",
                "and",
                "comprehensively."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                3,
                3,
                0,
                1,
                1,
                0,
                0,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The benefits of a New York Subway system is that a person can get from A to B without being stuck in traffic and subway trains are faster than buses. (Prediction: Negative) Natural Language Inference #REF , synonym substitutions #TARGET_REF , or adding adversarial sentences for QA #REF . More recent work on testing models' behaviour using #REF ) also used a pre-defined series of test types, e.g., adding negation, temporal change, and switching locations/person names. However, for safe deployment of NLP models in the real world, in addition to predefining a small or limited set of patterns which the model could be vulnerable to, it is also important to proactively discover and identify models' unrobust regions automatically and comprehensively.",
        "output": "The benefits of a New York Subway system is that a person can get from A to B without being stuck in traffic and subway trains are faster than buses. (Prediction: Negative) <BACK> Natural Language Inference #REF </BACK> , <INFO> synonym substitutions </INFO> #TARGET_REF , or <BACK> adding adversarial sentences for QA #REF </BACK> . <BACK> More recent work on testing models' behaviour using #REF ) also used a pre-defined series of test types, e.g., adding negation, temporal change, and switching locations/person names. </BACK> However, for safe deployment of NLP models in the real world, in addition to predefining a small or limited set of patterns which the model could be vulnerable to, it is also important to proactively discover and identify models' unrobust regions automatically and comprehensively."
    },
    {
        "gold": {
            "text": [
                "We",
                "compare",
                "the",
                "output",
                "of",
                "Epitran",
                "and",
                "Allosaurus",
                "on",
                "the",
                "ALFFA",
                "dataset.",
                "Following",
                "the",
                "practice",
                "of",
                "#REF",
                ",",
                "we",
                "used",
                "the",
                "editdistance",
                "10",
                "library",
                "to",
                "calculate",
                "the",
                "Phone",
                "Error",
                "Rate",
                "(PER).",
                "Having",
                "no",
                "ground",
                "truth",
                "phone",
                "annotations,",
                "we",
                "instead",
                "take",
                "Epitran's",
                "outputs",
                "as",
                "\"ground",
                "truth\"",
                "for",
                "comparison.",
                "The",
                "mean",
                "PER",
                "between",
                "the",
                "outputs",
                "is",
                "23.7%.",
                "This",
                "result",
                "is",
                "consistent",
                "with",
                "#TARGET_REF",
                ",",
                "which",
                "finds",
                "PERs",
                "as",
                "high",
                "as",
                "72.8%",
                "when",
                "testing",
                "on",
                "on",
                "the",
                "Bukusu",
                "(bxk),",
                "Saamia",
                "(lsm)",
                "and",
                "East",
                "Tusom",
                "languages",
                "(an",
                "endangered",
                "subdialect",
                "of",
                "the",
                "Tungkhulic",
                "language",
                "family).",
                "However,",
                "by",
                "training",
                "the",
                "phone",
                "recognizer",
                "on",
                "even",
                "minimal",
                "amounts",
                "of",
                "data",
                "in",
                "these",
                "languages,",
                "PERs",
                "were",
                "improved",
                "significantly."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2
            ]
        },
        "input": "We compare the output of Epitran and Allosaurus on the ALFFA dataset. Following the practice of #REF , we used the editdistance 10 library to calculate the Phone Error Rate (PER). Having no ground truth phone annotations, we instead take Epitran's outputs as \"ground truth\" for comparison. The mean PER between the outputs is 23.7%. This result is consistent with #TARGET_REF , which finds PERs as high as 72.8% when testing on on the Bukusu (bxk), Saamia (lsm) and East Tusom languages (an endangered subdialect of the Tungkhulic language family). However, by training the phone recognizer on even minimal amounts of data in these languages, PERs were improved significantly.",
        "output": "We compare the output of Epitran and Allosaurus on the ALFFA dataset. Following the practice of #REF , we used the editdistance 10 library to calculate the Phone Error Rate (PER). Having no ground truth phone annotations, we instead take Epitran's outputs as \"ground truth\" for comparison. <PERCEPT> The mean PER between the outputs is 23.7%. This result is consistent with #TARGET_REF , </PERCEPT> <INFO> which finds PERs as high as 72.8% when testing on on the Bukusu (bxk), Saamia (lsm) and East Tusom languages </INFO> <BACK> (an endangered subdialect of the Tungkhulic language family). </BACK> However, <PERCEPT> by training the phone recognizer on even minimal amounts of data in these languages, PERs were improved significantly. </PERCEPT>"
    },
    {
        "gold": {
            "text": [
                "Visual",
                "Entailment",
                "(VE):",
                "VE",
                "task",
                "aims",
                "to",
                "predict",
                "whether",
                "an",
                "image",
                "semantically",
                "entails",
                "the",
                "text",
                "and",
                "requires",
                "fine-grained",
                "reasoning",
                "ability",
                "in",
                "a",
                "model.",
                "VE",
                "dataset",
                "is",
                "built",
                "upon",
                "SNLI",
                "#TARGET_REF",
                "and",
                "Flickr30k.",
                "Each",
                "image-text",
                "pair",
                "is",
                "assigned",
                "with",
                "one",
                "of",
                "three",
                "classes:",
                "entailment,",
                "neutral,",
                "contradiction.",
                "As",
                "in",
                "UNITER,",
                "we",
                "formulate",
                "it",
                "as",
                "3-way",
                "classification",
                "problem",
                "based",
                "on",
                "h",
                "cls",
                ".",
                "The",
                "batch",
                "size",
                "is",
                "32",
                "per",
                "GPU",
                "while",
                "other",
                "finetuning",
                "strategies",
                "are",
                "the",
                "same."
            ],
            "context": [
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Visual Entailment (VE): VE task aims to predict whether an image semantically entails the text and requires fine-grained reasoning ability in a model. VE dataset is built upon SNLI #TARGET_REF and Flickr30k. Each image-text pair is assigned with one of three classes: entailment, neutral, contradiction. As in UNITER, we formulate it as 3-way classification problem based on h cls . The batch size is 32 per GPU while other finetuning strategies are the same.",
        "output": "<BACK> Visual Entailment (VE): VE task aims to predict whether an image semantically entails the text and requires fine-grained reasoning ability in a model. </BACK> <PERCEPT> VE dataset is built upon </PERCEPT> <INFO> SNLI #TARGET_REF </INFO> <BACK> and Flickr30k. </BACK> Each image-text pair is assigned with one of three classes: entailment, neutral, contradiction. As in UNITER, we formulate it as 3-way classification problem based on h cls . The batch size is 32 per GPU while other finetuning strategies are the same."
    },
    {
        "gold": {
            "text": [
                "Following",
                "the",
                "widely",
                "adopted",
                "fact",
                "verification",
                "pipeline",
                "#TARGET_REF",
                ",",
                "we",
                "take",
                "three",
                "steps",
                "to",
                "solve",
                "the",
                "FEVEROUS",
                "task",
                "(i)",
                "retrieving",
                "pages",
                "from",
                "the",
                "Wikipedia",
                "dump,",
                "(ii)",
                "extracting",
                "evidence",
                "from",
                "the",
                "retrieved",
                "pages,",
                "and",
                "(iii)",
                "verifying",
                "the",
                "claim",
                "according",
                "to",
                "extracted",
                "evidence."
            ],
            "context": [
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3
            ]
        },
        "input": "Following the widely adopted fact verification pipeline #TARGET_REF , we take three steps to solve the FEVEROUS task (i) retrieving pages from the Wikipedia dump, (ii) extracting evidence from the retrieved pages, and (iii) verifying the claim according to extracted evidence.",
        "output": "<PERCEPT> Following the widely adopted </PERCEPT> <INFO> fact verification pipeline #TARGET_REF , </INFO> <PERCEPT> we take three steps to solve the FEVEROUS task </PERCEPT> <BACK> (i) retrieving pages from the Wikipedia dump, (ii) extracting evidence from the retrieved pages, and (iii) verifying the claim according to extracted evidence. </BACK>"
    },
    {
        "gold": {
            "text": [
                "•",
                "Slot-gated",
                "#TARGET_REF",
                ")",
                "is",
                "an",
                "attentionbased",
                "BiLSTM",
                "model",
                "which",
                "builds",
                "on",
                "sepa-",
                "rate",
                "attended",
                "context",
                "for",
                "slot",
                "filling",
                "and",
                "intent",
                "classification",
                "while",
                "explicitly",
                "feeding",
                "the",
                "intent",
                "context",
                "into",
                "the",
                "process",
                "of",
                "slot",
                "filling",
                "via",
                "a",
                "gating",
                "mechanism.Metrics",
                "Model",
                "UCA",
                "U-F1(E)",
                "U-F1(I)",
                "U-F1(A)",
                "U-F1(O)",
                "T-F1",
                "T-F1(T)",
                "T-F1(S)",
                "T-F1(C)",
                "T-F1(D)",
                "T-F1(P)",
                "T-F1(O)",
                "JSA",
                "RNN-NLU",
                "(Liu•",
                "Inter-BiLSTM",
                "#REF",
                "combines",
                "two",
                "inter-connected",
                "BiLSTMs",
                "performing",
                "slot",
                "filling",
                "and",
                "intent",
                "classification",
                "respectively.",
                "The",
                "information",
                "flow",
                "between",
                "the",
                "two",
                "tasks",
                "occurs",
                "by",
                "passing",
                "the",
                "hidden",
                "states",
                "at",
                "each",
                "time",
                "step",
                "from",
                "each",
                "side",
                "to",
                "the",
                "other",
                "to",
                "support",
                "the",
                "decoding",
                "process."
            ],
            "context": [
                0,
                0,
                0,
                0,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                0,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "• Slot-gated #TARGET_REF ) is an attentionbased BiLSTM model which builds on sepa- rate attended context for slot filling and intent classification while explicitly feeding the intent context into the process of slot filling via a gating mechanism.Metrics Model UCA U-F1(E) U-F1(I) U-F1(A) U-F1(O) T-F1 T-F1(T) T-F1(S) T-F1(C) T-F1(D) T-F1(P) T-F1(O) JSA RNN-NLU (Liu• Inter-BiLSTM #REF combines two inter-connected BiLSTMs performing slot filling and intent classification respectively. The information flow between the two tasks occurs by passing the hidden states at each time step from each side to the other to support the decoding process.",
        "output": "• Slot-gated #TARGET_REF ) <INFO> is an attentionbased BiLSTM model which builds on sepa- rate attended context for slot filling and intent classification while explicitly feeding the intent context into the process of slot filling via a gating mechanism.Metrics Model UCA U-F1(E) U-F1(I) U-F1(A) U-F1(O) T-F1 T-F1(T) T-F1(S) T-F1(C) T-F1(D) T-F1(P) T-F1(O) JSA RNN-NLU </INFO> (Liu• Inter-BiLSTM <BACK> #REF combines two inter-connected BiLSTMs performing slot filling and intent classification respectively. </BACK> The information flow between the two tasks occurs by passing the hidden states at each time step from each side to the other to support the decoding process."
    },
    {
        "gold": {
            "text": [
                "about",
                "200K",
                "images",
                "and",
                "5.6M",
                "image-text",
                "pairs,",
                "where",
                "each",
                "image",
                "is",
                "associated",
                "with",
                "multiple",
                "captions.",
                "VQA2.0",
                "R@1",
                "/",
                "R@5/",
                "R@10",
                "R@1",
                "/",
                "R@5",
                "/",
                "R@10",
                "val",
                "/",
                "test",
                "dev",
                "/",
                "test-p",
                "test-dev",
                "/",
                "-std",
                "two-step",
                "pretraining",
                "ViLBert",
                "#REF",
                "MSCOCO-TR(5K)",
                "VCR",
                "R@1",
                "/",
                "R@5",
                "/",
                "R@10",
                "R@1",
                "/",
                "R@5",
                "/",
                "R@10",
                "R@1",
                "/",
                "R@5",
                "/",
                "R@10",
                "R@1",
                "/",
                "R@5",
                "/",
                "R@10",
                "Q→A",
                "QA→R",
                "Q→AR",
                "two-step",
                "pretraining",
                "Unicoder-VL",
                "#REF",
                "Implementation",
                "Details:",
                "We",
                "follow",
                "BERT",
                "to",
                "tokenize",
                "caption",
                "into",
                "word",
                "tokens",
                "by",
                "using",
                "Word-Piece,",
                "and",
                "resize",
                "the",
                "image",
                "into",
                "(800,",
                "1333)",
                "as",
                "prior",
                "works.",
                "For",
                "model",
                "architecture,",
                "a",
                "widely-used",
                "ResNet101",
                "for",
                "visual",
                "encoding",
                "and",
                "12-layer",
                "Transformer",
                "for",
                "multi-modal",
                "fusion",
                "are",
                "adopted",
                "for",
                "a",
                "fair",
                "comparison.",
                "Both",
                "networks",
                "are",
                "initialized",
                "with",
                "Im-ageNet",
                "and",
                "BERT",
                "pretrained",
                "parameters.",
                "Besides,",
                "following",
                "the",
                "majority",
                "of",
                "two-step",
                "methods,",
                "we",
                "apply",
                "the",
                "widely-used",
                "object",
                "detector",
                "BUTD",
                "#REF",
                "to",
                "generate",
                "object",
                "proposals",
                "as",
                "well",
                "as",
                "their",
                "RoI",
                "embeddings",
                "as",
                "our",
                "supervision."
            ],
            "context": [
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "about 200K images and 5.6M image-text pairs, where each image is associated with multiple captions. VQA2.0 R@1 / R@5/ R@10 R@1 / R@5 / R@10 val / test dev / test-p test-dev / -std two-step pretraining ViLBert #REF MSCOCO-TR(5K) VCR R@1 / R@5 / R@10 R@1 / R@5 / R@10 R@1 / R@5 / R@10 R@1 / R@5 / R@10 Q→A QA→R Q→AR two-step pretraining Unicoder-VL #REF Implementation Details: We follow BERT to tokenize caption into word tokens by using Word-Piece, and resize the image into (800, 1333) as prior works. For model architecture, a widely-used ResNet101 for visual encoding and 12-layer Transformer for multi-modal fusion are adopted for a fair comparison. Both networks are initialized with Im-ageNet and BERT pretrained parameters. Besides, following the majority of two-step methods, we apply the widely-used object detector BUTD #REF to generate object proposals as well as their RoI embeddings as our supervision.",
        "output": "<BACK> about 200K images and 5.6M image-text pairs, where each image is associated with multiple captions. </BACK> VQA2.0 R@1 / R@5/ R@10 R@1 / R@5 / R@10 val / test dev / test-p test-dev / -std two-step pretraining ViLBert #REF MSCOCO-TR(5K) VCR R@1 / R@5 / R@10 R@1 / R@5 / R@10 R@1 / R@5 / R@10 R@1 / R@5 / R@10 Q→A QA→R Q→AR two-step pretraining Unicoder-VL #REF Implementation Details: We follow BERT to tokenize caption into word tokens by using Word-Piece, and resize the image into (800, 1333) as prior works. For model architecture, a widely-used ResNet101 for visual encoding and 12-layer Transformer for multi-modal fusion are adopted for a fair comparison. Both networks are initialized with Im-ageNet and BERT pretrained parameters. Besides, following the majority of two-step methods, we apply the widely-used object detector BUTD #REF to generate object proposals as well as their RoI embeddings as our supervision."
    },
    {
        "gold": {
            "text": [
                "in",
                "working",
                "with",
                "methods",
                "that",
                "incorporate",
                "user",
                "and",
                "community",
                "information",
                "is",
                "having",
                "datasets",
                "where",
                "comments",
                "come",
                "from",
                "users",
                "belonging",
                "to",
                "some",
                "limited",
                "demographics",
                "only.",
                "We",
                "refer",
                "to",
                "this",
                "as",
                "demographic",
                "bias.",
                "Datasets",
                "with",
                "demographic",
                "bias",
                "will",
                "cause",
                "the",
                "methods",
                "to",
                "overfit",
                "to",
                "linguistic",
                "practices",
                "and",
                "dialects",
                "of",
                "users",
                "and",
                "communities",
                "belonging",
                "to",
                "specific",
                "demographics",
                "#REF",
                ",",
                "hence",
                "diminishing",
                "the",
                "power",
                "of",
                "the",
                "methods",
                "to",
                "generalize.",
                "In",
                "fact,",
                "this",
                "bias",
                "is",
                "not",
                "only",
                "a",
                "problem",
                "for",
                "methods",
                "we",
                "discussed,",
                "but",
                "for",
                "any",
                "NLP",
                "method",
                "in",
                "general.",
                "When",
                "it",
                "comes",
                "to",
                "methods",
                "that",
                "incorporate",
                "user",
                "or",
                "community",
                "information",
                "specifically,",
                "there",
                "are",
                "two",
                "other",
                "biases",
                "that",
                "must",
                "be",
                "kept",
                "in",
                "mind",
                "when",
                "constructing",
                "datasets,",
                "we",
                "refer",
                "to",
                "them",
                "as",
                "comment",
                "distribution",
                "bias",
                "and",
                "label",
                "distribution",
                "bias.",
                "Comment",
                "distribution",
                "bias",
                "occurs",
                "when",
                "the",
                "majority",
                "of",
                "comments",
                "in",
                "the",
                "dataset",
                "come",
                "from",
                "a",
                "small",
                "number",
                "of",
                "unique",
                "users.",
                "Such",
                "datasets",
                "allow",
                "the",
                "methods",
                "to",
                "simply",
                "overfit",
                "to",
                "the",
                "linguistic",
                "or",
                "social",
                "behaviors",
                "and",
                "community",
                "roles",
                "of",
                "specific",
                "users",
                "#REF",
                ".",
                "Label",
                "distribution",
                "bias",
                "occurs",
                "when",
                "only",
                "the",
                "abusive",
                "comments",
                "of",
                "a",
                "user",
                "are",
                "included",
                "in",
                "the",
                "dataset.",
                "Abuse",
                "is",
                "a",
                "relatively",
                "infrequent",
                "phenomenon,",
                "even",
                "at",
                "an",
                "individual",
                "level",
                "#REF",
                ".",
                "Only",
                "getting",
                "abusive",
                "comments",
                "of",
                "a",
                "user",
                "can",
                "make",
                "the",
                "methods",
                "simply",
                "associate",
                "the",
                "identity",
                "of",
                "the",
                "user",
                "to",
                "abusiveness",
                "when",
                "including",
                "user",
                "information.",
                "Moreover,",
                "datasets",
                "with",
                "this",
                "bias",
                "can",
                "also",
                "make",
                "phenomena",
                "like",
                "homophily",
                "appear",
                "overly",
                "effective",
                "in",
                "the",
                "detection",
                "of",
                "abuse",
                "by",
                "sampling",
                "only",
                "abusive",
                "comments",
                "from",
                "users",
                "who",
                "are",
                "close",
                "in",
                "the",
                "social",
                "network."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "in working with methods that incorporate user and community information is having datasets where comments come from users belonging to some limited demographics only. We refer to this as demographic bias. Datasets with demographic bias will cause the methods to overfit to linguistic practices and dialects of users and communities belonging to specific demographics #REF , hence diminishing the power of the methods to generalize. In fact, this bias is not only a problem for methods we discussed, but for any NLP method in general. When it comes to methods that incorporate user or community information specifically, there are two other biases that must be kept in mind when constructing datasets, we refer to them as comment distribution bias and label distribution bias. Comment distribution bias occurs when the majority of comments in the dataset come from a small number of unique users. Such datasets allow the methods to simply overfit to the linguistic or social behaviors and community roles of specific users #REF . Label distribution bias occurs when only the abusive comments of a user are included in the dataset. Abuse is a relatively infrequent phenomenon, even at an individual level #REF . Only getting abusive comments of a user can make the methods simply associate the identity of the user to abusiveness when including user information. Moreover, datasets with this bias can also make phenomena like homophily appear overly effective in the detection of abuse by sampling only abusive comments from users who are close in the social network.",
        "output": "in working with methods that incorporate user and community information is having datasets where comments come from users belonging to some limited demographics only. We refer to this as demographic bias. Datasets with demographic bias will cause the methods to overfit to linguistic practices and dialects of users and communities belonging to specific demographics #REF , hence diminishing the power of the methods to generalize. In fact, this bias is not only a problem for methods we discussed, but for any NLP method in general. When it comes to methods that incorporate user or community information specifically, there are two other biases that must be kept in mind when constructing datasets, we refer to them as comment distribution bias and label distribution bias. Comment distribution bias occurs when the majority of comments in the dataset come from a small number of unique users. Such datasets allow the methods to simply overfit to the linguistic or social behaviors and community roles of specific users #REF . Label distribution bias occurs when only the abusive comments of a user are included in the dataset. Abuse is a relatively infrequent phenomenon, even at an individual level #REF . Only getting abusive comments of a user can make the methods simply associate the identity of the user to abusiveness when including user information. Moreover, datasets with this bias can also make phenomena like homophily appear overly effective in the detection of abuse by sampling only abusive comments from users who are close in the social network."
    },
    {
        "gold": {
            "text": [
                "One",
                "of",
                "the",
                "most",
                "exciting",
                "outcomes",
                "is",
                "the",
                "deteriorated",
                "performance",
                "of",
                "the",
                "multilingual",
                "models",
                "using",
                "BT",
                "data,",
                "as",
                "we",
                "usually",
                "expect",
                "that",
                "added",
                "backtranslated",
                "texts",
                "would",
                "benefit",
                "performance.",
                "Using",
                "tags",
                "#REF",
                ")",
                "to",
                "differentiate",
                "which",
                "data",
                "is",
                "synthetic",
                "or",
                "not",
                "is",
                "only",
                "a",
                "simple",
                "step",
                "to",
                "address",
                "this",
                "issue,",
                "however,",
                "there",
                "could",
                "be",
                "evaluated",
                "more",
                "informed",
                "strategies",
                "for",
                "denoising",
                "or",
                "performing",
                "online",
                "data",
                "selection",
                "#TARGET_REF",
                "."
            ],
            "context": [
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1
            ]
        },
        "input": "One of the most exciting outcomes is the deteriorated performance of the multilingual models using BT data, as we usually expect that added backtranslated texts would benefit performance. Using tags #REF ) to differentiate which data is synthetic or not is only a simple step to address this issue, however, there could be evaluated more informed strategies for denoising or performing online data selection #TARGET_REF .",
        "output": "<PERCEPT> One of the most exciting outcomes is the deteriorated performance of the multilingual models using BT data, as we usually expect that added backtranslated texts would benefit performance. Using tags #REF ) to differentiate which data is synthetic or not is only a simple step to address this issue, </PERCEPT> however, <PERCEPT> there could be evaluated more informed </PERCEPT> <INFO> strategies for denoising or performing online data selection #TARGET_REF . </INFO>"
    },
    {
        "gold": {
            "text": [
                "As",
                "proposed",
                "by",
                "#TARGET_REF",
                ",",
                "adding",
                "the",
                "word",
                "embedding",
                "or",
                "distributive",
                "representation",
                "for",
                "the",
                "candidate",
                "strings",
                "can",
                "improve",
                "the",
                "performance",
                "of",
                "the",
                "model.",
                "The",
                "embeddings",
                "of",
                "the",
                "two",
                "candidate",
                "terms",
                "in",
                "the",
                "entity-attribute",
                "pair",
                "are",
                "concatenated",
                "to",
                "each",
                "side",
                "of",
                "the",
                "aggregated",
                "sentences",
                "vector",
                "described",
                "in",
                "the",
                "previous",
                "section.",
                "The",
                "embeddings",
                "for",
                "e",
                "and",
                "a",
                "are",
                "simply",
                "−",
                "→",
                "v",
                "e",
                "and",
                "−",
                "→",
                "v",
                "a",
                ".",
                "Thus",
                "the",
                "full",
                "representation",
                "of",
                "a",
                "entity-attribute",
                "pair",
                "is:−",
                "→",
                "v",
                "(e,a)",
                "=",
                "[",
                "−",
                "→",
                "v",
                "e",
                ",",
                "−",
                "→",
                "v",
                "sents(e,a)",
                ",",
                "−",
                "→",
                "v",
                "a",
                "]"
            ],
            "context": [
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3
            ]
        },
        "input": "As proposed by #TARGET_REF , adding the word embedding or distributive representation for the candidate strings can improve the performance of the model. The embeddings of the two candidate terms in the entity-attribute pair are concatenated to each side of the aggregated sentences vector described in the previous section. The embeddings for e and a are simply − → v e and − → v a . Thus the full representation of a entity-attribute pair is:− → v (e,a) = [ − → v e , − → v sents(e,a) , − → v a ]",
        "output": "<INFO> As proposed by #TARGET_REF , adding the word embedding or distributive representation for the candidate strings can improve the performance of the model. The embeddings of the two candidate terms in the entity-attribute pair are concatenated to each side of the aggregated sentences vector described in the previous section. </INFO> <BACK> The embeddings for e and a are simply − → v e and − → v a . Thus the full representation of a entity-attribute pair is:− → v (e,a) = [ − → v e , − → v sents(e,a) , − → v a ] </BACK>"
    },
    {
        "gold": {
            "text": [
                "Pretrained",
                "transformers",
                "such",
                "as",
                "BERT",
                "#REF",
                "have",
                "dramatically",
                "increased",
                "retrieval",
                "effectiveness",
                "in",
                "many",
                "tasks",
                "across",
                "a",
                "multitude",
                "of",
                "domains",
                "#REF",
                ".",
                "Nevertheless,",
                "in",
                "a",
                "standard",
                "\"retrieve-then-rerank\"",
                "setup,",
                "the",
                "application",
                "of",
                "pretrained",
                "transformer-based",
                "rerankers",
                "incurs",
                "large",
                "computational",
                "costs",
                "and",
                "long",
                "query",
                "latencies,",
                "making",
                "those",
                "rerankers",
                "unrealistic",
                "for",
                "many",
                "real-world",
                "applications.",
                "For",
                "example,",
                "according",
                "to",
                "the",
                "ColBERT",
                "paper",
                "#TARGET_REF",
                ",",
                "reranking",
                "1000",
                "hits",
                "from",
                "the",
                "MS",
                "MARCO",
                "passage",
                "dataset",
                "takes",
                "32.9",
                "seconds",
                "per",
                "query.",
                "Other",
                "researchers",
                "have",
                "noted",
                "the",
                "computational",
                "costs",
                "of",
                "transformer-based",
                "rankers",
                "(Hofstätter",
                "and",
                "Hanbury,",
                "*",
                "Equal",
                "contribution",
                "2019),",
                "and",
                "this",
                "realization",
                "has",
                "compelled",
                "the",
                "field",
                "to",
                "explore",
                "other",
                "approaches,",
                "for",
                "example,",
                "simplified",
                "models",
                "#REF",
                "and",
                "learned",
                "dense",
                "representations",
                "#REF",
                "."
            ],
            "context": [
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                0,
                3,
                3,
                1,
                1,
                1,
                1,
                1,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3
            ]
        },
        "input": "Pretrained transformers such as BERT #REF have dramatically increased retrieval effectiveness in many tasks across a multitude of domains #REF . Nevertheless, in a standard \"retrieve-then-rerank\" setup, the application of pretrained transformer-based rerankers incurs large computational costs and long query latencies, making those rerankers unrealistic for many real-world applications. For example, according to the ColBERT paper #TARGET_REF , reranking 1000 hits from the MS MARCO passage dataset takes 32.9 seconds per query. Other researchers have noted the computational costs of transformer-based rankers (Hofstätter and Hanbury, * Equal contribution 2019), and this realization has compelled the field to explore other approaches, for example, simplified models #REF and learned dense representations #REF .",
        "output": "<BACK> Pretrained transformers such as BERT #REF have dramatically increased retrieval effectiveness in many tasks across a multitude of domains #REF </BACK> . Nevertheless, <PERCEPT> in a standard \"retrieve-then-rerank\" setup, the application of pretrained transformer-based rerankers incurs large computational costs and long query latencies, making those rerankers unrealistic for many real-world applications. </PERCEPT> For example, <BACK> according to </BACK> <INFO> the ColBERT paper #TARGET_REF , </INFO> <BACK> reranking 1000 hits from the MS MARCO passage dataset takes 32.9 seconds per query. Other researchers have noted the computational costs of transformer-based rankers (Hofstätter and Hanbury, * Equal contribution 2019), and this realization has compelled the field to explore other approaches, for example, simplified models #REF and learned dense representations #REF . </BACK>"
    },
    {
        "gold": {
            "text": [
                "Interpretability",
                "There",
                "has",
                "been",
                "a",
                "lot",
                "of",
                "work",
                "on",
                "better",
                "interpreting",
                "models'",
                "decision",
                "process,",
                "e.g.,",
                "understanding",
                "BERT",
                "#REF",
                "and",
                "attention",
                "in",
                "transformers",
                "#REF",
                ",",
                "or",
                "through",
                "text",
                "generation",
                "models",
                "#REF",
                ".",
                "In",
                "this",
                "paper",
                "we",
                "utilize",
                "the",
                "attention",
                "scores",
                "as",
                "a",
                "generic",
                "way",
                "to",
                "understand",
                "what",
                "features",
                "a",
                "model",
                "relies",
                "on",
                "for",
                "making",
                "its",
                "predictions.",
                "Other",
                "common",
                "model",
                "interpretation",
                "techniques",
                "#REF",
                ",",
                "or",
                "more",
                "recent",
                "work",
                "on",
                "hierarchical",
                "attentions",
                "#REF",
                "and",
                "contrastive",
                "explanations",
                "#REF",
                ",",
                "can",
                "be",
                "used",
                "as",
                "well.",
                "In",
                "#REF",
                ",",
                "the",
                "authors",
                "found",
                "that",
                "attention",
                "scores",
                "can",
                "be",
                "manipulated",
                "to",
                "deceive",
                "human",
                "decision",
                "makers.",
                "The",
                "reliability",
                "of",
                "existing",
                "interpretation",
                "methods",
                "is",
                "a",
                "research",
                "topic",
                "by",
                "itself,",
                "and",
                "extra",
                "care",
                "needs",
                "to",
                "be",
                "taken",
                "when",
                "using",
                "attention",
                "for",
                "auditing",
                "models",
                "on",
                "fairness",
                "and",
                "accountability",
                "#TARGET_REF",
                "."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1
            ]
        },
        "input": "Interpretability There has been a lot of work on better interpreting models' decision process, e.g., understanding BERT #REF and attention in transformers #REF , or through text generation models #REF . In this paper we utilize the attention scores as a generic way to understand what features a model relies on for making its predictions. Other common model interpretation techniques #REF , or more recent work on hierarchical attentions #REF and contrastive explanations #REF , can be used as well. In #REF , the authors found that attention scores can be manipulated to deceive human decision makers. The reliability of existing interpretation methods is a research topic by itself, and extra care needs to be taken when using attention for auditing models on fairness and accountability #TARGET_REF .",
        "output": "Interpretability There has been a lot of work on better interpreting models' decision process, e.g., understanding BERT #REF and attention in transformers #REF , or through text generation models #REF . In this paper we utilize the attention scores as a generic way to understand what features a model relies on for making its predictions. Other common model interpretation techniques #REF , or more recent work on hierarchical attentions #REF and contrastive explanations #REF , can be used as well. In #REF , the authors found that attention scores can be manipulated to deceive human decision makers. The reliability of existing interpretation methods is a research topic by itself, and extra care needs to be taken when <INFO> using attention for auditing models on fairness and accountability #TARGET_REF . </INFO>"
    },
    {
        "gold": {
            "text": [
                "Despite",
                "great",
                "progress",
                "has",
                "been",
                "made",
                "over",
                "improved",
                "accuracy,",
                "deep",
                "learning",
                "models",
                "are",
                "known",
                "to",
                "be",
                "brittle",
                "to",
                "out-of-domain",
                "data",
                "#TARGET_REF",
                ",",
                "adversarial",
                "attacks",
                "#REF",
                ",",
                "partly",
                "due",
                "to",
                "sometimes",
                "the",
                "models",
                "have",
                "exploited",
                "spurious",
                "correlations",
                "in",
                "the",
                "existing",
                "training",
                "data",
                "#REF",
                ".",
                "In",
                "Figure",
                "1,",
                "we",
                "show",
                "an",
                "example",
                "of",
                "a",
                "sentiment",
                "classification",
                "model",
                "making",
                "spurious",
                "correlations",
                "over",
                "the",
                "phrases",
                "\"Spielberg\"",
                "and",
                "\"New",
                "York",
                "Subway\"",
                "due",
                "to",
                "their",
                "high",
                "co-occurrences",
                "with",
                "positive",
                "and",
                "negative",
                "labels",
                "respectively",
                "in",
                "the",
                "training",
                "data."
            ],
            "context": [
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Despite great progress has been made over improved accuracy, deep learning models are known to be brittle to out-of-domain data #TARGET_REF , adversarial attacks #REF , partly due to sometimes the models have exploited spurious correlations in the existing training data #REF . In Figure 1, we show an example of a sentiment classification model making spurious correlations over the phrases \"Spielberg\" and \"New York Subway\" due to their high co-occurrences with positive and negative labels respectively in the training data.",
        "output": "<BACK> Despite great progress has been made over improved accuracy, </BACK> <INFO> deep learning models are known to be brittle to out-of-domain data #TARGET_REF , </INFO> <BACK> adversarial attacks #REF , partly due to sometimes the models have exploited spurious correlations in the existing training data #REF . </BACK> In Figure 1, we show an example of a sentiment classification model making spurious correlations over the phrases \"Spielberg\" and \"New York Subway\" due to their high co-occurrences with positive and negative labels respectively in the training data."
    },
    {
        "gold": {
            "text": [
                "A",
                "semantic",
                "analysis",
                "of",
                "the",
                "selected",
                "keywords",
                "was",
                "done",
                "using",
                "WordNet",
                "(3.1).",
                "We",
                "used",
                "semantic",
                "relations",
                "such",
                "as",
                "hypernymy,",
                "troponymy",
                "and",
                "entailment",
                "#TARGET_REF",
                "to",
                "find",
                "the",
                "implications",
                "that",
                "the",
                "keywords",
                "may",
                "have,",
                "as",
                "far",
                "as",
                "their",
                "communicative",
                "goals",
                "are",
                "concerned."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                3,
                3,
                3,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2
            ]
        },
        "input": "A semantic analysis of the selected keywords was done using WordNet (3.1). We used semantic relations such as hypernymy, troponymy and entailment #TARGET_REF to find the implications that the keywords may have, as far as their communicative goals are concerned.",
        "output": "A semantic analysis of the selected keywords was done using WordNet (3.1). <PERCEPT> We used semantic relations such as </PERCEPT> <BACK> hypernymy, troponymy and </BACK> <INFO> entailment #TARGET_REF </INFO> <PERCEPT> to find the implications that the keywords may have, as far as their communicative goals are concerned. </PERCEPT>"
    },
    {
        "gold": {
            "text": [
                "Totale",
                "toolkit",
                "#REF",
                "was",
                "used",
                "to",
                "POS",
                "tag",
                "#TARGET_REF",
                "and",
                "lemmatize",
                "#REF",
                "words",
                "in",
                "the",
                "bilingual",
                "word",
                "list,",
                "POS",
                "tagger",
                "was",
                "also",
                "used",
                "in",
                "automatic",
                "paradigm",
                "classifying,",
                "see",
                "chapter",
                "3.3.1",
                "for",
                "further",
                "description."
            ],
            "context": [
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3
            ]
        },
        "input": "Totale toolkit #REF was used to POS tag #TARGET_REF and lemmatize #REF words in the bilingual word list, POS tagger was also used in automatic paradigm classifying, see chapter 3.3.1 for further description.",
        "output": "<PERCEPT> Totale toolkit #REF was used to </PERCEPT> <INFO> POS tag #TARGET_REF </INFO> <BACK> and lemmatize #REF words in the bilingual word list, POS tagger was also used in automatic paradigm classifying, see chapter 3.3.1 for further description. </BACK>"
    },
    {
        "gold": {
            "text": [
                "We",
                "would",
                "also",
                "like",
                "to",
                "validate",
                "our",
                "methods",
                "on",
                "a",
                "variety",
                "of",
                "other",
                "data",
                "sets",
                "and",
                "tasks.",
                "We",
                "selected",
                "the",
                "MasakhaNER",
                "dataset",
                "for",
                "evaluation",
                "because",
                "we",
                "specifically",
                "wished",
                "to",
                "evaluate",
                "results",
                "on",
                "ac-tual",
                "low-resource",
                "languages",
                "supported",
                "by",
                "both",
                "Allosaurus",
                "and",
                "Epitran.",
                "While",
                "there",
                "are",
                "still,",
                "we",
                "argue,",
                "detectable",
                "improvements",
                "in",
                "downstream",
                "results",
                "with",
                "our",
                "method,",
                "further",
                "work",
                "would",
                "benefit",
                "from",
                "additional",
                "evaluations",
                "on",
                "other",
                "data",
                "sets",
                "or",
                "tasks.",
                "In",
                "particular,",
                "the",
                "Swahili",
                "News",
                "Classification",
                "corpus",
                "#TARGET_REF",
                "corpus",
                "may",
                "provide",
                "a",
                "useful",
                "evaluation."
            ],
            "context": [
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1
            ]
        },
        "input": "We would also like to validate our methods on a variety of other data sets and tasks. We selected the MasakhaNER dataset for evaluation because we specifically wished to evaluate results on ac-tual low-resource languages supported by both Allosaurus and Epitran. While there are still, we argue, detectable improvements in downstream results with our method, further work would benefit from additional evaluations on other data sets or tasks. In particular, the Swahili News Classification corpus #TARGET_REF corpus may provide a useful evaluation.",
        "output": "<BACK> We would also like to validate our methods on a variety of other data sets and tasks. We selected the MasakhaNER dataset for evaluation because we specifically wished to evaluate results on ac-tual low-resource languages supported by both Allosaurus and Epitran. </BACK> While there are still, we argue, detectable improvements in downstream results with our method, further work would benefit from additional evaluations on other data sets or tasks. In particular, <INFO> the Swahili News Classification corpus #TARGET_REF corpus may provide a useful evaluation. </INFO>"
    },
    {
        "gold": {
            "text": [
                "Hierarchical",
                "Phrase-Based",
                "(HPB)",
                "SMT",
                "#REF",
                "is",
                "a",
                "tree-based",
                "model",
                "which",
                "extracts",
                "a",
                "synchronous",
                "Context-Free",
                "Grammar",
                "(CFG)",
                "automatically",
                "from",
                "the",
                "training",
                "corpus.",
                "HPB",
                "SMT",
                "is",
                "based",
                "on",
                "phrases",
                "extracted",
                "according",
                "to",
                "the",
                "PB",
                "model",
                "#TARGET_REF",
                ".",
                "Thus,",
                "HPB",
                "SMT",
                "tries",
                "to",
                "build",
                "upon",
                "the",
                "strengths",
                "of",
                "PB",
                "SMT",
                "and",
                "adds",
                "to",
                "it",
                "the",
                "ability",
                "to",
                "translate",
                "discontinuous",
                "phrases",
                "and",
                "learn",
                "phrase-reordering",
                "in",
                "hierarchical",
                "rules",
                "without",
                "a",
                "separate",
                "reordering",
                "model.",
                "HPB",
                "SMT",
                "uses",
                "hierarchical",
                "rules",
                "as",
                "a",
                "translation",
                "unit.",
                "These",
                "rules",
                "are",
                "rewrite",
                "rules",
                "with",
                "aligned",
                "pairs",
                "of",
                "right-hand",
                "sides,",
                "taking",
                "the",
                "following",
                "form:"
            ],
            "context": [
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Hierarchical Phrase-Based (HPB) SMT #REF is a tree-based model which extracts a synchronous Context-Free Grammar (CFG) automatically from the training corpus. HPB SMT is based on phrases extracted according to the PB model #TARGET_REF . Thus, HPB SMT tries to build upon the strengths of PB SMT and adds to it the ability to translate discontinuous phrases and learn phrase-reordering in hierarchical rules without a separate reordering model. HPB SMT uses hierarchical rules as a translation unit. These rules are rewrite rules with aligned pairs of right-hand sides, taking the following form:",
        "output": "<BACK> Hierarchical Phrase-Based (HPB) SMT #REF is a tree-based model which extracts a synchronous Context-Free Grammar (CFG) automatically from the training corpus. </BACK> <PERCEPT> HPB SMT is based on phrases extracted according to </PERCEPT> <INFO> the PB model #TARGET_REF . </INFO> <PERCEPT> Thus, HPB SMT tries to build upon the strengths of PB SMT and adds to it the ability to translate discontinuous phrases and learn phrase-reordering in hierarchical rules without a separate reordering model. </PERCEPT> HPB SMT uses hierarchical rules as a translation unit. These rules are rewrite rules with aligned pairs of right-hand sides, taking the following form:"
    },
    {
        "gold": {
            "text": [
                "The",
                "key",
                "insight",
                "of",
                "this",
                "paper",
                "is",
                "that",
                "entities",
                "that",
                "share",
                "many",
                "attributes",
                "are",
                "often",
                "similar.",
                "This",
                "is",
                "an",
                "extension",
                "of",
                "the",
                "distributional",
                "hypothesis,",
                "#TARGET_REF",
                ",",
                "which",
                "states",
                "that",
                "words",
                "with",
                "similar",
                "semantic",
                "meanings",
                "tend",
                "to",
                "appear",
                "in",
                "similar",
                "contexts,",
                "and",
                "builds",
                "on",
                "work",
                "that",
                "use",
                "referential",
                "attributes",
                "to",
                "estimate",
                "semantic",
                "relatedness",
                "#REF",
                ".",
                "For",
                "the",
                "attribute-aware",
                "embeddings,",
                "we",
                "argue",
                "that",
                "a",
                "good",
                "representation",
                "for",
                "an",
                "entity",
                "can",
                "be",
                "inferred",
                "from",
                "its",
                "most",
                "common",
                "attributes,",
                "which",
                "we",
                "may",
                "have",
                "access",
                "to",
                "from",
                "an",
                "external",
                "source",
                "of",
                "knowledge.",
                "In",
                "Figure",
                "1,",
                "we",
                "want",
                "to",
                "classify",
                "two",
                "candidate",
                "relations",
                "given",
                "the",
                "other",
                "known",
                "relations."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The key insight of this paper is that entities that share many attributes are often similar. This is an extension of the distributional hypothesis, #TARGET_REF , which states that words with similar semantic meanings tend to appear in similar contexts, and builds on work that use referential attributes to estimate semantic relatedness #REF . For the attribute-aware embeddings, we argue that a good representation for an entity can be inferred from its most common attributes, which we may have access to from an external source of knowledge. In Figure 1, we want to classify two candidate relations given the other known relations.",
        "output": "The key insight of this paper is that entities that share many attributes are often similar. This is an extension of <INFO> the distributional hypothesis, #TARGET_REF , which states that words with similar semantic meanings tend to appear in similar contexts, </INFO> <BACK> and builds on work that use referential attributes to estimate semantic relatedness #REF . </BACK> For the attribute-aware embeddings, we argue that a good representation for an entity can be inferred from its most common attributes, which we may have access to from an external source of knowledge. In Figure 1, we want to classify two candidate relations given the other known relations."
    },
    {
        "gold": {
            "text": [
                "Thus",
                "there",
                "are",
                "a",
                "number",
                "of",
                "situations",
                "in",
                "which",
                "gCNs",
                "are",
                "not",
                "sufficient.",
                "Given",
                "that",
                "gCNs",
                "can",
                "be",
                "represented",
                "by",
                "Tree",
                "Substitution",
                "Grammars,",
                "as",
                "in",
                "#TARGET_REF",
                ",",
                "which",
                "are",
                "in",
                "fact",
                "TAGs",
                "that",
                "do",
                "not",
                "allow",
                "precisely",
                "the",
                "kind",
                "of",
                "unbounded",
                "phenomena",
                "described",
                "by",
                "TAGs,",
                "this",
                "would",
                "suggest",
                "that",
                "using",
                "a",
                "TAG",
                "grammar",
                "to",
                "describe",
                "the",
                "gNCNs",
                "in",
                "order",
                "to",
                "decompose",
                "the",
                "trees",
                "would",
                "be",
                "feasible,",
                "and",
                "this",
                "is",
                "further",
                "an",
                "interesting",
                "question",
                "for",
                "theoretical",
                "reasons",
                "described",
                "below."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Thus there are a number of situations in which gCNs are not sufficient. Given that gCNs can be represented by Tree Substitution Grammars, as in #TARGET_REF , which are in fact TAGs that do not allow precisely the kind of unbounded phenomena described by TAGs, this would suggest that using a TAG grammar to describe the gNCNs in order to decompose the trees would be feasible, and this is further an interesting question for theoretical reasons described below.",
        "output": "Thus there are a number of situations in which gCNs are not sufficient. <PERCEPT> Given that </PERCEPT> <INFO> gCNs can be represented by Tree Substitution Grammars, </INFO> <PERCEPT> as in #TARGET_REF , </PERCEPT> <INFO> which are in fact TAGs that do not allow precisely the kind of unbounded phenomena described by TAGs, </INFO> <PERCEPT> this would suggest that using a TAG grammar to describe the gNCNs in order to decompose the trees would be feasible, </PERCEPT> and this is further an interesting question for theoretical reasons described below."
    },
    {
        "gold": {
            "text": [
                "•",
                "cosine",
                "similarity",
                "in",
                "Skip-gram",
                "#TARGET_REF",
                ","
            ],
            "context": [
                0,
                1,
                1,
                1,
                1,
                1,
                1
            ]
        },
        "input": "• cosine similarity in Skip-gram #TARGET_REF ,",
        "output": "• <INFO> cosine similarity in Skip-gram #TARGET_REF , </INFO>"
    },
    {
        "gold": {
            "text": [
                "The",
                "generator",
                "is",
                "based",
                "on",
                "a",
                "Relational",
                "Memory",
                "with",
                "self-attention",
                "#TARGET_REF",
                ".",
                "This",
                "model",
                "updates",
                "its",
                "\"internal",
                "values\"",
                "and",
                "produces",
                "its",
                "final",
                "output",
                "by",
                "selecting",
                "from",
                "its",
                "memory",
                "cells",
                "with",
                "a",
                "self-attention",
                "mechanism.",
                "Leveraging",
                "an",
                "idea",
                "similar",
                "to",
                "that",
                "of",
                "image-based",
                "conditional",
                "GANs",
                "#REF",
                ",",
                "we",
                "introduce",
                "an",
                "external",
                "conditioning",
                "into",
                "the",
                "generator.",
                "First,",
                "given",
                "the",
                "conditioning",
                "input",
                "c",
                "2",
                "R",
                "d",
                ",",
                "the",
                "model",
                "computes",
                "an",
                "embedding",
                "t",
                "for",
                "c",
                "using",
                "functionf",
                "✓",
                ":",
                "R",
                "d",
                "!",
                "R",
                "m",
                ",",
                "with",
                "m",
                "&lt,",
                "d.Function",
                "f",
                "✓",
                "has",
                "been",
                "implemented",
                "using",
                "a",
                "feed-forward",
                "neural",
                "network",
                "with",
                "a",
                "self-attention",
                "layer.",
                "The",
                "conditioning",
                "vector",
                "c",
                "may",
                "originate",
                "from",
                "any",
                "type",
                "of",
                "different",
                "source",
                "as",
                "long",
                "as",
                "it",
                "remains",
                "consistent",
                "during",
                "the",
                "individual",
                "training.",
                "Depending",
                "on",
                "the",
                "required",
                "task,",
                "as",
                "shown",
                "in",
                "the",
                "experiment",
                "phase,",
                "it",
                "will",
                "change.",
                "This",
                "vector",
                "c",
                "is",
                "the",
                "only",
                "link",
                "between",
                "the",
                "conditioning",
                "and",
                "the",
                "generative",
                "model,",
                "its",
                "influence",
                "on",
                "the",
                "final",
                "output",
                "will",
                "be",
                "crucial",
                "for",
                "the",
                "conditioning",
                "of",
                "the",
                "generated",
                "sentence.",
                "f",
                "✓",
                "has",
                "been",
                "adopted",
                "to",
                "give",
                "the",
                "model",
                "the",
                "ability",
                "to",
                "learn",
                "the",
                "best",
                "manipulation",
                "of",
                "the",
                "conditioning",
                "vector",
                "to",
                "insert",
                "into",
                "the",
                "memory."
            ],
            "context": [
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The generator is based on a Relational Memory with self-attention #TARGET_REF . This model updates its \"internal values\" and produces its final output by selecting from its memory cells with a self-attention mechanism. Leveraging an idea similar to that of image-based conditional GANs #REF , we introduce an external conditioning into the generator. First, given the conditioning input c 2 R d , the model computes an embedding t for c using functionf ✓ : R d ! R m , with m &lt, d.Function f ✓ has been implemented using a feed-forward neural network with a self-attention layer. The conditioning vector c may originate from any type of different source as long as it remains consistent during the individual training. Depending on the required task, as shown in the experiment phase, it will change. This vector c is the only link between the conditioning and the generative model, its influence on the final output will be crucial for the conditioning of the generated sentence. f ✓ has been adopted to give the model the ability to learn the best manipulation of the conditioning vector to insert into the memory.",
        "output": "<PERCEPT> The generator is based on </PERCEPT> <INFO> a Relational Memory with self-attention #TARGET_REF . </INFO> <PERCEPT> This model updates its \"internal values\" and produces its final output by selecting from its memory cells with a self-attention mechanism. </PERCEPT> Leveraging an idea similar to that of image-based conditional GANs #REF , we introduce an external conditioning into the generator. First, given the conditioning input c 2 R d , the model computes an embedding t for c using functionf ✓ : R d ! R m , with m &lt, d.Function f ✓ has been implemented using a feed-forward neural network with a self-attention layer. The conditioning vector c may originate from any type of different source as long as it remains consistent during the individual training. Depending on the required task, as shown in the experiment phase, it will change. This vector c is the only link between the conditioning and the generative model, its influence on the final output will be crucial for the conditioning of the generated sentence. f ✓ has been adopted to give the model the ability to learn the best manipulation of the conditioning vector to insert into the memory."
    },
    {
        "gold": {
            "text": [
                "The",
                "syntax",
                "discriminator",
                "takes",
                "as",
                "input",
                "either",
                "a",
                "real",
                "sentence,",
                "r",
                "=",
                "(r",
                "1",
                ",",
                ".",
                ".",
                ".",
                ",",
                "r",
                "n",
                "),",
                "or",
                "a",
                "generated",
                "one,g",
                "=",
                "(g",
                "1",
                ",",
                ".",
                ".",
                ".",
                ",",
                "g",
                "n",
                ").Similarly",
                "to",
                "many",
                "other",
                "works",
                "(e.g.,",
                "#TARGET_REF",
                "),",
                "the",
                "discriminator",
                "first",
                "transforms",
                "its",
                "input",
                "into",
                "an",
                "embedding",
                "matrix.",
                "This",
                "embedding",
                "allows",
                "learning",
                "a",
                "transformation",
                "that",
                "condenses",
                "the",
                "information",
                "brought",
                "in",
                "by",
                "each",
                "word",
                "optimally",
                "for",
                "any",
                "given",
                "task.",
                "The",
                "syntax",
                "discriminator",
                "is",
                "then",
                "built",
                "using",
                "two",
                "convolutional",
                "layers",
                "with",
                "ReLU",
                "activation",
                "functions,",
                "followed",
                "by",
                "a",
                "self-attention",
                "layer,",
                "again",
                "followed",
                "by",
                "two",
                "other",
                "convolutional",
                "layers",
                "with",
                "ReLU",
                "activation",
                "functions.",
                "The",
                "selfattention",
                "layer",
                "is",
                "used",
                "to",
                "attend",
                "to",
                "the",
                "output",
                "of",
                "the",
                "previous",
                "convolutional",
                "layer",
                "and",
                "select",
                "the",
                "most",
                "useful",
                "features.",
                "The",
                "final",
                "layers",
                "generate",
                "the",
                "decision."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The syntax discriminator takes as input either a real sentence, r = (r 1 , . . . , r n ), or a generated one,g = (g 1 , . . . , g n ).Similarly to many other works (e.g., #TARGET_REF ), the discriminator first transforms its input into an embedding matrix. This embedding allows learning a transformation that condenses the information brought in by each word optimally for any given task. The syntax discriminator is then built using two convolutional layers with ReLU activation functions, followed by a self-attention layer, again followed by two other convolutional layers with ReLU activation functions. The selfattention layer is used to attend to the output of the previous convolutional layer and select the most useful features. The final layers generate the decision.",
        "output": "The syntax discriminator takes as input either a real sentence, r = (r 1 , . . . , r n ), or a generated one,g = (g 1 , . . . , g n <PERCEPT> ).Similarly to </PERCEPT> <INFO> many other works (e.g., #TARGET_REF ), </INFO> <PERCEPT> the discriminator first transforms its input into an embedding matrix. </PERCEPT> <BACK> This embedding allows learning a transformation that condenses the information brought in by each word optimally for any given task. </BACK> The syntax discriminator is then built using two convolutional layers with ReLU activation functions, followed by a self-attention layer, again followed by two other convolutional layers with ReLU activation functions. The selfattention layer is used to attend to the output of the previous convolutional layer and select the most useful features. The final layers generate the decision."
    },
    {
        "gold": {
            "text": [
                "Neural",
                "approaches",
                "to",
                "sequence",
                "tagging",
                "are",
                "common",
                "due",
                "to",
                "extensive",
                "developments",
                "in",
                "named",
                "entity",
                "recognition.",
                "#REF",
                "introduced",
                "and",
                "cultivated",
                "the",
                "use",
                "of",
                "bidirectional",
                "LSTMs",
                "to",
                "incorporate",
                "features",
                "that",
                "could",
                "be",
                "used",
                "for",
                "sequence",
                "tagging",
                "using",
                "a",
                "CRF.",
                "#REF",
                "'s",
                "architecture",
                "and",
                "the",
                "NeuroNER",
                "program",
                "#REF",
                "provided",
                "a",
                "basic",
                "architecture",
                "and",
                "influenced",
                "multiple",
                "developments",
                "to",
                "most",
                "sequence",
                "labeling",
                "tasks,",
                "including",
                "event",
                "#REF",
                ".",
                "The",
                "task",
                "of",
                "event",
                "extraction",
                "in",
                "any",
                "language",
                "involves",
                "the",
                "identification",
                "of",
                "the",
                "event",
                "nugget",
                "#TARGET_REF",
                ".",
                "Prominent",
                "work",
                "has",
                "been",
                "done",
                "to",
                "analyze",
                "the",
                "lexical",
                "and",
                "semantic",
                "features",
                "of",
                "event",
                "representation",
                "#REF",
                ",",
                "which",
                "served",
                "as",
                "a",
                "basis",
                "for",
                "neural",
                "event",
                "nugget",
                "detection",
                "#REF",
                "."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3
            ]
        },
        "input": "Neural approaches to sequence tagging are common due to extensive developments in named entity recognition. #REF introduced and cultivated the use of bidirectional LSTMs to incorporate features that could be used for sequence tagging using a CRF. #REF 's architecture and the NeuroNER program #REF provided a basic architecture and influenced multiple developments to most sequence labeling tasks, including event #REF . The task of event extraction in any language involves the identification of the event nugget #TARGET_REF . Prominent work has been done to analyze the lexical and semantic features of event representation #REF , which served as a basis for neural event nugget detection #REF .",
        "output": "Neural approaches to sequence tagging are common due to extensive developments in named entity recognition. #REF introduced and cultivated the use of bidirectional LSTMs to incorporate features that could be used for sequence tagging using a CRF. #REF 's architecture and the NeuroNER program #REF provided a basic architecture and influenced multiple developments to most sequence labeling tasks, including event #REF . <INFO> The task of event extraction in any language involves the identification of the event nugget #TARGET_REF </INFO> . <BACK> Prominent work has been done to analyze the lexical and semantic features of event representation #REF , which served as a basis for neural event nugget detection #REF . </BACK>"
    },
    {
        "gold": {
            "text": [
                "Our",
                "baseline",
                "model",
                "is",
                "inspired",
                "by",
                "a",
                "hypernym",
                "classification",
                "model",
                "proposed",
                "by",
                "#TARGET_REF",
                ",",
                "also",
                "using",
                "a",
                "pair",
                "of",
                "terms",
                "with",
                "a",
                "set",
                "of",
                "support",
                "sentences",
                "where",
                "the",
                "terms",
                "co-occur."
            ],
            "context": [
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                0,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1
            ]
        },
        "input": "Our baseline model is inspired by a hypernym classification model proposed by #TARGET_REF , also using a pair of terms with a set of support sentences where the terms co-occur.",
        "output": "<PERCEPT> Our baseline model is inspired by </PERCEPT> <INFO> a hypernym classification model proposed by #TARGET_REF , </INFO> also <INFO> using a pair of terms with a set of support sentences where the terms co-occur. </INFO>"
    },
    {
        "gold": {
            "text": [
                "WorldTree.",
                "A",
                "number",
                "of",
                "approaches",
                "have",
                "been",
                "proposed",
                "for",
                "the",
                "explanation",
                "regeneration",
                "task",
                "on",
                "WorldTree,",
                "including",
                "those",
                "from",
                "previous",
                "iterations",
                "of",
                "this",
                "shared",
                "task.",
                "These",
                "approaches",
                "adopt",
                "a",
                "set",
                "of",
                "diverse",
                "techniques",
                "ranging",
                "from",
                "graph-based",
                "learning",
                "#REF",
                ",",
                "to",
                "Transformer-based",
                "language",
                "models",
                "#TARGET_REF",
                ",",
                "Integer",
                "Linear",
                "Programming",
                "#REF",
                ",",
                "and",
                "sparse",
                "retrieval",
                "models",
                "#REF",
                "."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3
            ]
        },
        "input": "WorldTree. A number of approaches have been proposed for the explanation regeneration task on WorldTree, including those from previous iterations of this shared task. These approaches adopt a set of diverse techniques ranging from graph-based learning #REF , to Transformer-based language models #TARGET_REF , Integer Linear Programming #REF , and sparse retrieval models #REF .",
        "output": "WorldTree. A number of approaches have been proposed for the explanation regeneration task on WorldTree, including those from previous iterations of this shared task. <PERCEPT> These approaches adopt a set of diverse techniques ranging from graph-based learning #REF , to </PERCEPT> <INFO> Transformer-based language models #TARGET_REF , </INFO> <BACK> Integer Linear Programming #REF , and sparse retrieval models #REF . </BACK>"
    },
    {
        "gold": {
            "text": [
                "mini-batches)",
                "2",
                ",",
                "we",
                "can",
                "indeed",
                "obtain",
                "A×B",
                "−1",
                "negatives",
                "for",
                "a",
                "given",
                "question,",
                "which",
                "is",
                "approximately",
                "A",
                "times",
                "as",
                "many",
                "as",
                "the",
                "original",
                "number",
                "of",
                "in-batch",
                "negatives.",
                "In",
                "this",
                "way,",
                "we",
                "can",
                "use",
                "more",
                "negatives",
                "in",
                "the",
                "training",
                "objective",
                "of",
                "Equation",
                "2,",
                "so",
                "that",
                "the",
                "results",
                "are",
                "expected",
                "to",
                "be",
                "improved.",
                "Denoised",
                "Hard",
                "Negatives",
                "Although",
                "the",
                "above",
                "strategy",
                "can",
                "increase",
                "the",
                "number",
                "of",
                "negatives,",
                "most",
                "of",
                "negatives",
                "are",
                "easy",
                "ones,",
                "which",
                "can",
                "be",
                "easily",
                "discriminated.",
                "While,",
                "hard",
                "negatives",
                "are",
                "shown",
                "to",
                "be",
                "important",
                "to",
                "train",
                "a",
                "dual-encoder",
                "#TARGET_REF",
                ".",
                "To",
                "obtain",
                "hard",
                "negatives,",
                "a",
                "straightforward",
                "method",
                "is",
                "to",
                "select",
                "the",
                "top-ranked",
                "passages",
                "(excluding",
                "the",
                "labeled",
                "positive",
                "passages)",
                "as",
                "negative",
                "samples.",
                "However,",
                "it",
                "is",
                "likely",
                "to",
                "bring",
                "false",
                "negatives",
                "(i.e.,",
                "unlabeled",
                "positives),",
                "since",
                "the",
                "annotators",
                "can",
                "only",
                "annotate",
                "a",
                "few",
                "top-retrieved",
                "passages",
                "(as",
                "discussed",
                "in",
                "Section",
                "1).",
                "Another",
                "note",
                "is",
                "that",
                "previous",
                "work",
                "mainly",
                "focuses",
                "on",
                "factoid",
                "questions,",
                "to",
                "which",
                "the",
                "answers",
                "are",
                "short",
                "and",
                "concise.",
                "Hence,",
                "it",
                "is",
                "not",
                "challenging",
                "to",
                "filter",
                "false",
                "negatives",
                "by",
                "using",
                "the",
                "short",
                "answers",
                "#REF",
                ".",
                "However,",
                "it",
                "cannot",
                "apply",
                "to",
                "non-factoid",
                "questions.",
                "In",
                "this",
                "paper,",
                "we",
                "aim",
                "to",
                "learn",
                "dense",
                "passage",
                "retrieval",
                "for",
                "both",
                "factoid",
                "questions",
                "and",
                "non-factoid",
                "questions,",
                "which",
                "needs",
                "a",
                "more",
                "effective",
                "way",
                "for",
                "denoising",
                "hard",
                "negatives."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                3,
                3,
                3,
                3,
                3,
                2,
                2,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "mini-batches) 2 , we can indeed obtain A×B −1 negatives for a given question, which is approximately A times as many as the original number of in-batch negatives. In this way, we can use more negatives in the training objective of Equation 2, so that the results are expected to be improved. Denoised Hard Negatives Although the above strategy can increase the number of negatives, most of negatives are easy ones, which can be easily discriminated. While, hard negatives are shown to be important to train a dual-encoder #TARGET_REF . To obtain hard negatives, a straightforward method is to select the top-ranked passages (excluding the labeled positive passages) as negative samples. However, it is likely to bring false negatives (i.e., unlabeled positives), since the annotators can only annotate a few top-retrieved passages (as discussed in Section 1). Another note is that previous work mainly focuses on factoid questions, to which the answers are short and concise. Hence, it is not challenging to filter false negatives by using the short answers #REF . However, it cannot apply to non-factoid questions. In this paper, we aim to learn dense passage retrieval for both factoid questions and non-factoid questions, which needs a more effective way for denoising hard negatives.",
        "output": "mini-batches) 2 , we can indeed obtain A×B −1 negatives for a given question, which is approximately A times as many as the original number of in-batch negatives. In this way, we can use more negatives in the training objective of Equation 2, so that the results are expected to be improved. Denoised Hard Negatives Although the above strategy can increase the number of negatives, most of negatives are easy ones, which can be easily discriminated. While, <INFO> hard negatives are shown to be important to train a dual-encoder #TARGET_REF . </INFO> <PERCEPT> To obtain hard negatives, a straightforward method is to select the top-ranked passages </PERCEPT> <BACK> (excluding the labeled positive passages) </BACK> <PERCEPT> as negative samples. </PERCEPT> However, it is likely to bring false negatives (i.e., unlabeled positives), since the annotators can only annotate a few top-retrieved passages (as discussed in Section 1). Another note is that previous work mainly focuses on factoid questions, to which the answers are short and concise. Hence, it is not challenging to filter false negatives by using the short answers #REF . However, it cannot apply to non-factoid questions. In this paper, we aim to learn dense passage retrieval for both factoid questions and non-factoid questions, which needs a more effective way for denoising hard negatives."
    },
    {
        "gold": {
            "text": [
                "Task",
                "Definition",
                "There",
                "are",
                "two",
                "subtasks",
                "in",
                "the",
                "LCP",
                "task.",
                "For",
                "subtask",
                "1,",
                "the",
                "goal",
                "is",
                "to",
                "predict",
                "the",
                "complexity",
                "score",
                "for",
                "a",
                "single",
                "word",
                "from",
                "the",
                "given",
                "context.",
                "As",
                "an",
                "example",
                "shown",
                "in",
                "Figure",
                "1,",
                "the",
                "'refuge'",
                "is",
                "the",
                "word",
                "that",
                "needs",
                "to",
                "be",
                "predicted",
                "and",
                "since",
                "the",
                "meaning",
                "of",
                "it",
                "is",
                "harder",
                "to",
                "get",
                "in",
                "the",
                "first",
                "context,",
                "its",
                "complexity",
                "score",
                "in",
                "the",
                "first",
                "context",
                "is",
                "much",
                "higher.",
                "For",
                "subtask",
                "2,",
                "the",
                "goal",
                "is",
                "to",
                "predict",
                "the",
                "complexity",
                "score",
                "for",
                "a",
                "multi-word",
                "expression",
                "from",
                "the",
                "given",
                "context.",
                "An",
                "example",
                "is",
                "also",
                "shown",
                "in",
                "the",
                "right",
                "part",
                "of",
                "Figure",
                "1.",
                "a",
                "5-point",
                "Likert",
                "scale:",
                "one",
                "for",
                "very",
                "easy,",
                "two",
                "for",
                "easy,",
                "three",
                "for",
                "neutral,",
                "four",
                "for",
                "difficult,",
                "and",
                "five",
                "for",
                "very",
                "difficult.",
                "The",
                "numerical",
                "labels",
                "were",
                "transformed",
                "to",
                "a",
                "0-1",
                "range",
                "as",
                "shown",
                "in",
                "Figure",
                "1.",
                "To",
                "add",
                "further",
                "variation",
                "to",
                "the",
                "data,",
                "three",
                "corpora",
                "were",
                "selected",
                "including",
                "Bible,",
                "Europarl",
                "#REF",
                "and",
                "Biomedical",
                "#TARGET_REF",
                ".",
                "Each",
                "corpus",
                "has",
                "its",
                "own",
                "unique",
                "language",
                "features",
                "and",
                "styles.",
                "In",
                "addition",
                "to",
                "single",
                "words,",
                "multi-word",
                "expressions",
                "were",
                "also",
                "selected",
                "for",
                "annotating.",
                "In",
                "the",
                "end,",
                "there",
                "were",
                "9476",
                "annotated",
                "contexts",
                "with",
                "5166",
                "unique",
                "words."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                3,
                3,
                3,
                3,
                1,
                1,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Task Definition There are two subtasks in the LCP task. For subtask 1, the goal is to predict the complexity score for a single word from the given context. As an example shown in Figure 1, the 'refuge' is the word that needs to be predicted and since the meaning of it is harder to get in the first context, its complexity score in the first context is much higher. For subtask 2, the goal is to predict the complexity score for a multi-word expression from the given context. An example is also shown in the right part of Figure 1. a 5-point Likert scale: one for very easy, two for easy, three for neutral, four for difficult, and five for very difficult. The numerical labels were transformed to a 0-1 range as shown in Figure 1. To add further variation to the data, three corpora were selected including Bible, Europarl #REF and Biomedical #TARGET_REF . Each corpus has its own unique language features and styles. In addition to single words, multi-word expressions were also selected for annotating. In the end, there were 9476 annotated contexts with 5166 unique words.",
        "output": "Task Definition There are two subtasks in the LCP task. For subtask 1, the goal is to predict the complexity score for a single word from the given context. As an example shown in Figure 1, the 'refuge' is the word that needs to be predicted and since the meaning of it is harder to get in the first context, its complexity score in the first context is much higher. For subtask 2, the goal is to predict the complexity score for a multi-word expression from the given context. An example is also shown in the right part of Figure 1. a 5-point Likert scale: one for very easy, two for easy, three for neutral, four for difficult, and five for very difficult. The numerical labels were transformed to a 0-1 range as shown in Figure 1. <PERCEPT> To add further variation to the data, three corpora were selected including </PERCEPT> <BACK> Bible, Europarl #REF and </BACK> <INFO> Biomedical #TARGET_REF </INFO> . <BACK> Each corpus has its own unique language features and styles. </BACK> In addition to single words, multi-word expressions were also selected for annotating. In the end, there were 9476 annotated contexts with 5166 unique words."
    },
    {
        "gold": {
            "text": [
                "translations",
                "uniformly",
                "at",
                "random",
                "and",
                "form",
                "a",
                "translated",
                "batch",
                "(B",
                "p",
                ")",
                "of",
                "the",
                "same",
                "size",
                "n.",
                "It",
                "is",
                "important",
                "to",
                "note",
                "that",
                "B",
                "o",
                "itself",
                "is",
                "taken",
                "from",
                "the",
                "combined",
                "dataset",
                "of",
                "source",
                "instances",
                "and",
                "translated",
                "instances.",
                "The",
                "two",
                "batches",
                "that",
                "form",
                "a",
                "pair",
                "are",
                "denoted",
                "as",
                "original",
                "batch",
                "and",
                "pair",
                "batch,",
                "respectively,",
                "in",
                "Figure",
                "4.",
                "We",
                "use",
                "the",
                "same",
                "mBERT",
                "network",
                "up",
                "to",
                "a",
                "specific",
                "layer",
                "as",
                "our",
                "encoder",
                "(enc)",
                "to",
                "transform",
                "B",
                "o",
                "and",
                "B",
                "p",
                "to",
                "get",
                "the",
                "embeddings,",
                "E",
                "o",
                ",",
                "E",
                "p",
                "∈",
                "R",
                "n",
                "*",
                "t",
                "*",
                "d",
                ",",
                "respectively.",
                "Then,",
                "we",
                "apply",
                "a",
                "global",
                "average",
                "pooling",
                "(gap)",
                "operation",
                "to",
                "aggregate",
                "the",
                "vector",
                "representations",
                "of",
                "t",
                "tokens",
                "into",
                "a",
                "single",
                "vector",
                "representation",
                "of",
                "dimension",
                "d",
                "for",
                "each",
                "instance",
                "in",
                "each",
                "batch.",
                "This",
                "will",
                "result",
                "in",
                "the",
                "aggregated",
                "embeddings",
                "O,",
                "P",
                "∈",
                "R",
                "n",
                "*",
                "d",
                "for",
                "B",
                "o",
                "and",
                "B",
                "p",
                ",",
                "respectively.",
                "With",
                "these",
                "n",
                "feature",
                "vectors",
                "in",
                "the",
                "original",
                "and",
                "the",
                "translated",
                "batch,",
                "we",
                "follow",
                "the",
                "CLIP",
                "#TARGET_REF",
                "approach",
                "and",
                "compute",
                "the",
                "contrastive",
                "loss",
                "using",
                "the",
                "cross-entropy",
                "loss",
                "(L",
                "ce",
                ").",
                "Specifically,",
                "we",
                "multiply",
                "the",
                "matrices",
                "O",
                "and",
                "P",
                "T",
                "to",
                "get",
                "the",
                "logits",
                "matrix",
                "Q",
                "∈",
                "R",
                "n",
                "*",
                "n",
                ".",
                "Then,",
                "we",
                "apply",
                "the",
                "cross-entropy",
                "loss",
                "L",
                "ce",
                "row-wise",
                "and",
                "column-wise",
                "to",
                "the",
                "logits",
                "matrix",
                "Q,",
                "with",
                "its",
                "diagonal",
                "locations",
                "as",
                "original",
                "classes",
                "for",
                "each",
                "row",
                "and",
                "column,",
                "respectively.O",
                "=",
                "gap(enc(B",
                "o",
                ")),(2)"
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2
            ]
        },
        "input": "translations uniformly at random and form a translated batch (B p ) of the same size n. It is important to note that B o itself is taken from the combined dataset of source instances and translated instances. The two batches that form a pair are denoted as original batch and pair batch, respectively, in Figure 4. We use the same mBERT network up to a specific layer as our encoder (enc) to transform B o and B p to get the embeddings, E o , E p ∈ R n * t * d , respectively. Then, we apply a global average pooling (gap) operation to aggregate the vector representations of t tokens into a single vector representation of dimension d for each instance in each batch. This will result in the aggregated embeddings O, P ∈ R n * d for B o and B p , respectively. With these n feature vectors in the original and the translated batch, we follow the CLIP #TARGET_REF approach and compute the contrastive loss using the cross-entropy loss (L ce ). Specifically, we multiply the matrices O and P T to get the logits matrix Q ∈ R n * n . Then, we apply the cross-entropy loss L ce row-wise and column-wise to the logits matrix Q, with its diagonal locations as original classes for each row and column, respectively.O = gap(enc(B o )),(2)",
        "output": "translations uniformly at random and form a translated batch (B p ) of the same size n. It is important to note that B o itself is taken from the combined dataset of source instances and translated instances. The two batches that form a pair are denoted as original batch and pair batch, respectively, in Figure 4. <PERCEPT> We use the same mBERT network up to a specific layer as our encoder (enc) to transform B o and B p to get the embeddings, E o , E p ∈ R n * t * d , respectively. Then, we apply a global average pooling (gap) operation to aggregate the vector representations of t tokens into a single vector representation of dimension d for each instance in each batch. This will result in the aggregated embeddings O, P ∈ R n * d for B o and B p </PERCEPT> , respectively. <PERCEPT> With these n feature vectors in the original and the translated batch, we follow </PERCEPT> <INFO> the CLIP #TARGET_REF approach </INFO> <PERCEPT> and compute the contrastive loss using the cross-entropy loss (L ce ). </PERCEPT> Specifically, <PERCEPT> we multiply the matrices O and P T to get the logits matrix Q ∈ R n * n . </PERCEPT> Then, <PERCEPT> we apply the cross-entropy loss L ce row-wise and column-wise to the logits matrix Q, with its diagonal locations as original classes for each row and column, respectively.O = gap(enc(B o )),(2) </PERCEPT>"
    },
    {
        "gold": {
            "text": [
                "POS",
                "tagger",
                "from",
                "Totale",
                "#TARGET_REF",
                ")",
                "was",
                "also",
                "used",
                "as",
                "the",
                "disambiguation",
                "module",
                "instead",
                "of",
                "the",
                "original",
                "apertium",
                "tagger."
            ],
            "context": [
                1,
                1,
                1,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2
            ]
        },
        "input": "POS tagger from Totale #TARGET_REF ) was also used as the disambiguation module instead of the original apertium tagger.",
        "output": "<INFO> POS tagger from Totale #TARGET_REF ) </INFO> <PERCEPT> was also used as the disambiguation module instead of the original apertium tagger. </PERCEPT>"
    },
    {
        "gold": {
            "text": [
                "chitecture",
                "#TARGET_REF",
                ".",
                "The",
                "cross-encoder",
                "is",
                "more",
                "effective",
                "and",
                "robust,",
                "while",
                "it",
                "is",
                "inefficient",
                "over",
                "a",
                "large",
                "number",
                "of",
                "candidates",
                "in",
                "inference.",
                "Hence,",
                "we",
                "first",
                "train",
                "a",
                "cross-encoder",
                "(following",
                "the",
                "architecture",
                "shown",
                "in",
                "Figure",
                "1b).",
                "Then,",
                "when",
                "sampling",
                "hard",
                "negatives",
                "from",
                "the",
                "top-ranked",
                "passages",
                "retrieved",
                "by",
                "a",
                "dense",
                "retriever,",
                "we",
                "select",
                "only",
                "the",
                "passages",
                "that",
                "are",
                "predicted",
                "as",
                "negatives",
                "by",
                "the",
                "cross-encoder",
                "with",
                "high",
                "confidence",
                "scores.",
                "The",
                "selected",
                "top-retrieved",
                "passages",
                "can",
                "be",
                "considered",
                "as",
                "denosied",
                "samples",
                "that",
                "are",
                "more",
                "reliable",
                "to",
                "be",
                "used",
                "as",
                "hard",
                "negatives."
            ],
            "context": [
                0,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "chitecture #TARGET_REF . The cross-encoder is more effective and robust, while it is inefficient over a large number of candidates in inference. Hence, we first train a cross-encoder (following the architecture shown in Figure 1b). Then, when sampling hard negatives from the top-ranked passages retrieved by a dense retriever, we select only the passages that are predicted as negatives by the cross-encoder with high confidence scores. The selected top-retrieved passages can be considered as denosied samples that are more reliable to be used as hard negatives.",
        "output": "chitecture <INFO> #TARGET_REF . </INFO> <PERCEPT> The cross-encoder is more effective and robust, while it is inefficient over a large number of candidates in inference. </PERCEPT> Hence, we first train a cross-encoder (following the architecture shown in Figure 1b). Then, when sampling hard negatives from the top-ranked passages retrieved by a dense retriever, we select only the passages that are predicted as negatives by the cross-encoder with high confidence scores. The selected top-retrieved passages can be considered as denosied samples that are more reliable to be used as hard negatives."
    },
    {
        "gold": {
            "text": [
                "Subjective",
                "evaluation",
                "was",
                "performed",
                "after",
                "first",
                "poor",
                "BLEU",
                "results",
                "triggered",
                "some",
                "distrust.",
                "Many",
                "authors",
                "agree",
                "that",
                "BLEU",
                "metric",
                "systematically",
                "penalizes",
                "RBMT",
                "systems",
                "#REF",
                "and",
                "it",
                "is",
                "not",
                "suited",
                "for",
                "highly",
                "inflexible",
                "languages.",
                "Authors",
                "of",
                "METEOR",
                "#TARGET_REF",
                ",",
                "#REF",
                ")",
                "state",
                "that",
                "their",
                "system",
                "fixes",
                "most",
                "of",
                "the",
                "problems",
                "encountered",
                "using",
                "BLEU",
                "metric,",
                "they",
                "state",
                "that",
                "METEOR",
                "correlates",
                "highly",
                "with",
                "human",
                "judgement.",
                "Unfortunately",
                "METEOR",
                "does",
                "not",
                "support",
                "our",
                "language",
                "pair,",
                "we",
                "hope",
                "to",
                "change",
                "this",
                "in",
                "the",
                "near",
                "future,",
                "see",
                "further",
                "work",
                "Separate",
                "scales",
                "for",
                "fluency",
                "and",
                "adequacy",
                "were",
                "developed",
                "under",
                "the",
                "assumption",
                "that",
                "a",
                "translation",
                "might",
                "be",
                "disfluent",
                "but",
                "contain",
                "all",
                "the",
                "information",
                "from",
                "the",
                "source."
            ],
            "context": [
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                1,
                1,
                1,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2
            ]
        },
        "input": "Subjective evaluation was performed after first poor BLEU results triggered some distrust. Many authors agree that BLEU metric systematically penalizes RBMT systems #REF and it is not suited for highly inflexible languages. Authors of METEOR #TARGET_REF , #REF ) state that their system fixes most of the problems encountered using BLEU metric, they state that METEOR correlates highly with human judgement. Unfortunately METEOR does not support our language pair, we hope to change this in the near future, see further work Separate scales for fluency and adequacy were developed under the assumption that a translation might be disfluent but contain all the information from the source.",
        "output": "<BACK> Subjective evaluation was performed after first poor BLEU results triggered some distrust. Many authors agree that BLEU metric systematically penalizes RBMT systems #REF and it is not suited for highly inflexible languages. Authors of </BACK> <INFO> METEOR #TARGET_REF , </INFO> <BACK> #REF ) state that their system fixes most of the problems encountered using BLEU metric, they state that METEOR correlates highly with human judgement. </BACK> <PERCEPT> Unfortunately METEOR does not support our language pair, we hope to change this in the near future, see further work Separate scales for fluency and adequacy were developed under the assumption that a translation might be disfluent but contain all the information from the source. </PERCEPT>"
    },
    {
        "gold": {
            "text": [
                "Similar",
                "to",
                "other",
                "NLP",
                "applications,",
                "the",
                "most",
                "recent",
                "attention",
                "in",
                "this",
                "area",
                "has",
                "been",
                "on",
                "neural",
                "approaches.",
                "#TARGET_REF",
                "demonstrated",
                "an",
                "effective",
                "way",
                "of",
                "applying",
                "a",
                "Bi-LSTM",
                "to",
                "the",
                "POS",
                "tagging",
                "task,",
                "achieving",
                "97.4%",
                "on",
                "the",
                "English",
                "Penn",
                "Treebank.",
                "#REF",
                "used",
                "a",
                "Bi-LSTM",
                "in",
                "their",
                "work",
                "on",
                "Arabic",
                "POS",
                "tagging,",
                "achieving",
                "95.50%.",
                "#REF",
                "used",
                "the",
                "LSTM-RNN",
                "model",
                "on",
                "the",
                "Quranic",
                "Arabic",
                "Corpus",
                "(QAC).",
                "They",
                "reported",
                "accuracy",
                "of",
                "99.76%",
                "at",
                "the",
                "word",
                "level",
                "and",
                "99.18%",
                "at",
                "the",
                "morpheme",
                "level.",
                "They",
                "also",
                "compared",
                "their",
                "system",
                "against",
                "the",
                "Word2Vec",
                "POS",
                "tagger,",
                "for",
                "which",
                "they",
                "reported",
                "accuracy",
                "levels",
                "of",
                "97.33%",
                "and",
                "99.55%",
                "for",
                "words",
                "and",
                "morphemes",
                "respectively."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Similar to other NLP applications, the most recent attention in this area has been on neural approaches. #TARGET_REF demonstrated an effective way of applying a Bi-LSTM to the POS tagging task, achieving 97.4% on the English Penn Treebank. #REF used a Bi-LSTM in their work on Arabic POS tagging, achieving 95.50%. #REF used the LSTM-RNN model on the Quranic Arabic Corpus (QAC). They reported accuracy of 99.76% at the word level and 99.18% at the morpheme level. They also compared their system against the Word2Vec POS tagger, for which they reported accuracy levels of 97.33% and 99.55% for words and morphemes respectively.",
        "output": "Similar to other NLP applications, <BACK> the most recent attention in this area has been on neural approaches. </BACK> <INFO> #TARGET_REF demonstrated an effective way of applying a Bi-LSTM to the POS tagging task, achieving 97.4% on the English Penn Treebank. </INFO> #REF used a Bi-LSTM in their work on Arabic POS tagging, achieving 95.50%. #REF used the LSTM-RNN model on the Quranic Arabic Corpus (QAC). They reported accuracy of 99.76% at the word level and 99.18% at the morpheme level. They also compared their system against the Word2Vec POS tagger, for which they reported accuracy levels of 97.33% and 99.55% for words and morphemes respectively."
    },
    {
        "gold": {
            "text": [
                "Sentences",
                "are",
                "represented",
                "with",
                "their",
                "shortest",
                "dependency",
                "path",
                "between",
                "two",
                "candidates,",
                "as",
                "proposed",
                "in",
                "#TARGET_REF",
                ".",
                "The",
                "bottom",
                "of",
                "Figure",
                "1",
                "shows",
                "an",
                "example",
                "of",
                "the",
                "shortest",
                "path",
                "between",
                "an",
                "entity",
                "and",
                "attribute",
                "for",
                "one",
                "sentence.",
                "We",
                "converts",
                "each",
                "sentence",
                "from",
                "a",
                "string",
                "to",
                "a",
                "list",
                "of",
                "terms,",
                "where",
                "the",
                "first",
                "and",
                "last",
                "term",
                "is",
                "either",
                "the",
                "entity",
                "or",
                "the",
                "attribute.",
                "Each",
                "term",
                "in",
                "the",
                "dependency",
                "path",
                "is",
                "represented",
                "by",
                "the",
                "lemma",
                "of",
                "the",
                "term,",
                "the",
                "part-of-speech",
                "tag,",
                "the",
                "dependency",
                "label,",
                "the",
                "direction",
                "of",
                "the",
                "dependency",
                "path",
                "to",
                "the",
                "parent",
                "(left,",
                "right",
                "or",
                "root).",
                "Each",
                "of",
                "these",
                "features",
                "is",
                "embedded",
                "and",
                "concatenated",
                "to",
                "produce",
                "a",
                "sequence",
                "of",
                "vectors",
                "that",
                "represents",
                "the",
                "dependency",
                "path.",
                "The",
                "concatenation",
                "is",
                "the",
                "edge",
                "representation−",
                "→",
                "v",
                "edge",
                "=",
                "[",
                "−",
                "→",
                "v",
                "lemma",
                ",",
                "−",
                "→",
                "v",
                "pos",
                ",",
                "−",
                "→",
                "v",
                "dep",
                ",",
                "−",
                "→",
                "v",
                "dir",
                "]The",
                "sequence",
                "of",
                "terms",
                "in",
                "each",
                "path",
                "is",
                "input",
                "into",
                "an",
                "LSTM",
                "to",
                "produce",
                "a",
                "single",
                "vector",
                "representation",
                "for",
                "the",
                "sentence,",
                "−",
                "→",
                "v",
                "s",
                ".",
                "This",
                "is",
                "repeated",
                "for",
                "each",
                "sentence",
                "producing",
                "one",
                "vector",
                "per",
                "sentence.",
                "The",
                "sentences",
                "are",
                "aggregated",
                "with",
                "a",
                "weighted",
                "mean",
                "of",
                "the",
                "sentence",
                "representations",
                "to",
                "form",
                "a",
                "representation",
                "of",
                "the",
                "multiset",
                "of",
                "sentences,",
                "−",
                "→",
                "v",
                "sents(e,a)",
                "."
            ],
            "context": [
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Sentences are represented with their shortest dependency path between two candidates, as proposed in #TARGET_REF . The bottom of Figure 1 shows an example of the shortest path between an entity and attribute for one sentence. We converts each sentence from a string to a list of terms, where the first and last term is either the entity or the attribute. Each term in the dependency path is represented by the lemma of the term, the part-of-speech tag, the dependency label, the direction of the dependency path to the parent (left, right or root). Each of these features is embedded and concatenated to produce a sequence of vectors that represents the dependency path. The concatenation is the edge representation− → v edge = [ − → v lemma , − → v pos , − → v dep , − → v dir ]The sequence of terms in each path is input into an LSTM to produce a single vector representation for the sentence, − → v s . This is repeated for each sentence producing one vector per sentence. The sentences are aggregated with a weighted mean of the sentence representations to form a representation of the multiset of sentences, − → v sents(e,a) .",
        "output": "<INFO> Sentences are represented with their shortest dependency path between two candidates, as proposed in #TARGET_REF . </INFO> <BACK> The bottom of Figure 1 shows an example of the shortest path between an entity and attribute for one sentence. </BACK> <PERCEPT> We converts each sentence from a string to a list of terms, where the first and last term is either the entity or the attribute. </PERCEPT> Each term in the dependency path is represented by the lemma of the term, the part-of-speech tag, the dependency label, the direction of the dependency path to the parent (left, right or root). Each of these features is embedded and concatenated to produce a sequence of vectors that represents the dependency path. The concatenation is the edge representation− → v edge = [ − → v lemma , − → v pos , − → v dep , − → v dir ]The sequence of terms in each path is input into an LSTM to produce a single vector representation for the sentence, − → v s . This is repeated for each sentence producing one vector per sentence. The sentences are aggregated with a weighted mean of the sentence representations to form a representation of the multiset of sentences, − → v sents(e,a) ."
    },
    {
        "gold": {
            "text": [
                "Toxicity",
                "Datasets",
                "in",
                "Online",
                "Community",
                "An",
                "extensive",
                "body",
                "of",
                "work",
                "has",
                "focused",
                "on",
                "datasets",
                "to",
                "detect",
                "toxicity",
                "including",
                "hate",
                "speech",
                "#REF",
                "and",
                "abusive",
                "language",
                "#TARGET_REF",
                ".",
                "However,",
                "the",
                "majority",
                "of",
                "toxicity",
                "datasets",
                "do",
                "not",
                "consider",
                "the",
                "context",
                "of",
                "a",
                "conversation,",
                "instead",
                "simply",
                "analysing",
                "a",
                "single",
                "utterance.",
                "Even",
                "if",
                "a",
                "model",
                "uses",
                "contextual",
                "information",
                "#REF",
                ",",
                "it",
                "is",
                "limited",
                "to",
                "metainformation",
                "(e.g.",
                "news",
                "title",
                "or",
                "user",
                "name)",
                "which",
                "is",
                "not",
                "sufficient",
                "to",
                "understand",
                "a",
                "conversation.",
                "In",
                "our",
                "research,",
                "context",
                "is",
                "defined",
                "as",
                "linguistic",
                "contextual",
                "information,",
                "particularly",
                "previous",
                "single",
                "or",
                "multiple",
                "utterances.",
                "Along",
                "similar",
                "lines,",
                "recent",
                "studies",
                "have",
                "focused",
                "on",
                "conversation",
                "aiming",
                "to",
                "discover",
                "warning",
                "signals",
                "#REF",
                ",",
                "to",
                "generate",
                "intervention",
                "responses",
                "#REF",
                ",",
                "or",
                "to",
                "measure",
                "the",
                "importance",
                "of",
                "context",
                "#REF",
                ".",
                "Existing",
                "toxicity",
                "datasets",
                "mainly",
                "focus",
                "on",
                "annotating",
                "at",
                "utterance-level,",
                "whereas",
                "ours",
                "conducts",
                "a",
                "dual-level",
                "annotation",
                "at",
                "utterance",
                "and",
                "token-level,",
                "while",
                "also",
                "providing",
                "a",
                "conversation",
                "history",
                "(see",
                "Table",
                "1).",
                "These",
                "extra",
                "features",
                "are",
                "what",
                "distinguish",
                "CONDA."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                0,
                0,
                0,
                0,
                1,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Toxicity Datasets in Online Community An extensive body of work has focused on datasets to detect toxicity including hate speech #REF and abusive language #TARGET_REF . However, the majority of toxicity datasets do not consider the context of a conversation, instead simply analysing a single utterance. Even if a model uses contextual information #REF , it is limited to metainformation (e.g. news title or user name) which is not sufficient to understand a conversation. In our research, context is defined as linguistic contextual information, particularly previous single or multiple utterances. Along similar lines, recent studies have focused on conversation aiming to discover warning signals #REF , to generate intervention responses #REF , or to measure the importance of context #REF . Existing toxicity datasets mainly focus on annotating at utterance-level, whereas ours conducts a dual-level annotation at utterance and token-level, while also providing a conversation history (see Table 1). These extra features are what distinguish CONDA.",
        "output": "Toxicity Datasets in Online Community <PERCEPT> An extensive body of work has focused on </PERCEPT> <INFO> datasets to detect toxicity including </INFO> hate speech #REF and <INFO> abusive language #TARGET_REF . </INFO> <PERCEPT> However, the majority of toxicity datasets do not consider the context of a conversation, instead simply analysing a single utterance. </PERCEPT> Even if a model uses contextual information #REF , it is limited to metainformation (e.g. news title or user name) which is not sufficient to understand a conversation. In our research, context is defined as linguistic contextual information, particularly previous single or multiple utterances. Along similar lines, recent studies have focused on conversation aiming to discover warning signals #REF , to generate intervention responses #REF , or to measure the importance of context #REF . <PERCEPT> Existing toxicity datasets mainly focus on annotating at utterance-level, whereas ours conducts a dual-level annotation at utterance and token-level, while also providing a conversation history </PERCEPT> <BACK> (see Table 1). </BACK> These extra features are what distinguish CONDA."
    },
    {
        "gold": {
            "text": [
                "To",
                "achieve",
                "this",
                "goal,",
                "we",
                "investigate",
                "whether",
                "due",
                "to",
                "the",
                "similarity",
                "between",
                "idioms",
                "and",
                "rare-words",
                "Schick",
                "and",
                "Schütze's",
                "BERT",
                "for",
                "Attentive",
                "Mimicking",
                "#TARGET_REF",
                ")",
                "(BERTRAM)",
                "model,",
                "which",
                "was",
                "designed",
                "for",
                "use",
                "with",
                "rare-words,",
                "can",
                "be",
                "used",
                "to",
                "explicitly",
                "learn",
                "high-quality",
                "embeddings",
                "for",
                "idiomatic",
                "expressions.",
                "We",
                "also",
                "investigate",
                "how",
                "many",
                "examples",
                "of",
                "each",
                "idiom",
                "are",
                "required",
                "to",
                "create",
                "embeddings",
                "that",
                "perform",
                "well",
                "on",
                "the",
                "task,",
                "as",
                "well",
                "as",
                "how",
                "the",
                "quality",
                "of",
                "contexts",
                "fed",
                "to",
                "the",
                "BERTRAM",
                "model",
                "effects",
                "the",
                "representations",
                "and",
                "performance",
                "on",
                "the",
                "task."
            ],
            "context": [
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "To achieve this goal, we investigate whether due to the similarity between idioms and rare-words Schick and Schütze's BERT for Attentive Mimicking #TARGET_REF ) (BERTRAM) model, which was designed for use with rare-words, can be used to explicitly learn high-quality embeddings for idiomatic expressions. We also investigate how many examples of each idiom are required to create embeddings that perform well on the task, as well as how the quality of contexts fed to the BERTRAM model effects the representations and performance on the task.",
        "output": "<PERCEPT> To achieve this goal, we investigate whether due to the similarity between idioms and rare-words Schick and Schütze's BERT </PERCEPT> <INFO> for Attentive Mimicking #TARGET_REF ) (BERTRAM) model, which was designed for use with rare-words, can be used to explicitly learn high-quality embeddings for idiomatic expressions. </INFO> We also investigate how many examples of each idiom are required to create embeddings that perform well on the task, as well as how the quality of contexts fed to the BERTRAM model effects the representations and performance on the task."
    },
    {
        "gold": {
            "text": [
                "We",
                "also",
                "study",
                "mitigating",
                "shortcuts",
                "by",
                "masking",
                "out",
                "the",
                "identified",
                "shortcuts.",
                "RM),",
                "and",
                "both",
                "(Train",
                "&amp,",
                "Test",
                "RM)",
                "as",
                "described",
                "in",
                "Sec",
                "3.4.",
                "We",
                "evaluate",
                "these",
                "three",
                "approaches",
                "in",
                "multiple",
                "settings:",
                "1)",
                "domain",
                "generalization,",
                "2)",
                "challenging",
                "datasets,",
                "3)",
                "gender",
                "bias.",
                "As",
                "shown",
                "in",
                "Table",
                "5,",
                "masking",
                "out",
                "shortcuts,",
                "especially",
                "in",
                "training",
                "data,",
                "can",
                "improve",
                "model's",
                "generalization",
                "to",
                "out-of-distribution",
                "data.",
                "Note",
                "in",
                "this",
                "setting,",
                "different",
                "from",
                "existing",
                "domain",
                "transfer",
                "work",
                "(Pan",
                "and",
                "Yang,",
                "2010),",
                "we",
                "do",
                "not",
                "assume",
                "access",
                "to",
                "labeled",
                "data",
                "in",
                "the",
                "target",
                "domain",
                "during",
                "training,",
                "instead",
                "we",
                "use",
                "our",
                "proposed",
                "approach",
                "to",
                "identify",
                "potential",
                "shortcuts",
                "that",
                "can",
                "generalize",
                "to",
                "unseen",
                "target",
                "domains.",
                "As",
                "a",
                "result,",
                "we",
                "also",
                "observe",
                "model's",
                "performance",
                "improvement",
                "on",
                "challenging",
                "datasets",
                "(Table",
                "7).",
                "fairer",
                "model.",
                "Note",
                "the",
                "original",
                "performance",
                "might",
                "degrade",
                "slightly",
                "due",
                "to",
                "models",
                "learning",
                "different",
                "but",
                "more",
                "robust",
                "feature",
                "representations,",
                "consistent",
                "with",
                "findings",
                "in",
                "existing",
                "work",
                "#TARGET_REF",
                "."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1
            ]
        },
        "input": "We also study mitigating shortcuts by masking out the identified shortcuts. RM), and both (Train &amp, Test RM) as described in Sec 3.4. We evaluate these three approaches in multiple settings: 1) domain generalization, 2) challenging datasets, 3) gender bias. As shown in Table 5, masking out shortcuts, especially in training data, can improve model's generalization to out-of-distribution data. Note in this setting, different from existing domain transfer work (Pan and Yang, 2010), we do not assume access to labeled data in the target domain during training, instead we use our proposed approach to identify potential shortcuts that can generalize to unseen target domains. As a result, we also observe model's performance improvement on challenging datasets (Table 7). fairer model. Note the original performance might degrade slightly due to models learning different but more robust feature representations, consistent with findings in existing work #TARGET_REF .",
        "output": "We also study mitigating shortcuts by masking out the identified shortcuts. RM), and both (Train &amp, Test RM) as described in Sec 3.4. We evaluate these three approaches in multiple settings: 1) domain generalization, 2) challenging datasets, 3) gender bias. As shown in Table 5, masking out shortcuts, especially in training data, can improve model's generalization to out-of-distribution data. Note in this setting, different from existing domain transfer work (Pan and Yang, 2010), we do not assume access to labeled data in the target domain during training, instead we use our proposed approach to identify potential shortcuts that can generalize to unseen target domains. As a result, we also observe model's performance improvement on challenging datasets (Table 7). fairer model. Note <PERCEPT> the original performance might degrade slightly due to models learning different but more robust feature representations, consistent with findings in </PERCEPT> <INFO> existing work #TARGET_REF . </INFO>"
    },
    {
        "gold": {
            "text": [
                "The",
                "SCE",
                "loss,",
                "which",
                "is",
                "often",
                "used",
                "to",
                "train",
                "a",
                "sequence-to-sequence",
                "(Seq2Seq)",
                "model",
                "#REF",
                ",",
                "is",
                "expressed",
                "as",
                "𝐿",
                "𝑆𝐶𝐸",
                "=",
                "−𝑙𝑜𝑔{𝑠𝑜𝑓𝑡𝑚𝑎𝑥",
                "𝑐",
                "}",
                ",",
                "where𝑠𝑜𝑓𝑡𝑚𝑎𝑥",
                "𝑐",
                "=",
                "𝑒",
                "𝑑",
                "𝑐",
                "∑",
                "𝑒",
                "𝑑",
                "𝑘",
                "|𝑉|",
                "𝑘.",
                "Therein,",
                "𝑉",
                "represents",
                "the",
                "lexicon,",
                "𝑑",
                "𝑘",
                "denotes",
                "the",
                "𝑘-th",
                "element",
                "of",
                "the",
                "output",
                "𝑑",
                "∈",
                "ℝ",
                "|𝑉|",
                ".",
                "#TARGET_REF",
                "defined",
                "Inverse",
                "Token",
                "Frequency",
                "(ITF)",
                "loss",
                "as",
                "shown",
                "below."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                3,
                3,
                3
            ]
        },
        "input": "The SCE loss, which is often used to train a sequence-to-sequence (Seq2Seq) model #REF , is expressed as 𝐿 𝑆𝐶𝐸 = −𝑙𝑜𝑔{𝑠𝑜𝑓𝑡𝑚𝑎𝑥 𝑐 } , where𝑠𝑜𝑓𝑡𝑚𝑎𝑥 𝑐 = 𝑒 𝑑 𝑐 ∑ 𝑒 𝑑 𝑘 |𝑉| 𝑘. Therein, 𝑉 represents the lexicon, 𝑑 𝑘 denotes the 𝑘-th element of the output 𝑑 ∈ ℝ |𝑉| . #TARGET_REF defined Inverse Token Frequency (ITF) loss as shown below.",
        "output": "The SCE loss, which is often used to train a sequence-to-sequence (Seq2Seq) model #REF , is expressed as 𝐿 𝑆𝐶𝐸 = −𝑙𝑜𝑔{𝑠𝑜𝑓𝑡𝑚𝑎𝑥 𝑐 } , where𝑠𝑜𝑓𝑡𝑚𝑎𝑥 𝑐 = 𝑒 𝑑 𝑐 ∑ 𝑒 𝑑 𝑘 |𝑉| 𝑘. Therein, 𝑉 represents the lexicon, 𝑑 𝑘 denotes the 𝑘-th element of the output 𝑑 ∈ ℝ |𝑉| . <INFO> #TARGET_REF defined Inverse Token Frequency (ITF) loss </INFO> <BACK> as shown below. </BACK>"
    },
    {
        "gold": {
            "text": [
                "The",
                "Spanish",
                "language",
                "news",
                "articles",
                "used",
                "in",
                "this",
                "study",
                "lack",
                "corresponding",
                "reference",
                "translations.",
                "Thus,",
                "unlike",
                "the",
                "case",
                "of",
                "our",
                "Russian",
                "data,",
                "no",
                "matter",
                "how",
                "high",
                "the",
                "quality",
                "of",
                "machine",
                "translations,",
                "no",
                "Spanish-English",
                "machine",
                "translation",
                "segment",
                "could",
                "possibly",
                "receive",
                "a",
                "score",
                "of",
                "12.",
                "For",
                "Spanish-English,",
                "we",
                "therefore",
                "follow",
                "the",
                "10-point",
                "adequacy",
                "scale",
                "of",
                "#TARGET_REF",
                ".",
                "This",
                "adequacy",
                "scale",
                "is",
                "shown",
                "in",
                "Table",
                "1b",
                "on",
                "page",
                "2,",
                "this",
                "scale",
                "is",
                "very",
                "similar",
                "to",
                "the",
                "former,",
                "but",
                "has",
                "a",
                "high",
                "of",
                "10",
                "(the",
                "meaning",
                "of",
                "the",
                "source",
                "sentence",
                "is",
                "fully",
                "conveyed",
                "in",
                "the",
                "English",
                "translation)",
                "instead",
                "of",
                "12."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The Spanish language news articles used in this study lack corresponding reference translations. Thus, unlike the case of our Russian data, no matter how high the quality of machine translations, no Spanish-English machine translation segment could possibly receive a score of 12. For Spanish-English, we therefore follow the 10-point adequacy scale of #TARGET_REF . This adequacy scale is shown in Table 1b on page 2, this scale is very similar to the former, but has a high of 10 (the meaning of the source sentence is fully conveyed in the English translation) instead of 12.",
        "output": "The Spanish language news articles used in this study lack corresponding reference translations. Thus, unlike the case of our Russian data, no matter how high the quality of machine translations, <BACK> no Spanish-English machine translation segment could possibly receive a score of 12. For Spanish-English, </BACK> <PERCEPT> we therefore follow the 10-point adequacy scale of </PERCEPT> <INFO> #TARGET_REF . </INFO> This adequacy scale is shown in Table 1b on page 2, this scale is very similar to the former, but has a high of 10 (the meaning of the source sentence is fully conveyed in the English translation) instead of 12."
    },
    {
        "gold": {
            "text": [
                "The",
                "DeKo",
                "rules",
                "can",
                "only",
                "work",
                "if",
                "they",
                "can",
                "refer",
                "to",
                "detailed",
                "information",
                "on",
                "lexical",
                "items",
                "-therefore",
                "the",
                "DeKo",
                "team",
                "and",
                "other",
                "researchers",
                "at",
                "the",
                "IMS",
                "in",
                "Stuttgart",
                "developed",
                "a",
                "highly",
                "flexible",
                "lexicon",
                "concept",
                "where",
                "different",
                "kinds",
                "of",
                "information",
                "are",
                "stored",
                "together",
                "with",
                "morphological",
                "elements",
                "(see",
                "#TARGET_REF",
                "for",
                "more",
                "details).",
                "At",
                "the",
                "moment",
                "the",
                "relevant",
                "information",
                "is",
                "still",
                "being",
                "collected",
                "and",
                "encoded",
                "into",
                "the",
                "IMSLex.",
                "Therefore,",
                "the",
                "DeKo",
                "rules",
                "as",
                "they",
                "stand",
                "now",
                "are",
                "much",
                "less",
                "specific",
                "than",
                "they",
                "should",
                "be."
            ],
            "context": [
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                3,
                1,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3
            ]
        },
        "input": "The DeKo rules can only work if they can refer to detailed information on lexical items -therefore the DeKo team and other researchers at the IMS in Stuttgart developed a highly flexible lexicon concept where different kinds of information are stored together with morphological elements (see #TARGET_REF for more details). At the moment the relevant information is still being collected and encoded into the IMSLex. Therefore, the DeKo rules as they stand now are much less specific than they should be.",
        "output": "<INFO> The DeKo rules can only work if they can refer to detailed information on lexical items -therefore the DeKo team and other researchers at the IMS in Stuttgart developed a highly flexible lexicon concept where different kinds of information are stored together with morphological elements </INFO> <BACK> (see </BACK> <INFO> #TARGET_REF </INFO> <BACK> for more details). </BACK> At the moment the relevant information is still being collected and encoded into the IMSLex. Therefore, <BACK> the DeKo rules as they stand now are much less specific than they should be. </BACK>"
    },
    {
        "gold": {
            "text": [
                "Parametrizing",
                "Curriculum",
                "Difficulty.",
                "Given",
                "the",
                "relative",
                "imbalance",
                "of",
                "this",
                "multinomial",
                "distribution,",
                "as",
                "seen",
                "in",
                "Figure",
                "3,",
                "we",
                "hypothesize",
                "that",
                "a",
                "LIGHT",
                "agent",
                "only",
                "learns",
                "to",
                "do",
                "well",
                "on",
                "certain",
                "types",
                "of",
                "objectives",
                "and",
                "not",
                "others-memorizing",
                "trajectories",
                "for",
                "less",
                "seen",
                "quest",
                "types,",
                "i.e.",
                "those",
                "found",
                "in",
                "the",
                "tail",
                "of",
                "the",
                "distribution.",
                "Preliminary",
                "evidence",
                "for",
                "this",
                "hypothesis",
                "is",
                "also",
                "seen",
                "in",
                "#TARGET_REF",
                ",",
                "where",
                "they",
                "show",
                "a",
                "positive",
                "correlation",
                "between",
                "the",
                "number",
                "of",
                "instances",
                "of",
                "a",
                "particular",
                "type",
                "of",
                "quest",
                "during",
                "training",
                "and",
                "the",
                "final",
                "test",
                "goal-achievement",
                "performance.",
                "Based",
                "on",
                "these",
                "observations",
                "and",
                "our",
                "initial",
                "hypothesis,",
                "we",
                "use",
                "this",
                "particular",
                "dimension",
                "to",
                "parametrize",
                "curriculum",
                "difficulty",
                "for",
                "training",
                "LIGHT",
                "agents-quest",
                "types",
                "that",
                "are",
                "rarer",
                "in",
                "the",
                "initial",
                "training",
                "data",
                "will",
                "be",
                "harder",
                "for",
                "the",
                "agent",
                "to",
                "generalize",
                "to",
                "in",
                "a",
                "zero-shot",
                "setting."
            ],
            "context": [
                0,
                0,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                3,
                3,
                3,
                3,
                3,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Parametrizing Curriculum Difficulty. Given the relative imbalance of this multinomial distribution, as seen in Figure 3, we hypothesize that a LIGHT agent only learns to do well on certain types of objectives and not others-memorizing trajectories for less seen quest types, i.e. those found in the tail of the distribution. Preliminary evidence for this hypothesis is also seen in #TARGET_REF , where they show a positive correlation between the number of instances of a particular type of quest during training and the final test goal-achievement performance. Based on these observations and our initial hypothesis, we use this particular dimension to parametrize curriculum difficulty for training LIGHT agents-quest types that are rarer in the initial training data will be harder for the agent to generalize to in a zero-shot setting.",
        "output": "Parametrizing Curriculum Difficulty. <PERCEPT> Given the relative imbalance of this multinomial distribution, </PERCEPT> <BACK> as seen in Figure 3, </BACK> <PERCEPT> we hypothesize that a LIGHT agent only learns to do well on certain types of objectives and not others-memorizing trajectories for less seen quest types, i.e. those found in the tail of the distribution. Preliminary evidence for this hypothesis is also seen in </PERCEPT> <INFO> #TARGET_REF , where they show a positive correlation between the number of instances of a particular type of quest during training and the final test goal-achievement performance. </INFO> <PERCEPT> Based on these observations and our initial hypothesis, we use this particular dimension to parametrize curriculum difficulty for training LIGHT agents-quest types </PERCEPT> that are rarer in the initial training data will be harder for the agent to generalize to in a zero-shot setting."
    },
    {
        "gold": {
            "text": [
                "•",
                "Byte-pair-encoding",
                "(BPE)",
                "#TARGET_REF",
                "6",
                ":",
                "For",
                "both",
                "the",
                "Chinese",
                "and",
                "English",
                "sides,",
                "we",
                "use",
                "BPE",
                "with",
                "32K",
                "operations."
            ],
            "context": [
                0,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                2,
                2,
                1,
                1,
                1,
                1
            ]
        },
        "input": "• Byte-pair-encoding (BPE) #TARGET_REF 6 : For both the Chinese and English sides, we use BPE with 32K operations.",
        "output": "• <INFO> Byte-pair-encoding (BPE) #TARGET_REF 6 : For both the Chinese and English sides, </INFO> <PERCEPT> we use </PERCEPT> <INFO> BPE with 32K operations. </INFO>"
    },
    {
        "gold": {
            "text": [
                "In",
                "SemEval",
                "2020-Task",
                "11",
                "#REF",
                ",",
                "the",
                "first",
                "sub-task",
                "-Span",
                "Identification",
                "-aims",
                "at",
                "detecting",
                "the",
                "beginning",
                "and",
                "the",
                "end",
                "offset",
                "for",
                "the",
                "propaganda",
                "spans",
                "in",
                "news",
                "articles.",
                "This",
                "sub-task",
                "is",
                "similar",
                "to",
                "SemEval",
                "2021-Task",
                "5.",
                "The",
                "proposed",
                "approaches",
                "for",
                "the",
                "sub-task",
                "can",
                "be",
                "broadly",
                "classified",
                "into",
                "Span",
                "Prediction",
                "or",
                "Token",
                "Classification.",
                "Most",
                "teams",
                "use",
                "multi-granular",
                "transformer-based",
                "systems",
                "for",
                "token",
                "classification/sequence",
                "tagging",
                "#REF",
                ".",
                "Inspired",
                "by",
                "#TARGET_REF",
                ",",
                "#REF",
                "use",
                "RoBERTa-CRF",
                "based",
                "systems.",
                "#REF",
                "use",
                "a",
                "variant",
                "of",
                "SpanBERT",
                "span",
                "prediction",
                "system."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "In SemEval 2020-Task 11 #REF , the first sub-task -Span Identification -aims at detecting the beginning and the end offset for the propaganda spans in news articles. This sub-task is similar to SemEval 2021-Task 5. The proposed approaches for the sub-task can be broadly classified into Span Prediction or Token Classification. Most teams use multi-granular transformer-based systems for token classification/sequence tagging #REF . Inspired by #TARGET_REF , #REF use RoBERTa-CRF based systems. #REF use a variant of SpanBERT span prediction system.",
        "output": "In SemEval 2020-Task 11 #REF , the first sub-task -Span Identification -aims at detecting the beginning and the end offset for the propaganda spans in news articles. This sub-task is similar to SemEval 2021-Task 5. The proposed approaches for the sub-task can be broadly classified into Span Prediction or Token Classification. Most teams use multi-granular transformer-based systems for token classification/sequence tagging #REF . <PERCEPT> Inspired by #TARGET_REF , #REF use RoBERTa-CRF based systems. </PERCEPT> #REF use a variant of SpanBERT span prediction system."
    },
    {
        "gold": {
            "text": [
                "adaptation",
                "of",
                "language",
                "model",
                "leads",
                "to",
                "an",
                "increase",
                "of",
                "5.99",
                "absolute",
                "BLEU",
                "points",
                "(25.41%",
                "relative)",
                "for",
                "the",
                "best",
                "performing",
                "system",
                "(CCG",
                "contextual",
                "labels",
                "system)",
                "over",
                "the",
                "corresponding",
                "TED-trained",
                "model",
                "score",
                "in",
                "Table",
                "2.",
                "Language",
                "model",
                "adaptation",
                "also",
                "caused",
                "the",
                "PB-SMT",
                "model",
                "scores",
                "to",
                "improve",
                "by",
                "5.16",
                "absolute",
                "BLEU",
                "points",
                "(21.99%",
                "relative)",
                "over",
                "the",
                "corresponding",
                "unadapted",
                "PBSMT",
                "models.",
                "As",
                "with",
                "the",
                "unadapted",
                "systems,",
                "the",
                "HPB-CCG",
                "contextual",
                "labels",
                "system",
                "is",
                "also",
                "the",
                "best",
                "performing",
                "system",
                "within",
                "all",
                "the",
                "systems",
                "with",
                "adapted",
                "language",
                "models,",
                "across",
                "all",
                "evaluation",
                "metrics.",
                "It",
                "outperformed",
                "the",
                "mixture-model",
                "adapted",
                "HPB",
                "systems",
                "by",
                "a",
                "statistically",
                "insignificant",
                "0.1",
                "absolute",
                "BLEU",
                "points",
                "(0.34%",
                "relative).",
                "However,",
                "it",
                "improved",
                "over",
                "the",
                "UN-enhanced",
                "mixture-model",
                "adapted",
                "PB",
                "system",
                "by",
                "0.93",
                "absolute",
                "BLEU",
                "points",
                "(3.25%",
                "relative)",
                "providing",
                "a",
                "statistically",
                "significance",
                "at",
                "p-level=0.05.",
                "The",
                "results",
                "further",
                "demonstrate",
                "that",
                "dropping",
                "features",
                "from",
                "CCG",
                "labels",
                "caused",
                "the",
                "performance",
                "of",
                "the",
                "CCG-based",
                "systems",
                "to",
                "deteriorate.",
                "For",
                "the",
                "Ar-En",
                "translation",
                "task,",
                "the",
                "best",
                "performing",
                "system",
                "i.e.",
                "the",
                "HPB-CCG",
                "contextual",
                "labels",
                "system",
                "(HPB-CCG",
                "context)",
                "was",
                "submitted",
                "as",
                "the",
                "primary",
                "run",
                "in",
                "the",
                "evaluation",
                "campaign."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "adaptation of language model leads to an increase of 5.99 absolute BLEU points (25.41% relative) for the best performing system (CCG contextual labels system) over the corresponding TED-trained model score in Table 2. Language model adaptation also caused the PB-SMT model scores to improve by 5.16 absolute BLEU points (21.99% relative) over the corresponding unadapted PBSMT models. As with the unadapted systems, the HPB-CCG contextual labels system is also the best performing system within all the systems with adapted language models, across all evaluation metrics. It outperformed the mixture-model adapted HPB systems by a statistically insignificant 0.1 absolute BLEU points (0.34% relative). However, it improved over the UN-enhanced mixture-model adapted PB system by 0.93 absolute BLEU points (3.25% relative) providing a statistically significance at p-level=0.05. The results further demonstrate that dropping features from CCG labels caused the performance of the CCG-based systems to deteriorate. For the Ar-En translation task, the best performing system i.e. the HPB-CCG contextual labels system (HPB-CCG context) was submitted as the primary run in the evaluation campaign.",
        "output": "adaptation of language model leads to an increase of 5.99 absolute BLEU points (25.41% relative) for the best performing system (CCG contextual labels system) over the corresponding TED-trained model score in Table 2. Language model adaptation also caused the PB-SMT model scores to improve by 5.16 absolute BLEU points (21.99% relative) over the corresponding unadapted PBSMT models. As with the unadapted systems, the HPB-CCG contextual labels system is also the best performing system within all the systems with adapted language models, across all evaluation metrics. It outperformed the mixture-model adapted HPB systems by a statistically insignificant 0.1 absolute BLEU points (0.34% relative). However, it improved over the UN-enhanced mixture-model adapted PB system by 0.93 absolute BLEU points (3.25% relative) providing a statistically significance at p-level=0.05. The results further demonstrate that dropping features from CCG labels caused the performance of the CCG-based systems to deteriorate. For the Ar-En translation task, the best performing system i.e. the HPB-CCG contextual labels system (HPB-CCG context) was submitted as the primary run in the evaluation campaign."
    },
    {
        "gold": {
            "text": [
                "Following",
                "the",
                "SAMT",
                "approach,",
                "CCG-augmented",
                "HPB",
                "SMT",
                "#TARGET_REF",
                "uses",
                "CCG",
                "#REF",
                "to",
                "label",
                "non-terminals.",
                "CCG",
                "has",
                "distinct",
                "advantages",
                "over",
                "phrase-structure",
                "grammar",
                "in",
                "the",
                "general",
                "SMT",
                "context,",
                "particularly",
                "in",
                "extracting",
                "non-terminal",
                "labels",
                "in",
                "HPB",
                "SMT.",
                "This",
                "section",
                "gives",
                "a",
                "brief",
                "introduction",
                "to",
                "CCG",
                "followed",
                "by",
                "a",
                "description",
                "of",
                "the",
                "approach",
                "of",
                "extracting",
                "non-terminal",
                "labels",
                "using",
                "the",
                "same."
            ],
            "context": [
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Following the SAMT approach, CCG-augmented HPB SMT #TARGET_REF uses CCG #REF to label non-terminals. CCG has distinct advantages over phrase-structure grammar in the general SMT context, particularly in extracting non-terminal labels in HPB SMT. This section gives a brief introduction to CCG followed by a description of the approach of extracting non-terminal labels using the same.",
        "output": "<PERCEPT> Following the SAMT approach, </PERCEPT> <INFO> CCG-augmented HPB SMT #TARGET_REF uses CCG #REF to label non-terminals. </INFO> <PERCEPT> CCG has distinct advantages over phrase-structure grammar in the general SMT context, particularly in extracting non-terminal labels in HPB SMT. </PERCEPT> <BACK> This section gives a brief introduction to CCG </BACK> followed by a description of the approach of extracting non-terminal labels using the same."
    },
    {
        "gold": {
            "text": [
                "called",
                "DMSNAP",
                "using",
                "a",
                "parallel",
                "marker-passing",
                "scheme.",
                "DM-SNAP",
                "is",
                "a",
                "SNAP",
                "implementation",
                "of",
                "the",
                "ΦDMDIALOG",
                "speechto-speech",
                "dialogue",
                "translation",
                "system",
                "#TARGET_REF",
                ",",
                "but",
                "with",
                "some",
                "modifications",
                "to",
                "meet",
                "hardware",
                "constraints.",
                "Despite",
                "its",
                "high",
                "performance,",
                "our",
                "system",
                "carries",
                "out",
                "sound",
                "syntactic",
                "and",
                "semantic",
                "analysis",
                "including",
                "lexical",
                "ambiguity,",
                "structural",
                "ambiguity,",
                "pronoun",
                "reference,",
                "control,",
                "unbounded",
                "dependency,",
                "and",
                "others."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3
            ]
        },
        "input": "called DMSNAP using a parallel marker-passing scheme. DM-SNAP is a SNAP implementation of the ΦDMDIALOG speechto-speech dialogue translation system #TARGET_REF , but with some modifications to meet hardware constraints. Despite its high performance, our system carries out sound syntactic and semantic analysis including lexical ambiguity, structural ambiguity, pronoun reference, control, unbounded dependency, and others.",
        "output": "called DMSNAP using a parallel marker-passing scheme. <BACK> DM-SNAP is a SNAP implementation of </BACK> <INFO> the ΦDMDIALOG speechto-speech dialogue translation system #TARGET_REF </INFO> <BACK> , but with some modifications to meet hardware constraints. </BACK> <PERCEPT> Despite its high performance, our system carries out sound syntactic and semantic analysis </PERCEPT> <BACK> including lexical ambiguity, structural ambiguity, pronoun reference, control, unbounded dependency, and others. </BACK>"
    },
    {
        "gold": {
            "text": [
                "Many",
                "of",
                "the",
                "lexical",
                "items",
                "classified",
                "into",
                "Levin's",
                "verb",
                "classes",
                "are",
                "listed",
                "as",
                "members",
                "of",
                "more",
                "than",
                "one",
                "semantic",
                "class",
                "#TARGET_REF",
                ".",
                "There",
                "are",
                "in",
                "fact",
                "3104",
                "verbs,",
                "but",
                "4194",
                "verb/class",
                "pairings,",
                "or",
                "verb",
                "senses,",
                "for",
                "an",
                "average",
                "of",
                "1.35",
                "senses",
                "per",
                "verb.",
                "Levin",
                "gives",
                "only",
                "a",
                "few",
                "informal",
                "indications",
                "about",
                "how",
                "to",
                "interpret",
                "a",
                "multiple",
                "listing",
                "for",
                "a",
                "verb.",
                "Sometimes",
                "the",
                "verb",
                "is",
                "listed",
                "in",
                "several",
                "classes",
                "because",
                "there",
                "is",
                "a",
                "systematic",
                "meaning",
                "relationship",
                "among",
                "them.",
                "Other",
                "times,",
                "the",
                "multiple",
                "categorization",
                "seems",
                "to",
                "be",
                "an",
                "idiosyncrasy",
                "involving",
                "two",
                "verbs",
                "that",
                "happen",
                "to",
                "have",
                "the",
                "same",
                "spelling,",
                "i.e.,",
                "homonyms.",
                "For",
                "example,",
                "the",
                "verb",
                "draw",
                "is",
                "listed",
                "as",
                "a",
                "remove",
                "verb",
                "(class",
                "10.1),",
                "as",
                "a",
                "scribble",
                "verb",
                "(class",
                "25.2)",
                "and",
                "as",
                "a",
                "performance",
                "verb",
                "(class",
                "26.7).",
                "While",
                "the",
                "latter",
                "two",
                "senses",
                "seem",
                "systematically",
                "related",
                "(both",
                "seem",
                "to",
                "be",
                "involved,",
                "for",
                "example,",
                "in",
                "a",
                "usage",
                "like",
                "draw",
                "a",
                "portrait)",
                "the",
                "remove",
                "sense",
                "(as",
                "in",
                "draw",
                "water",
                "from",
                "the",
                "well)",
                "is",
                "clearly",
                "distinct."
            ],
            "context": [
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                0,
                0,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3
            ]
        },
        "input": "Many of the lexical items classified into Levin's verb classes are listed as members of more than one semantic class #TARGET_REF . There are in fact 3104 verbs, but 4194 verb/class pairings, or verb senses, for an average of 1.35 senses per verb. Levin gives only a few informal indications about how to interpret a multiple listing for a verb. Sometimes the verb is listed in several classes because there is a systematic meaning relationship among them. Other times, the multiple categorization seems to be an idiosyncrasy involving two verbs that happen to have the same spelling, i.e., homonyms. For example, the verb draw is listed as a remove verb (class 10.1), as a scribble verb (class 25.2) and as a performance verb (class 26.7). While the latter two senses seem systematically related (both seem to be involved, for example, in a usage like draw a portrait) the remove sense (as in draw water from the well) is clearly distinct.",
        "output": "<INFO> Many of the lexical items classified into Levin's verb classes are listed as members of more than one semantic class #TARGET_REF . There are </INFO> in fact <INFO> 3104 verbs, but 4194 verb/class pairings, or verb senses, for an average of 1.35 senses per verb. </INFO> <PERCEPT> Levin gives only a few informal indications about how to interpret a multiple listing for a verb. Sometimes the verb is listed in several classes because there is a systematic meaning relationship among them. Other times, the multiple categorization seems to be an idiosyncrasy involving two verbs that happen to have the same spelling, </PERCEPT> <BACK> i.e., homonyms. For example, the verb draw is listed as a remove verb (class 10.1), as a scribble verb (class 25.2) and as a performance verb (class 26.7). While the latter two senses seem systematically related (both seem to be involved, for example, in a usage like draw a portrait) the remove sense (as in draw water from the well) is clearly distinct. </BACK>"
    },
    {
        "gold": {
            "text": [
                "•",
                "MS",
                "COCO",
                "#TARGET_REF",
                ")",
                ":",
                "an",
                "object",
                "detection",
                "and",
                "captioning",
                "dataset",
                "with",
                "&gt,200",
                "K",
                "labeled",
                "images",
                "and",
                "5",
                "captions",
                "in",
                "a",
                "sentence",
                "form",
                "for",
                "each",
                "image",
                "•",
                "Flicker30k",
                "#REF",
                ":",
                "31",
                "K",
                "images",
                "collected",
                "from",
                "Flickr,",
                "together",
                "with",
                "5",
                "reference",
                "sentences",
                "•",
                "ImageNET",
                "#REF",
                ":",
                "14",
                "M",
                "annotated",
                "images,",
                "hierarchically",
                "organized",
                "(w.r.t.",
                "WordNet)",
                "•",
                "MVSO",
                "#REF",
                ":",
                "15",
                "K",
                "visual",
                "concepts",
                "across",
                "12",
                "languages,",
                "7.36",
                "M",
                "images",
                "Additionally,",
                "there",
                "are",
                "multimodal",
                "datasets",
                "that",
                "were",
                "created",
                "for",
                "a",
                "specific",
                "task:"
            ],
            "context": [
                0,
                1,
                1,
                1,
                1,
                0,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "• MS COCO #TARGET_REF ) : an object detection and captioning dataset with &gt,200 K labeled images and 5 captions in a sentence form for each image • Flicker30k #REF : 31 K images collected from Flickr, together with 5 reference sentences • ImageNET #REF : 14 M annotated images, hierarchically organized (w.r.t. WordNet) • MVSO #REF : 15 K visual concepts across 12 languages, 7.36 M images Additionally, there are multimodal datasets that were created for a specific task:",
        "output": "• <INFO> MS COCO #TARGET_REF ) </INFO> : <INFO> an object detection and captioning dataset with &gt,200 K labeled images and 5 captions in a sentence form for each image </INFO> • Flicker30k #REF : 31 K images collected from Flickr, together with 5 reference sentences • ImageNET #REF : 14 M annotated images, hierarchically organized (w.r.t. WordNet) • MVSO #REF : 15 K visual concepts across 12 languages, 7.36 M images Additionally, there are multimodal datasets that were created for a specific task:"
    },
    {
        "gold": {
            "text": [
                "In",
                "addition,",
                "in",
                "order",
                "to",
                "have",
                "both",
                "the",
                "high",
                "performance",
                "of",
                "post-norm",
                "and",
                "the",
                "stable",
                "training",
                "of",
                "pre-norm",
                "(Nguyen",
                "and",
                "Salazar,",
                "2019),",
                "we",
                "use",
                "the",
                "methods",
                "mentioned",
                "in",
                "DeepNet",
                "#TARGET_REF",
                "For",
                "training",
                "the",
                "full-sentence",
                "translation",
                "model,",
                "given",
                "the",
                "source",
                "sentence",
                "x,",
                "the",
                "probability",
                "of",
                "predicting",
                "the",
                "target",
                "sentence",
                "y",
                "is",
                "as",
                "shown",
                "in",
                "Eq.",
                "1,",
                "and",
                "the",
                "training",
                "objective",
                "is",
                "to",
                "minimize",
                "the",
                "negative",
                "log-likelihood",
                "as",
                "shown",
                "in",
                "Eq.",
                "2.p(y|x)",
                "=",
                "|y|",
                "t=1",
                "p(y",
                "t",
                "|x,",
                "y",
                "&lt,t",
                ",",
                "θ)(1)loss",
                "f",
                "ull",
                "(θ)",
                "=",
                "−",
                "(x,y)∈D",
                "logp",
                "g",
                "(y|x,",
                "θ)",
                "(2)The",
                "batch",
                "size",
                "for",
                "training",
                "is",
                "4,096",
                "tokens",
                "per",
                "GPU,",
                "and",
                "we",
                "trained",
                "our",
                "model",
                "for",
                "7",
                "epochs",
                "on",
                "4",
                "NVIDIA",
                "V100",
                "GPUs",
                "for",
                "about",
                "10",
                "hours."
            ],
            "context": [
                0,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "In addition, in order to have both the high performance of post-norm and the stable training of pre-norm (Nguyen and Salazar, 2019), we use the methods mentioned in DeepNet #TARGET_REF For training the full-sentence translation model, given the source sentence x, the probability of predicting the target sentence y is as shown in Eq. 1, and the training objective is to minimize the negative log-likelihood as shown in Eq. 2.p(y|x) = |y| t=1 p(y t |x, y &lt,t , θ)(1)loss f ull (θ) = − (x,y)∈D logp g (y|x, θ) (2)The batch size for training is 4,096 tokens per GPU, and we trained our model for 7 epochs on 4 NVIDIA V100 GPUs for about 10 hours.",
        "output": "In addition, <BACK> in order to have both the high performance of post-norm and the stable training of pre-norm (Nguyen and Salazar, 2019), </BACK> <PERCEPT> we use the methods mentioned in </PERCEPT> <INFO> DeepNet #TARGET_REF </INFO> For training the full-sentence translation model, given the source sentence x, the probability of predicting the target sentence y is as shown in Eq. 1, and the training objective is to minimize the negative log-likelihood as shown in Eq. 2.p(y|x) = |y| t=1 p(y t |x, y &lt,t , θ)(1)loss f ull (θ) = − (x,y)∈D logp g (y|x, θ) (2)The batch size for training is 4,096 tokens per GPU, and we trained our model for 7 epochs on 4 NVIDIA V100 GPUs for about 10 hours."
    },
    {
        "gold": {
            "text": [
                "Though",
                "the",
                "importance",
                "of",
                "categorizing",
                "scientific",
                "literature",
                "according",
                "to",
                "context",
                "is",
                "apparent,",
                "the",
                "reported",
                "amount",
                "of",
                "research",
                "that",
                "has",
                "been",
                "carried",
                "out",
                "is",
                "insufficient.",
                "Along",
                "with",
                "a",
                "classification",
                "model,",
                "#REF",
                "also",
                "proposed",
                "an",
                "annotation",
                "scheme",
                "for",
                "the",
                "categorization",
                "of",
                "the",
                "citations.",
                "12",
                "classes",
                "were",
                "considered",
                "for",
                "annotation.",
                "From",
                "116",
                "articles,",
                "2829",
                "citation",
                "samples",
                "were",
                "gathered.",
                "These",
                "were",
                "used",
                "to",
                "train",
                "the",
                "machine",
                "learning",
                "model.",
                "113K",
                "algorithms",
                "were",
                "used",
                "for",
                "classification",
                "with",
                "hand-engineered",
                "features.",
                "One",
                "of",
                "such",
                "features",
                "was",
                "cue",
                "phrases.",
                "Features",
                "such",
                "as",
                "patternbased",
                "features,",
                "topic-based",
                "features,",
                "and",
                "prototypical",
                "argument",
                "features",
                "were",
                "used",
                "by",
                "D.",
                "#REF",
                "to",
                "separate",
                "the",
                "documents",
                "into",
                "its",
                "6",
                "corresponding",
                "classes.",
                "The",
                "RandomForest",
                "algorithm",
                "was",
                "used",
                "for",
                "classification.",
                "#REF",
                "also",
                "utilised",
                "Glove,",
                "ELMO",
                "word",
                "embedding",
                "features,",
                "and",
                "Bi-LSTM",
                "with",
                "attention",
                "models",
                "to",
                "aid",
                "in",
                "the",
                "classification",
                "of",
                "the",
                "citations.",
                "#TARGET_REF",
                "organized",
                "the",
                "first",
                "shared",
                "task",
                "on",
                "citation",
                "classification",
                "in",
                "2020,",
                "where",
                "different",
                "teams",
                "came",
                "up",
                "with",
                "different",
                "approaches",
                "to",
                "solve",
                "3c",
                "classification",
                "problem."
            ],
            "context": [
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1
            ]
        },
        "input": "Though the importance of categorizing scientific literature according to context is apparent, the reported amount of research that has been carried out is insufficient. Along with a classification model, #REF also proposed an annotation scheme for the categorization of the citations. 12 classes were considered for annotation. From 116 articles, 2829 citation samples were gathered. These were used to train the machine learning model. 113K algorithms were used for classification with hand-engineered features. One of such features was cue phrases. Features such as patternbased features, topic-based features, and prototypical argument features were used by D. #REF to separate the documents into its 6 corresponding classes. The RandomForest algorithm was used for classification. #REF also utilised Glove, ELMO word embedding features, and Bi-LSTM with attention models to aid in the classification of the citations. #TARGET_REF organized the first shared task on citation classification in 2020, where different teams came up with different approaches to solve 3c classification problem.",
        "output": "<BACK> Though the importance of categorizing scientific literature according to context is apparent, the reported amount of research that has been carried out is insufficient. Along with a classification model, #REF also proposed an annotation scheme for the categorization of the citations. </BACK> 12 classes were considered for annotation. From 116 articles, 2829 citation samples were gathered. These were used to train the machine learning model. 113K algorithms were used for classification with hand-engineered features. One of such features was cue phrases. Features such as patternbased features, topic-based features, and prototypical argument features were used by D. #REF to separate the documents into its 6 corresponding classes. The RandomForest algorithm was used for classification. #REF also utilised Glove, ELMO word embedding features, and Bi-LSTM with attention models to aid in the classification of the citations. <INFO> #TARGET_REF organized the first shared task on citation classification in 2020, where different teams came up with different approaches to solve 3c classification problem. </INFO>"
    },
    {
        "gold": {
            "text": [
                "i",
                "[CLS]",
                "Strategy",
                "-This",
                "is",
                "the",
                "default",
                "sentence",
                "pair",
                "classification",
                "architecture",
                "with",
                "transformers",
                "#TARGET_REF",
                "where",
                "the",
                "two",
                "sentences",
                "are",
                "concatenated",
                "with",
                "a",
                "[SEP]",
                "token",
                "and",
                "passed",
                "through",
                "a",
                "transformer",
                "model.",
                "Then",
                "the",
                "output",
                "of",
                "the",
                "#REF",
                "token",
                "is",
                "fed",
                "into",
                "a",
                "softmax",
                "layer",
                "to",
                "predict",
                "the",
                "labels",
                "(Figure",
                "1)."
            ],
            "context": [
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3
            ]
        },
        "input": "i [CLS] Strategy -This is the default sentence pair classification architecture with transformers #TARGET_REF where the two sentences are concatenated with a [SEP] token and passed through a transformer model. Then the output of the #REF token is fed into a softmax layer to predict the labels (Figure 1).",
        "output": "i <BACK> [CLS] Strategy -This is the default sentence pair classification architecture with </BACK> <INFO> transformers #TARGET_REF where the two sentences are concatenated with a [SEP] token and passed through a transformer model. </INFO> <BACK> Then the output of the #REF token is fed into a softmax layer to predict the labels (Figure 1). </BACK>"
    },
    {
        "gold": {
            "text": [
                "The",
                "correct",
                "answer",
                "and",
                "at",
                "least",
                "50%",
                "were",
                "inspired",
                "by",
                "the",
                "accuracy",
                "@",
                "x%",
                "approach",
                "used",
                "by",
                "different",
                "authors",
                "working",
                "with",
                "the",
                "Amazon",
                "dataset",
                "and",
                "performing",
                "similar",
                "tasks",
                "#TARGET_REF",
                ".",
                "In",
                "accuracy",
                "@",
                "x%",
                "the",
                "commonly",
                "used",
                "measure",
                "is",
                "accuracy",
                "@",
                "50%.",
                "This",
                "approach",
                "helps",
                "in",
                "identifying",
                "the",
                "top",
                "answers",
                "crossing",
                "a",
                "threshold",
                "and",
                "has",
                "better",
                "relationship",
                "in",
                "real",
                "world",
                "applications",
                "#REF",
                "."
            ],
            "context": [
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2
            ]
        },
        "input": "The correct answer and at least 50% were inspired by the accuracy @ x% approach used by different authors working with the Amazon dataset and performing similar tasks #TARGET_REF . In accuracy @ x% the commonly used measure is accuracy @ 50%. This approach helps in identifying the top answers crossing a threshold and has better relationship in real world applications #REF .",
        "output": "<INFO> The correct answer and at least 50% were inspired by the accuracy @ x% approach used by different authors working with the Amazon dataset and performing similar tasks #TARGET_REF . </INFO> <BACK> In accuracy @ x% the commonly used measure is accuracy @ 50%. </BACK> <PERCEPT> This approach helps in identifying the top answers crossing a threshold and has better relationship in real world applications #REF . </PERCEPT>"
    },
    {
        "gold": {
            "text": [
                "GLOVE",
                "#REF",
                "71.9",
                "ELMO",
                "#TARGET_REF",
                "80.2",
                "BERT",
                "BASE",
                "→",
                "LR",
                "#REF",
                "80.6",
                "BERT",
                "LARGE",
                "→",
                "LR",
                "#REF",
                "79",
                "BERT-base",
                "by",
                "adding",
                "an",
                "internal",
                "entity",
                "linker."
            ],
            "context": [
                0,
                0,
                0,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "GLOVE #REF 71.9 ELMO #TARGET_REF 80.2 BERT BASE → LR #REF 80.6 BERT LARGE → LR #REF 79 BERT-base by adding an internal entity linker.",
        "output": "GLOVE #REF 71.9 <INFO> ELMO #TARGET_REF </INFO> 80.2 BERT BASE → LR #REF 80.6 BERT LARGE → LR #REF 79 BERT-base by adding an internal entity linker."
    },
    {
        "gold": {
            "text": [
                "One",
                "frequently",
                "used",
                "NN",
                "architecture",
                "#TARGET_REF",
                ")",
                "that",
                "maximizes",
                "Eq.",
                "(",
                "3)",
                "is",
                "the",
                "O2M",
                "model",
                "trained",
                "with",
                "multi-task",
                "learning.",
                "Fig.",
                "1(a)",
                "shows",
                "the",
                "architecture",
                "of",
                "the",
                "model.",
                "The",
                "O2M",
                "model",
                "outputs",
                "several",
                "types",
                "of",
                "sequences",
                "independently.",
                "In",
                "other",
                "words,",
                "multi-task",
                "learning",
                "is",
                "derived",
                "by",
                "assuming",
                "conditional",
                "independence",
                "of",
                "output",
                "token",
                "types",
                "for",
                "Eq.",
                "(",
                "3),",
                "as",
                "follows:"
            ],
            "context": [
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "One frequently used NN architecture #TARGET_REF ) that maximizes Eq. ( 3) is the O2M model trained with multi-task learning. Fig. 1(a) shows the architecture of the model. The O2M model outputs several types of sequences independently. In other words, multi-task learning is derived by assuming conditional independence of output token types for Eq. ( 3), as follows:",
        "output": "<PERCEPT> One frequently used </PERCEPT> <INFO> NN architecture #TARGET_REF ) </INFO> <PERCEPT> that maximizes Eq. ( 3) is the O2M model trained with multi-task learning. </PERCEPT> <BACK> Fig. 1(a) shows the architecture of the model. </BACK> The O2M model outputs several types of sequences independently. In other words, multi-task learning is derived by assuming conditional independence of output token types for Eq. ( 3), as follows:"
    },
    {
        "gold": {
            "text": [
                "ChrF",
                "#TARGET_REF",
                "a",
                "character",
                "n-gram",
                "precision",
                "and",
                "recall",
                "enhanced",
                "with",
                "word",
                "n-grams.",
                "Since",
                "answers",
                "are",
                "largely",
                "made",
                "up",
                "of",
                "entities,",
                "ChrF",
                "score",
                "integration",
                "is",
                "only",
                "performed",
                "when",
                "the",
                "answer",
                "span",
                "is",
                "not",
                "present",
                "in",
                "the",
                "related",
                "context.",
                "In",
                "order",
                "to",
                "evaluate",
                "the",
                "quality",
                "of",
                "the",
                "translation,",
                "we",
                "manually",
                "corrected",
                "the",
                "translation",
                "errors",
                "in",
                "the",
                "output",
                "of",
                "a",
                "subset",
                "of",
                "the",
                "corpus",
                "composed",
                "of",
                "890",
                "QA",
                "pairs",
                "and",
                "107",
                "contexts.",
                "We",
                "obtain",
                "a",
                "BLEU",
                "score",
                "#REF",
                "We",
                "also",
                "explore",
                "mixed",
                "datasets",
                "training",
                "strategy",
                "with",
                "SQuAD-en",
                "train",
                "+",
                "FQuAD",
                "train",
                "for",
                "training",
                "models",
                "on",
                "a",
                "concatenation",
                "of",
                "the",
                "training",
                "data",
                "covering",
                "French-English",
                "language",
                "pairs",
                "to",
                "test",
                "the",
                "crosslingual",
                "transfer",
                "ability",
                "of",
                "multilingual",
                "models."
            ],
            "context": [
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "ChrF #TARGET_REF a character n-gram precision and recall enhanced with word n-grams. Since answers are largely made up of entities, ChrF score integration is only performed when the answer span is not present in the related context. In order to evaluate the quality of the translation, we manually corrected the translation errors in the output of a subset of the corpus composed of 890 QA pairs and 107 contexts. We obtain a BLEU score #REF We also explore mixed datasets training strategy with SQuAD-en train + FQuAD train for training models on a concatenation of the training data covering French-English language pairs to test the crosslingual transfer ability of multilingual models.",
        "output": "<INFO> ChrF #TARGET_REF a character n-gram precision and recall enhanced with word n-grams. </INFO> Since answers are largely made up of entities, <BACK> ChrF score integration is only performed when the answer span is not present in the related context. </BACK> In order to evaluate the quality of the translation, we manually corrected the translation errors in the output of a subset of the corpus composed of 890 QA pairs and 107 contexts. We obtain a BLEU score #REF We also explore mixed datasets training strategy with SQuAD-en train + FQuAD train for training models on a concatenation of the training data covering French-English language pairs to test the crosslingual transfer ability of multilingual models."
    },
    {
        "gold": {
            "text": [
                "Trained",
                "on",
                "20GB",
                "texts",
                "of",
                "both",
                "Vietnamese",
                "news",
                "and",
                "Vietnamese",
                "Wikipedia",
                "Bert4News",
                "#REF",
                "x",
                "Trained",
                "on",
                "more",
                "than",
                "20GB",
                "texts",
                "of",
                "Vietnamese",
                "news",
                "vElectra",
                "and",
                "ViBERT",
                "#REF",
                "x",
                "vElectra",
                "was",
                "trained",
                "on",
                "10GB",
                "texts,",
                "whereas",
                "ViBERT",
                "was",
                "trained",
                "on",
                "60GB",
                "texts",
                "of",
                "Vietnamese",
                "news",
                "VGG16",
                "#TARGET_REF",
                "x"
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                1,
                1,
                1
            ]
        },
        "input": "Trained on 20GB texts of both Vietnamese news and Vietnamese Wikipedia Bert4News #REF x Trained on more than 20GB texts of Vietnamese news vElectra and ViBERT #REF x vElectra was trained on 10GB texts, whereas ViBERT was trained on 60GB texts of Vietnamese news VGG16 #TARGET_REF x",
        "output": "Trained on 20GB texts of both Vietnamese news and Vietnamese Wikipedia Bert4News #REF x Trained on more than 20GB texts of Vietnamese news vElectra and ViBERT #REF x vElectra was trained on 10GB texts, whereas <BACK> ViBERT was trained on 60GB texts of Vietnamese news </BACK> <INFO> VGG16 #TARGET_REF x </INFO>"
    },
    {
        "gold": {
            "text": [
                "It",
                "is",
                "not",
                "clear",
                "how",
                "much",
                "WordNet",
                "synsets",
                "should",
                "be",
                "expected",
                "to",
                "overlap",
                "with",
                "Levin",
                "classes,",
                "and",
                "preliminary",
                "indications",
                "are",
                "that",
                "there",
                "is",
                "a",
                "wide",
                "discrepancy",
                "#REF",
                ",",
                "#TARGET_REF",
                ",",
                "#REF",
                ".",
                "However,",
                "it",
                "would",
                "be",
                "useful",
                "for",
                "the",
                "WordNet",
                "synsets",
                "to",
                "have",
                "access",
                "to",
                "the",
                "detailed",
                "syntactic",
                "information",
                "that",
                "the",
                "Levin",
                "classes",
                "contain,",
                "and",
                "it",
                "would",
                "be",
                "equally",
                "useful",
                "to",
                "have",
                "more",
                "guidance",
                "as",
                "to",
                "when",
                "membership",
                "in",
                "a",
                "Levin",
                "class",
                "does",
                "in",
                "fact",
                "indicate",
                "shared",
                "semantic",
                "components.",
                "Identification",
                "of",
                "these",
                "components",
                "is",
                "critical",
                "to",
                "the",
                "use",
                "of",
                "classes",
                "and",
                "their",
                "semantic",
                "features",
                "for",
                "translation",
                "purposes,",
                "whether",
                "transfer-based",
                "or",
                "interlingua",
                "based.",
                "Although",
                "Levin",
                "classes",
                "group",
                "together",
                "verbs",
                "with",
                "similar",
                "argument",
                "structures,",
                "the",
                "meanings",
                "of",
                "the",
                "verbs",
                "are",
                "not",
                "necessarily",
                "synonymous.",
                "Some",
                "classes",
                "such",
                "as",
                "break",
                "(break,",
                "chip,",
                "crack,",
                "crash,",
                "crush,",
                "fracture,",
                "rip,",
                "shatter,",
                "smash,",
                "snap,",
                "splinter,",
                "tear)",
                "and",
                "cut",
                "(chip,",
                "clip,",
                "cut,",
                "hack,",
                "hew,",
                "saw,",
                "scrape,",
                "scratch,",
                "slash,",
                "snip)",
                "contain",
                "verbs",
                "that",
                "are",
                "quite",
                "synonymous,",
                "but",
                "others,",
                "such",
                "as",
                "braid",
                "(bob,",
                "braid,",
                "brush,",
                "clip,",
                "coldcream,",
                "comb,",
                "condition,",
                "crimp,",
                "crop,",
                "curl,",
                "etc.)",
                "do",
                "not,",
                "which",
                "at",
                "least",
                "partly",
                "explains",
                "the",
                "lack",
                "of",
                "overlap",
                "between",
                "Levin",
                "and",
                "WordNet."
            ],
            "context": [
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "It is not clear how much WordNet synsets should be expected to overlap with Levin classes, and preliminary indications are that there is a wide discrepancy #REF , #TARGET_REF , #REF . However, it would be useful for the WordNet synsets to have access to the detailed syntactic information that the Levin classes contain, and it would be equally useful to have more guidance as to when membership in a Levin class does in fact indicate shared semantic components. Identification of these components is critical to the use of classes and their semantic features for translation purposes, whether transfer-based or interlingua based. Although Levin classes group together verbs with similar argument structures, the meanings of the verbs are not necessarily synonymous. Some classes such as break (break, chip, crack, crash, crush, fracture, rip, shatter, smash, snap, splinter, tear) and cut (chip, clip, cut, hack, hew, saw, scrape, scratch, slash, snip) contain verbs that are quite synonymous, but others, such as braid (bob, braid, brush, clip, coldcream, comb, condition, crimp, crop, curl, etc.) do not, which at least partly explains the lack of overlap between Levin and WordNet.",
        "output": "<INFO> It is not clear how much WordNet synsets should be expected to overlap with Levin classes, and preliminary indications are that there is a wide discrepancy #REF , #TARGET_REF , #REF . </INFO> However, <BACK> it would be useful for the WordNet synsets to have access to the detailed syntactic information that the Levin classes contain, and it would be equally useful to have more guidance as to when membership in a Levin class does in fact indicate shared semantic components. </BACK> <PERCEPT> Identification of these components is critical to the use of classes and their semantic features for translation purposes, whether transfer-based or interlingua based. Although Levin classes group together verbs with similar argument structures, the meanings of the verbs are not necessarily synonymous. </PERCEPT> Some classes such as break (break, chip, crack, crash, crush, fracture, rip, shatter, smash, snap, splinter, tear) and cut (chip, clip, cut, hack, hew, saw, scrape, scratch, slash, snip) contain verbs that are quite synonymous, but others, such as braid (bob, braid, brush, clip, coldcream, comb, condition, crimp, crop, curl, etc.) do not, which at least partly explains the lack of overlap between Levin and WordNet."
    },
    {
        "gold": {
            "text": [
                "sufficient",
                "to",
                "solve",
                "the",
                "task,",
                "as",
                "was",
                "also",
                "argued",
                "in",
                "#TARGET_REF",
                "and",
                "#REF",
                ".",
                "Finally,",
                "we",
                "note",
                "an",
                "interesting",
                "trend",
                "that",
                "the",
                "decoupled",
                "variant",
                "favors",
                "recall",
                "whereas",
                "the",
                "coupled",
                "variant",
                "favors",
                "precision,",
                "across",
                "all",
                "models.",
                "In",
                "summary,",
                "all",
                "models",
                "perform",
                "substantially",
                "below",
                "human",
                "agreement,",
                "leaving",
                "a",
                "large",
                "room",
                "for",
                "improvement."
            ],
            "context": [
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                1,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "sufficient to solve the task, as was also argued in #TARGET_REF and #REF . Finally, we note an interesting trend that the decoupled variant favors recall whereas the coupled variant favors precision, across all models. In summary, all models perform substantially below human agreement, leaving a large room for improvement.",
        "output": "<BACK> sufficient to solve the task, as was also argued in </BACK> <INFO> #TARGET_REF </INFO> <BACK> and #REF . </BACK> Finally, we note an interesting trend that the decoupled variant favors recall whereas the coupled variant favors precision, across all models. In summary, all models perform substantially below human agreement, leaving a large room for improvement."
    },
    {
        "gold": {
            "text": [
                "We",
                "use",
                "the",
                "Chinese",
                "sentences",
                "as",
                "system",
                "input",
                "and",
                "their",
                "corresponding",
                "English",
                "translations",
                "as",
                "the",
                "reference",
                "translations.",
                "We",
                "use",
                "the",
                "open",
                "source",
                "statistical",
                "machine",
                "translation",
                "decoder",
                "Moses",
                "(?)",
                "for",
                "the",
                "experiments,",
                "translating",
                "the",
                "PropBank",
                "Chinese",
                "sentences",
                "into",
                "English",
                "with",
                "the",
                "same",
                "model",
                "trained",
                "for",
                "our",
                "participation",
                "in",
                "the",
                "IWSLT",
                "2007",
                "evaluation",
                "campaign",
                "#TARGET_REF",
                ".The",
                "English",
                "translations",
                "generated",
                "by",
                "the",
                "decoder",
                "are",
                "the",
                "system",
                "output.",
                "Based",
                "on",
                "the",
                "system",
                "input",
                "and",
                "the",
                "reference",
                "We",
                "first",
                "randomly",
                "select",
                "50",
                "bi-sentences,",
                "without",
                "any",
                "constraint",
                "on",
                "the",
                "translation",
                "accuracy",
                "of",
                "the",
                "predicate",
                "verbs,",
                "to",
                "form",
                "the",
                "first",
                "observation",
                "data",
                "set",
                "(data",
                "set",
                "A)."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "We use the Chinese sentences as system input and their corresponding English translations as the reference translations. We use the open source statistical machine translation decoder Moses (?) for the experiments, translating the PropBank Chinese sentences into English with the same model trained for our participation in the IWSLT 2007 evaluation campaign #TARGET_REF .The English translations generated by the decoder are the system output. Based on the system input and the reference We first randomly select 50 bi-sentences, without any constraint on the translation accuracy of the predicate verbs, to form the first observation data set (data set A).",
        "output": "We use the Chinese sentences as system input and their corresponding English translations as the reference translations. <PERCEPT> We use the open source statistical machine translation decoder Moses (?) for the experiments, translating the PropBank Chinese sentences into English with the same model trained for our participation in </PERCEPT> <INFO> the IWSLT 2007 evaluation campaign #TARGET_REF </INFO> .The English translations generated by the decoder are the system output. Based on the system input and the reference We first randomly select 50 bi-sentences, without any constraint on the translation accuracy of the predicate verbs, to form the first observation data set (data set A)."
    },
    {
        "gold": {
            "text": [
                "Our",
                "GB",
                "parser",
                "is",
                "an",
                "extension",
                "of",
                "the",
                "message-passing",
                "approach",
                "proposed",
                "by",
                "#REF",
                "and",
                "#TARGET_REF",
                ",",
                "which",
                "uses",
                "a",
                "network",
                "to",
                "encode",
                "the",
                "grammar.",
                "The",
                "nodes",
                "in",
                "the",
                "grammar",
                "network",
                "represent",
                "grammatical",
                "categories",
                "(e.g.,",
                "NP,",
                "Nbar,",
                "N)",
                "or",
                "subcategories,",
                "such",
                "as",
                "V:NP",
                "(i.e.,",
                "a",
                "transitive",
                "verb",
                "that",
                "takes",
                "an",
                "NP",
                "as",
                "complement).",
                "Figure",
                "l.a",
                "depicts",
                "a",
                "portion",
                "of",
                "the",
                "grammar",
                "network",
                "for",
                "English."
            ],
            "context": [
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                3,
                3,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Our GB parser is an extension of the message-passing approach proposed by #REF and #TARGET_REF , which uses a network to encode the grammar. The nodes in the grammar network represent grammatical categories (e.g., NP, Nbar, N) or subcategories, such as V:NP (i.e., a transitive verb that takes an NP as complement). Figure l.a depicts a portion of the grammar network for English.",
        "output": "<PERCEPT> Our GB parser is an extension of </PERCEPT> <INFO> the message-passing approach proposed by </INFO> <BACK> #REF and </BACK> <INFO> #TARGET_REF , which uses a network to encode the grammar. </INFO> The nodes in the grammar network represent grammatical categories (e.g., NP, Nbar, N) or subcategories, such as V:NP (i.e., a transitive verb that takes an NP as complement). Figure l.a depicts a portion of the grammar network for English."
    },
    {
        "gold": {
            "text": [
                "In",
                "all",
                "configurations,",
                "the",
                "performance",
                "in",
                "terms",
                "of",
                "EM",
                "and",
                "F1",
                "on",
                "PIAF",
                "remains",
                "significantly",
                "lower",
                "than",
                "that",
                "obtained",
                "on",
                "FQuAD",
                "since",
                "the",
                "PIAF",
                "corpus",
                "does",
                "not",
                "include",
                "multiple",
                "responses",
                "as",
                "pointed",
                "out",
                "by",
                "d",
                "#TARGET_REF",
                ".",
                "Unsurprisingly,",
                "PIAF",
                "dev",
                "offer",
                "a",
                "more",
                "challenging",
                "evaluation",
                "set,",
                "where",
                "the",
                "answer",
                "extraction",
                "performance",
                "are",
                "lower.",
                "Indeed,",
                "the",
                "corpus",
                "is",
                "more",
                "diversified",
                "with",
                "questions",
                "on",
                "191",
                "different",
                "Wikipedia",
                "articles,",
                "whereas",
                "on",
                "FQuAD",
                "dev",
                "it",
                "only",
                "covers",
                "18."
            ],
            "context": [
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "In all configurations, the performance in terms of EM and F1 on PIAF remains significantly lower than that obtained on FQuAD since the PIAF corpus does not include multiple responses as pointed out by d #TARGET_REF . Unsurprisingly, PIAF dev offer a more challenging evaluation set, where the answer extraction performance are lower. Indeed, the corpus is more diversified with questions on 191 different Wikipedia articles, whereas on FQuAD dev it only covers 18.",
        "output": "<BACK> In all configurations, the performance in terms of EM and F1 on PIAF remains significantly lower than that obtained on FQuAD since the PIAF corpus does not include multiple responses as pointed out by d </BACK> <INFO> #TARGET_REF . </INFO> Unsurprisingly, PIAF dev offer a more challenging evaluation set, where the answer extraction performance are lower. Indeed, the corpus is more diversified with questions on 191 different Wikipedia articles, whereas on FQuAD dev it only covers 18."
    },
    {
        "gold": {
            "text": [
                "Explainability",
                "is",
                "an",
                "important",
                "concept",
                "within",
                "abusive",
                "language",
                "detection.",
                "#TARGET_REF",
                "noted",
                "in",
                "their",
                "work",
                "that",
                "explainable",
                "ML",
                "techniques",
                "can",
                "promote",
                "restorative",
                "and",
                "procedural",
                "justice",
                "by",
                "surfacing",
                "the",
                "norms",
                "that",
                "have",
                "been",
                "violated",
                "and",
                "clarifying",
                "how",
                "they",
                "have",
                "been",
                "violated.",
                "That",
                "said,",
                "there",
                "has",
                "been",
                "limited",
                "discussion",
                "of",
                "the",
                "issue",
                "within",
                "the",
                "domain",
                "of",
                "abusive",
                "language",
                "detection.",
                "In",
                "this",
                "section,",
                "we",
                "first",
                "formalize",
                "the",
                "properties",
                "that",
                "an",
                "explainable",
                "detection",
                "method",
                "should",
                "aim",
                "to",
                "exhibit",
                "in",
                "order",
                "to",
                "thoroughly",
                "substantiate",
                "its",
                "decisions.",
                "We",
                "then",
                "describe",
                "how",
                "user",
                "and",
                "community",
                "information",
                "play",
                "an",
                "important",
                "role",
                "in",
                "the",
                "realization",
                "of",
                "each",
                "of",
                "the",
                "properties.",
                "Finally,",
                "we",
                "discuss",
                "what",
                "it",
                "means",
                "to",
                "operationalize",
                "explainability",
                "within",
                "abusive",
                "language",
                "detection",
                "in",
                "an",
                "effective",
                "manner."
            ],
            "context": [
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Explainability is an important concept within abusive language detection. #TARGET_REF noted in their work that explainable ML techniques can promote restorative and procedural justice by surfacing the norms that have been violated and clarifying how they have been violated. That said, there has been limited discussion of the issue within the domain of abusive language detection. In this section, we first formalize the properties that an explainable detection method should aim to exhibit in order to thoroughly substantiate its decisions. We then describe how user and community information play an important role in the realization of each of the properties. Finally, we discuss what it means to operationalize explainability within abusive language detection in an effective manner.",
        "output": "<BACK> Explainability is an important concept within abusive language detection. </BACK> <INFO> #TARGET_REF noted in their work that explainable ML techniques can promote restorative and procedural justice by surfacing the norms that have been violated and clarifying how they have been violated. </INFO> That said, there has been limited discussion of the issue within the domain of abusive language detection. In this section, we first formalize the properties that an explainable detection method should aim to exhibit in order to thoroughly substantiate its decisions. We then describe how user and community information play an important role in the realization of each of the properties. Finally, we discuss what it means to operationalize explainability within abusive language detection in an effective manner."
    },
    {
        "gold": {
            "text": [
                "For",
                "the",
                "cosine",
                "similarity",
                "calculation,we",
                "use",
                "word2vec",
                "to",
                "calculate",
                "the",
                "sentence",
                "vector",
                "as",
                "sum",
                "of",
                "the",
                "word",
                "vectors",
                "of",
                "the",
                "words",
                "in",
                "the",
                "sentence.",
                "The",
                "calculation",
                "of",
                "sentence",
                "vector",
                "was",
                "to",
                "take",
                "advantage",
                "of",
                "the",
                "compositionality",
                "property",
                "using",
                "word2vec",
                "#TARGET_REF",
                ".",
                "We",
                "used",
                "word",
                "vectors",
                "of",
                "dimension",
                "100",
                "trained",
                "on",
                "the",
                "2015",
                "wikidump."
            ],
            "context": [
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2
            ]
        },
        "input": "For the cosine similarity calculation,we use word2vec to calculate the sentence vector as sum of the word vectors of the words in the sentence. The calculation of sentence vector was to take advantage of the compositionality property using word2vec #TARGET_REF . We used word vectors of dimension 100 trained on the 2015 wikidump.",
        "output": "<PERCEPT> For the cosine similarity calculation,we use word2vec to calculate the sentence vector as sum of the word vectors of the words in the sentence. </PERCEPT> <INFO> The calculation of sentence vector was to take advantage of the compositionality property using word2vec #TARGET_REF . </INFO> <PERCEPT> We used word vectors of dimension 100 trained on the 2015 wikidump. </PERCEPT>"
    },
    {
        "gold": {
            "text": [
                "There",
                "is",
                "a",
                "recent",
                "explosion",
                "of",
                "explanation-centred",
                "datasets",
                "for",
                "multi-hop",
                "question",
                "answering",
                "#REF",
                ".",
                "However,",
                "most",
                "of",
                "these",
                "datasets",
                "require",
                "the",
                "aggregation",
                "of",
                "only",
                "two",
                "sentences",
                "or",
                "paragraphs,",
                "making",
                "it",
                "hard",
                "to",
                "evaluate",
                "the",
                "robustness",
                "of",
                "the",
                "models",
                "in",
                "terms",
                "of",
                "semantic",
                "drift.",
                "On",
                "the",
                "other",
                "hand,",
                "the",
                "WorldTree",
                "corpus",
                "#TARGET_REF",
                "used",
                "in",
                "this",
                "shared",
                "task",
                "is",
                "explicitly",
                "designed",
                "to",
                "test",
                "multi-hop",
                "inference",
                "models",
                "on",
                "the",
                "reconstruction",
                "of",
                "long",
                "inference",
                "chains",
                "requiring",
                "the",
                "aggregation",
                "of",
                "an",
                "average",
                "of",
                "6",
                "facts,",
                "and",
                "as",
                "many",
                "as",
                "16",
                "facts."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                1,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2
            ]
        },
        "input": "There is a recent explosion of explanation-centred datasets for multi-hop question answering #REF . However, most of these datasets require the aggregation of only two sentences or paragraphs, making it hard to evaluate the robustness of the models in terms of semantic drift. On the other hand, the WorldTree corpus #TARGET_REF used in this shared task is explicitly designed to test multi-hop inference models on the reconstruction of long inference chains requiring the aggregation of an average of 6 facts, and as many as 16 facts.",
        "output": "There is a recent explosion of explanation-centred datasets for multi-hop question answering #REF . However, most of these datasets require the aggregation of only two sentences or paragraphs, making it hard to evaluate the robustness of the models in terms of semantic drift. On the other hand, <INFO> the WorldTree corpus #TARGET_REF </INFO> <PERCEPT> used in this shared task is explicitly designed to test multi-hop inference models on the reconstruction of long inference chains requiring the aggregation of an average of 6 facts, and as many as 16 facts. </PERCEPT>"
    },
    {
        "gold": {
            "text": [
                "Designing",
                "neural",
                "diversity",
                "metrics.",
                "In",
                "spite",
                "of",
                "growing",
                "interest",
                "in",
                "NLG",
                "models",
                "that",
                "produce",
                "diverse",
                "outputs,",
                "there",
                "is",
                "currently",
                "no",
                "principled",
                "neu-ral",
                "method",
                "for",
                "evaluating",
                "the",
                "diversity",
                "of",
                "an",
                "NLG",
                "system.",
                "As",
                "described",
                "in",
                "#TARGET_REF",
                ",",
                "existing",
                "automatic",
                "diversity",
                "metrics",
                "(e.g.",
                "Self-BLEU)",
                "perform",
                "worse",
                "than",
                "humans",
                "on",
                "the",
                "task",
                "of",
                "estimating",
                "content",
                "diversity,",
                "indicating",
                "a",
                "low",
                "correlation",
                "between",
                "metrics",
                "and",
                "human",
                "judgments."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3
            ]
        },
        "input": "Designing neural diversity metrics. In spite of growing interest in NLG models that produce diverse outputs, there is currently no principled neu-ral method for evaluating the diversity of an NLG system. As described in #TARGET_REF , existing automatic diversity metrics (e.g. Self-BLEU) perform worse than humans on the task of estimating content diversity, indicating a low correlation between metrics and human judgments.",
        "output": "Designing neural diversity metrics. In <PERCEPT> spite of growing interest in NLG models that produce diverse outputs, there is currently no principled neu-ral method for evaluating the diversity of an NLG system. As described in </PERCEPT> <INFO> #TARGET_REF , </INFO> <BACK> existing automatic diversity metrics (e.g. Self-BLEU) perform worse than humans on the task of estimating content diversity, indicating a low correlation between metrics and human judgments. </BACK>"
    },
    {
        "gold": {
            "text": [
                "serve.",
                "Designers",
                "of",
                "the",
                "method.",
                "For",
                "the",
                "designers",
                "of",
                "the",
                "detection",
                "method,",
                "explainability",
                "can",
                "serve",
                "as",
                "a",
                "principled",
                "mechanism",
                "for",
                "understanding",
                "and",
                "reasoning",
                "about",
                "the",
                "behavior",
                "of",
                "their",
                "method,",
                "which",
                "is",
                "important",
                "for",
                "multiple",
                "reasons.",
                "Firstly,",
                "if",
                "the",
                "detection",
                "method",
                "exhibits",
                "all",
                "the",
                "four",
                "properties",
                "of",
                "explain-ability,",
                "then",
                "the",
                "designers",
                "can",
                "easily",
                "gain",
                "insights",
                "into",
                "the",
                "factors",
                "that",
                "contributed",
                "to",
                "the",
                "decision",
                "made",
                "by",
                "the",
                "method",
                "given",
                "a",
                "comment.",
                "This",
                "can",
                "allow",
                "the",
                "designers",
                "to",
                "recognize",
                "when",
                "the",
                "method",
                "may",
                "be",
                "overly",
                "relying",
                "on",
                "a",
                "specific",
                "factor,",
                "e.g.,",
                "the",
                "demographic",
                "traits.",
                "In",
                "the",
                "case",
                "of",
                "social",
                "feature",
                "engineering",
                "and",
                "user",
                "embeddings",
                "based",
                "methods,",
                "operationalization",
                "of",
                "explainability",
                "via",
                "feature",
                "attribution",
                "such",
                "as",
                "LIME",
                "#REF",
                "and",
                "Integrated",
                "Gradients",
                "#TARGET_REF",
                "can",
                "be",
                "effective",
                "in",
                "offering",
                "such",
                "insights.",
                "For",
                "social",
                "graph",
                "based",
                "methods",
                "that",
                "employ",
                "graph",
                "neural",
                "networks,",
                "attribution",
                "techniques",
                "like",
                "GNNExplainer",
                "#REF",
                "can",
                "be",
                "used",
                "instead.",
                "The",
                "second",
                "reason",
                "why",
                "explainability",
                "is",
                "important",
                "for",
                "the",
                "designers",
                "is",
                "because",
                "it",
                "can",
                "allow",
                "them",
                "to",
                "optimize",
                "the",
                "method",
                "by",
                "removing",
                "inputs",
                "that",
                "do",
                "not",
                "contribute",
                "significantly.",
                "Here",
                "again,",
                "explainability",
                "via",
                "feature",
                "attribution",
                "can",
                "be",
                "effective.",
                "Lastly,",
                "explainability",
                "is",
                "also",
                "important",
                "for",
                "the",
                "designers",
                "to",
                "understand",
                "how",
                "their",
                "method",
                "would",
                "perform",
                "in",
                "cases",
                "where",
                "a",
                "user",
                "may",
                "try",
                "obfuscate",
                "abusive",
                "language",
                "#REF",
                ".",
                "Counterfactual",
                "explanations",
                "can",
                "constitute",
                "an",
                "effective",
                "operationalization",
                "for",
                "the",
                "designers",
                "to",
                "identify",
                "the",
                "parts",
                "of",
                "their",
                "method",
                "that",
                "are",
                "most",
                "vulnerable",
                "to",
                "obfuscations."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "serve. Designers of the method. For the designers of the detection method, explainability can serve as a principled mechanism for understanding and reasoning about the behavior of their method, which is important for multiple reasons. Firstly, if the detection method exhibits all the four properties of explain-ability, then the designers can easily gain insights into the factors that contributed to the decision made by the method given a comment. This can allow the designers to recognize when the method may be overly relying on a specific factor, e.g., the demographic traits. In the case of social feature engineering and user embeddings based methods, operationalization of explainability via feature attribution such as LIME #REF and Integrated Gradients #TARGET_REF can be effective in offering such insights. For social graph based methods that employ graph neural networks, attribution techniques like GNNExplainer #REF can be used instead. The second reason why explainability is important for the designers is because it can allow them to optimize the method by removing inputs that do not contribute significantly. Here again, explainability via feature attribution can be effective. Lastly, explainability is also important for the designers to understand how their method would perform in cases where a user may try obfuscate abusive language #REF . Counterfactual explanations can constitute an effective operationalization for the designers to identify the parts of their method that are most vulnerable to obfuscations.",
        "output": "serve. Designers of the method. For the designers of the detection method, explainability can serve as a principled mechanism for understanding and reasoning about the behavior of their method, which is important for multiple reasons. Firstly, if the detection method exhibits all the four properties of explain-ability, then the designers can easily gain insights into the factors that contributed to the decision made by the method given a comment. This can allow the designers to recognize when the method may be overly relying on a specific factor, e.g., the demographic traits. In the case of social feature engineering and user embeddings based methods, <BACK> operationalization of explainability via feature attribution such as LIME #REF </BACK> and <INFO> Integrated Gradients #TARGET_REF </INFO> <PERCEPT> can be effective in offering such insights. </PERCEPT> For social graph based methods that employ graph neural networks, attribution techniques like GNNExplainer #REF can be used instead. The second reason why explainability is important for the designers is because it can allow them to optimize the method by removing inputs that do not contribute significantly. Here again, explainability via feature attribution can be effective. Lastly, explainability is also important for the designers to understand how their method would perform in cases where a user may try obfuscate abusive language #REF . Counterfactual explanations can constitute an effective operationalization for the designers to identify the parts of their method that are most vulnerable to obfuscations."
    },
    {
        "gold": {
            "text": [
                "Our",
                "training",
                "data",
                "include",
                "five",
                "datasets",
                "for",
                "target-based",
                "sentiment",
                "classification:",
                "SemEval17",
                "#TARGET_REF",
                ",",
                "entities",
                "#REF",
                ",",
                "open",
                "domain",
                "#REF",
                ",",
                "Irish",
                "politics",
                "#REF",
                ",",
                "and",
                "our",
                "annotations",
                "of",
                "positive/negative",
                "norms",
                "toward",
                "norm",
                "targets",
                "(",
                "§5.1).",
                "These",
                "annotations",
                "highly",
                "improve",
                "classification",
                "of",
                "sentiments",
                "expressed",
                "through",
                "advocacy",
                "and",
                "opposition",
                "in",
                "normative",
                "statements.",
                "Pretraining",
                "on",
                "general",
                "sentiment",
                "resourcessubjectivity",
                "lexicon",
                "#REF",
                "and",
                "sen-timent140",
                "#REF",
                "-also",
                "helps",
                "(",
                "Table",
                "3:",
                "Mapping",
                "between",
                "corpus-specific",
                "labels",
                "and",
                "our",
                "labels",
                "for",
                "the",
                "causality",
                "module.",
                "†",
                "The",
                "order",
                "of",
                "two",
                "input",
                "texts",
                "are",
                "reversed.",
                "‡",
                "The",
                "second",
                "input",
                "text",
                "is",
                "replaced",
                "with",
                "a",
                "random",
                "text",
                "in",
                "the",
                "corpus."
            ],
            "context": [
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                0,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Our training data include five datasets for target-based sentiment classification: SemEval17 #TARGET_REF , entities #REF , open domain #REF , Irish politics #REF , and our annotations of positive/negative norms toward norm targets ( §5.1). These annotations highly improve classification of sentiments expressed through advocacy and opposition in normative statements. Pretraining on general sentiment resourcessubjectivity lexicon #REF and sen-timent140 #REF -also helps ( Table 3: Mapping between corpus-specific labels and our labels for the causality module. † The order of two input texts are reversed. ‡ The second input text is replaced with a random text in the corpus.",
        "output": "<PERCEPT> Our training data include five datasets for target-based sentiment classification: </PERCEPT> <INFO> SemEval17 </INFO> #TARGET_REF , <BACK> entities #REF , open domain #REF , Irish politics #REF , and our annotations of positive/negative norms toward norm targets ( §5.1). </BACK> <PERCEPT> These annotations highly improve classification of sentiments expressed through advocacy and opposition in normative statements. </PERCEPT> Pretraining on general sentiment resourcessubjectivity lexicon #REF and sen-timent140 #REF -also helps ( Table 3: Mapping between corpus-specific labels and our labels for the causality module. † The order of two input texts are reversed. ‡ The second input text is replaced with a random text in the corpus."
    },
    {
        "gold": {
            "text": [
                "One",
                "of",
                "the",
                "crucial",
                "issues",
                "regarding",
                "the",
                "evaluation",
                "of",
                "multi-hop",
                "inference",
                "models",
                "is",
                "the",
                "possibility",
                "to",
                "achieve",
                "strong",
                "overall",
                "performance",
                "without",
                "using",
                "real",
                "compositional",
                "methods",
                "#TARGET_REF",
                ".",
                "Therefore,",
                "in",
                "order",
                "to",
                "evaluate",
                "multi-hop",
                "inference",
                "more",
                "explicitly,",
                "we",
                "break",
                "down",
                "the",
                "performance",
                "of",
                "each",
                "model",
                "with",
                "respect",
                "to",
                "the",
                "difficulty",
                "of",
                "accessing",
                "specific",
                "facts",
                "in",
                "an",
                "explanation",
                "via",
                "direct",
                "lexical",
                "overlap.",
                "This",
                "comes",
                "from",
                "the",
                "assumption",
                "that",
                "facts",
                "sharing",
                "many",
                "terms",
                "with",
                "question",
                "or",
                "answer",
                "are",
                "relatively",
                "easier",
                "to",
                "find",
                "and",
                "rank",
                "highly."
            ],
            "context": [
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3
            ]
        },
        "input": "One of the crucial issues regarding the evaluation of multi-hop inference models is the possibility to achieve strong overall performance without using real compositional methods #TARGET_REF . Therefore, in order to evaluate multi-hop inference more explicitly, we break down the performance of each model with respect to the difficulty of accessing specific facts in an explanation via direct lexical overlap. This comes from the assumption that facts sharing many terms with question or answer are relatively easier to find and rank highly.",
        "output": "<INFO> One of the crucial issues regarding the evaluation of multi-hop inference models is the possibility to achieve strong overall performance without using real compositional methods #TARGET_REF . </INFO> Therefore, <PERCEPT> in order to evaluate multi-hop inference more explicitly, we break down the performance of each model with respect to the difficulty of accessing specific facts in an explanation via direct lexical overlap. </PERCEPT> <BACK> This comes from the assumption that facts sharing many terms with question or answer are relatively easier to find and rank highly. </BACK>"
    },
    {
        "gold": {
            "text": [
                "All",
                "methods",
                "and",
                "materials",
                "discussed",
                "in",
                "this",
                "paper",
                "were",
                "tested",
                "on",
                "a",
                "fully",
                "functional",
                "machine",
                "translation",
                "system",
                "based",
                "on",
                "Apertium",
                "#REF",
                "and",
                "#TARGET_REF",
                ",",
                "an",
                "opensource",
                "RBMT",
                "toolkit."
            ],
            "context": [
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                3,
                3,
                3,
                1,
                1,
                1,
                1,
                1,
                1
            ]
        },
        "input": "All methods and materials discussed in this paper were tested on a fully functional machine translation system based on Apertium #REF and #TARGET_REF , an opensource RBMT toolkit.",
        "output": "<PERCEPT> All methods and materials discussed in this paper were tested on a fully functional machine translation system based on </PERCEPT> <BACK> Apertium #REF and </BACK> <INFO> #TARGET_REF , an opensource RBMT toolkit. </INFO>"
    },
    {
        "gold": {
            "text": [
                "The",
                "aforementioned",
                "correlations,",
                "however,",
                "are",
                "byproducts",
                "rather",
                "than",
                "core",
                "mechanisms",
                "of",
                "argumentative",
                "relations.",
                "In",
                "order",
                "to",
                "decide",
                "whether",
                "a",
                "statement",
                "supports",
                "or",
                "attacks",
                "another,",
                "we",
                "cannot",
                "ignore",
                "the",
                "logical",
                "relation",
                "between",
                "them.",
                "Textual",
                "entailment",
                "was",
                "found",
                "to",
                "inform",
                "argumentative",
                "relations",
                "#REF",
                "and",
                "used",
                "to",
                "detect",
                "arguments",
                "#TARGET_REF",
                ".",
                "Similarly,",
                "there",
                "is",
                "evidence",
                "that",
                "the",
                "opinions",
                "of",
                "two",
                "statements",
                "toward",
                "the",
                "same",
                "concept",
                "constitute",
                "their",
                "argumentative",
                "relations",
                "#REF",
                ".",
                "Causality",
                "between",
                "events",
                "also",
                "received",
                "attention,",
                "and",
                "causality",
                "graph",
                "construction",
                "was",
                "proposed",
                "for",
                "argument",
                "analysis",
                "#REF",
                ".",
                "Additionally,",
                "in",
                "argumentation",
                "theory,",
                "Walton's",
                "argumentation",
                "schemes",
                "#REF",
                "specify",
                "common",
                "reasoning",
                "patterns",
                "people",
                "use",
                "to",
                "form",
                "an",
                "argument.",
                "This",
                "motivates",
                "our",
                "work",
                "to",
                "investigate",
                "logical",
                "mechanisms",
                "in",
                "four",
                "categories:",
                "factual",
                "consistency,",
                "sentiment",
                "coherence,",
                "causal",
                "relation,",
                "and",
                "normative",
                "relation."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                1,
                1,
                1,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                1,
                1,
                1,
                1,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The aforementioned correlations, however, are byproducts rather than core mechanisms of argumentative relations. In order to decide whether a statement supports or attacks another, we cannot ignore the logical relation between them. Textual entailment was found to inform argumentative relations #REF and used to detect arguments #TARGET_REF . Similarly, there is evidence that the opinions of two statements toward the same concept constitute their argumentative relations #REF . Causality between events also received attention, and causality graph construction was proposed for argument analysis #REF . Additionally, in argumentation theory, Walton's argumentation schemes #REF specify common reasoning patterns people use to form an argument. This motivates our work to investigate logical mechanisms in four categories: factual consistency, sentiment coherence, causal relation, and normative relation.",
        "output": "The aforementioned correlations, however, are byproducts rather than core mechanisms of argumentative relations. In order to decide whether a statement supports or attacks another, we cannot ignore the logical relation between them. <INFO> Textual entailment was </INFO> <BACK> found to inform argumentative relations #REF and </BACK> <INFO> used to detect arguments #TARGET_REF . </INFO> Similarly, there is evidence that the opinions of two statements toward the same concept constitute their argumentative relations #REF . Causality between events also received attention, and causality graph construction was proposed for argument analysis #REF . Additionally, in argumentation theory, Walton's argumentation schemes #REF specify common reasoning patterns people use to form an argument. This motivates our work to investigate logical mechanisms in four categories: factual consistency, sentiment coherence, causal relation, and normative relation."
    },
    {
        "gold": {
            "text": [
                "Even",
                "in",
                "the",
                "presence",
                "of",
                "such",
                "an",
                "agreement,",
                "learning",
                "to",
                "disentangle",
                "the",
                "surface",
                "realization",
                "of",
                "the",
                "underlying",
                "factors",
                "of",
                "data",
                "(e.g.,",
                "semantics,",
                "syntactic,",
                "lexical)",
                "in",
                "the",
                "representation",
                "space",
                "is",
                "a",
                "nontrivial",
                "task.",
                "Additionally,",
                "there",
                "is",
                "no",
                "established",
                "study",
                "for",
                "evaluating",
                "such",
                "models",
                "in",
                "NLP.",
                "A",
                "handful",
                "of",
                "recent",
                "works",
                "have",
                "looked",
                "into",
                "disentanglement",
                "for",
                "text",
                "by",
                "splitting",
                "the",
                "representation",
                "space",
                "into",
                "predefined",
                "disentangled",
                "subspaces",
                "such",
                "as",
                "style",
                "and",
                "content",
                "#TARGET_REF",
                ",",
                "or",
                "syntax",
                "and",
                "semantics",
                "#REF",
                ",",
                "and",
                "rely",
                "on",
                "supervision",
                "during",
                "training.",
                "However,",
                "a",
                "generalizable",
                "and",
                "realistic",
                "approach",
                "needs",
                "to",
                "be",
                "unsupervised",
                "and",
                "capable",
                "of",
                "identifying",
                "the",
                "underlying",
                "factors",
                "solely",
                "via",
                "the",
                "regularities",
                "presented",
                "in",
                "data."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                1,
                1,
                1,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Even in the presence of such an agreement, learning to disentangle the surface realization of the underlying factors of data (e.g., semantics, syntactic, lexical) in the representation space is a nontrivial task. Additionally, there is no established study for evaluating such models in NLP. A handful of recent works have looked into disentanglement for text by splitting the representation space into predefined disentangled subspaces such as style and content #TARGET_REF , or syntax and semantics #REF , and rely on supervision during training. However, a generalizable and realistic approach needs to be unsupervised and capable of identifying the underlying factors solely via the regularities presented in data.",
        "output": "Even in the presence of such an agreement, learning to disentangle the surface realization of the underlying factors of data (e.g., semantics, syntactic, lexical) in the representation space is a nontrivial task. Additionally, there is no established study for evaluating such models in NLP. <BACK> A handful of recent works have looked into </BACK> <INFO> disentanglement for text by splitting the representation space into predefined disentangled subspaces such as style and content #TARGET_REF , </INFO> <BACK> or syntax and semantics #REF , and </BACK> <INFO> rely on supervision during training. </INFO> However, a generalizable and realistic approach needs to be unsupervised and capable of identifying the underlying factors solely via the regularities presented in data."
    },
    {
        "gold": {
            "text": [
                "Most",
                "of",
                "the",
                "works",
                "that",
                "study",
                "this",
                "task",
                "commonly",
                "point",
                "first",
                "to",
                "surface-level",
                "features,",
                "such",
                "as",
                "bag",
                "of",
                "words",
                "and",
                "lexicon-based",
                "approaches,",
                "with",
                "negative",
                "words",
                "as",
                "features",
                "#TARGET_REF",
                "."
            ],
            "context": [
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                0
            ]
        },
        "input": "Most of the works that study this task commonly point first to surface-level features, such as bag of words and lexicon-based approaches, with negative words as features #TARGET_REF .",
        "output": "<PERCEPT> Most of the works that study this task commonly point first to surface-level features, such as bag of words and lexicon-based approaches, with </PERCEPT> <INFO> negative words as features #TARGET_REF </INFO> ."
    },
    {
        "gold": {
            "text": [
                "In",
                "this",
                "Spanish-English",
                "example,",
                "the",
                "clitic",
                "can",
                "climb",
                "over",
                "an",
                "unlimited",
                "number",
                "of",
                "'trigger",
                "verbs'",
                "#TARGET_REF",
                ")",
                "(indicated",
                "by",
                "the",
                "ellipses",
                "in",
                "the",
                "example),",
                "and",
                "for",
                "certain",
                "TAG",
                "grammars",
                "this",
                "can",
                "correspond",
                "to",
                "a",
                "pair",
                "of",
                "derivation",
                "trees",
                "as",
                "in",
                "Figure",
                "2.",
                "In",
                "this",
                "pair",
                "of",
                "trees,",
                "his",
                "corresponds",
                "to",
                "both",
                "los",
                "and",
                "the",
                "clitic",
                "le.",
                "Both",
                "his",
                "and",
                "los",
                "are",
                "fixed",
                "in",
                "relation",
                "to",
                "the",
                "root",
                "of",
                "the",
                "tree,",
                "but",
                "le",
                "is",
                "an",
                "unbounded",
                "distance",
                "from",
                "it,",
                "so",
                "it",
                "is",
                "not",
                "possible",
                "to",
                "form",
                "a",
                "gCN",
                "in",
                "the",
                "Spanish",
                "tree",
                "for",
                "pairing",
                "without",
                "the",
                "unbounded",
                "and",
                "unrelated",
                "recursively-inserted",
                "verbs,",
                "hence",
                "requiring",
                "infinitely",
                "many",
                "transfer",
                "rules."
            ],
            "context": [
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "In this Spanish-English example, the clitic can climb over an unlimited number of 'trigger verbs' #TARGET_REF ) (indicated by the ellipses in the example), and for certain TAG grammars this can correspond to a pair of derivation trees as in Figure 2. In this pair of trees, his corresponds to both los and the clitic le. Both his and los are fixed in relation to the root of the tree, but le is an unbounded distance from it, so it is not possible to form a gCN in the Spanish tree for pairing without the unbounded and unrelated recursively-inserted verbs, hence requiring infinitely many transfer rules.",
        "output": "<PERCEPT> In this Spanish-English example, the clitic can climb over </PERCEPT> <INFO> an unlimited number of 'trigger verbs' #TARGET_REF ) </INFO> <BACK> (indicated by the ellipses in the example), </BACK> <PERCEPT> and for certain TAG grammars this can correspond to a pair of derivation trees as in </PERCEPT> <BACK> Figure 2. </BACK> In this pair of trees, his corresponds to both los and the clitic le. Both his and los are fixed in relation to the root of the tree, but le is an unbounded distance from it, so it is not possible to form a gCN in the Spanish tree for pairing without the unbounded and unrelated recursively-inserted verbs, hence requiring infinitely many transfer rules."
    },
    {
        "gold": {
            "text": [
                "reality",
                "or",
                "truth",
                "of.",
                "One",
                "of",
                "its",
                "troponym",
                "is",
                "to",
                "concede,",
                "profess,",
                "confess",
                "which",
                "is",
                "defined",
                "as",
                "to",
                "admit",
                "(to",
                "a",
                "wrongdoing).",
                "The",
                "superordinate",
                "concept",
                "of",
                "this",
                "chain",
                "is",
                "the",
                "verb",
                "think,",
                "cogitate,",
                "cerebrate",
                "which",
                "is",
                "defined",
                "as-to",
                "use",
                "or",
                "exercise",
                "the",
                "mind",
                "or",
                "one's",
                "power",
                "of",
                "reason",
                "in",
                "order",
                "to",
                "make",
                "inferences,",
                "decisions,",
                "or",
                "arrive",
                "at",
                "a",
                "solution",
                "or",
                "judgments.",
                "Thus,",
                "it",
                "is",
                "clear",
                "from",
                "the",
                "semantic",
                "hierarchy",
                "that",
                "to",
                "apologize",
                "is",
                "to",
                "undergo",
                "a",
                "logical",
                "thought",
                "process,",
                "the",
                "natural",
                "entailment",
                "of",
                "which",
                "is",
                "to",
                "admit",
                "to",
                "a",
                "wrong.",
                "Once",
                "the",
                "wrongdoing",
                "is",
                "admitted",
                "the",
                "natural",
                "consequence",
                "should",
                "be",
                "to",
                "take",
                "responsibility",
                "and",
                "offer",
                "amends.",
                "For",
                "instance,",
                "apology",
                "number",
                "2",
                "says-I",
                "sincerely",
                "apologize",
                "to",
                "all",
                "Satyamites",
                "and",
                "stakeholders.",
                "This",
                "is",
                "a",
                "clear",
                "admission",
                "of",
                "wrongdoing.",
                "The",
                "selected",
                "concept",
                "of",
                "the",
                "verb",
                "regret",
                "is",
                "defined",
                "as",
                "to",
                "feel",
                "remorse",
                "for,",
                "feel",
                "sorry",
                "for",
                "or",
                "be",
                "contrite",
                "about.",
                "Its",
                "inherited",
                "hypernymy",
                "is",
                "to",
                "feel,",
                "experience,",
                "which",
                "is",
                "defined",
                "as",
                "to",
                "undergo",
                "an",
                "emotional",
                "sensation",
                "or",
                "be",
                "in",
                "a",
                "particular",
                "state",
                "of",
                "mind.",
                "Thus,",
                "to",
                "regret",
                "is",
                "to",
                "undergo",
                "a",
                "feeling",
                "by",
                "the",
                "offender",
                "about",
                "the",
                "wrongdoing.",
                "In",
                "the",
                "corpus",
                "apology",
                "number",
                "10,",
                "the",
                "Amazon",
                "India",
                "letter",
                "states,",
                "To",
                "the",
                "extent",
                "that",
                "these",
                "items",
                "offered",
                "by",
                "a",
                "third-party",
                "seller",
                "in",
                "Canada",
                "offended",
                "Indian",
                "sensibilities,",
                "Amazon",
                "regrets",
                "the",
                "same."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "reality or truth of. One of its troponym is to concede, profess, confess which is defined as to admit (to a wrongdoing). The superordinate concept of this chain is the verb think, cogitate, cerebrate which is defined as-to use or exercise the mind or one's power of reason in order to make inferences, decisions, or arrive at a solution or judgments. Thus, it is clear from the semantic hierarchy that to apologize is to undergo a logical thought process, the natural entailment of which is to admit to a wrong. Once the wrongdoing is admitted the natural consequence should be to take responsibility and offer amends. For instance, apology number 2 says-I sincerely apologize to all Satyamites and stakeholders. This is a clear admission of wrongdoing. The selected concept of the verb regret is defined as to feel remorse for, feel sorry for or be contrite about. Its inherited hypernymy is to feel, experience, which is defined as to undergo an emotional sensation or be in a particular state of mind. Thus, to regret is to undergo a feeling by the offender about the wrongdoing. In the corpus apology number 10, the Amazon India letter states, To the extent that these items offered by a third-party seller in Canada offended Indian sensibilities, Amazon regrets the same.",
        "output": "reality or truth of. One of its troponym is to concede, profess, confess which is defined as to admit (to a wrongdoing). The superordinate concept of this chain is the verb think, cogitate, cerebrate which is defined as-to use or exercise the mind or one's power of reason in order to make inferences, decisions, or arrive at a solution or judgments. Thus, it is clear from the semantic hierarchy that to apologize is to undergo a logical thought process, the natural entailment of which is to admit to a wrong. Once the wrongdoing is admitted the natural consequence should be to take responsibility and offer amends. For instance, apology number 2 says-I sincerely apologize to all Satyamites and stakeholders. This is a clear admission of wrongdoing. The selected concept of the verb regret is defined as to feel remorse for, feel sorry for or be contrite about. Its inherited hypernymy is to feel, experience, which is defined as to undergo an emotional sensation or be in a particular state of mind. Thus, to regret is to undergo a feeling by the offender about the wrongdoing. In the corpus apology number 10, the Amazon India letter states, To the extent that these items offered by a third-party seller in Canada offended Indian sensibilities, Amazon regrets the same."
    },
    {
        "gold": {
            "text": [
                "A",
                "bilingual",
                "parallel",
                "corpus",
                "#REF",
                "was",
                "used",
                "for",
                "automatic",
                "evaluation",
                "using",
                "BLEU",
                "metric",
                "#REF",
                ".",
                "The",
                "results",
                "are",
                "presented",
                "in",
                "Table",
                "2.",
                "The",
                "values",
                "are",
                "quite",
                "low,",
                "partly",
                "due",
                "to",
                "reasons",
                "explained",
                "in",
                "(Callison-Burch",
                "et",
                "al.,",
                "partly",
                "due",
                "to",
                "unknown",
                "words",
                "in",
                "test",
                "corpus.",
                "Figure",
                "6",
                "shows",
                "results",
                "of",
                "evaluation",
                "of",
                "translation",
                "quality",
                "using",
                "subjective",
                "measures",
                "using",
                "methodology",
                "#TARGET_REF",
                ".",
                "The",
                "methodology",
                "is",
                "explained",
                "in",
                "chapter",
                "4.1.",
                "Four",
                "independent",
                "evaluators",
                "(two",
                "native",
                "speakers)",
                "evaluated",
                "sets",
                "of",
                "100",
                "sentences",
                "using",
                "this",
                "methodology."
            ],
            "context": [
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                3,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3
            ]
        },
        "input": "A bilingual parallel corpus #REF was used for automatic evaluation using BLEU metric #REF . The results are presented in Table 2. The values are quite low, partly due to reasons explained in (Callison-Burch et al., partly due to unknown words in test corpus. Figure 6 shows results of evaluation of translation quality using subjective measures using methodology #TARGET_REF . The methodology is explained in chapter 4.1. Four independent evaluators (two native speakers) evaluated sets of 100 sentences using this methodology.",
        "output": "<BACK> A bilingual parallel corpus #REF was used for automatic evaluation using BLEU metric #REF </BACK> . The results are presented in Table 2. The values are quite low, partly due to reasons explained in (Callison-Burch et al., partly due to unknown words in test corpus. <BACK> Figure 6 shows </BACK> <PERCEPT> results of </PERCEPT> <INFO> evaluation of translation quality using subjective measures using methodology #TARGET_REF . </INFO> <PERCEPT> The methodology is explained in chapter 4.1. </PERCEPT> <BACK> Four independent evaluators (two native speakers) evaluated sets of 100 sentences using this methodology. </BACK>"
    },
    {
        "gold": {
            "text": [
                "Therefore,",
                "we",
                "additionally",
                "use",
                "two",
                "distantly",
                "labeled",
                "entity",
                "typing",
                "datasets",
                "derived",
                "from",
                "Wikipedia.",
                "We",
                "leverage",
                "past",
                "work",
                "in",
                "using",
                "types",
                "derived",
                "from",
                "Wikipedia",
                "categories",
                "#TARGET_REF",
                ",",
                "which",
                "contain",
                "type",
                "information",
                "and",
                "are",
                "widely",
                "annotated",
                "across",
                "Wikipedia",
                "articles.",
                "We",
                "select",
                "the",
                "appropriate",
                "dataset",
                "for",
                "each",
                "setting",
                "depending",
                "on",
                "task-specific",
                "requirements",
                "(see",
                "Section",
                "6).",
                "For",
                "all",
                "datasets,",
                "we",
                "compute",
                "entity",
                "typing",
                "macro",
                "F1",
                "using",
                "development",
                "examples",
                "(1k)",
                "to",
                "check",
                "model",
                "convergence."
            ],
            "context": [
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Therefore, we additionally use two distantly labeled entity typing datasets derived from Wikipedia. We leverage past work in using types derived from Wikipedia categories #TARGET_REF , which contain type information and are widely annotated across Wikipedia articles. We select the appropriate dataset for each setting depending on task-specific requirements (see Section 6). For all datasets, we compute entity typing macro F1 using development examples (1k) to check model convergence.",
        "output": "Therefore, <BACK> we additionally use two distantly labeled entity typing datasets derived from Wikipedia. </BACK> <PERCEPT> We leverage past work in using types derived from </PERCEPT> <INFO> Wikipedia categories #TARGET_REF , which contain type information and are widely annotated across Wikipedia articles. </INFO> <BACK> We select the appropriate dataset for each setting depending on task-specific requirements (see Section 6). </BACK> For all datasets, we compute entity typing macro F1 using development examples (1k) to check model convergence."
    },
    {
        "gold": {
            "text": [
                "Each",
                "annotated",
                "document",
                "consists",
                "of",
                "a",
                "title",
                "and",
                "3",
                "paragraphs",
                "of",
                "text,",
                "and",
                "contains",
                "a",
                "list",
                "of",
                "non-pronominal",
                "base-NPs",
                "(most",
                "identified",
                "by",
                "SpaCy",
                "#TARGET_REF",
                "9",
                "but",
                "some",
                "added",
                "manually",
                "by",
                "the",
                "annotators),",
                "a",
                "list",
                "of",
                "coreference",
                "clusters",
                "over",
                "the",
                "NPs,",
                "and",
                "a",
                "list",
                "of",
                "NP-relations",
                "that",
                "hold",
                "in",
                "the",
                "text.",
                "Each",
                "relation",
                "is",
                "a",
                "triplet",
                "consisting",
                "of",
                "two",
                "NPs",
                "from",
                "the",
                "NP",
                "list,",
                "and",
                "a",
                "connecting",
                "element",
                "which",
                "is",
                "one",
                "of",
                "23",
                "prepositions",
                "(displayed",
                "in",
                "Table",
                "1)",
                "10",
                "or",
                "a",
                "''member(s)",
                "of''",
                "relation",
                "designating",
                "set-membership.",
                "The",
                "list",
                "of",
                "NP",
                "relations",
                "is",
                "exhaustive,",
                "and",
                "aims",
                "to",
                "cover",
                "all",
                "and",
                "only",
                "valid",
                "NP-NP",
                "relations",
                "in",
                "the",
                "document."
            ],
            "context": [
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                2,
                2,
                2,
                1,
                1,
                1,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Each annotated document consists of a title and 3 paragraphs of text, and contains a list of non-pronominal base-NPs (most identified by SpaCy #TARGET_REF 9 but some added manually by the annotators), a list of coreference clusters over the NPs, and a list of NP-relations that hold in the text. Each relation is a triplet consisting of two NPs from the NP list, and a connecting element which is one of 23 prepositions (displayed in Table 1) 10 or a ''member(s) of'' relation designating set-membership. The list of NP relations is exhaustive, and aims to cover all and only valid NP-NP relations in the document.",
        "output": "<BACK> Each annotated document </BACK> consists of a title and 3 paragraphs of text, and <BACK> contains a list of non-pronominal base-NPs </BACK> <PERCEPT> (most identified by </PERCEPT> <INFO> SpaCy #TARGET_REF 9 </INFO> <BACK> but some added manually by the annotators), </BACK> a list of coreference clusters over the NPs, and a list of NP-relations that hold in the text. Each relation is a triplet consisting of two NPs from the NP list, and a connecting element which is one of 23 prepositions (displayed in Table 1) 10 or a ''member(s) of'' relation designating set-membership. The list of NP relations is exhaustive, and aims to cover all and only valid NP-NP relations in the document."
    },
    {
        "gold": {
            "text": [
                "Coreference",
                "Arc",
                "Prediction",
                "(CAP)",
                "We",
                "use",
                "the",
                "English",
                "CAP",
                "dataset",
                "derived",
                "from",
                "PreCo",
                "#TARGET_REF",
                "by",
                "#REF",
                ".",
                "The",
                "creators",
                "of",
                "the",
                "dataset",
                "partition",
                "the",
                "data",
                "by",
                "cosine",
                "similarity",
                "of",
                "GloVe",
                "#REF",
                "embeddings",
                "of",
                "mention",
                "spans",
                "and",
                "balance",
                "the",
                "number",
                "of",
                "positive",
                "and",
                "negative",
                "examples",
                "in",
                "each",
                "bucket,",
                "so",
                "that",
                "models",
                "do",
                "not",
                "solve",
                "the",
                "task",
                "by",
                "capturing",
                "surface",
                "features",
                "of",
                "entity",
                "mention",
                "spans.",
                "The",
                "original",
                "data",
                "split",
                "provides",
                "8k",
                "examples",
                "for",
                "each",
                "of",
                "the",
                "training,",
                "development,",
                "and",
                "test",
                "sets."
            ],
            "context": [
                0,
                0,
                0,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                2,
                2,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Coreference Arc Prediction (CAP) We use the English CAP dataset derived from PreCo #TARGET_REF by #REF . The creators of the dataset partition the data by cosine similarity of GloVe #REF embeddings of mention spans and balance the number of positive and negative examples in each bucket, so that models do not solve the task by capturing surface features of entity mention spans. The original data split provides 8k examples for each of the training, development, and test sets.",
        "output": "Coreference Arc Prediction (CAP) <PERCEPT> We use the English CAP dataset derived from </PERCEPT> <INFO> PreCo #TARGET_REF </INFO> <PERCEPT> by #REF . </PERCEPT> The creators of the dataset partition the data by cosine similarity of GloVe #REF embeddings of mention spans and balance the number of positive and negative examples in each bucket, so that models do not solve the task by capturing surface features of entity mention spans. The original data split provides 8k examples for each of the training, development, and test sets."
    },
    {
        "gold": {
            "text": [
                "Apertium,",
                "the",
                "open-source",
                "MT",
                "platform",
                "that",
                "was",
                "used",
                "as",
                "basis",
                "in",
                "the",
                "case",
                "study,",
                "is",
                "described",
                "in",
                "the",
                "first",
                "section",
                "following",
                "the",
                "introduction.",
                "Materials",
                "and",
                "methods",
                "describe",
                "already",
                "available",
                "language",
                "processing",
                "tools",
                "and",
                "materials,",
                "mainly",
                "corpora.",
                "The",
                "newly",
                "developed",
                "methods",
                "are",
                "described",
                "in",
                "the",
                "same",
                "section.",
                "Following",
                "section",
                "describes",
                "results",
                "and",
                "evaluation",
                "methods.",
                "The",
                "last",
                "section",
                "describes",
                "discussion",
                "and",
                "further",
                "work.",
                "The",
                "modules",
                "are",
                "shown",
                "on",
                "Figure",
                "1,",
                "where",
                "the",
                "specially",
                "addressed",
                "modules",
                "are",
                "marked",
                "with",
                "a",
                "new",
                "colour",
                "and",
                "the",
                "two",
                "newly",
                "added",
                "modules",
                "are",
                "inserted.",
                "Each",
                "group's",
                "data",
                "creation",
                "was",
                "addressed",
                "by",
                "a",
                "particular",
                "method,",
                "monolingual",
                "dictionaries",
                "were",
                "constructed",
                "using",
                "bilingual",
                "dictionary",
                "data",
                "and",
                "applying",
                "automatic",
                "paradigm",
                "tagging",
                "techniques,",
                "bilingual",
                "dictionary",
                "was",
                "constructed",
                "using",
                "available",
                "bilingual",
                "word-list",
                "but",
                "a",
                "few",
                "methods",
                "for",
                "automatic",
                "bilingual",
                "dictionary",
                "construction",
                "were",
                "investigated,",
                "a",
                "method",
                "for",
                "automatic",
                "structural",
                "shallow-transfer",
                "rule",
                "construction",
                "(Sánchez-",
                "#TARGET_REF",
                "will",
                "be",
                "used",
                "to",
                "construct",
                "a",
                "set",
                "of",
                "structural",
                "transfer",
                "rules."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2
            ]
        },
        "input": "Apertium, the open-source MT platform that was used as basis in the case study, is described in the first section following the introduction. Materials and methods describe already available language processing tools and materials, mainly corpora. The newly developed methods are described in the same section. Following section describes results and evaluation methods. The last section describes discussion and further work. The modules are shown on Figure 1, where the specially addressed modules are marked with a new colour and the two newly added modules are inserted. Each group's data creation was addressed by a particular method, monolingual dictionaries were constructed using bilingual dictionary data and applying automatic paradigm tagging techniques, bilingual dictionary was constructed using available bilingual word-list but a few methods for automatic bilingual dictionary construction were investigated, a method for automatic structural shallow-transfer rule construction (Sánchez- #TARGET_REF will be used to construct a set of structural transfer rules.",
        "output": "Apertium, the open-source MT platform that was used as basis in the case study, is described in the first section following the introduction. Materials and methods describe already available language processing tools and materials, mainly corpora. The newly developed methods are described in the same section. Following section describes results and evaluation methods. The last section describes discussion and further work. The modules are shown on Figure 1, where the specially addressed modules are marked with a new colour and the two newly added modules are inserted. <BACK> Each group's data creation was addressed by a particular method, </BACK> monolingual dictionaries were constructed using bilingual dictionary data and applying automatic paradigm tagging techniques, bilingual dictionary was constructed using available bilingual word-list but a few methods for automatic bilingual dictionary construction were investigated, <INFO> a method for automatic structural shallow-transfer rule construction (Sánchez- #TARGET_REF </INFO> <PERCEPT> will be used to construct a set of structural transfer rules. </PERCEPT>"
    },
    {
        "gold": {
            "text": [
                "The",
                "Semantic",
                "Network",
                "Array",
                "Processor",
                "(SNAP)",
                "is",
                "a",
                "highly",
                "parallel",
                "array",
                "processor",
                "fully",
                "optimized",
                "for",
                "semantic",
                "network",
                "processing",
                "with",
                "marker-passing",
                "mechanism.",
                "In",
                "order",
                "to",
                "facilitate",
                "efficient",
                "propagation",
                "of",
                "markers",
                "and",
                "to",
                "ease",
                "development",
                "of",
                "applications,",
                "a",
                "set",
                "of",
                "marker",
                "propagation",
                "instructions",
                "has",
                "been",
                "microcoded.",
                "SNAP",
                "supports",
                "propagation",
                "of",
                "markers",
                "containing",
                "(1)",
                "bit-vectors.",
                "(2)",
                "address,",
                "and",
                "(3)",
                "numeric",
                "value.",
                "By",
                "limiting",
                "content",
                "of",
                "markers,",
                "significant",
                "reduction",
                "in",
                "cost",
                "and",
                "resource",
                "has",
                "been",
                "attained",
                "without",
                "undermining",
                "performance",
                "requirements",
                "for",
                "knowledge",
                "processing.",
                "Several",
                "AI",
                "applications",
                "such",
                "as",
                "natural",
                "language",
                "processing",
                "system,",
                "classification",
                "system",
                "#TARGET_REF",
                ",",
                "and",
                "rule-based",
                "system",
                "has",
                "been",
                "developed",
                "on",
                "SNAP."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                3,
                3,
                1,
                1,
                1,
                0,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                3
            ]
        },
        "input": "The Semantic Network Array Processor (SNAP) is a highly parallel array processor fully optimized for semantic network processing with marker-passing mechanism. In order to facilitate efficient propagation of markers and to ease development of applications, a set of marker propagation instructions has been microcoded. SNAP supports propagation of markers containing (1) bit-vectors. (2) address, and (3) numeric value. By limiting content of markers, significant reduction in cost and resource has been attained without undermining performance requirements for knowledge processing. Several AI applications such as natural language processing system, classification system #TARGET_REF , and rule-based system has been developed on SNAP.",
        "output": "The Semantic Network Array Processor (SNAP) is a highly parallel array processor fully optimized for semantic network processing with marker-passing mechanism. In order to facilitate efficient propagation of markers and to ease development of applications, a set of marker propagation instructions has been microcoded. SNAP supports propagation of markers containing (1) bit-vectors. (2) address, and (3) numeric value. By limiting content of markers, significant reduction in cost and resource has been attained without undermining performance requirements for knowledge processing. Several AI applications such as <BACK> natural language processing system, </BACK> <INFO> classification system #TARGET_REF </INFO> , and <BACK> rule-based system has been developed on SNAP. </BACK>"
    },
    {
        "gold": {
            "text": [
                "Data",
                "I",
                "use",
                "Universal",
                "Dependencies",
                "(UD)",
                "2.8",
                "2",
                "#TARGET_REF",
                "and",
                "the",
                "automatically-parsed",
                "Wikipedia",
                "datasets",
                "released",
                "as",
                "part",
                "of",
                "the",
                "CoNLL",
                "2017",
                "Shared",
                "Task",
                "#REF",
                "as",
                "a",
                "source",
                "of",
                "attributive",
                "adjective-noun",
                "pairs.",
                "I",
                "extract",
                "all",
                "pairs",
                "of",
                "words",
                "linked",
                "by",
                "a",
                "dependency",
                "of",
                "type",
                "amod",
                "where",
                "the",
                "head",
                "has",
                "universal",
                "part-of-speech",
                "(UPOS)",
                "NOUN",
                "and",
                "the",
                "dependent",
                "has",
                "UPOS",
                "ADJ.",
                "I",
                "represent",
                "the",
                "pair",
                "using",
                "the",
                "downcased",
                "wordforms",
                "of",
                "the",
                "adjective",
                "and",
                "noun."
            ],
            "context": [
                0,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                1,
                0,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Data I use Universal Dependencies (UD) 2.8 2 #TARGET_REF and the automatically-parsed Wikipedia datasets released as part of the CoNLL 2017 Shared Task #REF as a source of attributive adjective-noun pairs. I extract all pairs of words linked by a dependency of type amod where the head has universal part-of-speech (UPOS) NOUN and the dependent has UPOS ADJ. I represent the pair using the downcased wordforms of the adjective and noun.",
        "output": "Data <PERCEPT> I use </PERCEPT> <INFO> Universal Dependencies (UD) 2.8 2 #TARGET_REF </INFO> and <BACK> the automatically-parsed Wikipedia datasets </BACK> released as part of the CoNLL 2017 Shared Task #REF <PERCEPT> as a source of attributive adjective-noun pairs. I extract all pairs of words linked by a dependency of type amod where the head has universal part-of-speech (UPOS) NOUN and the dependent has UPOS ADJ. </PERCEPT> I represent the pair using the downcased wordforms of the adjective and noun."
    },
    {
        "gold": {
            "text": [
                "Training",
                "Following",
                "#TARGET_REF",
                ",",
                "the",
                "loss",
                "is",
                "a",
                "sum",
                "of",
                "binary",
                "cross-entropy",
                "losses",
                "over",
                "all",
                "entity",
                "types",
                "T",
                "over",
                "all",
                "training",
                "examples",
                "D.",
                "That",
                "is,",
                "we",
                "treat",
                "each",
                "type",
                "prediction",
                "for",
                "each",
                "example",
                "as",
                "an",
                "independent",
                "binary",
                "decision,",
                "with",
                "shared",
                "parameters",
                "in",
                "the",
                "BERT",
                "encoder."
            ],
            "context": [
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Training Following #TARGET_REF , the loss is a sum of binary cross-entropy losses over all entity types T over all training examples D. That is, we treat each type prediction for each example as an independent binary decision, with shared parameters in the BERT encoder.",
        "output": "<INFO> Training Following #TARGET_REF , the loss is a sum of binary cross-entropy losses over all entity types T over all training examples D. </INFO> That is, we treat each type prediction for each example as an independent binary decision, with shared parameters in the BERT encoder."
    },
    {
        "gold": {
            "text": [
                "Among",
                "the",
                "similarity",
                "measures",
                "of",
                "#TARGET_REF",
                ",",
                "generally",
                "FastText",
                "seems",
                "to",
                "be",
                "the",
                "best",
                "model.",
                "So,",
                "following",
                "the",
                "cited",
                "work,",
                "we",
                "create",
                "a",
                "relatedness",
                "matrix",
                "based",
                "on",
                "the",
                "cosine",
                "similarity",
                "of",
                "FastText",
                "vectors.",
                "That",
                "is,",
                "if",
                "v",
                "i",
                ",",
                "v",
                "j",
                "are",
                "vectors",
                "corresponding",
                "to",
                "words",
                "w",
                "i",
                ",",
                "w",
                "j",
                ",",
                "thens",
                "F",
                "(w",
                "i",
                ",",
                "w",
                "j",
                ")",
                "=",
                "cos(v",
                "i",
                ",",
                "v",
                "j",
                ").For",
                "comparability",
                "with",
                "the",
                "other",
                "methods,",
                "we",
                "train",
                "our",
                "FastText",
                "models",
                "on",
                "the",
                "above",
                "corpora",
                "for",
                "English",
                "and",
                "Hungarian",
                "in",
                "300",
                "dimensions,",
                "using",
                "window",
                "size",
                "10."
            ],
            "context": [
                2,
                1,
                1,
                1,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2
            ]
        },
        "input": "Among the similarity measures of #TARGET_REF , generally FastText seems to be the best model. So, following the cited work, we create a relatedness matrix based on the cosine similarity of FastText vectors. That is, if v i , v j are vectors corresponding to words w i , w j , thens F (w i , w j ) = cos(v i , v j ).For comparability with the other methods, we train our FastText models on the above corpora for English and Hungarian in 300 dimensions, using window size 10.",
        "output": "<PERCEPT> Among </PERCEPT> <INFO> the similarity measures of #TARGET_REF , </INFO> <PERCEPT> generally FastText seems to be the best model. </PERCEPT> So, <PERCEPT> following the cited work, we create a relatedness matrix based on the cosine similarity of FastText vectors. </PERCEPT> That <PERCEPT> is, if v i , v j are vectors corresponding to words w i , w j , thens F (w i , w j ) = cos(v i , v j ).For comparability with the other methods, we train our FastText models on the above corpora for English and Hungarian in 300 dimensions, using window size 10. </PERCEPT>"
    },
    {
        "gold": {
            "text": [
                "Adopting",
                "the",
                "idiom",
                "principle",
                "#REF",
                "to",
                "produce",
                "a",
                "single",
                "token",
                "representation",
                "for",
                "MWEs",
                "has",
                "been",
                "used",
                "widely",
                "within",
                "static",
                "embedding",
                "distributional",
                "semantic",
                "models",
                "#TARGET_REF",
                ".",
                "Within",
                "contextualised",
                "representation",
                "models,",
                "#REF",
                "show",
                "that",
                "the",
                "contextualised",
                "representations",
                "produced",
                "by",
                "context2vec",
                "#REF",
                "and",
                "BERT",
                "#REF",
                "models",
                "can",
                "be",
                "used",
                "to",
                "differentiate",
                "between",
                "idiomatic",
                "and",
                "literal",
                "uses",
                "of",
                "MWEs.",
                "However,",
                "the",
                "MWEs",
                "are",
                "only",
                "represented",
                "by",
                "one",
                "token",
                "in",
                "the",
                "input,",
                "before",
                "being",
                "broken",
                "into",
                "many",
                "tokens",
                "using",
                "BERTs",
                "word",
                "piece",
                "tokenizer.",
                "Tayyar",
                "Madabushi",
                "et",
                "al.,",
                "2021",
                "add",
                "a",
                "token",
                "to",
                "the",
                "BERT",
                "embedding",
                "matrix",
                "and",
                "shows",
                "that",
                "this",
                "method",
                "improves",
                "representations",
                "through",
                "increased",
                "performance",
                "on",
                "their",
                "proposed",
                "STS",
                "task.",
                "The",
                "embeddings",
                "they",
                "add",
                "to",
                "BERT",
                "are",
                "randomly",
                "initialised,",
                "however,",
                "and",
                "only",
                "trained",
                "during",
                "the",
                "fine-tun",
                "step",
                "on",
                "limited",
                "data.",
                "Form",
                "embeddings",
                "are",
                "then",
                "learnt",
                "using",
                "trained",
                "ngram",
                "character",
                "embeddings,",
                "before",
                "being",
                "passed",
                "with",
                "a",
                "context",
                "into",
                "a",
                "BERT",
                "model.",
                "The",
                "output",
                "of",
                "the",
                "BERT",
                "model",
                "forms",
                "the",
                "embedding",
                "for",
                "that",
                "specific",
                "context.",
                "To",
                "incorporate",
                "knowledge",
                "from",
                "many",
                "contexts",
                "an",
                "attention",
                "layer",
                "is",
                "applied",
                "over",
                "the",
                "outputs",
                "for",
                "each",
                "context",
                "to",
                "get",
                "the",
                "final",
                "embedding.",
                "There",
                "exist",
                "other",
                "models",
                "to",
                "produce",
                "effective",
                "embeddings",
                "from",
                "a",
                "small",
                "number",
                "of",
                "contexts",
                "#REF",
                ",",
                "however,",
                "BERTRAM",
                "is",
                "the",
                "only",
                "model",
                "that",
                "is",
                "non-bag-ofwords",
                "and",
                "incorporates",
                "both",
                "form",
                "and",
                "context",
                "information",
                "when",
                "creating",
                "the",
                "embedding."
            ],
            "context": [
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Adopting the idiom principle #REF to produce a single token representation for MWEs has been used widely within static embedding distributional semantic models #TARGET_REF . Within contextualised representation models, #REF show that the contextualised representations produced by context2vec #REF and BERT #REF models can be used to differentiate between idiomatic and literal uses of MWEs. However, the MWEs are only represented by one token in the input, before being broken into many tokens using BERTs word piece tokenizer. Tayyar Madabushi et al., 2021 add a token to the BERT embedding matrix and shows that this method improves representations through increased performance on their proposed STS task. The embeddings they add to BERT are randomly initialised, however, and only trained during the fine-tun step on limited data. Form embeddings are then learnt using trained ngram character embeddings, before being passed with a context into a BERT model. The output of the BERT model forms the embedding for that specific context. To incorporate knowledge from many contexts an attention layer is applied over the outputs for each context to get the final embedding. There exist other models to produce effective embeddings from a small number of contexts #REF , however, BERTRAM is the only model that is non-bag-ofwords and incorporates both form and context information when creating the embedding.",
        "output": "<INFO> Adopting the idiom principle #REF to produce a single token representation for MWEs has been used widely within static embedding distributional semantic models #TARGET_REF . </INFO> Within contextualised representation models, #REF show that the contextualised representations produced by context2vec #REF and BERT #REF models can be used to differentiate between idiomatic and literal uses of MWEs. However, the MWEs are only represented by one token in the input, before being broken into many tokens using BERTs word piece tokenizer. Tayyar Madabushi et al., 2021 add a token to the BERT embedding matrix and shows that this method improves representations through increased performance on their proposed STS task. The embeddings they add to BERT are randomly initialised, however, and only trained during the fine-tun step on limited data. Form embeddings are then learnt using trained ngram character embeddings, before being passed with a context into a BERT model. The output of the BERT model forms the embedding for that specific context. To incorporate knowledge from many contexts an attention layer is applied over the outputs for each context to get the final embedding. There exist other models to produce effective embeddings from a small number of contexts #REF , however, BERTRAM is the only model that is non-bag-ofwords and incorporates both form and context information when creating the embedding."
    },
    {
        "gold": {
            "text": [
                "As",
                "shown",
                "in",
                "previous",
                "work",
                "#TARGET_REF",
                ",",
                "increasing",
                "the",
                "depth",
                "of",
                "the",
                "Transformer",
                "encoder",
                "can",
                "substantially",
                "improve",
                "model",
                "performance,",
                "therefore",
                "we",
                "train",
                "the",
                "Transformer",
                "with",
                "deep",
                "encoder",
                "to",
                "obtain",
                "a",
                "better",
                "source",
                "representation."
            ],
            "context": [
                1,
                1,
                1,
                1,
                1,
                1,
                0,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1
            ]
        },
        "input": "As shown in previous work #TARGET_REF , increasing the depth of the Transformer encoder can substantially improve model performance, therefore we train the Transformer with deep encoder to obtain a better source representation.",
        "output": "<INFO> As shown in previous work #TARGET_REF </INFO> , <INFO> increasing the depth of the Transformer encoder can substantially improve model performance, therefore we train the Transformer with deep encoder to obtain a better source representation. </INFO>"
    },
    {
        "gold": {
            "text": [
                "In",
                "this",
                "work,",
                "we",
                "explore",
                "a",
                "set",
                "of",
                "interpretable",
                "entity",
                "representations",
                "that",
                "are",
                "simultaneously",
                "human",
                "and",
                "machine",
                "readable.",
                "The",
                "key",
                "idea",
                "of",
                "this",
                "approach",
                "is",
                "to",
                "use",
                "fine-grained",
                "entity",
                "typing",
                "models",
                "with",
                "large",
                "type",
                "inventories",
                "#REF",
                ".",
                "Given",
                "an",
                "entity",
                "mention",
                "and",
                "context",
                "words,",
                "our",
                "typing",
                "model",
                "outputs",
                "a",
                "highdimensional",
                "vector",
                "whose",
                "values",
                "are",
                "associated",
                "with",
                "predefined",
                "fine-grained",
                "entity",
                "types.",
                "Each",
                "value",
                "ranges",
                "between",
                "0",
                "and",
                "1,",
                "corresponding",
                "to",
                "the",
                "confidence",
                "of",
                "the",
                "model's",
                "decision",
                "that",
                "the",
                "entity",
                "has",
                "the",
                "property",
                "given",
                "by",
                "the",
                "corresponding",
                "type.",
                "We",
                "use",
                "pre-trained",
                "Transformer-based",
                "entity",
                "typing",
                "models,",
                "trained",
                "either",
                "on",
                "a",
                "supervised",
                "entity",
                "typing",
                "dataset",
                "#TARGET_REF",
                "or",
                "on",
                "a",
                "distantlysupervised",
                "dataset",
                "derived",
                "from",
                "Wikipedia",
                "categories",
                "#REF",
                ".",
                "The",
                "type",
                "vectors",
                "from",
                "these",
                "models,",
                "which",
                "contain",
                "tens",
                "of",
                "thousands",
                "of",
                "types,",
                "are",
                "then",
                "used",
                "as",
                "contextualized",
                "entity",
                "embeddings",
                "in",
                "downstream",
                "tasks."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "In this work, we explore a set of interpretable entity representations that are simultaneously human and machine readable. The key idea of this approach is to use fine-grained entity typing models with large type inventories #REF . Given an entity mention and context words, our typing model outputs a highdimensional vector whose values are associated with predefined fine-grained entity types. Each value ranges between 0 and 1, corresponding to the confidence of the model's decision that the entity has the property given by the corresponding type. We use pre-trained Transformer-based entity typing models, trained either on a supervised entity typing dataset #TARGET_REF or on a distantlysupervised dataset derived from Wikipedia categories #REF . The type vectors from these models, which contain tens of thousands of types, are then used as contextualized entity embeddings in downstream tasks.",
        "output": "In this work, we explore a set of interpretable entity representations that are simultaneously human and machine readable. The key idea of this approach is to use fine-grained entity typing models with large type inventories #REF . Given an entity mention and context words, our typing model outputs a highdimensional vector whose values are associated with predefined fine-grained entity types. Each value ranges between 0 and 1, corresponding to the confidence of the model's decision that the entity has the property given by the corresponding type. <PERCEPT> We use pre-trained Transformer-based entity typing models, </PERCEPT> <INFO> trained either on a supervised entity typing dataset #TARGET_REF </INFO> <BACK> or on a distantlysupervised dataset derived from Wikipedia categories #REF . </BACK> The type vectors from these models, which contain tens of thousands of types, are then used as contextualized entity embeddings in downstream tasks."
    },
    {
        "gold": {
            "text": [
                "During",
                "the",
                "fine-tuning",
                "phase,",
                "we",
                "first",
                "apply",
                "finetuning",
                "on",
                "a",
                "small",
                "spoken",
                "corpus.",
                "For",
                "better",
                "domain",
                "adaptation,",
                "we",
                "adopt",
                "mixed",
                "fine-tuning",
                "#REF",
                ",",
                "which",
                "trains",
                "on",
                "a",
                "mixed",
                "dataset",
                "that",
                "includes",
                "a",
                "subsampled",
                "general",
                "corpus",
                "and",
                "an",
                "upsampled",
                "spoken",
                "corpus.",
                "Thirdly,",
                "we",
                "propose",
                "a",
                "method",
                "called",
                "\"in-domain",
                "mixed",
                "fine-tuning\",",
                "which",
                "further",
                "improve",
                "the",
                "BLEU",
                "score",
                "than",
                "mixed",
                "finetuning.",
                "Specifically,",
                "inspired",
                "by",
                "in-domain",
                "data",
                "filtering",
                "#TARGET_REF",
                ",",
                "we",
                "mixed",
                "upsampled",
                "spoken",
                "data",
                "with",
                "selected",
                "in-domain",
                "data",
                "from",
                "general",
                "corpus",
                "rather",
                "than",
                "random",
                "subsampled."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                2,
                2,
                1,
                1,
                1,
                1,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2
            ]
        },
        "input": "During the fine-tuning phase, we first apply finetuning on a small spoken corpus. For better domain adaptation, we adopt mixed fine-tuning #REF , which trains on a mixed dataset that includes a subsampled general corpus and an upsampled spoken corpus. Thirdly, we propose a method called \"in-domain mixed fine-tuning\", which further improve the BLEU score than mixed finetuning. Specifically, inspired by in-domain data filtering #TARGET_REF , we mixed upsampled spoken data with selected in-domain data from general corpus rather than random subsampled.",
        "output": "During the fine-tuning phase, we first apply finetuning on a small spoken corpus. For better domain adaptation, we adopt mixed fine-tuning #REF , which trains on a mixed dataset that includes a subsampled general corpus and an upsampled spoken corpus. Thirdly, <BACK> we propose a method called \"in-domain mixed fine-tuning\", which further improve the BLEU score than mixed finetuning. </BACK> Specifically, <PERCEPT> inspired by </PERCEPT> <INFO> in-domain data filtering #TARGET_REF </INFO> , <PERCEPT> we mixed upsampled spoken data with selected in-domain data from general corpus rather than random subsampled. </PERCEPT>"
    },
    {
        "gold": {
            "text": [
                "Reasoning",
                "based",
                "on",
                "causal",
                "relation",
                "between",
                "events",
                "is",
                "used",
                "in",
                "two",
                "types",
                "of",
                "argumentation:",
                "argument",
                "from",
                "cause",
                "to",
                "effect",
                "and",
                "argument",
                "from",
                "effect",
                "to",
                "cause",
                "#TARGET_REF",
                "Effect-to-cause",
                "(E2C)",
                "reasoning",
                "has",
                "the",
                "reversed",
                "direction,",
                "S",
                "describes",
                "an",
                "observation",
                "and",
                "C",
                "is",
                "a",
                "reasonable",
                "explanation",
                "that",
                "may",
                "have",
                "caused",
                "it.",
                "If",
                "C",
                "causes",
                "(obstructs)",
                "S,",
                "then",
                "S",
                "is",
                "likely",
                "to",
                "support",
                "(attack)",
                "C,",
                "as",
                "in:",
                "The",
                "probabilities",
                "are",
                "computed",
                "by",
                "a",
                "causality",
                "module",
                "(",
                "§4.3).Claim:",
                "St.",
                "Andrew"
            ],
            "context": [
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Reasoning based on causal relation between events is used in two types of argumentation: argument from cause to effect and argument from effect to cause #TARGET_REF Effect-to-cause (E2C) reasoning has the reversed direction, S describes an observation and C is a reasonable explanation that may have caused it. If C causes (obstructs) S, then S is likely to support (attack) C, as in: The probabilities are computed by a causality module ( §4.3).Claim: St. Andrew",
        "output": "<PERCEPT> Reasoning based on causal relation between events is used </PERCEPT> <INFO> in two types of argumentation: argument from cause to effect and argument from effect to cause #TARGET_REF </INFO> Effect-to-cause (E2C) reasoning has the reversed direction, S describes an observation and C is a reasonable explanation that may have caused it. If C causes (obstructs) S, then S is likely to support (attack) C, as in: The probabilities are computed by a causality module ( §4.3).Claim: St. Andrew"
    },
    {
        "gold": {
            "text": [
                "To",
                "date,",
                "several",
                "approaches",
                "to",
                "automated",
                "detection",
                "of",
                "abusive",
                "language",
                "have",
                "been",
                "proposed,",
                "including",
                "rule-based",
                "#REF",
                ",",
                "linguistic",
                "and",
                "social",
                "feature",
                "engineering",
                "#REF",
                ",",
                "utilizing",
                "distributed",
                "representations",
                "from",
                "neural",
                "networks",
                "#REF",
                "or",
                "applying",
                "deep",
                "neural",
                "networks",
                "directly",
                "#REF",
                ".",
                "Researchers",
                "have",
                "also",
                "explored",
                "multi-task",
                "learning",
                "settings",
                "with",
                "objectives",
                "such",
                "as",
                "emotion",
                "detection",
                "#REF",
                ".",
                "We",
                "refer",
                "the",
                "reader",
                "to",
                "recent",
                "surveys",
                "of",
                "the",
                "field",
                "#TARGET_REF",
                "for",
                "a",
                "detailed",
                "literature",
                "review."
            ],
            "context": [
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1
            ]
        },
        "input": "To date, several approaches to automated detection of abusive language have been proposed, including rule-based #REF , linguistic and social feature engineering #REF , utilizing distributed representations from neural networks #REF or applying deep neural networks directly #REF . Researchers have also explored multi-task learning settings with objectives such as emotion detection #REF . We refer the reader to recent surveys of the field #TARGET_REF for a detailed literature review.",
        "output": "<BACK> To date, several approaches to automated detection of abusive language have been proposed, </BACK> including rule-based #REF , linguistic and social feature engineering #REF , utilizing distributed representations from neural networks #REF or applying deep neural networks directly #REF . Researchers have also explored multi-task learning settings with objectives such as emotion detection #REF . <PERCEPT> We refer the reader to </PERCEPT> <INFO> recent surveys of the field #TARGET_REF for a detailed literature review. </INFO>"
    },
    {
        "gold": {
            "text": [
                "To",
                "apply",
                "RDS",
                "#TARGET_REF",
                "for",
                "the",
                "data",
                "splitting",
                "process,",
                "it",
                "requires",
                "to",
                "have",
                "baseline",
                "learners",
                "to",
                "obtain",
                "rewards",
                "for",
                "the",
                "reinforced",
                "process.",
                "It",
                "is",
                "recommended",
                "to",
                "choose",
                "representative",
                "baseline",
                "learners,",
                "to",
                "let",
                "the",
                "reinforced",
                "learner",
                "better",
                "capture",
                "different",
                "learning",
                "behaviors.",
                "The",
                "use",
                "of",
                "these",
                "baseline",
                "learners",
                "is",
                "important",
                "since",
                "each",
                "learner",
                "will",
                "behave",
                "differently",
                "depending",
                "on",
                "the",
                "patterns",
                "contained",
                "in",
                "the",
                "target",
                "data.",
                "As",
                "a",
                "result,",
                "RDS",
                "helps",
                "to",
                "increase",
                "the",
                "diversity",
                "of",
                "the",
                "data",
                "samples",
                "in",
                "different",
                "sets.",
                "Here",
                "we",
                "employ",
                "three",
                "models",
                "to",
                "classify",
                "reliable",
                "news",
                "using",
                "textual",
                "features",
                "as",
                "follows:",
                "Bi-LSTM",
                "network",
                "is",
                "a",
                "standard",
                "baseline",
                "for",
                "most",
                "of",
                "text",
                "classification",
                "tasks."
            ],
            "context": [
                2,
                2,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "To apply RDS #TARGET_REF for the data splitting process, it requires to have baseline learners to obtain rewards for the reinforced process. It is recommended to choose representative baseline learners, to let the reinforced learner better capture different learning behaviors. The use of these baseline learners is important since each learner will behave differently depending on the patterns contained in the target data. As a result, RDS helps to increase the diversity of the data samples in different sets. Here we employ three models to classify reliable news using textual features as follows: Bi-LSTM network is a standard baseline for most of text classification tasks.",
        "output": "<PERCEPT> To apply </PERCEPT> <INFO> RDS #TARGET_REF </INFO> <PERCEPT> for the data splitting process, it requires to have baseline learners to obtain rewards for the reinforced process. </PERCEPT> <BACK> It is recommended to choose representative baseline learners, to let the reinforced learner better capture different learning behaviors. </BACK> The use of these baseline learners is important since each learner will behave differently depending on the patterns contained in the target data. As a result, <PERCEPT> RDS helps to increase the diversity of the data samples in different sets. </PERCEPT> Here we employ three models to classify reliable news using textual features as follows: Bi-LSTM network is a standard baseline for most of text classification tasks."
    },
    {
        "gold": {
            "text": [
                "the",
                "bronze",
                "and",
                "silver",
                "data,",
                "then",
                "\"fine-tuning\"",
                "on",
                "gold",
                "data.",
                "Since",
                "we",
                "are",
                "using",
                "a",
                "Transformer",
                "model",
                "like",
                "them,",
                "we",
                "expected",
                "this",
                "technique",
                "could",
                "also",
                "boost",
                "our",
                "parser's",
                "performance.",
                "Thus,",
                "we",
                "tested",
                "our",
                "model",
                "with",
                "5",
                "epochs",
                "training",
                "on",
                "silver",
                "and",
                "bronze",
                "followed",
                "by",
                "5",
                "epochs",
                "on",
                "gold.",
                "The",
                "results",
                "are",
                "shown",
                "in",
                "Table",
                "5.",
                "They",
                "confirm",
                "that",
                "more",
                "data",
                "means",
                "better",
                "results",
                "even",
                "when",
                "the",
                "data",
                "is",
                "not",
                "perfect.",
                "Although",
                "the",
                "bigger",
                "training",
                "set",
                "also",
                "increases",
                "the",
                "number",
                "of",
                "classes",
                "for",
                "all",
                "three",
                "labels",
                "more",
                "than",
                "10-fold,",
                "the",
                "model",
                "seems",
                "to",
                "handle",
                "it",
                "just",
                "fine.",
                "The",
                "only",
                "downside",
                "is",
                "the",
                "longer",
                "training",
                "time:",
                "as",
                "the",
                "silver",
                "and",
                "bronze",
                "sets",
                "for",
                "English",
                "are,",
                "respectively,",
                "21",
                "and",
                "25",
                "times",
                "larger",
                "than",
                "the",
                "gold",
                "one,",
                "the",
                "time",
                "consumption",
                "jumps",
                "from",
                "a",
                "few",
                "minutes",
                "to",
                "more",
                "than",
                "10",
                "hours.",
                "2019)",
                "with",
                "the",
                "addition",
                "of",
                "BERT",
                "embeddings,",
                "and",
                "their",
                "\"best\"",
                "model",
                "encodes",
                "the",
                "character",
                "embedding",
                "and",
                "the",
                "BERT",
                "embedding",
                "separately",
                "before",
                "feeding",
                "their",
                "concatenated",
                "vector",
                "into",
                "the",
                "decoder,",
                "which",
                "achieved",
                "state-of-the-art",
                "results.",
                "Worth",
                "noting",
                "is",
                "their",
                "claim",
                "that",
                "it's",
                "best",
                "to",
                "keep",
                "BERT",
                "parameters",
                "\"frozen\",",
                "which",
                "we",
                "did",
                "not",
                "find",
                "to",
                "be",
                "the",
                "case",
                "for",
                "our",
                "model:",
                "in",
                "preliminary",
                "experimentation,",
                "finetuning",
                "BERT",
                "parameters",
                "with",
                "our",
                "model",
                "outperformed",
                "a",
                "corresponding",
                "frozen",
                "model",
                "by",
                "20%",
                "in",
                "Counter",
                "f-score."
            ],
            "context": [
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "the bronze and silver data, then \"fine-tuning\" on gold data. Since we are using a Transformer model like them, we expected this technique could also boost our parser's performance. Thus, we tested our model with 5 epochs training on silver and bronze followed by 5 epochs on gold. The results are shown in Table 5. They confirm that more data means better results even when the data is not perfect. Although the bigger training set also increases the number of classes for all three labels more than 10-fold, the model seems to handle it just fine. The only downside is the longer training time: as the silver and bronze sets for English are, respectively, 21 and 25 times larger than the gold one, the time consumption jumps from a few minutes to more than 10 hours. 2019) with the addition of BERT embeddings, and their \"best\" model encodes the character embedding and the BERT embedding separately before feeding their concatenated vector into the decoder, which achieved state-of-the-art results. Worth noting is their claim that it's best to keep BERT parameters \"frozen\", which we did not find to be the case for our model: in preliminary experimentation, finetuning BERT parameters with our model outperformed a corresponding frozen model by 20% in Counter f-score.",
        "output": "<INFO> the bronze and silver data, then \"fine-tuning\" on gold data. </INFO> <PERCEPT> Since we are using a Transformer model like them, we expected this technique could also boost our parser's performance. </PERCEPT> Thus, we tested our model with 5 epochs training on silver and bronze followed by 5 epochs on gold. The results are shown in Table 5. They confirm that more data means better results even when the data is not perfect. Although the bigger training set also increases the number of classes for all three labels more than 10-fold, the model seems to handle it just fine. The only downside is the longer training time: as the silver and bronze sets for English are, respectively, 21 and 25 times larger than the gold one, the time consumption jumps from a few minutes to more than 10 hours. 2019) with the addition of BERT embeddings, and their \"best\" model encodes the character embedding and the BERT embedding separately before feeding their concatenated vector into the decoder, which achieved state-of-the-art results. Worth noting is their claim that it's best to keep BERT parameters \"frozen\", which we did not find to be the case for our model: in preliminary experimentation, finetuning BERT parameters with our model outperformed a corresponding frozen model by 20% in Counter f-score."
    },
    {
        "gold": {
            "text": [
                "The",
                "subject",
                "NP",
                "'Bill'",
                "is",
                "coindexed",
                "with",
                "the",
                "trace",
                "in",
                "the",
                "more",
                "deeply",
                "embedded",
                "relative",
                "clause.",
                "If",
                "we",
                "assume,",
                "following",
                "#TARGET_REF",
                ",",
                "that",
                "relative",
                "clause",
                "formation",
                "involves",
                "movement",
                "from",
                "an",
                "inner",
                "clause",
                "into",
                "an",
                "outer",
                "subject",
                "position,",
                "then",
                "the",
                "grammaticality",
                "of",
                "the",
                "above",
                "example",
                "suggests",
                "that",
                "the",
                "Trace",
                "theory",
                "must",
                "be",
                "parameterized",
                "so",
                "that",
                "crossing",
                "more",
                "than",
                "one",
                "barrier",
                "is",
                "allowed",
                "in",
                "Korean.",
                "Our",
                "formulation",
                "of",
                "this",
                "parametric",
                "distinction",
                "is",
                "as",
                "follows:"
            ],
            "context": [
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                2,
                2,
                2,
                2,
                0,
                0,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The subject NP 'Bill' is coindexed with the trace in the more deeply embedded relative clause. If we assume, following #TARGET_REF , that relative clause formation involves movement from an inner clause into an outer subject position, then the grammaticality of the above example suggests that the Trace theory must be parameterized so that crossing more than one barrier is allowed in Korean. Our formulation of this parametric distinction is as follows:",
        "output": "<BACK> The subject NP 'Bill' is coindexed with the trace in the more deeply embedded relative clause. </BACK> <PERCEPT> If we assume, following </PERCEPT> #TARGET_REF , <INFO> that relative clause formation involves movement from an inner clause into an outer subject position, </INFO> <PERCEPT> then the grammaticality of the above example suggests that the Trace theory must be parameterized so that crossing more than one barrier is allowed in Korean. </PERCEPT> Our formulation of this parametric distinction is as follows:"
    },
    {
        "gold": {
            "text": [
                "Wiggins",
                "(",
                "2013)",
                "gives",
                "yet",
                "another",
                "perspective",
                "of",
                "investment",
                "and",
                "MT",
                "implementation",
                "in",
                "Asia",
                "Online",
                "3",
                "business",
                "environment.",
                "Some",
                "of",
                "the",
                "advantages",
                "of",
                "providing",
                "MT-incorporated",
                "solutions",
                "#TARGET_REF",
                "list",
                "are",
                "reduced",
                "translation",
                "costs,",
                "faster",
                "delivery",
                "time,",
                "expansion",
                "of",
                "existing",
                "relationships",
                "with",
                "clients,",
                "broadening",
                "offered",
                "functionality",
                "and",
                "opening",
                "new",
                "market",
                "possibilities."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1
            ]
        },
        "input": "Wiggins ( 2013) gives yet another perspective of investment and MT implementation in Asia Online 3 business environment. Some of the advantages of providing MT-incorporated solutions #TARGET_REF list are reduced translation costs, faster delivery time, expansion of existing relationships with clients, broadening offered functionality and opening new market possibilities.",
        "output": "Wiggins ( 2013) gives yet another perspective of investment and MT implementation in Asia Online 3 business environment. <INFO> Some of the advantages of providing MT-incorporated solutions #TARGET_REF list are reduced translation costs, faster delivery time, expansion of existing relationships with clients, broadening offered functionality and opening new market possibilities. </INFO>"
    },
    {
        "gold": {
            "text": [
                "Here,",
                "we",
                "summarize",
                "the",
                "pre-training",
                "tasks",
                "for",
                "the",
                "encoders",
                "mentioned",
                "in",
                "Section",
                "4.2.",
                "These",
                "tasks",
                "are",
                "unchanged",
                "from",
                "those",
                "described",
                "in",
                "#TARGET_REF",
                "."
            ],
            "context": [
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                1,
                1
            ]
        },
        "input": "Here, we summarize the pre-training tasks for the encoders mentioned in Section 4.2. These tasks are unchanged from those described in #TARGET_REF .",
        "output": "Here, <PERCEPT> we summarize the pre-training tasks for the encoders </PERCEPT> <BACK> mentioned in Section 4.2. These tasks are unchanged from those described in </BACK> <INFO> #TARGET_REF . </INFO>"
    },
    {
        "gold": {
            "text": [
                "There",
                "remain,",
                "too,",
                "crucial",
                "classes",
                "of",
                "cases",
                "that",
                "seem",
                "to",
                "need",
                "symbolic",
                "inference:",
                "an",
                "old,",
                "self-serving,",
                "one",
                "will",
                "do",
                "such",
                "as",
                "\"The",
                "soldiers",
                "fired",
                "at",
                "the",
                "women",
                "and",
                "I",
                "saw",
                "several",
                "fall\"",
                "#TARGET_REF",
                "."
            ],
            "context": [
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1
            ]
        },
        "input": "There remain, too, crucial classes of cases that seem to need symbolic inference: an old, self-serving, one will do such as \"The soldiers fired at the women and I saw several fall\" #TARGET_REF .",
        "output": "<BACK> There remain, too, crucial classes of cases that seem to need symbolic inference: </BACK> <INFO> an old, self-serving, one will do such as \"The soldiers fired at the women and I saw several fall\" #TARGET_REF . </INFO>"
    },
    {
        "gold": {
            "text": [
                "was",
                "almost",
                "absent",
                "from",
                "the",
                "repertoire",
                "of",
                "Indian",
                "corporates",
                "and",
                "public",
                "figures",
                "#REF",
                ".",
                "Even",
                "written",
                "apologies",
                "were",
                "very",
                "few",
                "and",
                "were",
                "offered",
                "only",
                "when",
                "there",
                "was",
                "a",
                "strong",
                "demand",
                "from",
                "different",
                "sections",
                "of",
                "society.",
                "However,",
                "the",
                "new",
                "generation",
                "e-commerce",
                "companies",
                "seem",
                "to",
                "be",
                "heralding",
                "an",
                "attitudinal",
                "change",
                "in",
                "this",
                "corporate",
                "practice.",
                "This",
                "could",
                "be",
                "due",
                "to",
                "the",
                "increasing",
                "digital",
                "customer",
                "base",
                "for",
                "India",
                "Inc.",
                "India's",
                "internet",
                "user",
                "base",
                "has",
                "grown",
                "to",
                "324.95",
                "million",
                "in",
                "September",
                "2015,",
                "a",
                "27.73%",
                "YOY",
                "growth",
                "(TRAI,",
                "2016).",
                "On",
                "social",
                "media",
                "platforms",
                "situations",
                "can",
                "escalate",
                "rapidly,",
                "breaking",
                "down",
                "the",
                "traditional",
                "barriers",
                "of",
                "time,",
                "location,",
                "and",
                "gatekeepers",
                "of",
                "information",
                "#REF",
                ".",
                "Thus,",
                "in",
                "stark",
                "contrast",
                "to",
                "the",
                "past,",
                "we",
                "see",
                "a",
                "spate",
                "of",
                "apology",
                "e-mails,",
                "tweets",
                "and",
                "blog",
                "posts",
                "being",
                "offered",
                "by",
                "e-commerce",
                "players",
                "(ibid).",
                "Figure",
                "1",
                "shows",
                "the",
                "rising",
                "trend",
                "of",
                "apologies",
                "being",
                "given",
                "publicly",
                "in",
                "the",
                "written",
                "digital",
                "media,",
                "with",
                "a",
                "sharp",
                "increase",
                "from",
                "the",
                "year",
                "2016",
                "to",
                "2017.",
                "Since",
                "the",
                "practice",
                "of",
                "offering",
                "a",
                "public",
                "apology",
                "is",
                "relatively",
                "new",
                "for",
                "Indian",
                "businesses,",
                "it",
                "is",
                "to",
                "be",
                "understood",
                "that",
                "an",
                "apology",
                "not",
                "delivered",
                "effectively",
                "rather",
                "than",
                "mitigating",
                "the",
                "damage,",
                "can",
                "escalate",
                "the",
                "damage",
                "done.",
                "In",
                "this",
                "context,",
                "it",
                "is",
                "important",
                "to",
                "analyze",
                "the",
                "lexical",
                "choice",
                "made",
                "in",
                "these",
                "apologies",
                "and",
                "the",
                "implications",
                "thereof."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "was almost absent from the repertoire of Indian corporates and public figures #REF . Even written apologies were very few and were offered only when there was a strong demand from different sections of society. However, the new generation e-commerce companies seem to be heralding an attitudinal change in this corporate practice. This could be due to the increasing digital customer base for India Inc. India's internet user base has grown to 324.95 million in September 2015, a 27.73% YOY growth (TRAI, 2016). On social media platforms situations can escalate rapidly, breaking down the traditional barriers of time, location, and gatekeepers of information #REF . Thus, in stark contrast to the past, we see a spate of apology e-mails, tweets and blog posts being offered by e-commerce players (ibid). Figure 1 shows the rising trend of apologies being given publicly in the written digital media, with a sharp increase from the year 2016 to 2017. Since the practice of offering a public apology is relatively new for Indian businesses, it is to be understood that an apology not delivered effectively rather than mitigating the damage, can escalate the damage done. In this context, it is important to analyze the lexical choice made in these apologies and the implications thereof.",
        "output": "was almost absent from the repertoire of Indian corporates and public figures #REF . Even written apologies were very few and were offered only when there was a strong demand from different sections of society. However, the new generation e-commerce companies seem to be heralding an attitudinal change in this corporate practice. This could be due to the increasing digital customer base for India Inc. India's internet user base has grown to 324.95 million in September 2015, a 27.73% YOY growth (TRAI, 2016). On social media platforms situations can escalate rapidly, breaking down the traditional barriers of time, location, and gatekeepers of information #REF . Thus, in stark contrast to the past, we see a spate of apology e-mails, tweets and blog posts being offered by e-commerce players (ibid). Figure 1 shows the rising trend of apologies being given publicly in the written digital media, with a sharp increase from the year 2016 to 2017. Since the practice of offering a public apology is relatively new for Indian businesses, it is to be understood that an apology not delivered effectively rather than mitigating the damage, can escalate the damage done. In this context, it is important to analyze the lexical choice made in these apologies and the implications thereof."
    },
    {
        "gold": {
            "text": [
                "BART",
                "movistar",
                "rider",
                "alejandro",
                "valverde",
                "won",
                "fleche",
                "wallonne",
                "on",
                "wednesday.",
                "team",
                "sky's",
                "chris",
                "froome",
                "fell",
                "in",
                "the",
                "final",
                "12km",
                "but",
                "finished",
                "the",
                "race.",
                "philippe",
                "gilbert",
                "pulled",
                "out",
                "of",
                "the",
                "race",
                "after",
                "a",
                "bad",
                "crash",
                "50km",
                "from",
                "the",
                "end.",
                "click",
                "here",
                "for",
                "more",
                "cycling",
                "news.",
                "2021,",
                "#TARGET_REF",
                "has",
                "shown",
                "that",
                "few-shot",
                "learning",
                "can",
                "be",
                "an",
                "effective",
                "fine-tuning",
                "method",
                "of",
                "pre-trained",
                "models",
                "for",
                "text",
                "generation",
                "tasks.",
                "Therefore,",
                "we",
                "investigate",
                "our",
                "model's",
                "performance",
                "in",
                "a",
                "few-shot",
                "setting.",
                "Specifically,",
                "we",
                "randomly",
                "sample",
                "100/1000",
                "examples",
                "from",
                "the",
                "training",
                "set",
                "of",
                "CNNDM/XSum,",
                "and",
                "fine-tune",
                "the",
                "models",
                "that",
                "are",
                "pre-trained",
                "using",
                "MLE",
                "loss",
                "on",
                "those",
                "examples.",
                "More",
                "training",
                "details",
                "can",
                "be",
                "found",
                "in",
                "Appendix",
                "C.",
                "The",
                "results",
                "are",
                "shown",
                "in",
                "Tab.",
                "11.",
                "All",
                "experiments",
                "are",
                "repeated",
                "three",
                "times,",
                "and",
                "the",
                "reported",
                "results",
                "are",
                "the",
                "average",
                "performance.",
                "The",
                "results",
                "indicate",
                "that",
                "our",
                "model",
                "can",
                "achieve",
                "improvement",
                "over",
                "the",
                "baseline",
                "model",
                "under",
                "the",
                "few-shot",
                "learning",
                "setting",
                "with",
                "a",
                "small",
                "computational",
                "overhead."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "BART movistar rider alejandro valverde won fleche wallonne on wednesday. team sky's chris froome fell in the final 12km but finished the race. philippe gilbert pulled out of the race after a bad crash 50km from the end. click here for more cycling news. 2021, #TARGET_REF has shown that few-shot learning can be an effective fine-tuning method of pre-trained models for text generation tasks. Therefore, we investigate our model's performance in a few-shot setting. Specifically, we randomly sample 100/1000 examples from the training set of CNNDM/XSum, and fine-tune the models that are pre-trained using MLE loss on those examples. More training details can be found in Appendix C. The results are shown in Tab. 11. All experiments are repeated three times, and the reported results are the average performance. The results indicate that our model can achieve improvement over the baseline model under the few-shot learning setting with a small computational overhead.",
        "output": "BART movistar rider alejandro valverde won fleche wallonne on wednesday. team sky's chris froome fell in the final 12km but finished the race. philippe gilbert pulled out of the race after a bad crash 50km from the end. click here for more cycling news. 2021, <INFO> #TARGET_REF has shown that few-shot learning can be an effective fine-tuning method of pre-trained models for text generation tasks. </INFO> Therefore, <PERCEPT> we investigate our model's performance in a few-shot setting. </PERCEPT> Specifically, we randomly sample 100/1000 examples from the training set of CNNDM/XSum, and fine-tune the models that are pre-trained using MLE loss on those examples. More training details can be found in Appendix C. The results are shown in Tab. 11. All experiments are repeated three times, and the reported results are the average performance. The results indicate that our model can achieve improvement over the baseline model under the few-shot learning setting with a small computational overhead."
    },
    {
        "gold": {
            "text": [
                "Results.",
                "For",
                "the",
                "SVM,",
                "the",
                "3-1",
                "pretraining",
                "without",
                "data",
                "augmentation",
                "resulted",
                "in",
                "the",
                "highest",
                "dev",
                "set",
                "accuracy",
                "(32.03%),",
                "though",
                "accuracy",
                "was",
                "only",
                "slightly",
                "better",
                "than",
                "for",
                "direct",
                "single-trial",
                "training",
                "(31.93%).",
                "For",
                "the",
                "Transformer,",
                "the",
                "3-1",
                "pretraining",
                "scheme",
                "with",
                "250k",
                "data",
                "augmentation",
                "obtained",
                "the",
                "highest",
                "single-trial",
                "decoding",
                "accuracy",
                "(39.41%)",
                "on",
                "the",
                "dev",
                "set.",
                "Indeed,",
                "Wilcoxon",
                "signed-rank",
                "test",
                "#TARGET_REF",
                "confirmed",
                "that",
                "the",
                "best",
                "dev",
                "set",
                "Transformer",
                "performed",
                "significantly",
                "better",
                "on",
                "the",
                "test",
                "set",
                "after",
                "3-1",
                "pretraining",
                "than",
                "after",
                "direct",
                "single-trial",
                "training",
                "(p",
                "&lt,",
                "0.01)."
            ],
            "context": [
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                1,
                1,
                1,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2
            ]
        },
        "input": "Results. For the SVM, the 3-1 pretraining without data augmentation resulted in the highest dev set accuracy (32.03%), though accuracy was only slightly better than for direct single-trial training (31.93%). For the Transformer, the 3-1 pretraining scheme with 250k data augmentation obtained the highest single-trial decoding accuracy (39.41%) on the dev set. Indeed, Wilcoxon signed-rank test #TARGET_REF confirmed that the best dev set Transformer performed significantly better on the test set after 3-1 pretraining than after direct single-trial training (p &lt, 0.01).",
        "output": "Results. <BACK> For the SVM, the 3-1 pretraining without data augmentation resulted in the highest dev set accuracy (32.03%), though accuracy was only slightly better than for direct single-trial training (31.93%). For the Transformer, the 3-1 pretraining scheme with 250k data augmentation obtained the highest single-trial decoding accuracy (39.41%) on the dev set. </BACK> Indeed, <INFO> Wilcoxon signed-rank test </INFO> #TARGET_REF <PERCEPT> confirmed that the best dev set Transformer performed significantly better on the test set after 3-1 pretraining than after direct single-trial training (p &lt, 0.01). </PERCEPT>"
    },
    {
        "gold": {
            "text": [
                "All",
                "three",
                "subtasks",
                "are",
                "addressed",
                "simultaneously",
                "using",
                "a",
                "Multi-Task",
                "Learning",
                "(MTL)",
                "architecture",
                "#TARGET_REF",
                "that",
                "leverages",
                "acquired",
                "knowledge",
                "from",
                "one",
                "subtask",
                "to",
                "another.",
                "Furthermore,",
                "we",
                "approached",
                "the",
                "challenge",
                "of",
                "unbalanced",
                "classes",
                "in",
                "the",
                "first",
                "subtask",
                "by",
                "considering",
                "class",
                "weights",
                "and",
                "by",
                "augmenting",
                "the",
                "training",
                "data",
                "set."
            ],
            "context": [
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3
            ]
        },
        "input": "All three subtasks are addressed simultaneously using a Multi-Task Learning (MTL) architecture #TARGET_REF that leverages acquired knowledge from one subtask to another. Furthermore, we approached the challenge of unbalanced classes in the first subtask by considering class weights and by augmenting the training data set.",
        "output": "<PERCEPT> All three subtasks are addressed simultaneously using a </PERCEPT> <INFO> Multi-Task Learning (MTL) architecture #TARGET_REF that leverages acquired knowledge from one subtask to another. </INFO> <BACK> Furthermore, we approached the challenge of unbalanced classes in the first subtask by considering class weights and by augmenting the training data set. </BACK>"
    },
    {
        "gold": {
            "text": [
                "Our",
                "main",
                "contribution",
                "is",
                "to",
                "change",
                "the",
                "target",
                "distribution",
                "of",
                "abstractive",
                "models",
                "from",
                "a",
                "one-point",
                "deterministic",
                "distribution",
                "assumed",
                "by",
                "MLE",
                "training",
                "to",
                "a",
                "non-deterministic",
                "distribution",
                "in",
                "which",
                "candidate",
                "summaries",
                "are",
                "also",
                "assigned",
                "probability",
                "mass",
                "according",
                "to",
                "their",
                "quality.",
                "The",
                "new",
                "SOTA",
                "performance",
                "on",
                "#REF",
                "and",
                "XSum",
                "#TARGET_REF",
                "datasets",
                "demonstrated",
                "the",
                "effectiveness",
                "of",
                "our",
                "method.",
                "Our",
                "in-depth",
                "analysis",
                "also",
                "found",
                "that",
                "the",
                "abstractive",
                "models",
                "trained",
                "using",
                "our",
                "method",
                "can",
                "estimate",
                "the",
                "candidate",
                "summary",
                "quality",
                "more",
                "accurately,",
                "in",
                "concert",
                "with",
                "the",
                "the",
                "objective",
                "of",
                "our",
                "training",
                "paradigm."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                2,
                2,
                2,
                2,
                3,
                3,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Our main contribution is to change the target distribution of abstractive models from a one-point deterministic distribution assumed by MLE training to a non-deterministic distribution in which candidate summaries are also assigned probability mass according to their quality. The new SOTA performance on #REF and XSum #TARGET_REF datasets demonstrated the effectiveness of our method. Our in-depth analysis also found that the abstractive models trained using our method can estimate the candidate summary quality more accurately, in concert with the the objective of our training paradigm.",
        "output": "Our main contribution is to change the target distribution of abstractive models from a one-point deterministic distribution assumed by MLE training to a non-deterministic distribution in which candidate summaries are also assigned probability mass according to their quality. <PERCEPT> The new SOTA performance on </PERCEPT> <BACK> #REF and </BACK> <INFO> XSum #TARGET_REF datasets </INFO> <PERCEPT> demonstrated the effectiveness of our method. </PERCEPT> Our in-depth analysis also found that the abstractive models trained using our method can estimate the candidate summary quality more accurately, in concert with the the objective of our training paradigm."
    },
    {
        "gold": {
            "text": [
                "Early",
                "work",
                "in",
                "opinion",
                "question",
                "answering",
                "addressed",
                "separating",
                "facts",
                "from",
                "opinions",
                "#TARGET_REF",
                ",",
                "and",
                "the",
                "authors",
                "used",
                "a",
                "Naïve",
                "Bayes",
                "classifier",
                "to",
                "identify",
                "polarity",
                "of",
                "the",
                "opinions.",
                "#REF",
                "aimed",
                "at",
                "identifying",
                "the",
                "opinion",
                "holder",
                "of",
                "the",
                "opinions.",
                "#REF",
                "explained",
                "the",
                "differences",
                "between",
                "fact",
                "based",
                "and",
                "opinionated",
                "answers",
                "and",
                "how",
                "traditional",
                "QA",
                "systems",
                "will",
                "not",
                "be",
                "able",
                "to",
                "handle",
                "multiple",
                "perspectives",
                "for",
                "answers.",
                "Some",
                "works",
                "aimed",
                "at",
                "using",
                "community",
                "based",
                "question-answers",
                "to",
                "provide",
                "unique",
                "answers",
                "to",
                "questions",
                "#REF",
                ".",
                "#REF",
                "made",
                "use",
                "of",
                "online",
                "reviews",
                "to",
                "answer",
                "questions",
                "on",
                "aspects",
                "of",
                "a",
                "product.",
                "#REF",
                "and",
                "#REF",
                "used",
                "graphs",
                "and",
                "trees",
                "to",
                "answer",
                "opinion",
                "questions.",
                "#REF",
                "modeled",
                "ambiguity",
                "and",
                "subjectivity",
                "in",
                "opinion",
                "QA",
                "using",
                "statistical",
                "models.",
                "#REF",
                "give",
                "baselines",
                "for",
                "answer",
                "generation",
                "systems",
                "given",
                "the",
                "question",
                "and",
                "reviews.",
                "We",
                "use",
                "their",
                "results",
                "as",
                "the",
                "baseline",
                "for",
                "our",
                "evaluation.",
                "We",
                "also",
                "discuss",
                "the",
                "dataset",
                "from",
                "this",
                "paper",
                "in",
                "4.2.",
                "While",
                "most",
                "systems",
                "used",
                "in",
                "the",
                "works",
                "described",
                "above",
                "are",
                "supervised",
                "learning",
                "models,",
                "our",
                "system",
                "used",
                "unsupervised",
                "learning",
                "to",
                "answer",
                "binary",
                "(yes/no)",
                "questions."
            ],
            "context": [
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Early work in opinion question answering addressed separating facts from opinions #TARGET_REF , and the authors used a Naïve Bayes classifier to identify polarity of the opinions. #REF aimed at identifying the opinion holder of the opinions. #REF explained the differences between fact based and opinionated answers and how traditional QA systems will not be able to handle multiple perspectives for answers. Some works aimed at using community based question-answers to provide unique answers to questions #REF . #REF made use of online reviews to answer questions on aspects of a product. #REF and #REF used graphs and trees to answer opinion questions. #REF modeled ambiguity and subjectivity in opinion QA using statistical models. #REF give baselines for answer generation systems given the question and reviews. We use their results as the baseline for our evaluation. We also discuss the dataset from this paper in 4.2. While most systems used in the works described above are supervised learning models, our system used unsupervised learning to answer binary (yes/no) questions.",
        "output": "<INFO> Early work in opinion question answering addressed separating facts from opinions #TARGET_REF , and the authors used a Naïve Bayes classifier to identify polarity of the opinions. </INFO> #REF aimed at identifying the opinion holder of the opinions. #REF explained the differences between fact based and opinionated answers and how traditional QA systems will not be able to handle multiple perspectives for answers. Some works aimed at using community based question-answers to provide unique answers to questions #REF . #REF made use of online reviews to answer questions on aspects of a product. #REF and #REF used graphs and trees to answer opinion questions. #REF modeled ambiguity and subjectivity in opinion QA using statistical models. #REF give baselines for answer generation systems given the question and reviews. We use their results as the baseline for our evaluation. We also discuss the dataset from this paper in 4.2. While most systems used in the works described above are supervised learning models, our system used unsupervised learning to answer binary (yes/no) questions."
    },
    {
        "gold": {
            "text": [
                "Natural",
                "Questions",
                "Test",
                "MRR@10",
                "R@50",
                "R@1000",
                "R@5",
                "R@20",
                "R@100",
                "BM25",
                "(anserini)",
                "#REF",
                "#REF",
                "BERTbase",
                "----78.4",
                "85.4",
                "ANCE",
                "(single)",
                "#REF",
                "RoBERTabase",
                "33.0",
                "-95.9",
                "-81.9",
                "87.5",
                "ME-BERT",
                "#TARGET_REF",
                "BERTlarge",
                "33.8",
                "-----RocketQA",
                "ERNIEbase",
                "37.0",
                "85.5",
                "97.9",
                "74.0",
                "82.7",
                "88.5"
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Natural Questions Test MRR@10 R@50 R@1000 R@5 R@20 R@100 BM25 (anserini) #REF #REF BERTbase ----78.4 85.4 ANCE (single) #REF RoBERTabase 33.0 -95.9 -81.9 87.5 ME-BERT #TARGET_REF BERTlarge 33.8 -----RocketQA ERNIEbase 37.0 85.5 97.9 74.0 82.7 88.5",
        "output": "Natural Questions Test MRR@10 R@50 R@1000 R@5 R@20 R@100 BM25 (anserini) #REF #REF BERTbase ----78.4 85.4 ANCE (single) #REF RoBERTabase 33.0 -95.9 -81.9 87.5 <INFO> ME-BERT #TARGET_REF </INFO> BERTlarge 33.8 -----RocketQA ERNIEbase 37.0 85.5 97.9 74.0 82.7 88.5"
    },
    {
        "gold": {
            "text": [
                "Benchmarks",
                "for",
                "fact",
                "verification",
                "on",
                "structured",
                "evidence",
                "are",
                "built",
                "on",
                "tables",
                "collected",
                "from",
                "Wikipedia",
                "#REF",
                "or",
                "scientific",
                "articles",
                "#REF",
                ".",
                "Many",
                "previous",
                "works",
                "search",
                "latent",
                "programs",
                "as",
                "an",
                "intermediary",
                "to",
                "reason",
                "over",
                "the",
                "given",
                "table.",
                "They",
                "directly",
                "encode",
                "programs",
                "#REF",
                "or",
                "construct",
                "heterogeneous",
                "graphs",
                "#TARGET_REF",
                "with",
                "the",
                "claim,",
                "the",
                "table",
                "and",
                "the",
                "programs.",
                "Another",
                "way",
                "is",
                "to",
                "linearize",
                "the",
                "input",
                "table",
                "and",
                "perform",
                "table",
                "pre-training",
                "#REF",
                "and",
                "add",
                "additional",
                "table-aware",
                "embeddings",
                "#REF",
                "to",
                "enhance",
                "the",
                "table",
                "encoding.",
                "However,",
                "in",
                "these",
                "datasets,",
                "the",
                "evidence",
                "is",
                "only",
                "one",
                "given",
                "table,",
                "and",
                "models",
                "are",
                "not",
                "requested",
                "to",
                "find",
                "out",
                "the",
                "evidence",
                "cells",
                "explicitly."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                2,
                3,
                3,
                3,
                3,
                3,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Benchmarks for fact verification on structured evidence are built on tables collected from Wikipedia #REF or scientific articles #REF . Many previous works search latent programs as an intermediary to reason over the given table. They directly encode programs #REF or construct heterogeneous graphs #TARGET_REF with the claim, the table and the programs. Another way is to linearize the input table and perform table pre-training #REF and add additional table-aware embeddings #REF to enhance the table encoding. However, in these datasets, the evidence is only one given table, and models are not requested to find out the evidence cells explicitly.",
        "output": "Benchmarks for fact verification on structured evidence are built on tables collected from Wikipedia #REF or scientific articles #REF . <BACK> Many previous works search latent programs as an intermediary to reason over the given table. </BACK> <PERCEPT> They </PERCEPT> <BACK> directly encode programs #REF or </BACK> <INFO> construct heterogeneous graphs #TARGET_REF with the claim, the table and the programs. </INFO> Another way is to linearize the input table and perform table pre-training #REF and add additional table-aware embeddings #REF to enhance the table encoding. However, in these datasets, the evidence is only one given table, and models are not requested to find out the evidence cells explicitly."
    },
    {
        "gold": {
            "text": [
                "Multi-Task",
                "Learning",
                "represents",
                "a",
                "training",
                "strategy",
                "where",
                "a",
                "shared",
                "model",
                "is",
                "simultaneously",
                "learning",
                "multiple",
                "tasks.",
                "#REF",
                "analysed",
                "the",
                "techniques",
                "applied",
                "in",
                "MTL",
                "and",
                "compared",
                "the",
                "hard",
                "parameter",
                "sharing",
                "and",
                "soft",
                "parameter",
                "sharing",
                "paradigms,",
                "concluding",
                "that",
                "the",
                "former",
                "is",
                "still",
                "pervasive",
                "in",
                "nowadays",
                "approaches.",
                "MTL",
                "proved",
                "to",
                "fasten",
                "the",
                "convergence",
                "and",
                "to",
                "improve",
                "the",
                "model",
                "performance",
                "in",
                "a",
                "variety",
                "of",
                "NLP",
                "applications,",
                "including",
                "named",
                "entity",
                "recognition",
                "#REF",
                ",",
                "fake",
                "news",
                "detection",
                "#REF",
                ",",
                "multilingual",
                "offensive",
                "language",
                "identification",
                "#REF",
                ",",
                "sentiment",
                "analysis",
                "#REF",
                ",",
                "humor",
                "classification",
                "#REF",
                ",",
                "recommender",
                "systems",
                "#REF",
                ",",
                "and",
                "even",
                "question",
                "answering",
                "#REF",
                ".",
                "MTL",
                "also",
                "increases",
                "performance",
                "in",
                "conjunction",
                "with",
                "semi-supervised",
                "learning",
                "#REF",
                ",",
                "curriculum",
                "learning",
                "#REF",
                ",",
                "sequence-tosequence",
                "#REF",
                ",",
                "reinforcement",
                "learning",
                "#REF",
                ",",
                "and",
                "adversarial",
                "learning",
                "#TARGET_REF",
                "."
            ],
            "context": [
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                1,
                1,
                0,
                0
            ]
        },
        "input": "Multi-Task Learning represents a training strategy where a shared model is simultaneously learning multiple tasks. #REF analysed the techniques applied in MTL and compared the hard parameter sharing and soft parameter sharing paradigms, concluding that the former is still pervasive in nowadays approaches. MTL proved to fasten the convergence and to improve the model performance in a variety of NLP applications, including named entity recognition #REF , fake news detection #REF , multilingual offensive language identification #REF , sentiment analysis #REF , humor classification #REF , recommender systems #REF , and even question answering #REF . MTL also increases performance in conjunction with semi-supervised learning #REF , curriculum learning #REF , sequence-tosequence #REF , reinforcement learning #REF , and adversarial learning #TARGET_REF .",
        "output": "<BACK> Multi-Task Learning represents a training strategy where a shared model is simultaneously learning multiple tasks. </BACK> #REF analysed the techniques applied in MTL and compared the hard parameter sharing and soft parameter sharing paradigms, concluding that the former is still pervasive in nowadays approaches. MTL proved to fasten the convergence and to improve the model performance in a variety of NLP applications, including named entity recognition #REF , fake news detection #REF , multilingual offensive language identification #REF , sentiment analysis #REF , humor classification #REF , recommender systems #REF , and even question answering #REF . <PERCEPT> MTL also increases performance in conjunction with </PERCEPT> <BACK> semi-supervised learning #REF , curriculum learning #REF , sequence-tosequence #REF , reinforcement learning #REF , and </BACK> <INFO> adversarial learning </INFO> #TARGET_REF ."
    },
    {
        "gold": {
            "text": [
                "With",
                "the",
                "advent",
                "of",
                "social",
                "media,",
                "anti-social",
                "and",
                "abusive",
                "behavior",
                "has",
                "become",
                "a",
                "prominent",
                "occurrence",
                "online.",
                "Undesirable",
                "psychological",
                "effects",
                "of",
                "abuse",
                "on",
                "individuals",
                "make",
                "it",
                "an",
                "important",
                "societal",
                "problem",
                "of",
                "our",
                "time.",
                "#TARGET_REF",
                "studied",
                "the",
                "ill-effects",
                "of",
                "online",
                "abuse",
                "on",
                "children,",
                "concluding",
                "that",
                "children",
                "may",
                "develop",
                "depression,",
                "anxiety,",
                "and",
                "other",
                "mental",
                "health",
                "problems",
                "as",
                "a",
                "result",
                "of",
                "their",
                "encounters",
                "online.",
                "Pew",
                "Research",
                "Center,",
                "in",
                "its",
                "latest",
                "report",
                "on",
                "online",
                "harassment",
                "#REF",
                ",",
                "revealed",
                "that",
                "40%",
                "of",
                "adults",
                "in",
                "the",
                "United",
                "States",
                "have",
                "experienced",
                "abusive",
                "behavior",
                "online,",
                "of",
                "which",
                "18%",
                "have",
                "faced",
                "severe",
                "forms",
                "of",
                "harassment,",
                "e.g.,",
                "that",
                "of",
                "sexual",
                "nature.",
                "These",
                "statistics",
                "stress",
                "the",
                "need",
                "for",
                "automated",
                "detection",
                "and",
                "moderation",
                "systems.",
                "Hence,",
                "in",
                "recent",
                "years,",
                "a",
                "new",
                "research",
                "effort",
                "on",
                "abusive",
                "language",
                "detection",
                "has",
                "sprung",
                "up",
                "in",
                "NLP."
            ],
            "context": [
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "With the advent of social media, anti-social and abusive behavior has become a prominent occurrence online. Undesirable psychological effects of abuse on individuals make it an important societal problem of our time. #TARGET_REF studied the ill-effects of online abuse on children, concluding that children may develop depression, anxiety, and other mental health problems as a result of their encounters online. Pew Research Center, in its latest report on online harassment #REF , revealed that 40% of adults in the United States have experienced abusive behavior online, of which 18% have faced severe forms of harassment, e.g., that of sexual nature. These statistics stress the need for automated detection and moderation systems. Hence, in recent years, a new research effort on abusive language detection has sprung up in NLP.",
        "output": "<BACK> With the advent of social media, anti-social and abusive behavior has become a prominent occurrence online. </BACK> Undesirable psychological effects of abuse on individuals make it an important societal problem of our time. <INFO> #TARGET_REF studied the ill-effects of online abuse on children, concluding that children may develop depression, anxiety, and other mental health problems as a result of their encounters online. </INFO> Pew Research Center, in its latest report on online harassment #REF , revealed that 40% of adults in the United States have experienced abusive behavior online, of which 18% have faced severe forms of harassment, e.g., that of sexual nature. <PERCEPT> These statistics stress the need for automated detection and moderation systems. </PERCEPT> Hence, in recent years, a new research effort on abusive language detection has sprung up in NLP."
    },
    {
        "gold": {
            "text": [
                "For",
                "each",
                "language,",
                "I",
                "use",
                "the",
                "fastText",
                "aligned",
                "word",
                "vectors",
                "#TARGET_REF",
                ",",
                "3",
                "limiting",
                "the",
                "vocabulary",
                "set",
                "V",
                "to",
                "the",
                "top",
                "200,000",
                "vectors",
                "by",
                "frequency.",
                "For",
                "the",
                "target",
                "word",
                "vocabulary",
                "W",
                ",",
                "I",
                "take",
                "the",
                "10,000",
                "most",
                "frequent",
                "wordforms",
                "among",
                "all",
                "attributive",
                "adjectives",
                "extracted",
                "from",
                "the",
                "entire",
                "CoNLL",
                "Wikipedia",
                "dataset."
            ],
            "context": [
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2
            ]
        },
        "input": "For each language, I use the fastText aligned word vectors #TARGET_REF , 3 limiting the vocabulary set V to the top 200,000 vectors by frequency. For the target word vocabulary W , I take the 10,000 most frequent wordforms among all attributive adjectives extracted from the entire CoNLL Wikipedia dataset.",
        "output": "<PERCEPT> For each language, I use </PERCEPT> <INFO> the fastText aligned word vectors #TARGET_REF , </INFO> <PERCEPT> 3 limiting the vocabulary set V to the top 200,000 vectors by frequency. For the target word vocabulary W , I take the 10,000 most frequent wordforms among all attributive adjectives extracted from the entire CoNLL Wikipedia dataset. </PERCEPT>"
    },
    {
        "gold": {
            "text": [
                "gender-laden",
                "words:",
                "(1,",
                "2)",
                "Gender-Coded",
                "Word",
                "Prevalence:",
                "#REF",
                "define",
                "masculine-and-feminine-themed",
                "words",
                "from",
                "an",
                "experiment",
                "on",
                "job",
                "ads",
                "that",
                "discouraged",
                "female",
                "applicants.",
                "(3)",
                "Superlative",
                "Prevalence",
                "#REF",
                "assess",
                "the",
                "relative",
                "frequency",
                "of",
                "positive",
                "and",
                "negative",
                "superlatives",
                "used",
                "to",
                "describe",
                "male",
                "versus",
                "female",
                "job",
                "candidates",
                "in",
                "recommendation",
                "letters.",
                "We",
                "use",
                "an",
                "established",
                "set",
                "of",
                "superlative",
                "words",
                "#REF",
                ".",
                "(4)",
                "Gender-Laden",
                "Scoring:",
                "#TARGET_REF",
                "analyse",
                "32",
                "properties",
                "related",
                "to",
                "a",
                "set",
                "of",
                "norms",
                "to",
                "score",
                "2,311",
                "words",
                "based",
                "on",
                "their",
                "\"gender-ladenness\".",
                "(5)",
                "Connotation",
                "Frames:",
                "Sap",
                "et",
                "al.",
                "(",
                "2017)",
                "define",
                "linguistic",
                "markers",
                "of",
                "power",
                "and",
                "agency",
                "associated",
                "with",
                "female",
                "versus",
                "male",
                "characters",
                "in",
                "modern",
                "films.",
                "(6)",
                "NRC",
                "VAD",
                "Lexicon:",
                "Mohammad",
                "(2018)",
                "presents",
                "a",
                "lexicon",
                "of",
                "words",
                "coded",
                "by",
                "valence,",
                "arousal,",
                "and",
                "dominance",
                "whose",
                "interpretation",
                "may",
                "interact",
                "with",
                "gender.",
                "5",
                "Dimensionality",
                "Reduction",
                "We",
                "employ",
                "principal",
                "component",
                "analysis",
                "(PCA)",
                "on",
                "the",
                "six",
                "bias",
                "measures",
                "on",
                "real-world",
                "job",
                "ads",
                "to",
                "collapse",
                "them",
                "into",
                "interpretable",
                "components.",
                "We",
                "then",
                "replicate",
                "the",
                "PCA",
                "on",
                "synthetic",
                "job",
                "ads",
                "(zero-shot)",
                "and",
                "project",
                "all",
                "data",
                "points",
                "onto",
                "the",
                "first",
                "two",
                "principal",
                "components",
                "of",
                "real",
                "job",
                "ads",
                "and",
                "vice",
                "versa."
            ],
            "context": [
                2,
                2,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "gender-laden words: (1, 2) Gender-Coded Word Prevalence: #REF define masculine-and-feminine-themed words from an experiment on job ads that discouraged female applicants. (3) Superlative Prevalence #REF assess the relative frequency of positive and negative superlatives used to describe male versus female job candidates in recommendation letters. We use an established set of superlative words #REF . (4) Gender-Laden Scoring: #TARGET_REF analyse 32 properties related to a set of norms to score 2,311 words based on their \"gender-ladenness\". (5) Connotation Frames: Sap et al. ( 2017) define linguistic markers of power and agency associated with female versus male characters in modern films. (6) NRC VAD Lexicon: Mohammad (2018) presents a lexicon of words coded by valence, arousal, and dominance whose interpretation may interact with gender. 5 Dimensionality Reduction We employ principal component analysis (PCA) on the six bias measures on real-world job ads to collapse them into interpretable components. We then replicate the PCA on synthetic job ads (zero-shot) and project all data points onto the first two principal components of real job ads and vice versa.",
        "output": "<PERCEPT> gender-laden words: </PERCEPT> <BACK> (1, 2) Gender-Coded Word Prevalence: </BACK> #REF define masculine-and-feminine-themed words from an experiment on job ads that discouraged female applicants. <BACK> (3) Superlative Prevalence </BACK> #REF assess the relative frequency of positive and negative superlatives used to describe male versus female job candidates in recommendation letters. We use an established set of superlative words #REF . <INFO> (4) Gender-Laden Scoring: #TARGET_REF analyse 32 properties related to a set of norms to score 2,311 words based on their \"gender-ladenness\". </INFO> <BACK> (5) Connotation Frames: </BACK> Sap et al. ( 2017) define linguistic markers of power and agency associated with female versus male characters in modern films. <BACK> (6) NRC VAD Lexicon: </BACK> Mohammad (2018) presents a lexicon of words coded by valence, arousal, and dominance whose interpretation may interact with gender. 5 Dimensionality Reduction <PERCEPT> We employ principal component analysis (PCA) on the six bias measures on real-world job ads to collapse them into interpretable components. </PERCEPT> We then replicate the PCA on synthetic job ads (zero-shot) and project all data points onto the first two principal components of real job ads and vice versa."
    },
    {
        "gold": {
            "text": [
                "Among",
                "the",
                "EC",
                "projects",
                "working",
                "in",
                "this",
                "direction",
                "we",
                "mention",
                "LE",
                "SPARKLE",
                "(Shallow",
                "PARsing",
                "and",
                "Knowledge",
                "extraction",
                "for",
                "Language",
                "Engineering",
                "2",
                "),",
                "combining",
                "shallow",
                "parsing",
                "and",
                "lexical",
                "acquisition",
                "techniques",
                "capable",
                "of",
                "learning",
                "(from",
                "large",
                "corpora)",
                "aspects",
                "of",
                "word",
                "knowledge",
                "required",
                "for",
                "LE",
                "applications",
                "#TARGET_REF",
                ".",
                "The",
                "project",
                "(http://www.ilc.pi.cnr.it/sparkle.html)",
                "is",
                "positioned",
                "as",
                "research",
                "on",
                "the",
                "development",
                "of",
                "methodologies",
                "and",
                "techniques",
                "for",
                "application-or",
                "domaindependent",
                "lexical",
                "resources",
                "to",
                "be",
                "acquired",
                "(semi)automatically",
                "from",
                "texts,",
                "an",
                "area",
                "which",
                "is",
                "crucial",
                "to",
                "most",
                "NLP",
                "applications.",
                "Economically",
                "feasible",
                "development",
                "of",
                "language",
                "models",
                "and",
                "of",
                "substantial",
                "lexical",
                "resources",
                "for",
                "real-world",
                "NLP",
                "applications",
                "needs",
                "to",
                "be",
                "based",
                "on",
                "substantially",
                "(semi)-automated",
                "techniques",
                "and",
                "flexible",
                "tools",
                "for",
                "analysing",
                "and",
                "extracting",
                "lexical",
                "information",
                "from",
                "textual",
                "corpora,",
                "otherwise",
                "coverage",
                "and/or",
                "accuracy",
                "will",
                "remain",
                "inadequate."
            ],
            "context": [
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Among the EC projects working in this direction we mention LE SPARKLE (Shallow PARsing and Knowledge extraction for Language Engineering 2 ), combining shallow parsing and lexical acquisition techniques capable of learning (from large corpora) aspects of word knowledge required for LE applications #TARGET_REF . The project (http://www.ilc.pi.cnr.it/sparkle.html) is positioned as research on the development of methodologies and techniques for application-or domaindependent lexical resources to be acquired (semi)automatically from texts, an area which is crucial to most NLP applications. Economically feasible development of language models and of substantial lexical resources for real-world NLP applications needs to be based on substantially (semi)-automated techniques and flexible tools for analysing and extracting lexical information from textual corpora, otherwise coverage and/or accuracy will remain inadequate.",
        "output": "<PERCEPT> Among the EC projects working in this direction we mention </PERCEPT> <INFO> LE SPARKLE (Shallow PARsing and Knowledge extraction for Language Engineering 2 ), combining shallow parsing and lexical acquisition techniques capable of learning (from large corpora) aspects of word knowledge required for LE applications #TARGET_REF . </INFO> The project (http://www.ilc.pi.cnr.it/sparkle.html) is positioned as research on the development of methodologies and techniques for application-or domaindependent lexical resources to be acquired (semi)automatically from texts, an area which is crucial to most NLP applications. Economically feasible development of language models and of substantial lexical resources for real-world NLP applications needs to be based on substantially (semi)-automated techniques and flexible tools for analysing and extracting lexical information from textual corpora, otherwise coverage and/or accuracy will remain inadequate."
    },
    {
        "gold": {
            "text": [
                "For",
                "model",
                "training,",
                "we",
                "used",
                "Adam",
                "with",
                "batch",
                "size",
                "of",
                "60,",
                "learning",
                "rate",
                "of",
                "3e-5,",
                "L2",
                "weight",
                "decay",
                "of",
                "0.01,",
                "learning",
                "rate",
                "warm",
                "up",
                "over",
                "the",
                "first",
                "10,000",
                "steps,",
                "and",
                "linear",
                "decay",
                "of",
                "learning",
                "rate.",
                "Our",
                "models",
                "were",
                "trained",
                "by",
                "one",
                "Tesla",
                "V100",
                "GPU",
                "card",
                "with",
                "32GB",
                "memory,",
                "and",
                "implemented",
                "on",
                "PyTorch",
                "with",
                "the",
                "Huggingface's",
                "Transformer",
                "#TARGET_REF",
                ".",
                "All",
                "Transformer-based",
                "methods",
                "were",
                "trained",
                "with",
                "30",
                "epochs,",
                "taken",
                "about",
                "4-5",
                "hours",
                "on",
                "the",
                "ComVE",
                "dataset",
                "and",
                "7-9",
                "hours",
                "on",
                "the",
                "α-NLG",
                "dataset."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3
            ]
        },
        "input": "For model training, we used Adam with batch size of 60, learning rate of 3e-5, L2 weight decay of 0.01, learning rate warm up over the first 10,000 steps, and linear decay of learning rate. Our models were trained by one Tesla V100 GPU card with 32GB memory, and implemented on PyTorch with the Huggingface's Transformer #TARGET_REF . All Transformer-based methods were trained with 30 epochs, taken about 4-5 hours on the ComVE dataset and 7-9 hours on the α-NLG dataset.",
        "output": "For model training, we used Adam with batch size of 60, learning rate of 3e-5, L2 weight decay of 0.01, learning rate warm up over the first 10,000 steps, and linear decay of learning rate. <PERCEPT> Our models were trained by one Tesla V100 GPU card with 32GB memory, and implemented on PyTorch with the </PERCEPT> <INFO> Huggingface's Transformer #TARGET_REF . </INFO> <BACK> All Transformer-based methods were trained with 30 epochs, taken about 4-5 hours on the ComVE dataset and 7-9 hours on the α-NLG dataset. </BACK>"
    },
    {
        "gold": {
            "text": [
                "The",
                "parameterized",
                "framework",
                "described",
                "above",
                "has",
                "been",
                "implemented",
                "in",
                "C++",
                "and",
                "successfully",
                "tested",
                "on",
                "well",
                "known,",
                "translationally",
                "divergent",
                "sentences.",
                "#TARGET_REF",
                "We",
                "ran",
                "the",
                "parameterized",
                "parser",
                "on",
                "both",
                "the",
                "English",
                "and",
                "Korean",
                "sentences",
                "shown",
                "here.",
                "The",
                "results",
                "shown",
                "in",
                "Figure",
                "5",
                "which",
                "were",
                "obtained",
                "from",
                "running",
                "the",
                "program",
                "on",
                "a",
                "Sparcstation",
                "ELC.",
                "3",
                "In",
                "general,",
                "the",
                "times",
                "demonstrate",
                "a",
                "speedup",
                "of",
                "2",
                "to",
                "3",
                "orders",
                "of",
                "magnitude",
                "over",
                "previous",
                "principlebased",
                "parsers",
                "on",
                "analogous",
                "examples",
                "such",
                "as",
                "those",
                "given",
                "in",
                "#REF",
                ".",
                "Even",
                "more",
                "significant",
                "is",
                "the",
                "negligible",
                "difference",
                "in",
                "processing",
                "time",
                "between",
                "the",
                "two",
                "languages,",
                "despite",
                "radical",
                "differences",
                "in",
                "structure,",
                "particularly",
                "with",
                "respect",
                "to",
                "head-complement",
                "positioning.",
                "This",
                "is",
                "an",
                "improvement",
                "over",
                "previous",
                "parameterized",
                "approaches",
                "in",
                "which",
                "cross-linguistic",
                "divergences",
                "frequently",
                "induced",
                "timing",
                "discrepancies",
                "of",
                "1-2",
                "orders",
                "of",
                "magnitude",
                "due",
                "to",
                "the",
                "head-initial",
                "bias",
                "that",
                "underlies",
                "most",
                "parsing",
                "designs."
            ],
            "context": [
                1,
                1,
                1,
                3,
                3,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                2,
                2,
                1,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The parameterized framework described above has been implemented in C++ and successfully tested on well known, translationally divergent sentences. #TARGET_REF We ran the parameterized parser on both the English and Korean sentences shown here. The results shown in Figure 5 which were obtained from running the program on a Sparcstation ELC. 3 In general, the times demonstrate a speedup of 2 to 3 orders of magnitude over previous principlebased parsers on analogous examples such as those given in #REF . Even more significant is the negligible difference in processing time between the two languages, despite radical differences in structure, particularly with respect to head-complement positioning. This is an improvement over previous parameterized approaches in which cross-linguistic divergences frequently induced timing discrepancies of 1-2 orders of magnitude due to the head-initial bias that underlies most parsing designs.",
        "output": "<INFO> The parameterized framework </INFO> <BACK> described above </BACK> <INFO> has been implemented in C++ and successfully tested on </INFO> <PERCEPT> well known, </PERCEPT> <INFO> translationally divergent sentences. #TARGET_REF </INFO> <PERCEPT> We ran the parameterized parser on both the English and Korean sentences </PERCEPT> shown here. <BACK> The results shown in Figure 5 </BACK> <PERCEPT> which were obtained from running the program on a Sparcstation ELC. </PERCEPT> 3 In general, the times demonstrate a speedup of 2 to 3 orders of magnitude over previous principlebased parsers on analogous examples such as those given in #REF . Even more significant is the negligible difference in processing time between the two languages, despite radical differences in structure, particularly with respect to head-complement positioning. This is an improvement over previous parameterized approaches in which cross-linguistic divergences frequently induced timing discrepancies of 1-2 orders of magnitude due to the head-initial bias that underlies most parsing designs."
    },
    {
        "gold": {
            "text": [
                "In",
                "the",
                "second",
                "objective,",
                "we",
                "quantify",
                "the",
                "contribution",
                "of",
                "each",
                "modality",
                "and",
                "their",
                "aspects",
                "given",
                "the",
                "situation",
                "to",
                "mimic",
                "human",
                "heuristic",
                "processing",
                "capability.",
                "Language",
                "comprehension",
                "involves",
                "complex",
                "sequential",
                "decision",
                "making",
                "and",
                "is",
                "affected",
                "by",
                "both",
                "uncertainty",
                "about",
                "the",
                "current",
                "input",
                "and",
                "lack",
                "of",
                "knowledge",
                "about",
                "the",
                "upcoming",
                "material.",
                "Thus,",
                "people",
                "use",
                "-to",
                "a",
                "large",
                "extent",
                "-fast",
                "and",
                "frugal",
                "heuristics,",
                "i.e.",
                "choosing",
                "a",
                "good-enough",
                "representation",
                "#TARGET_REF",
                ".",
                "The",
                "heuristic",
                "view",
                "provides",
                "a",
                "valid",
                "explanation",
                "for",
                "scenarios",
                "with",
                "a",
                "conversation",
                "inside",
                "noisy",
                "conditions.",
                "Instead",
                "of",
                "waiting",
                "/",
                "asking",
                "for",
                "clarification,",
                "the",
                "model",
                "will",
                "reach",
                "a",
                "good-enough",
                "decision",
                "based",
                "on",
                "all",
                "information",
                "gathered",
                "through",
                "all",
                "available",
                "input",
                "channels.",
                "In",
                "order",
                "to",
                "do",
                "that,",
                "the",
                "set",
                "of",
                "important",
                "features",
                "given",
                "the",
                "situated",
                "setting",
                "should",
                "be",
                "chosen",
                "automatically."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                1,
                1,
                1,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "In the second objective, we quantify the contribution of each modality and their aspects given the situation to mimic human heuristic processing capability. Language comprehension involves complex sequential decision making and is affected by both uncertainty about the current input and lack of knowledge about the upcoming material. Thus, people use -to a large extent -fast and frugal heuristics, i.e. choosing a good-enough representation #TARGET_REF . The heuristic view provides a valid explanation for scenarios with a conversation inside noisy conditions. Instead of waiting / asking for clarification, the model will reach a good-enough decision based on all information gathered through all available input channels. In order to do that, the set of important features given the situated setting should be chosen automatically.",
        "output": "In the second objective, we quantify the contribution of each modality and their aspects given the situation to mimic human heuristic processing capability. <BACK> Language comprehension involves complex sequential decision making and is affected by both uncertainty about the current input and lack of knowledge about the upcoming material. Thus, people use -to a large extent -fast and frugal heuristics, i.e. </BACK> <INFO> choosing a good-enough representation #TARGET_REF </INFO> . The heuristic view provides a valid explanation for scenarios with a conversation inside noisy conditions. Instead of waiting / asking for clarification, the model will reach a good-enough decision based on all information gathered through all available input channels. In order to do that, the set of important features given the situated setting should be chosen automatically."
    },
    {
        "gold": {
            "text": [
                "To",
                "ensure",
                "the",
                "task",
                "was",
                "as",
                "realistic",
                "as",
                "possible,",
                "we",
                "selected",
                "English",
                "documentation",
                "for",
                "a",
                "well-known",
                "online",
                "file",
                "storage",
                "and",
                "sharing",
                "service.",
                "We",
                "made",
                "an",
                "initial",
                "assumption",
                "that",
                "the",
                "original",
                "English",
                "instructions",
                "published",
                "by",
                "the",
                "developer",
                "were",
                "reasonably",
                "usable,",
                "given",
                "that",
                "the",
                "service",
                "has",
                "over",
                "50",
                "million",
                "users",
                "#TARGET_REF",
                ".",
                "As",
                "native",
                "speakers",
                "of",
                "English,",
                "both",
                "authors",
                "judged",
                "the",
                "documentation",
                "to",
                "be",
                "of",
                "reasonable",
                "quality",
                "and",
                "well-formed.",
                "These",
                "were",
                "initial",
                "assumptions",
                "which",
                "would",
                "be",
                "tested",
                "in",
                "the",
                "project."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "To ensure the task was as realistic as possible, we selected English documentation for a well-known online file storage and sharing service. We made an initial assumption that the original English instructions published by the developer were reasonably usable, given that the service has over 50 million users #TARGET_REF . As native speakers of English, both authors judged the documentation to be of reasonable quality and well-formed. These were initial assumptions which would be tested in the project.",
        "output": "To ensure the task was as realistic as possible, we selected English documentation for a well-known online file storage and sharing service. <PERCEPT> We made an initial assumption that the original English instructions published by the developer were reasonably usable, given that </PERCEPT> <INFO> the service has over 50 million users #TARGET_REF . </INFO> As native speakers of English, both authors judged the documentation to be of reasonable quality and well-formed. These were initial assumptions which would be tested in the project."
    },
    {
        "gold": {
            "text": [
                "Unsupervised",
                "systems",
                "Majority",
                "of",
                "the",
                "unsupervised",
                "WSD",
                "systems",
                "use",
                "external",
                "knowledge",
                "bases",
                "like",
                "WordNet",
                "#REF",
                "and",
                "BabelNet",
                "#REF",
                ".",
                "For",
                "each",
                "input",
                "word,",
                "its",
                "correct",
                "meaning",
                "according",
                "to",
                "the",
                "context",
                "can",
                "be",
                "found",
                "using",
                "graph-based",
                "techniques",
                "from",
                "those",
                "external",
                "knowledge",
                "bases.",
                "However,",
                "these",
                "approaches",
                "are",
                "only",
                "limited",
                "to",
                "the",
                "languages",
                "supported",
                "by",
                "used",
                "knowledge",
                "bases.",
                "More",
                "recent",
                "works",
                "like",
                "Hettiarachchi",
                "and",
                "Ranasinghe",
                "(2020a),",
                "#REF",
                "propose",
                "to",
                "use",
                "stacked",
                "word",
                "embeddings",
                "#REF",
                "obtained",
                "by",
                "general",
                "purpose",
                "pretrained",
                "contextualised",
                "word",
                "embedding",
                "models",
                "such",
                "as",
                "BERT",
                "#REF",
                "and",
                "Flair",
                "#TARGET_REF",
                "for",
                "unsupervised",
                "WSD.",
                "Despite",
                "their",
                "ability",
                "to",
                "scale",
                "over",
                "different",
                "languages,",
                "unsupervised",
                "approaches",
                "fall",
                "behind",
                "supervised",
                "systems",
                "in",
                "terms",
                "of",
                "accuracy."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                1,
                1,
                1,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Unsupervised systems Majority of the unsupervised WSD systems use external knowledge bases like WordNet #REF and BabelNet #REF . For each input word, its correct meaning according to the context can be found using graph-based techniques from those external knowledge bases. However, these approaches are only limited to the languages supported by used knowledge bases. More recent works like Hettiarachchi and Ranasinghe (2020a), #REF propose to use stacked word embeddings #REF obtained by general purpose pretrained contextualised word embedding models such as BERT #REF and Flair #TARGET_REF for unsupervised WSD. Despite their ability to scale over different languages, unsupervised approaches fall behind supervised systems in terms of accuracy.",
        "output": "Unsupervised systems Majority of the unsupervised WSD systems use external knowledge bases like WordNet #REF and BabelNet #REF . For each input word, its correct meaning according to the context can be found using graph-based techniques from those external knowledge bases. However, these approaches are only limited to the languages supported by used knowledge bases. <BACK> More recent works like Hettiarachchi and Ranasinghe (2020a), #REF propose to use stacked word embeddings #REF obtained by general purpose pretrained contextualised word embedding models such as BERT #REF and </BACK> <INFO> Flair #TARGET_REF for unsupervised WSD. </INFO> Despite their ability to scale over different languages, unsupervised approaches fall behind supervised systems in terms of accuracy."
    },
    {
        "gold": {
            "text": [
                "Data",
                "acquisition",
                "in",
                "DeKo",
                "was",
                "done",
                "on",
                "the",
                "basis",
                "of",
                "a",
                "corpus:",
                "we",
                "used",
                "German",
                "newspaper",
                "corpora,",
                "which",
                "were",
                "tagged",
                "with",
                "the",
                "TreeTagger",
                "#TARGET_REF",
                ")",
                "and",
                "lemmatized",
                "with",
                "DMOR",
                "#REF",
                ",",
                "for",
                "searching",
                "and",
                "pre-processing",
                "we",
                "used",
                "the",
                "Corpus",
                "Query",
                "Processor",
                "#REF",
                ")",
                "and",
                "a",
                "number",
                "of",
                "Perl",
                "scripts.",
                "In",
                "acquiring",
                "and",
                "systematizing",
                "the",
                "data",
                "we",
                "made",
                "a",
                "distinction",
                "between",
                "word",
                "formation",
                "involving",
                "selecting",
                "elements",
                "(roughly",
                "derivation)",
                "and",
                "word",
                "formation",
                "involving",
                "only",
                "categories",
                "(compounding).",
                "For",
                "expository",
                "purposes",
                "we",
                "concentrate",
                "on",
                "a",
                "derivation",
                "process",
                "here",
                "and",
                "only",
                "briefly",
                "describe",
                "a",
                "compounding",
                "process",
                "below."
            ],
            "context": [
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                3,
                3,
                3,
                3,
                3,
                3,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Data acquisition in DeKo was done on the basis of a corpus: we used German newspaper corpora, which were tagged with the TreeTagger #TARGET_REF ) and lemmatized with DMOR #REF , for searching and pre-processing we used the Corpus Query Processor #REF ) and a number of Perl scripts. In acquiring and systematizing the data we made a distinction between word formation involving selecting elements (roughly derivation) and word formation involving only categories (compounding). For expository purposes we concentrate on a derivation process here and only briefly describe a compounding process below.",
        "output": "<BACK> Data acquisition in DeKo was done on the basis of a corpus: </BACK> <PERCEPT> we used German newspaper corpora, which were tagged with </PERCEPT> <INFO> the TreeTagger #TARGET_REF ) </INFO> <BACK> and lemmatized with DMOR #REF , </BACK> <PERCEPT> for searching and pre-processing we used the Corpus Query Processor #REF ) and a number of Perl scripts. </PERCEPT> In acquiring and systematizing the data we made a distinction between word formation involving selecting elements (roughly derivation) and word formation involving only categories (compounding). For expository purposes we concentrate on a derivation process here and only briefly describe a compounding process below."
    },
    {
        "gold": {
            "text": [
                "As",
                "seen",
                "in",
                "Figure",
                "1,",
                "we",
                "focus",
                "on",
                "creating",
                "agents",
                "in",
                "#TARGET_REF",
                ",",
                "a",
                "large-scale",
                "crowdsourced",
                "fantasy",
                "text-adventure",
                "game,",
                "consisting",
                "of",
                "rich",
                "textual",
                "worlds-locations,",
                "objects,",
                "and",
                "characters",
                "with",
                "personas,",
                "and",
                "quests-motivations",
                "for",
                "each",
                "character.",
                "To",
                "complete",
                "these",
                "quests,",
                "an",
                "agent",
                "must:",
                "(1)",
                "maintain",
                "character",
                "via",
                "its",
                "persona,",
                "and",
                "(2)",
                "reason",
                "in",
                "a",
                "partially",
                "observable",
                "world",
                "about",
                "potential",
                "actions",
                "and",
                "utterances",
                "based",
                "on",
                "incomplete",
                "descriptions",
                "of",
                "the",
                "locations,",
                "objects,",
                "and",
                "other",
                "characters.",
                "This",
                "requires",
                "several",
                "human",
                "like",
                "competencies",
                "such",
                "as",
                "commonsense",
                "reasoning,",
                "dynamic",
                "natural",
                "language",
                "understanding,",
                "and",
                "operating",
                "in",
                "combinatorially",
                "sized",
                "language-based",
                "stateaction",
                "spaces.",
                "Although",
                "recent",
                "work",
                "has",
                "provided",
                "evidence",
                "showing",
                "that",
                "interactive",
                "language",
                "learning",
                "via",
                "reinforcement",
                "learning",
                "(RL)",
                "in",
                "text",
                "games",
                "can",
                "be",
                "significantly",
                "more",
                "sample",
                "efficient",
                "than",
                "static",
                "supervised",
                "learning",
                "#REF",
                "when",
                "creating",
                "goal-driven",
                "natural",
                "language",
                "agents,",
                "their",
                "ability",
                "to",
                "robustly",
                "generalize",
                "to",
                "novel",
                "scenarios",
                "is",
                "limited."
            ],
            "context": [
                3,
                3,
                3,
                3,
                3,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "As seen in Figure 1, we focus on creating agents in #TARGET_REF , a large-scale crowdsourced fantasy text-adventure game, consisting of rich textual worlds-locations, objects, and characters with personas, and quests-motivations for each character. To complete these quests, an agent must: (1) maintain character via its persona, and (2) reason in a partially observable world about potential actions and utterances based on incomplete descriptions of the locations, objects, and other characters. This requires several human like competencies such as commonsense reasoning, dynamic natural language understanding, and operating in combinatorially sized language-based stateaction spaces. Although recent work has provided evidence showing that interactive language learning via reinforcement learning (RL) in text games can be significantly more sample efficient than static supervised learning #REF when creating goal-driven natural language agents, their ability to robustly generalize to novel scenarios is limited.",
        "output": "<BACK> As seen in Figure 1, </BACK> <PERCEPT> we focus on creating agents in </PERCEPT> <INFO> #TARGET_REF , a large-scale crowdsourced fantasy text-adventure game, consisting of rich textual worlds-locations, objects, and characters with personas, and quests-motivations for each character. </INFO> <BACK> To complete these quests, an agent must: (1) maintain character via its persona, and (2) reason in a partially observable world about potential actions and utterances based on incomplete descriptions of the locations, objects, and other characters. This requires several human like competencies such as commonsense reasoning, dynamic natural language understanding, and operating in combinatorially sized language-based stateaction spaces. </BACK> Although recent work has provided evidence showing that interactive language learning via reinforcement learning (RL) in text games can be significantly more sample efficient than static supervised learning #REF when creating goal-driven natural language agents, their ability to robustly generalize to novel scenarios is limited."
    },
    {
        "gold": {
            "text": [
                "Similar",
                "to",
                "#TARGET_REF",
                ",",
                "we",
                "preprocess",
                "the",
                "data",
                "as",
                "follows:"
            ],
            "context": [
                2,
                2,
                0,
                0,
                2,
                2,
                2,
                2,
                2,
                2
            ]
        },
        "input": "Similar to #TARGET_REF , we preprocess the data as follows:",
        "output": "<PERCEPT> Similar to </PERCEPT> #TARGET_REF , <PERCEPT> we preprocess the data as follows: </PERCEPT>"
    },
    {
        "gold": {
            "text": [
                "Nonetheless,",
                "the",
                "established",
                "convention",
                "incorporates",
                "a",
                "distancing",
                "from",
                "the",
                "offence.",
                "Also,",
                "writers",
                "use",
                "apologies",
                "when",
                "they",
                "are",
                "apologising",
                "in",
                "a",
                "role",
                "(e.g.",
                "as",
                "the",
                "representative",
                "of",
                "an",
                "organisation).",
                "When",
                "speaking",
                "personally,",
                "they",
                "use",
                "other",
                "forms,",
                "typically",
                "sorry",
                "#TARGET_REF",
                ".",
                "Another",
                "possibility",
                "is",
                "that",
                "use",
                "of",
                "the",
                "noun",
                "form",
                "enables",
                "the",
                "writer",
                "to",
                "avoid",
                "the",
                "personal",
                "pronoun,",
                "creating",
                "a",
                "distance",
                "between",
                "the",
                "writer",
                "and",
                "the",
                "responsibility",
                "for",
                "the",
                "offence",
                "(ibid).",
                "In",
                "our",
                "data,",
                "individuals",
                "have",
                "not",
                "used",
                "this",
                "form",
                "at",
                "all",
                "and",
                "of",
                "the",
                "seven",
                "occurrences",
                "of",
                "the",
                "noun",
                "form,",
                "six",
                "are",
                "by",
                "individuals",
                "as",
                "representative",
                "of",
                "an",
                "organisation.",
                "This",
                "co-relates",
                "to",
                "Harrison's",
                "finding",
                "that",
                "the",
                "word",
                "apology/",
                "apologies",
                "help",
                "the",
                "writers",
                "to",
                "distance",
                "themselves",
                "from",
                "the",
                "instance",
                "or",
                "event."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Nonetheless, the established convention incorporates a distancing from the offence. Also, writers use apologies when they are apologising in a role (e.g. as the representative of an organisation). When speaking personally, they use other forms, typically sorry #TARGET_REF . Another possibility is that use of the noun form enables the writer to avoid the personal pronoun, creating a distance between the writer and the responsibility for the offence (ibid). In our data, individuals have not used this form at all and of the seven occurrences of the noun form, six are by individuals as representative of an organisation. This co-relates to Harrison's finding that the word apology/ apologies help the writers to distance themselves from the instance or event.",
        "output": "Nonetheless, the established convention incorporates a distancing from the offence. Also, <PERCEPT> writers use apologies when they are apologising in a role </PERCEPT> <BACK> (e.g. as the representative of an organisation). </BACK> <INFO> When speaking personally, they use other forms, typically sorry #TARGET_REF . </INFO> Another possibility is that use of the noun form enables the writer to avoid the personal pronoun, creating a distance between the writer and the responsibility for the offence (ibid). In our data, individuals have not used this form at all and of the seven occurrences of the noun form, six are by individuals as representative of an organisation. This co-relates to Harrison's finding that the word apology/ apologies help the writers to distance themselves from the instance or event."
    },
    {
        "gold": {
            "text": [
                "Our",
                "baseline",
                "is",
                "the",
                "mBERT",
                "model",
                "#TARGET_REF",
                ",",
                "which",
                "is",
                "pre-trained",
                "using",
                "pretext",
                "tasks",
                "like",
                "Masked",
                "Language",
                "Modelling",
                "and",
                "Next",
                "Sentence",
                "Prediction",
                "on",
                "a",
                "multilingual",
                "text",
                "corpus",
                "that",
                "includes",
                "our",
                "target",
                "languages,",
                "Hindi",
                "and",
                "Tamil.",
                "The",
                "default",
                "output",
                "head",
                "of",
                "mBERT",
                "is",
                "replaced",
                "with",
                "the",
                "head",
                "for",
                "the",
                "question-answering",
                "task.",
                "This",
                "is",
                "done",
                "by",
                "adding",
                "separate",
                "output",
                "heads",
                "for",
                "classifying",
                "the",
                "start",
                "and",
                "end",
                "positions",
                "as",
                "shown",
                "in",
                "#REF",
                "."
            ],
            "context": [
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2
            ]
        },
        "input": "Our baseline is the mBERT model #TARGET_REF , which is pre-trained using pretext tasks like Masked Language Modelling and Next Sentence Prediction on a multilingual text corpus that includes our target languages, Hindi and Tamil. The default output head of mBERT is replaced with the head for the question-answering task. This is done by adding separate output heads for classifying the start and end positions as shown in #REF .",
        "output": "<PERCEPT> Our baseline is </PERCEPT> <INFO> the mBERT model #TARGET_REF , </INFO> <PERCEPT> which is pre-trained using pretext tasks like Masked Language Modelling and Next Sentence Prediction on a multilingual text corpus that includes our target languages, Hindi and Tamil. The default output head of </PERCEPT> <INFO> mBERT </INFO> <PERCEPT> is replaced with the head for the question-answering task. This is done by adding separate output heads for classifying the start and end positions as shown in #REF . </PERCEPT>"
    },
    {
        "gold": {
            "text": [
                "For",
                "sake",
                "of",
                "comparison,",
                "we",
                "have",
                "computed",
                "BLEU",
                "at",
                "word",
                "level",
                "using",
                "BPE",
                "method",
                "#REF",
                ".",
                "We",
                "computed",
                "the",
                "subwords",
                "units",
                "in",
                "the",
                "output",
                "side",
                "of",
                "the",
                "neural",
                "network",
                "as",
                "done",
                "with",
                "Factored",
                "approach.",
                "We",
                "set",
                "the",
                "number",
                "of",
                "merge",
                "operations",
                "for",
                "the",
                "BPE",
                "algorithm,",
                "as",
                "explained",
                "in",
                "the",
                "paper",
                "#TARGET_REF",
                ",",
                "following",
                "equation",
                "12."
            ],
            "context": [
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                3,
                3,
                3
            ]
        },
        "input": "For sake of comparison, we have computed BLEU at word level using BPE method #REF . We computed the subwords units in the output side of the neural network as done with Factored approach. We set the number of merge operations for the BPE algorithm, as explained in the paper #TARGET_REF , following equation 12.",
        "output": "<PERCEPT> For sake of comparison, we have computed BLEU at word level using BPE method #REF . We computed the subwords units in the output side of the neural network as done with Factored approach. We set the number of merge operations for the BPE algorithm, as explained in </PERCEPT> <INFO> the paper #TARGET_REF , </INFO> <BACK> following equation 12. </BACK>"
    },
    {
        "gold": {
            "text": [
                "In",
                "#REF",
                ",",
                "a",
                "multilingual",
                "dataset",
                "was",
                "provided",
                "containing",
                "English,",
                "German,",
                "Spanish",
                "and",
                "French",
                "and",
                "there",
                "were",
                "two",
                "subtasks:",
                "binary",
                "classification",
                "and",
                "probabilistic",
                "classification.",
                "The",
                "submitted",
                "systems",
                "mainly",
                "use",
                "traditional",
                "machine",
                "learning",
                "classifiers(e.g.",
                "SVM,",
                "Random",
                "Forests)",
                "with",
                "features",
                "#REF",
                ",",
                "deep",
                "learning",
                "methods",
                "#TARGET_REF",
                "and",
                "ensemble",
                "methods",
                "#REF",
                ".",
                "More",
                "recently,",
                "#REF",
                "propose",
                "a",
                "new",
                "perspective",
                "by",
                "treating",
                "CWI",
                "as",
                "a",
                "sequence",
                "labeling",
                "task",
                "that",
                "can",
                "detect",
                "both",
                "complex",
                "words",
                "and",
                "phrases.",
                "All",
                "these",
                "methods",
                "are",
                "different",
                "from",
                "ours",
                "which",
                "utilizes",
                "heterogeneous",
                "PLMs",
                "with",
                "various",
                "training",
                "strategies."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                3,
                3,
                3,
                2,
                3,
                3,
                3,
                1,
                1,
                1,
                1,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "In #REF , a multilingual dataset was provided containing English, German, Spanish and French and there were two subtasks: binary classification and probabilistic classification. The submitted systems mainly use traditional machine learning classifiers(e.g. SVM, Random Forests) with features #REF , deep learning methods #TARGET_REF and ensemble methods #REF . More recently, #REF propose a new perspective by treating CWI as a sequence labeling task that can detect both complex words and phrases. All these methods are different from ours which utilizes heterogeneous PLMs with various training strategies.",
        "output": "In #REF , a multilingual dataset was provided containing English, German, Spanish and French and there were two subtasks: binary classification and probabilistic classification. <PERCEPT> The submitted systems mainly use traditional machine learning classifiers(e.g. </PERCEPT> <BACK> SVM, Random Forests) </BACK> <PERCEPT> with </PERCEPT> <BACK> features #REF , </BACK> <INFO> deep learning methods #TARGET_REF </INFO> <BACK> and ensemble methods #REF </BACK> . More recently, #REF propose a new perspective by treating CWI as a sequence labeling task that can detect both complex words and phrases. All these methods are different from ours which utilizes heterogeneous PLMs with various training strategies."
    },
    {
        "gold": {
            "text": [
                "layer",
                "to",
                "extract",
                "Arabic",
                "NEs.",
                "They",
                "used",
                "two",
                "attention",
                "units,",
                "the",
                "embedding",
                "attention",
                "layer,",
                "and",
                "the",
                "self-attention",
                "unit.",
                "They",
                "achieved",
                "an",
                "F1",
                "score",
                "of",
                "91.31",
                "to",
                "achieve",
                "a",
                "new",
                "stateof-the-art",
                "on",
                "a",
                "large",
                "dataset",
                "proposed",
                "for",
                "evaluation",
                "in",
                "the",
                "same",
                "work.",
                "At",
                "the",
                "same",
                "time,",
                "#REF",
                "used",
                "character",
                "Convolutional",
                "Neural",
                "Networks",
                "(CNN)",
                "as",
                "a",
                "replacement",
                "for",
                "characterlevel",
                "bidirectional",
                "Long",
                "Short-Term",
                "Memory",
                "(LSTM)",
                "in",
                "Arabic",
                "NER.",
                "Their",
                "proposed",
                "system",
                "was",
                "able",
                "to",
                "outperform",
                "the",
                "state-of-art",
                "systems,",
                "including",
                "character-level",
                "Bi-LSTM",
                "on",
                "various",
                "standard",
                "Arabic",
                "NER",
                "corpora.",
                "#REF",
                "proposed",
                "AraBERTv0.1",
                "which",
                "involves",
                "pretraining",
                "the",
                "BERT",
                "transformer",
                "model",
                "for",
                "the",
                "Arabic",
                "language.",
                "They",
                "compared",
                "AraBERTv0.1",
                "and",
                "the",
                "Bi-LSTM-CRF",
                "model",
                "on",
                "ANERCorp,",
                "the",
                "former",
                "achieved",
                "84.2",
                "F1",
                "scores",
                "whereas",
                "the",
                "later",
                "achieved",
                "81.7.",
                "Most",
                "recently,",
                "#TARGET_REF",
                "proposed",
                "a",
                "transfer",
                "learning",
                "approach",
                "for",
                "Arabic",
                "NER",
                "with",
                "Deep",
                "Neural",
                "Networks",
                "where",
                "they",
                "showed",
                "that",
                "their",
                "model",
                "outperformed",
                "significantly",
                "the",
                "Bi-LSTM-CRF",
                "model.",
                "We",
                "have",
                "not",
                "considered",
                "this",
                "approach",
                "here",
                "because",
                "our",
                "aim",
                "is",
                "not",
                "to",
                "create",
                "a",
                "new",
                "state",
                "of",
                "the",
                "art",
                "model",
                "but",
                "to",
                "show",
                "the",
                "effectiveness",
                "of",
                "incorporating",
                "language",
                "specific",
                "characteristics",
                "in",
                "the",
                "form",
                "of",
                "embeddings."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2
            ]
        },
        "input": "layer to extract Arabic NEs. They used two attention units, the embedding attention layer, and the self-attention unit. They achieved an F1 score of 91.31 to achieve a new stateof-the-art on a large dataset proposed for evaluation in the same work. At the same time, #REF used character Convolutional Neural Networks (CNN) as a replacement for characterlevel bidirectional Long Short-Term Memory (LSTM) in Arabic NER. Their proposed system was able to outperform the state-of-art systems, including character-level Bi-LSTM on various standard Arabic NER corpora. #REF proposed AraBERTv0.1 which involves pretraining the BERT transformer model for the Arabic language. They compared AraBERTv0.1 and the Bi-LSTM-CRF model on ANERCorp, the former achieved 84.2 F1 scores whereas the later achieved 81.7. Most recently, #TARGET_REF proposed a transfer learning approach for Arabic NER with Deep Neural Networks where they showed that their model outperformed significantly the Bi-LSTM-CRF model. We have not considered this approach here because our aim is not to create a new state of the art model but to show the effectiveness of incorporating language specific characteristics in the form of embeddings.",
        "output": "layer to extract Arabic NEs. They used two attention units, the embedding attention layer, and the self-attention unit. They achieved an F1 score of 91.31 to achieve a new stateof-the-art on a large dataset proposed for evaluation in the same work. At the same time, #REF used character Convolutional Neural Networks (CNN) as a replacement for characterlevel bidirectional Long Short-Term Memory (LSTM) in Arabic NER. Their proposed system was able to outperform the state-of-art systems, including character-level Bi-LSTM on various standard Arabic NER corpora. #REF proposed AraBERTv0.1 which involves pretraining the BERT transformer model for the Arabic language. They compared AraBERTv0.1 and the Bi-LSTM-CRF model on ANERCorp, the former achieved 84.2 F1 scores whereas the later achieved 81.7. Most recently, <INFO> #TARGET_REF proposed a transfer learning approach for Arabic NER with Deep Neural Networks where they showed that their model outperformed significantly the Bi-LSTM-CRF model. </INFO> <PERCEPT> We have not considered this approach here because our aim is not to create a new state of the art model but to show the effectiveness of incorporating language specific characteristics in the form of embeddings. </PERCEPT>"
    },
    {
        "gold": {
            "text": [
                "We",
                "anticipate",
                "exploring",
                "various",
                "extensions",
                "to",
                "and",
                "validations",
                "of",
                "this",
                "method",
                "in",
                "the",
                "future.",
                "Specifically,",
                "we",
                "would",
                "like",
                "to",
                "explore",
                "methods",
                "that",
                "might",
                "mitigate",
                "performance",
                "degradation",
                "due",
                "to",
                "a",
                "lack",
                "of",
                "word",
                "boundaries",
                "in",
                "our",
                "method.",
                "Subword",
                "tokenization",
                "techniques,",
                "such",
                "as",
                "Byte-Pair",
                "Encodings",
                "(BPE)",
                "#REF",
                ",",
                "or",
                "character-based",
                "word",
                "segmentation",
                "techniques",
                "might",
                "help",
                "in",
                "detecting",
                "and",
                "exploiting",
                "repeating",
                "patterns",
                "within",
                "the",
                "phonetic",
                "representation.",
                "Furthermore,",
                "the",
                "word",
                "embedding",
                "techniques",
                "used",
                "by",
                "#REF",
                "or",
                "#TARGET_REF",
                "have",
                "been",
                "shown",
                "to",
                "work",
                "well,",
                "and",
                "would",
                "be",
                "worth",
                "investigating",
                "how",
                "the",
                "removal",
                "of",
                "spacedelimited",
                "word",
                "boundaries",
                "would",
                "affect",
                "this."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                1,
                1,
                1,
                1,
                1,
                1,
                3,
                3,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2
            ]
        },
        "input": "We anticipate exploring various extensions to and validations of this method in the future. Specifically, we would like to explore methods that might mitigate performance degradation due to a lack of word boundaries in our method. Subword tokenization techniques, such as Byte-Pair Encodings (BPE) #REF , or character-based word segmentation techniques might help in detecting and exploiting repeating patterns within the phonetic representation. Furthermore, the word embedding techniques used by #REF or #TARGET_REF have been shown to work well, and would be worth investigating how the removal of spacedelimited word boundaries would affect this.",
        "output": "We anticipate exploring various extensions to and validations of this method in the future. Specifically, we would like to explore methods that might mitigate performance degradation due to a lack of word boundaries in our method. Subword tokenization techniques, such as Byte-Pair Encodings (BPE) #REF , or character-based word segmentation techniques might help in detecting and exploiting repeating patterns within the phonetic representation. Furthermore, <INFO> the word embedding techniques used by </INFO> <BACK> #REF or </BACK> <INFO> #TARGET_REF </INFO> <PERCEPT> have been shown to work well, and would be worth investigating how the removal of spacedelimited word boundaries would affect this. </PERCEPT>"
    },
    {
        "gold": {
            "text": [
                "BERT-based",
                "models",
                "have",
                "the",
                "advantage",
                "that",
                "we",
                "can",
                "directly",
                "use",
                "the",
                "attention",
                "scores",
                "as",
                "explanations",
                "of",
                "model",
                "decisions.",
                "For",
                "models",
                "with",
                "other",
                "architectures,",
                "we",
                "can",
                "use",
                "explanation",
                "techniques",
                "such",
                "as",
                "LIME",
                "#REF",
                "or",
                "Path",
                "Integrated",
                "Gradient",
                "approaches",
                "#TARGET_REF",
                "to",
                "provide",
                "explanations."
            ],
            "context": [
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                3,
                3,
                3,
                1,
                1,
                1,
                1,
                1,
                2,
                2,
                2
            ]
        },
        "input": "BERT-based models have the advantage that we can directly use the attention scores as explanations of model decisions. For models with other architectures, we can use explanation techniques such as LIME #REF or Path Integrated Gradient approaches #TARGET_REF to provide explanations.",
        "output": "<BACK> BERT-based models have the advantage that we can directly use the attention scores as explanations of model decisions. </BACK> <PERCEPT> For models with other architectures, we can use explanation techniques such as </PERCEPT> <BACK> LIME #REF or </BACK> <INFO> Path Integrated Gradient approaches #TARGET_REF </INFO> <PERCEPT> to provide explanations. </PERCEPT>"
    },
    {
        "gold": {
            "text": [
                "We",
                "trained",
                "and",
                "qualified",
                "23",
                "workers",
                "on",
                "the",
                "Amazon",
                "Mechanical",
                "Turk",
                "(AMT)",
                "platform,",
                "to",
                "participate",
                "in",
                "the",
                "coreference,",
                "NP",
                "relations,",
                "and",
                "consolidation",
                "tasks.",
                "We",
                "follow",
                "the",
                "controlled",
                "crowdsourcing",
                "protocol",
                "suggested",
                "by",
                "#TARGET_REF",
                "and",
                "#REF",
                ",",
                "giving",
                "detailed",
                "than",
                "two",
                "prepositions",
                "for",
                "the",
                "same",
                "NP",
                "pairs",
                "is",
                "not",
                "common,",
                "and",
                "two",
                "prepositions",
                "occur",
                "in",
                "11.6%",
                "of",
                "the",
                "test-set.",
                "For",
                "simplicity,",
                "in",
                "this",
                "work,",
                "we",
                "consider",
                "a",
                "single",
                "preposition",
                "for",
                "each",
                "NP",
                "pair,",
                "but",
                "the",
                "collected",
                "data",
                "may",
                "contain",
                "two",
                "prepositions",
                "for",
                "some",
                "pairs.",
                "We",
                "paid",
                "$1.50,",
                "$2.50,",
                "and",
                "$1.5)",
                "for",
                "each",
                "HIT",
                "in",
                "the",
                "coreference,",
                "NP-relations,",
                "and",
                "consolidation",
                "tasks,",
                "respectively.",
                "The",
                "price",
                "for",
                "the",
                "NPrelations",
                "task",
                "was",
                "raised",
                "to",
                "$2.70",
                "for",
                "the",
                "test",
                "and",
                "out-of-domain",
                "subsets.",
                "We",
                "additionally",
                "paid",
                "bonus",
                "payments",
                "on",
                "multiple",
                "occasions.",
                "Overall,",
                "we",
                "aimed",
                "at",
                "paying",
                "at",
                "least",
                "the",
                "minimum",
                "wage",
                "in",
                "the",
                "United",
                "States."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "We trained and qualified 23 workers on the Amazon Mechanical Turk (AMT) platform, to participate in the coreference, NP relations, and consolidation tasks. We follow the controlled crowdsourcing protocol suggested by #TARGET_REF and #REF , giving detailed than two prepositions for the same NP pairs is not common, and two prepositions occur in 11.6% of the test-set. For simplicity, in this work, we consider a single preposition for each NP pair, but the collected data may contain two prepositions for some pairs. We paid $1.50, $2.50, and $1.5) for each HIT in the coreference, NP-relations, and consolidation tasks, respectively. The price for the NPrelations task was raised to $2.70 for the test and out-of-domain subsets. We additionally paid bonus payments on multiple occasions. Overall, we aimed at paying at least the minimum wage in the United States.",
        "output": "We trained and qualified 23 workers on the Amazon Mechanical Turk (AMT) platform, to participate in the coreference, NP relations, and consolidation tasks. <PERCEPT> We follow the controlled crowdsourcing protocol suggested by </PERCEPT> <INFO> #TARGET_REF </INFO> <BACK> and #REF , giving detailed than two prepositions for the same NP pairs is not common, and two prepositions occur in 11.6% of the test-set. </BACK> For simplicity, in this work, we consider a single preposition for each NP pair, but the collected data may contain two prepositions for some pairs. We paid $1.50, $2.50, and $1.5) for each HIT in the coreference, NP-relations, and consolidation tasks, respectively. The price for the NPrelations task was raised to $2.70 for the test and out-of-domain subsets. We additionally paid bonus payments on multiple occasions. Overall, we aimed at paying at least the minimum wage in the United States."
    },
    {
        "gold": {
            "text": [
                "Early",
                "studies",
                "for",
                "scientific",
                "literature",
                "link",
                "formulae",
                "to",
                "Wikipedia",
                "page",
                "#TARGET_REF",
                ".",
                "Even",
                "though",
                "this",
                "can",
                "provide",
                "additional",
                "information",
                "regarding",
                "the",
                "mathematical",
                "expression,",
                "a",
                "reader",
                "might",
                "find",
                "it",
                "harder",
                "to",
                "understand",
                "the",
                "Wikipedia",
                "page",
                "as",
                "it",
                "is",
                "presented",
                "in",
                "many",
                "unrelated",
                "forms.",
                "Linking",
                "to",
                "the",
                "description",
                "in",
                "the",
                "same",
                "document",
                "is",
                "more",
                "practical",
                "#REF",
                "as",
                "the",
                "descriptions",
                "are",
                "dedicated",
                "to",
                "the",
                "symbols",
                "and",
                "the",
                "context",
                "presented",
                "in",
                "the",
                "document."
            ],
            "context": [
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Early studies for scientific literature link formulae to Wikipedia page #TARGET_REF . Even though this can provide additional information regarding the mathematical expression, a reader might find it harder to understand the Wikipedia page as it is presented in many unrelated forms. Linking to the description in the same document is more practical #REF as the descriptions are dedicated to the symbols and the context presented in the document.",
        "output": "<INFO> Early studies for scientific literature link formulae to Wikipedia page #TARGET_REF . </INFO> <PERCEPT> Even though this can provide additional information regarding the mathematical expression, a reader might find it harder to understand the Wikipedia page </PERCEPT> <BACK> as it is presented in many unrelated forms. </BACK> Linking to the description in the same document is more practical #REF as the descriptions are dedicated to the symbols and the context presented in the document."
    },
    {
        "gold": {
            "text": [
                "CoNLL",
                "bart",
                ":",
                "we",
                "use",
                "a",
                "fine-tuned",
                "BART",
                "model",
                "#TARGET_REF"
            ],
            "context": [
                3,
                3,
                3,
                2,
                2,
                2,
                2,
                1,
                1,
                1
            ]
        },
        "input": "CoNLL bart : we use a fine-tuned BART model #TARGET_REF",
        "output": "<BACK> CoNLL bart : </BACK> <PERCEPT> we use a fine-tuned </PERCEPT> <INFO> BART model #TARGET_REF </INFO>"
    },
    {
        "gold": {
            "text": [
                "Word",
                "co-occurrence",
                "probabilities",
                "are",
                "a",
                "key",
                "ingredient",
                "in",
                "usage-based",
                "cognitive",
                "models",
                "of",
                "language.",
                "By",
                "word",
                "co-occurrence",
                "probabilities,",
                "I",
                "mean",
                "the",
                "probability",
                "of",
                "a",
                "word",
                "w",
                "given",
                "some",
                "other",
                "single",
                "word",
                "c,",
                "p(w",
                "|",
                "c),",
                "where",
                "words",
                "w",
                "and",
                "c",
                "have",
                "some",
                "specific",
                "relationship,",
                "for",
                "example",
                "adjectives",
                "that",
                "attributively",
                "modify",
                "nouns",
                "or",
                "nouns",
                "serving",
                "as",
                "direct",
                "objects",
                "of",
                "verbs",
                "#TARGET_REF",
                "."
            ],
            "context": [
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3
            ]
        },
        "input": "Word co-occurrence probabilities are a key ingredient in usage-based cognitive models of language. By word co-occurrence probabilities, I mean the probability of a word w given some other single word c, p(w | c), where words w and c have some specific relationship, for example adjectives that attributively modify nouns or nouns serving as direct objects of verbs #TARGET_REF .",
        "output": "<BACK> Word co-occurrence probabilities are a key ingredient in usage-based cognitive models of language. </BACK> <PERCEPT> By word co-occurrence probabilities, I mean </PERCEPT> <INFO> the probability of a word w given some other single word c, p(w | c), where words w and c have some specific relationship, </INFO> <BACK> for example adjectives that attributively modify nouns or nouns serving as direct objects of verbs #TARGET_REF . </BACK>"
    },
    {
        "gold": {
            "text": [
                "firstly",
                "utilizes",
                "off-the-shelf",
                "detectors",
                "to",
                "parse",
                "images",
                "into",
                "a",
                "set",
                "of",
                "object",
                "tokens,",
                "and",
                "then",
                "builds",
                "a",
                "multi-layer",
                "Transformer",
                "to",
                "learn",
                "visual",
                "and",
                "language",
                "embeddings",
                "jointly.",
                "In",
                "order",
                "to",
                "facilitate",
                "the",
                "multi-modal",
                "learning,",
                "those",
                "networks",
                "are",
                "typically",
                "trained",
                "via",
                "a",
                "set",
                "of",
                "carefully",
                "designed",
                "BERT-like",
                "objectives",
                "(e.g.",
                "Image-Text",
                "Matching).",
                "Despite",
                "its",
                "promising",
                "performance,",
                "the",
                "two-step",
                "strategy",
                "suffers",
                "from",
                "several",
                "limitations:",
                "1)",
                "limited",
                "visual",
                "object",
                "concepts",
                "as",
                "the",
                "external",
                "detectors",
                "are",
                "trained",
                "on",
                "a",
                "predefined",
                "set",
                "of",
                "object",
                "categories,",
                "2)",
                "lack",
                "of",
                "context",
                "cues",
                "outside",
                "of",
                "the",
                "object",
                "regions,",
                "which",
                "are",
                "crucial",
                "for",
                "complex",
                "reasoning",
                "tasks,",
                "3)",
                "sub-optimal",
                "visual",
                "representation",
                "due",
                "to",
                "stage-wise",
                "training,",
                "and",
                "4)",
                "computational",
                "inefficiency",
                "caused",
                "by",
                "additional",
                "detection",
                "modules.",
                "To",
                "overcome",
                "those",
                "limitations,",
                "recent",
                "works",
                "attempt",
                "to",
                "learn",
                "a",
                "joint",
                "visual-linguistic",
                "representations",
                "in",
                "an",
                "end-to-end",
                "manner",
                "#TARGET_REF",
                ".",
                "These",
                "methods",
                "directly",
                "take",
                "dense",
                "visual",
                "features",
                "from",
                "image",
                "grids",
                "as",
                "inputs",
                "to",
                "a",
                "multi-modal",
                "Transformer",
                "network,",
                "and",
                "hence",
                "do",
                "not",
                "rely",
                "on",
                "external",
                "object",
                "detectors",
                "in",
                "both",
                "pretraining",
                "and",
                "finetuning",
                "stages.",
                "Such",
                "model",
                "design",
                "significantly",
                "simplifies",
                "overall",
                "network",
                "architecture",
                "and",
                "allows",
                "deeper",
                "integration",
                "between",
                "visual",
                "and",
                "language",
                "features.",
                "However,",
                "using",
                "grid-level",
                "features",
                "makes",
                "it",
                "difficult",
                "to",
                "capture",
                "object-level",
                "visual",
                "concepts,",
                "which",
                "often",
                "results",
                "in",
                "less",
                "expressive",
                "multi-modal",
                "representations",
                "and",
                "inferior",
                "performances",
                "in",
                "downstream",
                "tasks."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                3,
                0,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                0,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2
            ]
        },
        "input": "firstly utilizes off-the-shelf detectors to parse images into a set of object tokens, and then builds a multi-layer Transformer to learn visual and language embeddings jointly. In order to facilitate the multi-modal learning, those networks are typically trained via a set of carefully designed BERT-like objectives (e.g. Image-Text Matching). Despite its promising performance, the two-step strategy suffers from several limitations: 1) limited visual object concepts as the external detectors are trained on a predefined set of object categories, 2) lack of context cues outside of the object regions, which are crucial for complex reasoning tasks, 3) sub-optimal visual representation due to stage-wise training, and 4) computational inefficiency caused by additional detection modules. To overcome those limitations, recent works attempt to learn a joint visual-linguistic representations in an end-to-end manner #TARGET_REF . These methods directly take dense visual features from image grids as inputs to a multi-modal Transformer network, and hence do not rely on external object detectors in both pretraining and finetuning stages. Such model design significantly simplifies overall network architecture and allows deeper integration between visual and language features. However, using grid-level features makes it difficult to capture object-level visual concepts, which often results in less expressive multi-modal representations and inferior performances in downstream tasks.",
        "output": "firstly utilizes off-the-shelf detectors to parse images into a set of object tokens, and then builds a multi-layer Transformer to learn visual and language embeddings jointly. In order to facilitate the multi-modal learning, those networks are typically trained via a set of carefully designed BERT-like objectives (e.g. Image-Text Matching). Despite its promising performance, the two-step strategy suffers from several limitations: 1) <BACK> limited visual object concepts </BACK> as the external detectors are trained on a predefined set of object categories, 2) <BACK> lack of context cues outside of the object regions, </BACK> which are crucial for complex reasoning tasks, 3) <BACK> sub-optimal visual representation </BACK> due to stage-wise training, <BACK> and </BACK> 4) <BACK> computational inefficiency </BACK> caused by additional detection modules. <PERCEPT> To overcome those limitations, </PERCEPT> <INFO> recent works attempt to learn a joint visual-linguistic representations in an end-to-end manner #TARGET_REF </INFO> . <INFO> These methods directly take dense visual features from image grids as inputs to a multi-modal Transformer network, and hence do not rely on external object detectors in both pretraining and finetuning stages. </INFO> <PERCEPT> Such model design significantly simplifies overall network architecture and allows deeper integration between visual and language features. However, using grid-level features makes it difficult to capture object-level visual concepts, which often results in less expressive multi-modal representations and inferior performances in downstream tasks. </PERCEPT>"
    },
    {
        "gold": {
            "text": [
                "However,",
                "sentence",
                "diversity",
                "is",
                "based",
                "not",
                "only",
                "on",
                "individual",
                "tokens,",
                "but",
                "also",
                "on",
                "the",
                "token",
                "sequence.",
                "We",
                "are",
                "able",
                "to",
                "compute",
                "weights",
                "for",
                "loss",
                "functions",
                "dynamically,",
                "depending",
                "on",
                "the",
                "context,",
                "while",
                "retaining",
                "the",
                "fluency",
                "of",
                "generated",
                "sentences.",
                "We",
                "propose",
                "such",
                "a",
                "loss",
                "function,",
                "Inverse",
                "N-gram",
                "Frequency",
                "(INF)",
                "loss,",
                "which",
                "uses",
                "the",
                "inverse",
                "of",
                "the",
                "frequency",
                "of",
                "the",
                "n-gram",
                "of",
                "the",
                "tokens,",
                "rather",
                "than",
                "the",
                "token",
                "frequency.",
                "We",
                "built",
                "a",
                "neural",
                "dialogue",
                "system",
                "trained",
                "by",
                "INF",
                "loss",
                "using",
                "huge",
                "amounts",
                "of",
                "dialogue",
                "data",
                "extracted",
                "from",
                "Twitter.",
                "After",
                "comparing",
                "models",
                "using",
                "the",
                "SCE",
                "loss,",
                "the",
                "ITF",
                "loss,",
                "and",
                "the",
                "INF",
                "loss,",
                "we",
                "evaluated",
                "their",
                "diversity",
                "and",
                "fluency.",
                "Results",
                "show",
                "that",
                "our",
                "proposed",
                "INF",
                "loss",
                "model",
                "outperformed",
                "the",
                "SCE",
                "loss",
                "and",
                "ITF",
                "loss",
                "models",
                "for",
                "most",
                "automatic",
                "assessment",
                "measures",
                "such",
                "as",
                "DIST-N",
                "#TARGET_REF",
                "and",
                "ROUGE",
                "#REF",
                ".",
                "Our",
                "INF",
                "loss",
                "model",
                "also",
                "achieved",
                "higher",
                "scores",
                "on",
                "our",
                "human",
                "evaluations",
                "of",
                "coherence",
                "and",
                "richness."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "However, sentence diversity is based not only on individual tokens, but also on the token sequence. We are able to compute weights for loss functions dynamically, depending on the context, while retaining the fluency of generated sentences. We propose such a loss function, Inverse N-gram Frequency (INF) loss, which uses the inverse of the frequency of the n-gram of the tokens, rather than the token frequency. We built a neural dialogue system trained by INF loss using huge amounts of dialogue data extracted from Twitter. After comparing models using the SCE loss, the ITF loss, and the INF loss, we evaluated their diversity and fluency. Results show that our proposed INF loss model outperformed the SCE loss and ITF loss models for most automatic assessment measures such as DIST-N #TARGET_REF and ROUGE #REF . Our INF loss model also achieved higher scores on our human evaluations of coherence and richness.",
        "output": "However, sentence diversity is based not only on individual tokens, but also on the token sequence. We are able to compute weights for loss functions dynamically, depending on the context, while retaining the fluency of generated sentences. We propose such a loss function, Inverse N-gram Frequency (INF) loss, which uses the inverse of the frequency of the n-gram of the tokens, rather than the token frequency. We built a neural dialogue system trained by INF loss using huge amounts of dialogue data extracted from Twitter. <BACK> After comparing models using the SCE loss, the ITF loss, and the INF loss, we evaluated their diversity and fluency. </BACK> <PERCEPT> Results show that our proposed INF loss model outperformed the SCE loss and ITF loss models for most automatic assessment measures such as </PERCEPT> <INFO> DIST-N #TARGET_REF </INFO> <BACK> and ROUGE #REF . </BACK> Our INF loss model also achieved higher scores on our human evaluations of coherence and richness."
    },
    {
        "gold": {
            "text": [
                "Technical",
                "Details",
                "All",
                "neural",
                "models",
                "are",
                "trained",
                "using",
                "cross-entropy",
                "loss",
                "and",
                "optimized",
                "with",
                "Adam",
                "(Kingma",
                "and",
                "Ba,",
                "2015),",
                "using",
                "the",
                "AllenNLP",
                "library",
                "#REF",
                ".",
                "We",
                "train",
                "using",
                "a",
                "1e",
                "−",
                "5",
                "learning",
                "rate",
                "for",
                "40",
                "epochs,",
                "with",
                "early",
                "stopping",
                "based",
                "the",
                "F1",
                "metric",
                "on",
                "the",
                "development",
                "set.",
                "We",
                "use",
                "SpanBERT",
                "#TARGET_REF",
                "as",
                "the",
                "pretrained",
                "MLM,",
                "as",
                "it",
                "was",
                "found",
                "to",
                "work",
                "well",
                "on",
                "span-based",
                "tasks",
                "with",
                "its",
                "base",
                "and",
                "the",
                "large",
                "variants.",
                "The",
                "anchor",
                "and",
                "complement",
                "encoding",
                "MLPs",
                "have",
                "one",
                "500-dim",
                "hidden",
                "layer",
                "and",
                "output",
                "500dim",
                "representations.",
                "The",
                "prediction",
                "MLPs",
                "have",
                "one",
                "100-dim",
                "hidden",
                "layer.",
                "All",
                "MLPs",
                "use",
                "the",
                "ReLU",
                "activation.",
                "We",
                "used",
                "the",
                "same",
                "hyperparameters",
                "for",
                "all",
                "baselines",
                "and",
                "did",
                "not",
                "tune",
                "them.",
                "18"
            ],
            "context": [
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Technical Details All neural models are trained using cross-entropy loss and optimized with Adam (Kingma and Ba, 2015), using the AllenNLP library #REF . We train using a 1e − 5 learning rate for 40 epochs, with early stopping based the F1 metric on the development set. We use SpanBERT #TARGET_REF as the pretrained MLM, as it was found to work well on span-based tasks with its base and the large variants. The anchor and complement encoding MLPs have one 500-dim hidden layer and output 500dim representations. The prediction MLPs have one 100-dim hidden layer. All MLPs use the ReLU activation. We used the same hyperparameters for all baselines and did not tune them. 18",
        "output": "<BACK> Technical Details All neural models are trained using cross-entropy loss and optimized with Adam (Kingma and Ba, 2015), using the AllenNLP library #REF . We train using a 1e − 5 learning rate for 40 epochs, with early stopping based the F1 metric on the development set. </BACK> <INFO> We use SpanBERT #TARGET_REF as the pretrained MLM, as it was found to work well on span-based tasks with its base and the large variants. </INFO> The anchor and complement encoding MLPs have one 500-dim hidden layer and output 500dim representations. The prediction MLPs have one 100-dim hidden layer. All MLPs use the ReLU activation. We used the same hyperparameters for all baselines and did not tune them. 18"
    },
    {
        "gold": {
            "text": [
                "Explicit",
                "vs.",
                "Implicit",
                "NP",
                "Relations",
                "Next,",
                "we",
                "analyze",
                "the",
                "composition",
                "of",
                "the",
                "relations",
                "in",
                "the",
                "dataset,",
                "as",
                "to",
                "whether",
                "these",
                "relations",
                "are",
                "implicit",
                "or",
                "explicit.",
                "While",
                "there",
                "is",
                "no",
                "accepted",
                "definition",
                "of",
                "explicit-implicit",
                "distinction",
                "in",
                "the",
                "literature",
                "#REF",
                ",",
                "here",
                "we",
                "adapt",
                "a",
                "definition",
                "originally",
                "used",
                "by",
                "#TARGET_REF",
                "for",
                "another",
                "phenomenon,",
                "implicit",
                "arguments:",
                "14",
                "In",
                "an",
                "implicit",
                "relation",
                "the",
                "anchor",
                "and",
                "the",
                "complement",
                "are",
                "not",
                "syntactically",
                "connected",
                "to",
                "each",
                "other",
                "and",
                "might",
                "not",
                "even",
                "appear",
                "in",
                "the",
                "same",
                "sentence.",
                "This",
                "implies,",
                "for",
                "example,",
                "that",
                "any",
                "inter-sentential",
                "relations",
                "are",
                "implicit",
                "15",
                ",",
                "while",
                "relations",
                "within",
                "one",
                "sentence",
                "can",
                "be",
                "either",
                "implicit",
                "or",
                "explicit.",
                "We",
                "sample",
                "three",
                "documents",
                "from",
                "the",
                "test-set,",
                "containing",
                "590",
                "links",
                "in",
                "total,",
                "and",
                "count",
                "the",
                "number",
                "of",
                "relations",
                "of",
                "each",
                "type.",
                "Our",
                "manual",
                "analysis",
                "reveals",
                "that",
                "89.8%",
                "of",
                "the",
                "relations",
                "are",
                "implicit."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Explicit vs. Implicit NP Relations Next, we analyze the composition of the relations in the dataset, as to whether these relations are implicit or explicit. While there is no accepted definition of explicit-implicit distinction in the literature #REF , here we adapt a definition originally used by #TARGET_REF for another phenomenon, implicit arguments: 14 In an implicit relation the anchor and the complement are not syntactically connected to each other and might not even appear in the same sentence. This implies, for example, that any inter-sentential relations are implicit 15 , while relations within one sentence can be either implicit or explicit. We sample three documents from the test-set, containing 590 links in total, and count the number of relations of each type. Our manual analysis reveals that 89.8% of the relations are implicit.",
        "output": "Explicit vs. Implicit NP Relations Next, we analyze the composition of the relations in the dataset, as to whether these relations are implicit or explicit. <PERCEPT> While there is no accepted definition of explicit-implicit distinction in the literature #REF , here we adapt a definition originally used by #TARGET_REF for another phenomenon, implicit arguments: </PERCEPT> 14 In an implicit relation the anchor and the complement are not syntactically connected to each other and might not even appear in the same sentence. This implies, for example, that any inter-sentential relations are implicit 15 , while relations within one sentence can be either implicit or explicit. We sample three documents from the test-set, containing 590 links in total, and count the number of relations of each type. Our manual analysis reveals that 89.8% of the relations are implicit."
    },
    {
        "gold": {
            "text": [
                "Proposed",
                "model",
                "I",
                "propose",
                "a",
                "log-bilinear",
                "model",
                "#TARGET_REF",
                "using",
                "word",
                "embeddings",
                "as",
                "input:",
                "1"
            ],
            "context": [
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                0
            ]
        },
        "input": "Proposed model I propose a log-bilinear model #TARGET_REF using word embeddings as input: 1",
        "output": "<PERCEPT> Proposed model I propose a </PERCEPT> <INFO> log-bilinear model #TARGET_REF </INFO> <PERCEPT> using word embeddings as input: </PERCEPT> 1"
    },
    {
        "gold": {
            "text": [
                "beyond",
                "the",
                "test",
                "set",
                "whose",
                "prediction",
                "can",
                "be",
                "altered.",
                "Robustness",
                "beyond",
                "the",
                "test",
                "set.",
                "Several",
                "works",
                "have",
                "studied",
                "model",
                "robustness",
                "beyond",
                "test",
                "sets",
                "but",
                "mostly",
                "focused",
                "on",
                "computer",
                "vision",
                "tasks.",
                "#REF",
                "demonstrate",
                "that",
                "a",
                "robustly",
                "trained",
                "model",
                "could",
                "still",
                "be",
                "vulnerable",
                "to",
                "small",
                "perturbations",
                "if",
                "the",
                "input",
                "comes",
                "from",
                "a",
                "distribution",
                "only",
                "slightly",
                "different",
                "than",
                "a",
                "normal",
                "test",
                "set",
                "(e.g.,",
                "images",
                "with",
                "slightly",
                "different",
                "contrasts).",
                "#REF",
                "study",
                "more",
                "sources",
                "of",
                "common",
                "corruptions",
                "such",
                "as",
                "brightness,",
                "motion",
                "blur",
                "and",
                "fog.",
                "Unlike",
                "in",
                "computer",
                "vision",
                "where",
                "simple",
                "image",
                "transformations",
                "can",
                "be",
                "used,",
                "in",
                "our",
                "natural",
                "language",
                "setting,",
                "generating",
                "a",
                "valid",
                "example",
                "beyond",
                "test",
                "set",
                "is",
                "more",
                "challenging",
                "because",
                "language",
                "semantics",
                "and",
                "grammar",
                "must",
                "be",
                "maintained.",
                "Counterfactual",
                "fairness.",
                "#REF",
                "propose",
                "counterfactual",
                "fairness",
                "and",
                "consider",
                "a",
                "model",
                "fair",
                "if",
                "changing",
                "the",
                "protected",
                "attributes",
                "does",
                "not",
                "affect",
                "the",
                "distribution",
                "of",
                "prediction.",
                "We",
                "follow",
                "the",
                "definition",
                "and",
                "focus",
                "on",
                "evaluating",
                "the",
                "counterfactual",
                "bias",
                "between",
                "pairs",
                "of",
                "protected",
                "tokens.",
                "Existing",
                "literature",
                "quantifies",
                "fairness",
                "on",
                "a",
                "test",
                "dataset",
                "or",
                "through",
                "templates",
                "#REF",
                ".",
                "For",
                "instance,",
                "#REF",
                "quantify",
                "the",
                "absolute",
                "counterfactual",
                "token",
                "fairness",
                "gap",
                "on",
                "the",
                "test",
                "set,",
                "Prabhakaran",
                "et",
                "al.",
                "(",
                "2019)",
                "study",
                "perturbation",
                "sensitivity",
                "for",
                "named",
                "entities",
                "on",
                "a",
                "given",
                "set",
                "of",
                "corpus.",
                "#REF",
                ",",
                "#REF",
                "study",
                "how",
                "language",
                "generation",
                "models",
                "respond",
                "differently",
                "to",
                "prompt",
                "sentences",
                "containing",
                "mentions",
                "of",
                "different",
                "demographic",
                "groups.",
                "In",
                "contrast,",
                "our",
                "method",
                "quantifies",
                "the",
                "bias",
                "on",
                "the",
                "constructed",
                "neighborhood."
            ],
            "context": [
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "beyond the test set whose prediction can be altered. Robustness beyond the test set. Several works have studied model robustness beyond test sets but mostly focused on computer vision tasks. #REF demonstrate that a robustly trained model could still be vulnerable to small perturbations if the input comes from a distribution only slightly different than a normal test set (e.g., images with slightly different contrasts). #REF study more sources of common corruptions such as brightness, motion blur and fog. Unlike in computer vision where simple image transformations can be used, in our natural language setting, generating a valid example beyond test set is more challenging because language semantics and grammar must be maintained. Counterfactual fairness. #REF propose counterfactual fairness and consider a model fair if changing the protected attributes does not affect the distribution of prediction. We follow the definition and focus on evaluating the counterfactual bias between pairs of protected tokens. Existing literature quantifies fairness on a test dataset or through templates #REF . For instance, #REF quantify the absolute counterfactual token fairness gap on the test set, Prabhakaran et al. ( 2019) study perturbation sensitivity for named entities on a given set of corpus. #REF , #REF study how language generation models respond differently to prompt sentences containing mentions of different demographic groups. In contrast, our method quantifies the bias on the constructed neighborhood.",
        "output": "<PERCEPT> beyond the test set whose prediction can be altered. </PERCEPT> Robustness beyond the test set. Several works have studied model robustness beyond test sets but mostly focused on computer vision tasks. #REF demonstrate that a robustly trained model could still be vulnerable to small perturbations if the input comes from a distribution only slightly different than a normal test set (e.g., images with slightly different contrasts). #REF study more sources of common corruptions such as brightness, motion blur and fog. Unlike in computer vision where simple image transformations can be used, in our natural language setting, generating a valid example beyond test set is more challenging because language semantics and grammar must be maintained. Counterfactual fairness. #REF propose counterfactual fairness and consider a model fair if changing the protected attributes does not affect the distribution of prediction. We follow the definition and focus on evaluating the counterfactual bias between pairs of protected tokens. Existing literature quantifies fairness on a test dataset or through templates #REF . For instance, #REF quantify the absolute counterfactual token fairness gap on the test set, Prabhakaran et al. ( 2019) study perturbation sensitivity for named entities on a given set of corpus. #REF , #REF study how language generation models respond differently to prompt sentences containing mentions of different demographic groups. In contrast, our method quantifies the bias on the constructed neighborhood."
    },
    {
        "gold": {
            "text": [
                "One",
                "of",
                "the",
                "central",
                "subjects",
                "of",
                "artificial",
                "intelligence",
                "research",
                "has",
                "long",
                "been",
                "the",
                "development",
                "of",
                "agents",
                "that",
                "play",
                "various",
                "games",
                "at",
                "the",
                "human",
                "level",
                "or",
                "better.",
                "Most",
                "studies",
                "in",
                "the",
                "field",
                "focus",
                "on",
                "combinatorial",
                "games,",
                "that",
                "can",
                "be",
                "easily",
                "formalized",
                "mathematically,",
                "such",
                "as",
                "chess",
                "and",
                "go",
                "(see,",
                "for",
                "example,",
                "#TARGET_REF",
                ".",
                "The",
                "popular",
                "board",
                "game",
                "Codenames",
                "is",
                "different",
                "from",
                "these",
                "in",
                "many",
                "aspects",
                "and",
                "may",
                "provide",
                "an",
                "excellent",
                "experimental",
                "ground",
                "in",
                "areas",
                "such",
                "as",
                "predicting",
                "human",
                "behavior",
                "or",
                "implementing",
                "human-machine",
                "cooperation."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "One of the central subjects of artificial intelligence research has long been the development of agents that play various games at the human level or better. Most studies in the field focus on combinatorial games, that can be easily formalized mathematically, such as chess and go (see, for example, #TARGET_REF . The popular board game Codenames is different from these in many aspects and may provide an excellent experimental ground in areas such as predicting human behavior or implementing human-machine cooperation.",
        "output": "One of the central subjects of artificial intelligence research has long been the development of agents that play various games at the human level or better. <PERCEPT> Most studies in the field focus on combinatorial games, that can be easily formalized mathematically, </PERCEPT> <BACK> such as chess and go (see, for example, #TARGET_REF . </BACK> The popular board game Codenames is different from these in many aspects and may provide an excellent experimental ground in areas such as predicting human behavior or implementing human-machine cooperation."
    },
    {
        "gold": {
            "text": [
                "We",
                "built",
                "a",
                "Transformer-based",
                "ASR",
                "system",
                "using",
                "the",
                "ESPnet",
                "toolkit",
                "#REF",
                ".",
                "The",
                "Transformer",
                "architecture",
                "and",
                "hyper-parameters",
                "for",
                "training/decoding",
                "are",
                "based",
                "on",
                "existing",
                "recipes",
                "in",
                "ESPnet.",
                "We",
                "investigated",
                "three",
                "models:",
                "selfattention-based",
                "CTC",
                "#TARGET_REF",
                ",",
                "the",
                "Transformer",
                "#REF",
                ",",
                "and",
                "a",
                "hybrid",
                "Transformer",
                "trained",
                "with",
                "an",
                "auxiliary",
                "CTC",
                "objective",
                "(Transformer+CTC)",
                "#REF",
                ".",
                "The",
                "CTC",
                "model",
                "was",
                "used",
                "in",
                "prior",
                "studies",
                "based",
                "on",
                "O2O",
                "models,",
                "e.g.,",
                "#REF",
                ".",
                "During",
                "training,",
                "the",
                "CTC",
                "model",
                "was",
                "regularized",
                "with",
                "the",
                "Transformer",
                "decoder",
                "in",
                "the",
                "multitask",
                "learning",
                "fashion",
                "similar",
                "to",
                "Transformer+CTC.",
                "Such",
                "regularization",
                "techniques",
                "yield",
                "a",
                "significant",
                "improvement",
                "over",
                "a",
                "pure",
                "CTC",
                "baseline",
                "#REF",
                "."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "We built a Transformer-based ASR system using the ESPnet toolkit #REF . The Transformer architecture and hyper-parameters for training/decoding are based on existing recipes in ESPnet. We investigated three models: selfattention-based CTC #TARGET_REF , the Transformer #REF , and a hybrid Transformer trained with an auxiliary CTC objective (Transformer+CTC) #REF . The CTC model was used in prior studies based on O2O models, e.g., #REF . During training, the CTC model was regularized with the Transformer decoder in the multitask learning fashion similar to Transformer+CTC. Such regularization techniques yield a significant improvement over a pure CTC baseline #REF .",
        "output": "We built a Transformer-based ASR system using the ESPnet toolkit #REF . The Transformer architecture and hyper-parameters for training/decoding are based on existing recipes in ESPnet. <PERCEPT> We investigated three models: </PERCEPT> <INFO> selfattention-based CTC #TARGET_REF , </INFO> <PERCEPT> the Transformer #REF , and a hybrid Transformer trained with an auxiliary CTC objective (Transformer+CTC) #REF . </PERCEPT> <BACK> The CTC model was used in prior studies based on O2O models, e.g., #REF . During training, the CTC model was regularized with the Transformer decoder in the multitask learning fashion similar to Transformer+CTC. </BACK> Such regularization techniques yield a significant improvement over a pure CTC baseline #REF ."
    },
    {
        "gold": {
            "text": [
                "Evaluation",
                "For",
                "this",
                "task,",
                "we",
                "have",
                "classified",
                "the",
                "generated",
                "sentences",
                "in",
                "terms",
                "of",
                "their",
                "sentiment",
                "using",
                "a",
                "Bidirectional-LSTM",
                "as",
                "classifier.",
                "In",
                "addition,",
                "we",
                "have",
                "evaluated",
                "two",
                "quality",
                "metrics:",
                "1)",
                "the",
                "novelty",
                "of",
                "each",
                "generated",
                "sentence",
                "(Eq.",
                "5)",
                "using",
                "the",
                "definition",
                "from",
                "#TARGET_REF",
                ",",
                "where",
                "JS",
                "is",
                "the",
                "Jaccard",
                "similarity",
                "and",
                "C",
                "j",
                "are",
                "the",
                "training",
                "set",
                "sentences.",
                "The",
                "novelty",
                "measures",
                "the",
                "diversity",
                "between",
                "the",
                "generated",
                "data",
                "and",
                "the",
                "training",
                "corpus,",
                "and",
                "2)",
                "the",
                "diversity",
                "metric",
                "(Eq.",
                "6),",
                "a",
                "measure",
                "of",
                "the",
                "model's",
                "ability",
                "to",
                "generate",
                "diverse",
                "sentences",
                "and",
                "avoid",
                "mode",
                "collapse."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                3,
                3,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Evaluation For this task, we have classified the generated sentences in terms of their sentiment using a Bidirectional-LSTM as classifier. In addition, we have evaluated two quality metrics: 1) the novelty of each generated sentence (Eq. 5) using the definition from #TARGET_REF , where JS is the Jaccard similarity and C j are the training set sentences. The novelty measures the diversity between the generated data and the training corpus, and 2) the diversity metric (Eq. 6), a measure of the model's ability to generate diverse sentences and avoid mode collapse.",
        "output": "Evaluation For this task, we have classified the generated sentences in terms of their sentiment using a Bidirectional-LSTM as classifier. In addition, <PERCEPT> we have evaluated two quality metrics: 1) the novelty of each generated sentence </PERCEPT> <BACK> (Eq. 5) </BACK> <PERCEPT> using the </PERCEPT> <INFO> definition from #TARGET_REF , where JS is the Jaccard similarity and C j are the training set sentences. </INFO> <BACK> The novelty measures the diversity between the generated data and the training corpus, </BACK> and 2) the diversity metric (Eq. 6), a measure of the model's ability to generate diverse sentences and avoid mode collapse."
    },
    {
        "gold": {
            "text": [
                "3.",
                "#TARGET_REF",
                "establishes",
                "the",
                "current",
                "state",
                "of",
                "the",
                "art",
                "for",
                "data",
                "driven",
                "models",
                "in",
                "temporal",
                "and",
                "event",
                "extent",
                "information",
                "in",
                "Italian.",
                "The",
                "system",
                "is",
                "a",
                "modification",
                "of",
                "the",
                "TipSem",
                "system.",
                "We",
                "compares",
                "our",
                "models",
                "to",
                "their",
                "reported",
                "scores.",
                "However,",
                "the",
                "corpus",
                "used",
                "in",
                "#REF",
                "is",
                "the",
                "Ita-TimeBank",
                "which",
                "has",
                "been",
                "augmented",
                "with",
                "further",
                "annotations",
                "and",
                "resources,",
                "while",
                "our",
                "system",
                "uses",
                "just",
                "the",
                "Ita-TimeBank",
                "for",
                "event",
                "extraction."
            ],
            "context": [
                0,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "3. #TARGET_REF establishes the current state of the art for data driven models in temporal and event extent information in Italian. The system is a modification of the TipSem system. We compares our models to their reported scores. However, the corpus used in #REF is the Ita-TimeBank which has been augmented with further annotations and resources, while our system uses just the Ita-TimeBank for event extraction.",
        "output": "3. <INFO> #TARGET_REF establishes the current state of the art for data driven models in temporal and event extent information in Italian. </INFO> <PERCEPT> The system is a modification of the TipSem system. We compares our models to their reported scores. </PERCEPT> However, the corpus used in #REF is the Ita-TimeBank which has been augmented with further annotations and resources, while our system uses just the Ita-TimeBank for event extraction."
    },
    {
        "gold": {
            "text": [
                "(b)",
                "One-to-one",
                "model",
                "with",
                "a",
                "conditional",
                "chain",
                "mapping.\"",
                "!\"#",
                "#",
                "\"",
                "#\"#",
                "#",
                "$",
                "#",
                "\"",
                "$",
                "\"#",
                "!!",
                "\"",
                "!\"!",
                "#",
                "\"",
                "#\"!",
                "#",
                "\"",
                "!\"#",
                "#",
                "\"",
                "#\"#",
                "#",
                "$",
                "#",
                "\"",
                "$",
                "\"",
                "\"!",
                "#",
                "\"",
                "$",
                "!",
                "\"#",
                "!",
                "!\"+##',*",
                "!\"#$%#&amp,'#(-(.()*",
                "!",
                "\"",
                "#",
                "\"",
                "#",
                "$",
                "#",
                "\"(c)",
                "One-to-one",
                "model",
                "with",
                "a",
                "single",
                "sequence.",
                "data,",
                "including",
                "spoken",
                "dialogue",
                "systems",
                "#REF",
                ".",
                "This",
                "study",
                "aims",
                "to",
                "endow",
                "existing",
                "E2E",
                "ASR",
                "models",
                "with",
                "the",
                "ability",
                "to",
                "produce",
                "such",
                "linguistic",
                "annotations.",
                "Prior",
                "work",
                "explored",
                "using",
                "E2E",
                "ASR",
                "systems",
                "to",
                "predict",
                "multiple",
                "kinds",
                "of",
                "labels.",
                "Fig.",
                "1",
                "shows",
                "a",
                "diagram",
                "of",
                "these",
                "systems.",
                "These",
                "approaches",
                "use",
                "one",
                "of",
                "the",
                "following",
                "models:",
                "a",
                "one-to-many",
                "(O2M)",
                "model",
                "#REF",
                ",",
                "a",
                "one-to-one",
                "(O2O)",
                "model",
                "with",
                "a",
                "conditional",
                "chain",
                "mapping",
                "#REF",
                ",",
                "or",
                "an",
                "O2O",
                "model",
                "with",
                "a",
                "single",
                "sequence",
                "#TARGET_REF",
                "."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1
            ]
        },
        "input": "(b) One-to-one model with a conditional chain mapping.\" !\"# # \" #\"# # $ # \" $ \"# !! \" !\"! # \" #\"! # \" !\"# # \" #\"# # $ # \" $ \" \"! # \" $ ! \"# ! !\"+##',* !\"#$%#&amp,'#(-(.()* ! \" # \" # $ # \"(c) One-to-one model with a single sequence. data, including spoken dialogue systems #REF . This study aims to endow existing E2E ASR models with the ability to produce such linguistic annotations. Prior work explored using E2E ASR systems to predict multiple kinds of labels. Fig. 1 shows a diagram of these systems. These approaches use one of the following models: a one-to-many (O2M) model #REF , a one-to-one (O2O) model with a conditional chain mapping #REF , or an O2O model with a single sequence #TARGET_REF .",
        "output": "(b) One-to-one model with a conditional chain mapping.\" !\"# # \" #\"# # $ # \" $ \"# !! \" !\"! # \" #\"! # \" !\"# # \" #\"# # $ # \" $ \" \"! # \" $ ! \"# ! !\"+##',* !\"#$%#&amp,'#(-(.()* ! \" # \" # $ # \"(c) One-to-one model with a single sequence. data, including spoken dialogue systems #REF . This study aims to endow existing E2E ASR models with the ability to produce such linguistic annotations. <BACK> Prior work explored using E2E ASR systems to predict multiple kinds of labels. Fig. 1 shows a diagram of these systems. These approaches use one of the following models: a one-to-many (O2M) model #REF , a one-to-one (O2O) model with a conditional chain mapping #REF , or </BACK> <INFO> an O2O model with a single sequence #TARGET_REF . </INFO>"
    },
    {
        "gold": {
            "text": [
                "where",
                "X",
                "∈",
                "{A,",
                "B,",
                "C,",
                "D}).",
                "We",
                "consider",
                "two",
                "settings",
                "where",
                "each",
                "generative",
                "factor",
                "is",
                "embedded",
                "in",
                "a",
                "single",
                "dimension",
                "(denoted",
                "by",
                "Ex.1),",
                "or",
                "two",
                "dimensions",
                "(denoted",
                "by",
                "Ex.2).",
                "In",
                "each",
                "setting",
                "we",
                "uniformly",
                "sample",
                "20",
                "values",
                "from",
                "-1",
                "to",
                "1",
                "to",
                "represent",
                "20",
                "assignments",
                "per",
                "factor",
                "and",
                "use",
                "them",
                "to",
                "allocate",
                "the",
                "assignments",
                "into",
                "distinctive",
                "bins",
                "per",
                "each",
                "corresponding",
                "dimension.",
                "By",
                "concatenating",
                "dimensions",
                "for",
                "each",
                "generative",
                "factor,",
                "we",
                "construct",
                "two",
                "ideal",
                "disentangled",
                "representations",
                "for",
                "data",
                "points",
                "in",
                "this",
                "toy",
                "dataset,",
                "amounting",
                "to",
                "4",
                "and",
                "8",
                "dimensional",
                "representations,",
                "respectively.",
                "Using",
                "these",
                "representations",
                "(skipping",
                "the",
                "encoding",
                "step),",
                "we",
                "measured",
                "the",
                "above",
                "metrics.",
                "Table",
                "1",
                "(Ex.1",
                "and",
                "Ex.2",
                "columns)",
                "summarises",
                "the",
                "results,",
                "illustrating",
                "that",
                "out",
                "of",
                "the",
                "6",
                "metrics,",
                "#REF",
                ",",
                "Ridgeway",
                "and",
                "Mozer",
                "(2018),",
                "#REF",
                "are",
                "the",
                "only",
                "ones",
                "that",
                "reach",
                "the",
                "potential",
                "maximum",
                "(i.e.,",
                "100),",
                "while",
                "#TARGET_REF",
                "exhibits",
                "its",
                "sensitivity",
                "towards",
                "completeness",
                "when",
                "we",
                "allocate",
                "two",
                "dimensions",
                "per",
                "factors."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                2,
                1,
                1,
                1,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2
            ]
        },
        "input": "where X ∈ {A, B, C, D}). We consider two settings where each generative factor is embedded in a single dimension (denoted by Ex.1), or two dimensions (denoted by Ex.2). In each setting we uniformly sample 20 values from -1 to 1 to represent 20 assignments per factor and use them to allocate the assignments into distinctive bins per each corresponding dimension. By concatenating dimensions for each generative factor, we construct two ideal disentangled representations for data points in this toy dataset, amounting to 4 and 8 dimensional representations, respectively. Using these representations (skipping the encoding step), we measured the above metrics. Table 1 (Ex.1 and Ex.2 columns) summarises the results, illustrating that out of the 6 metrics, #REF , Ridgeway and Mozer (2018), #REF are the only ones that reach the potential maximum (i.e., 100), while #TARGET_REF exhibits its sensitivity towards completeness when we allocate two dimensions per factors.",
        "output": "where X ∈ {A, B, C, D}). We consider two settings where each generative factor is embedded in a single dimension (denoted by Ex.1), or two dimensions (denoted by Ex.2). In each setting we uniformly sample 20 values from -1 to 1 to represent 20 assignments per factor and use them to allocate the assignments into distinctive bins per each corresponding dimension. By concatenating dimensions for each generative factor, we construct two ideal disentangled representations for data points in this toy dataset, amounting to 4 and 8 dimensional representations, respectively. Using these representations (skipping the encoding step), we measured the above metrics. <BACK> Table 1 (Ex.1 and Ex.2 columns) summarises the results, illustrating that out of the 6 metrics, #REF , Ridgeway and Mozer (2018), #REF are the only ones that reach the potential maximum (i.e., 100), </BACK> <PERCEPT> while </PERCEPT> <INFO> #TARGET_REF exhibits its sensitivity towards completeness </INFO> <PERCEPT> when we allocate two dimensions per factors. </PERCEPT>"
    },
    {
        "gold": {
            "text": [
                "These",
                "methods",
                "directly",
                "incorporate",
                "handengineered",
                "features",
                "and",
                "personal",
                "traits",
                "of",
                "users",
                "or",
                "their",
                "communities",
                "in",
                "order",
                "to",
                "model",
                "the",
                "likelihood",
                "of",
                "abusive",
                "language",
                "in",
                "the",
                "users'",
                "comments,",
                "a",
                "process",
                "known",
                "as",
                "profiling",
                "#REF",
                ".",
                "#TARGET_REF",
                "included",
                "the",
                "age",
                "of",
                "users",
                "alongside",
                "other",
                "traditional",
                "lexicon-based",
                "features",
                "to",
                "detect",
                "cyber-bullying,",
                "while",
                "Galán-García",
                "et",
                "al.",
                "(",
                "2016)",
                "utilized",
                "the",
                "time",
                "of",
                "publication,",
                "geo-position,",
                "and",
                "language",
                "in",
                "the",
                "profile",
                "of",
                "Twitter",
                "users.",
                "#REF",
                "exploited",
                "gender",
                "of",
                "Twitter",
                "users",
                "on",
                "top",
                "of",
                "character",
                "n-gram",
                "counts",
                "to",
                "improve",
                "detection",
                "of",
                "sexism",
                "and",
                "racism",
                "in",
                "a",
                "dataset",
                "comprising",
                "racist,",
                "sexist",
                "and",
                "benign",
                "tweets",
                "-they",
                "noted",
                "that",
                "the",
                "F",
                "1",
                "increased",
                "slightly",
                "from",
                "73.89%",
                "to",
                "73.93%",
                "when",
                "the",
                "gender",
                "feature",
                "was",
                "included.",
                "Using",
                "the",
                "same",
                "setup,",
                "Unsvåg",
                "and",
                "Gambäck",
                "(2018)",
                "showed",
                "that",
                "the",
                "inclusion",
                "of",
                "social",
                "community",
                "(i.e.,",
                "number",
                "of",
                "followers",
                "and",
                "friends)",
                "and",
                "activity",
                "(i.e.,",
                "number",
                "of",
                "status",
                "updates",
                "and",
                "favorites)",
                "features",
                "of",
                "users",
                "alongside",
                "their",
                "gender",
                "further",
                "enhanced",
                "performance",
                "by",
                "3",
                "F",
                "1",
                "points",
                "over",
                "the",
                "n-gram",
                "baseline."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "These methods directly incorporate handengineered features and personal traits of users or their communities in order to model the likelihood of abusive language in the users' comments, a process known as profiling #REF . #TARGET_REF included the age of users alongside other traditional lexicon-based features to detect cyber-bullying, while Galán-García et al. ( 2016) utilized the time of publication, geo-position, and language in the profile of Twitter users. #REF exploited gender of Twitter users on top of character n-gram counts to improve detection of sexism and racism in a dataset comprising racist, sexist and benign tweets -they noted that the F 1 increased slightly from 73.89% to 73.93% when the gender feature was included. Using the same setup, Unsvåg and Gambäck (2018) showed that the inclusion of social community (i.e., number of followers and friends) and activity (i.e., number of status updates and favorites) features of users alongside their gender further enhanced performance by 3 F 1 points over the n-gram baseline.",
        "output": "These methods directly incorporate handengineered features and personal traits of users or their communities in order to model the likelihood of abusive language in the users' comments, a process known as profiling #REF . <INFO> #TARGET_REF included the age of users alongside other traditional lexicon-based features to detect cyber-bullying, </INFO> <BACK> while Galán-García et al. ( 2016) utilized the time of publication, geo-position, and language in the profile of Twitter users. </BACK> #REF exploited gender of Twitter users on top of character n-gram counts to improve detection of sexism and racism in a dataset comprising racist, sexist and benign tweets -they noted that the F 1 increased slightly from 73.89% to 73.93% when the gender feature was included. Using the same setup, Unsvåg and Gambäck (2018) showed that the inclusion of social community (i.e., number of followers and friends) and activity (i.e., number of status updates and favorites) features of users alongside their gender further enhanced performance by 3 F 1 points over the n-gram baseline."
    },
    {
        "gold": {
            "text": [
                "Hope",
                "and",
                "Keller",
                "(2013),",
                "for",
                "example,",
                "use",
                "a",
                "graph",
                "of",
                "co-occurrences",
                "for",
                "word",
                "sense",
                "induction.",
                "Later",
                "#TARGET_REF",
                "use",
                "a",
                "similar",
                "method",
                "to",
                "disambiguate",
                "word",
                "embedding",
                "models."
            ],
            "context": [
                3,
                3,
                3,
                3,
                0,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1
            ]
        },
        "input": "Hope and Keller (2013), for example, use a graph of co-occurrences for word sense induction. Later #TARGET_REF use a similar method to disambiguate word embedding models.",
        "output": "<BACK> Hope and Keller (2013), </BACK> for example, <BACK> use a graph of co-occurrences for word sense induction. </BACK> <INFO> Later #TARGET_REF use a similar method to disambiguate word embedding models. </INFO>"
    },
    {
        "gold": {
            "text": [
                "A",
                "CSC",
                "captures",
                "ordering",
                "constraints",
                "of",
                "natural",
                "language,",
                "and",
                "it",
                "roughly",
                "corresponds",
                "to",
                "phrase",
                "structure",
                "rules.",
                "CSCs",
                "can",
                "be",
                "used",
                "to",
                "represent",
                "syntax",
                "and",
                "semantics",
                "of",
                "sentences",
                "at",
                "different",
                "levels",
                "of",
                "abstraction",
                "from",
                "instances",
                "of",
                "surface",
                "sequence",
                "to",
                "linguistically",
                "motivated",
                "grammar",
                "such",
                "as",
                "Lexical-Functional",
                "Grammar",
                "(LFG)",
                "#TARGET_REF",
                ".",
                "As",
                "shown",
                "in",
                "figure",
                "2,",
                "a",
                "CSC",
                "consists",
                "of",
                "a",
                "root",
                "node",
                "(CSR),",
                "element",
                "nodes",
                "(CSE),",
                "a",
                "FIRST",
                "link,",
                "a",
                "LAST",
                "link,",
                "NEXT",
                "link(s)",
                "and",
                "ROLE",
                "links.",
                "A",
                "CSR",
                "is",
                "a",
                "representative",
                "node",
                "for",
                "the",
                "meaning",
                "of",
                "the",
                "entire",
                "CSC",
                "structure,",
                "CSRs",
                "are",
                "connected",
                "to",
                "their",
                "designated",
                "interlingual",
                "concepts",
                "by",
                "ENG",
                "or",
                "JPN.",
                "Each",
                "CSC",
                "has",
                "one",
                "or",
                "more",
                "CSEs",
                "linked",
                "to",
                "a",
                "CSR",
                "by",
                "ROLE",
                "links.",
                "The",
                "ordering",
                "constraints",
                "between",
                "two",
                "concept",
                "sequence",
                "element",
                "nodes",
                "are",
                "represented",
                "by",
                "NEXT",
                "link.",
                "FIRST",
                "and",
                "LAST",
                "links",
                "in",
                "each",
                "CSC",
                "points",
                "to",
                "the",
                "first",
                "and",
                "last",
                "elements,",
                "respectively.",
                "Also,",
                "each",
                "CSE",
                "represents",
                "the",
                "relevant",
                "case",
                "role,",
                "and",
                "the",
                "case",
                "role",
                "has",
                "a",
                "selectional",
                "restriction.",
                "Since",
                "we",
                "want",
                "to",
                "avoid",
                "heavy",
                "symbolic",
                "operations",
                "during",
                "parsing,",
                "ROLE",
                "links",
                "and",
                "associated",
                "constraint",
                "links",
                "are",
                "used",
                "instead",
                "of",
                "performing",
                "type",
                "and",
                "value",
                "consistency",
                "check",
                "by",
                "unification.",
                "Therefore",
                "each",
                "CSE",
                "is",
                "used",
                "for",
                "both",
                "enforcing",
                "the",
                "ordering",
                "constraint",
                "and",
                "capturing",
                "semantic",
                "information."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                1,
                1,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "A CSC captures ordering constraints of natural language, and it roughly corresponds to phrase structure rules. CSCs can be used to represent syntax and semantics of sentences at different levels of abstraction from instances of surface sequence to linguistically motivated grammar such as Lexical-Functional Grammar (LFG) #TARGET_REF . As shown in figure 2, a CSC consists of a root node (CSR), element nodes (CSE), a FIRST link, a LAST link, NEXT link(s) and ROLE links. A CSR is a representative node for the meaning of the entire CSC structure, CSRs are connected to their designated interlingual concepts by ENG or JPN. Each CSC has one or more CSEs linked to a CSR by ROLE links. The ordering constraints between two concept sequence element nodes are represented by NEXT link. FIRST and LAST links in each CSC points to the first and last elements, respectively. Also, each CSE represents the relevant case role, and the case role has a selectional restriction. Since we want to avoid heavy symbolic operations during parsing, ROLE links and associated constraint links are used instead of performing type and value consistency check by unification. Therefore each CSE is used for both enforcing the ordering constraint and capturing semantic information.",
        "output": "A CSC captures ordering constraints of natural language, and it roughly corresponds to phrase structure rules. <BACK> CSCs can be used to represent syntax and semantics of sentences at different levels of abstraction from instances of surface sequence to linguistically motivated grammar such as </BACK> <INFO> Lexical-Functional Grammar (LFG) #TARGET_REF </INFO> . As shown in figure 2, a CSC consists of a root node (CSR), element nodes (CSE), a FIRST link, a LAST link, NEXT link(s) and ROLE links. A CSR is a representative node for the meaning of the entire CSC structure, CSRs are connected to their designated interlingual concepts by ENG or JPN. Each CSC has one or more CSEs linked to a CSR by ROLE links. The ordering constraints between two concept sequence element nodes are represented by NEXT link. FIRST and LAST links in each CSC points to the first and last elements, respectively. Also, each CSE represents the relevant case role, and the case role has a selectional restriction. Since we want to avoid heavy symbolic operations during parsing, ROLE links and associated constraint links are used instead of performing type and value consistency check by unification. Therefore each CSE is used for both enforcing the ordering constraint and capturing semantic information."
    },
    {
        "gold": {
            "text": [
                "In",
                "all",
                "experiments",
                "involving",
                "BERT,",
                "we",
                "use",
                "the",
                "BERT",
                "Base-uncased",
                "model",
                "#REF",
                ".",
                "It",
                "has",
                "12",
                "layers",
                "and",
                "each",
                "layer",
                "contains",
                "12",
                "attention",
                "heads,",
                "summing",
                "to",
                "144",
                "attention",
                "heads.",
                "We",
                "fine-tune",
                "and",
                "evaluate",
                "the",
                "pre-trained",
                "model",
                "2",
                "on",
                "sentence",
                "entailment",
                "task",
                "MNLI-M,",
                "the",
                "question",
                "similarity",
                "task",
                "QQP,",
                "the",
                "question-answering",
                "task",
                "QNLI,",
                "and",
                "the",
                "movie",
                "review",
                "task",
                "SST-2",
                "from",
                "the",
                "GLUE",
                "Benchmark",
                "#TARGET_REF",
                ".",
                "We",
                "report",
                "accuracies",
                "on",
                "the",
                "official",
                "development",
                "sets",
                "of",
                "the",
                "considered",
                "GLUE",
                "tasks.",
                "For",
                "each",
                "of",
                "the",
                "four",
                "GLUE",
                "tasks,",
                "namely",
                "MNLI-M,",
                "QQP,",
                "QNLI",
                "and",
                "SST-2,",
                "we",
                "tried",
                "combinations",
                "of",
                "batch",
                "size",
                "and",
                "learning",
                "rate",
                "from",
                "{8,",
                "16,",
                "32,",
                "64,",
                "128}",
                "and",
                "{2,",
                "3,",
                "4,",
                "5}",
                "×",
                "10",
                "−5",
                "respectively",
                "and",
                "selected",
                "the",
                "best",
                "performing",
                "configuration.",
                "The",
                "exact",
                "hyperparameters",
                "used",
                "for",
                "each",
                "of",
                "the",
                "tasks",
                "have",
                "been",
                "made",
                "available",
                "with",
                "the",
                "code",
                "released",
                "3",
                ".",
                "Each",
                "BERT",
                "experiment",
                "was",
                "run",
                "on",
                "a",
                "single",
                "Cloud",
                "TPU",
                "(v2-8)."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "In all experiments involving BERT, we use the BERT Base-uncased model #REF . It has 12 layers and each layer contains 12 attention heads, summing to 144 attention heads. We fine-tune and evaluate the pre-trained model 2 on sentence entailment task MNLI-M, the question similarity task QQP, the question-answering task QNLI, and the movie review task SST-2 from the GLUE Benchmark #TARGET_REF . We report accuracies on the official development sets of the considered GLUE tasks. For each of the four GLUE tasks, namely MNLI-M, QQP, QNLI and SST-2, we tried combinations of batch size and learning rate from {8, 16, 32, 64, 128} and {2, 3, 4, 5} × 10 −5 respectively and selected the best performing configuration. The exact hyperparameters used for each of the tasks have been made available with the code released 3 . Each BERT experiment was run on a single Cloud TPU (v2-8).",
        "output": "In all experiments involving BERT, we use the BERT Base-uncased model #REF . It has 12 layers and each layer contains 12 attention heads, summing to 144 attention heads. <PERCEPT> We fine-tune and evaluate the pre-trained model 2 on sentence entailment task MNLI-M, the question similarity task QQP, the question-answering task QNLI, and the movie review task SST-2 from the </PERCEPT> <INFO> GLUE Benchmark #TARGET_REF . </INFO> We report accuracies on the official development sets of the considered GLUE tasks. For each of the four GLUE tasks, namely MNLI-M, QQP, QNLI and SST-2, we tried combinations of batch size and learning rate from {8, 16, 32, 64, 128} and {2, 3, 4, 5} × 10 −5 respectively and selected the best performing configuration. The exact hyperparameters used for each of the tasks have been made available with the code released 3 . Each BERT experiment was run on a single Cloud TPU (v2-8)."
    },
    {
        "gold": {
            "text": [
                "Systems",
                "that",
                "perform",
                "shallow",
                "semantic",
                "parsing",
                "on",
                "Chinese",
                "texts",
                "are",
                "likewise",
                "based",
                "on",
                "classifiers",
                "and",
                "trained",
                "on",
                "the",
                "Chinese",
                "PropBank",
                "and",
                "the",
                "bilingual",
                "Chinese-English",
                "Parallel",
                "PropBank",
                "#TARGET_REF",
                ",",
                "#REF",
                ",",
                "#REF",
                ").",
                "It",
                "is",
                "interesting",
                "to",
                "note",
                "that,",
                "despite",
                "the",
                "very",
                "different",
                "characteristics",
                "of",
                "Chinese",
                "verbs",
                "#REF",
                "from",
                "those",
                "in",
                "English,",
                "the",
                "core",
                "algorithm",
                "of",
                "a",
                "shallow",
                "semantic",
                "parser",
                "remains",
                "the",
                "same.",
                "As",
                "was",
                "found",
                "to",
                "be",
                "the",
                "case",
                "in",
                "English,",
                "SVM",
                "classifiers",
                "have",
                "been",
                "found",
                "to",
                "outperform",
                "maximum",
                "entropy",
                "classifiers",
                "for",
                "this",
                "task",
                "#REF",
                ".",
                "The",
                "primary",
                "difference",
                "lies",
                "in",
                "the",
                "feature",
                "set",
                "chosen",
                "to",
                "represent",
                "semantic",
                "information."
            ],
            "context": [
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Systems that perform shallow semantic parsing on Chinese texts are likewise based on classifiers and trained on the Chinese PropBank and the bilingual Chinese-English Parallel PropBank #TARGET_REF , #REF , #REF ). It is interesting to note that, despite the very different characteristics of Chinese verbs #REF from those in English, the core algorithm of a shallow semantic parser remains the same. As was found to be the case in English, SVM classifiers have been found to outperform maximum entropy classifiers for this task #REF . The primary difference lies in the feature set chosen to represent semantic information.",
        "output": "<INFO> Systems that perform shallow semantic parsing on Chinese texts are likewise based on classifiers and trained on the Chinese PropBank and the bilingual Chinese-English Parallel PropBank #TARGET_REF , #REF , #REF ). </INFO> It is interesting to note that, despite the very different characteristics of Chinese verbs #REF from those in English, the core algorithm of a shallow semantic parser remains the same. As was found to be the case in English, SVM classifiers have been found to outperform maximum entropy classifiers for this task #REF . The primary difference lies in the feature set chosen to represent semantic information."
    },
    {
        "gold": {
            "text": [
                "In",
                "the",
                "domain",
                "of",
                "multi-lingual",
                "and",
                "cross",
                "lingual",
                "event",
                "detection,",
                "#REF",
                "uses",
                "a",
                "combination",
                "of",
                "both",
                "LSTMs",
                "and",
                "CNNs",
                "for",
                "creating",
                "a",
                "language",
                "independent",
                "architecture",
                "for",
                "capturing",
                "events,",
                "while",
                "#TARGET_REF",
                "used",
                "stacked",
                "RNNs",
                "for",
                "sequence",
                "labeling",
                "and",
                "a",
                "language",
                "discriminator",
                "to",
                "learn",
                "language",
                "features.",
                "The",
                "latter",
                "architecture",
                "implements",
                "the",
                "use",
                "of",
                "the",
                "character",
                "embeddings,",
                "but",
                "does",
                "not",
                "identify",
                "the",
                "relevant",
                "features",
                "independent",
                "of",
                "the",
                "word",
                "embeddings."
            ],
            "context": [
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3
            ]
        },
        "input": "In the domain of multi-lingual and cross lingual event detection, #REF uses a combination of both LSTMs and CNNs for creating a language independent architecture for capturing events, while #TARGET_REF used stacked RNNs for sequence labeling and a language discriminator to learn language features. The latter architecture implements the use of the character embeddings, but does not identify the relevant features independent of the word embeddings.",
        "output": "<PERCEPT> In the domain of multi-lingual and cross lingual event detection, </PERCEPT> <BACK> #REF uses a combination of both LSTMs and CNNs for creating a language independent architecture for capturing events, </BACK> while <INFO> #TARGET_REF used stacked RNNs for sequence labeling and a language discriminator to learn language features. </INFO> <BACK> The latter architecture implements the use of the character embeddings, but does not identify the relevant features independent of the word embeddings. </BACK>"
    },
    {
        "gold": {
            "text": [
                "To",
                "examine",
                "the",
                "performance",
                "of",
                "these",
                "models",
                "on",
                "real-world",
                "downstream",
                "task",
                "setting,",
                "we",
                "consider",
                "the",
                "classification",
                "task.",
                "For",
                "our",
                "classification",
                "datasets,",
                "we",
                "use",
                "DBpedia",
                "(14",
                "classes)",
                "and",
                "Yahoo",
                "Question",
                "(10",
                "classes)",
                "#TARGET_REF",
                ".",
                "Each",
                "class",
                "of",
                "these",
                "two",
                "datasets",
                "has",
                "(10k,",
                "1k,",
                "1k)",
                "randomly",
                "chosen",
                "sentences",
                "in",
                "(train,",
                "dev,",
                "test)",
                "sets.",
                "We",
                "train",
                "Vanilla-VAE,",
                "β-VAE",
                "(β",
                "=",
                "0.2),",
                "CCI-VAE",
                "(C",
                "=",
                "10),",
                "and",
                "MAT-VAE",
                "(β",
                "=",
                "0.01,",
                "λ",
                "=",
                "0.1)",
                "from",
                "Table",
                "3",
                "on",
                "DBpedia",
                "and",
                "Yahoo",
                "(without",
                "the",
                "labels),",
                "then",
                "freeze",
                "the",
                "trained",
                "encoders",
                "and",
                "place",
                "a",
                "classifier",
                "on",
                "top",
                "to",
                "use",
                "the",
                "mean",
                "vector",
                "representations",
                "from",
                "the",
                "encoder",
                "as",
                "a",
                "feature",
                "to",
                "train",
                "a",
                "classifier."
            ],
            "context": [
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "To examine the performance of these models on real-world downstream task setting, we consider the classification task. For our classification datasets, we use DBpedia (14 classes) and Yahoo Question (10 classes) #TARGET_REF . Each class of these two datasets has (10k, 1k, 1k) randomly chosen sentences in (train, dev, test) sets. We train Vanilla-VAE, β-VAE (β = 0.2), CCI-VAE (C = 10), and MAT-VAE (β = 0.01, λ = 0.1) from Table 3 on DBpedia and Yahoo (without the labels), then freeze the trained encoders and place a classifier on top to use the mean vector representations from the encoder as a feature to train a classifier.",
        "output": "<BACK> To examine the performance of these models on real-world downstream task setting, we consider the classification task. </BACK> <PERCEPT> For our classification datasets, we use </PERCEPT> <INFO> DBpedia (14 classes) and Yahoo Question (10 classes) #TARGET_REF . </INFO> <PERCEPT> Each class of these two datasets has (10k, 1k, 1k) randomly chosen sentences in (train, dev, test) sets. </PERCEPT> We train Vanilla-VAE, β-VAE (β = 0.2), CCI-VAE (C = 10), and MAT-VAE (β = 0.01, λ = 0.1) from Table 3 on DBpedia and Yahoo (without the labels), then freeze the trained encoders and place a classifier on top to use the mean vector representations from the encoder as a feature to train a classifier."
    },
    {
        "gold": {
            "text": [
                "We",
                "initialize",
                "attribute",
                "embeddings",
                "with",
                "Glove",
                "#TARGET_REF",
                "word",
                "embeddings.",
                "For",
                "an",
                "entity,",
                "we",
                "take",
                "all",
                "the",
                "known",
                "attributes",
                "from",
                "K",
                "e",
                ".",
                "The",
                "representation",
                "of",
                "each",
                "entity",
                "is",
                "the",
                "weighted",
                "sum",
                "of",
                "the",
                "known",
                "attributes,",
                "with",
                "learned",
                "attention",
                "weights.",
                "The",
                "weights",
                "are",
                "shared",
                "between",
                "entities",
                "and",
                "attributes."
            ],
            "context": [
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "We initialize attribute embeddings with Glove #TARGET_REF word embeddings. For an entity, we take all the known attributes from K e . The representation of each entity is the weighted sum of the known attributes, with learned attention weights. The weights are shared between entities and attributes.",
        "output": "<PERCEPT> We initialize attribute embeddings with </PERCEPT> <INFO> Glove #TARGET_REF word embeddings. </INFO> For an entity, we take all the known attributes from K e . The representation of each entity is the weighted sum of the known attributes, with learned attention weights. The weights are shared between entities and attributes."
    },
    {
        "gold": {
            "text": [
                "For",
                "semantics,",
                "the",
                "situation",
                "is",
                "even",
                "more",
                "complicated.",
                "While",
                "BERT's",
                "performance",
                "on",
                "natural",
                "language",
                "understanding",
                "tasks",
                "set",
                "a",
                "new",
                "state",
                "of",
                "the",
                "art,",
                "more",
                "targeted",
                "tests",
                "of",
                "its",
                "semantic",
                "abilities",
                "have",
                "yielded",
                "less",
                "positive",
                "results.",
                "BERT",
                "has",
                "limited",
                "knowledge",
                "of",
                "lexical",
                "semantic",
                "relations",
                "such",
                "as",
                "hypernymy",
                "#TARGET_REF",
                "and",
                "antonymy",
                "#REF",
                ".",
                "Moreover,",
                "it",
                "has",
                "fragile",
                "representations",
                "of",
                "named",
                "entities",
                "#REF",
                ",",
                "and",
                "imprecise",
                "representations",
                "of",
                "numbers",
                "#REF",
                ".",
                "These",
                "flaws",
                "comprise",
                "specific",
                "linguistic",
                "phenomena",
                "that",
                "BERTScore,",
                "due",
                "to",
                "its",
                "use",
                "of",
                "BERT,",
                "might",
                "be",
                "unable",
                "to",
                "handle,",
                "and",
                "thus",
                "merit",
                "investigation."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2
            ]
        },
        "input": "For semantics, the situation is even more complicated. While BERT's performance on natural language understanding tasks set a new state of the art, more targeted tests of its semantic abilities have yielded less positive results. BERT has limited knowledge of lexical semantic relations such as hypernymy #TARGET_REF and antonymy #REF . Moreover, it has fragile representations of named entities #REF , and imprecise representations of numbers #REF . These flaws comprise specific linguistic phenomena that BERTScore, due to its use of BERT, might be unable to handle, and thus merit investigation.",
        "output": "For semantics, the situation is even more complicated. While BERT's performance on natural language understanding tasks set a new state of the art, more targeted tests of its semantic abilities have yielded less positive results. <INFO> BERT has limited knowledge of lexical semantic relations such as hypernymy #TARGET_REF </INFO> <BACK> and antonymy #REF . Moreover, it has fragile representations of named entities #REF , and imprecise representations of numbers #REF . These flaws comprise specific linguistic phenomena that BERTScore, due to its use of BERT, </BACK> <PERCEPT> might be unable to handle, and thus merit investigation. </PERCEPT>"
    },
    {
        "gold": {
            "text": [
                "where",
                "-5",
                "meant",
                "that",
                "the",
                "MT",
                "output",
                "had",
                "greatly",
                "hindered",
                "their",
                "work",
                "and",
                "5",
                "meant",
                "that",
                "the",
                "MT",
                "output",
                "had",
                "greatly",
                "helped",
                "their",
                "work",
                "(see",
                "Table",
                "5).",
                "Overall,",
                "users",
                "were",
                "more",
                "positive",
                "about",
                "having",
                "the",
                "MT",
                "output",
                "displayed",
                "when",
                "translating,",
                "with",
                "only",
                "one",
                "out",
                "of",
                "six",
                "claiming",
                "that",
                "it",
                "hindered",
                "the",
                "process.",
                "In",
                "the",
                "case",
                "of",
                "translators,",
                "however,",
                "three",
                "out",
                "of",
                "six",
                "heavily",
                "penalized",
                "its",
                "use,",
                "one",
                "reported",
                "that",
                "it",
                "was",
                "better",
                "than",
                "not",
                "having",
                "it,",
                "and",
                "two",
                "reported",
                "some",
                "benefit.",
                "T1",
                "commented",
                "that",
                "translation",
                "and",
                "post-editing",
                "required",
                "different",
                "skills",
                "and",
                "that",
                "should",
                "the",
                "same",
                "time",
                "be",
                "spent",
                "in",
                "post-editing",
                "and",
                "translating,",
                "the",
                "translation",
                "would",
                "most",
                "probably",
                "be",
                "of",
                "better",
                "quality.",
                "T6",
                "was",
                "the",
                "most",
                "positive",
                "of",
                "all",
                "with",
                "regards",
                "to",
                "MT",
                "and",
                "admitted",
                "that",
                "the",
                "output",
                "helped",
                "in",
                "acquiring",
                "the",
                "terminology",
                "but",
                "was",
                "hopeless",
                "with",
                "syntax,",
                "which",
                "needed",
                "a",
                "complete",
                "rework.",
                "T2,",
                "T3",
                "and",
                "T4",
                "indicated",
                "that",
                "the",
                "MT",
                "output",
                "had",
                "clearly",
                "interfered",
                "in",
                "their",
                "job.",
                "T2",
                "reported",
                "that",
                "MT",
                "output",
                "slowed",
                "down",
                "the",
                "process",
                "considerably",
                "because",
                "reading,",
                "understanding",
                "and",
                "considering",
                "what",
                "to",
                "reuse",
                "from",
                "it",
                "was",
                "very",
                "time-consuming.",
                "T3",
                "commented",
                "that",
                "translating",
                "from",
                "scratch",
                "was",
                "easier",
                "and",
                "faster,",
                "and",
                "that",
                "even",
                "checking",
                "the",
                "MT",
                "output",
                "for",
                "terminology",
                "would",
                "most",
                "often",
                "not",
                "help.",
                "T4",
                "claimed",
                "that",
                "the",
                "MT",
                "system",
                "did",
                "not",
                "translate",
                "the",
                "order",
                "of",
                "the",
                "phrases",
                "properly,",
                "which",
                "rendered",
                "the",
                "translation",
                "incomprehensible.",
                "Interestingly,",
                "T3",
                "and",
                "T4",
                "did",
                "benefit",
                "from",
                "postediting."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "where -5 meant that the MT output had greatly hindered their work and 5 meant that the MT output had greatly helped their work (see Table 5). Overall, users were more positive about having the MT output displayed when translating, with only one out of six claiming that it hindered the process. In the case of translators, however, three out of six heavily penalized its use, one reported that it was better than not having it, and two reported some benefit. T1 commented that translation and post-editing required different skills and that should the same time be spent in post-editing and translating, the translation would most probably be of better quality. T6 was the most positive of all with regards to MT and admitted that the output helped in acquiring the terminology but was hopeless with syntax, which needed a complete rework. T2, T3 and T4 indicated that the MT output had clearly interfered in their job. T2 reported that MT output slowed down the process considerably because reading, understanding and considering what to reuse from it was very time-consuming. T3 commented that translating from scratch was easier and faster, and that even checking the MT output for terminology would most often not help. T4 claimed that the MT system did not translate the order of the phrases properly, which rendered the translation incomprehensible. Interestingly, T3 and T4 did benefit from postediting.",
        "output": "where -5 meant that the MT output had greatly hindered their work and 5 meant that the MT output had greatly helped their work (see Table 5). Overall, users were more positive about having the MT output displayed when translating, with only one out of six claiming that it hindered the process. In the case of translators, however, three out of six heavily penalized its use, one reported that it was better than not having it, and two reported some benefit. T1 commented that translation and post-editing required different skills and that should the same time be spent in post-editing and translating, the translation would most probably be of better quality. T6 was the most positive of all with regards to MT and admitted that the output helped in acquiring the terminology but was hopeless with syntax, which needed a complete rework. T2, T3 and T4 indicated that the MT output had clearly interfered in their job. T2 reported that MT output slowed down the process considerably because reading, understanding and considering what to reuse from it was very time-consuming. T3 commented that translating from scratch was easier and faster, and that even checking the MT output for terminology would most often not help. T4 claimed that the MT system did not translate the order of the phrases properly, which rendered the translation incomprehensible. Interestingly, T3 and T4 did benefit from postediting."
    },
    {
        "gold": {
            "text": [
                "In",
                "these",
                "experiments",
                "we",
                "have",
                "compared",
                "the",
                "conditioned",
                "text",
                "generation",
                "of",
                "CTERM-GAN",
                "with",
                "that",
                "of",
                "the",
                "state-of-the-art",
                "adversarial",
                "architectures",
                "-Se-qGAN",
                "#REF",
                ",",
                "RelGAN",
                "#TARGET_REF",
                ",",
                "and",
                "TGVAE",
                "#REF",
                "-and",
                "a",
                "classic",
                "auto-regressive",
                "LSTM",
                "language",
                "model",
                "with",
                "an",
                "initial",
                "conditioning,",
                "in",
                "terms",
                "of",
                "both",
                "syntactic",
                "and",
                "semantic",
                "quality.",
                "The",
                "main",
                "goal",
                "is",
                "to",
                "ensure",
                "a",
                "good",
                "quality",
                "for",
                "the",
                "generation",
                "by",
                "introducing",
                "a",
                "conditioning",
                "on",
                "the",
                "semantic",
                "of",
                "the",
                "sentence.",
                "In",
                "this",
                "task,",
                "the",
                "conditioning",
                "consists",
                "of",
                "the",
                "word",
                "distribution",
                "for",
                "a",
                "topic",
                "extracted",
                "from",
                "a",
                "sentence,",
                "either",
                "provided",
                "by",
                "the",
                "user",
                "or,",
                "as",
                "in",
                "our",
                "case,",
                "sampled",
                "from",
                "the",
                "dataset.",
                "Any",
                "type",
                "of",
                "topic",
                "model",
                "can",
                "be",
                "adopted:",
                "in",
                "our",
                "case,",
                "an",
                "LDA",
                "model",
                "#REF",
                "has",
                "been",
                "trained",
                "on",
                "a",
                "starting",
                "dataset",
                "in",
                "order",
                "to",
                "have",
                "a",
                "distribution",
                "of",
                "the",
                "topics",
                "covered",
                "within",
                "the",
                "corpus.",
                "The",
                "LDA",
                "model,",
                "both",
                "in",
                "training",
                "and",
                "in",
                "inference,",
                "given",
                "an",
                "input",
                "sentence,",
                "builds",
                "a",
                "distribution",
                "on",
                "the",
                "vocabulary.",
                "In",
                "turn,",
                "this",
                "distribution",
                "influences",
                "the",
                "model's",
                "sentence",
                "generation",
                "thanks",
                "to",
                "its",
                "inclusion",
                "in",
                "the",
                "generation",
                "process.",
                "Most",
                "likely,",
                "improving",
                "the",
                "quality",
                "of",
                "the",
                "topic",
                "extraction",
                "is",
                "likely",
                "to",
                "improve",
                "the",
                "final",
                "results",
                "of",
                "the",
                "model.",
                "Eventually,",
                "the",
                "extracted",
                "distribution",
                "is",
                "used",
                "as",
                "the",
                "condition-",
                "ing",
                "input,",
                "c,",
                "for",
                "the",
                "relational",
                "memory",
                "during",
                "the",
                "generation,",
                "as",
                "described",
                "in",
                "Section",
                "3."
            ],
            "context": [
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                3,
                3,
                3,
                1,
                1,
                1,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                3,
                0,
                3,
                0,
                0,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "In these experiments we have compared the conditioned text generation of CTERM-GAN with that of the state-of-the-art adversarial architectures -Se-qGAN #REF , RelGAN #TARGET_REF , and TGVAE #REF -and a classic auto-regressive LSTM language model with an initial conditioning, in terms of both syntactic and semantic quality. The main goal is to ensure a good quality for the generation by introducing a conditioning on the semantic of the sentence. In this task, the conditioning consists of the word distribution for a topic extracted from a sentence, either provided by the user or, as in our case, sampled from the dataset. Any type of topic model can be adopted: in our case, an LDA model #REF has been trained on a starting dataset in order to have a distribution of the topics covered within the corpus. The LDA model, both in training and in inference, given an input sentence, builds a distribution on the vocabulary. In turn, this distribution influences the model's sentence generation thanks to its inclusion in the generation process. Most likely, improving the quality of the topic extraction is likely to improve the final results of the model. Eventually, the extracted distribution is used as the condition- ing input, c, for the relational memory during the generation, as described in Section 3.",
        "output": "In <PERCEPT> these experiments we have compared the conditioned text generation of CTERM-GAN with that of the state-of-the-art adversarial architectures </PERCEPT> <BACK> -Se-qGAN #REF , </BACK> <INFO> RelGAN #TARGET_REF , </INFO> <BACK> and TGVAE #REF </BACK> -and a classic auto-regressive LSTM language model with an initial conditioning, <BACK> in terms of </BACK> both <BACK> syntactic </BACK> and semantic <BACK> quality. </BACK> The main goal is to ensure a good quality for the generation by introducing a conditioning on the semantic of the sentence. In this task, the conditioning consists of the word distribution for a topic extracted from a sentence, either provided by the user or, as in our case, sampled from the dataset. Any type of topic model can be adopted: in our case, an LDA model #REF has been trained on a starting dataset in order to have a distribution of the topics covered within the corpus. The LDA model, both in training and in inference, given an input sentence, builds a distribution on the vocabulary. In turn, this distribution influences the model's sentence generation thanks to its inclusion in the generation process. Most likely, improving the quality of the topic extraction is likely to improve the final results of the model. Eventually, the extracted distribution is used as the condition- ing input, c, for the relational memory during the generation, as described in Section 3."
    },
    {
        "gold": {
            "text": [
                "Simultaneous",
                "translation",
                "#TARGET_REF",
                "consists",
                "in",
                "generating",
                "a",
                "translation",
                "before",
                "the",
                "source",
                "speaker",
                "finishes",
                "speaking.",
                "It",
                "is",
                "widely",
                "used",
                "in",
                "many",
                "real-time",
                "scenarios",
                "such",
                "as",
                "international",
                "conferences,",
                "business",
                "negotiations",
                "and",
                "legal",
                "proceedings.",
                "The",
                "challenge",
                "of",
                "Simultaneous",
                "machine",
                "translation",
                "is",
                "to",
                "find",
                "a",
                "read-write",
                "policy",
                "that",
                "balances",
                "translation",
                "quality",
                "and",
                "latency.",
                "The",
                "translation",
                "quality",
                "will",
                "decline",
                "if",
                "the",
                "machine",
                "translation",
                "system",
                "reads",
                "insufficient",
                "source",
                "information.",
                "When",
                "reading",
                "wider",
                "source",
                "text,",
                "latency",
                "will",
                "increase."
            ],
            "context": [
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Simultaneous translation #TARGET_REF consists in generating a translation before the source speaker finishes speaking. It is widely used in many real-time scenarios such as international conferences, business negotiations and legal proceedings. The challenge of Simultaneous machine translation is to find a read-write policy that balances translation quality and latency. The translation quality will decline if the machine translation system reads insufficient source information. When reading wider source text, latency will increase.",
        "output": "<INFO> Simultaneous translation #TARGET_REF consists in generating a translation before the source speaker finishes speaking. </INFO> <PERCEPT> It is widely used in many real-time scenarios </PERCEPT> <BACK> such as international conferences, business negotiations and legal proceedings. </BACK> <PERCEPT> The challenge of Simultaneous machine translation is to find a read-write policy that balances translation quality and latency. </PERCEPT> <BACK> The translation quality will decline if the machine translation system reads insufficient source information. </BACK> When reading wider source text, latency will increase."
    },
    {
        "gold": {
            "text": [
                "Among",
                "four",
                "submitted",
                "systems,",
                "MaChAmp",
                "(der",
                "Goot,",
                "2022)",
                "and",
                "AN(L)P",
                "#REF",
                "teams",
                "used",
                "the",
                "default",
                "tokenizer",
                "from",
                "either",
                "BERT",
                "or",
                "mBERT,",
                "which",
                "are",
                "not",
                "designed",
                "for",
                "scientific",
                "documents.",
                "Consequently,",
                "they",
                "are",
                "unable",
                "to",
                "correctly",
                "segment",
                "the",
                "mathematic",
                "source,",
                "hence,",
                "they",
                "#TARGET_REF",
                "and",
                "JBNU-CCLab",
                "(Lee",
                "and",
                "Na,",
                "2022)",
                "achieved",
                "much",
                "higher",
                "performances",
                "thanks",
                "to",
                "SciBERT",
                "tokenizer",
                "because",
                "it",
                "is",
                "trained",
                "on",
                "scientific",
                "literature.",
                "However,",
                "the",
                "SciBERT",
                "tokenizer",
                "is",
                "far",
                "from",
                "perfect",
                "such",
                "that",
                "JBNU-CCLab",
                "further",
                "proposed",
                "to",
                "tokenize",
                "the",
                "mathematical",
                "formulae",
                "using",
                "a",
                "customized",
                "rule-based",
                "tokenizer",
                "based",
                "on",
                "capital",
                "letters,",
                "numbers,",
                "and",
                "special",
                "characters(e.g.",
                "%,",
                "$,",
                "{,",
                "}).",
                "Hence,",
                "they",
                "achieved",
                "state-of-the-art",
                "performance",
                "on",
                "both",
                "NER",
                "and",
                "RE",
                "subtasks."
            ],
            "context": [
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                1,
                1,
                3,
                3,
                3,
                3,
                3,
                3,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Among four submitted systems, MaChAmp (der Goot, 2022) and AN(L)P #REF teams used the default tokenizer from either BERT or mBERT, which are not designed for scientific documents. Consequently, they are unable to correctly segment the mathematic source, hence, they #TARGET_REF and JBNU-CCLab (Lee and Na, 2022) achieved much higher performances thanks to SciBERT tokenizer because it is trained on scientific literature. However, the SciBERT tokenizer is far from perfect such that JBNU-CCLab further proposed to tokenize the mathematical formulae using a customized rule-based tokenizer based on capital letters, numbers, and special characters(e.g. %, $, {, }). Hence, they achieved state-of-the-art performance on both NER and RE subtasks.",
        "output": "<BACK> Among four submitted systems, MaChAmp (der Goot, 2022) and AN(L)P #REF teams used the default tokenizer from either BERT or mBERT, which are not designed for scientific documents. </BACK> Consequently, <PERCEPT> they are unable to correctly segment the mathematic source, </PERCEPT> hence, <INFO> they #TARGET_REF </INFO> <BACK> and JBNU-CCLab (Lee and Na, 2022) </BACK> <PERCEPT> achieved much higher performances thanks to SciBERT tokenizer because it is trained on scientific literature. </PERCEPT> However, the SciBERT tokenizer is far from perfect such that JBNU-CCLab further proposed to tokenize the mathematical formulae using a customized rule-based tokenizer based on capital letters, numbers, and special characters(e.g. %, $, {, }). Hence, they achieved state-of-the-art performance on both NER and RE subtasks."
    },
    {
        "gold": {
            "text": [
                "For",
                "the",
                "downstream",
                "NER",
                "tasks",
                "we",
                "map",
                "or",
                "encode",
                "the",
                "NER",
                "annotations",
                "into",
                "the",
                "phonetic",
                "representation.",
                "We",
                "thus",
                "edited",
                "the",
                "labels",
                "(PER,",
                "ORG,",
                "DATE,",
                "and",
                "LOC)",
                "to",
                "convert",
                "them",
                "from",
                "word-level",
                "labels",
                "to",
                "phone-level",
                "labels",
                "as",
                "shown",
                "in",
                "Fig.",
                "3.",
                "Unlike",
                "#TARGET_REF",
                ",",
                "we",
                "leave",
                "in",
                "the",
                "B-and",
                "I-prefixes."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "For the downstream NER tasks we map or encode the NER annotations into the phonetic representation. We thus edited the labels (PER, ORG, DATE, and LOC) to convert them from word-level labels to phone-level labels as shown in Fig. 3. Unlike #TARGET_REF , we leave in the B-and I-prefixes.",
        "output": "For the downstream NER tasks we map or encode the NER annotations into the phonetic representation. <BACK> We thus edited the labels (PER, ORG, DATE, and LOC) to convert them from word-level labels to phone-level labels as shown in Fig. 3. </BACK> <INFO> Unlike #TARGET_REF </INFO> , we leave in the B-and I-prefixes."
    },
    {
        "gold": {
            "text": [
                "A",
                "three-fold",
                "analysis",
                "of",
                "the",
                "selected",
                "keywords",
                "was",
                "done.",
                "The",
                "semantics",
                "of",
                "the",
                "words",
                "was",
                "studied",
                "by",
                "using",
                "WordNet.",
                "In",
                "dialogue",
                "acts",
                "such",
                "as",
                "apologizing,",
                "thanking,",
                "or",
                "expressing",
                "sympathy,",
                "affective",
                "language",
                "is",
                "often",
                "employed",
                "to",
                "represent",
                "and",
                "convey",
                "psychological",
                "attitudes",
                "#REF",
                ".",
                "Also,",
                "there",
                "is",
                "what",
                "is",
                "called",
                "a",
                "'heartfelt",
                "apology'",
                "as",
                "against",
                "'routine",
                "apology",
                "#TARGET_REF",
                ").",
                "Hence,",
                "it",
                "was",
                "decided",
                "to",
                "further",
                "explore",
                "the",
                "sentiments",
                "and",
                "emotions",
                "associated",
                "with",
                "the",
                "keywords.",
                "The",
                "sentiments",
                "were",
                "studied",
                "using",
                "SentiWordNet",
                "and",
                "the",
                "emotion",
                "labels",
                "were",
                "determined",
                "through",
                "WordNet-Affect.",
                "The",
                "analysis",
                "and",
                "conclusions",
                "thus",
                "drawn",
                "are",
                "presented",
                "below."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "A three-fold analysis of the selected keywords was done. The semantics of the words was studied by using WordNet. In dialogue acts such as apologizing, thanking, or expressing sympathy, affective language is often employed to represent and convey psychological attitudes #REF . Also, there is what is called a 'heartfelt apology' as against 'routine apology #TARGET_REF ). Hence, it was decided to further explore the sentiments and emotions associated with the keywords. The sentiments were studied using SentiWordNet and the emotion labels were determined through WordNet-Affect. The analysis and conclusions thus drawn are presented below.",
        "output": "A three-fold analysis of the selected keywords was done. The semantics of the words was studied by using WordNet. In dialogue acts such as apologizing, thanking, or expressing sympathy, affective language is often employed to represent and convey psychological attitudes #REF . Also, <PERCEPT> there is what is called a 'heartfelt apology' as against </PERCEPT> <INFO> 'routine apology #TARGET_REF </INFO> ). Hence, it was decided to further explore the sentiments and emotions associated with the keywords. The sentiments were studied using SentiWordNet and the emotion labels were determined through WordNet-Affect. The analysis and conclusions thus drawn are presented below."
    },
    {
        "gold": {
            "text": [
                "Producing",
                "K",
                "outputs",
                "during",
                "inference.",
                "In",
                "order",
                "to",
                "generate",
                "K",
                "different",
                "outputs",
                "on",
                "test",
                "set,",
                "we",
                "follow",
                "#TARGET_REF",
                "to",
                "enumerate",
                "all",
                "latent",
                "variables",
                "z",
                "and",
                "then",
                "greedily",
                "decoding",
                "each",
                "token",
                "by",
                "ŷt",
                "=",
                "arg",
                "max",
                "p(y|ŷ",
                "1:t−1",
                ",",
                "z,",
                "x).",
                "In",
                "other",
                "words,",
                "we",
                "ask",
                "each",
                "expert",
                "to",
                "seek",
                "different",
                "sets",
                "of",
                "concepts",
                "on",
                "the",
                "knowledge",
                "graph,",
                "and",
                "use",
                "the",
                "selected",
                "concepts",
                "to",
                "generate",
                "K",
                "different",
                "outputs.",
                "Notably,",
                "this",
                "decoding",
                "procedure",
                "is",
                "efficient",
                "and",
                "easily",
                "parallelizable.",
                "Furthermore,",
                "to",
                "make",
                "fair",
                "comparisons",
                "with",
                "sampling-based",
                "methods,",
                "we",
                "use",
                "greedy",
                "decoding",
                "without",
                "any",
                "sampling",
                "strategy."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                0,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2
            ]
        },
        "input": "Producing K outputs during inference. In order to generate K different outputs on test set, we follow #TARGET_REF to enumerate all latent variables z and then greedily decoding each token by ŷt = arg max p(y|ŷ 1:t−1 , z, x). In other words, we ask each expert to seek different sets of concepts on the knowledge graph, and use the selected concepts to generate K different outputs. Notably, this decoding procedure is efficient and easily parallelizable. Furthermore, to make fair comparisons with sampling-based methods, we use greedy decoding without any sampling strategy.",
        "output": "Producing K outputs during inference. <PERCEPT> In order to generate K different outputs on test set, we follow </PERCEPT> <INFO> #TARGET_REF </INFO> <PERCEPT> to enumerate all latent variables z and then greedily decoding each token by ŷt = arg max p(y|ŷ 1:t−1 , z, x). </PERCEPT> In other words, <PERCEPT> we ask each expert to seek different sets of concepts on the knowledge graph, and use the selected concepts to generate K different outputs. </PERCEPT> Notably, <PERCEPT> this decoding procedure is efficient and easily parallelizable. </PERCEPT> Furthermore, <PERCEPT> to make fair comparisons with sampling-based methods, we use greedy decoding without any sampling strategy. </PERCEPT>"
    },
    {
        "gold": {
            "text": [
                "As",
                "#TARGET_REF",
                "showed,",
                "they",
                "might",
                "associate",
                "words",
                "that",
                "are",
                "not",
                "in",
                "a",
                "strong",
                "direct",
                "connection,",
                "but",
                "are",
                "only",
                "indirectly",
                "related",
                "(e.g.",
                "religion",
                "is",
                "not",
                "related",
                "to",
                "tree,",
                "but",
                "both",
                "are",
                "related",
                "to",
                "Christmas,",
                "therefore",
                "religion",
                "could",
                "be",
                "a",
                "clue",
                "for",
                "tree)."
            ],
            "context": [
                2,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3
            ]
        },
        "input": "As #TARGET_REF showed, they might associate words that are not in a strong direct connection, but are only indirectly related (e.g. religion is not related to tree, but both are related to Christmas, therefore religion could be a clue for tree).",
        "output": "<PERCEPT> As </PERCEPT> <INFO> #TARGET_REF </INFO> <PERCEPT> showed, they might associate words that are not in a strong direct connection, but are only indirectly related </PERCEPT> <BACK> (e.g. religion is not related to tree, but both are related to Christmas, therefore religion could be a clue for tree). </BACK>"
    },
    {
        "gold": {
            "text": [
                "Our",
                "study",
                "is",
                "also",
                "related",
                "to",
                "attribution",
                "approaches,",
                "which",
                "aims",
                "to",
                "find",
                "features",
                "or",
                "regions",
                "of",
                "input",
                "that",
                "are",
                "important",
                "for",
                "tasks.",
                "Different",
                "types",
                "of",
                "techniques,",
                "including",
                "gradient-based",
                "#REF",
                ",",
                "are",
                "applied",
                "for",
                "reinforcement",
                "learning",
                "#REF",
                ",",
                "computer",
                "vision",
                "#REF",
                ",",
                "and",
                "text",
                "classification",
                "#TARGET_REF",
                ".",
                "While",
                "these",
                "works",
                "focus",
                "on",
                "interpreting",
                "model",
                "behaviors,",
                "we",
                "aim",
                "to",
                "find",
                "salient",
                "words",
                "beyond",
                "input",
                "and",
                "utilize",
                "them",
                "as",
                "action",
                "representations."
            ],
            "context": [
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                1,
                1,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2
            ]
        },
        "input": "Our study is also related to attribution approaches, which aims to find features or regions of input that are important for tasks. Different types of techniques, including gradient-based #REF , are applied for reinforcement learning #REF , computer vision #REF , and text classification #TARGET_REF . While these works focus on interpreting model behaviors, we aim to find salient words beyond input and utilize them as action representations.",
        "output": "<BACK> Our study is also related to attribution approaches, which aims to find features or regions of input that are important for tasks. </BACK> <PERCEPT> Different types of techniques, including gradient-based #REF , are applied for </PERCEPT> <BACK> reinforcement learning #REF , computer vision #REF , </BACK> <INFO> and text classification #TARGET_REF . </INFO> <PERCEPT> While these works focus on interpreting model behaviors, we aim to find salient words beyond input and utilize them as action representations. </PERCEPT>"
    },
    {
        "gold": {
            "text": [
                "Furthermore,",
                "we",
                "extend",
                "the",
                "double",
                "perturbation",
                "framework",
                "to",
                "evaluate",
                "counterfactual",
                "biases",
                "#TARGET_REF",
                ")",
                "(",
                "§4)",
                "in",
                "English.",
                "When",
                "the",
                "test",
                "dataset",
                "is",
                "small,",
                "our",
                "framework",
                "can",
                "help",
                "improve",
                "the",
                "evaluation",
                "robustness",
                "by",
                "revealing",
                "the",
                "hidden",
                "biases",
                "not",
                "directly",
                "shown",
                "in",
                "the",
                "test",
                "dataset.",
                "Intuitively,",
                "a",
                "fair",
                "model",
                "should",
                "make",
                "the",
                "same",
                "prediction",
                "for",
                "nearly",
                "identical",
                "examples",
                "referencing",
                "different",
                "groups",
                "#REF",
                "with",
                "different",
                "protected",
                "attributes",
                "(e.g.,",
                "gender,",
                "race).",
                "In",
                "our",
                "evaluation,",
                "we",
                "consider",
                "a",
                "model",
                "biased",
                "if",
                "substituting",
                "tokens",
                "associated",
                "with",
                "protected",
                "attributes",
                "changes",
                "the",
                "expected",
                "prediction,",
                "which",
                "is",
                "the",
                "average",
                "prediction",
                "among",
                "all",
                "examples",
                "within",
                "the",
                "neighborhood.",
                "For",
                "instance,",
                "a",
                "toxicity",
                "classifier",
                "is",
                "biased",
                "if",
                "it",
                "tends",
                "to",
                "increase",
                "the",
                "toxicity",
                "if",
                "we",
                "substitute",
                "straight",
                "→",
                "gay",
                "in",
                "an",
                "input",
                "sentence",
                "#REF",
                ".",
                "In",
                "the",
                "experiments,",
                "we",
                "evaluate",
                "the",
                "expected",
                "sentiment",
                "predictions",
                "on",
                "pairs",
                "of",
                "protected",
                "tokens",
                "(e.g.,",
                "(he,",
                "she),",
                "(gay,",
                "straight)),",
                "and",
                "demonstrate",
                "that",
                "our",
                "method",
                "is",
                "able",
                "to",
                "reveal",
                "the",
                "hidden",
                "model",
                "biases."
            ],
            "context": [
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Furthermore, we extend the double perturbation framework to evaluate counterfactual biases #TARGET_REF ) ( §4) in English. When the test dataset is small, our framework can help improve the evaluation robustness by revealing the hidden biases not directly shown in the test dataset. Intuitively, a fair model should make the same prediction for nearly identical examples referencing different groups #REF with different protected attributes (e.g., gender, race). In our evaluation, we consider a model biased if substituting tokens associated with protected attributes changes the expected prediction, which is the average prediction among all examples within the neighborhood. For instance, a toxicity classifier is biased if it tends to increase the toxicity if we substitute straight → gay in an input sentence #REF . In the experiments, we evaluate the expected sentiment predictions on pairs of protected tokens (e.g., (he, she), (gay, straight)), and demonstrate that our method is able to reveal the hidden model biases.",
        "output": "Furthermore, <PERCEPT> we extend the double perturbation framework to evaluate </PERCEPT> <INFO> counterfactual biases </INFO> #TARGET_REF ) ( §4) in English. When the test dataset is small, our framework can help improve the evaluation robustness by revealing the hidden biases not directly shown in the test dataset. Intuitively, a fair model should make the same prediction for nearly identical examples referencing different groups #REF with different protected attributes (e.g., gender, race). In our evaluation, we consider a model biased if substituting tokens associated with protected attributes changes the expected prediction, which is the average prediction among all examples within the neighborhood. For instance, a toxicity classifier is biased if it tends to increase the toxicity if we substitute straight → gay in an input sentence #REF . In the experiments, we evaluate the expected sentiment predictions on pairs of protected tokens (e.g., (he, she), (gay, straight)), and demonstrate that our method is able to reveal the hidden model biases."
    },
    {
        "gold": {
            "text": [
                "where,",
                "h",
                "i",
                "(f,",
                "e)",
                "denotes",
                "the",
                "different",
                "components",
                "for",
                "translating",
                "the",
                "source",
                "sentence",
                "f",
                "into",
                "the",
                "target",
                "sentence",
                "e.",
                "K",
                "is",
                "the",
                "number",
                "of",
                "components",
                "(or",
                "features)",
                "used",
                "and",
                "λ",
                "i",
                "are",
                "the",
                "corresponding",
                "weights",
                "of",
                "the",
                "components.",
                "The",
                "Moses",
                "SMT",
                "system",
                "#REF",
                ",",
                "which",
                "implements",
                "this",
                "particular",
                "model,",
                "was",
                "used",
                "for",
                "all",
                "our",
                "PBSMT",
                "translation",
                "experiments.",
                "Different",
                "component",
                "weights",
                "(λ",
                "i",
                ")",
                "were",
                "estimated",
                "using",
                "a",
                "discriminative",
                "training",
                "method",
                "known",
                "as",
                "Minimum",
                "Error",
                "Rate",
                "Training",
                "(MERT)",
                "#TARGET_REF",
                ",",
                "on",
                "a",
                "held",
                "out",
                "development",
                "set",
                "(devset)."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2
            ]
        },
        "input": "where, h i (f, e) denotes the different components for translating the source sentence f into the target sentence e. K is the number of components (or features) used and λ i are the corresponding weights of the components. The Moses SMT system #REF , which implements this particular model, was used for all our PBSMT translation experiments. Different component weights (λ i ) were estimated using a discriminative training method known as Minimum Error Rate Training (MERT) #TARGET_REF , on a held out development set (devset).",
        "output": "where, h i (f, e) denotes the different components for translating the source sentence f into the target sentence e. K is the number of components (or features) used and λ i are the corresponding weights of the components. The Moses SMT system #REF , which implements this particular model, was used for all our PBSMT translation experiments. <PERCEPT> Different component weights (λ i ) were estimated using </PERCEPT> <INFO> a discriminative training method known as Minimum Error Rate Training (MERT) #TARGET_REF , </INFO> <PERCEPT> on a held out development set (devset). </PERCEPT>"
    },
    {
        "gold": {
            "text": [
                "Recently,",
                "#TARGET_REF",
                "and",
                "#REF",
                "have",
                "nevertheless",
                "shown",
                "that",
                "the",
                "commonly",
                "adopted",
                "practices",
                "(the",
                "number",
                "of",
                "iterations,",
                "the",
                "choice",
                "of",
                "model",
                "layers)",
                "when",
                "fine-tuning",
                "Transformers-based",
                "langage",
                "models",
                "are",
                "inappropriate",
                "under",
                "resource",
                "constrained",
                "conditions",
                "and",
                "adversely",
                "affect",
                "the",
                "stability",
                "of",
                "models",
                "performances",
                "as",
                "overfitting,",
                "label",
                "noise",
                "memorization",
                "or",
                "catastrophic",
                "forgetting.",
                "Added",
                "to",
                "this,",
                "because",
                "the",
                "pretraining",
                "process",
                "is",
                "particularly",
                "constraining,",
                "various",
                "works",
                "have",
                "been",
                "oriented",
                "towards",
                "the",
                "research",
                "and",
                "training",
                "of",
                "efficient",
                "models,",
                "both",
                "in",
                "terms",
                "of",
                "available",
                "capacities",
                "and",
                "resources",
                "and",
                "in",
                "terms",
                "of",
                "environmental",
                "footprint."
            ],
            "context": [
                1,
                1,
                3,
                3,
                1,
                0,
                1,
                1,
                1,
                1,
                1,
                1,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                0,
                0,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Recently, #TARGET_REF and #REF have nevertheless shown that the commonly adopted practices (the number of iterations, the choice of model layers) when fine-tuning Transformers-based langage models are inappropriate under resource constrained conditions and adversely affect the stability of models performances as overfitting, label noise memorization or catastrophic forgetting. Added to this, because the pretraining process is particularly constraining, various works have been oriented towards the research and training of efficient models, both in terms of available capacities and resources and in terms of environmental footprint.",
        "output": "<INFO> Recently, #TARGET_REF </INFO> <BACK> and #REF </BACK> <INFO> have </INFO> nevertheless <INFO> shown that the commonly adopted practices </INFO> <BACK> (the number of iterations, the choice of model layers) </BACK> <INFO> when fine-tuning Transformers-based langage models are inappropriate under resource constrained conditions and adversely affect the stability of models performances as overfitting, label noise memorization or catastrophic forgetting. </INFO> Added to this, <BACK> because the pretraining process is particularly constraining, various works have been oriented towards the research and training of efficient models, </BACK> both in terms of available capacities and resources and in terms of environmental footprint."
    },
    {
        "gold": {
            "text": [
                "MMB",
                "believed",
                "30",
                "years",
                "ago",
                "that",
                "constructed",
                "entities",
                "such",
                "as",
                "dictionaries",
                "and",
                "thesauri",
                "(especially",
                "the",
                "latter)",
                "constituted",
                "real",
                "resources",
                "for",
                "computational",
                "language",
                "processing",
                "#TARGET_REF",
                ".",
                "That",
                "was",
                "at",
                "a",
                "time",
                "when",
                "any",
                "computational",
                "operations",
                "on",
                "such",
                "entities",
                "were",
                "often",
                "dismissed",
                "by",
                "those",
                "working",
                "in",
                "other",
                "areas",
                "of",
                "computational",
                "linguistics",
                "as",
                "low-grade",
                "concordance",
                "work.",
                "Betty",
                "May",
                "compacted",
                "the",
                "whole",
                "of",
                "Roget's",
                "thesaurus",
                "for",
                "MMB,",
                "from",
                "1,000",
                "'heads'",
                "to",
                "800,",
                "and",
                "had",
                "them",
                "cardpunched.",
                "That",
                "formed",
                "the",
                "basis",
                "for",
                "a",
                "range",
                "of",
                "experiments",
                "on",
                "Hollerith",
                "sorting",
                "machines",
                "which",
                "contributed",
                "to",
                "Karen",
                "Sparck",
                "Jones'",
                "seminal",
                "thesis",
                "work",
                "Synonymy",
                "and",
                "semantics",
                "classification",
                "#REF",
                ".",
                "MMB",
                "believed",
                "that",
                "thesauri",
                "such",
                "as",
                "Roget's",
                "were",
                "not",
                "just",
                "fallible",
                "human",
                "constructs",
                "but",
                "real",
                "resources",
                "with",
                "some",
                "mathematical",
                "structure",
                "that",
                "was",
                "also",
                "a",
                "guide",
                "to",
                "the",
                "structures",
                "which",
                "humans",
                "use",
                "to",
                "process",
                "language.",
                "She",
                "would",
                "often",
                "refer",
                "to",
                "'Roget's",
                "unconscious'",
                "by",
                "which",
                "she",
                "meant",
                "that",
                "the",
                "patterns",
                "of",
                "cross-references",
                "from",
                "word",
                "to",
                "word",
                "across",
                "the",
                "thesaurus",
                "had",
                "underlying",
                "generalisations",
                "and",
                "patterns."
            ],
            "context": [
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "MMB believed 30 years ago that constructed entities such as dictionaries and thesauri (especially the latter) constituted real resources for computational language processing #TARGET_REF . That was at a time when any computational operations on such entities were often dismissed by those working in other areas of computational linguistics as low-grade concordance work. Betty May compacted the whole of Roget's thesaurus for MMB, from 1,000 'heads' to 800, and had them cardpunched. That formed the basis for a range of experiments on Hollerith sorting machines which contributed to Karen Sparck Jones' seminal thesis work Synonymy and semantics classification #REF . MMB believed that thesauri such as Roget's were not just fallible human constructs but real resources with some mathematical structure that was also a guide to the structures which humans use to process language. She would often refer to 'Roget's unconscious' by which she meant that the patterns of cross-references from word to word across the thesaurus had underlying generalisations and patterns.",
        "output": "<INFO> MMB believed 30 years ago that constructed entities such as dictionaries and thesauri (especially the latter) constituted real resources for computational language processing #TARGET_REF . </INFO> <PERCEPT> That was at a time when any computational operations on such entities were often dismissed by those working in other areas of computational linguistics as low-grade concordance work. </PERCEPT> Betty May compacted the whole of Roget's thesaurus for MMB, from 1,000 'heads' to 800, and had them cardpunched. That formed the basis for a range of experiments on Hollerith sorting machines which contributed to Karen Sparck Jones' seminal thesis work Synonymy and semantics classification #REF . MMB believed that thesauri such as Roget's were not just fallible human constructs but real resources with some mathematical structure that was also a guide to the structures which humans use to process language. She would often refer to 'Roget's unconscious' by which she meant that the patterns of cross-references from word to word across the thesaurus had underlying generalisations and patterns."
    },
    {
        "gold": {
            "text": [
                "In",
                "the",
                "past",
                "few",
                "years,",
                "Natural",
                "Language",
                "Processing",
                "(NLP)",
                "researchers",
                "have",
                "proposed",
                "several",
                "online",
                "game/community",
                "toxicity",
                "analysis",
                "frameworks",
                "Figure",
                "1:",
                "An",
                "example",
                "intent/slot",
                "annotation",
                "from",
                "the",
                "CONDA",
                "(CONtextual",
                "Dual-Annotated)",
                "dataset.",
                "#TARGET_REF",
                "and",
                "datasets",
                "#REF",
                ".",
                "However,",
                "existing",
                "datasets",
                "(1)",
                "focus",
                "only",
                "on",
                "the",
                "single",
                "utterance",
                "level",
                "without",
                "deeper",
                "understanding",
                "of",
                "context",
                "in",
                "the",
                "whole",
                "conversation/chat,",
                "and",
                "(2)",
                "do",
                "not",
                "explicitly",
                "use",
                "semantic",
                "clues",
                "from",
                "the",
                "words",
                "within",
                "the",
                "utterance."
            ],
            "context": [
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "In the past few years, Natural Language Processing (NLP) researchers have proposed several online game/community toxicity analysis frameworks Figure 1: An example intent/slot annotation from the CONDA (CONtextual Dual-Annotated) dataset. #TARGET_REF and datasets #REF . However, existing datasets (1) focus only on the single utterance level without deeper understanding of context in the whole conversation/chat, and (2) do not explicitly use semantic clues from the words within the utterance.",
        "output": "<BACK> In the past few years, Natural Language Processing (NLP) researchers have proposed several online game/community toxicity analysis frameworks </BACK> Figure 1: An example intent/slot annotation from the CONDA (CONtextual Dual-Annotated) dataset. #TARGET_REF <BACK> and datasets </BACK> #REF . However, existing datasets (1) focus only on the single utterance level without deeper understanding of context in the whole conversation/chat, and (2) do not explicitly use semantic clues from the words within the utterance."
    },
    {
        "gold": {
            "text": [
                "To",
                "model",
                "the",
                "relational",
                "information",
                "in",
                "the",
                "commonsen",
                "KG,",
                "we",
                "employ",
                "the",
                "relational",
                "graph",
                "convolutional",
                "network",
                "(R-GCN)",
                "#TARGET_REF",
                "which",
                "generalizes",
                "GCN",
                "with",
                "relation",
                "specific",
                "weight",
                "matrices.",
                "We",
                "follow",
                "#REF",
                "and",
                "#REF",
                "to",
                "use",
                "a",
                "non-parametric",
                "compositional",
                "operation",
                "ϕ(•)",
                "to",
                "combine",
                "the",
                "concept",
                "node",
                "embedding",
                "and",
                "the",
                "relation",
                "embedding.",
                "Specifically,",
                "given",
                "the",
                "input",
                "subgraph",
                "G",
                "x",
                "=",
                "{V",
                "x",
                ",",
                "E",
                "x",
                "}",
                "and",
                "an",
                "R-GCN",
                "with",
                "L",
                "layers,",
                "we",
                "update",
                "the",
                "embedding",
                "of",
                "each",
                "node",
                "v",
                "∈",
                "V",
                "x",
                "at",
                "the",
                "(l+1)-th",
                "layer",
                "by",
                "aggregating",
                "information",
                "from",
                "the",
                "embeddings",
                "of",
                "its",
                "neighbours",
                "in",
                "N",
                "(v)",
                "at",
                "the",
                "l-th",
                "layer:"
            ],
            "context": [
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "To model the relational information in the commonsen KG, we employ the relational graph convolutional network (R-GCN) #TARGET_REF which generalizes GCN with relation specific weight matrices. We follow #REF and #REF to use a non-parametric compositional operation ϕ(•) to combine the concept node embedding and the relation embedding. Specifically, given the input subgraph G x = {V x , E x } and an R-GCN with L layers, we update the embedding of each node v ∈ V x at the (l+1)-th layer by aggregating information from the embeddings of its neighbours in N (v) at the l-th layer:",
        "output": "<PERCEPT> To model the relational information in the commonsen KG, we employ </PERCEPT> <INFO> the relational graph convolutional network (R-GCN) #TARGET_REF which generalizes GCN with relation specific weight matrices. </INFO> We follow #REF and #REF to use a non-parametric compositional operation ϕ(•) to combine the concept node embedding and the relation embedding. Specifically, given the input subgraph G x = {V x , E x } and an R-GCN with L layers, we update the embedding of each node v ∈ V x at the (l+1)-th layer by aggregating information from the embeddings of its neighbours in N (v) at the l-th layer:"
    },
    {
        "gold": {
            "text": [
                "Recent",
                "work",
                "has",
                "identified",
                "that",
                "consecutive",
                "lay-",
                "ers",
                "of",
                "BERT",
                "have",
                "similar",
                "functionality",
                "#TARGET_REF",
                ".",
                "To",
                "study",
                "this,",
                "we",
                "considered",
                "configurations",
                "where",
                "six",
                "even",
                "and",
                "odd",
                "alternate",
                "layers",
                "are",
                "pruned",
                "and",
                "compare",
                "it",
                "with",
                "other",
                "strategies",
                "of",
                "pruning",
                "50%",
                "layers",
                "of",
                "BERT",
                "(Table",
                "6).",
                "We",
                "observe",
                "that",
                "the",
                "odd",
                "configuration",
                "performs",
                "better",
                "than",
                "the",
                "Top",
                "6",
                "and",
                "Bottom",
                "6",
                "configurations,",
                "indicating",
                "a",
                "preference",
                "to",
                "avoid",
                "pruning",
                "of",
                "consecutive",
                "layers.",
                "Effect",
                "of",
                "Fine-Tuning.",
                "Recent",
                "studies",
                "#REF",
                "have",
                "reported",
                "that",
                "when",
                "fine-tuning",
                "BERT",
                "for",
                "specific",
                "tasks,",
                "the",
                "top",
                "layers",
                "change",
                "much",
                "more",
                "than",
                "the",
                "lower",
                "layers.",
                "We",
                "now",
                "evaluate",
                "this",
                "for",
                "fine-tuning",
                "after",
                "pruning."
            ],
            "context": [
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Recent work has identified that consecutive lay- ers of BERT have similar functionality #TARGET_REF . To study this, we considered configurations where six even and odd alternate layers are pruned and compare it with other strategies of pruning 50% layers of BERT (Table 6). We observe that the odd configuration performs better than the Top 6 and Bottom 6 configurations, indicating a preference to avoid pruning of consecutive layers. Effect of Fine-Tuning. Recent studies #REF have reported that when fine-tuning BERT for specific tasks, the top layers change much more than the lower layers. We now evaluate this for fine-tuning after pruning.",
        "output": "<INFO> Recent work has identified that consecutive lay- ers of BERT have similar functionality #TARGET_REF . </INFO> <PERCEPT> To study this, we considered configurations where six even and odd alternate layers are pruned and compare it with other strategies of pruning 50% layers of BERT </PERCEPT> <BACK> (Table 6). </BACK> We observe that the odd configuration performs better than the Top 6 and Bottom 6 configurations, indicating a preference to avoid pruning of consecutive layers. Effect of Fine-Tuning. Recent studies #REF have reported that when fine-tuning BERT for specific tasks, the top layers change much more than the lower layers. We now evaluate this for fine-tuning after pruning."
    },
    {
        "gold": {
            "text": [
                "The",
                "distribution",
                "over",
                "the",
                "vocabulary",
                "of",
                "the",
                "next",
                "word",
                "is",
                "evaluated",
                "using",
                "the",
                "memory",
                "output",
                "o",
                "t",
                "as",
                "in",
                "Eq.",
                "3",
                "with",
                "a",
                "feed-forward",
                "layer.",
                "Then,",
                "the",
                "next",
                "soft",
                "word,",
                "ŷt",
                ",",
                "is",
                "sampled",
                "using",
                "the",
                "Gumbelsoftmax",
                "relaxation",
                "#TARGET_REF",
                "with",
                "temperature",
                "T",
                "(Eq.",
                "4).",
                "The",
                "temperature",
                "value",
                "greatly",
                "influences",
                "the",
                "quality-diversity",
                "trade-off,",
                "more",
                "details",
                "on",
                "these",
                "parameters",
                "are",
                "provided",
                "in",
                "Appendix",
                "D."
            ],
            "context": [
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2
            ]
        },
        "input": "The distribution over the vocabulary of the next word is evaluated using the memory output o t as in Eq. 3 with a feed-forward layer. Then, the next soft word, ŷt , is sampled using the Gumbelsoftmax relaxation #TARGET_REF with temperature T (Eq. 4). The temperature value greatly influences the quality-diversity trade-off, more details on these parameters are provided in Appendix D.",
        "output": "<BACK> The distribution over the vocabulary of the next word is evaluated using the memory output o t as in Eq. 3 with a feed-forward layer. Then, the next soft word, ŷt , is sampled using </BACK> <INFO> the Gumbelsoftmax relaxation #TARGET_REF with temperature T (Eq. 4). </INFO> <PERCEPT> The temperature value greatly influences the quality-diversity trade-off, more details on these parameters are provided in Appendix D. </PERCEPT>"
    },
    {
        "gold": {
            "text": [
                "We",
                "apply",
                "the",
                "proposed",
                "framework",
                "and",
                "quantify",
                "second-order",
                "robustness",
                "through",
                "two",
                "second-order",
                "attacks",
                "(",
                "§3).",
                "We",
                "experiment",
                "with",
                "English",
                "sentiment",
                "classification",
                "on",
                "the",
                "SST-2",
                "dataset",
                "#TARGET_REF",
                "across",
                "various",
                "model",
                "architectures.",
                "Surprisingly,",
                "although",
                "robustly",
                "trained",
                "CNN",
                "#REF",
                "and",
                "Transformer",
                "#REF",
                "can",
                "achieve",
                "high",
                "robustness",
                "under",
                "strong",
                "attacks",
                "#REF",
                ")",
                "(23.0%-71.6%",
                "success",
                "rates),",
                "for",
                "around",
                "96.0%",
                "of",
                "the",
                "test",
                "examples",
                "our",
                "attacks",
                "can",
                "find",
                "a",
                "vulnerable",
                "example",
                "by",
                "perturbing",
                "1.3",
                "words",
                "on",
                "average.",
                "This",
                "finding",
                "indicates",
                "that",
                "these",
                "robustly",
                "trained",
                "models,",
                "despite",
                "being",
                "first-order",
                "robust,",
                "are",
                "not",
                "second-order",
                "robust."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "We apply the proposed framework and quantify second-order robustness through two second-order attacks ( §3). We experiment with English sentiment classification on the SST-2 dataset #TARGET_REF across various model architectures. Surprisingly, although robustly trained CNN #REF and Transformer #REF can achieve high robustness under strong attacks #REF ) (23.0%-71.6% success rates), for around 96.0% of the test examples our attacks can find a vulnerable example by perturbing 1.3 words on average. This finding indicates that these robustly trained models, despite being first-order robust, are not second-order robust.",
        "output": "We apply the proposed framework and quantify second-order robustness through two second-order attacks ( §3). <PERCEPT> We experiment with English sentiment classification on </PERCEPT> <INFO> the SST-2 dataset #TARGET_REF </INFO> <PERCEPT> across various model architectures. </PERCEPT> Surprisingly, although robustly trained CNN #REF and Transformer #REF can achieve high robustness under strong attacks #REF ) (23.0%-71.6% success rates), for around 96.0% of the test examples our attacks can find a vulnerable example by perturbing 1.3 words on average. This finding indicates that these robustly trained models, despite being first-order robust, are not second-order robust."
    },
    {
        "gold": {
            "text": [
                "GPT-3",
                "displays",
                "strong",
                "zero-shot",
                "abilities",
                "#REF",
                ",",
                "i.e.,",
                "using",
                "a",
                "simple",
                "instruction",
                "or",
                "\"prompt\"",
                "as",
                "input,",
                "the",
                "model",
                "will",
                "extend",
                "or",
                "complete",
                "the",
                "text",
                "accordingly",
                "without",
                "any",
                "pre-defined",
                "examples.",
                "Prompt-engineering",
                "thus",
                "refers",
                "to",
                "manipulations",
                "and",
                "perturbations",
                "of",
                "this",
                "prompt",
                "to",
                "context-force",
                "the",
                "desired",
                "output",
                "behaviour",
                "#REF",
                ".",
                "In",
                "contrast",
                "to",
                "zero-shot,",
                "GPT-3",
                "can",
                "be",
                "fine-tuned",
                "over",
                "a",
                "dataset",
                "with",
                "desired",
                "inputoutput",
                "pairs",
                "#TARGET_REF",
                ".",
                "To",
                "conduct",
                "the",
                "experiment",
                "to",
                "compare",
                "neutral",
                "and",
                "diversityencouraging",
                "prompts,",
                "we",
                "compile",
                "a",
                "list",
                "of",
                "18",
                "prompts.",
                "Nine",
                "of",
                "them",
                "are",
                "designated",
                "\"neutral\"",
                "and",
                "used",
                "as",
                "our",
                "\"zero-shot\"",
                "prompts.",
                "These",
                "simply",
                "specify",
                "a",
                "task",
                "of",
                "generating",
                "an",
                "ad",
                "for",
                "a",
                "given",
                "job",
                "but",
                "are",
                "syntactically",
                "varied.",
                "The",
                "other",
                "nine",
                "prompts",
                "are",
                "\"equality",
                "and",
                "diversity",
                "prompts\",",
                "which",
                "we",
                "call",
                "\"engineered\"",
                "prompts.",
                "Tab.",
                "3",
                "displays",
                "all",
                "18",
                "prompts",
                "with",
                "their",
                "respective",
                "bias",
                "and",
                "realism",
                "scores."
            ],
            "context": [
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "GPT-3 displays strong zero-shot abilities #REF , i.e., using a simple instruction or \"prompt\" as input, the model will extend or complete the text accordingly without any pre-defined examples. Prompt-engineering thus refers to manipulations and perturbations of this prompt to context-force the desired output behaviour #REF . In contrast to zero-shot, GPT-3 can be fine-tuned over a dataset with desired inputoutput pairs #TARGET_REF . To conduct the experiment to compare neutral and diversityencouraging prompts, we compile a list of 18 prompts. Nine of them are designated \"neutral\" and used as our \"zero-shot\" prompts. These simply specify a task of generating an ad for a given job but are syntactically varied. The other nine prompts are \"equality and diversity prompts\", which we call \"engineered\" prompts. Tab. 3 displays all 18 prompts with their respective bias and realism scores.",
        "output": "<BACK> GPT-3 displays strong zero-shot abilities #REF </BACK> , i.e., using a simple instruction or \"prompt\" as input, the model will extend or complete the text accordingly without any pre-defined examples. Prompt-engineering thus refers to manipulations and perturbations of this prompt to context-force the desired output behaviour #REF . <PERCEPT> In contrast to zero-shot, </PERCEPT> <INFO> GPT-3 can be fine-tuned over a dataset with desired inputoutput pairs #TARGET_REF . </INFO> To conduct the experiment to compare neutral and diversityencouraging prompts, we compile a list of 18 prompts. Nine of them are designated \"neutral\" and used as our \"zero-shot\" prompts. These simply specify a task of generating an ad for a given job but are syntactically varied. The other nine prompts are \"equality and diversity prompts\", which we call \"engineered\" prompts. Tab. 3 displays all 18 prompts with their respective bias and realism scores."
    },
    {
        "gold": {
            "text": [
                "10,000",
                "ads",
                "was",
                "not",
                "feasible.",
                "Therefore,",
                "we",
                "train",
                "a",
                "discriminator",
                "model",
                "tasked",
                "with",
                "the",
                "binary",
                "prediction",
                "of",
                "whether",
                "a",
                "given",
                "input",
                "text",
                "was",
                "generated",
                "by",
                "a",
                "human",
                "or",
                "GPT-3",
                "and",
                "validate",
                "a",
                "sample",
                "of",
                "ads",
                "using",
                "human",
                "annotators.",
                "Real",
                "ads",
                "were",
                "longer",
                "(M",
                "=",
                "2,846",
                "characters,",
                "SD",
                "=",
                "2,038)",
                "than",
                "generated",
                "ones",
                "(M",
                "=",
                "514,",
                "SD",
                "=",
                "210)",
                "so",
                "we",
                "truncate",
                "texts",
                "to",
                "500",
                "characters.",
                "For",
                "prediction,",
                "we",
                "use",
                "a",
                "Multinominal",
                "Naive-Bayes",
                "(MNB)",
                "model,",
                "which",
                "we",
                "train,",
                "validate",
                "and",
                "test",
                "using",
                "an",
                "80:10:10",
                "split",
                "taken",
                "from",
                "the",
                "real",
                "and",
                "generated",
                "ads",
                "(described",
                "in",
                "Sec.",
                "3.2).",
                "6",
                "For",
                "our",
                "realism",
                "metric,",
                "we",
                "then",
                "use",
                "this",
                "model's",
                "predicted",
                "probability",
                "that",
                "an",
                "ad",
                "is",
                "real.",
                "To",
                "assess",
                "the",
                "robustness",
                "of",
                "this",
                "metric,",
                "we",
                "randomly",
                "sample",
                "10",
                "ads",
                "from",
                "each",
                "job",
                "category",
                "(female-biased,",
                "male-biased",
                "and",
                "neutral)",
                "for",
                "each",
                "experimental",
                "condition",
                "(N",
                "=",
                "150).",
                "We",
                "then",
                "ask",
                "three",
                "independent",
                "annotators",
                "to",
                "label",
                "the",
                "ad",
                "for",
                "whether",
                "it",
                "was",
                "human-or",
                "machine-generated",
                "and",
                "take",
                "the",
                "majority",
                "vote.",
                "7",
                "The",
                "accuracy",
                "of",
                "the",
                "majority",
                "label",
                "compared",
                "against",
                "the",
                "ground",
                "truth",
                "ad",
                "origin",
                "(real-world",
                "or",
                "GPT-3",
                "generated)",
                "proxies",
                "ad",
                "quality",
                "and",
                "realism."
            ],
            "context": [
                2,
                2,
                2,
                2,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "10,000 ads was not feasible. Therefore, we train a discriminator model tasked with the binary prediction of whether a given input text was generated by a human or GPT-3 and validate a sample of ads using human annotators. Real ads were longer (M = 2,846 characters, SD = 2,038) than generated ones (M = 514, SD = 210) so we truncate texts to 500 characters. For prediction, we use a Multinominal Naive-Bayes (MNB) model, which we train, validate and test using an 80:10:10 split taken from the real and generated ads (described in Sec. 3.2). 6 For our realism metric, we then use this model's predicted probability that an ad is real. To assess the robustness of this metric, we randomly sample 10 ads from each job category (female-biased, male-biased and neutral) for each experimental condition (N = 150). We then ask three independent annotators to label the ad for whether it was human-or machine-generated and take the majority vote. 7 The accuracy of the majority label compared against the ground truth ad origin (real-world or GPT-3 generated) proxies ad quality and realism.",
        "output": "<PERCEPT> 10,000 ads was not feasible. </PERCEPT> Therefore, we train a discriminator model tasked with the binary prediction of whether a given input text was generated by a human or GPT-3 and validate a sample of ads using human annotators. Real ads were longer (M = 2,846 characters, SD = 2,038) than generated ones (M = 514, SD = 210) so we truncate texts to 500 characters. For prediction, we use a Multinominal Naive-Bayes (MNB) model, which we train, validate and test using an 80:10:10 split taken from the real and generated ads (described in Sec. 3.2). 6 For our realism metric, we then use this model's predicted probability that an ad is real. To assess the robustness of this metric, we randomly sample 10 ads from each job category (female-biased, male-biased and neutral) for each experimental condition (N = 150). We then ask three independent annotators to label the ad for whether it was human-or machine-generated and take the majority vote. 7 The accuracy of the majority label compared against the ground truth ad origin (real-world or GPT-3 generated) proxies ad quality and realism."
    },
    {
        "gold": {
            "text": [
                "The",
                "final",
                "dataset",
                "contained",
                "17,520",
                "unique",
                "emotion-annotated",
                "subtitles",
                "as",
                "shown",
                "in",
                "table",
                "3.",
                "In",
                "addition",
                "there",
                "are",
                "some",
                "6.5k",
                "subtitles",
                "annotated",
                "as",
                "neutral.",
                "The",
                "label",
                "distribution",
                "can",
                "be",
                "seen",
                "in",
                "table",
                "3",
                "The",
                "emotion",
                "labels",
                "are",
                "surprisingly",
                "balanced",
                "with",
                "the",
                "exception",
                "of",
                "anger",
                "and",
                "anticipation,",
                "which",
                "are",
                "more",
                "common",
                "than",
                "the",
                "other",
                "labels.",
                "In",
                "comparison",
                "with",
                "one",
                "of",
                "the",
                "most",
                "well-known",
                "emotion",
                "datasets",
                "using",
                "the",
                "same",
                "annotation",
                "scheme,",
                "the",
                "NRC",
                "emotion",
                "lexicon",
                "(EmoLex)",
                "#TARGET_REF",
                ",",
                "the",
                "distribution",
                "differs",
                "somewhat.",
                "Although",
                "anger",
                "is",
                "a",
                "large",
                "category",
                "in",
                "both",
                "datasets,",
                "fear",
                "is",
                "average",
                "in",
                "our",
                "dataset,",
                "but",
                "the",
                "largest",
                "category",
                "in",
                "EmoLex.",
                "It",
                "is",
                "hard",
                "to",
                "speculate",
                "why",
                "this",
                "is,",
                "but",
                "one",
                "possible",
                "reason",
                "is",
                "the",
                "different",
                "source",
                "data."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2
            ]
        },
        "input": "The final dataset contained 17,520 unique emotion-annotated subtitles as shown in table 3. In addition there are some 6.5k subtitles annotated as neutral. The label distribution can be seen in table 3 The emotion labels are surprisingly balanced with the exception of anger and anticipation, which are more common than the other labels. In comparison with one of the most well-known emotion datasets using the same annotation scheme, the NRC emotion lexicon (EmoLex) #TARGET_REF , the distribution differs somewhat. Although anger is a large category in both datasets, fear is average in our dataset, but the largest category in EmoLex. It is hard to speculate why this is, but one possible reason is the different source data.",
        "output": "The final dataset contained 17,520 unique emotion-annotated subtitles as shown in table 3. In addition there are some 6.5k subtitles annotated as neutral. The label distribution can be seen in table 3 <BACK> The emotion labels are surprisingly balanced with the exception of anger and anticipation, </BACK> which are more common than the other labels. <PERCEPT> In comparison with one of the most well-known </PERCEPT> <INFO> emotion datasets </INFO> <PERCEPT> using the same annotation scheme, </PERCEPT> <INFO> the NRC emotion lexicon (EmoLex) #TARGET_REF , </INFO> <PERCEPT> the distribution differs somewhat. Although anger is a large category in both datasets, fear is average in our dataset, but the largest category in EmoLex. It is hard to speculate why this is, but one possible reason is the different source data. </PERCEPT>"
    },
    {
        "gold": {
            "text": [
                "14th",
                "century",
                "#TARGET_REF",
                ".",
                "Even",
                "state-of-the-art",
                "multilingual",
                "NLP",
                "systems",
                "perform",
                "sub-optimally",
                "on",
                "Dravidian",
                "languages",
                "#REF",
                ".",
                "This",
                "can",
                "be",
                "explained",
                "by",
                "the",
                "fact",
                "that",
                "multilingual",
                "language",
                "models",
                "are",
                "often",
                "jointly",
                "trained",
                "on",
                "100+",
                "languages",
                "and",
                "Indian",
                "languages",
                "constitute",
                "only",
                "a",
                "small",
                "fraction",
                "of",
                "their",
                "vocabulary",
                "and",
                "training",
                "data",
                "(as",
                "shown",
                "in",
                "Figure",
                "2).",
                "Machine",
                "learning",
                "models",
                "and",
                "tools",
                "have",
                "been",
                "proposed",
                "for",
                "many",
                "Natural",
                "Language",
                "Understanding",
                "tasks.",
                "In",
                "this",
                "work,",
                "we",
                "focus",
                "on",
                "Extractive",
                "Question-Answering",
                "(QA),",
                "where",
                "the",
                "goal",
                "is",
                "to",
                "localize",
                "the",
                "answer",
                "to",
                "a",
                "question",
                "within",
                "a",
                "large",
                "context",
                "(see",
                "Figure",
                "1).",
                "Specifically,",
                "we",
                "aim",
                "to",
                "develop",
                "a",
                "common",
                "multilingual",
                "question",
                "answering",
                "model",
                "for",
                "multiple",
                "Indian",
                "languages.",
                "A",
                "multilingual",
                "model",
                "has",
                "several",
                "advantages:",
                "(1)",
                "learning",
                "of",
                "cues",
                "across",
                "different",
                "languages,",
                "(2)",
                "a",
                "single",
                "model",
                "for",
                "many",
                "languages,",
                "and",
                "(3)",
                "avoiding",
                "dependency",
                "on",
                "English",
                "translation",
                "during",
                "inference.",
                "In",
                "this",
                "work,",
                "we",
                "start",
                "with",
                "a",
                "pre-trained",
                "multilingual",
                "Bidi-",
                "rectional",
                "Encoder",
                "Representations",
                "from",
                "Transformers",
                "(mBERT)",
                "model",
                "and",
                "further",
                "pre-train",
                "it",
                "with",
                "SQuAD",
                "#REF",
                ",",
                "a",
                "large-scale",
                "question",
                "answering",
                "dataset",
                "in",
                "English.",
                "The",
                "resulting",
                "English-language",
                "mBERT-QA",
                "model",
                "is",
                "fine-tuned",
                "and",
                "evaluated",
                "for",
                "Indian",
                "languages",
                "Tamil",
                "and",
                "Hindi",
                "using",
                "the",
                "ChAII",
                "dataset",
                "#REF",
                "."
            ],
            "context": [
                1,
                1,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "14th century #TARGET_REF . Even state-of-the-art multilingual NLP systems perform sub-optimally on Dravidian languages #REF . This can be explained by the fact that multilingual language models are often jointly trained on 100+ languages and Indian languages constitute only a small fraction of their vocabulary and training data (as shown in Figure 2). Machine learning models and tools have been proposed for many Natural Language Understanding tasks. In this work, we focus on Extractive Question-Answering (QA), where the goal is to localize the answer to a question within a large context (see Figure 1). Specifically, we aim to develop a common multilingual question answering model for multiple Indian languages. A multilingual model has several advantages: (1) learning of cues across different languages, (2) a single model for many languages, and (3) avoiding dependency on English translation during inference. In this work, we start with a pre-trained multilingual Bidi- rectional Encoder Representations from Transformers (mBERT) model and further pre-train it with SQuAD #REF , a large-scale question answering dataset in English. The resulting English-language mBERT-QA model is fine-tuned and evaluated for Indian languages Tamil and Hindi using the ChAII dataset #REF .",
        "output": "<INFO> 14th century #TARGET_REF . </INFO> Even state-of-the-art multilingual NLP systems perform sub-optimally on Dravidian languages #REF . This can be explained by the fact that multilingual language models are often jointly trained on 100+ languages and Indian languages constitute only a small fraction of their vocabulary and training data (as shown in Figure 2). Machine learning models and tools have been proposed for many Natural Language Understanding tasks. In this work, we focus on Extractive Question-Answering (QA), where the goal is to localize the answer to a question within a large context (see Figure 1). Specifically, we aim to develop a common multilingual question answering model for multiple Indian languages. A multilingual model has several advantages: (1) learning of cues across different languages, (2) a single model for many languages, and (3) avoiding dependency on English translation during inference. In this work, we start with a pre-trained multilingual Bidi- rectional Encoder Representations from Transformers (mBERT) model and further pre-train it with SQuAD #REF , a large-scale question answering dataset in English. The resulting English-language mBERT-QA model is fine-tuned and evaluated for Indian languages Tamil and Hindi using the ChAII dataset #REF ."
    },
    {
        "gold": {
            "text": [
                "The",
                "Transformer",
                "architecture",
                "#REF",
                "has",
                "been",
                "successful",
                "in",
                "a",
                "wide",
                "range",
                "of",
                "natural",
                "language",
                "processing",
                "tasks,",
                "including",
                "machine",
                "translation",
                "#REF",
                ",",
                "language",
                "modeling",
                "#REF",
                ",",
                "question-answering",
                "#REF",
                ",",
                "and",
                "many",
                "more.",
                "Transformers",
                "pre-trained",
                "on",
                "large",
                "amounts",
                "of",
                "text",
                "with",
                "a",
                "language",
                "modeling",
                "(LM)",
                "objective,",
                "have",
                "become",
                "the",
                "standard",
                "in",
                "NLP,",
                "exhibiting",
                "surprising",
                "amounts",
                "of",
                "linguistic",
                "and",
                "world",
                "knowledge",
                "#TARGET_REF",
                "."
            ],
            "context": [
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1
            ]
        },
        "input": "The Transformer architecture #REF has been successful in a wide range of natural language processing tasks, including machine translation #REF , language modeling #REF , question-answering #REF , and many more. Transformers pre-trained on large amounts of text with a language modeling (LM) objective, have become the standard in NLP, exhibiting surprising amounts of linguistic and world knowledge #TARGET_REF .",
        "output": "<BACK> The Transformer architecture #REF has been successful in a wide range of natural language processing tasks, including machine translation #REF , language modeling #REF , question-answering #REF , and many more. </BACK> <INFO> Transformers pre-trained on large amounts of text with a language modeling (LM) objective, have become the standard in NLP, exhibiting surprising amounts of linguistic and world knowledge #TARGET_REF . </INFO>"
    },
    {
        "gold": {
            "text": [
                "Word",
                "co-occurrence",
                "probabilities",
                "are",
                "hard",
                "to",
                "estimate",
                "accurately",
                "from",
                "text",
                "data",
                "because",
                "empirical",
                "counts",
                "of",
                "a",
                "particular",
                "pair",
                "of",
                "words",
                "in",
                "a",
                "particular",
                "relation",
                "are",
                "often",
                "sparse.",
                "This",
                "limitation",
                "makes",
                "it",
                "hard",
                "to",
                "evaluate",
                "cognitive",
                "theories",
                "that",
                "operate",
                "on",
                "co-occurrence",
                "probabilities.",
                "Although",
                "high-performance",
                "pretrained",
                "language",
                "models",
                "now",
                "exist",
                "#TARGET_REF",
                ",",
                "the",
                "probabilities",
                "of",
                "interest",
                "often",
                "cannot",
                "be",
                "read",
                "off",
                "of",
                "these",
                "models",
                "directly,",
                "because",
                "w",
                "and",
                "c",
                "might",
                "be",
                "defined",
                "by",
                "relations",
                "that",
                "cannot",
                "be",
                "straightforwardly",
                "detected",
                "in",
                "terms",
                "of",
                "linear",
                "word",
                "order",
                "or",
                "templates.",
                "For",
                "example,",
                "suppose",
                "we",
                "are",
                "interested",
                "in",
                "the",
                "distribution",
                "of",
                "adjectives",
                "attributively",
                "modifying",
                "a",
                "noun",
                "in",
                "English.",
                "It",
                "would",
                "not",
                "do",
                "to",
                "ask",
                "a",
                "language",
                "model",
                "for",
                "the",
                "distribution",
                "of",
                "words",
                "immediately",
                "preceding",
                "a",
                "noun,",
                "because",
                "some",
                "of",
                "these",
                "words",
                "will",
                "not",
                "be",
                "attributive",
                "adjectives."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Word co-occurrence probabilities are hard to estimate accurately from text data because empirical counts of a particular pair of words in a particular relation are often sparse. This limitation makes it hard to evaluate cognitive theories that operate on co-occurrence probabilities. Although high-performance pretrained language models now exist #TARGET_REF , the probabilities of interest often cannot be read off of these models directly, because w and c might be defined by relations that cannot be straightforwardly detected in terms of linear word order or templates. For example, suppose we are interested in the distribution of adjectives attributively modifying a noun in English. It would not do to ask a language model for the distribution of words immediately preceding a noun, because some of these words will not be attributive adjectives.",
        "output": "Word co-occurrence probabilities are hard to estimate accurately from text data because empirical counts of a particular pair of words in a particular relation are often sparse. This limitation makes it hard to evaluate cognitive theories that operate on co-occurrence probabilities. Although <INFO> high-performance pretrained language models now exist #TARGET_REF </INFO> , the probabilities of interest often cannot be read off of these models directly, because w and c might be defined by relations that cannot be straightforwardly detected in terms of linear word order or templates. For example, suppose we are interested in the distribution of adjectives attributively modifying a noun in English. It would not do to ask a language model for the distribution of words immediately preceding a noun, because some of these words will not be attributive adjectives."
    },
    {
        "gold": {
            "text": [
                "Giménez",
                "and",
                "Màrquez",
                "(2007b)",
                "and",
                "#TARGET_REF",
                "introduced",
                "and",
                "refined",
                "a",
                "set",
                "of",
                "new",
                "MT",
                "evaluation",
                "metrics",
                "employing",
                "rich",
                "assortments",
                "of",
                "features",
                "reflecting",
                "various",
                "kinds",
                "of",
                "similarity",
                "at",
                "lexical,",
                "shallow",
                "syntactic,",
                "deep",
                "syntactic,",
                "shallow",
                "semantic,",
                "and",
                "deep",
                "semantic",
                "levels."
            ],
            "context": [
                3,
                3,
                3,
                3,
                3,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1
            ]
        },
        "input": "Giménez and Màrquez (2007b) and #TARGET_REF introduced and refined a set of new MT evaluation metrics employing rich assortments of features reflecting various kinds of similarity at lexical, shallow syntactic, deep syntactic, shallow semantic, and deep semantic levels.",
        "output": "<BACK> Giménez and Màrquez (2007b) and </BACK> <INFO> #TARGET_REF introduced and refined a set of new MT evaluation metrics employing rich assortments of features reflecting various kinds of similarity at lexical, shallow syntactic, deep syntactic, shallow semantic, and deep semantic levels. </INFO>"
    },
    {
        "gold": {
            "text": [
                "The",
                "TNT",
                "tagger",
                "#TARGET_REF",
                ",",
                "which",
                "was",
                "used",
                "in",
                "the",
                "process,",
                "relies",
                "heavily",
                "on",
                "context",
                "to",
                "disambiguate",
                "ambiguities.",
                "In",
                "a",
                "word",
                "list",
                "each",
                "word",
                "is",
                "treated",
                "separately,",
                "there",
                "is",
                "no",
                "context,",
                "so",
                "the",
                "word",
                "tagging",
                "quality",
                "is",
                "lower",
                "than",
                "the",
                "values",
                "on",
                "running",
                "text."
            ],
            "context": [
                1,
                1,
                1,
                1,
                1,
                3,
                3,
                3,
                3,
                3,
                3,
                1,
                2,
                1,
                1,
                1,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The TNT tagger #TARGET_REF , which was used in the process, relies heavily on context to disambiguate ambiguities. In a word list each word is treated separately, there is no context, so the word tagging quality is lower than the values on running text.",
        "output": "<INFO> The TNT tagger #TARGET_REF , </INFO> <BACK> which was used in the process, </BACK> <INFO> relies </INFO> <PERCEPT> heavily </PERCEPT> <INFO> on context to disambiguate ambiguities. </INFO> In a word list each word is treated separately, there is no context, so the word tagging quality is lower than the values on running text."
    },
    {
        "gold": {
            "text": [
                "We",
                "have",
                "also",
                "collated",
                "eye",
                "tracking",
                "measurements",
                "such",
                "as",
                "total",
                "fixation",
                "counts,",
                "average",
                "fixation",
                "duration",
                "and",
                "percentage",
                "change",
                "in",
                "pupil",
                "dilation,",
                "all",
                "of",
                "which",
                "are",
                "shown",
                "to",
                "be",
                "indicators",
                "of",
                "cognitive",
                "load",
                "#TARGET_REF",
                "."
            ],
            "context": [
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1
            ]
        },
        "input": "We have also collated eye tracking measurements such as total fixation counts, average fixation duration and percentage change in pupil dilation, all of which are shown to be indicators of cognitive load #TARGET_REF .",
        "output": "<BACK> We have also collated eye tracking measurements such as total fixation counts, average fixation duration and percentage change in pupil dilation, </BACK> <INFO> all of which are shown to be indicators of cognitive load #TARGET_REF . </INFO>"
    },
    {
        "gold": {
            "text": [
                "Instead,",
                "we",
                "can",
                "use",
                "methods",
                "that",
                "require",
                "only",
                "raw",
                "corpora.",
                "For",
                "this,",
                "the",
                "results",
                "of",
                "#TARGET_REF",
                "are",
                "the",
                "most",
                "important",
                "studies",
                "of",
                "associations.",
                "They",
                "have",
                "shown",
                "that",
                "the",
                "amount",
                "of",
                "co-occurrences",
                "of",
                "words",
                "in",
                "a",
                "corpus",
                "is",
                "a",
                "good",
                "indicator",
                "of",
                "the",
                "semantic",
                "relationship",
                "between",
                "them",
                "and",
                "is",
                "also",
                "suitable",
                "for",
                "measuring",
                "the",
                "strength",
                "of",
                "associations.",
                "Bel",
                "#REF",
                "also",
                "predict",
                "associations",
                "from",
                "co-occurrences,",
                "using",
                "a",
                "network",
                "of",
                "bigram",
                "counts.",
                "Similar",
                "to",
                "their",
                "methods,",
                "we",
                "use",
                "weighted",
                "co-occurrences",
                "explicitly",
                "to",
                "model",
                "the",
                "connection",
                "of",
                "words",
                "(for",
                "details,",
                "see",
                "4.1.)."
            ],
            "context": [
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Instead, we can use methods that require only raw corpora. For this, the results of #TARGET_REF are the most important studies of associations. They have shown that the amount of co-occurrences of words in a corpus is a good indicator of the semantic relationship between them and is also suitable for measuring the strength of associations. Bel #REF also predict associations from co-occurrences, using a network of bigram counts. Similar to their methods, we use weighted co-occurrences explicitly to model the connection of words (for details, see 4.1.).",
        "output": "Instead, <PERCEPT> we can use methods that require only raw corpora. For this, </PERCEPT> <INFO> the results of #TARGET_REF are the most important studies of associations. </INFO> <PERCEPT> They have shown that </PERCEPT> <INFO> the amount of co-occurrences of words in a corpus is a good indicator of the semantic relationship between them and is also suitable for measuring the strength of associations. </INFO> Bel #REF also predict associations from co-occurrences, using a network of bigram counts. Similar to their methods, we use weighted co-occurrences explicitly to model the connection of words (for details, see 4.1.)."
    },
    {
        "gold": {
            "text": [
                "Visual",
                "Embedding",
                "We",
                "adopt",
                "a",
                "CNN",
                "backbone",
                "to",
                "extract",
                "image",
                "features",
                "V",
                "=",
                "{v",
                "i",
                "}",
                "L",
                "i=1",
                "for",
                "each",
                "image",
                "I",
                "where",
                "L",
                "is",
                "the",
                "size",
                "of",
                "feature",
                "grids",
                "and",
                "v",
                "i",
                "∈",
                "R",
                "dv",
                "is",
                "a",
                "feature",
                "vector",
                "with",
                "dimension",
                "d",
                "v",
                ".",
                "In",
                "addition,",
                "each",
                "feature",
                "is",
                "further",
                "concatenated",
                "with",
                "its",
                "2-D",
                "sine",
                "position",
                "embedding",
                "#REF",
                ".",
                "Following",
                "SOHO,",
                "we",
                "use",
                "a",
                "ResNet-101",
                "#TARGET_REF",
                "as",
                "the",
                "visual",
                "backbone,",
                "followed",
                "by",
                "additional",
                "1x1",
                "Conv",
                "and",
                "2x2",
                "strides",
                "Max-pooling",
                "to",
                "reduce",
                "the",
                "memory",
                "footprint."
            ],
            "context": [
                0,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2
            ]
        },
        "input": "Visual Embedding We adopt a CNN backbone to extract image features V = {v i } L i=1 for each image I where L is the size of feature grids and v i ∈ R dv is a feature vector with dimension d v . In addition, each feature is further concatenated with its 2-D sine position embedding #REF . Following SOHO, we use a ResNet-101 #TARGET_REF as the visual backbone, followed by additional 1x1 Conv and 2x2 strides Max-pooling to reduce the memory footprint.",
        "output": "Visual Embedding <BACK> We adopt a CNN backbone to extract image features </BACK> V = {v i } L i=1 for each image I where L is the size of feature grids and v i ∈ R dv is a feature vector with dimension d v . In addition, each feature is further concatenated with its 2-D sine position embedding #REF . <PERCEPT> Following SOHO, we use </PERCEPT> <INFO> a ResNet-101 #TARGET_REF </INFO> <PERCEPT> as the visual backbone, followed by additional 1x1 Conv and 2x2 strides Max-pooling to reduce the memory footprint. </PERCEPT>"
    },
    {
        "gold": {
            "text": [
                "especially",
                "to",
                "the",
                "affected",
                "families",
                "and",
                "to",
                "everyone",
                "whom",
                "we",
                "have",
                "offended.",
                "This",
                "is",
                "an",
                "unequivocal",
                "expression",
                "of",
                "apology",
                "and",
                "shows",
                "that",
                "tenderers",
                "do",
                "not",
                "want",
                "to",
                "make",
                "any",
                "excuses",
                "for",
                "their",
                "wrongdoing.",
                "The",
                "gloss",
                "of",
                "selected",
                "sense",
                "of",
                "the",
                "noun",
                "regret",
                "is",
                "sadness",
                "associated",
                "with",
                "some",
                "wrong",
                "done",
                "or",
                "some",
                "disappointment.",
                "The",
                "direct",
                "hypernymy",
                "of",
                "this",
                "is",
                "the",
                "concept",
                "of",
                "sadness",
                "which",
                "is",
                "emotions",
                "experienced",
                "when",
                "not",
                "in",
                "a",
                "state",
                "of",
                "well-being.",
                "This",
                "is",
                "followed",
                "by",
                "the",
                "concept",
                "of",
                "feeling",
                "or",
                "the",
                "experiencing",
                "of",
                "affective",
                "and",
                "emotional",
                "states.",
                "Thus",
                "the",
                "hypernymy",
                "relation",
                "makes",
                "it",
                "clear",
                "that",
                "regret",
                "is",
                "a",
                "kind",
                "of",
                "feeling",
                "associated",
                "with",
                "sadness.",
                "From",
                "a",
                "communicative",
                "point",
                "of",
                "view,",
                "it",
                "is",
                "simply",
                "an",
                "expression",
                "of",
                "an",
                "emotion",
                "on",
                "the",
                "part",
                "of",
                "the",
                "tenderer",
                "of",
                "the",
                "apology",
                "and",
                "not",
                "necessarily",
                "expression",
                "of",
                "remorse",
                "or",
                "liability.",
                "For",
                "example,",
                "in",
                "apology",
                "number",
                "13,",
                "the",
                "Member",
                "of",
                "Parliament",
                "states,",
                "I",
                "write",
                "to",
                "convey",
                "my",
                "regrets",
                "for",
                "the",
                "unfortunate",
                "incident",
                "that",
                "took",
                "place",
                "on",
                "23rd",
                "March",
                "2017",
                "in",
                "the",
                "Air",
                "India",
                "flight",
                "No.",
                "AI",
                "852,",
                "seat",
                "No.1F.",
                "Given",
                "that",
                "the",
                "writer",
                "only",
                "uses",
                "the",
                "noun",
                "regret,",
                "it",
                "can",
                "be",
                "implied",
                "that",
                "the",
                "writer",
                "feels",
                "sad",
                "about",
                "the",
                "incident",
                "but",
                "not",
                "necessarily",
                "repentant.",
                "However,",
                "it",
                "is",
                "important",
                "to",
                "look",
                "at",
                "the",
                "results",
                "of",
                "SentiWordNet",
                "and",
                "WordNet-Affect",
                "to",
                "understand",
                "the",
                "implications",
                "and",
                "underlying",
                "emotions",
                "and",
                "sentiments",
                "before",
                "arriving",
                "at",
                "any",
                "further",
                "conclusions."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "especially to the affected families and to everyone whom we have offended. This is an unequivocal expression of apology and shows that tenderers do not want to make any excuses for their wrongdoing. The gloss of selected sense of the noun regret is sadness associated with some wrong done or some disappointment. The direct hypernymy of this is the concept of sadness which is emotions experienced when not in a state of well-being. This is followed by the concept of feeling or the experiencing of affective and emotional states. Thus the hypernymy relation makes it clear that regret is a kind of feeling associated with sadness. From a communicative point of view, it is simply an expression of an emotion on the part of the tenderer of the apology and not necessarily expression of remorse or liability. For example, in apology number 13, the Member of Parliament states, I write to convey my regrets for the unfortunate incident that took place on 23rd March 2017 in the Air India flight No. AI 852, seat No.1F. Given that the writer only uses the noun regret, it can be implied that the writer feels sad about the incident but not necessarily repentant. However, it is important to look at the results of SentiWordNet and WordNet-Affect to understand the implications and underlying emotions and sentiments before arriving at any further conclusions.",
        "output": "especially to the affected families and to everyone whom we have offended. This is an unequivocal expression of apology and shows that tenderers do not want to make any excuses for their wrongdoing. The gloss of selected sense of the noun regret is sadness associated with some wrong done or some disappointment. The direct hypernymy of this is the concept of sadness which is emotions experienced when not in a state of well-being. This is followed by the concept of feeling or the experiencing of affective and emotional states. Thus the hypernymy relation makes it clear that regret is a kind of feeling associated with sadness. From a communicative point of view, it is simply an expression of an emotion on the part of the tenderer of the apology and not necessarily expression of remorse or liability. For example, in apology number 13, the Member of Parliament states, I write to convey my regrets for the unfortunate incident that took place on 23rd March 2017 in the Air India flight No. AI 852, seat No.1F. Given that the writer only uses the noun regret, it can be implied that the writer feels sad about the incident but not necessarily repentant. However, it is important to look at the results of SentiWordNet and WordNet-Affect to understand the implications and underlying emotions and sentiments before arriving at any further conclusions."
    },
    {
        "gold": {
            "text": [
                "Performance",
                "is",
                "plotted",
                "on",
                "the",
                "vertical",
                "axis",
                "and",
                "measured",
                "for",
                "different",
                "frequency",
                "bins.",
                "The",
                "Zipfian",
                "distribution",
                "of",
                "terms",
                "suggests",
                "assessing",
                "performance",
                "by",
                "frequency",
                "ranges",
                "growing",
                "by",
                "a",
                "constant",
                "factor",
                "#TARGET_REF",
                ",",
                "accordingly",
                "we",
                "used",
                "a",
                "logarithmic",
                "scale",
                "with",
                "base=3.",
                "The",
                "number",
                "of",
                "terms",
                "per",
                "bin",
                "is",
                "given",
                "in",
                "parentheses.",
                "Some",
                "of",
                "the",
                "previous",
                "work",
                "cited",
                "in",
                "Section",
                "2",
                "described",
                "performance",
                "for",
                "selected",
                "subsets",
                "of",
                "terms,",
                "typically",
                "high",
                "frequency",
                "terms",
                "that",
                "are",
                "easier",
                "to",
                "translate.",
                "We",
                "believe",
                "presenting",
                "translation",
                "accuracy",
                "as",
                "a",
                "function",
                "of",
                "source",
                "term",
                "frequency",
                "is",
                "more",
                "informative."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                1,
                1,
                1,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Performance is plotted on the vertical axis and measured for different frequency bins. The Zipfian distribution of terms suggests assessing performance by frequency ranges growing by a constant factor #TARGET_REF , accordingly we used a logarithmic scale with base=3. The number of terms per bin is given in parentheses. Some of the previous work cited in Section 2 described performance for selected subsets of terms, typically high frequency terms that are easier to translate. We believe presenting translation accuracy as a function of source term frequency is more informative.",
        "output": "Performance is plotted on the vertical axis and measured for different frequency bins. <BACK> The Zipfian distribution of terms suggests assessing performance by frequency ranges growing </BACK> <INFO> by a constant factor #TARGET_REF </INFO> , accordingly we used a logarithmic scale with base=3. The number of terms per bin is given in parentheses. Some of the previous work cited in Section 2 described performance for selected subsets of terms, typically high frequency terms that are easier to translate. We believe presenting translation accuracy as a function of source term frequency is more informative."
    },
    {
        "gold": {
            "text": [
                "Despite",
                "of",
                "the",
                "advantages",
                "of",
                "using",
                "CCG",
                "categories",
                "to",
                "label",
                "non-terminals",
                "in",
                "the",
                "HPB",
                "system",
                "compared",
                "with",
                "SAMT",
                "labels,",
                "richness",
                "of",
                "CCG",
                "categories",
                "still",
                "leads",
                "to",
                "a",
                "large",
                "number",
                "of",
                "different",
                "non-terminal",
                "labels.",
                "This",
                "causes",
                "fragmentation",
                "of",
                "rule",
                "probabilities",
                "and",
                "consequently",
                "affects",
                "translation",
                "quality",
                "negatively.",
                "A",
                "CCG",
                "category",
                "C",
                "takes",
                "the",
                "form",
                "of",
                "C=(T\\L)/R",
                "where",
                "L",
                "represents",
                "the",
                "left",
                "argument",
                "category,",
                "R",
                "the",
                "right",
                "argument",
                "category,",
                "and",
                "T",
                "the",
                "resulting",
                "category.",
                "Each",
                "of",
                "these",
                "constituent",
                "categories",
                "might",
                "be",
                "atomic",
                "or",
                "complex.",
                "Furthermore,",
                "some",
                "atomic",
                "CCG",
                "categories",
                "have",
                "features",
                "expressed",
                "between",
                "brackets",
                "which",
                "describe",
                "certain",
                "syntactic",
                "information.",
                "For",
                "example,",
                "the",
                "atomic",
                "category",
                "S",
                "might",
                "have",
                "a",
                "feature",
                "attached",
                "to",
                "it",
                "which",
                "distinguishes",
                "types",
                "of",
                "sentences",
                "such",
                "as",
                "declarative",
                "S[dcl]",
                "or",
                "wh-question",
                "S[wq].",
                "All",
                "the",
                "additional",
                "information",
                "represented",
                "in",
                "a",
                "single",
                "CCG",
                "category",
                "increases",
                "the",
                "number",
                "of",
                "different",
                "CCG",
                "categories",
                "and",
                "leads",
                "to",
                "label",
                "sparsity",
                "problem.",
                "In",
                "order",
                "to",
                "address",
                "this",
                "problem,",
                "we",
                "simplify",
                "CCG",
                "non-terminal",
                "labels",
                "by",
                "reducing",
                "the",
                "amount",
                "of",
                "the",
                "information",
                "represented",
                "in",
                "them",
                "using",
                "the",
                "following",
                "approaches",
                "#TARGET_REF",
                ":"
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1
            ]
        },
        "input": "Despite of the advantages of using CCG categories to label non-terminals in the HPB system compared with SAMT labels, richness of CCG categories still leads to a large number of different non-terminal labels. This causes fragmentation of rule probabilities and consequently affects translation quality negatively. A CCG category C takes the form of C=(T\\L)/R where L represents the left argument category, R the right argument category, and T the resulting category. Each of these constituent categories might be atomic or complex. Furthermore, some atomic CCG categories have features expressed between brackets which describe certain syntactic information. For example, the atomic category S might have a feature attached to it which distinguishes types of sentences such as declarative S[dcl] or wh-question S[wq]. All the additional information represented in a single CCG category increases the number of different CCG categories and leads to label sparsity problem. In order to address this problem, we simplify CCG non-terminal labels by reducing the amount of the information represented in them using the following approaches #TARGET_REF :",
        "output": "Despite of the advantages of using CCG categories to label non-terminals in the HPB system compared with SAMT labels, richness of CCG categories still leads to a large number of different non-terminal labels. This causes fragmentation of rule probabilities and consequently affects translation quality negatively. A CCG category C takes the form of C=(T\\L)/R where L represents the left argument category, R the right argument category, and T the resulting category. Each of these constituent categories might be atomic or complex. Furthermore, some atomic CCG categories have features expressed between brackets which describe certain syntactic information. For example, the atomic category S might have a feature attached to it which distinguishes types of sentences such as declarative S[dcl] or wh-question S[wq]. <BACK> All the additional information represented in a single CCG category increases the number of different CCG categories and leads to label sparsity problem. </BACK> <PERCEPT> In order to address this problem, we simplify CCG non-terminal labels by </PERCEPT> <INFO> reducing the amount of the information represented in them using the following approaches #TARGET_REF : </INFO>"
    },
    {
        "gold": {
            "text": [
                "We",
                "define",
                "text-level",
                "bias",
                "as",
                "the",
                "frequency",
                "of",
                "certain",
                "words",
                "which",
                "are",
                "recognised",
                "as",
                "favouring",
                "one",
                "gender",
                "over",
                "another.",
                "The",
                "problem",
                "is",
                "then",
                "in",
                "defining",
                "this",
                "list",
                "of",
                "words.",
                "To",
                "avoid",
                "overfitting",
                "to",
                "one",
                "axis",
                "of",
                "gender",
                "bias,",
                "we",
                "construct",
                "a",
                "composite",
                "score",
                "based",
                "on",
                "pre-existing",
                "lists",
                "which",
                "have",
                "in",
                "turn",
                "been",
                "defined",
                "through",
                "experiments",
                "and",
                "empirical",
                "assessments",
                "#TARGET_REF",
                ".",
                "The",
                "presence",
                "of",
                "words",
                "which",
                "are",
                "more",
                "likely",
                "to",
                "be",
                "associated",
                "with",
                "one",
                "gender",
                "does",
                "not",
                "directly",
                "result",
                "in",
                "biased",
                "outcomes.",
                "Bias",
                "may",
                "be",
                "more",
                "accurately",
                "measured",
                "as",
                "the",
                "relative",
                "gender",
                "distribution",
                "of",
                "applicants",
                "who",
                "apply",
                "to",
                "a",
                "given",
                "ad.",
                "In",
                "this",
                "work,",
                "we",
                "focus",
                "on",
                "gendered",
                "word",
                "lists",
                "as",
                "one",
                "overt",
                "presentation",
                "of",
                "gender",
                "bias",
                "but",
                "encourage",
                "further",
                "research",
                "to",
                "empirically",
                "measure",
                "allocational",
                "harm,",
                "so",
                "long",
                "as",
                "any",
                "experiments",
                "consider",
                "the",
                "ethical",
                "issues",
                "of",
                "posting",
                "\"fake\"",
                "ads",
                "online."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                0,
                0,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "We define text-level bias as the frequency of certain words which are recognised as favouring one gender over another. The problem is then in defining this list of words. To avoid overfitting to one axis of gender bias, we construct a composite score based on pre-existing lists which have in turn been defined through experiments and empirical assessments #TARGET_REF . The presence of words which are more likely to be associated with one gender does not directly result in biased outcomes. Bias may be more accurately measured as the relative gender distribution of applicants who apply to a given ad. In this work, we focus on gendered word lists as one overt presentation of gender bias but encourage further research to empirically measure allocational harm, so long as any experiments consider the ethical issues of posting \"fake\" ads online.",
        "output": "We define text-level bias as the frequency of certain <BACK> words which are recognised as favouring one gender over another. The problem is then in defining this list of words. </BACK> <PERCEPT> To avoid overfitting to one axis of gender bias, we construct a composite score based on </PERCEPT> <INFO> pre-existing lists which have </INFO> in turn <INFO> been defined through experiments and empirical assessments #TARGET_REF </INFO> . The presence of words which are more likely to be associated with one gender does not directly result in biased outcomes. Bias may be more accurately measured as the relative gender distribution of applicants who apply to a given ad. In this work, we focus on gendered word lists as one overt presentation of gender bias but encourage further research to empirically measure allocational harm, so long as any experiments consider the ethical issues of posting \"fake\" ads online."
    },
    {
        "gold": {
            "text": [
                "The",
                "type",
                "of",
                "information",
                "recovered",
                "by",
                "the",
                "NP",
                "Enrichment",
                "task",
                "complements",
                "well-established",
                "core",
                "NLP",
                "tasks",
                "such",
                "as",
                "entity",
                "typing,",
                "entity",
                "linking,",
                "coreference",
                "resolution,",
                "and",
                "semantic-role",
                "labeling",
                "#REF",
                ".",
                "We",
                "believe",
                "it",
                "serves",
                "as",
                "an",
                "important",
                "and",
                "much-needed",
                "building",
                "block",
                "for",
                "downstream",
                "applications",
                "that",
                "require",
                "text",
                "understanding,",
                "including",
                "information",
                "retrieval,",
                "relation",
                "extraction",
                "and",
                "event",
                "extraction,",
                "question",
                "answering,",
                "and",
                "so",
                "on.",
                "In",
                "particular,",
                "the",
                "NP",
                "Enrichment",
                "task",
                "neatly",
                "encapsulates",
                "much",
                "of",
                "the",
                "long-range",
                "information",
                "that",
                "is",
                "often",
                "required",
                "by",
                "such",
                "applications.",
                "Take",
                "for",
                "example",
                "a",
                "system",
                "that",
                "attempts",
                "to",
                "extract",
                "reports",
                "on",
                "police",
                "shooting",
                "incidents",
                "#TARGET_REF",
                ",",
                "with",
                "the",
                "following",
                "challenging,",
                "but",
                "not",
                "uncommon,",
                "passage:",
                "2"
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The type of information recovered by the NP Enrichment task complements well-established core NLP tasks such as entity typing, entity linking, coreference resolution, and semantic-role labeling #REF . We believe it serves as an important and much-needed building block for downstream applications that require text understanding, including information retrieval, relation extraction and event extraction, question answering, and so on. In particular, the NP Enrichment task neatly encapsulates much of the long-range information that is often required by such applications. Take for example a system that attempts to extract reports on police shooting incidents #TARGET_REF , with the following challenging, but not uncommon, passage: 2",
        "output": "The type of information recovered by the NP Enrichment task complements well-established core NLP tasks such as entity typing, entity linking, coreference resolution, and semantic-role labeling #REF . We believe it serves as an important and much-needed building block for downstream applications that require text understanding, including information retrieval, relation extraction and event extraction, question answering, and so on. In particular, the NP Enrichment task neatly encapsulates much of the long-range information that is often required by such applications. Take for example <INFO> a system that attempts to extract reports on police shooting incidents #TARGET_REF </INFO> , with the following challenging, but not uncommon, passage: 2"
    },
    {
        "gold": {
            "text": [
                "Ten",
                "#TARGET_REF",
                "describes",
                "the",
                "opposition",
                "between",
                "the",
                "coverage",
                "of",
                "the",
                "lexical",
                "component",
                "in",
                "the",
                "standard",
                "approach",
                "and",
                "in",
                "WM",
                "in",
                "terms",
                "of",
                "two",
                "orthogonal",
                "dichotomies:"
            ],
            "context": [
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1
            ]
        },
        "input": "Ten #TARGET_REF describes the opposition between the coverage of the lexical component in the standard approach and in WM in terms of two orthogonal dichotomies:",
        "output": "<INFO> Ten #TARGET_REF describes the opposition between the coverage of the lexical component in the standard approach and in WM in terms of two orthogonal dichotomies: </INFO>"
    },
    {
        "gold": {
            "text": [
                "Table",
                "2",
                "shows",
                "the",
                "main",
                "experimental",
                "results.",
                "We",
                "can",
                "see",
                "that",
                "RocketQA",
                "significantly",
                "outperforms",
                "all",
                "the",
                "baselines",
                "on",
                "both",
                "MSMARCO",
                "and",
                "NQ",
                "datasets.",
                "Another",
                "observation",
                "is",
                "that",
                "the",
                "dense",
                "retrievers",
                "are",
                "overall",
                "better",
                "than",
                "the",
                "sparse",
                "retrievers.",
                "Such",
                "a",
                "finding",
                "has",
                "also",
                "been",
                "reported",
                "in",
                "previous",
                "studies",
                "#TARGET_REF",
                ",",
                "which",
                "indicates",
                "the",
                "effectiveness",
                "of",
                "the",
                "dense",
                "retrieval",
                "approach."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1
            ]
        },
        "input": "Table 2 shows the main experimental results. We can see that RocketQA significantly outperforms all the baselines on both MSMARCO and NQ datasets. Another observation is that the dense retrievers are overall better than the sparse retrievers. Such a finding has also been reported in previous studies #TARGET_REF , which indicates the effectiveness of the dense retrieval approach.",
        "output": "Table 2 shows the main experimental results. We can see that RocketQA significantly outperforms all the baselines on both MSMARCO and NQ datasets. <BACK> Another observation is that the dense retrievers are overall better than the sparse retrievers. Such a finding has also been reported in </BACK> <INFO> previous studies #TARGET_REF , which indicates the effectiveness of the dense retrieval approach. </INFO>"
    },
    {
        "gold": {
            "text": [
                "Third,",
                "it",
                "is",
                "expensive",
                "to",
                "acquire",
                "large-scale",
                "training",
                "data",
                "for",
                "open-domain",
                "QA.",
                "MSMARCO",
                "and",
                "Natural",
                "Questions",
                "#TARGET_REF",
                "are",
                "two",
                "largest",
                "datasets",
                "for",
                "open-domain",
                "QA.",
                "They",
                "are",
                "created",
                "from",
                "commercial",
                "search",
                "engines,",
                "and",
                "have",
                "516K",
                "and",
                "300K",
                "annotated",
                "questions,",
                "respectively.",
                "However,",
                "it",
                "is",
                "still",
                "insufficient",
                "to",
                "cover",
                "all",
                "the",
                "topics",
                "of",
                "questions",
                "issued",
                "by",
                "users",
                "to",
                "search",
                "engines."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Third, it is expensive to acquire large-scale training data for open-domain QA. MSMARCO and Natural Questions #TARGET_REF are two largest datasets for open-domain QA. They are created from commercial search engines, and have 516K and 300K annotated questions, respectively. However, it is still insufficient to cover all the topics of questions issued by users to search engines.",
        "output": "Third, it is expensive to acquire large-scale training data for open-domain QA. <BACK> MSMARCO and </BACK> <INFO> Natural Questions #TARGET_REF are two largest datasets for open-domain QA. </INFO> <BACK> They are created from commercial search engines, and have 516K and 300K annotated questions, respectively. </BACK> However, it is still insufficient to cover all the topics of questions issued by users to search engines."
    },
    {
        "gold": {
            "text": [
                "At",
                "the",
                "answer",
                "level",
                "the",
                "top",
                "candidate",
                "sentences",
                "(up",
                "to",
                "10)",
                "returned",
                "by",
                "our",
                "system",
                "were",
                "compared",
                "against",
                "the",
                "review",
                "snippets",
                "as",
                "the",
                "gold",
                "standard.",
                "The",
                "review",
                "snippets",
                "were",
                "top",
                "review",
                "sentences",
                "returned",
                "by",
                "the",
                "system",
                "used",
                "by",
                "#TARGET_REF",
                "#REF",
                "Average",
                "ROUGE",
                "scores",
                "are",
                "reported",
                "in",
                "Table",
                "2.",
                "Both",
                "systems",
                "aim",
                "at",
                "providing",
                "the",
                "best",
                "candidate",
                "sentences.",
                "Looking",
                "at",
                "the",
                "precision",
                "scores,",
                "it",
                "is",
                "clear",
                "that",
                "our",
                "system",
                "performance",
                "is",
                "good",
                "in",
                "terms",
                "of",
                "returning",
                "relevant",
                "sentences,",
                "similar",
                "in",
                "content",
                "to",
                "the",
                "gold",
                "standard.",
                "The",
                "sim",
                "method",
                "still",
                "is",
                "the",
                "best",
                "performing",
                "method.",
                "We",
                "say",
                "this",
                "because,",
                "ROUGE-L",
                "looks",
                "for",
                "the",
                "longest",
                "common",
                "sub",
                "se-",
                "Looking",
                "at",
                "the",
                "similarity",
                "scores,",
                "it",
                "is",
                "clear",
                "that",
                "the",
                "candidate",
                "sentences",
                "returned",
                "by",
                "our",
                "system",
                "is",
                "almost",
                "exactly",
                "similar",
                "to",
                "the",
                "sentences",
                "returned",
                "by",
                "#REF",
                ".",
                "Once",
                "again",
                "our",
                "system",
                "is",
                "able",
                "to",
                "perform",
                "on",
                "par",
                "with",
                "a",
                "more",
                "complicated",
                "system."
            ],
            "context": [
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                1,
                1,
                1,
                1,
                1,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "At the answer level the top candidate sentences (up to 10) returned by our system were compared against the review snippets as the gold standard. The review snippets were top review sentences returned by the system used by #TARGET_REF #REF Average ROUGE scores are reported in Table 2. Both systems aim at providing the best candidate sentences. Looking at the precision scores, it is clear that our system performance is good in terms of returning relevant sentences, similar in content to the gold standard. The sim method still is the best performing method. We say this because, ROUGE-L looks for the longest common sub se- Looking at the similarity scores, it is clear that the candidate sentences returned by our system is almost exactly similar to the sentences returned by #REF . Once again our system is able to perform on par with a more complicated system.",
        "output": "<PERCEPT> At the answer level the top candidate sentences (up to 10) returned by our system were compared against the review snippets as the gold standard. </PERCEPT> <BACK> The review snippets were top review sentences returned by </BACK> <INFO> the system used by #TARGET_REF </INFO> <BACK> #REF Average ROUGE scores are reported in Table 2. </BACK> <PERCEPT> Both systems aim at providing the best candidate sentences. Looking at the precision scores, it is clear that our system performance is good in terms of returning relevant sentences, similar in content to the gold standard. </PERCEPT> The sim method still is the best performing method. We say this because, ROUGE-L looks for the longest common sub se- Looking at the similarity scores, it is clear that the candidate sentences returned by our system is almost exactly similar to the sentences returned by #REF . Once again our system is able to perform on par with a more complicated system."
    },
    {
        "gold": {
            "text": [
                "The",
                "MOST",
                "FREQUENT",
                "baseline",
                "chooses",
                "the",
                "most",
                "frequently",
                "observed",
                "entity",
                "for",
                "a",
                "given",
                "mention",
                "as",
                "a",
                "prediction,",
                "based",
                "on",
                "a",
                "prior",
                "probability",
                "p",
                "prior",
                "computed",
                "from",
                "link",
                "counts",
                "on",
                "Wikipedia.",
                "All",
                "baselines",
                "except",
                "MOST",
                "FREQUENT",
                "combine",
                "the",
                "classifier",
                "output",
                "and",
                "the",
                "prior",
                "probability",
                "to",
                "make",
                "a",
                "prediction:",
                "arg",
                "max",
                "c",
                "p",
                "prior",
                "(c)",
                "+",
                "p",
                "classifier",
                "(c)",
                ".",
                "10",
                "data.",
                "Our",
                "approach",
                "outperforms",
                "all",
                "baselines,",
                "indicating",
                "that",
                "our",
                "entity",
                "representations",
                "include",
                "useful",
                "information",
                "about",
                "entities",
                "out-of-the-box.",
                "Such",
                "a",
                "performance",
                "gap",
                "is",
                "expected",
                "since",
                "our",
                "entity",
                "representations",
                "can",
                "directly",
                "encode",
                "some",
                "factual",
                "knowledge",
                "from",
                "Wikipedia.",
                "However,",
                "these",
                "results",
                "also",
                "imply",
                "that",
                "pre-trained",
                "LMs",
                "do",
                "not",
                "have",
                "enough",
                "factual",
                "information",
                "out-of-the-box,",
                "they",
                "may",
                "rely",
                "on",
                "in-domain",
                "fine-tuning",
                "to",
                "achieve",
                "high",
                "performance",
                "in",
                "the",
                "target",
                "domain,",
                "and",
                "often",
                "fail",
                "to",
                "generalize",
                "to",
                "new",
                "settings.",
                "Note",
                "that",
                "while",
                "these",
                "accuracies",
                "are",
                "significantly",
                "below",
                "the",
                "supervised",
                "state-of-the-art",
                "(95%),",
                "they",
                "are",
                "competitive",
                "with",
                "the",
                "\"zero-shot\"",
                "entity",
                "results",
                "from",
                "recent",
                "past",
                "work",
                "#TARGET_REF",
                "."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                1
            ]
        },
        "input": "The MOST FREQUENT baseline chooses the most frequently observed entity for a given mention as a prediction, based on a prior probability p prior computed from link counts on Wikipedia. All baselines except MOST FREQUENT combine the classifier output and the prior probability to make a prediction: arg max c p prior (c) + p classifier (c) . 10 data. Our approach outperforms all baselines, indicating that our entity representations include useful information about entities out-of-the-box. Such a performance gap is expected since our entity representations can directly encode some factual knowledge from Wikipedia. However, these results also imply that pre-trained LMs do not have enough factual information out-of-the-box, they may rely on in-domain fine-tuning to achieve high performance in the target domain, and often fail to generalize to new settings. Note that while these accuracies are significantly below the supervised state-of-the-art (95%), they are competitive with the \"zero-shot\" entity results from recent past work #TARGET_REF .",
        "output": "The MOST FREQUENT baseline chooses the most frequently observed entity for a given mention as a prediction, based on a prior probability p prior computed from link counts on Wikipedia. All baselines except MOST FREQUENT combine the classifier output and the prior probability to make a prediction: arg max c p prior (c) + p classifier (c) . 10 data. Our approach outperforms all baselines, indicating that our entity representations include useful information about entities out-of-the-box. Such a performance gap is expected since our entity representations can directly encode some factual knowledge from Wikipedia. However, these results also imply that pre-trained LMs do not have enough factual information out-of-the-box, they may rely on in-domain fine-tuning to achieve high performance in the target domain, and often fail to generalize to new settings. Note that <PERCEPT> while these accuracies are significantly below the supervised state-of-the-art (95%), they are competitive with the \"zero-shot\" entity results from </PERCEPT> <INFO> recent past work #TARGET_REF . </INFO>"
    },
    {
        "gold": {
            "text": [
                "This",
                "paper",
                "reports",
                "the",
                "submissions",
                "of",
                "the",
                "Am-rita_CEN_NLP",
                "team",
                "for",
                "the",
                "3C",
                "Citation",
                "Context",
                "Classification",
                "shared",
                "task",
                "#REF",
                ".",
                "We",
                "used",
                "deep",
                "learning",
                "and",
                "machine",
                "learning",
                "models",
                "developed",
                "using",
                "Bi-LSTM",
                "and",
                "Random",
                "Forest",
                "algorithms",
                "#TARGET_REF",
                ",",
                "#REF",
                ",",
                "#REF",
                "to",
                "complete",
                "the",
                "subtasks.",
                "They",
                "will",
                "be",
                "elaborated",
                "upon",
                "in",
                "Section",
                "4."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                3,
                3,
                1,
                1,
                1,
                0,
                3,
                3,
                3,
                3,
                2,
                2,
                2,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "This paper reports the submissions of the Am-rita_CEN_NLP team for the 3C Citation Context Classification shared task #REF . We used deep learning and machine learning models developed using Bi-LSTM and Random Forest algorithms #TARGET_REF , #REF , #REF to complete the subtasks. They will be elaborated upon in Section 4.",
        "output": "This paper reports the submissions of the Am-rita_CEN_NLP team for <BACK> the 3C Citation Context Classification shared task #REF . </BACK> <PERCEPT> We used deep learning and machine learning models developed using </PERCEPT> <BACK> Bi-LSTM and </BACK> <INFO> Random Forest algorithms </INFO> #TARGET_REF <BACK> , #REF , #REF </BACK> <PERCEPT> to complete the subtasks. </PERCEPT> They will be elaborated upon in Section 4."
    },
    {
        "gold": {
            "text": [
                "This",
                "2021",
                "instantiation",
                "of",
                "the",
                "Shared",
                "Task",
                "on",
                "Explanation",
                "Regeneration",
                "focuses",
                "on",
                "the",
                "theme",
                "of",
                "determining",
                "relevance",
                "in",
                "large",
                "multi-hop",
                "explanations.",
                "To",
                "this",
                "end,",
                "participants",
                "were",
                "given",
                "access",
                "to",
                "a",
                "large",
                "pre-release",
                "dataset",
                "of",
                "approximately",
                "250k",
                "explanatory",
                "relevancy",
                "ratings",
                "that",
                "augment",
                "the",
                "2020",
                "shared",
                "task",
                "data",
                "#TARGET_REF",
                ",",
                "and",
                "were",
                "tasked",
                "with",
                "ranking",
                "the",
                "facts",
                "most",
                "critical",
                "to",
                "assembling",
                "large",
                "explanations",
                "for",
                "a",
                "given",
                "question",
                "highest.",
                "Similarly",
                "to",
                "the",
                "previous",
                "instances",
                "of",
                "our",
                "competition,",
                "the",
                "shared",
                "task",
                "has",
                "been",
                "organized",
                "on",
                "the",
                "CodaLab",
                "platform.",
                "1",
                "We",
                "released",
                "train",
                "and",
                "development",
                "datasets",
                "along",
                "with",
                "the",
                "baseline",
                "solution",
                "in",
                "advance",
                "to",
                "allow",
                "one",
                "to",
                "get",
                "to",
                "know",
                "the",
                "task",
                "specifics.",
                "2",
                "We",
                "ran",
                "the",
                "practice",
                "phase",
                "from",
                "February",
                "15",
                "till",
                "March",
                "9,",
                "2021.",
                "Then",
                "we",
                "released",
                "the",
                "test",
                "dataset",
                "without",
                "answers",
                "and",
                "ran",
                "the",
                "official",
                "evaluation",
                "phase",
                "from",
                "March",
                "10",
                "till",
                "March",
                "24,",
                "2021.",
                "After",
                "that",
                "we",
                "established",
                "postcompetition",
                "phase",
                "to",
                "enable",
                "long-term",
                "evaluation",
                "of",
                "the",
                "methods",
                "beyond",
                "our",
                "shared",
                "task.",
                "Participating",
                "systems",
                "substantially",
                "increased",
                "task",
                "performance",
                "compared",
                "to",
                "a",
                "supplied",
                "baseline",
                "system",
                "by",
                "32%,",
                "while",
                "achieving",
                "moderate",
                "overall",
                "absolute",
                "task",
                "performance",
                "-highlighting",
                "both",
                "the",
                "success",
                "of",
                "this",
                "shared",
                "task,",
                "as",
                "well",
                "as",
                "the",
                "continued",
                "challenge",
                "of",
                "determining",
                "relevancy",
                "in",
                "large",
                "multi-hop",
                "inference",
                "problems."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                1,
                1,
                1,
                1,
                1,
                1,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "This 2021 instantiation of the Shared Task on Explanation Regeneration focuses on the theme of determining relevance in large multi-hop explanations. To this end, participants were given access to a large pre-release dataset of approximately 250k explanatory relevancy ratings that augment the 2020 shared task data #TARGET_REF , and were tasked with ranking the facts most critical to assembling large explanations for a given question highest. Similarly to the previous instances of our competition, the shared task has been organized on the CodaLab platform. 1 We released train and development datasets along with the baseline solution in advance to allow one to get to know the task specifics. 2 We ran the practice phase from February 15 till March 9, 2021. Then we released the test dataset without answers and ran the official evaluation phase from March 10 till March 24, 2021. After that we established postcompetition phase to enable long-term evaluation of the methods beyond our shared task. Participating systems substantially increased task performance compared to a supplied baseline system by 32%, while achieving moderate overall absolute task performance -highlighting both the success of this shared task, as well as the continued challenge of determining relevancy in large multi-hop inference problems.",
        "output": "This 2021 instantiation of the Shared Task on Explanation Regeneration focuses on the theme of determining relevance in large multi-hop explanations. To this end, <BACK> participants were given access to a large pre-release dataset of approximately 250k explanatory relevancy ratings that augment the </BACK> <INFO> 2020 shared task data #TARGET_REF , </INFO> <BACK> and were tasked with ranking the facts most critical to assembling large explanations for a given question highest. </BACK> Similarly to the previous instances of our competition, the shared task has been organized on the CodaLab platform. 1 We released train and development datasets along with the baseline solution in advance to allow one to get to know the task specifics. 2 We ran the practice phase from February 15 till March 9, 2021. Then we released the test dataset without answers and ran the official evaluation phase from March 10 till March 24, 2021. After that we established postcompetition phase to enable long-term evaluation of the methods beyond our shared task. Participating systems substantially increased task performance compared to a supplied baseline system by 32%, while achieving moderate overall absolute task performance -highlighting both the success of this shared task, as well as the continued challenge of determining relevancy in large multi-hop inference problems."
    },
    {
        "gold": {
            "text": [
                "We",
                "analyzed",
                "the",
                "results",
                "related",
                "to",
                "the",
                "keywords",
                "in",
                "WordNet-Affect",
                "#TARGET_REF",
                "),",
                "a",
                "linguistic",
                "resource",
                "for",
                "the",
                "lexical",
                "representation",
                "of",
                "affective",
                "knowledge.",
                "In",
                "this",
                "the",
                "affective",
                "concepts",
                "representing",
                "emotional",
                "state",
                "are",
                "individuated",
                "by",
                "synsets",
                "marked",
                "with",
                "the",
                "alabel",
                "EMOTION.",
                "There",
                "are",
                "also",
                "other",
                "a-labels",
                "for",
                "those",
                "concepts",
                "representing",
                "moods,",
                "situations",
                "eliciting",
                "emotions,",
                "or",
                "emotional",
                "responses."
            ],
            "context": [
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3
            ]
        },
        "input": "We analyzed the results related to the keywords in WordNet-Affect #TARGET_REF ), a linguistic resource for the lexical representation of affective knowledge. In this the affective concepts representing emotional state are individuated by synsets marked with the alabel EMOTION. There are also other a-labels for those concepts representing moods, situations eliciting emotions, or emotional responses.",
        "output": "<PERCEPT> We analyzed the results related to the keywords in </PERCEPT> <INFO> WordNet-Affect #TARGET_REF ), a linguistic resource for the lexical representation of affective knowledge. </INFO> <BACK> In this the affective concepts representing emotional state are individuated by synsets marked with the alabel EMOTION. There are also other a-labels for those concepts representing moods, situations eliciting emotions, or emotional responses. </BACK>"
    },
    {
        "gold": {
            "text": [
                "Objective",
                "and",
                "subjective",
                "evaluation",
                "methods",
                "were",
                "used",
                "in",
                "final",
                "testing",
                "as",
                "only",
                "a",
                "correct",
                "mixture",
                "of",
                "methods",
                "minimizes",
                "evaluation",
                "bias.",
                "Translation",
                "quality",
                "evaluation",
                "was",
                "conducted",
                "using",
                "subjective",
                "evaluation",
                "methods,",
                "where",
                "a",
                "group",
                "of",
                "native",
                "and",
                "near-native",
                "speakers",
                "scored",
                "translations.",
                "Automatic",
                "objective",
                "measures",
                "#REF",
                "were",
                "used",
                "to",
                "ensure",
                "wider",
                "coverage.",
                "Bilingual",
                "corpus",
                "#TARGET_REF",
                "was",
                "use",
                "d",
                "in",
                "all",
                "evaluation",
                "processes."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2
            ]
        },
        "input": "Objective and subjective evaluation methods were used in final testing as only a correct mixture of methods minimizes evaluation bias. Translation quality evaluation was conducted using subjective evaluation methods, where a group of native and near-native speakers scored translations. Automatic objective measures #REF were used to ensure wider coverage. Bilingual corpus #TARGET_REF was use d in all evaluation processes.",
        "output": "Objective and subjective evaluation methods were used in final testing as only a correct mixture of methods minimizes evaluation bias. Translation quality evaluation was conducted using subjective evaluation methods, where a group of native and near-native speakers scored translations. Automatic objective measures #REF were used to ensure wider coverage. <INFO> Bilingual corpus #TARGET_REF </INFO> <PERCEPT> was use d in all evaluation processes. </PERCEPT>"
    },
    {
        "gold": {
            "text": [
                "text",
                "generation",
                "of",
                "CTERM-GAN",
                "with",
                "that",
                "of",
                "SeqGAN",
                "#REF",
                ",",
                "SentiGAN",
                "#REF",
                "and",
                "an",
                "RNNLM",
                "baseline",
                "#REF",
                ".",
                "Following",
                "the",
                "experiments",
                "carried",
                "out",
                "in",
                "#TARGET_REF",
                ",",
                "the",
                "conditioning",
                "has",
                "been",
                "performed",
                "based",
                "on",
                "only",
                "two",
                "sentiments,",
                "positive",
                "or",
                "negative.",
                "In",
                "this",
                "case,",
                "the",
                "conditioning",
                "vector,",
                "c,",
                "taken",
                "as",
                "input",
                "by",
                "CTERM-GAN",
                "is",
                "a",
                "one-hot",
                "binary",
                "variable",
                "representing",
                "the",
                "desired",
                "sentiment.",
                "The",
                "RNNLM",
                "and",
                "SeqGAN",
                "models",
                "have",
                "been",
                "trained",
                "separately",
                "on",
                "the",
                "two",
                "sentiments",
                "by",
                "treating",
                "positive",
                "and",
                "negative",
                "sentences",
                "as",
                "two",
                "separate",
                "datasets,",
                "while",
                "SentiGAN",
                "and",
                "the",
                "proposed",
                "model",
                "have",
                "been",
                "trained",
                "jointly.",
                "This",
                "procedure",
                "makes",
                "the",
                "results",
                "comparable",
                "although",
                "it",
                "is",
                "clear",
                "how",
                "the",
                "flexibility",
                "of",
                "SentiGAN",
                "and",
                "CTERM-GAN",
                "makes",
                "these",
                "models",
                "more",
                "general.",
                "Dataset",
                "We",
                "have",
                "used",
                "two",
                "datasets,",
                "Movie",
                "Reviews",
                "(MR)",
                "#REF",
                "and",
                "Customer",
                "Reviews",
                "(CR)",
                "#REF",
                ",",
                "where",
                "individual",
                "sentences",
                "are",
                "annotated",
                "as",
                "either",
                "positive",
                "or",
                "negative.",
                "The",
                "Movie",
                "Reviews",
                "dataset",
                "consists",
                "of",
                "user",
                "reviews",
                "of",
                "movies,",
                "with",
                "2,",
                "133",
                "positive",
                "and",
                "2,",
                "370",
                "negative",
                "sentences.",
                "The",
                "Customer",
                "Reviews",
                "dataset",
                "consists",
                "of",
                "1,",
                "500",
                "reviews",
                "of",
                "products",
                "sold",
                "online,",
                "with",
                "positive/negative",
                "annotation",
                "at",
                "sentence",
                "level.",
                "For",
                "this",
                "task,",
                "only",
                "sentences",
                "of",
                "length",
                "shorter",
                "than",
                "15",
                "words",
                "have",
                "been",
                "retained,",
                "to",
                "be",
                "able",
                "to",
                "use",
                "the",
                "same",
                "preprocessing",
                "as",
                "#REF",
                "."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "text generation of CTERM-GAN with that of SeqGAN #REF , SentiGAN #REF and an RNNLM baseline #REF . Following the experiments carried out in #TARGET_REF , the conditioning has been performed based on only two sentiments, positive or negative. In this case, the conditioning vector, c, taken as input by CTERM-GAN is a one-hot binary variable representing the desired sentiment. The RNNLM and SeqGAN models have been trained separately on the two sentiments by treating positive and negative sentences as two separate datasets, while SentiGAN and the proposed model have been trained jointly. This procedure makes the results comparable although it is clear how the flexibility of SentiGAN and CTERM-GAN makes these models more general. Dataset We have used two datasets, Movie Reviews (MR) #REF and Customer Reviews (CR) #REF , where individual sentences are annotated as either positive or negative. The Movie Reviews dataset consists of user reviews of movies, with 2, 133 positive and 2, 370 negative sentences. The Customer Reviews dataset consists of 1, 500 reviews of products sold online, with positive/negative annotation at sentence level. For this task, only sentences of length shorter than 15 words have been retained, to be able to use the same preprocessing as #REF .",
        "output": "text generation of CTERM-GAN with that of SeqGAN #REF , <INFO> SentiGAN </INFO> #REF and an RNNLM baseline #REF . <PERCEPT> Following the experiments carried out in #TARGET_REF , the conditioning has been performed based on only two sentiments, positive or negative. </PERCEPT> In this case, the conditioning vector, c, taken as input by CTERM-GAN is a one-hot binary variable representing the desired sentiment. The RNNLM and SeqGAN models have been trained separately on the two sentiments by treating positive and negative sentences as two separate datasets, while SentiGAN and the proposed model have been trained jointly. This procedure makes the results comparable although it is clear how the flexibility of SentiGAN and CTERM-GAN makes these models more general. Dataset We have used two datasets, Movie Reviews (MR) #REF and Customer Reviews (CR) #REF , where individual sentences are annotated as either positive or negative. The Movie Reviews dataset consists of user reviews of movies, with 2, 133 positive and 2, 370 negative sentences. The Customer Reviews dataset consists of 1, 500 reviews of products sold online, with positive/negative annotation at sentence level. For this task, only sentences of length shorter than 15 words have been retained, to be able to use the same preprocessing as #REF ."
    },
    {
        "gold": {
            "text": [
                "Pairwise",
                "diversity",
                "(⇓).",
                "Referred",
                "as",
                "\"self-\"",
                "(e.g.,",
                "self-BLEU)",
                "#TARGET_REF",
                ",",
                "it",
                "measures",
                "the",
                "within-distribution",
                "similarity.",
                "This",
                "metric",
                "computes",
                "the",
                "average",
                "of",
                "sentence-level",
                "metrics",
                "between",
                "all",
                "pairwise",
                "combinations",
                "of",
                "hypotheses",
                "{Y",
                "(1)",
                ",",
                "•",
                "•",
                "•",
                ",",
                "Y",
                "(K)",
                "}",
                "generated",
                "from",
                "each",
                "source",
                "sequence",
                "X.",
                "Lower",
                "pairwise",
                "metric",
                "indicates",
                "high",
                "diversity",
                "between",
                "generated",
                "hypotheses."
            ],
            "context": [
                3,
                3,
                3,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Pairwise diversity (⇓). Referred as \"self-\" (e.g., self-BLEU) #TARGET_REF , it measures the within-distribution similarity. This metric computes the average of sentence-level metrics between all pairwise combinations of hypotheses {Y (1) , • • • , Y (K) } generated from each source sequence X. Lower pairwise metric indicates high diversity between generated hypotheses.",
        "output": "<BACK> Pairwise diversity (⇓). </BACK> <INFO> Referred as \"self-\" (e.g., self-BLEU) #TARGET_REF , </INFO> <BACK> it measures the within-distribution similarity. </BACK> This metric computes the average of sentence-level metrics between all pairwise combinations of hypotheses {Y (1) , • • • , Y (K) } generated from each source sequence X. Lower pairwise metric indicates high diversity between generated hypotheses."
    },
    {
        "gold": {
            "text": [
                "There",
                "are",
                "relatively",
                "few",
                "studies",
                "of",
                "the",
                "usability",
                "of",
                "raw",
                "machine",
                "translated",
                "documentation",
                "by",
                "real",
                "end-users.",
                "For",
                "example,",
                "Tomita's",
                "work",
                "#REF",
                "focused",
                "on",
                "the",
                "concept",
                "of",
                "content",
                "comprehension.",
                "#REF",
                "evaluated",
                "the",
                "informativeness,",
                "comprehension,",
                "and",
                "fluency",
                "of",
                "MT",
                "output,",
                "where",
                "participants",
                "had",
                "no",
                "reference",
                "to",
                "the",
                "source",
                "text,",
                "while",
                "#REF",
                "measured",
                "the",
                "concept",
                "of",
                "usefulness.",
                "#TARGET_REF",
                "measure",
                "the",
                "readability",
                "of",
                "MT",
                "output.",
                "While",
                "comprehensibility",
                "and",
                "readability",
                "are",
                "frequently",
                "considered",
                "to",
                "be",
                "components",
                "of",
                "usability,",
                "these",
                "studies",
                "address",
                "only",
                "specific",
                "aspects",
                "of",
                "the",
                "concept",
                "of",
                "usability.",
                "#REF",
                "in",
                "which",
                "real",
                "end",
                "users'",
                "needs",
                "are",
                "evaluated",
                "in",
                "the",
                "context",
                "of",
                "web",
                "usability",
                "comes",
                "closest",
                "to",
                "the",
                "study",
                "presented",
                "here.",
                "However,",
                "Gaspari's",
                "focus",
                "is",
                "on",
                "the",
                "usability",
                "of",
                "online",
                "MT",
                "systems,",
                "as",
                "opposed",
                "to",
                "the",
                "text",
                "they",
                "generate."
            ],
            "context": [
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "There are relatively few studies of the usability of raw machine translated documentation by real end-users. For example, Tomita's work #REF focused on the concept of content comprehension. #REF evaluated the informativeness, comprehension, and fluency of MT output, where participants had no reference to the source text, while #REF measured the concept of usefulness. #TARGET_REF measure the readability of MT output. While comprehensibility and readability are frequently considered to be components of usability, these studies address only specific aspects of the concept of usability. #REF in which real end users' needs are evaluated in the context of web usability comes closest to the study presented here. However, Gaspari's focus is on the usability of online MT systems, as opposed to the text they generate.",
        "output": "<PERCEPT> There are relatively few studies of the usability of raw machine translated documentation by real end-users. </PERCEPT> For example, Tomita's work #REF focused on the concept of content comprehension. <BACK> #REF evaluated the informativeness, comprehension, and fluency of MT output, where participants had no reference to the source text, while #REF measured the concept of usefulness. </BACK> <INFO> #TARGET_REF measure the readability of MT output. </INFO> <PERCEPT> While comprehensibility and readability are frequently considered to be components of usability, these studies address only specific aspects of the concept of usability. </PERCEPT> #REF in which real end users' needs are evaluated in the context of web usability comes closest to the study presented here. However, Gaspari's focus is on the usability of online MT systems, as opposed to the text they generate."
    },
    {
        "gold": {
            "text": [
                ")where",
                "k",
                "is",
                "the",
                "number",
                "of",
                "language",
                "models",
                "which",
                "are",
                "being",
                "interpolated,",
                "µ",
                "i",
                "the",
                "interpolation",
                "weights",
                "and",
                "V",
                "is",
                "the",
                "vocabulary",
                "of",
                "the",
                "specific",
                "language",
                "model.",
                "The",
                "interpolation",
                "weights",
                "are",
                "estimated",
                "using",
                "Expectation",
                "Maximization",
                "(EM)",
                "#TARGET_REF",
                "over",
                "the",
                "log-likelihood",
                "in",
                "#REF",
                ":N",
                "t=1",
                "log",
                "k",
                "i=1",
                "µ",
                "i",
                "(f",
                "*",
                "i",
                "(w",
                "t",
                "|h",
                "t",
                ")",
                "+",
                "λ",
                "i",
                "(h",
                "t",
                ")P",
                "r",
                "mix",
                "(w",
                "t",
                "|",
                "ht",
                "))",
                "(6)where",
                "the",
                "index",
                "t",
                "scans",
                "over",
                "all",
                "the",
                "n-grams",
                "in",
                "the",
                "training",
                "corpora.",
                "This",
                "mixture",
                "model",
                "was",
                "used",
                "to",
                "combine",
                "the",
                "'in-domain'",
                "language",
                "model",
                "with",
                "an",
                "'out-of-domain'",
                "one,",
                "with",
                "the",
                "mixture",
                "weights",
                "being",
                "estimated",
                "on",
                "the",
                "'in-domain'",
                "training",
                "data",
                "by",
                "applying",
                "a",
                "cross-validation",
                "scheme.",
                "Further",
                "improvements",
                "on",
                "this",
                "mixture",
                "models",
                "were",
                "achieved",
                "using",
                "parameter",
                "tying",
                "to",
                "the",
                "most-recent",
                "context",
                "words",
                "#REF",
                "."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                1,
                1,
                1,
                1,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": ")where k is the number of language models which are being interpolated, µ i the interpolation weights and V is the vocabulary of the specific language model. The interpolation weights are estimated using Expectation Maximization (EM) #TARGET_REF over the log-likelihood in #REF :N t=1 log k i=1 µ i (f * i (w t |h t ) + λ i (h t )P r mix (w t | ht )) (6)where the index t scans over all the n-grams in the training corpora. This mixture model was used to combine the 'in-domain' language model with an 'out-of-domain' one, with the mixture weights being estimated on the 'in-domain' training data by applying a cross-validation scheme. Further improvements on this mixture models were achieved using parameter tying to the most-recent context words #REF .",
        "output": ")where k is the number of language models which are being interpolated, µ i the interpolation weights and V is the vocabulary of the specific language model. <BACK> The interpolation weights are estimated using </BACK> <INFO> Expectation Maximization (EM) #TARGET_REF </INFO> <BACK> over the log-likelihood in #REF :N t=1 log k i=1 µ i (f * i (w t |h t ) + λ i (h t )P r mix (w t | ht )) </BACK> (6)where the index t scans over all the n-grams in the training corpora. This mixture model was used to combine the 'in-domain' language model with an 'out-of-domain' one, with the mixture weights being estimated on the 'in-domain' training data by applying a cross-validation scheme. Further improvements on this mixture models were achieved using parameter tying to the most-recent context words #REF ."
    },
    {
        "gold": {
            "text": [
                "Suffix",
                "arrays",
                "were",
                "introduced",
                "by",
                "Manber",
                "and",
                "Myers",
                "who",
                "gave",
                "a",
                "Θ(N",
                "log",
                "N)",
                "construction",
                "algorithm",
                "#REF",
                ".",
                "While",
                "several",
                "linear",
                "time",
                "suffix",
                "array",
                "construction",
                "algorithms",
                "have",
                "now",
                "been",
                "introduced",
                "#TARGET_REF",
                "it",
                "is",
                "not",
                "clear",
                "that",
                "their",
                "asymptotic",
                "gains",
                "make",
                "them",
                "a",
                "better",
                "choice",
                "than",
                "well-tuned",
                "supralinear",
                "methods",
                "on",
                "corpora",
                "of",
                "interest",
                "#REF",
                "."
            ],
            "context": [
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2
            ]
        },
        "input": "Suffix arrays were introduced by Manber and Myers who gave a Θ(N log N) construction algorithm #REF . While several linear time suffix array construction algorithms have now been introduced #TARGET_REF it is not clear that their asymptotic gains make them a better choice than well-tuned supralinear methods on corpora of interest #REF .",
        "output": "<BACK> Suffix arrays were introduced by Manber and Myers who gave a Θ(N log N) construction algorithm #REF . </BACK> While <INFO> several linear time suffix array construction algorithms have now been introduced #TARGET_REF </INFO> <PERCEPT> it is not clear that their asymptotic gains make them a better choice than well-tuned supralinear methods on corpora of interest #REF . </PERCEPT>"
    },
    {
        "gold": {
            "text": [
                "The",
                "training",
                "paradigm",
                "proposed",
                "in",
                "this",
                "paper",
                "may",
                "be",
                "extended",
                "to",
                "any",
                "Seq2Seq",
                "model.",
                "However,",
                "it",
                "can",
                "be",
                "a",
                "non-trivial",
                "overhead",
                "to",
                "generate",
                "the",
                "candidate",
                "summaries",
                "using",
                "large",
                "neural",
                "models",
                "on",
                "the",
                "entire",
                "training",
                "set.",
                "On",
                "the",
                "other",
                "hand,",
                "recent",
                "work",
                "#TARGET_REF",
                "Reference",
                "chelsea",
                "forward",
                "tammy",
                "abraham",
                "nets",
                "first-half",
                "double",
                "for",
                "chelsea.",
                "dominic",
                "solanke",
                "adds",
                "a",
                "third",
                "late",
                "on",
                "as",
                "chelsea",
                "look",
                "set",
                "to",
                "win",
                "trophy.",
                "manchester",
                "city",
                "struggle",
                "without",
                "injured",
                "star",
                "thierry",
                "ambrose.",
                "read:",
                "mourinho",
                "warns",
                "his",
                "young",
                "chelsea",
                "players",
                "he",
                "can",
                "not",
                "play",
                "them",
                "all.",
                "click",
                "here",
                "to",
                "read",
                "our",
                "match",
                "report",
                "from",
                "man",
                "city",
                "'s",
                "academy",
                "stadium."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The training paradigm proposed in this paper may be extended to any Seq2Seq model. However, it can be a non-trivial overhead to generate the candidate summaries using large neural models on the entire training set. On the other hand, recent work #TARGET_REF Reference chelsea forward tammy abraham nets first-half double for chelsea. dominic solanke adds a third late on as chelsea look set to win trophy. manchester city struggle without injured star thierry ambrose. read: mourinho warns his young chelsea players he can not play them all. click here to read our match report from man city 's academy stadium.",
        "output": "The training paradigm proposed in this paper may be extended to any Seq2Seq model. However, it can be a non-trivial overhead to generate the candidate summaries using large neural models on the entire training set. On the other hand, <INFO> recent work #TARGET_REF </INFO> <PERCEPT> Reference chelsea forward tammy abraham nets first-half double for chelsea. dominic solanke adds a third late on as chelsea look set to win trophy. manchester city struggle without injured star thierry ambrose. </PERCEPT> read: mourinho warns his young chelsea players he can not play them all. click here to read our match report from man city 's academy stadium."
    },
    {
        "gold": {
            "text": [
                "•",
                "Top-k",
                "attention",
                "is",
                "a",
                "highly",
                "accurate",
                "approximation",
                "to",
                "vanilla",
                "attention",
                "and",
                "is",
                "a",
                "plug-andplay",
                "replacement",
                "at",
                "both",
                "multi-head",
                "attention",
                "and",
                "feed-forward",
                "layers",
                "of",
                "a",
                "Transformer.",
                "This",
                "is",
                "unlike",
                "past",
                "attention",
                "variants",
                "#TARGET_REF",
                "that",
                "require",
                "an",
                "expensive",
                "corrective",
                "pretraining",
                "stage",
                "to",
                "adjust",
                "model",
                "weights",
                "to",
                "the",
                "new",
                "variant,",
                "which",
                "can",
                "be",
                "prohibitive",
                "for",
                "large",
                "models.",
                "We",
                "show",
                "top-k",
                "attention",
                "can",
                "replace",
                "vanilla",
                "attention",
                "in",
                "a",
                "zero-shot",
                "inference",
                "setup",
                "and",
                "at",
                "finetuning",
                "time",
                "without",
                "any",
                "corrective",
                "pre-training."
            ],
            "context": [
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "• Top-k attention is a highly accurate approximation to vanilla attention and is a plug-andplay replacement at both multi-head attention and feed-forward layers of a Transformer. This is unlike past attention variants #TARGET_REF that require an expensive corrective pretraining stage to adjust model weights to the new variant, which can be prohibitive for large models. We show top-k attention can replace vanilla attention in a zero-shot inference setup and at finetuning time without any corrective pre-training.",
        "output": "• <BACK> Top-k attention is a highly accurate approximation to vanilla attention and is a plug-andplay replacement at both multi-head attention and feed-forward layers of a Transformer. </BACK> <PERCEPT> This is unlike past </PERCEPT> <INFO> attention variants #TARGET_REF that require an expensive corrective pretraining stage to adjust model weights to the new variant, </INFO> <PERCEPT> which can be prohibitive for large models. </PERCEPT> We show top-k attention can replace vanilla attention in a zero-shot inference setup and at finetuning time without any corrective pre-training."
    },
    {
        "gold": {
            "text": [
                "This",
                "parsing",
                "algorithm",
                "is",
                "similar",
                "to",
                "the",
                "shift-reduce",
                "parser",
                "except",
                "that",
                "our",
                "algorithms",
                "handles",
                "ambiguities,",
                "parallel",
                "processing",
                "of",
                "each",
                "hypothesis,",
                "and",
                "top-down",
                "predictions",
                "of",
                "possible",
                "next",
                "input",
                "symbol.",
                "The",
                "generation",
                "algorithm",
                "implemented",
                "on",
                "SNAP",
                "is",
                "a",
                "version",
                "of",
                "the",
                "lexically",
                "guided",
                "bottom-up",
                "algorithm",
                "which",
                "is",
                "described",
                "in",
                "#TARGET_REF",
                "."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                2,
                2,
                2,
                2,
                2,
                2
            ]
        },
        "input": "This parsing algorithm is similar to the shift-reduce parser except that our algorithms handles ambiguities, parallel processing of each hypothesis, and top-down predictions of possible next input symbol. The generation algorithm implemented on SNAP is a version of the lexically guided bottom-up algorithm which is described in #TARGET_REF .",
        "output": "This parsing algorithm is similar to the shift-reduce parser except that our algorithms handles ambiguities, parallel processing of each hypothesis, and top-down predictions of possible next input symbol. <PERCEPT> The generation algorithm implemented on SNAP is </PERCEPT> <INFO> a version of the lexically guided bottom-up algorithm </INFO> <PERCEPT> which is described in #TARGET_REF . </PERCEPT>"
    },
    {
        "gold": {
            "text": [
                "uses",
                "a",
                "joint",
                "transformer",
                "architecture",
                "that",
                "encodes",
                "sequences",
                "of",
                "phonemes",
                "and",
                "sequences",
                "of",
                "text",
                "simultaneously.",
                "However,",
                "this",
                "joint",
                "model",
                "is",
                "utilized",
                "to",
                "learn",
                "representations",
                "that",
                "are",
                "more",
                "robust",
                "to",
                "transcription",
                "errors.",
                "The",
                "architecture",
                "still",
                "requires",
                "text",
                "inputs",
                "(from",
                "ASR",
                "transcriptions)",
                "and",
                "generates",
                "outputs",
                "in",
                "both",
                "text",
                "and",
                "phoneme",
                "representations.",
                "In",
                "contrast,",
                "our",
                "approach",
                "allows",
                "for",
                "text",
                "input,",
                "audio",
                "input,",
                "or",
                "text",
                "plus",
                "audio",
                "input",
                "to",
                "language",
                "models.",
                "Similarly,",
                "in",
                "#REF",
                "and",
                "#TARGET_REF",
                "investigate",
                "the",
                "potential",
                "of",
                "phoneme-based",
                "or",
                "phoneme",
                "aware",
                "representations",
                "and",
                "models,",
                "showing",
                "gains",
                "in",
                "performance,",
                "language",
                "transfer,",
                "and",
                "flexibility",
                "across",
                "written",
                "scripts.",
                "These",
                "works",
                "conduct",
                "training",
                "on",
                "text-based",
                "data",
                "only,",
                "using",
                "Epitran",
                "to",
                "convert",
                "to",
                "phonemes.",
                "#REF",
                "transforms",
                "unlabeled",
                "text",
                "(i.e.,",
                "not",
                "aligned",
                "with",
                "corresponding",
                "audio",
                "files)",
                "into",
                "phonemes",
                "in",
                "a",
                "scheme",
                "to",
                "train",
                "speech",
                "recognition",
                "models",
                "without",
                "any",
                "labeled",
                "data.",
                "This",
                "scheme",
                "involves",
                "a",
                "generator",
                "model",
                "trained",
                "jointly",
                "with",
                "a",
                "discriminator",
                "model.",
                "The",
                "generator",
                "model",
                "converts",
                "audio,",
                "segmented",
                "into",
                "phonetic",
                "units",
                "into",
                "predicted",
                "phonemes,",
                "and",
                "the",
                "discriminator",
                "model",
                "attempts",
                "to",
                "discriminate",
                "between",
                "these",
                "predicted",
                "phonemes",
                "and",
                "the",
                "phonemes",
                "transliterated",
                "from",
                "unlabeled",
                "text.",
                "Although",
                "both",
                "text",
                "and",
                "audio",
                "are",
                "utilized",
                "in",
                "this",
                "work,",
                "they",
                "are",
                "not",
                "input",
                "to",
                "the",
                "same",
                "model",
                "and",
                "the",
                "primary",
                "output",
                "of",
                "the",
                "training",
                "scheme",
                "is",
                "a",
                "model",
                "that",
                "creates",
                "good",
                "phonetic",
                "speech",
                "representations",
                "from",
                "input",
                "audio."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "uses a joint transformer architecture that encodes sequences of phonemes and sequences of text simultaneously. However, this joint model is utilized to learn representations that are more robust to transcription errors. The architecture still requires text inputs (from ASR transcriptions) and generates outputs in both text and phoneme representations. In contrast, our approach allows for text input, audio input, or text plus audio input to language models. Similarly, in #REF and #TARGET_REF investigate the potential of phoneme-based or phoneme aware representations and models, showing gains in performance, language transfer, and flexibility across written scripts. These works conduct training on text-based data only, using Epitran to convert to phonemes. #REF transforms unlabeled text (i.e., not aligned with corresponding audio files) into phonemes in a scheme to train speech recognition models without any labeled data. This scheme involves a generator model trained jointly with a discriminator model. The generator model converts audio, segmented into phonetic units into predicted phonemes, and the discriminator model attempts to discriminate between these predicted phonemes and the phonemes transliterated from unlabeled text. Although both text and audio are utilized in this work, they are not input to the same model and the primary output of the training scheme is a model that creates good phonetic speech representations from input audio.",
        "output": "uses a joint transformer architecture that encodes sequences of phonemes and sequences of text simultaneously. However, this joint model is utilized to learn representations that are more robust to transcription errors. The architecture still requires text inputs (from ASR transcriptions) and generates outputs in both text and phoneme representations. In contrast, our approach allows for text input, audio input, or text plus audio input to language models. Similarly, in <BACK> #REF and </BACK> <INFO> #TARGET_REF investigate the potential of phoneme-based or phoneme aware representations and models, showing gains in performance, language transfer, and flexibility across written scripts. </INFO> <BACK> These works conduct training on text-based data only, using Epitran to convert to phonemes. </BACK> #REF transforms unlabeled text (i.e., not aligned with corresponding audio files) into phonemes in a scheme to train speech recognition models without any labeled data. This scheme involves a generator model trained jointly with a discriminator model. The generator model converts audio, segmented into phonetic units into predicted phonemes, and the discriminator model attempts to discriminate between these predicted phonemes and the phonemes transliterated from unlabeled text. Although both text and audio are utilized in this work, they are not input to the same model and the primary output of the training scheme is a model that creates good phonetic speech representations from input audio."
    },
    {
        "gold": {
            "text": [
                "In",
                "sequential",
                "decision",
                "making",
                "problems",
                "in",
                "particular,",
                "this",
                "generalization",
                "gap",
                "is",
                "the",
                "result",
                "of",
                "an",
                "agent",
                "simply",
                "memorizing",
                "trajectories,",
                "e.g.",
                "the",
                "sequence",
                "of",
                "actions",
                "and",
                "dialogues",
                "required",
                "to",
                "finish",
                "a",
                "game,",
                "and",
                "thus",
                "being",
                "unable",
                "to",
                "react",
                "in",
                "novel",
                "scenarios-i.e.",
                "the",
                "agent",
                "learns",
                "from",
                "the",
                "head",
                "the",
                "training",
                "data",
                "and",
                "simply",
                "memorizes",
                "the",
                "long",
                "tail.",
                "One",
                "way",
                "of",
                "decreasing",
                "this",
                "generalization",
                "gap",
                "is",
                "by",
                "training",
                "agents",
                "on",
                "procedurally",
                "generated",
                "environments-wherein",
                "the",
                "agent",
                "learns",
                "a",
                "family",
                "of",
                "parametrized",
                "tasks",
                "with",
                "a",
                "significantly",
                "larger",
                "state-action",
                "spaces",
                "than",
                "singular",
                "environments,",
                "thus",
                "effectively",
                "making",
                "the",
                "memorization",
                "of",
                "trajectories",
                "impossible",
                "#TARGET_REF",
                ".",
                "Drawing",
                "inspiration",
                "from",
                "all",
                "of",
                "these",
                "ideas,",
                "we",
                "create",
                "a",
                "method",
                "that",
                "learns",
                "to",
                "create",
                "a",
                "training",
                "curriculum",
                "of",
                "increasingly",
                "more",
                "difficult",
                "novel",
                "procedurally",
                "generated",
                "environments."
            ],
            "context": [
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "In sequential decision making problems in particular, this generalization gap is the result of an agent simply memorizing trajectories, e.g. the sequence of actions and dialogues required to finish a game, and thus being unable to react in novel scenarios-i.e. the agent learns from the head the training data and simply memorizes the long tail. One way of decreasing this generalization gap is by training agents on procedurally generated environments-wherein the agent learns a family of parametrized tasks with a significantly larger state-action spaces than singular environments, thus effectively making the memorization of trajectories impossible #TARGET_REF . Drawing inspiration from all of these ideas, we create a method that learns to create a training curriculum of increasingly more difficult novel procedurally generated environments.",
        "output": "<BACK> In sequential decision making problems in particular, this generalization gap is the result of an agent simply memorizing trajectories, </BACK> e.g. the sequence of actions and dialogues required to finish a game, and thus being unable to react in novel scenarios-i.e. the agent learns from the head the training data and simply memorizes the long tail. <PERCEPT> One way of decreasing this generalization gap is by </PERCEPT> <INFO> training agents on procedurally generated environments-wherein the agent learns a family of parametrized tasks with a significantly larger state-action spaces than singular environments, thus effectively making the memorization of trajectories impossible #TARGET_REF . </INFO> Drawing inspiration from all of these ideas, we create a method that learns to create a training curriculum of increasingly more difficult novel procedurally generated environments."
    },
    {
        "gold": {
            "text": [
                "We",
                "have",
                "just",
                "seen",
                "that",
                "certain",
                "types",
                "of",
                "syntactic",
                "parameterization",
                "may",
                "be",
                "captured",
                "in",
                "the",
                "grammar",
                "network",
                "(e.g.,",
                "Xbar",
                "parameters",
                "such",
                "as",
                "constituent",
                "order).",
                "In",
                "addition",
                "to",
                "these,",
                "there",
                "are",
                "syntactic",
                "parameters",
                "that",
                "must",
                "be",
                "programmed",
                "into",
                "the",
                "message-passing",
                "mechanism",
                "itself,",
                "not",
                "just",
                "into",
                "the",
                "grammar",
                "network.",
                "Figure",
                "2",
                "shows",
                "the",
                "syntactic",
                "parameter",
                "settings",
                "for",
                "English",
                "and",
                "Korean.",
                "The",
                "English",
                "settings",
                "are",
                "drawn",
                "from",
                "#TARGET_REF",
                ".",
                "The",
                "same",
                "paradigm",
                "was",
                "followed",
                "in",
                "our",
                "analysis",
                "of",
                "Korean",
                "parameters.",
                "The",
                "remainder",
                "of",
                "this",
                "paper",
                "will",
                "focus",
                "on",
                "how",
                "we",
                "automatically",
                "precompile",
                "the",
                "English",
                "and",
                "Korean",
                "parameter",
                "settings",
                "concerning",
                "Xbar",
                "theory",
                "into",
                "the",
                "grammar",
                "network",
                "(i.e.,",
                "Basic",
                "Categories,",
                "Pre-tenninals,",
                "Constituent",
                "Order,",
                "Specifiers,",
                "and",
                "Adjunction)."
            ],
            "context": [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                3,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0
            ]
        },
        "input": "We have just seen that certain types of syntactic parameterization may be captured in the grammar network (e.g., Xbar parameters such as constituent order). In addition to these, there are syntactic parameters that must be programmed into the message-passing mechanism itself, not just into the grammar network. Figure 2 shows the syntactic parameter settings for English and Korean. The English settings are drawn from #TARGET_REF . The same paradigm was followed in our analysis of Korean parameters. The remainder of this paper will focus on how we automatically precompile the English and Korean parameter settings concerning Xbar theory into the grammar network (i.e., Basic Categories, Pre-tenninals, Constituent Order, Specifiers, and Adjunction).",
        "output": "We have just seen that certain types of syntactic parameterization may be captured in the grammar network (e.g., Xbar parameters such as constituent order). In addition to these, <BACK> there are syntactic parameters that must be programmed into the message-passing mechanism itself, </BACK> not just into the grammar network. <BACK> Figure 2 shows the syntactic parameter settings for English </BACK> and Korean. <PERCEPT> The English settings are drawn from #TARGET_REF . </PERCEPT> The same paradigm was followed in our analysis of Korean parameters. <BACK> The remainder of this paper will focus on how we automatically precompile the English </BACK> and Korean <BACK> parameter settings concerning Xbar theory into the grammar network </BACK> (i.e., Basic Categories, Pre-tenninals, Constituent Order, Specifiers, and Adjunction)."
    }
]