{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d11cd646",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/deallab/anaconda3/envs/lasse_automatic_cce/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\" #'nvidia-smi' for check gpu cpacity (GPU 4 -> GPU 3 on dgx2); multiple gpus \"0,1,2,3\"\n",
    "import pandas as pd\n",
    "import re\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForTokenClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5bc7dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cce_df = pd.read_csv(\"C:/Users/joelt/Downloads/full_data.csv\")\n",
    "cce_df = pd.read_csv(\"/raid/deallab/CCE_Data/raw_data/finecite/full_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a97964da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                        087922a2-e3ce-415a-8149-d146175ee6de\n",
       "citing_title                               USST's System for AutoSimTrans 2022\n",
       "citing_pub_year                                                           2022\n",
       "citing_authors                                               Jiahui Zhu;Jun Yu\n",
       "cited_title                  ['unknown', 'Learning to translate in real-tim...\n",
       "cited_pub_year                                                ['2018', '2017']\n",
       "cited_authors                ['Mingbo Ma;Liang Huang;Hao Xiong;Renjie Zheng...\n",
       "citation_type                                                            group\n",
       "paragraph                    Simultaneous;translation;<ref type=\"group\">(Gu...\n",
       "target_reference_location                                                    2\n",
       "context_location1            [1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,3...\n",
       "context_location2            [1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,3...\n",
       "iaa_macro                                                                 0.85\n",
       "iaa_total                                                                 0.88\n",
       "iaa_inf                                                                    1.0\n",
       "iaa_perc                                                                   1.0\n",
       "iaa_back                                                                  0.56\n",
       "guideline_version                                                          2.1\n",
       "Name: 21, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cce_df.iloc[21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7dce59d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Simultaneous;translation;<ref type=\"group\">(Gu et al., 2017; Ma et al., 2018)</ref>;consists;in;generating;a;translation;before;the;source;speaker;finishes;speaking.;It;is;widely;used;in;many;real-time;scenarios;such;as;international;conferences,;business;negotiations;and;legal;proceedings.;The;challenge;of;Simultaneous;machine;translation;is;to;find;a;read-write;policy;that;balances;translation;quality;and;latency.;The;translation;quality;will;decline;if;the;machine;translation;system;reads;insufficient;source;information.;When;reading;wider;source;text,;latency;will;increase.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cce_df[\"paragraph\"][21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a971e024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,0,0,0,0,0,0,0,0]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cce_df[\"context_location1\"][21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "868a1ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paragraph</th>\n",
       "      <th>Scope</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Neural Machine Translation (NMT) has opened se...</td>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>As shown in Table 1, the size of the 'in-domai...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Automatic extraction of events has gained siza...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The subject NP 'Bill' is coindexed with the tr...</td>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Self-training [TREF] ) uses a source-to-target...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Paragraph  \\\n",
       "0  Neural Machine Translation (NMT) has opened se...   \n",
       "1  As shown in Table 1, the size of the 'in-domai...   \n",
       "2  Automatic extraction of events has gained siza...   \n",
       "3  The subject NP 'Bill' is coindexed with the tr...   \n",
       "4  Self-training [TREF] ) uses a source-to-target...   \n",
       "\n",
       "                                               Scope  \n",
       "0  [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...  \n",
       "1  [0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...  \n",
       "2  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...  \n",
       "3  [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process the DataFrame\n",
    "results = []\n",
    "\n",
    "for index, row in cce_df.iterrows():\n",
    "    # Clean the paragraph by replacing <ref> tags with '[TREF]'\n",
    "    clean_paragraph = re.sub(r'<ref.*?>.*?</ref>', '[TREF]', row[\"paragraph\"])\n",
    "\n",
    "    # Split the cleaned paragraph into words using ';' as the delimiter\n",
    "    words = clean_paragraph.split(';')\n",
    "\n",
    "    # Process the context_location1 list\n",
    "    context_location1 = eval(row[\"context_location1\"])\n",
    "\n",
    "    # Check if the lengths match, and map the context_location1 to the words\n",
    "    if len(context_location1) == len(words):\n",
    "        # Aggregate the mapped results for the current row\n",
    "        mapped_result = list(zip(context_location1, words))\n",
    "        \n",
    "        # Separate the numbers and words into separate lists\n",
    "        numbers = [str(item[0]) for item in mapped_result]  # Convert numbers to strings\n",
    "        mapped_words = [item[1].strip() for item in mapped_result]  # Strip extra spaces from words\n",
    "        \n",
    "        results.append({\n",
    "            \"Paragraph\": (' ').join(mapped_words),\n",
    "            \"Scope\": numbers\n",
    "        })\n",
    "    else:\n",
    "        results.append({\n",
    "            \"Paragraph\": \"Length of context_location1 and words don't match\",\n",
    "            \"Scope\": \"Mismatch\"\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81780de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert the Scope column elements to lists of integers\n",
    "def convert_scope_to_int(scope):\n",
    "    if isinstance(scope, str):\n",
    "        # Convert string representation of list to an actual list of integers\n",
    "        scope = eval(scope)\n",
    "    # Ensure all elements in the list are integers\n",
    "    return [int(i) for i in scope]\n",
    "\n",
    "# Apply the conversion to the Scope column\n",
    "df[\"Scope\"] = df[\"Scope\"].apply(convert_scope_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92a92f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89, 89)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[\"Paragraph\"][0]), len(df[\"Scope\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "788c63e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_df = pd.DataFrame(columns=['par', 'label'])\n",
    "# for idx, row in df.iterrows():\n",
    "#     par = []\n",
    "#     par_label = []\n",
    "#     for word, label in zip(row['Paragraph'], row['Scope']):\n",
    "#        token = tokenizer.encode(word, add_special_tokens=False)\n",
    "#        par.extend(token)\n",
    "#        par_label.extend([label]*len(token))\n",
    "#     assert len(par) == len(par_label), f'{len(par)}, {len(par_label)}'\n",
    "#     tokenized_df.loc[len(tokenized_df)] = [par, par_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d4adda9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>par</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[8989, 4269, 22333, 25416, 8368, 8673, 8, 4752...</td>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[2170, 70463, 258, 2620, 16, 11, 1820, 2190, 1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[63890, 327, 27523, 1073, 12670, 4752, 70, 269...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[791, 11760, 27321, 6, 28576, 6, 285, 1030, 98...</td>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[12363, 86470, 20961, 6124, 60, 8, 4881, 64, 2...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 par  \\\n",
       "0  [8989, 4269, 22333, 25416, 8368, 8673, 8, 4752...   \n",
       "1  [2170, 70463, 258, 2620, 16, 11, 1820, 2190, 1...   \n",
       "2  [63890, 327, 27523, 1073, 12670, 4752, 70, 269...   \n",
       "3  [791, 11760, 27321, 6, 28576, 6, 285, 1030, 98...   \n",
       "4  [12363, 86470, 20961, 6124, 60, 8, 4881, 64, 2...   \n",
       "\n",
       "                                               label  \n",
       "0  [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...  \n",
       "2  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...  \n",
       "3  [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenized_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b78f65e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89 89 299\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "# par = str(df.loc[0, 'Paragraph'])\n",
    "# labels = df.loc[0, 'Scope']\n",
    "# par_tokenized = tokenizer.tokenize(par, add_special_tokens=False)\n",
    "# print(len(par.split()), len(labels), len(par_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c4342c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1055/1055 [00:00<00:00, 2548.10 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1055\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer for LLaMA 3\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "\n",
    "# Check if the tokenizer has an eos_token and set it as the padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize and prepare dataset\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the paragraphs\n",
    "    tokens = tokenizer(examples[\"Paragraph\"], padding=\"max_length\", truncation=True, max_length=786)\n",
    "    \n",
    "    # Process labels (Scope)\n",
    "    max_label_length = 786  # Same as max_length for consistency\n",
    "    padded_labels = []\n",
    "    \n",
    "    for label_list in examples[\"Scope\"]:\n",
    "        # Truncate if necessary\n",
    "        if len(label_list) > max_label_length:\n",
    "            label_list = label_list[:max_label_length]\n",
    "        \n",
    "        # Pad with -100\n",
    "        padded_label = label_list + [-100] * (max_label_length - len(label_list))\n",
    "        padded_labels.append(padded_label)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    tokens[\"labels\"] = torch.tensor(padded_labels, dtype=torch.long)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Convert the DataFrame to a Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Apply the tokenization function to the dataset\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True, \n",
    "    remove_columns=dataset.column_names  # Remove all original columns\n",
    ")\n",
    "\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2861eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a function to process the labels\n",
    "# def process_labels(examples):\n",
    "#     return {\"labels\": examples['Scope']}\n",
    "\n",
    "# # Apply the label processing to the dataset\n",
    "# labeled_dataset = tokenized_dataset.map(process_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d9414a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.04it/s]\n",
      "Some weights of LlamaForTokenClassification were not initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B and are newly initialized: ['score.bias', 'score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the model for token classification\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3-8B\",\n",
    "    num_labels=4  # Adjust this number based on your specific task\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7856058f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,  # Limit the number of saved checkpoints\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6c7587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(f\"Evaluation Results: {eval_results}\")\n",
    "\n",
    "trainer.save_model(\"./trained_llama_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4541ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline for text classification\n",
    "classification_pipeline = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, return_all_scores=True)\n",
    "\n",
    "# Define your new input paragraph\n",
    "paragraph = \"Neural Machine Translation (NMT) has opened several research directions to exploit as many and diverse data as possible. Massive multilingual NMT models, for instance, take advantage of several language-pair datasets in a single system [TREF] . This offers several advantages, such as a simple training process and enhanced performance of the language-pairs with little data (although sometimes detrimental to the high-resource language-pairs). However, massive models of dozens of languages are not necessarily the best outcome, as it is demonstrated that smaller clusters still offer the same benefits [TREF] .\"\n",
    "\n",
    "# Split the paragraph into words\n",
    "words = paragraph.split()\n",
    "\n",
    "word_predictions = []\n",
    "\n",
    "# Iterate over each word in the paragraph\n",
    "for word in words:\n",
    "    # Tokenize the word and get the input tensors\n",
    "    tokenized_word = tokenizer(word, return_tensors='pt', truncation=True, max_length=512)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokenized_word)\n",
    "\n",
    "    # Get the logits and apply softmax to get probabilities\n",
    "    scores = torch.softmax(outputs.logits, dim=1).detach().numpy()\n",
    "\n",
    "    # Get the label with the highest score\n",
    "    max_score_idx = scores.argmax(axis=1)[0]\n",
    "    max_score = scores[0, max_score_idx]\n",
    "    label = f\"{max_score_idx}\"\n",
    "\n",
    "    word_predictions.append({'word': word, 'scope': label, 'score': max_score})\n",
    "\n",
    "for prediction in word_predictions:\n",
    "    print(f\"Word: {prediction['word']}, Predicted Scope: {prediction['scope']}, Score: {prediction['score']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lasse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
