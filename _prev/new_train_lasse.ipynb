{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/explorer/anaconda3/envs/lasse/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import re\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, Trainer, TrainingArguments\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_id = 'mistralai/Mistral-7B-Instruct-v0.3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "cce_df = pd.read_csv(\"./data/finecite/full_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paragraph</th>\n",
       "      <th>Scope</th>\n",
       "      <th>Sem_struc_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Neural Machine Translation (NMT) has opened se...</td>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...</td>\n",
       "      <td>&lt;BACKBROUND&gt; Neural Machine Translation (NMT) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>As shown in Table 1, the size of the 'in-domai...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...</td>\n",
       "      <td>&lt;BACKBROUND&gt; the size of the 'in-domain' TED t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Automatic extraction of events has gained siza...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>&lt;PERCEPTION&gt; Automatic extraction of events ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The subject NP 'Bill' is coindexed with the tr...</td>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...</td>\n",
       "      <td>&lt;BACKBROUND&gt; The subject NP 'Bill' is coindexe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Self-training [GTREF] ) uses a source-to-targe...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>&lt;INFORMATION&gt; Self-training [GTREF] ) uses a s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050</th>\n",
       "      <td>For the final-stage neural reranker, we experi...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 0, 0, 0, 0, ...</td>\n",
       "      <td>&lt;BACKBROUND&gt; BERT-large and T5-base &lt;/BACKBROU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>Trained on 20GB texts of both Vietnamese news ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>&lt;BACKBROUND&gt; ViBERT was trained on 60GB texts ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>Pretraining Corpus: Following the E2E pretrain...</td>\n",
       "      <td>[0, 0, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>&lt;PERCEPTION&gt; Following &lt;/PERCEPTION&gt;&lt;INFORMATI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>The nouns are organized as an inheritance syst...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 3, 3, 3, ...</td>\n",
       "      <td>&lt;INFORMATION&gt; The nouns are organized as an in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>Arabic being a morphologically rich language, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>&lt;PERCEPTION&gt; For all the available Chinese dat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1055 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Paragraph  \\\n",
       "0     Neural Machine Translation (NMT) has opened se...   \n",
       "1     As shown in Table 1, the size of the 'in-domai...   \n",
       "2     Automatic extraction of events has gained siza...   \n",
       "3     The subject NP 'Bill' is coindexed with the tr...   \n",
       "4     Self-training [GTREF] ) uses a source-to-targe...   \n",
       "...                                                 ...   \n",
       "1050  For the final-stage neural reranker, we experi...   \n",
       "1051  Trained on 20GB texts of both Vietnamese news ...   \n",
       "1052  Pretraining Corpus: Following the E2E pretrain...   \n",
       "1053  The nouns are organized as an inheritance syst...   \n",
       "1054  Arabic being a morphologically rich language, ...   \n",
       "\n",
       "                                                  Scope  \\\n",
       "0     [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...   \n",
       "1     [0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...   \n",
       "2     [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "3     [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...   \n",
       "4     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "...                                                 ...   \n",
       "1050  [0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 0, 0, 0, 0, ...   \n",
       "1051  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1052  [0, 0, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, ...   \n",
       "1053  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 3, 3, 3, ...   \n",
       "1054  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                      Sem_struc_context  \n",
       "0     <BACKBROUND> Neural Machine Translation (NMT) ...  \n",
       "1     <BACKBROUND> the size of the 'in-domain' TED t...  \n",
       "2     <PERCEPTION> Automatic extraction of events ha...  \n",
       "3     <BACKBROUND> The subject NP 'Bill' is coindexe...  \n",
       "4     <INFORMATION> Self-training [GTREF] ) uses a s...  \n",
       "...                                                 ...  \n",
       "1050  <BACKBROUND> BERT-large and T5-base </BACKBROU...  \n",
       "1051  <BACKBROUND> ViBERT was trained on 60GB texts ...  \n",
       "1052  <PERCEPTION> Following </PERCEPTION><INFORMATI...  \n",
       "1053  <INFORMATION> The nouns are organized as an in...  \n",
       "1054  <PERCEPTION> For all the available Chinese dat...  \n",
       "\n",
       "[1055 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def label_mapping(label):\n",
    "    if label == 1: return 'INFORMATION'\n",
    "    if label == 2: return 'PERCEPTION'\n",
    "    if label == 3: return 'BACKBROUND'\n",
    "\n",
    "def replace_ref(word):\n",
    "    if re.match(r'single', word):\n",
    "        return '[REF]'\n",
    "    else:\n",
    "        return '[GREF]'\n",
    "\n",
    "\n",
    "# Process the DataFrame\n",
    "results = []\n",
    "for index, row in cce_df.iterrows():\n",
    "    \n",
    "    #clean all ';' from references\n",
    "    clean_paragraph = re.sub(r'<ref.*?>.*?</ref>',lambda x: x.group().replace(';',','), row['paragraph'])\n",
    "    \n",
    "    #create word list of par\n",
    "    words_par = clean_paragraph.split(';')\n",
    "    \n",
    "    #replace (G)TREF\n",
    "    words_par[row['target_reference_location']] = '[TREF]' if re.search(r'single', words_par[row['target_reference_location']]) else '[GTREF]'\n",
    "    \n",
    "    # Clean the paragraph by replacing <ref> tags with '[TREF]'\n",
    "    words_par = [replace_ref(word) if re.search(r'<ref.*?>.*?</ref>', word) else word for word in words_par]\n",
    "    \n",
    "\n",
    "    # Process the context_location1 list\n",
    "    context_location1 = eval(row[\"context_location1\"])\n",
    "\n",
    "    # check whether context label and word list are same length\n",
    "    assert len(context_location1) == len(words_par), f'The labels are of length {len(context_location1)}, while the word list is of length {len(words_par)}'\n",
    "    \n",
    "    # Check if the lengths match, and map the context_location1 to the words\n",
    "    if len(context_location1) == len(words_par):\n",
    "        # Aggregate the mapped results for the current row\n",
    "        mapped_result = list(zip(context_location1, words_par))\n",
    "        \n",
    "        # Separate the numbers and words into separate lists\n",
    "        numbers = [item[0] for item in mapped_result]  # Convert numbers to strings\n",
    "        mapped_words = [item[1].strip() for item in mapped_result]  # Strip extra spaces from words\n",
    "\n",
    "        sem_structured_context = ''\n",
    "        staged_context = []\n",
    "        prev_label = 0\n",
    "        for label, context in mapped_result:\n",
    "            if label != prev_label and prev_label != 0:\n",
    "                sem_structured_context += f\"<{label_mapping(prev_label)}> {' '.join(staged_context)} </{label_mapping(prev_label)}>\"\n",
    "                staged_context = []\n",
    "            if label != 0: staged_context.append(context)\n",
    "            prev_label = label\n",
    "                \n",
    "            \n",
    "        \n",
    "        \n",
    "        results.append({\n",
    "            \"Paragraph\": ' '.join(mapped_words),\n",
    "            \"Scope\": numbers,\n",
    "            \"Sem_struc_context\": str(sem_structured_context)\n",
    "        })\n",
    "    else:\n",
    "        results.append({\n",
    "            \"Paragraph\": \"Length of context_location1 and words don't match\",\n",
    "            \"Scope\": \"Mismatch\",\n",
    "            \"Sem_struc_context\": 'Mismatch'\n",
    "        })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fine_tune_prompt_xml( \n",
    "    input_str: str,\n",
    "    label_str: str,\n",
    "    tokenizer,\n",
    ") -> torch.Tensor:\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    rule_set (List[str]): List of strings representing entity labels and its\n",
    "                          corresponding description\n",
    "    input_str (str): Actual input string on which detections need to be\n",
    "                     performed\n",
    "    label_str (str): Expected output string corresponding to input_str\n",
    "    tokenizer (PreTrainedTokenizerBase): A tokenizer corresponding to the model\n",
    "                                         being fine-tuned\n",
    "\n",
    "    Returns: \n",
    "    torch.Tensor: Tensor of tokenized input ids\n",
    "    \"\"\"\n",
    "\n",
    "    usr_msg1 = \"You are given a excerpt from a scientific text including citation marker, marked as [REF], [GREF], [TREF], and [GTREF].\" \\\n",
    "        \"[REF] is a placehoder for a single reference, [GREF] for multiple references, and [TREF] or [GTREF] are the references targeted for annotation\" \\\n",
    "        \"You are further given a list of context types, which discribe different information entities in relation to the reference marker. \" \\\n",
    "        \"Your task is to detect and identify all instances of the supplied context types of the as target marked reference in the provided text. \" \\\n",
    "        \"The output should only contain the the citation context, marked with the three context types\" \\\n",
    "        \"Each span of text that belongs to a certain context type should be enclosed with the XML-tag\" \\\n",
    "        \"For example, a span of thext belonging to INFORMATION should be enclosed within <INFORMATION></INFORMATION> tags.\" \\\n",
    "        \"Ensure that all context spans are identified. Do not perform false identifications.\" \\\n",
    "        f\"\"\"\\n\\nList Of Entities\\n\n",
    "        INFORMATION: content from the cited paper (what is cited?)\n",
    "        PERCEPTION: perception or use of the cited content (how is cited/used?)\n",
    "        BACKGROUND: backgound of the citation (why is it cited?)\n",
    "        \"\"\"\\\n",
    "        \"\\n\\n\" \\\n",
    "        \"Are the instructions clear to you?\"\n",
    "    \n",
    "    asst_msg1 = \"Yes, the instructions are clear to me.\"\\\n",
    "                # \"To clarify, as you have provide me with an excerpt from a scientific text, my task is as follows:\"\\\n",
    "                # \"Identify all instances of the provided reference markers [REF], [GREF], [TREF], and [GTREF] in the text.\"\\\n",
    "                # \"Detect and identify all instances of the specified context types INFORMATION, PERCEPTION, and BACKGROUND related to the targeted reference mark\"\\\n",
    "    \n",
    "    usr_msg2 = \"Neural Machine Translation (NMT) has opened several research directions to exploit as many and diverse data as possible. Massive multilingual NMT models, for instance, take advantage of several language-pair datasets in a single system [TREF] . This offers several advantages, such as a ...\"\n",
    "\n",
    "    asst_msg2 = \"<BACKBROUND> Neural Machine Translation (NMT) has opened several research directions to exploit as many and diverse data as possible. </BACKBROUND><INFORMATION> Massive multilingual NMT models, for instance, take advantage of several language-pair datasets in a single system [TREF] . </INFORMATION>\"\n",
    "\n",
    "    usr_msg3 = \"Give a brief explanation of why your answer is correct.\"\n",
    "\n",
    "    asst_msg3 = \"The provided text discusses the advantages and limitations of massive multilingual NMT models.\" \\\n",
    "                \"The context type 'BACKGROUND' is used to explain the purpose of these models, which is to exploit as many and diverse data as possible.\"\\\n",
    "                \"The context type 'INFORMATION' is used to provide specific details about the massive multilingual NMT models, such as the fact that they take advantage of several language-pair datasets in a single system.\"\\\n",
    "                \"The reference marker [TREF] is used to cite the source of this information. The context type 'PERCEPTION' is not used in this text, as it refers to the perception or use of the cited content, and in this case, there is no explicit mention of how the information is being used or perceived\"\n",
    "    \n",
    "    usr_msg4 = \"Great! I am now going to give you another user utterance. Please detect the context types in it \" \\\n",
    "                \"according to the previous instructions. Do not include an explanation in your answer.\"\n",
    "    \n",
    "    asst_msg4 = \"Sure! Please give me the user utterance.\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": usr_msg1},\n",
    "        {\"role\": \"assistant\", \"content\": asst_msg1},\n",
    "        # {\"role\": \"user\", \"content\": usr_msg2},\n",
    "        # {\"role\": \"assistant\", \"content\": asst_msg2},\n",
    "        # {\"role\": \"user\", \"content\": usr_msg3},\n",
    "        # {\"role\": \"assistant\", \"content\": asst_msg3},\n",
    "        # {\"role\": \"user\", \"content\": usr_msg4},\n",
    "        # {\"role\": \"assistant\", \"content\": asst_msg4},\n",
    "        {\"role\": \"user\", \"content\": input_str},\n",
    "        #{\"role\": \"assistant\", \"content\": label_str},\n",
    "    ]\n",
    "    encoded_input_ids = tokenizer.apply_chat_template(messages)\n",
    "\n",
    "    return {'input_ids': encoded_input_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 10,485,760 || all params: 7,258,509,312 || trainable%: 0.1445\n"
     ]
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_storage=torch.bfloat16,\n",
    ")\n",
    "\n",
    "LMmodel = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config = bnb_config,\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    device_map = 'auto'\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(target_modules=[ \"v_proj\", \"q_proj\", \"up_proj\", \"o_proj\", \"k_proj\", \"down_proj\", \"gate_proj\" ], inference_mode=False, r=4, lora_alpha=32, lora_dropout=0.1)\n",
    "\n",
    "LMmodel = get_peft_model(LMmodel, peft_config)\n",
    "\n",
    "LMmodel.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
    "from transformers.utils import PaddingStrategy\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
    "\n",
    "@dataclass\n",
    "class CustomDataCollatorWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "\n",
    "    Args:\n",
    "        tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):\n",
    "            The tokenizer used for encoding the data.\n",
    "        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "\n",
    "            - `True` or `'longest'` (default): Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence is provided).\n",
    "            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
    "              acceptable input length for the model if that argument is not provided.\n",
    "            - `False` or `'do_not_pad'`: No padding (i.e., can output a batch with sequences of different lengths).\n",
    "        max_length (`int`, *optional*):\n",
    "            Maximum length of the returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (`int`, *optional*):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "        return_tensors (`str`, *optional*, defaults to `\"pt\"`):\n",
    "            The type of Tensor to return. Allowable values are \"np\", \"pt\" and \"tf\".\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    return_tensors: str = \"pt\"\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        batch = self.tokenizer.pad(\n",
    "            features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=self.return_tensors,\n",
    "        )\n",
    "        labels = batch[\"input_ids\"].clone()\n",
    "        \n",
    "        # Set loss mask for all pad tokens\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        # Compute loss mask for appropriate tokens only\n",
    "        for i in range(batch['input_ids'].shape[0]):\n",
    "            \n",
    "            # Decode the training input\n",
    "            text_content = self.tokenizer.decode(batch['input_ids'][i][1:])  # slicing from [1:] is important because tokenizer adds bos token\n",
    "            \n",
    "            # Extract substrings for prompt text in the training input\n",
    "            # The training input ends at the last user msg ending in [/INST]\n",
    "            prompt_gen_boundary = text_content.rfind(\"[/INST]\") + len(\"[/INST]\")\n",
    "            prompt_text = text_content[:prompt_gen_boundary]\n",
    "            \n",
    "            # print(f\"\"\"PROMPT TEXT:\\n{prompt_text}\"\"\")\n",
    "            \n",
    "            # retokenize the prompt text only\n",
    "            prompt_text_tokenized = self.tokenizer(\n",
    "                prompt_text,\n",
    "                return_overflowing_tokens=False,\n",
    "                return_length=False,\n",
    "            )\n",
    "            # compute index where prompt text ends in the training input\n",
    "            prompt_tok_idx = len(prompt_text_tokenized['input_ids'])\n",
    "            \n",
    "            # Set loss mask for all tokens in prompt text\n",
    "            labels[i][range(prompt_tok_idx)] = -100\n",
    "            \n",
    "            # print(\"================DEBUGGING INFORMATION===============\")\n",
    "            # for idx, tok in enumerate(labels[i]):\n",
    "            #     token_id = batch['input_ids'][i][idx]\n",
    "            #     decoded_token_id = self.tokenizer.decode(batch['input_ids'][i][idx])\n",
    "            #     print(f\"\"\"TOKID: {token_id} | LABEL: {tok} || DECODED: {decoded_token_id}\"\"\")\n",
    "                    \n",
    "        batch[\"labels\"] = labels\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/211 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 211/211 [00:00<00:00, 1961.75 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# split data in train / test\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=96, shuffle=True)\n",
    "\n",
    "# Convert the DataFrame to a Dataset\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "\n",
    "#Apply the tokenization function to the dataset\n",
    "# train_ds = train_ds.map(\n",
    "#     lambda row: get_fine_tune_prompt_xml(row['Paragraph'], row['Sem_struc_context'], tokenizer), \n",
    "#     batched=False, \n",
    "#     remove_columns=train_ds.column_names  # Remove all original columns\n",
    "# )\n",
    "\n",
    "test_ds = test_ds.map(\n",
    "    lambda row: get_fine_tune_prompt_xml(row['Paragraph'], row['Sem_struc_context'], tokenizer), \n",
    "    batched=False, \n",
    "    remove_columns=test_ds.column_names  # Remove all original columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/explorer/anaconda3/envs/lasse/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/explorer/anaconda3/envs/lasse/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/explorer/anaconda3/envs/lasse/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/explorer/anaconda3/envs/lasse/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:407: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 800\n",
    "training_arguments = SFTConfig(\n",
    "    output_dir=\"./tmp\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "trainer = SFTTrainer(\n",
    "    model=LMmodel,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    dataset_text_field=\"input_ids\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    # Using custom data collator inside SFTTrainer\n",
    "    data_collator=CustomDataCollatorWithPadding(\n",
    "        tokenizer=tokenizer, \n",
    "        padding=\"longest\", \n",
    "        max_length=max_seq_length, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "/home/explorer/anaconda3/envs/lasse/lib/python3.11/site-packages/transformers/generation/utils.py:1885: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' <BACKBROUND> FastText embeddings are also used for Hindi, French, Spanish </BACKBROUND><INFORMATION> Italian word representations </INFORMATION></s>\\n\\nThe bi-LSTM model is trained on a fixed 300 hidden dimensions for all the bi-LSTMs in the architecture [GREF]. The model is trained on a dataset of 100,000 sentences [TREF]. The dataset is a combination of scientific papers and news articles [GREF]. The dataset is split into training, validation, and test sets with a ratio of 80:10:10 respectively [TREF].\\n\\nThe model is evaluated on a test set of 10,000 sentences [TREF]. The evaluation metric used is F1-score [TREF]. The F1-score is calculated for each label in the multi-label classification problem [TREF]. The average F1-score across all labels is reported as the final performance of the model [TREF].\\n\\nThe model is also evaluated on a separate validation set of 1,000 sentences [TREF]. The validation set is used to tune the hyperparameters of the model [TREF]. The hyperparameters that are tuned include the learning rate, batch size, number of epochs, and dropout rate [TREF].\\n\\nThe model is implemented using TensorFlow [TREF]. The code is available on GitHub [GTREF].\\n\\nThe model is also compared with other state-of-the-art models [GREF]. The comparison is done on the same test set of 10,000 sentences [TREF]. The comparison is based on the F1-score [TREF]. The results show that the proposed model outperforms the other models [GREF].\\n\\nThe proposed model is also evaluated on a different dataset of 50,000 sentences [TREF]. The dataset is a combination of scientific papers and news articles [TREF]. The dataset is split into training, validation, and test sets with a ratio of 80:10:10 respectively [TREF].\\n\\nThe model is also evaluated on a different evaluation metric, accuracy [TREF]. The accuracy is calculated for each label in the multi-label classification problem [TREF]. The average accuracy across all labels is reported as the final performance of the model [TREF].\\n\\nThe model is also evaluated on a different dataset of 100,000 sentences [TREF]. The dataset is a combination of scientific papers and news articles [TREF]. The'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator=CustomDataCollatorWithPadding(\n",
    "    tokenizer=tokenizer, \n",
    "    padding=\"longest\", \n",
    "    max_length=max_seq_length, \n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "res = trainer.model.generate(data_collator([test_ds[0]])['input_ids'], max_new_tokens=512)\n",
    "tokenizer.decode(res[0]).split('[/INST]')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' <BACKBROUND> Our system outperforms the R-Net baseline (Rouge-L: 40.22) used by [GREF] . </BACKBROUND><PERCEPTION> If there is a trade-off sought between computing time and accuracy, our system performs similar to or better than the baseline used by </PERCEPTION><INFORMATION> [TREF] ROUGE score is not the best metric for tasks such as opinion question answering. </INFORMATION> We believe the cosine similarity is a better metric to measure how close the retrieved answer is to the gold standard. Overall the sim method is able to provide an answer more than 70% similar to the gold standard answer 91.5% of the time. From the sentences returned by our system as candidate answers, 72% of the time at least half the candidate sentences are good answers. </PERCEPTION> This shows that our system is consistent and accurate at providing good answers.</s>'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = trainer.model.generate(data_collator([test_ds[3]])['input_ids'], max_new_tokens=512)\n",
    "tokenizer.decode(res[0]).split('[/INST]')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([no for no in data_collator([test_ds[4]])['labels'][0] if no != -100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2532' max='2532' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2532/2532 24:37, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.247800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.182700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.135200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.115100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.087900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2532, training_loss=0.15242057120630526, metrics={'train_runtime': 1477.8347, 'train_samples_per_second': 1.713, 'train_steps_per_second': 1.713, 'total_flos': 5.957338321974067e+16, 'train_loss': 0.15242057120630526, 'epoch': 3.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='422' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 04:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'math' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#evaluate the fine tuned model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n\u001b[0;32m----> 3\u001b[0m perplexity \u001b[38;5;241m=\u001b[39m \u001b[43mmath\u001b[49m\u001b[38;5;241m.\u001b[39mexp(eval_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m eval_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperplexity\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m perplexity\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFine-tuned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Results: Perplexity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mperplexity\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'math' is not defined"
     ]
    }
   ],
   "source": [
    "#evaluate the fine tuned model\n",
    "eval_results = trainer.evaluate()\n",
    "perplexity = math.exp(eval_results['eval_loss'])\n",
    "eval_results['perplexity'] = perplexity\n",
    "print(f\"Fine-tuned {model_id} Results: Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_runtime': 48.6469,\n",
       " 'eval_samples_per_second': 4.337,\n",
       " 'eval_steps_per_second': 4.337,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lasse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
