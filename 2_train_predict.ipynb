{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import traceback\n",
    "import pprint\n",
    "import json\n",
    "\n",
    "import argparse\n",
    "import openai\n",
    "# from openai.error import RateLimitError\n",
    "import tqdm\n",
    "from monty.serialization import loadfn, dumpfn\n",
    "import warnings\n",
    "import datetime\n",
    "\n",
    "from constants import DATADIR\n",
    "from util import dump_jsonl\n",
    "import nbimporter\n",
    "from annotate import preprocess_text, sentence_is_paradigm\n",
    "\n",
    "\n",
    "START_TOKEN = \"\\n###\\n\"\n",
    "UNKNOWN_STR = \"unknown\"\n",
    "STOP_TOKEN = \"\\nEND\"\n",
    "WHITESPACE = \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_completion_from_sentence_json(\n",
    "        sentence_json,\n",
    "        write_links=True,\n",
    "        write_nonlinked_basemats=True,\n",
    "        write_nonlinked_dopants=True,\n",
    "        write_results=True,\n",
    "        write_modifiers=True,\n",
    "        stop_token=STOP_TOKEN,\n",
    "        whitespace=WHITESPACE,\n",
    "        fmt=\"eng\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Create an LLM completion (target) from a sentence json according to different schemas.\n",
    "    Used for training.\n",
    "\n",
    "    Args:\n",
    "        sentence_json (dict): A dictionary for a sentence with \"sentence_text\" field and other keys\n",
    "            relevant for doping (basemats, dopants, doping_modifierts, dopants2basemats, results.\n",
    "        write_links (bool): Whether to write the links between dopants and basemats.\n",
    "        write_nonlinked_basemats (bool): Whether to write \"isolated\" basemats.\n",
    "        write_nonlinked_dopants (bool): whether to write \"isolated\" dopants\n",
    "        write_results (bool): Whether to write the results.\n",
    "        write_modifiers (bool): Whether to write the doping modifiers.\n",
    "        stop_token (str): The stop token to use.\n",
    "        whitespace (str): The whitespace to use.\n",
    "        fmt (str): The format to create a completion in. Note this does not mean the schema, but only\n",
    "            whether the completion should be written as english sentences or as stringified JSON. Should\n",
    "            be either \"json\" or \"eng\"; to use EngExtra as in the publication, use write_results=True and\n",
    "            write_modifiers=True.\n",
    "\n",
    "    Returns:\n",
    "        str: The GPT-3 completion to be used for training.\n",
    "\n",
    "    \"\"\"\n",
    "    if fmt not in (\"eng\", \"json\"):\n",
    "        raise ValueError(f\"Value of fmt='{fmt}' not valid!\")\n",
    "\n",
    "    if fmt == \"json\":\n",
    "        keys = [\"basemats\", \"dopants\", \"dopants2basemats\"]\n",
    "        if write_results:\n",
    "            keys.append(\"results\")\n",
    "        if write_modifiers:\n",
    "            keys.append(\"doping_modifiers\")\n",
    "        subjson = {k: v for k, v in sentence_json.items() if k in keys}\n",
    "        output = json.dumps(subjson, indent=1)\n",
    "    else:\n",
    "        output = \"\"\n",
    "        basemats = sentence_json[\"basemats\"]\n",
    "        dopants = sentence_json[\"dopants\"]\n",
    "        modifiers = sentence_json[\"doping_modifiers\"]\n",
    "        links = sentence_json[\"dopants2basemats\"]\n",
    "        results = sentence_json[\"results\"]\n",
    "\n",
    "        basemats_left = copy.deepcopy(basemats)\n",
    "        dopants_left = copy.deepcopy(dopants)\n",
    "\n",
    "        if links and write_links:\n",
    "            for dopant_id, d2b_links in links.items():\n",
    "                dopant = dopants[dopant_id]\n",
    "                for basemat_id in d2b_links:\n",
    "                    basemat = basemats[basemat_id]\n",
    "                    output += f\"The host '{basemat}' was doped with '{dopant}'.\\n\"\n",
    "\n",
    "                    if basemat_id in basemats_left:\n",
    "                        basemats_left.pop(basemat_id)\n",
    "                dopants_left.pop(dopant_id)\n",
    "\n",
    "        if basemats_left and write_nonlinked_basemats:\n",
    "            for basemat in basemats_left.values():\n",
    "                output += f\"The host '{basemat}' was doped.\\n\"\n",
    "\n",
    "        if dopants_left and write_nonlinked_dopants:\n",
    "            for dopant in dopants_left.values():\n",
    "                output += f\"'{dopant}' is a dopant.\\n\"\n",
    "\n",
    "        if write_results:\n",
    "            for result in results.values():\n",
    "                output += f\"'{result}' is a likely solid solution.\\n\"\n",
    "\n",
    "        if modifiers and write_modifiers:\n",
    "            modifier_str = \", \".join([f\"'{m}'\" for m in modifiers])\n",
    "            output += f\"Modifiers of the doping are: {modifier_str}.\\n\"\n",
    "\n",
    "        if not output:\n",
    "            output = \"There is no doping information.\\n\"\n",
    "\n",
    "    output = whitespace + output + stop_token\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_entities_from_llm_completion(text, fmt=\"eng\"):\n",
    "    \"\"\"\n",
    "    Obtain entities as a dictionary (to be converted to json) from a GPT-3 completion\n",
    "    string. Used for decoding LLM string replies to structured doping data.\n",
    "\n",
    "    Args:\n",
    "        text (str): The LLM completion string.\n",
    "        fmt (str): The format to decode from, either \"eng\" or \"json\". Extra entities\n",
    "            are automatically decoded if present.\n",
    "\n",
    "    Returns:\n",
    "        (dict): The structured doping entities representing a graph (JSON document).\n",
    "    \"\"\"\n",
    "    if fmt not in (\"eng\", \"json\"):\n",
    "        raise ValueError(f\"Value of fmt='{fmt}' not valid!\")\n",
    "\n",
    "    ents = {\n",
    "        \"basemats\": {},\n",
    "        \"dopants\": {},\n",
    "        \"results\": {},\n",
    "        \"doping_modifiers\": {},\n",
    "        \"dopants2basemats\": {},\n",
    "    }\n",
    "\n",
    "    if not text:\n",
    "        return ents\n",
    "\n",
    "    if fmt == \"json\":\n",
    "        try:\n",
    "            ents = json.loads(text)\n",
    "        except json.decoder.JSONDecodeError:\n",
    "            warnings.warn(f\"Could not json decode entry '{text}'\")\n",
    "        return ents\n",
    "\n",
    "    # todo: implement doping modifiers and results\n",
    "    text = text.strip()\n",
    "\n",
    "    if \"There is no doping information\" in text:\n",
    "        return ents\n",
    "\n",
    "    lines = [l for l in text.split(\"\\n\") if l]\n",
    "\n",
    "    dopant_counter = 0\n",
    "    basemat_counter = 0\n",
    "\n",
    "    results = []\n",
    "    modifiers = []\n",
    "\n",
    "    for l in lines:\n",
    "        inverted_basemats = {v: k for k, v in ents[\"basemats\"].items()}\n",
    "        inverted_dopants = {v: k for k, v in ents[\"dopants\"].items()}\n",
    "\n",
    "        if l[-1] == \".\":\n",
    "            l = l[:-1]\n",
    "\n",
    "        # print(l)\n",
    "        basemat = None\n",
    "        dopant = None\n",
    "        result = None\n",
    "        modifier_list = None\n",
    "\n",
    "        if \"The host\" in l and \"was doped with\" in l:\n",
    "            # has basemats and dopants linked\n",
    "            try:\n",
    "                left, right = l.split(\"was doped with\")\n",
    "            except ValueError:\n",
    "                return ents\n",
    "\n",
    "            right = [r.strip() for r in right.split(\"'\") if r.strip()]\n",
    "            left = [le.strip() for le in left.split(\"'\") if le.strip() and \"The host\" not in le]\n",
    "\n",
    "            if not right or not left:\n",
    "                return ents\n",
    "\n",
    "            if len(left) != 1:\n",
    "                left = [\" \".join(left)]\n",
    "            elif len(right) != 1:\n",
    "                right = [\" \".join(right)]\n",
    "                # raise BaseException(f\"Left or right split on link was longer than 1!\\nLeft was {left} and right was {right}\")\n",
    "\n",
    "            basemat = left[0]\n",
    "            dopant = right[0]\n",
    "\n",
    "        elif \"The host\" in l and \"was doped\" in l:\n",
    "            left, _ = l.split(\"was doped\")\n",
    "            left = [le.strip() for le in left.split(\"'\") if le.strip() and \"The host\" not in le]\n",
    "\n",
    "            if len(left) != 1:\n",
    "                # raise BaseException(f\"Left split on basemat was longer than 1!\\nLeft was {left}\")\n",
    "                left = [\" \".join(left)]\n",
    "            basemat = left[0]\n",
    "\n",
    "        elif \"is a dopant\" in l:\n",
    "\n",
    "            split = l.split(\"is a dopant\")\n",
    "            left = \"\".join(split[:-1])\n",
    "            left = [le.strip() for le in left.split(\"'\") if le.strip()]\n",
    "\n",
    "            if len(left) != 1:\n",
    "                # raise BaseException(f\"Left split on dopant was longer than 1!\\nLeft was {left}\")\n",
    "                left = [\" \".join(left)]\n",
    "            dopant = left[0]\n",
    "\n",
    "        elif \"is a likely solid solution\" in l:\n",
    "            left, _ = l.split(\"is a likely solid solution\")\n",
    "            left = [le.strip() for le in left.split(\"'\") if le.strip()]\n",
    "            if len(left) != 1:\n",
    "                left = \" \".join(left)\n",
    "            else:\n",
    "                left = left[0]\n",
    "            result = left\n",
    "\n",
    "        elif \"Modifiers of the doping are\" in l:\n",
    "            if l[-1] == \".\":\n",
    "                l = l[:-1]\n",
    "            _, right = l.split(\"Modifiers of the doping are:\")\n",
    "            right = [ri.strip() for ri in right.split(\"'\") if\n",
    "                     len(ri.strip()) > 1]\n",
    "            modifier_list = right\n",
    "\n",
    "        else:\n",
    "            warnings.warn(f\"Line {l} gave no parsable data!\")\n",
    "            continue\n",
    "\n",
    "        if basemat:\n",
    "            if basemat in inverted_basemats:\n",
    "                bid = inverted_basemats[basemat]\n",
    "            else:\n",
    "                bid = f\"b{basemat_counter}\"\n",
    "                basemat_counter += 1\n",
    "\n",
    "            ents[\"basemats\"][bid] = basemat\n",
    "\n",
    "        if dopant:\n",
    "            if dopant in inverted_dopants:\n",
    "                did = inverted_dopants[dopant]\n",
    "            else:\n",
    "                did = f\"d{dopant_counter}\"\n",
    "                dopant_counter += 1\n",
    "\n",
    "            ents[\"dopants\"][did] = dopant\n",
    "\n",
    "        if basemat and dopant:\n",
    "            if did in ents[\"dopants2basemats\"]:\n",
    "                ents[\"dopants2basemats\"][did].append(bid)\n",
    "            else:\n",
    "                ents[\"dopants2basemats\"][did] = [bid]\n",
    "\n",
    "        if dopant and not basemat:\n",
    "            if did not in ents[\"dopants2basemats\"]:\n",
    "                ents[\"dopants2basemats\"] = []\n",
    "\n",
    "        if result:\n",
    "            results.append(result)\n",
    "        if modifier_list:\n",
    "            modifiers += modifier_list\n",
    "\n",
    "    ents[\"doping_modifiers\"] = {f\"m{i}\": m for i, m in enumerate(modifiers)}\n",
    "    ents[\"results\"] = {f\"r{i}\": r for i, r in enumerate(results)}\n",
    "    # print(f\"Text:\\n{text}\\n\\nResulted in {pprint.pformat(ents)}\\n\")\n",
    "    return ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_prompt_from_sentence_json(\n",
    "        sentence_json,\n",
    "        include_relevance_hint=False,\n",
    "        include_question=True,\n",
    "        start_token=START_TOKEN,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create an LLM prompt from a sentence's json representation.\n",
    "\n",
    "    Args:\n",
    "        sentence_json (dict): The JSON dict representation of the sentence.\n",
    "        include_relevance_hint (bool): Whether to include a hint about the relevance of the sentence.\n",
    "            Not used in publication, and in practice, does not actually affect performance.\n",
    "        include_question (bool): Whether to include a question about the sentence (i.e., an instruction).\n",
    "        start_token (str): The start token to use.\n",
    "\n",
    "    Returns:\n",
    "        str: The prompt for the LLM.\n",
    "    \"\"\"\n",
    "    text = sentence_json[\"sentence_text\"]\n",
    "    relevant = sentence_json[\"relevant\"]\n",
    "\n",
    "    if relevant:\n",
    "        relevance_hint = \"This text probably has information about doping.\"\n",
    "    else:\n",
    "        relevance_hint = \"This text probably does not have information about doping.\"\n",
    "\n",
    "    if include_relevance_hint:\n",
    "        text = f\"{text}\\n\\n{relevance_hint}\"\n",
    "\n",
    "    if include_question:\n",
    "        text = f\"{text}\\n\\nExtract doping information from this sentence.\"\n",
    "\n",
    "    return f\"{text}\\n{start_token}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_jsonl(\n",
    "        abstracts_raw_data,\n",
    "        output_filename,\n",
    "        include_irrelevant=False,\n",
    "        dry_run=False,\n",
    "        prompt_kwargs={},\n",
    "        completion_kwargs={},\n",
    "        fmt=\"eng\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a JSONL file from a list of abstracts (annotated or LLM-completed).\n",
    "    Used for training of the LLM.\n",
    "\n",
    "    Dry run means it will the prompts and completions to the console and not write them to file.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        abstracts_raw_data ([dict]): List of documents to create the JSONL from.\n",
    "        output_filename (str): The filename to write the JSONL to.\n",
    "        include_irrelevant (bool): Whether to include irrelevant sentences.\n",
    "        dry_run (bool): Whether to do a dry run (i.e., not write to file).\n",
    "        prompt_kwargs (dict): Keyword arguments to pass to llm_prompt_from_sentence_json.\n",
    "        completion_kwargs (dict): Keyword arguments to pass to llm_completion_from_sentence_json.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    \"\"\"\n",
    "    completions = []\n",
    "    prompts = []\n",
    "\n",
    "    for i, abstract_extracted in enumerate(abstracts_raw_data):\n",
    "        for s in abstract_extracted[\"doping_sentences\"]:\n",
    "\n",
    "            if not s[\"relevant\"] and not include_irrelevant:\n",
    "                if dry_run:\n",
    "                    print(\"SKIPPED FOR RELEVANCE\", s[\"sentence_text\"])\n",
    "                continue\n",
    "\n",
    "            prompt = llm_prompt_from_sentence_json(s, **prompt_kwargs)\n",
    "            completion = llm_completion_from_sentence_json(s, fmt=fmt, **completion_kwargs)\n",
    "\n",
    "            if dry_run:\n",
    "                print(abstract_extracted[\"doi\"])\n",
    "                pprint.pprint(s)\n",
    "                print(\"n\")\n",
    "                print(f\"PROMPT:\\n{prompt}\\n\")\n",
    "                print(f\"COMPLETION:\\n{completion}\\n\")\n",
    "                print(\"-\"*30 + \"\\n\\n\")\n",
    "\n",
    "            prompts.append(prompt)\n",
    "            completions.append(completion)\n",
    "\n",
    "    if dry_run:\n",
    "        print(\"File not written as dry_run=True\")\n",
    "    else:\n",
    "        with open(output_filename, \"w\") as f:\n",
    "            for i, c in enumerate(completions):\n",
    "                sample = {\n",
    "                    \"prompt\": prompts[i],\n",
    "                    \"completion\": completions[i]\n",
    "                }\n",
    "\n",
    "                j = json.dumps(sample)\n",
    "                f.write(j)\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "        print(f\"file written to {output_filename} with {len(completions)} sentence samples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentences_json_for_inference(entry):\n",
    "    \"\"\"\n",
    "    Prepare an entry for prediction with an LLM.\n",
    "    Entry must have abstract, doi, and title fields.\n",
    "\n",
    "    Args:\n",
    "        entry (dict): The entry to prepare.\n",
    "\n",
    "    Returns:\n",
    "        (dict): The updated, preprocessed entry.\n",
    "    \"\"\"\n",
    "    title = entry[\"title\"]\n",
    "    doi = entry[\"doi\"]\n",
    "    text = entry[\"text\"]\n",
    "    title_and_text = f\"{title}. {text}\" if title else text\n",
    "    sentences, cems_per_sentence = preprocess_text(title_and_text)\n",
    "\n",
    "    entry = {\n",
    "        \"doi\": doi,\n",
    "        \"title\": title,\n",
    "        \"text\": text,\n",
    "        \"doping_sentences\": [{\"sentence_text\": s, \"sentence_cems\": cems_per_sentence[i]} for i, s in enumerate(sentences)]\n",
    "    }\n",
    "    return entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Major core functions\n",
    "def gpt3_finetune(\n",
    "        data_training,\n",
    "        training_filename,\n",
    "        fmt=\"eng\",\n",
    "        write_extras=False,\n",
    "        n_epochs=7\n",
    "):\n",
    "    \"\"\"\n",
    "    Fine tune a doping model using data from the annotation script.\n",
    "\n",
    "    MUST adhere to the annotation script formatting for the json.\n",
    "\n",
    "    Args:\n",
    "        data_training (list): The training data, in the annotation script heirarchical format.\n",
    "        training_filename (str): the name of the file you want to save\n",
    "            the jsonl training tuples to. E.g., \"my_GPT3_training_file_version1.jsonl\".\n",
    "        fmt (str): Either \"json\" or \"eng\". Note to use ExtraEng use \"eng\" with write_extras=True.\n",
    "        write_extras (bool): Whether to write extras' information (results, modifiers) to the\n",
    "            training file.\n",
    "        n_epochs (int): The number of epochs to use for training.\n",
    "\n",
    "    Returns:\n",
    "        data_training_dois ([str]): The list of dois included here for training.\n",
    "        training_filename (str): The name of the training file output as jsonl.\n",
    "    \"\"\"\n",
    "    print(\"loading training set\")\n",
    "    data_training_dois = [d[\"doi\"] for d in data_training]\n",
    "    print(\"training set loaded.\")\n",
    "\n",
    "    create_jsonl(\n",
    "        data_training,\n",
    "        output_filename=training_filename,\n",
    "        include_irrelevant=False,\n",
    "        dry_run=False,\n",
    "        prompt_kwargs=dict(\n",
    "            include_relevance_hint=False,\n",
    "            include_question=True\n",
    "        ),\n",
    "        completion_kwargs=dict(\n",
    "            write_links=True,\n",
    "            write_nonlinked_dopants=True,\n",
    "            write_nonlinked_basemats=True,\n",
    "            write_results=write_extras,\n",
    "            write_modifiers=write_extras\n",
    "        ),\n",
    "        fmt=fmt\n",
    "    )\n",
    "    print(f\"JSONL written to {training_filename}.\")\n",
    "\n",
    "    os.system(f\"openai api fine_tunes.create -t '{training_filename}' -m 'davinci' --n_epochs={n_epochs}\")\n",
    "\n",
    "    print(f\"Model fine-tuning is in progress. Raw training JSONL data stored at {training_filename}.\")\n",
    "    return data_training_dois, training_filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt3_infer(\n",
    "        data_inference,\n",
    "        model,\n",
    "        output_filename=None,\n",
    "        save_every_n=100,\n",
    "        halt_on_error=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Infer gpt3 entries from raw data (e.g., from a dump of a mongodb query).\n",
    "\n",
    "    Args:\n",
    "        data_inference ([dict]): List of documents for inference. MUST have\n",
    "            the following fields: \"text\", \"title\", \"doi\".\n",
    "        model (str): The OpenAI GPT3 model name to use.\n",
    "        output_filename (str): The filename to write the final outputs to. If not\n",
    "            specified, will automatically name the file according to datetime.\n",
    "        save_every_n (int): How often to write a backup file for the inferred data.\n",
    "            Data will automatically be saved every time a rate limit error\n",
    "            occurs.\n",
    "        halt_on_error (bool): Whether to halt the inference on an exception\n",
    "            which is NOT a RateLimitError. If False, will not halt; if true,\n",
    "            will halt.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(f\"Loaded {len(data_inference)} samples for inference.\")\n",
    "    print(f\"Using {model} for prediction\")\n",
    "\n",
    "    gpt3_predictions = []\n",
    "    jsonl_data = []\n",
    "    for d in tqdm.tqdm(data_inference, desc=\"Texts processed\"):\n",
    "        dt = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "        dois_skipped = []\n",
    "        entry_json = create_sentences_json_for_inference(d)\n",
    "\n",
    "        sentences_json = entry_json[\"doping_sentences\"]\n",
    "        for s_json in sentences_json:\n",
    "            text = s_json[\"sentence_text\"]\n",
    "            cems = s_json[\"sentence_cems\"]\n",
    "\n",
    "            if sentence_is_paradigm(text, cems):\n",
    "                s_json[\"relevant\"] = True\n",
    "                prompt = llm_prompt_from_sentence_json(\n",
    "                    s_json,\n",
    "                    include_relevance_hint=False,\n",
    "                    include_question=True\n",
    "                )\n",
    "\n",
    "                has_response = False\n",
    "                while not has_response:\n",
    "                    try:\n",
    "                        response = openai.Completion.create(\n",
    "                            model=model,\n",
    "                            prompt=prompt,\n",
    "                            max_tokens=512,\n",
    "                            n=1,\n",
    "                            # top_p=1,\n",
    "                            temperature=0,\n",
    "                            stop=[STOP_TOKEN],\n",
    "                            logprobs=5\n",
    "                        ).choices[0]\n",
    "                        has_response = True\n",
    "                    except RateLimitError:\n",
    "                        warnings.warn(\"Ran into rate limit error, sleeping for 60 seconds and dumping midstream...\")\n",
    "                        dumpfn(gpt3_predictions, os.path.join(DATADIR, f\"midstream_ratelimit_{dt}.json\"))\n",
    "                        time.sleep(60)\n",
    "                        print(\"Resuming...\")\n",
    "                        continue\n",
    "                    except BaseException as BE:\n",
    "                        if halt_on_error:\n",
    "                            raise BE\n",
    "                        else:\n",
    "                            exc_type, exc_value, exc_traceback = sys.exc_info()\n",
    "                            warnings.warn(f\"Ran into external error: {BE}\")\n",
    "                            traceback.print_exception(exc_type, exc_value,\n",
    "                                                      exc_traceback,\n",
    "                                                      limit=2,\n",
    "                                                      file=sys.stdout)\n",
    "                            print(\"Resuming...\")\n",
    "                            break\n",
    "\n",
    "                # Record predictions, or put None if Error not halted on\n",
    "                s_json[\"llm_completion\"] = response.text if has_response else None\n",
    "                s_json[\"gpt3_logprobs_numbers\"] = response.logprobs.token_logprobs if has_response else None\n",
    "                s_json[\"gpt3_logprobs_tokens\"] = response.logprobs.tokens if has_response else None\n",
    "\n",
    "            else:\n",
    "                prompt = None\n",
    "                s_json[\"relevant\"] = False\n",
    "                s_json[\"llm_completion\"] = None\n",
    "                s_json[\"gpt3_logprobs_numbers\"] = None\n",
    "                s_json[\"gpt3_logprobs_tokens\"] = None\n",
    "\n",
    "            if prompt:\n",
    "                jsonl_data.append({\n",
    "                    \"prompt\": prompt,\n",
    "                    \"completion\": s_json[\"llm_completion\"],\n",
    "                })\n",
    "\n",
    "        gpt3_predictions.append(entry_json)\n",
    "        if len(gpt3_predictions) % save_every_n == 0:\n",
    "            print(f\"Saving {len(gpt3_predictions)} docs midstream\")\n",
    "            dumpfn(gpt3_predictions, os.path.join(DATADIR, f\"midstream_{dt}.json\"))\n",
    "\n",
    "    dumpfn(gpt3_predictions, output_filename)\n",
    "    jsonl_filename = output_filename.replace(\".json\", \".jsonl\")\n",
    "    dump_jsonl(jsonl_data, jsonl_filename)\n",
    "    print(f\"Dumped {len(gpt3_predictions)} total to {output_filename} (and raw jsonl to {jsonl_filename}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt3_decode(inferred_filename, output_filename, fmt=\"eng\"):\n",
    "    \"\"\"\n",
    "    Decode and coalesce GPT-3 completions to structured graphs.\n",
    "\n",
    "    Simply adds an \"entity_graph_raw\" key to each sample using the\n",
    "    \"doping_sentences\" as input.\n",
    "\n",
    "    Args:\n",
    "        inferred_filename (str): The filename holding the GPT-3 inferences, generated\n",
    "            by gpt3_infer.\n",
    "        output_filename (str): The filename to write structured graphs to.\n",
    "        fmt (str): The format to use (eng or json).\n",
    "    \"\"\"\n",
    "    inferred_samples = loadfn(inferred_filename)\n",
    "\n",
    "    for abstract_json in tqdm.tqdm(inferred_samples):\n",
    "        for sentence_json in abstract_json[\"doping_sentences\"]:\n",
    "            ents = decode_entities_from_llm_completion(sentence_json[\"llm_completion\"], fmt=fmt)\n",
    "            sentence_json[\"entity_graph_raw\"] = ents\n",
    "\n",
    "    n_decoded = len(inferred_samples)\n",
    "    dumpfn(inferred_samples, output_filename)\n",
    "\n",
    "    print(f\"Decoded {n_decoded} samples to file {output_filename}\")\n",
    "    return output_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import necessary libraries and define helper functions\n",
    "import argparse\n",
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "import warnings\n",
    "import json\n",
    "import openai\n",
    "\n",
    "# Function to load JSON data from a file\n",
    "def loadfn(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# Function to finetune GPT-3 (placeholder implementation)\n",
    "def gpt3_finetune(data_training, training_filename, fmt, write_extras, n_epochs):\n",
    "    print(f\"Training with {len(data_training)} samples. Output will be saved to {training_filename}.\")\n",
    "\n",
    "# Function to perform inference using GPT-3 (placeholder implementation)\n",
    "def gpt3_infer(data_inference, output_filename, model, save_every_n, halt_on_error):\n",
    "    print(f\"Inferring with model {model}. Results will be saved to {output_filename}.\")\n",
    "\n",
    "# Function to decode GPT-3 inference results (placeholder implementation)\n",
    "def gpt3_decode(inferred_filename, output_filename, fmt):\n",
    "    print(f\"Decoding results from {inferred_filename}. Decoded results will be saved to {output_filename}.\")\n",
    "\n",
    "# Function to simulate user input (this is a placeholder)\n",
    "def wrap_input(prompt):\n",
    "    return input(prompt)\n",
    "\n",
    "# Cell 2: Setup configuration parameters\n",
    "DATADIR = r\"D:\\Jupyter\\NERRE LLM\\data\"  # Adjust this to the correct directory as needed\n",
    "dt = datetime.datetime.now().strftime(\"%Y-%m-%d_%H.%M.%S\")\n",
    "\n",
    "# Hardcoded parameters to replace argparse arguments\n",
    "op_type = \"train\"  # or \"predict\"\n",
    "api_key = \"your_openai_api_key_here\"\n",
    "schema_type = \"eng\"  # \"eng\", \"engextra\", or \"json\"\n",
    "training_json = os.path.join(DATADIR, \"train.json\")\n",
    "training_jsonl_output = None  # Set to None to use default\n",
    "training_n_epochs = 5  # Number of epochs to train for\n",
    "inference_model_name = \"your_trained_model_name\"\n",
    "inference_json = os.path.join(DATADIR, \"test.json\")\n",
    "inference_json_raw_output = None  # Set to None to use default\n",
    "inference_json_final_output = None  # Set to None to use default\n",
    "inference_halt_on_error = True\n",
    "inference_save_every_n = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training JSONL file will be saved to D:\\Jupyter\\NERRE LLM\\data\\training_eng_2024-07-10_19.27.23.jsonl\n",
      "Inference JSONL file will be saved to D:\\Jupyter\\NERRE LLM\\data\\inference_raw_eng_2024-07-10_19.27.23.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell 3: Configure output file paths\n",
    "if not training_jsonl_output:\n",
    "    training_jsonl_output = os.path.join(DATADIR, f\"training_{schema_type}_{dt}.jsonl\")\n",
    "    print(f\"Training JSONL file will be saved to {training_jsonl_output}\")\n",
    "\n",
    "if not inference_json_raw_output:\n",
    "    inference_json_raw_output = os.path.join(DATADIR, f\"inference_raw_{schema_type}_{dt}.json\")\n",
    "    print(f\"Inference JSONL file will be saved to {inference_json_raw_output}\")\n",
    "\n",
    "if not inference_json_final_output:\n",
    "    inference_json_final_output = os.path.join(DATADIR, f\"inference_decoded_{schema_type}_{dt}.json\")\n",
    "\n",
    "openai.api_key = api_key\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 4: Define schema type\n",
    "st = schema_type.lower()\n",
    "if st == \"eng\":\n",
    "    fmt = \"eng\"\n",
    "    write_extras = False\n",
    "elif st == \"engextra\":\n",
    "    fmt = \"eng\"\n",
    "    write_extras = True\n",
    "elif st == \"json\":\n",
    "    fmt = \"json\"\n",
    "    write_extras = False\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"Unknown schema type: {st}. Choose from 'json', 'eng', or 'engextra'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 162 samples. Output will be saved to D:\\Jupyter\\NERRE LLM\\data\\training_eng_2024-07-10_19.27.23.jsonl.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell 5: Perform training or prediction based on op_type\n",
    "if op_type == \"train\":\n",
    "    data_training = loadfn(training_json)\n",
    "    gpt3_finetune(\n",
    "        data_training=data_training,\n",
    "        training_filename=training_jsonl_output,\n",
    "        fmt=fmt,\n",
    "        write_extras=write_extras,\n",
    "        n_epochs=training_n_epochs\n",
    "    )\n",
    "elif op_type == \"predict\":\n",
    "    data_infer = loadfn(inference_json)\n",
    "    data_infer = [{k: d[k] for k in (\"title\", \"text\", \"doi\")} for d in data_infer]\n",
    "\n",
    "    if not inference_model_name:\n",
    "        raise ValueError(\"No inference_model_name specified!\")\n",
    "\n",
    "    gpt3_infer(\n",
    "        data_inference=data_infer,\n",
    "        output_filename=inference_json_raw_output,\n",
    "        model=inference_model_name,\n",
    "        save_every_n=inference_save_every_n,\n",
    "        halt_on_error=inference_halt_on_error\n",
    "    )\n",
    "\n",
    "    gpt3_decode(\n",
    "        inferred_filename=inference_json_raw_output,\n",
    "        output_filename=inference_json_final_output,\n",
    "        fmt=fmt\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"Op type {op_type} unknown; choose from train or predict.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
