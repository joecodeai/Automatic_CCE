{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/explorer/anaconda3/envs/lasse/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, Trainer, TrainingArguments\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import f1_score\n",
    "from statistics import mean\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import re\n",
    "\n",
    "from prompts import PromptForAutoCCA\n",
    "import evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'mistral'\n",
    "DATA = 'acl_arc' # finecite, multicite\n",
    "SCHEMA = 'JSON1' # XML, JSON1, JSON2\n",
    "\n",
    "INPUT = f'./data/{DATA}/{MODEL}/{SCHEMA}/'\n",
    "OUTPUT = f'./output/{DATA}/{MODEL}/{SCHEMA}/'\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_mapping = {\n",
    "    \"mistral\":'mistralai/Mistral-7B-Instruct-v0.3',\n",
    "}\n",
    "model_id = model_mapping[MODEL]\n",
    "\n",
    "max_seq_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 10,485,760 || all params: 7,258,509,312 || trainable%: 0.1445\n"
     ]
    }
   ],
   "source": [
    "#tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "#model\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_storage=torch.bfloat16,\n",
    ")\n",
    "\n",
    "LMmodel = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config = bnb_config,\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    device_map ='auto'\n",
    ")\n",
    "LMmodel.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "peft_config = LoraConfig(target_modules=[ \"v_proj\", \"q_proj\", \"up_proj\", \"o_proj\", \"k_proj\", \"down_proj\", \"gate_proj\" ], inference_mode=False, r=4, lora_alpha=32, lora_dropout=0.1)\n",
    "\n",
    "LMmodel = get_peft_model(LMmodel, peft_config)\n",
    "\n",
    "LMmodel.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5/5 [00:00<00:00, 151.39 examples/s]\n",
      "Map: 100%|██████████| 5/5 [00:00<00:00, 1026.76 examples/s]\n",
      "Map: 100%|██████████| 5/5 [00:00<00:00, 830.98 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "with open(INPUT + 'train.json', 'r') as file:\n",
    "    train_data = json.load(file)\n",
    "    \n",
    "with open(INPUT + 'test.json', 'r') as file:\n",
    "    test_data = json.load(file)\n",
    "\n",
    "# Convert the DataFrame to a Dataset\n",
    "train_ds = Dataset.from_list(train_data[:5])\n",
    "test_ds = Dataset.from_list(test_data[:5])\n",
    "\n",
    "# initialize prompt class\n",
    "\n",
    "prompt = PromptForAutoCCA(tokenizer, DATA, SCHEMA)\n",
    "\n",
    "#Apply the tokenization function to the dataset\n",
    "train_ds = train_ds.map(\n",
    "    lambda row: prompt.create_sample(row['input'], row['output']), \n",
    "    batched=False, \n",
    "    remove_columns=train_ds.column_names\n",
    ")\n",
    "\n",
    "dev_ds = test_ds.map(\n",
    "    lambda row: prompt.create_sample(row['input'], row['output']), \n",
    "    batched=False, \n",
    "    remove_columns=test_ds.column_names\n",
    ")\n",
    "\n",
    "eval_ds = test_ds.map(\n",
    "    lambda row: prompt.create_sample(row['input'], row['output'], for_generation=True), \n",
    "    batched=False, \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Data Collator\n",
    "class CustomDataCollator:\n",
    "    def __init__(self, tokenizer, padding, max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.padding = padding\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, features):\n",
    "        batch = self.tokenizer.pad(\n",
    "            features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        labels = batch[\"input_ids\"].clone()\n",
    "        \n",
    "        # Compute loss mask for output token only\n",
    "        for i in range(batch['input_ids'].shape[0]):\n",
    "            \n",
    "            # Decode whole sample\n",
    "            text_content = self.tokenizer.decode(batch['input_ids'][i][1:])  \n",
    "            \n",
    "            # Extract output boundary\n",
    "            output_boundary = text_content.rfind(\"[/INST]\") + len(\"[/INST]\")\n",
    "            prompt_text = text_content[:output_boundary]\n",
    "            \n",
    "            # tokenize promt text\n",
    "            prompt_text_tokenized = self.tokenizer(\n",
    "                prompt_text,\n",
    "                return_overflowing_tokens=False,\n",
    "                return_length=False,\n",
    "            )\n",
    "            # get length of promt text\n",
    "            promt_text_len = len(prompt_text_tokenized['input_ids'])\n",
    "            \n",
    "            # set loss mask for promt text\n",
    "            labels[i][range(promt_text_len)] = -100\n",
    "            \n",
    "                    \n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "# init data collator\n",
    "data_collator=CustomDataCollator(\n",
    "    tokenizer=tokenizer, \n",
    "    padding=\"longest\", \n",
    "    max_length=max_seq_length, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trainer\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=OUTPUT,\n",
    "    eval_strategy = 'epoch',\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs = 3,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps = 5,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy = 'epoch',\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    label_names = ['labels'],\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=LMmodel,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=dev_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:05<00:00,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' {\"label\": [\"COMPARE_CONTRAST\"]}', ' {\"label\": [\"BACKGROUND\"]}', ' {\"label\": [\"COMPARE_CONTRAST\"]}', ' {\"label\": [\"USE\"]}', ' {\"label\": [\"COMPARE_CONTRAST\"]}']\n",
      "COMPARE_CONTRAST\n",
      "BACKGROUND\n",
      "COMPARE_CONTRAST\n",
      "USE\n",
      "COMPARE_CONTRAST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(evaluate)\n",
    "\n",
    "evaluator = evaluate.Evaluator(trainer.model, tokenizer, 20, eval_ds, DATA, SCHEMA, DEVICE)\n",
    "evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import logging\n",
    "# logging.set_verbosity_error()\n",
    "\n",
    "# class Evaluator:\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         model,\n",
    "#         tokenizer,\n",
    "#         max_new_tokens,\n",
    "#         eval_dataset,\n",
    "#         task,\n",
    "#         schema,\n",
    "#         cuda_device,\n",
    "#     ):\n",
    "#         self.model = model\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.max_new_tokens = max_new_tokens\n",
    "#         self.eval_dataset = eval_dataset\n",
    "#         self.task = task\n",
    "#         self.schema = schema\n",
    "#         self.device = cuda_device\n",
    "        \n",
    "#         model.to(self.device)\n",
    "\n",
    "#     def evaluate(self):\n",
    "#         predictions = []\n",
    "#         for sample in self.eval_dataset:\n",
    "#             gold_label = sample['gold']['label'][0]\n",
    "#             input_ids = sample['input_ids']     \n",
    "                   \n",
    "#             #generate task completion\n",
    "#             output = self.generate(input_ids)\n",
    "#             print(output)\n",
    "            \n",
    "#             #evaluate structure\n",
    "#             struc_eval = self.evaluate_structure(output)   \n",
    "#             print(struc_eval)\n",
    "            \n",
    "#             # add to predictions if ok\n",
    "#             if struc_eval: \n",
    "#                 json_dict = json.loads(output)\n",
    "#                 predictions.append({\n",
    "#                     'gold':gold_label,\n",
    "#                     'pred': json_dict['label']\n",
    "#                 })\n",
    "            \n",
    "#         #qualitative eval\n",
    "#         qual_eval = self.evaluate_quality(predictions)\n",
    "#         print(qual_eval)\n",
    "    \n",
    "#     def generate(self, input_ids):\n",
    "#         input_ids = torch.tensor([input_ids]).to(self.device) \n",
    "#         res = self.model.generate(input_ids, max_new_tokens = self.max_new_tokens)\n",
    "#         output = tokenizer.decode(res[0]).split('[/INST]')[-1]\n",
    "#         output = re.sub('</s>', '', output)\n",
    "#         return output\n",
    "    \n",
    "#     def evaluate_structure(self, output):\n",
    "#         #check if json\n",
    "#         is_valid_json = self.is_json(output)\n",
    "#         if not is_valid_json: return False\n",
    "        \n",
    "#         json_dict = json.loads(output)\n",
    "#         #check if has correct keys\n",
    "#         has_expected_keys = self.has_keys(json_dict, ['label'])\n",
    "#         if not has_expected_keys: return False\n",
    "        \n",
    "#         #check if class assigned labels exist\n",
    "#         has_exisiting_classes = self.class_exists([json_dict['label']], ['BACKGROUND','MOTIVATION','COMPARE_CONTRAST','USE'])\n",
    "#         return has_exisiting_classes\n",
    "        \n",
    "    \n",
    "#     def is_json(self, input_json):\n",
    "#         try:\n",
    "#             json.loads(input_json)\n",
    "#         except ValueError as e:\n",
    "#             return False\n",
    "#         return True\n",
    "    \n",
    "#     def has_keys(self, input_json, keys):\n",
    "#         for key in keys:\n",
    "#             if key not in input_json.keys():\n",
    "#                 return False\n",
    "#         return True\n",
    "    \n",
    "#     def class_exists(self, class_list, expected_classes):\n",
    "#         for cls in class_list:\n",
    "#             print(cls)\n",
    "#             if cls not in expected_classes:\n",
    "#                 return False\n",
    "#         return True\n",
    "    \n",
    "#     def evaluate_quality(self, preds):\n",
    "#         accuracy = mean([1 if sample['gold'] == sample['pred'] else 0 for sample in preds])\n",
    "#         return accuracy\n",
    "    \n",
    "\n",
    "\n",
    "# def calculate_metrics(eval_df):\n",
    "#     valid_json = sum(eval_df['is_valid_json']) / len(eval_df)\n",
    "#     eval_df.dropna(inplace=True)\n",
    "#     macro_f1 = f1_score([int(no) for no in eval_df['label']], [int(no) for no in eval_df['label_pred']], average='macro')\n",
    "#     micro_f1 = f1_score([int(no) for no in eval_df['label']], [int(no) for no in eval_df['label_pred']], average='micro')\n",
    "#     return valid_json, micro_f1, macro_f1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lasse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
