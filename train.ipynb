{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from JSON\n",
    "with open('./data/gpt3/inference_decoded_eng.json', 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare lists to store the extracted texts and completions\n",
    "texts = []\n",
    "completions = []\n",
    "\n",
    "# Iterate through each entry in the data\n",
    "for entry in data:\n",
    "    # Iterate through each 'doping_sentence' in the current entry\n",
    "    for doping_sentence in entry.get('doping_sentences', []):\n",
    "        # Extract 'sentence_text' and 'llm_completion'\n",
    "        text = doping_sentence.get('sentence_text', '')\n",
    "        completion = doping_sentence.get('llm_completion', '')\n",
    "        \n",
    "        # Append to the lists\n",
    "        texts.append(text)\n",
    "        completions.append(completion)\n",
    "\n",
    "# Create a DataFrame from the lists\n",
    "df = pd.DataFrame({'sentence_text': texts, 'doping': completions})\n",
    "df = df.dropna(subset=['doping'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_text</th>\n",
       "      <th>doping</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Comparison of chemical bath-deposited ZnO film...</td>\n",
       "      <td>The host 'ZnO' was doped with 'Al'.\\nThe host...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A comparative study is presented on chemical b...</td>\n",
       "      <td>The host 'ZnO' was doped with 'Al'.\\nThe host...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The study reveals marked differences in dopant...</td>\n",
       "      <td>There is no doping information.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The presence of dopant in the solution induces...</td>\n",
       "      <td>There is no doping information.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All films are (002)-textured, whereas the latt...</td>\n",
       "      <td>The host 'Zn' was doped.\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       sentence_text  \\\n",
       "0  Comparison of chemical bath-deposited ZnO film...   \n",
       "1  A comparative study is presented on chemical b...   \n",
       "2  The study reveals marked differences in dopant...   \n",
       "3  The presence of dopant in the solution induces...   \n",
       "4  All films are (002)-textured, whereas the latt...   \n",
       "\n",
       "                                              doping  \n",
       "0   The host 'ZnO' was doped with 'Al'.\\nThe host...  \n",
       "1   The host 'ZnO' was doped with 'Al'.\\nThe host...  \n",
       "2                  There is no doping information.\\n  \n",
       "3                  There is no doping information.\\n  \n",
       "4                         The host 'Zn' was doped.\\n  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding Token: <|end_of_text|>\n",
      "Padding Token ID: 128001\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Check if the tokenizer has a pad token\n",
    "if tokenizer.pad_token is None:\n",
    "    # Define a pad token (use the end of sentence token as pad token if needed)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"Padding Token: {tokenizer.pad_token}\")\n",
    "print(f\"Padding Token ID: {tokenizer.pad_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a9c0e3784044845a2814c8edfe77d9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/80 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    # Tokenize the input sentences\n",
    "    inputs = tokenizer(\n",
    "        examples['sentence_text'], \n",
    "        max_length=128, \n",
    "        truncation=True, \n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"  \n",
    "    )\n",
    "\n",
    "    # Tokenize the labels\n",
    "    labels = tokenizer(\n",
    "        examples['doping'], \n",
    "        max_length=32,\n",
    "        truncation=True, \n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"  \n",
    "    )\n",
    "\n",
    "    # Ensure labels are not padded with -100 to ignore them in loss computation\n",
    "    labels['input_ids'] = [\n",
    "        [(label if label != tokenizer.pad_token_id else -100) for label in label_list] \n",
    "        for label_list in labels['input_ids']\n",
    "    ]\n",
    "\n",
    "    # Convert inputs and labels to tensors\n",
    "    inputs = {key: val for key, val in inputs.items()}\n",
    "    labels = {key: val for key, val in labels.items()}\n",
    "    \n",
    "    inputs['labels'] = labels['input_ids']\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "# Convert the filtered DataFrame to a Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Apply the preprocessing function\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,  \n",
    "    per_device_eval_batch_size=4,   \n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Initialize the model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset,  # Use a separate validation set for better evaluation\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./trained_model')\n",
    "tokenizer.save_pretrained('./trained_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained('./trained_model')\n",
    "tokenizer = AutoTokenizer.from_pretrained('./trained_model')\n",
    "\n",
    "# Example inference\n",
    "inputs = tokenizer(\"Example sentence.\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
