{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/explorer/anaconda3/envs/lasse/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import importlib\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, Trainer, TrainingArguments\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import f1_score\n",
    "from statistics import mean\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import re\n",
    "\n",
    "import prompts\n",
    "import evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'mistral'\n",
    "DATA = 'multicite' #acl_arc, finecite, multicite\n",
    "SCHEMA = 'XML' # XML, JSON1, JSON2\n",
    "\n",
    "INPUT = f'./data/{DATA}/{MODEL}/{SCHEMA}/'\n",
    "OUTPUT = f'./output/{DATA}/{MODEL}/{SCHEMA}/'\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_mapping = {\n",
    "    \"mistral\":'mistralai/Mistral-7B-Instruct-v0.3',\n",
    "}\n",
    "model_id = model_mapping[MODEL]\n",
    "\n",
    "max_seq_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "#tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "#model\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_storage=torch.bfloat16,\n",
    ")\n",
    "\n",
    "LMmodel = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config = bnb_config,\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    device_map ='auto'\n",
    ")\n",
    "LMmodel.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "peft_config = LoraConfig(target_modules=[ \"v_proj\", \"q_proj\", \"up_proj\", \"o_proj\", \"k_proj\", \"down_proj\", \"gate_proj\" ], inference_mode=False, r=4, lora_alpha=32, lora_dropout=0.1)\n",
    "\n",
    "LMmodel = get_peft_model(LMmodel, peft_config)\n",
    "\n",
    "LMmodel.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/844 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 844/844 [00:00<00:00, 1026.92 examples/s]\n",
      "Map: 100%|██████████| 20/20 [00:00<00:00, 894.23 examples/s]\n",
      "Map: 100%|██████████| 20/20 [00:00<00:00, 1363.89 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "with open(INPUT + 'train.json', 'r') as file:\n",
    "    train_data = json.load(file)\n",
    "    \n",
    "with open(INPUT + 'test.json', 'r') as file:\n",
    "    test_data = json.load(file)\n",
    "\n",
    "# Convert the DataFrame to a Dataset\n",
    "train_ds = Dataset.from_list(train_data)\n",
    "test_ds = Dataset.from_list(test_data[:20])\n",
    "\n",
    "# initialize prompt class\n",
    "\n",
    "prompt = prompts.PromptForAutoCCA(tokenizer, DATA, SCHEMA)\n",
    "\n",
    "#Apply the tokenization function to the dataset\n",
    "train_ds = train_ds.map(\n",
    "    lambda row: prompt.create_sample(row['input'], row['output']), \n",
    "    batched=False, \n",
    "    remove_columns=train_ds.column_names\n",
    ")\n",
    "\n",
    "dev_ds = test_ds.map(\n",
    "    lambda row: prompt.create_sample(row['input'], row['output']), \n",
    "    batched=False, \n",
    "    remove_columns=test_ds.column_names\n",
    ")\n",
    "\n",
    "eval_ds = test_ds.map(\n",
    "    lambda row: prompt.create_sample(row['input'], row['output'], for_generation=True), \n",
    "    batched=False, \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Data Collator\n",
    "class CustomDataCollator:\n",
    "    def __init__(self, tokenizer, padding, max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.padding = padding\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, features):\n",
    "        batch = self.tokenizer.pad(\n",
    "            features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        labels = batch[\"input_ids\"].clone()\n",
    "        \n",
    "        # Compute loss mask for output token only\n",
    "        for i in range(batch['input_ids'].shape[0]):\n",
    "            \n",
    "            # Decode whole sample\n",
    "            text_content = self.tokenizer.decode(batch['input_ids'][i][1:])  \n",
    "            \n",
    "            # Extract output boundary\n",
    "            output_boundary = text_content.rfind(\"[/INST]\") + len(\"[/INST]\")\n",
    "            prompt_text = text_content[:output_boundary]\n",
    "            \n",
    "            # tokenize promt text\n",
    "            prompt_text_tokenized = self.tokenizer(\n",
    "                prompt_text,\n",
    "                return_overflowing_tokens=False,\n",
    "                return_length=False,\n",
    "            )\n",
    "            # get length of promt text\n",
    "            promt_text_len = len(prompt_text_tokenized['input_ids'])\n",
    "            \n",
    "            # set loss mask for promt text\n",
    "            labels[i][range(promt_text_len)] = -100\n",
    "            \n",
    "                    \n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "# init data collator\n",
    "data_collator=CustomDataCollator(\n",
    "    tokenizer=tokenizer, \n",
    "    padding=\"longest\", \n",
    "    max_length=max_seq_length, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trainer\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=OUTPUT,\n",
    "    eval_strategy = 'epoch',\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs = 3,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps = 50,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy = 'epoch',\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    label_names = ['labels'],\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=LMmodel,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=dev_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There is a person sitting on a horse. he is holding a horse thread and he is wearing a cap. there are flags, board on the left side. we can see in the background sky, trees. Contrastive Learning Recently, contrastive learning has been widely studied in unsupervised representation learning for vision, #REF , language #REF , or multi-modal #REF . The goal is to learn semantic representation between two views by allowing the positive sample to be similar (in semantic space) and negatives to be dissimilar semantically simultaneously. CLIP #REF and MIL-NCE #TARGET_REF has demonstrated the effectiveness for learning the semantic mapping between vision and language. Previous attempts mainly exploit the InfoNCE #REF objective to maximize a lower bound of the mutual information. This paper extends the multimodal contrastive learning between the trace in the image and captioning sentence. In the same image, they correspond to each other semantically. This motivates us to design a contrastive loss for better 2022 alignment between the trace and language.', 'We use the OPUS #TARGET_REF parallel movie subtitle corpus of subtitles collected from opensubtitles.org as a multi-domain proxy. As the movies we use for source data cover several different genres and, although scripted, represents real human language used in a multitude of situations similar to many social media platforms.', 'In this paper, we developed a semi-supervised strategy to detect toxic comments in the Brazilian Portuguese language. Semi-supervision is the problem of learning from labeled and unlabeled data #TARGET_REF , in which given a point set X = {x 1 , ..., x l , x l+1 , ..., x n } and a label set L = {1, ..., c}, the first l points have labels {y 1 , ..., y l } ∈ L and the remaining points are unlabeled #REF .', 'In practice, the separation of question encoding and passage encoding is desirable, so that the dense representations of all passages can be precomputed for efficient retrieval. Here, we adopt two independent neural networks initialized from pre-trained LMs for the two encoders E q (•) and E p (•) separately, and take the representations at the first token (e.g., #TARGET_REF symbol in BERT) as the output for encoding.', 'The same approach is applied on diacritic model instead of using character information we use diacritics and instead of using C-Bi-LSTM we use D-Bi-LSTM. We extract the forward and backward outputs of the trained D-Bi-LSTM as individually-trained diacritic embeddings. It is worth noting that both of these models are trained on diacritized version of the datasets. Also it is important to mention that the output from this step are weights to initialize the character and diacritic embedding layers in the combination model and both of these sets of weights have been trained individually in separate models. The justification of this step can be found in Section ( 6) #TARGET_REF where the experiments showed that training these embeddings separately and using them as individually-trained embedding in the final model improves the performance.', 'In this work, we introduce a framework to automatically identify spurious correlations exploited by the model, sometimes also denoted as \"shortcuts\" in prior work #REF 1 , at a large scale. Our proposed framework differs from existing literature with a focus more on automatic shortcut identification, instead of pre-defining a limited set of shortcuts or learning from human annotations (Table 1). Our framework works as follows: given a task and a trained model, we first utilize interpretability methods, e.g., attention scores #REF and integrated gradient #REF which are commonly used for interpreting model\\'s decisions, to automatically extract tokens that the model deems as important for task label Objective Approach for shortcut identification #TARGET_REF Robustness against known shortcuts Pre-defined #REF Robustness against known shortcuts Pre-defined #REF Robustness against unknown shortcuts A low-capacity model to specifically learn shortcuts #REF prediction. We then introduce two extra steps to further categorize the extracted tokens to be \"genuine\" or \"spurious\". We utilize a cross-dataset analysis to identify tokens that are more likely to be \"shortcut\".', \"In a Rapid Serial Visual Presentation (RSVP) paradigm, sentences were presented one word at a time, on average every ≈ 240 ms, in a white monospace font on a grey background approximately in the centre of the screen, at the optimal viewing position #TARGET_REF . Each word subtended a horizontal angle 0.76 • to the left and 11.81 • to the right from the centre. Sentences were separated by 500 ms of a white central fixation cross (see Figure 1). On approximately 20% of the sentences in each session, the participant was prompted to verbalise the previous sentence back to the experimenter. An accuracy score of 93% across all sessions confirmed that the participant successfully attended the sentences. Stimuli were presented using PsychoPy #REF on an LCD monitor with a resolution of 1920x1080 pixels and 60 Hz refresh rate. The subject's head was stabilised with a chin-rest. Gilching, Germany). Channel impedances were kept below 15 kΩ. Data were preprocessed using MNE-Python #REF . Individual EEG sessions were band-pass filtered between 1-40 Hz, downsampled to 250 Hz and re-referenced to average reference. Noisy channels were determined based on visual inspection and interpolated. Non-neuronal components (e.g. ocular, muscular, electrical) were removed via Independent Component Analysis (ICA) individually for each recording session (an average of 4 components were removed per EEG session).\", 'End-to-end automatic speech recognition (E2E ASR), which transcribes speech using a single neural network (NN), has recently gained traction #TARGET_REF . Existing E2E ASR models generate audio transcripts by sequentially producing likely graphemes, or multi-graphemic units, from which lexical items of a language can be recovered. However, other linguistic annotations such as phonemic transcripts, part-of-speech (POS) tags, or word boundaries, help understand the underlying audio characteristics #REF . Such linguistic annotations are especially important in natural language processing (NLP) tasks done on audio !! ! \" !\"! # \" #\"! # $ # \" $ ! \"! \" !\"# # \" #\"# # $ # \" $ \" \"# \" # \" # \" # \" # $ # \" # \"', \"In this paper, we focus on measuring and mitigating gender-biased language in machine-generated job ads, a use case of large-scale language models which risks representational and allocational harms #REF . Representational harms come from the conditioning of a job's suitability to a given individual based on their gender. When jobs are valued unequally (either by financial, social or intellectual status), this, in turn, can reinforce gendered power hierarchies and negative societal divisions. Gender-biased language may result in an unequal distribution of job applications if it dissuades gender-diverse candidates from applying #REF . Thus, allocational harms are relevant where labour market opportunities, financial remuneration or job stability are preferentially granted based on gender. We know from prior NLP research that GPT models reflect occupational stereotypes in society #TARGET_REF , confirming the risk of representational harm, but not how this translates into allocational harms in applied settings. To measure bias, our experiment employs lists of gender-coded words. These lists are potentially in themselves biased, having been defined by a research group under a particular cultural bias or as the result of biased data. To mitigate this concern, we use multiple measures to cover the blind spots or specific biases present in any single list. However, our proposed metric may better capture the most obvious, text-level aspects of gender-biased language and will be less effective to find covert, but equally as damaging, forms of gender bias in job ads, or job search more broadly.\", 'Offensive language can include various categories such as threats, vilification, insults, calumniation, discrimination and swearing #TARGET_REF . Detection of such language is necessary for ease of moderation of content on social media. Despite their popularity, toxicity detection tasks have focused majorly on sequence classification, rather than sequence tagging. Finding which spans make a comment or document toxic in nature is crucial in explaining the reasons behind their toxicity. Additionally, such attributions would allow for more efficient semi-automated quality-based moderation of content, especially for verbose documents, in comparison to quantitative toxicity scores.', 'Generating multiple valid outputs given a source sequence has a wide range of applications, such as machine translation #REF , paraphrase generation #REF , question generation #TARGET_REF , dialogue system #REF , and story generation #REF . For example, in machine translation, there are often many plausible and semantically equivalent translations due to information asymmetry between different languages #REF .', 't , x t )|1 ≤ t ≤ n d }, let {b t |1 ≤ t ≤ n d }be the dialogue state for each turn, where b t ∈ {0, 1} N b and N b is the number of all slot-value pairs. Dialogue state tracking is usually formulated as a multilabel learning problem where the state at turn t predicted by modeling the conditional distribution p(bt |c t ) = p(b t |u t , x t−1 , b t−1 ), where b t−1 is the dialogue state in the previous turn. To model this conditional distribution, a state tracking model p B (u t , x t−1 , b t−1 ) mainly employs an utterance en-coder, a context encoder to work with a slot-value predictor that estimates whether a slot-value pair should be included in the dialogue states #TARGET_REF . Specifically, the predictor takes as input a slot-value pair (s i , e i ), and the encoded utterances h utt ∈ R D and context h ctx ∈ R D from the utterance encoder f utt (u t , x t−1 ) and context encoder f ctx (b t−1 ) respectively, and D is the hidden dimension. The prediction is then performed by aggregating the results of slot-value predictor f val (h utt , h ctx , (s i , e i )) for the complete N b slotvalue pairs. We optimize the state tracking model using the cross-entropy loss:L = d i t=1:n d − log(b t •p B (u t , x t−1 , c t−1 )) (4)where the parameters of p B , which include f utt , f ctx , and f val , are jointly trained.', 'where h v and h r are node embedding and relation embedding. We define the compositional operation as ϕ(h u , h r ) = h u −h r inspired by the TransE #TARGET_REF . The relation embedding is also updated via another linear transformation:', 'ArXiv open-sources the LaTeX version of their articles, when available. In order to make our Symlink dataset open-access to the whole community, we crawled the metadata of these articles and only selected articles under the CC BY license. Once obtained the LaTeX project, we extracted all the paragraphs from the .tex files. We filtered out all short paragraphs with less than 50 words and paragraphs without symbols. Since a formula can be composed in multiple ways such as inline formulae (between $ $), displayed formulae (between $$ $$), or using commands e.g. array, to keep the original TeX format of the formulae, all of these math objects are masked before tokenization. Then, we used the SciBERT tokenizer #TARGET_REF to tokenize the text. The original math object is then restored. As we observed that many papers have nested math objects, we deleted all the nested objects, hence, having non-nested LaTeX data. This is helpful as it makes the LaTeX documents more similar to the ones generated by the PDF-to-LaTeX tools, which do not contain nested objects.', 'Taking inspiration from gradient checkpointing #TARGET_REF , we observe that if the inputs Q C , K, V are available during the backward pass, we can re-compute o C and then use the produced intermediate activations to computed (Q C ) from d (o C ). Once d (Q C) is computed, we can again discard the intermediate activations and gradients produced during this step and move on to the next chunk. This ensures that the peak memory usage during the backward pass through the attention layer is bounded by the memory required to backpropagate through a single chunk.', '• Joint: we concatenate the training examples from Quoref and CoNLL-to-QA converted 11 The only difference of TASE in our experiments and the reported results in #TARGET_REF is the number of training epochs. For a fair comparison, we train all models for the same number of iterations.', 'To answer these questions, we first establish in section 2 a state-of-the-art that is meant to be broad enough to have a shallow overview depicting the ins and outs and issues around the usability of Transformer-based models whose breadcrumb trail is the issue of resources. Then, we present in the section 3 the recent progress of the questionanswering task, through the use of these latest models. In sections 4 and 5 we introduce our model and present our experiments on the usability of Transformers models in a question-answering task for #REF and PIAF #REF corpora. We propose to address the instability relating to data scarcity by investigating various training strategies with data augmentation, hyperparameters optimization and cross-lingual transfer. Finally, we present a new compact model for French based on ALBERT #TARGET_REF 1 , and compare it to existing monolingual and multilingual models, large and compact, under constrained conditions (notably on learning data).', 'The caption generation backbone is a transformerbased encoder-decoder proposed by #TARGET_REF , which mainly employs a multi-head attention mechanism and achieves top-tier performance in many sequential related tasks. Here, we highlight several task-oriented modifications.', 'The new lexico-structural transfer approach is more similar to the interlingua approach in that they both have a predicate-argument structure representation of the meaning of the sentence, which gives them roughly equivalent semantic depth. Another similarity is that the new transfer approach can also combine lexical items from several languages together into a single transfer lexicon entry, greatly simplifying the task of adding the mapping to a new language #TARGET_REF . An important remaining difference is that the interlingua approach would claim that a single predicate-argument structure can serve as a common representation for many languages, whereas the transfer approach allows for language-specific predicate-argument structures.. A fundamental assumption of either approach, and the most important similarity, is that these classifications can be made based on distinguished semantic features, and that these semantic features will be relevant to classification schemes in other languages. Whether the classification schemes serve as a means of associating a single logical form composed of semantic primitives with many lexical items, as in the LCS approach, or as a means of enriching a set of logical forms with a collection of semantic features, the classifications still have to be determined, and the associations with semantic features have to be made. The rest of this paper discusses specific issues with respect to the association of semantic features with the classifications in English verbs.', 'Both the encoder and decoder are six-layer Transformer #TARGET_REF , the number of heads of Multi-Head Attention is 8, the token embedding dimension is 512, and the ratio of Dropout is 0.1. Adam (Kingma and Ba, 2015) was used as the optimization method for parameters during training. The learning rate of Adam was set to 0.001. Hyperparameters λ, which adjust the frequency of ITF model and INF model, were set as 0.2, 0.4, 0.6, or 0.8. The INF model uses bi-gram as its n-gram function.']\n",
      "['There is a person sitting on a horse. he is holding a horse thread and he is wearing a cap. there are flags, board on the left side. we can see in the background sky, trees. Contrastive Learning Recently, contrastive learning has been widely studied in unsupervised representation learning for vision, #REF , language #REF , or multi-modal #REF . <PERCEPTION> The goal is to learn semantic representation between two views by allowing the positive sample to be similar </PERCEPTION> <BACKGROUND> (in semantic space) </BACKGROUND> <PERCEPTION> and negatives to be dissimilar semantically simultaneously. </PERCEPTION> <BACKGROUND> CLIP #REF and </BACKGROUND> <INFORMATION> MIL-NCE #TARGET_REF has demonstrated the effectiveness for learning the semantic mapping between vision and language. </INFORMATION> Previous attempts mainly exploit the InfoNCE #REF objective to maximize a lower bound of the mutual information. This paper extends the multimodal contrastive learning between the trace in the image and captioning sentence. In the same image, they correspond to each other semantically. This motivates us to design a contrastive loss for better 2022 alignment between the trace and language.', '<PERCEPTION> We use the </PERCEPTION> <INFORMATION> OPUS #TARGET_REF parallel movie subtitle corpus of subtitles collected from opensubtitles.org </INFORMATION> <PERCEPTION> as a multi-domain proxy. </PERCEPTION> As the movies we use for source data cover several different genres and, although scripted, represents real human language used in a multitude of situations similar to many social media platforms.', 'In this paper, we developed a semi-supervised strategy to detect toxic comments in the Brazilian Portuguese language. <INFORMATION> Semi-supervision is the problem of learning from labeled and unlabeled data #TARGET_REF , in which given a point set X = {x 1 , ..., x l , x l+1 , ..., x n } and a label set L = {1, ..., c}, the first l points have labels {y 1 , ..., y l } ∈ L </INFORMATION> <BACKGROUND> and the remaining points are unlabeled #REF . </BACKGROUND>', 'In practice, <PERCEPTION> the separation of question encoding and passage encoding is desirable, so that the dense representations of all passages can be precomputed for efficient retrieval. </PERCEPTION> Here, <PERCEPTION> we adopt two independent neural networks initialized from pre-trained LMs for the two encoders E q (•) and E p (•) separately, and take the representations at the first token </PERCEPTION> (e.g., <INFORMATION> #TARGET_REF symbol in BERT) </INFORMATION> <PERCEPTION> as the output for encoding. </PERCEPTION>', 'The same approach is applied on diacritic model instead of using character information we use diacritics and instead of using C-Bi-LSTM we use D-Bi-LSTM. We extract the forward and backward outputs of the trained D-Bi-LSTM as individually-trained diacritic embeddings. It is worth noting that both of these models are trained on diacritized version of the datasets. Also it is important to mention that the output from this step are weights to initialize the character and diacritic embedding layers in the combination model and both of these sets of weights have been trained individually in separate models. <BACKGROUND> The justification of this step can be found in Section ( 6) </BACKGROUND> <INFORMATION> #TARGET_REF where the experiments showed that training these embeddings separately and using them as individually-trained embedding in the final model improves the performance. </INFORMATION>', 'In this work, we introduce a framework to automatically identify spurious correlations exploited by the model, sometimes also denoted as \"shortcuts\" in prior work #REF 1 , at a large scale. Our proposed framework differs from existing literature with a focus more on automatic shortcut identification, instead of pre-defining a limited set of shortcuts or learning from human annotations (Table 1). Our framework works as follows: <PERCEPTION> given a task and a trained model, we first utilize interpretability methods, </PERCEPTION> <BACKGROUND> e.g., attention scores #REF and integrated gradient #REF </BACKGROUND> which are commonly used for interpreting model\\'s decisions, <PERCEPTION> to automatically extract tokens that the model deems as important for task label Objective Approach for </PERCEPTION> <INFORMATION> shortcut identification #TARGET_REF </INFORMATION> Robustness against known shortcuts Pre-defined #REF Robustness against known shortcuts Pre-defined #REF Robustness against unknown shortcuts A low-capacity model to specifically learn shortcuts #REF prediction. We then introduce two extra steps to further categorize the extracted tokens to be \"genuine\" or \"spurious\". We utilize a cross-dataset analysis to identify tokens that are more likely to be \"shortcut\".', \"<INFORMATION> In a Rapid Serial Visual Presentation (RSVP) paradigm, sentences were presented one word at a time, on average every ≈ 240 ms, in a white monospace font on a grey background approximately in the centre of the screen, at the optimal viewing position #TARGET_REF . </INFORMATION> <BACKGROUND> Each word subtended a horizontal angle 0.76 • to the left and 11.81 • to the right from the centre. Sentences were separated by 500 ms of a white central fixation cross (see Figure 1). </BACKGROUND> On approximately 20% of the sentences in each session, the participant was prompted to verbalise the previous sentence back to the experimenter. An accuracy score of 93% across all sessions confirmed that the participant successfully attended the sentences. Stimuli were presented using PsychoPy #REF on an LCD monitor with a resolution of 1920x1080 pixels and 60 Hz refresh rate. The subject's head was stabilised with a chin-rest. Gilching, Germany). Channel impedances were kept below 15 kΩ. Data were preprocessed using MNE-Python #REF . Individual EEG sessions were band-pass filtered between 1-40 Hz, downsampled to 250 Hz and re-referenced to average reference. Noisy channels were determined based on visual inspection and interpolated. Non-neuronal components (e.g. ocular, muscular, electrical) were removed via Independent Component Analysis (ICA) individually for each recording session (an average of 4 components were removed per EEG session).\", '<INFORMATION> End-to-end automatic speech recognition (E2E ASR), which transcribes speech using a single neural network (NN), </INFORMATION> <PERCEPTION> has recently gained traction #TARGET_REF . </PERCEPTION> <BACKGROUND> Existing E2E ASR models generate audio transcripts by sequentially producing likely graphemes, or multi-graphemic units, from which lexical items of a language can be recovered. </BACKGROUND> However, other linguistic annotations such as phonemic transcripts, part-of-speech (POS) tags, or word boundaries, help understand the underlying audio characteristics #REF . Such linguistic annotations are especially important in natural language processing (NLP) tasks done on audio !! ! \" !\"! # \" #\"! # $ # \" $ ! \"! \" !\"# # \" #\"# # $ # \" $ \" \"# \" # \" # \" # \" # $ # \" # \"', \"In this paper, we focus on measuring and mitigating gender-biased language in machine-generated job ads, a use case of large-scale language models which risks representational and allocational harms #REF . Representational harms come from the conditioning of a job's suitability to a given individual based on their gender. When jobs are valued unequally (either by financial, social or intellectual status), this, in turn, can reinforce gendered power hierarchies and negative societal divisions. <BACKGROUND> Gender-biased language may result in an unequal distribution of job applications if it dissuades gender-diverse candidates from applying #REF . </BACKGROUND> Thus, allocational harms are relevant where labour market opportunities, financial remuneration or job stability are preferentially granted based on gender. <PERCEPTION> We know from prior NLP research that </PERCEPTION> <INFORMATION> GPT models reflect occupational stereotypes in society #TARGET_REF , confirming the risk of representational harm, but not how this translates into allocational harms in applied settings. </INFORMATION> To measure bias, our experiment employs lists of gender-coded words. These lists are potentially in themselves biased, having been defined by a research group under a particular cultural bias or as the result of biased data. To mitigate this concern, we use multiple measures to cover the blind spots or specific biases present in any single list. However, our proposed metric may better capture the most obvious, text-level aspects of gender-biased language and will be less effective to find covert, but equally as damaging, forms of gender bias in job ads, or job search more broadly.\", '<INFORMATION> Offensive language can include various categories such as threats, vilification, insults, calumniation, discrimination and swearing #TARGET_REF . </INFORMATION> <BACKGROUND> Detection of such language is necessary for ease of moderation of content on social media. </BACKGROUND> Despite their popularity, toxicity detection tasks have focused majorly on sequence classification, rather than sequence tagging. <BACKGROUND> Finding which spans make a comment or document toxic in nature is crucial in explaining the reasons behind their toxicity. </BACKGROUND> Additionally, such attributions would allow for more efficient semi-automated quality-based moderation of content, especially for verbose documents, in comparison to quantitative toxicity scores.', '<BACKGROUND> Generating multiple valid outputs given a source sequence has a wide range of applications, such as machine translation #REF , paraphrase generation #REF , </BACKGROUND> <INFORMATION> question generation #TARGET_REF , </INFORMATION> <BACKGROUND> dialogue system #REF , and story generation #REF . </BACKGROUND> For example, in machine translation, there are often many plausible and semantically equivalent translations due to information asymmetry between different languages #REF .', 't , x t )|1 ≤ t ≤ n d }, let {b t |1 ≤ t ≤ n d }be the dialogue state for each turn, where b t ∈ {0, 1} N b and N b is the number of all slot-value pairs. Dialogue state tracking is usually formulated as a multilabel learning problem where the state at turn t predicted by modeling the conditional distribution p(bt |c t ) = p(b t |u t , x t−1 , b t−1 ), where b t−1 is the dialogue state in the previous turn. <PERCEPTION> To model this conditional distribution, a state tracking model p B (u t , x t−1 , b t−1 ) mainly employs an utterance en-coder, a context encoder to work with </PERCEPTION> <INFORMATION> a slot-value predictor that estimates whether a slot-value pair should be included in the dialogue states #TARGET_REF . </INFORMATION> <BACKGROUND> Specifically, the predictor takes as input a slot-value pair (s i , e i ), and the encoded utterances h utt ∈ R D and context h ctx ∈ R D from the utterance encoder f utt (u t , x t−1 ) and context encoder f ctx (b t−1 ) respectively, and D is the hidden dimension. </BACKGROUND> The prediction is then performed by aggregating the results of slot-value predictor f val (h utt , h ctx , (s i , e i )) for the complete N b slotvalue pairs. We optimize the state tracking model using the cross-entropy loss:L = d i t=1:n d − log(b t •p B (u t , x t−1 , c t−1 )) (4)where the parameters of p B , which include f utt , f ctx , and f val , are jointly trained.', 'where h v and h r are node embedding and relation embedding. <PERCEPTION> We define the compositional operation as ϕ(h u , h r ) = h u −h r inspired by </PERCEPTION> <INFORMATION> the TransE #TARGET_REF . </INFORMATION> The relation embedding is also updated via another linear transformation:', 'ArXiv open-sources the LaTeX version of their articles, when available. In order to make our Symlink dataset open-access to the whole community, we crawled the metadata of these articles and only selected articles under the CC BY license. Once obtained the LaTeX project, we extracted all the paragraphs from the .tex files. We filtered out all short paragraphs with less than 50 words and paragraphs without symbols. <BACKGROUND> Since a formula can be composed in multiple ways such as inline formulae (between $ $), displayed formulae (between $$ $$), or using commands </BACKGROUND> e.g. array, to keep the original TeX format of the formulae, all of these math objects are masked before tokenization. Then, <PERCEPTION> we used </PERCEPTION> <INFORMATION> the SciBERT tokenizer #TARGET_REF </INFORMATION> <PERCEPTION> to tokenize the text. The original math object is then restored. </PERCEPTION> As we observed that many papers have nested math objects, we deleted all the nested objects, hence, having non-nested LaTeX data. This is helpful as it makes the LaTeX documents more similar to the ones generated by the PDF-to-LaTeX tools, which do not contain nested objects.', '<PERCEPTION> Taking inspiration from </PERCEPTION> <INFORMATION> gradient checkpointing </INFORMATION> #TARGET_REF , <BACKGROUND> we observe that if the inputs Q C , K, V are available during the backward pass, we can re-compute o C and then use the produced intermediate activations to computed (Q C ) from d (o C ). Once d (Q C) is computed, we can again discard the intermediate activations and gradients produced during this step and move on to the next chunk. </BACKGROUND> This ensures that the peak memory usage during the backward pass through the attention layer is bounded by the memory required to backpropagate through a single chunk.', '• Joint: <PERCEPTION> we concatenate the training examples from Quoref and CoNLL-to-QA converted 11 The only difference of TASE in our experiments and the reported </PERCEPTION> <INFORMATION> results in #TARGET_REF </INFORMATION> <PERCEPTION> is the number of training epochs. </PERCEPTION> For a fair comparison, we train all models for the same number of iterations.', 'To answer these questions, we first establish in section 2 a state-of-the-art that is meant to be broad enough to have a shallow overview depicting the ins and outs and issues around the usability of Transformer-based models whose breadcrumb trail is the issue of resources. Then, we present in the section 3 the recent progress of the questionanswering task, through the use of these latest models. In sections 4 and 5 we introduce our model and present our experiments on the usability of Transformers models in a question-answering task for #REF and PIAF #REF corpora. We propose to address the instability relating to data scarcity by investigating various training strategies with data augmentation, hyperparameters optimization and cross-lingual transfer. Finally, <PERCEPTION> we present a new compact model for French based on </PERCEPTION> <INFORMATION> ALBERT #TARGET_REF </INFORMATION> <PERCEPTION> 1 , and compare it to existing monolingual and multilingual models, large and compact, under constrained conditions </PERCEPTION> <BACKGROUND> (notably on learning data). </BACKGROUND>', '<BACKGROUND> The caption generation backbone is a transformerbased encoder-decoder proposed by </BACKGROUND> <INFORMATION> #TARGET_REF , which mainly employs a multi-head attention mechanism and achieves top-tier performance in many sequential related tasks. </INFORMATION> Here, we highlight several task-oriented modifications.', '<PERCEPTION> The new lexico-structural transfer approach is more similar to the interlingua approach </PERCEPTION> in that they both have a predicate-argument structure representation of the meaning of the sentence, which gives them roughly equivalent semantic depth. <PERCEPTION> Another similarity is that </PERCEPTION> <INFORMATION> the new transfer approach can also combine lexical items from several languages together into a single transfer lexicon entry, greatly simplifying the task of adding the mapping to a new language #TARGET_REF . </INFORMATION> <PERCEPTION> An important remaining difference is that the interlingua approach would claim that a single predicate-argument structure can serve as a common representation for many languages, whereas the transfer approach allows for language-specific predicate-argument structures.. </PERCEPTION> <BACKGROUND> A fundamental assumption of either approach, and the most important similarity, is that these classifications can be made based on distinguished semantic features, and that these semantic features will be relevant to classification schemes in other languages. </BACKGROUND> Whether the classification schemes serve as a means of associating a single logical form composed of semantic primitives with many lexical items, as in the LCS approach, or as a means of enriching a set of logical forms with a collection of semantic features, the classifications still have to be determined, and the associations with semantic features have to be made. The rest of this paper discusses specific issues with respect to the association of semantic features with the classifications in English verbs.', '<INFORMATION> Both the encoder and decoder are six-layer Transformer #TARGET_REF </INFORMATION> , the number of heads of Multi-Head Attention is 8, the token embedding dimension is 512, and the ratio of Dropout is 0.1. Adam (Kingma and Ba, 2015) was used as the optimization method for parameters during training. The learning rate of Adam was set to 0.001. Hyperparameters λ, which adjust the frequency of ITF model and INF model, were set as 0.2, 0.4, 0.6, or 0.8. The INF model uses bi-gram as its n-gram function.']\n",
      "[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "[(' MIL-NCE #TARGET_REF has demonstrated the effectiveness for learning the semantic mapping between vision and language. ', 0), (' The goal is to learn semantic representation between two views by allowing the positive sample to be similar ', 1), (' and negatives to be dissimilar semantically simultaneously. ', 1), (' (in semantic space) ', 2), (' CLIP #REF and ', 2)]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[(' OPUS #TARGET_REF parallel movie subtitle corpus of subtitles collected from opensubtitles.org ', 0), (' We use the ', 1), (' as a multi-domain proxy. ', 1)]\n",
      "[2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[(' Semi-supervision is the problem of learning from labeled and unlabeled data #TARGET_REF , in which given a point set X = {x 1 , ..., x l , x l+1 , ..., x n } and a label set L = {1, ..., c}, the first l points have labels {y 1 , ..., y l } ∈ L ', 0), (' and the remaining points are unlabeled #REF . ', 2)]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "[(' #TARGET_REF symbol in BERT) ', 0), (' the separation of question encoding and passage encoding is desirable, so that the dense representations of all passages can be precomputed for efficient retrieval. ', 1), (' we adopt two independent neural networks initialized from pre-trained LMs for the two encoders E q (•) and E p (•) separately, and take the representations at the first token ', 1), (' as the output for encoding. ', 1)]\n",
      "[0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 1, 1, 1, 2, 2, 2, 2, 2]\n",
      "[0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 1, 1, 1, 2, 2, 2, 2, 2]\n",
      "[(' #TARGET_REF where the experiments showed that training these embeddings separately and using them as individually-trained embedding in the final model improves the performance. ', 0), (' The justification of this step can be found in Section ( 6) ', 2)]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[(' shortcut identification #TARGET_REF ', 0), (' given a task and a trained model, we first utilize interpretability methods, ', 1), (' to automatically extract tokens that the model deems as important for task label Objective Approach for ', 1), (' e.g., attention scores #REF and integrated gradient #REF ', 2)]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[(' In a Rapid Serial Visual Presentation (RSVP) paradigm, sentences were presented one word at a time, on average every ≈ 240 ms, in a white monospace font on a grey background approximately in the centre of the screen, at the optimal viewing position #TARGET_REF . ', 0), (' Each word subtended a horizontal angle 0.76 • to the left and 11.81 • to the right from the centre. Sentences were separated by 500 ms of a white central fixation cross (see Figure 1). ', 2)]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[(' End-to-end automatic speech recognition (E2E ASR), which transcribes speech using a single neural network (NN), ', 0), (' has recently gained traction #TARGET_REF . ', 1), (' Existing E2E ASR models generate audio transcripts by sequentially producing likely graphemes, or multi-graphemic units, from which lexical items of a language can be recovered. ', 2)]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[(' GPT models reflect occupational stereotypes in society #TARGET_REF , confirming the risk of representational harm, but not how this translates into allocational harms in applied settings. ', 0), (' We know from prior NLP research that ', 1), (' Gender-biased language may result in an unequal distribution of job applications if it dissuades gender-diverse candidates from applying #REF . ', 2)]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[(' Offensive language can include various categories such as threats, vilification, insults, calumniation, discrimination and swearing #TARGET_REF . ', 0), (' Detection of such language is necessary for ease of moderation of content on social media. ', 2), (' Finding which spans make a comment or document toxic in nature is crucial in explaining the reasons behind their toxicity. ', 2)]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[(' question generation #TARGET_REF , ', 0), (' Generating multiple valid outputs given a source sequence has a wide range of applications, such as machine translation #REF , paraphrase generation #REF , ', 2), (' dialogue system #REF , and story generation #REF . ', 2)]\n",
      "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[(' a slot-value predictor that estimates whether a slot-value pair should be included in the dialogue states #TARGET_REF . ', 0), (' To model this conditional distribution, a state tracking model p B (u t , x t−1 , b t−1 ) mainly employs an utterance en-coder, a context encoder to work with ', 1), (' Specifically, the predictor takes as input a slot-value pair (s i , e i ), and the encoded utterances h utt ∈ R D and context h ctx ∈ R D from the utterance encoder f utt (u t , x t−1 ) and context encoder f ctx (b t−1 ) respectively, and D is the hidden dimension. ', 2)]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[(' the TransE #TARGET_REF . ', 0), (' We define the compositional operation as ϕ(h u , h r ) = h u −h r inspired by ', 1)]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[(' the SciBERT tokenizer #TARGET_REF ', 0), (' we used ', 1), (' to tokenize the text. The original math object is then restored. ', 1), (' Since a formula can be composed in multiple ways such as inline formulae (between $ $), displayed formulae (between $$ $$), or using commands ', 2)]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[(' gradient checkpointing ', 0), (' Taking inspiration from ', 1), (' we observe that if the inputs Q C , K, V are available during the backward pass, we can re-compute o C and then use the produced intermediate activations to computed (Q C ) from d (o C ). Once d (Q C) is computed, we can again discard the intermediate activations and gradients produced during this step and move on to the next chunk. ', 2)]\n",
      "[2, 2, 2, 1, 1, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[2, 2, 2, 1, 1, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[(' results in #TARGET_REF ', 0), (' we concatenate the training examples from Quoref and CoNLL-to-QA converted 11 The only difference of TASE in our experiments and the reported ', 1), (' is the number of training epochs. ', 1)]\n",
      "[0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[(' ALBERT #TARGET_REF ', 0), (' we present a new compact model for French based on ', 1), (' 1 , and compare it to existing monolingual and multilingual models, large and compact, under constrained conditions ', 1), (' (notably on learning data). ', 2)]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3]\n",
      "[(' #TARGET_REF , which mainly employs a multi-head attention mechanism and achieves top-tier performance in many sequential related tasks. ', 0), (' The caption generation backbone is a transformerbased encoder-decoder proposed by ', 2)]\n",
      "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
      "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
      "[(' the new transfer approach can also combine lexical items from several languages together into a single transfer lexicon entry, greatly simplifying the task of adding the mapping to a new language #TARGET_REF . ', 0), (' The new lexico-structural transfer approach is more similar to the interlingua approach ', 1), (' Another similarity is that ', 1), (' An important remaining difference is that the interlingua approach would claim that a single predicate-argument structure can serve as a common representation for many languages, whereas the transfer approach allows for language-specific predicate-argument structures.. ', 1), (' A fundamental assumption of either approach, and the most important similarity, is that these classifications can be made based on distinguished semantic features, and that these semantic features will be relevant to classification schemes in other languages. ', 2)]\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[(' Both the encoder and decoder are six-layer Transformer #TARGET_REF ', 0)]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[({'context': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'text': ['There', 'is', 'a', 'person', 'sitting', 'on', 'a', 'horse.', 'he', 'is', 'holding', 'a', 'horse', 'thread', 'and', 'he', 'is', 'wearing', 'a', 'cap.', 'there', 'are', 'flags,', 'board', 'on', 'the', 'left', 'side.', 'we', 'can', 'see', 'in', 'the', 'background', 'sky,', 'trees.', 'Contrastive', 'Learning', 'Recently,', 'contrastive', 'learning', 'has', 'been', 'widely', 'studied', 'in', 'unsupervised', 'representation', 'learning', 'for', 'vision,', '#REF', ',', 'language', '#REF', ',', 'or', 'multi-modal', '#REF', '.', 'The', 'goal', 'is', 'to', 'learn', 'semantic', 'representation', 'between', 'two', 'views', 'by', 'allowing', 'the', 'positive', 'sample', 'to', 'be', 'similar', '(in', 'semantic', 'space)', 'and', 'negatives', 'to', 'be', 'dissimilar', 'semantically', 'simultaneously.', 'CLIP', '#REF', 'and', 'MIL-NCE', '#TARGET_REF', 'has', 'demonstrated', 'the', 'effectiveness', 'for', 'learning', 'the', 'semantic', 'mapping', 'between', 'vision', 'and', 'language.', 'Previous', 'attempts', 'mainly', 'exploit', 'the', 'InfoNCE', '#REF', 'objective', 'to', 'maximize', 'a', 'lower', 'bound', 'of', 'the', 'mutual', 'information.', 'This', 'paper', 'extends', 'the', 'multimodal', 'contrastive', 'learning', 'between', 'the', 'trace', 'in', 'the', 'image', 'and', 'captioning', 'sentence.', 'In', 'the', 'same', 'image,', 'they', 'correspond', 'to', 'each', 'other', 'semantically.', 'This', 'motivates', 'us', 'to', 'design', 'a', 'contrastive', 'loss', 'for', 'better', '2022', 'alignment', 'between', 'the', 'trace', 'and', 'language.']}, {'context': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}), ({'context': [2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'text': ['We', 'use', 'the', 'OPUS', '#TARGET_REF', 'parallel', 'movie', 'subtitle', 'corpus', 'of', 'subtitles', 'collected', 'from', 'opensubtitles.org', 'as', 'a', 'multi-domain', 'proxy.', 'As', 'the', 'movies', 'we', 'use', 'for', 'source', 'data', 'cover', 'several', 'different', 'genres', 'and,', 'although', 'scripted,', 'represents', 'real', 'human', 'language', 'used', 'in', 'a', 'multitude', 'of', 'situations', 'similar', 'to', 'many', 'social', 'media', 'platforms.']}, {'context': [2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}), ({'context': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3], 'text': ['In', 'this', 'paper,', 'we', 'developed', 'a', 'semi-supervised', 'strategy', 'to', 'detect', 'toxic', 'comments', 'in', 'the', 'Brazilian', 'Portuguese', 'language.', 'Semi-supervision', 'is', 'the', 'problem', 'of', 'learning', 'from', 'labeled', 'and', 'unlabeled', 'data', '#TARGET_REF', ',', 'in', 'which', 'given', 'a', 'point', 'set', 'X', '=', '{x', '1', ',', '...,', 'x', 'l', ',', 'x', 'l+1', ',', '...,', 'x', 'n', '}', 'and', 'a', 'label', 'set', 'L', '=', '{1,', '...,', 'c},', 'the', 'first', 'l', 'points', 'have', 'labels', '{y', '1', ',', '...,', 'y', 'l', '}', '∈', 'L', 'and', 'the', 'remaining', 'points', 'are', 'unlabeled', '#REF', '.']}, {'context': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3]}), ({'context': [0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 1, 1, 1, 2, 2, 2, 2, 2], 'text': ['In', 'practice,', 'the', 'separation', 'of', 'question', 'encoding', 'and', 'passage', 'encoding', 'is', 'desirable,', 'so', 'that', 'the', 'dense', 'representations', 'of', 'all', 'passages', 'can', 'be', 'precomputed', 'for', 'efficient', 'retrieval.', 'Here,', 'we', 'adopt', 'two', 'independent', 'neural', 'networks', 'initialized', 'from', 'pre-trained', 'LMs', 'for', 'the', 'two', 'encoders', 'E', 'q', '(•)', 'and', 'E', 'p', '(•)', 'separately,', 'and', 'take', 'the', 'representations', 'at', 'the', 'first', 'token', '(e.g.,', '#TARGET_REF', 'symbol', 'in', 'BERT)', 'as', 'the', 'output', 'for', 'encoding.']}, {'context': [0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 1, 1, 1, 2, 2, 2, 2, 2]}), ({'context': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'text': ['The', 'same', 'approach', 'is', 'applied', 'on', 'diacritic', 'model', 'instead', 'of', 'using', 'character', 'information', 'we', 'use', 'diacritics', 'and', 'instead', 'of', 'using', 'C-Bi-LSTM', 'we', 'use', 'D-Bi-LSTM.', 'We', 'extract', 'the', 'forward', 'and', 'backward', 'outputs', 'of', 'the', 'trained', 'D-Bi-LSTM', 'as', 'individually-trained', 'diacritic', 'embeddings.', 'It', 'is', 'worth', 'noting', 'that', 'both', 'of', 'these', 'models', 'are', 'trained', 'on', 'diacritized', 'version', 'of', 'the', 'datasets.', 'Also', 'it', 'is', 'important', 'to', 'mention', 'that', 'the', 'output', 'from', 'this', 'step', 'are', 'weights', 'to', 'initialize', 'the', 'character', 'and', 'diacritic', 'embedding', 'layers', 'in', 'the', 'combination', 'model', 'and', 'both', 'of', 'these', 'sets', 'of', 'weights', 'have', 'been', 'trained', 'individually', 'in', 'separate', 'models.', 'The', 'justification', 'of', 'this', 'step', 'can', 'be', 'found', 'in', 'Section', '(', '6)', '#TARGET_REF', 'where', 'the', 'experiments', 'showed', 'that', 'training', 'these', 'embeddings', 'separately', 'and', 'using', 'them', 'as', 'individually-trained', 'embedding', 'in', 'the', 'final', 'model', 'improves', 'the', 'performance.']}, {'context': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}), ({'context': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'text': ['In', 'this', 'work,', 'we', 'introduce', 'a', 'framework', 'to', 'automatically', 'identify', 'spurious', 'correlations', 'exploited', 'by', 'the', 'model,', 'sometimes', 'also', 'denoted', 'as', '\"shortcuts\"', 'in', 'prior', 'work', '#REF', '1', ',', 'at', 'a', 'large', 'scale.', 'Our', 'proposed', 'framework', 'differs', 'from', 'existing', 'literature', 'with', 'a', 'focus', 'more', 'on', 'automatic', 'shortcut', 'identification,', 'instead', 'of', 'pre-defining', 'a', 'limited', 'set', 'of', 'shortcuts', 'or', 'learning', 'from', 'human', 'annotations', '(Table', '1).', 'Our', 'framework', 'works', 'as', 'follows:', 'given', 'a', 'task', 'and', 'a', 'trained', 'model,', 'we', 'first', 'utilize', 'interpretability', 'methods,', 'e.g.,', 'attention', 'scores', '#REF', 'and', 'integrated', 'gradient', '#REF', 'which', 'are', 'commonly', 'used', 'for', 'interpreting', \"model's\", 'decisions,', 'to', 'automatically', 'extract', 'tokens', 'that', 'the', 'model', 'deems', 'as', 'important', 'for', 'task', 'label', 'Objective', 'Approach', 'for', 'shortcut', 'identification', '#TARGET_REF', 'Robustness', 'against', 'known', 'shortcuts', 'Pre-defined', '#REF', 'Robustness', 'against', 'known', 'shortcuts', 'Pre-defined', '#REF', 'Robustness', 'against', 'unknown', 'shortcuts', 'A', 'low-capacity', 'model', 'to', 'specifically', 'learn', 'shortcuts', '#REF', 'prediction.', 'We', 'then', 'introduce', 'two', 'extra', 'steps', 'to', 'further', 'categorize', 'the', 'extracted', 'tokens', 'to', 'be', '\"genuine\"', 'or', '\"spurious\".', 'We', 'utilize', 'a', 'cross-dataset', 'analysis', 'to', 'identify', 'tokens', 'that', 'are', 'more', 'likely', 'to', 'be', '\"shortcut\".']}, {'context': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}), ({'context': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'text': ['In', 'a', 'Rapid', 'Serial', 'Visual', 'Presentation', '(RSVP)', 'paradigm,', 'sentences', 'were', 'presented', 'one', 'word', 'at', 'a', 'time,', 'on', 'average', 'every', '≈', '240', 'ms,', 'in', 'a', 'white', 'monospace', 'font', 'on', 'a', 'grey', 'background', 'approximately', 'in', 'the', 'centre', 'of', 'the', 'screen,', 'at', 'the', 'optimal', 'viewing', 'position', '#TARGET_REF', '.', 'Each', 'word', 'subtended', 'a', 'horizontal', 'angle', '0.76', '•', 'to', 'the', 'left', 'and', '11.81', '•', 'to', 'the', 'right', 'from', 'the', 'centre.', 'Sentences', 'were', 'separated', 'by', '500', 'ms', 'of', 'a', 'white', 'central', 'fixation', 'cross', '(see', 'Figure', '1).', 'On', 'approximately', '20%', 'of', 'the', 'sentences', 'in', 'each', 'session,', 'the', 'participant', 'was', 'prompted', 'to', 'verbalise', 'the', 'previous', 'sentence', 'back', 'to', 'the', 'experimenter.', 'An', 'accuracy', 'score', 'of', '93%', 'across', 'all', 'sessions', 'confirmed', 'that', 'the', 'participant', 'successfully', 'attended', 'the', 'sentences.', 'Stimuli', 'were', 'presented', 'using', 'PsychoPy', '#REF', 'on', 'an', 'LCD', 'monitor', 'with', 'a', 'resolution', 'of', '1920x1080', 'pixels', 'and', '60', 'Hz', 'refresh', 'rate.', 'The', \"subject's\", 'head', 'was', 'stabilised', 'with', 'a', 'chin-rest.', 'Gilching,', 'Germany).', 'Channel', 'impedances', 'were', 'kept', 'below', '15', 'kΩ.', 'Data', 'were', 'preprocessed', 'using', 'MNE-Python', '#REF', '.', 'Individual', 'EEG', 'sessions', 'were', 'band-pass', 'filtered', 'between', '1-40', 'Hz,', 'downsampled', 'to', '250', 'Hz', 'and', 're-referenced', 'to', 'average', 'reference.', 'Noisy', 'channels', 'were', 'determined', 'based', 'on', 'visual', 'inspection', 'and', 'interpolated.', 'Non-neuronal', 'components', '(e.g.', 'ocular,', 'muscular,', 'electrical)', 'were', 'removed', 'via', 'Independent', 'Component', 'Analysis', '(ICA)', 'individually', 'for', 'each', 'recording', 'session', '(an', 'average', 'of', '4', 'components', 'were', 'removed', 'per', 'EEG', 'session).']}, {'context': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}), ({'context': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'text': ['End-to-end', 'automatic', 'speech', 'recognition', '(E2E', 'ASR),', 'which', 'transcribes', 'speech', 'using', 'a', 'single', 'neural', 'network', '(NN),', 'has', 'recently', 'gained', 'traction', '#TARGET_REF', '.', 'Existing', 'E2E', 'ASR', 'models', 'generate', 'audio', 'transcripts', 'by', 'sequentially', 'producing', 'likely', 'graphemes,', 'or', 'multi-graphemic', 'units,', 'from', 'which', 'lexical', 'items', 'of', 'a', 'language', 'can', 'be', 'recovered.', 'However,', 'other', 'linguistic', 'annotations', 'such', 'as', 'phonemic', 'transcripts,', 'part-of-speech', '(POS)', 'tags,', 'or', 'word', 'boundaries,', 'help', 'understand', 'the', 'underlying', 'audio', 'characteristics', '#REF', '.', 'Such', 'linguistic', 'annotations', 'are', 'especially', 'important', 'in', 'natural', 'language', 'processing', '(NLP)', 'tasks', 'done', 'on', 'audio', '!!', '!', '\"', '!\"!', '#', '\"', '#\"!', '#', '$', '#', '\"', '$', '!', '\"!', '\"', '!\"#', '#', '\"', '#\"#', '#', '$', '#', '\"', '$', '\"', '\"#', '\"', '#', '\"', '#', '\"', '#', '\"', '#', '$', '#', '\"', '#', '\"']}, {'context': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}), ({'context': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'text': ['In', 'this', 'paper,', 'we', 'focus', 'on', 'measuring', 'and', 'mitigating', 'gender-biased', 'language', 'in', 'machine-generated', 'job', 'ads,', 'a', 'use', 'case', 'of', 'large-scale', 'language', 'models', 'which', 'risks', 'representational', 'and', 'allocational', 'harms', '#REF', '.', 'Representational', 'harms', 'come', 'from', 'the', 'conditioning', 'of', 'a', \"job's\", 'suitability', 'to', 'a', 'given', 'individual', 'based', 'on', 'their', 'gender.', 'When', 'jobs', 'are', 'valued', 'unequally', '(either', 'by', 'financial,', 'social', 'or', 'intellectual', 'status),', 'this,', 'in', 'turn,', 'can', 'reinforce', 'gendered', 'power', 'hierarchies', 'and', 'negative', 'societal', 'divisions.', 'Gender-biased', 'language', 'may', 'result', 'in', 'an', 'unequal', 'distribution', 'of', 'job', 'applications', 'if', 'it', 'dissuades', 'gender-diverse', 'candidates', 'from', 'applying', '#REF', '.', 'Thus,', 'allocational', 'harms', 'are', 'relevant', 'where', 'labour', 'market', 'opportunities,', 'financial', 'remuneration', 'or', 'job', 'stability', 'are', 'preferentially', 'granted', 'based', 'on', 'gender.', 'We', 'know', 'from', 'prior', 'NLP', 'research', 'that', 'GPT', 'models', 'reflect', 'occupational', 'stereotypes', 'in', 'society', '#TARGET_REF', ',', 'confirming', 'the', 'risk', 'of', 'representational', 'harm,', 'but', 'not', 'how', 'this', 'translates', 'into', 'allocational', 'harms', 'in', 'applied', 'settings.', 'To', 'measure', 'bias,', 'our', 'experiment', 'employs', 'lists', 'of', 'gender-coded', 'words.', 'These', 'lists', 'are', 'potentially', 'in', 'themselves', 'biased,', 'having', 'been', 'defined', 'by', 'a', 'research', 'group', 'under', 'a', 'particular', 'cultural', 'bias', 'or', 'as', 'the', 'result', 'of', 'biased', 'data.', 'To', 'mitigate', 'this', 'concern,', 'we', 'use', 'multiple', 'measures', 'to', 'cover', 'the', 'blind', 'spots', 'or', 'specific', 'biases', 'present', 'in', 'any', 'single', 'list.', 'However,', 'our', 'proposed', 'metric', 'may', 'better', 'capture', 'the', 'most', 'obvious,', 'text-level', 'aspects', 'of', 'gender-biased', 'language', 'and', 'will', 'be', 'less', 'effective', 'to', 'find', 'covert,', 'but', 'equally', 'as', 'damaging,', 'forms', 'of', 'gender', 'bias', 'in', 'job', 'ads,', 'or', 'job', 'search', 'more', 'broadly.']}, {'context': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}), ({'context': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'text': ['Offensive', 'language', 'can', 'include', 'various', 'categories', 'such', 'as', 'threats,', 'vilification,', 'insults,', 'calumniation,', 'discrimination', 'and', 'swearing', '#TARGET_REF', '.', 'Detection', 'of', 'such', 'language', 'is', 'necessary', 'for', 'ease', 'of', 'moderation', 'of', 'content', 'on', 'social', 'media.', 'Despite', 'their', 'popularity,', 'toxicity', 'detection', 'tasks', 'have', 'focused', 'majorly', 'on', 'sequence', 'classification,', 'rather', 'than', 'sequence', 'tagging.', 'Finding', 'which', 'spans', 'make', 'a', 'comment', 'or', 'document', 'toxic', 'in', 'nature', 'is', 'crucial', 'in', 'explaining', 'the', 'reasons', 'behind', 'their', 'toxicity.', 'Additionally,', 'such', 'attributions', 'would', 'allow', 'for', 'more', 'efficient', 'semi-automated', 'quality-based', 'moderation', 'of', 'content,', 'especially', 'for', 'verbose', 'documents,', 'in', 'comparison', 'to', 'quantitative', 'toxicity', 'scores.']}, {'context': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}), ({'context': [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'text': ['Generating', 'multiple', 'valid', 'outputs', 'given', 'a', 'source', 'sequence', 'has', 'a', 'wide', 'range', 'of', 'applications,', 'such', 'as', 'machine', 'translation', '#REF', ',', 'paraphrase', 'generation', '#REF', ',', 'question', 'generation', '#TARGET_REF', ',', 'dialogue', 'system', '#REF', ',', 'and', 'story', 'generation', '#REF', '.', 'For', 'example,', 'in', 'machine', 'translation,', 'there', 'are', 'often', 'many', 'plausible', 'and', 'semantically', 'equivalent', 'translations', 'due', 'to', 'information', 'asymmetry', 'between', 'different', 'languages', '#REF', '.']}, {'context': [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}), ({'context': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'text': ['t', ',', 'x', 't', ')|1', '≤', 't', '≤', 'n', 'd', '},', 'let', '{b', 't', '|1', '≤', 't', '≤', 'n', 'd', '}be', 'the', 'dialogue', 'state', 'for', 'each', 'turn,', 'where', 'b', 't', '∈', '{0,', '1}', 'N', 'b', 'and', 'N', 'b', 'is', 'the', 'number', 'of', 'all', 'slot-value', 'pairs.', 'Dialogue', 'state', 'tracking', 'is', 'usually', 'formulated', 'as', 'a', 'multilabel', 'learning', 'problem', 'where', 'the', 'state', 'at', 'turn', 't', 'predicted', 'by', 'modeling', 'the', 'conditional', 'distribution', 'p(bt', '|c', 't', ')', '=', 'p(b', 't', '|u', 't', ',', 'x', 't−1', ',', 'b', 't−1', '),', 'where', 'b', 't−1', 'is', 'the', 'dialogue', 'state', 'in', 'the', 'previous', 'turn.', 'To', 'model', 'this', 'conditional', 'distribution,', 'a', 'state', 'tracking', 'model', 'p', 'B', '(u', 't', ',', 'x', 't−1', ',', 'b', 't−1', ')', 'mainly', 'employs', 'an', 'utterance', 'en-coder,', 'a', 'context', 'encoder', 'to', 'work', 'with', 'a', 'slot-value', 'predictor', 'that', 'estimates', 'whether', 'a', 'slot-value', 'pair', 'should', 'be', 'included', 'in', 'the', 'dialogue', 'states', '#TARGET_REF', '.', 'Specifically,', 'the', 'predictor', 'takes', 'as', 'input', 'a', 'slot-value', 'pair', '(s', 'i', ',', 'e', 'i', '),', 'and', 'the', 'encoded', 'utterances', 'h', 'utt', '∈', 'R', 'D', 'and', 'context', 'h', 'ctx', '∈', 'R', 'D', 'from', 'the', 'utterance', 'encoder', 'f', 'utt', '(u', 't', ',', 'x', 't−1', ')', 'and', 'context', 'encoder', 'f', 'ctx', '(b', 't−1', ')', 'respectively,', 'and', 'D', 'is', 'the', 'hidden', 'dimension.', 'The', 'prediction', 'is', 'then', 'performed', 'by', 'aggregating', 'the', 'results', 'of', 'slot-value', 'predictor', 'f', 'val', '(h', 'utt', ',', 'h', 'ctx', ',', '(s', 'i', ',', 'e', 'i', '))', 'for', 'the', 'complete', 'N', 'b', 'slotvalue', 'pairs.', 'We', 'optimize', 'the', 'state', 'tracking', 'model', 'using', 'the', 'cross-entropy', 'loss:L', '=', 'd', 'i', 't=1:n', 'd', '−', 'log(b', 't', '•p', 'B', '(u', 't', ',', 'x', 't−1', ',', 'c', 't−1', '))', '(4)where', 'the', 'parameters', 'of', 'p', 'B', ',', 'which', 'include', 'f', 'utt', ',', 'f', 'ctx', ',', 'and', 'f', 'val', ',', 'are', 'jointly', 'trained.']}, {'context': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}), ({'context': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'text': ['where', 'h', 'v', 'and', 'h', 'r', 'are', 'node', 'embedding', 'and', 'relation', 'embedding.', 'We', 'define', 'the', 'compositional', 'operation', 'as', 'ϕ(h', 'u', ',', 'h', 'r', ')', '=', 'h', 'u', '−h', 'r', 'inspired', 'by', 'the', 'TransE', '#TARGET_REF', '.', 'The', 'relation', 'embedding', 'is', 'also', 'updated', 'via', 'another', 'linear', 'transformation:']}, {'context': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}), ({'context': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'text': ['ArXiv', 'open-sources', 'the', 'LaTeX', 'version', 'of', 'their', 'articles,', 'when', 'available.', 'In', 'order', 'to', 'make', 'our', 'Symlink', 'dataset', 'open-access', 'to', 'the', 'whole', 'community,', 'we', 'crawled', 'the', 'metadata', 'of', 'these', 'articles', 'and', 'only', 'selected', 'articles', 'under', 'the', 'CC', 'BY', 'license.', 'Once', 'obtained', 'the', 'LaTeX', 'project,', 'we', 'extracted', 'all', 'the', 'paragraphs', 'from', 'the', '.tex', 'files.', 'We', 'filtered', 'out', 'all', 'short', 'paragraphs', 'with', 'less', 'than', '50', 'words', 'and', 'paragraphs', 'without', 'symbols.', 'Since', 'a', 'formula', 'can', 'be', 'composed', 'in', 'multiple', 'ways', 'such', 'as', 'inline', 'formulae', '(between', '$', '$),', 'displayed', 'formulae', '(between', '$$', '$$),', 'or', 'using', 'commands', 'e.g.', 'array,', 'to', 'keep', 'the', 'original', 'TeX', 'format', 'of', 'the', 'formulae,', 'all', 'of', 'these', 'math', 'objects', 'are', 'masked', 'before', 'tokenization.', 'Then,', 'we', 'used', 'the', 'SciBERT', 'tokenizer', '#TARGET_REF', 'to', 'tokenize', 'the', 'text.', 'The', 'original', 'math', 'object', 'is', 'then', 'restored.', 'As', 'we', 'observed', 'that', 'many', 'papers', 'have', 'nested', 'math', 'objects,', 'we', 'deleted', 'all', 'the', 'nested', 'objects,', 'hence,', 'having', 'non-nested', 'LaTeX', 'data.', 'This', 'is', 'helpful', 'as', 'it', 'makes', 'the', 'LaTeX', 'documents', 'more', 'similar', 'to', 'the', 'ones', 'generated', 'by', 'the', 'PDF-to-LaTeX', 'tools,', 'which', 'do', 'not', 'contain', 'nested', 'objects.']}, {'context': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}), ({'context': [2, 2, 2, 1, 1, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'text': ['Taking', 'inspiration', 'from', 'gradient', 'checkpointing', '#TARGET_REF', ',', 'we', 'observe', 'that', 'if', 'the', 'inputs', 'Q', 'C', ',', 'K,', 'V', 'are', 'available', 'during', 'the', 'backward', 'pass,', 'we', 'can', 're-compute', 'o', 'C', 'and', 'then', 'use', 'the', 'produced', 'intermediate', 'activations', 'to', 'computed', '(Q', 'C', ')', 'from', 'd', '(o', 'C', ').', 'Once', 'd', '(Q', 'C)', 'is', 'computed,', 'we', 'can', 'again', 'discard', 'the', 'intermediate', 'activations', 'and', 'gradients', 'produced', 'during', 'this', 'step', 'and', 'move', 'on', 'to', 'the', 'next', 'chunk.', 'This', 'ensures', 'that', 'the', 'peak', 'memory', 'usage', 'during', 'the', 'backward', 'pass', 'through', 'the', 'attention', 'layer', 'is', 'bounded', 'by', 'the', 'memory', 'required', 'to', 'backpropagate', 'through', 'a', 'single', 'chunk.']}, {'context': [2, 2, 2, 1, 1, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}), ({'context': [0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'text': ['•', 'Joint:', 'we', 'concatenate', 'the', 'training', 'examples', 'from', 'Quoref', 'and', 'CoNLL-to-QA', 'converted', '11', 'The', 'only', 'difference', 'of', 'TASE', 'in', 'our', 'experiments', 'and', 'the', 'reported', 'results', 'in', '#TARGET_REF', 'is', 'the', 'number', 'of', 'training', 'epochs.', 'For', 'a', 'fair', 'comparison,', 'we', 'train', 'all', 'models', 'for', 'the', 'same', 'number', 'of', 'iterations.']}, {'context': [0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}), ({'context': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3], 'text': ['To', 'answer', 'these', 'questions,', 'we', 'first', 'establish', 'in', 'section', '2', 'a', 'state-of-the-art', 'that', 'is', 'meant', 'to', 'be', 'broad', 'enough', 'to', 'have', 'a', 'shallow', 'overview', 'depicting', 'the', 'ins', 'and', 'outs', 'and', 'issues', 'around', 'the', 'usability', 'of', 'Transformer-based', 'models', 'whose', 'breadcrumb', 'trail', 'is', 'the', 'issue', 'of', 'resources.', 'Then,', 'we', 'present', 'in', 'the', 'section', '3', 'the', 'recent', 'progress', 'of', 'the', 'questionanswering', 'task,', 'through', 'the', 'use', 'of', 'these', 'latest', 'models.', 'In', 'sections', '4', 'and', '5', 'we', 'introduce', 'our', 'model', 'and', 'present', 'our', 'experiments', 'on', 'the', 'usability', 'of', 'Transformers', 'models', 'in', 'a', 'question-answering', 'task', 'for', '#REF', 'and', 'PIAF', '#REF', 'corpora.', 'We', 'propose', 'to', 'address', 'the', 'instability', 'relating', 'to', 'data', 'scarcity', 'by', 'investigating', 'various', 'training', 'strategies', 'with', 'data', 'augmentation,', 'hyperparameters', 'optimization', 'and', 'cross-lingual', 'transfer.', 'Finally,', 'we', 'present', 'a', 'new', 'compact', 'model', 'for', 'French', 'based', 'on', 'ALBERT', '#TARGET_REF', '1', ',', 'and', 'compare', 'it', 'to', 'existing', 'monolingual', 'and', 'multilingual', 'models,', 'large', 'and', 'compact,', 'under', 'constrained', 'conditions', '(notably', 'on', 'learning', 'data).']}, {'context': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3]}), ({'context': [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'text': ['The', 'caption', 'generation', 'backbone', 'is', 'a', 'transformerbased', 'encoder-decoder', 'proposed', 'by', '#TARGET_REF', ',', 'which', 'mainly', 'employs', 'a', 'multi-head', 'attention', 'mechanism', 'and', 'achieves', 'top-tier', 'performance', 'in', 'many', 'sequential', 'related', 'tasks.', 'Here,', 'we', 'highlight', 'several', 'task-oriented', 'modifications.']}, {'context': [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]}), ({'context': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'text': ['The', 'new', 'lexico-structural', 'transfer', 'approach', 'is', 'more', 'similar', 'to', 'the', 'interlingua', 'approach', 'in', 'that', 'they', 'both', 'have', 'a', 'predicate-argument', 'structure', 'representation', 'of', 'the', 'meaning', 'of', 'the', 'sentence,', 'which', 'gives', 'them', 'roughly', 'equivalent', 'semantic', 'depth.', 'Another', 'similarity', 'is', 'that', 'the', 'new', 'transfer', 'approach', 'can', 'also', 'combine', 'lexical', 'items', 'from', 'several', 'languages', 'together', 'into', 'a', 'single', 'transfer', 'lexicon', 'entry,', 'greatly', 'simplifying', 'the', 'task', 'of', 'adding', 'the', 'mapping', 'to', 'a', 'new', 'language', '#TARGET_REF', '.', 'An', 'important', 'remaining', 'difference', 'is', 'that', 'the', 'interlingua', 'approach', 'would', 'claim', 'that', 'a', 'single', 'predicate-argument', 'structure', 'can', 'serve', 'as', 'a', 'common', 'representation', 'for', 'many', 'languages,', 'whereas', 'the', 'transfer', 'approach', 'allows', 'for', 'language-specific', 'predicate-argument', 'structures..', 'A', 'fundamental', 'assumption', 'of', 'either', 'approach,', 'and', 'the', 'most', 'important', 'similarity,', 'is', 'that', 'these', 'classifications', 'can', 'be', 'made', 'based', 'on', 'distinguished', 'semantic', 'features,', 'and', 'that', 'these', 'semantic', 'features', 'will', 'be', 'relevant', 'to', 'classification', 'schemes', 'in', 'other', 'languages.', 'Whether', 'the', 'classification', 'schemes', 'serve', 'as', 'a', 'means', 'of', 'associating', 'a', 'single', 'logical', 'form', 'composed', 'of', 'semantic', 'primitives', 'with', 'many', 'lexical', 'items,', 'as', 'in', 'the', 'LCS', 'approach,', 'or', 'as', 'a', 'means', 'of', 'enriching', 'a', 'set', 'of', 'logical', 'forms', 'with', 'a', 'collection', 'of', 'semantic', 'features,', 'the', 'classifications', 'still', 'have', 'to', 'be', 'determined,', 'and', 'the', 'associations', 'with', 'semantic', 'features', 'have', 'to', 'be', 'made.', 'The', 'rest', 'of', 'this', 'paper', 'discusses', 'specific', 'issues', 'with', 'respect', 'to', 'the', 'association', 'of', 'semantic', 'features', 'with', 'the', 'classifications', 'in', 'English', 'verbs.']}, {'context': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}), ({'context': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'text': ['Both', 'the', 'encoder', 'and', 'decoder', 'are', 'six-layer', 'Transformer', '#TARGET_REF', ',', 'the', 'number', 'of', 'heads', 'of', 'Multi-Head', 'Attention', 'is', '8,', 'the', 'token', 'embedding', 'dimension', 'is', '512,', 'and', 'the', 'ratio', 'of', 'Dropout', 'is', '0.1.', 'Adam', '(Kingma', 'and', 'Ba,', '2015)', 'was', 'used', 'as', 'the', 'optimization', 'method', 'for', 'parameters', 'during', 'training.', 'The', 'learning', 'rate', 'of', 'Adam', 'was', 'set', 'to', '0.001.', 'Hyperparameters', 'λ,', 'which', 'adjust', 'the', 'frequency', 'of', 'ITF', 'model', 'and', 'INF', 'model,', 'were', 'set', 'as', '0.2,', '0.4,', '0.6,', 'or', '0.8.', 'The', 'INF', 'model', 'uses', 'bi-gram', 'as', 'its', 'n-gram', 'function.']}, {'context': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]})]\n",
      "{'struc_acc': 1, 'micro': 1.0, 'macro': 1.0, 'total_f1': 1.0, 'inf_f1': 1.0, 'perc_f1': 1.0, 'back_f1': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(evaluate)\n",
    "\n",
    "evaluator = evaluate.Evaluator(trainer.model, tokenizer, eval_ds, DATA, SCHEMA, DEVICE)\n",
    "evaluator.evaluate(test_data=eval_ds['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TAG'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.match(r'<([^\\/]+?)>', '<TAG>').group(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lasse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
