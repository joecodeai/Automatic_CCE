{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/explorer/anaconda3/envs/lasse/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "You are given a list of citation function classes and an excerpt from a scientific text in which citations are marked #REF and #TARGET_REF. Assign the correct function classes to the #TARGET_REF. To do this, first, locate #TARGET_REF, second, select the best fitting function class by considering the target citation context, third, reflect on whether you made the correct choice and respond when you are sure it is correct. Reply by adding a self closing XML tag containing the right function class directly after the #TARGET_REF (e.g: #TARGET_REF<USE/>) \n",
      "\n",
      "List Citation Function Classes\n",
      "\n",
      "                BACK: Citation provides background Information. \n",
      "\n",
      "                USE: Citation is used in citing paper. \n",
      "\n",
      "                COMP: Cited paper is judged as similar, different, or is agreed or disagreed with. \n",
      "\n",
      "                MOT: Citing paper is motivated by citation. \n",
      "\n",
      "                EXT: Citing paper extends citation.\n",
      "\n",
      "                FUT: Citation mentions possible future work. \n",
      "\n",
      "                 Are the instructions clear to you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, the instructions are clear to me. I will determine the right function class of the targeted citation (#TARGET_REF) based on the provided citation function classes and respond in XML fromat.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "In a similar vain to #TARGET_REF and #REF, the method extends an existing flat shallow-parsing method to handle composite structures.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "In a similar vain to #TARGET_REF<COMP/> and #REF, the method extends an existing flat shallow-parsing method to handle composite structures.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import importlib\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, Trainer, TrainingArguments, TrainerCallback, EarlyStoppingCallback\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import Dataset\n",
    "import json\n",
    "\n",
    "import prompts\n",
    "import evaluate\n",
    "\n",
    "MODEL = 'llama'\n",
    "DATA = 'acl_arc' #acl_arc, finecite, multicite\n",
    "SCHEMA = 'XML' # XML, JSON\n",
    "DATA_SIZE = None # 1000\n",
    "\n",
    "\n",
    "INPUT = f'./data/{DATA}/{SCHEMA}/'\n",
    "OUTPUT = f'./output/{DATA}/{MODEL}/{SCHEMA}/'\n",
    "if DATA_SIZE:\n",
    "    INPUT = f'./data/{DATA}/{DATA_SIZE}/{SCHEMA}/'\n",
    "    OUTPUT = f'./output/{DATA}/{MODEL}/{DATA_SIZE}/{SCHEMA}/'\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_mapping = {\n",
    "    \"mistral\":'mistralai/Mistral-7B-Instruct-v0.3',\n",
    "    \"scitulu\":'allenai/scitulu-7b',\n",
    "    'llama':'meta-llama/Meta-Llama-3.1-8B-Instruct',\n",
    "}\n",
    "model_id = model_mapping[MODEL]\n",
    "\n",
    "max_seq_length = 1024\n",
    "\n",
    "#tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "prompt = prompts.PromptForAutoCCA(tokenizer, MODEL, DATA, SCHEMA)\n",
    "\n",
    "print(tokenizer.decode(prompt.create_sample('', '',)['input_ids']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lasse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
