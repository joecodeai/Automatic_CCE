{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "from random import randint\n",
    "import json\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#length limitations\n",
    "max_input_len = 300\n",
    "data_size = None#1000\n",
    "\n",
    "INPUT = '../data/multicite/'\n",
    "OUTPUT = f'../data/multicite/'\n",
    "if data_size: OUTPUT = f'../data/multicite/{data_size}/'\n",
    "\n",
    "# as ther is not such a hughe difference between the token count we use mistrals tokenizer for the evaluation\n",
    "tokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "with open('../data/multicite/full_raw.json', 'r') as file:\n",
    "    full_raw = json.load(file)\n",
    "\n",
    "#split data into train / test\n",
    "train_data_keys, test_data_keys = train_test_split(list(full_raw.keys()), test_size=0.2, random_state=82)\n",
    "train_data, test_data = map(lambda keys: {x: full_raw[x] for x in keys}, [train_data_keys, test_data_keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper\n",
    "def label_mapping(label):\n",
    "    if label == '@BACK@': return 'BACKGROUND'\n",
    "    if label == '@MOT@': return 'MOTIVATION'\n",
    "    if label == '@USE@': return 'USE'\n",
    "    if label == '@EXT@': return 'EXTENDS'\n",
    "    if label == '@SIM@': return 'SIMILARITY'\n",
    "    if label == '@DIF@': return 'DIFFERENCE'\n",
    "    if label == '@FUT@': return 'FUTURE'\n",
    "    if label =='BACKGROUND': return 0\n",
    "    if label =='MOTIVATION': return 1\n",
    "    if label =='USE': return 2\n",
    "    if label =='EXTENDS': return 3\n",
    "    if label =='SIMILARITY': return 4\n",
    "    if label =='DIFFERENCE': return 5\n",
    "    if label =='FUTURE': return 6\n",
    "    print(label)\n",
    "\n",
    "def seperate_segments(text_json):\n",
    "    # create sent lookup\n",
    "    sent_lookup = {sent_entry['sent_id']:sent_entry['text'] for sent_entry in text_json}\n",
    "\n",
    "    # sort sentences keys:\n",
    "    sorted_keys = sorted(list(sent_lookup.keys()), key=lambda x: int(x.split('-')[-1]))\n",
    "    \n",
    "    #seperate segments\n",
    "    key_sec_lookup, sec_arr = {}, [[]]\n",
    "    current_sec = 0\n",
    "    sec_idx = 0\n",
    "    for key in sorted_keys:\n",
    "        text = sent_lookup[key]\n",
    "        if re.match(r'----------------------------------', text):\n",
    "            current_sec += 1\n",
    "            sec_idx = 0\n",
    "            sec_arr.append([])\n",
    "            key_sec_lookup[key] = -1\n",
    "        else:\n",
    "            sec_arr[current_sec].append(text)\n",
    "            key_sec_lookup[key]= (current_sec, sec_idx)\n",
    "            sec_idx += 1\n",
    "    return key_sec_lookup, sec_arr\n",
    "\n",
    "def get_cit_dict(label_json):\n",
    "    res_dict = {}\n",
    "    for label, context_data in label_json.items():\n",
    "        if label == '@UNSURE@': continue\n",
    "        if len(context_data['gold_contexts']) != len(context_data['cite_sentences']):continue\n",
    "        for sent_id, context in zip(context_data['cite_sentences'], context_data['gold_contexts']):\n",
    "            if sent_id in res_dict:\n",
    "                res_dict[sent_id]['label'].append(label)\n",
    "                res_dict[sent_id]['context'] = list(set(res_dict[sent_id]['context'] + context))\n",
    "            else:\n",
    "                res_dict[sent_id] = {\n",
    "                    'label': [label],\n",
    "                    'context': context\n",
    "                }\n",
    "    return res_dict\n",
    "\n",
    "def replace_citations(text):\n",
    "    #replace and fix #AUTHOR_TAG\n",
    "    text = re.sub(r'#AUTHOR_TAG', '#TARGET_REF', text)\n",
    "    rext = re.sub(r'(?:van |)(?:[A-Z][a-z]+-?)+ ?(?:and [A-Za-z-]{2,}|et al?t?\\.)?(?: |,)*?#TARGET_REF', '#TARGET_REF', text) #fix wrong parsing\n",
    "    \n",
    "    #replace all other citatons with #REF\n",
    "    text = re.sub(r'(?:van |)(?:[A-Z][a-z]+-?)+ ?(?:and [A-Za-z-]{2,}|et al?t?\\.)?(?: |,)*?(?:\\d{4}|9\\d|\\[ ?\\d{1,2} ?\\]|\\( ?\\d{4} ?\\))', '#REF', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def preprocess_data(data, max_len):\n",
    "    res_df = pd.DataFrame(columns=['text', 'label', 'context'])\n",
    "    for big_key in data.keys():\n",
    "        sent_lookup, sent_arr = seperate_segments(data[big_key]['x'])\n",
    "        cit_dict = get_cit_dict(data[big_key]['y'])\n",
    "\n",
    "        for key in cit_dict.keys():\n",
    "            \n",
    "            #check if key in lookup\n",
    "            if key not in sent_lookup.keys(): continue\n",
    "            \n",
    "            #set basic data\n",
    "            labels, context = cit_dict[key].values()\n",
    "            sec_id, citing_sent_id = sent_lookup[key]\n",
    "            heading = sent_arr[sec_id][0]\n",
    "\n",
    "            #add all context sentences and enclosed sentences to input_arr\n",
    "            sorted_context = sorted(context, key=lambda x: int(x.split('-')[-1]))\n",
    "            context_sent_ids = [sent_lookup[context_sent][1] if sent_lookup[context_sent]!= -1 else -1 for context_sent in sorted_context]\n",
    "            if -1 in context_sent_ids: continue\n",
    "            input_arr = list(range(context_sent_ids[0], context_sent_ids[-1] +1))\n",
    "            #chekc if input_arr > 5 -> continue\n",
    "            if len(input_arr) > 5 or len(input_arr) ==0 or len(sent_arr[sec_id]) < 3: continue\n",
    "\n",
    "            # randomly add sentences to front / back until 5 sentences or all sentences are added\n",
    "            while len(input_arr) < 5 and len(input_arr) < len(sent_arr[sec_id])-1:\n",
    "                chance = randint(0,1)\n",
    "                prev = input_arr[0] -1\n",
    "                next = input_arr[-1] + 1\n",
    "                if chance == 0 and prev > 0:\n",
    "                    input_arr.insert(0, prev)\n",
    "                elif chance == 1 and next < len(sent_arr[sec_id]):\n",
    "                    input_arr.append(next)\n",
    "\n",
    "            #create res_text\n",
    "            res_text = []\n",
    "            for i, idx in enumerate(input_arr):\n",
    "                sent = sent_arr[sec_id][idx]\n",
    "                \n",
    "                #replace all xml annotation\n",
    "                if idx == citing_sent_id:\n",
    "                    clean_sent = re.sub(r'<span.*?>(.*?)<\\/span>',' #TARGET_REF', sent)\n",
    "                    clean_sent = ' '.join(clean_sent.split())\n",
    "                else:\n",
    "                    clean_sent = re.sub(r'<span.*?>(.*?)<\\/span>',r'\\1', sent)\n",
    "                    \n",
    "                #replace all other reference marker\n",
    "                clean_sent = re.sub(r'(?:van |)(?:[A-Z][a-z]+-?)+ ?(?:and [A-Za-z-]{2,}|et al?t?\\.)?(?: |,)*?(?:\\d{4}|9\\d|\\[ ?\\d{1,2} ?\\]|\\( ?\\d{4} ?\\))', '#REF', clean_sent)\n",
    "                \n",
    "                #add to text\n",
    "                res_text.append(clean_sent)\n",
    "            \n",
    "            # labels\n",
    "            res_labels = [label_mapping(label) for label in labels]\n",
    "            \n",
    "            # context\n",
    "            res_context = [1 if id in context_sent_ids else 0 for id in input_arr]\n",
    "            \n",
    "            #add to res_df\n",
    "            res_df.loc[len(res_df)] = [res_text, res_labels, res_context]\n",
    "            \n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create different task shemata for acl-arc\n",
    "'''data schema\n",
    "    [{\n",
    "        \"gold\": {\n",
    "            \"text\": [<str>],\n",
    "            \"label\": [<str>]\n",
    "        },\n",
    "        \"input\": <str>\n",
    "        \"output\": <str>\n",
    "    }]\n",
    "'''\n",
    "\n",
    "def create_xml_data(df):\n",
    "    res_data = []\n",
    "    for idx, row in df.iterrows():\n",
    "        text = row['text']\n",
    "        labels = row['label']\n",
    "        context = row['context']\n",
    "        input = ' '.join(row['text'])\n",
    "        \n",
    "        #set context XML tags\n",
    "        sent_arr = []\n",
    "        prev_label = 0\n",
    "        for sent, label in zip(row['text'],row['context']):\n",
    "            if prev_label == 0 and label == 1:\n",
    "                sent = '<CONT> ' + sent\n",
    "            elif prev_label == 1 and label == 0:\n",
    "                sent ='</CONT> ' + sent\n",
    "            sent_arr.append(sent)\n",
    "            prev_label = label\n",
    "            \n",
    "        output = ' '.join(sent_arr)\n",
    "        #add closing tag if last sentence is context\n",
    "        if prev_label == 1:\n",
    "            output += ' </CONT>'\n",
    "        \n",
    "        \n",
    "        #set label XML tag\n",
    "        tag_location = output.find('#TARGET_REF') + len('#TARGET_REF')\n",
    "        label_tags = ''.join([f'<{label}/>' for label in labels])\n",
    "        output = output[:tag_location] + label_tags + output[tag_location:]\n",
    "        \n",
    "        # add example to response\n",
    "        res_data.append({\n",
    "            \"gold\": {\n",
    "                \"text\": text,\n",
    "                \"label\": labels,\n",
    "                \"context\": context\n",
    "            },\n",
    "            \"input\": input,\n",
    "            \"output\": output\n",
    "        })\n",
    "    return res_data\n",
    "\n",
    "def create_json_data(df):\n",
    "    res_data = []\n",
    "    for idx, row in df.iterrows():\n",
    "        text = row['text']\n",
    "        label = row['label']\n",
    "        context = row['context']\n",
    "        \n",
    "        #create input\n",
    "        input = ' '.join(row['text'])\n",
    "        \n",
    "        #create json object\n",
    "        output = {\"label\": label,\n",
    "                  \"context\": [text for text, context_label in zip(text, context) if context_label == 1]}\n",
    "        \n",
    "        # add example to response\n",
    "        res_data.append({\n",
    "            \"gold\": {\n",
    "                \"text\": text,\n",
    "                \"label\": label,\n",
    "                \"context\": context\n",
    "            },\n",
    "            \"input\": input,\n",
    "            \"output\": json.dumps(output)\n",
    "        })\n",
    "    return res_data\n",
    "\n",
    "# def create_json_2_data(df):\n",
    "#     res_data = []\n",
    "#     for idx, row in df.iterrows():\n",
    "#         text = row['text']\n",
    "#         label = row['label']\n",
    "#         context = row['context']\n",
    "#         input = [f'sent{idx}: {sent}\\n' for idx, sent in enumerate(text)]\n",
    "        \n",
    "#         #create json object\n",
    "#         output = {\"label\": label,\n",
    "#                   \"context\": [f'sent{idx}' for idx, context_label in enumerate(context) if context_label == 1]}\n",
    "        \n",
    "#         # add example to response\n",
    "#         res_data.append({\n",
    "#             \"gold\": {\n",
    "#                 \"text\": text,\n",
    "#                 \"label\": label,\n",
    "#                 \"context\": context\n",
    "#             },\n",
    "#             \"input\": ' '.join(input),\n",
    "#             \"output\": json.dumps(output)\n",
    "#         })\n",
    "#     return res_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "test_df_clean = preprocess_data(test_data, max_input_len)\n",
    "train_df_clean = preprocess_data(train_data, max_input_len)\n",
    "\n",
    "# create XML data\n",
    "test_xml = create_xml_data(test_df_clean)\n",
    "train_xml = create_xml_data(train_df_clean)\n",
    "XML_OUTPUT = OUTPUT + 'XML/'\n",
    "os.makedirs(XML_OUTPUT, exist_ok=True)\n",
    "with open(XML_OUTPUT+ 'train.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(train_xml, f, ensure_ascii=False, indent=4)\n",
    "with open(XML_OUTPUT+ 'test.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(test_xml, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# create JSON 1 data\n",
    "test_json1 = create_json_data(test_df_clean)\n",
    "train_json1 = create_json_data(train_df_clean)\n",
    "JSON1_OUTPUT = OUTPUT + 'JSON/'\n",
    "os.makedirs(JSON1_OUTPUT, exist_ok=True)\n",
    "with open(JSON1_OUTPUT + '/train.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(train_json1, f, ensure_ascii=False, indent=4)\n",
    "with open(JSON1_OUTPUT + 'test.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(test_json1, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# # create JSON 2 data\n",
    "# test_json2 = create_json_2_data(test_df_clean)\n",
    "# train_json2 = create_json_2_data(train_df_clean)\n",
    "# JSON2_OUTPUT = OUTPUT + 'JSON2/'\n",
    "# os.makedirs(JSON2_OUTPUT, exist_ok=True)\n",
    "# with open(JSON2_OUTPUT + 'train.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(train_json2, f, ensure_ascii=False, indent=4)\n",
    "# with open(JSON2_OUTPUT + 'test.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(test_json2, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lasse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
