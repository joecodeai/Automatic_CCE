{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/explorer/anaconda3/envs/lasse/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "from random import randint\n",
    "import json\n",
    "import os\n",
    "\n",
    "#length limitations\n",
    "max_input_len = 300\n",
    "data_size = None #1000\n",
    "\n",
    "INPUT = '../data/acl_arc/'\n",
    "OUTPUT = f'../data/acl_arc/'\n",
    "if data_size: OUTPUT = f'../data/acl_arc/{data_size}/'\n",
    "\n",
    "# as ther is not such a hughe difference between the token count we use mistrals tokenizer for the evaluation\n",
    "tokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "train_raw = pd.read_csv(INPUT + 'train_raw.txt',sep='\\t')\n",
    "test_raw = pd.read_csv(INPUT + 'test_raw.txt',sep='\\t')\n",
    "\n",
    "#extract relevant data\n",
    "train_df = train_raw[['cite_context_paragraph','citation_class_label', 'section_title']] # citation_context\n",
    "train_df.columns = ['text', 'label', 'section_title']\n",
    "test_df = test_raw[['cite_context_paragraph','citation_class_label', 'section_title']] # citation_context\n",
    "test_df.columns = ['text', 'label', 'section_title']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper \n",
    "def preprocess_df(df, max_len):\n",
    "    res_df = pd.DataFrame(columns=['text', 'label','section_title'])\n",
    "    for idx, row in df.iterrows():\n",
    "        \n",
    "        # load text and replace citations\n",
    "        text = eval(row['text']) # for whole paragraph\n",
    "        text = [replace_citations(sent) for sent in text]\n",
    "        \n",
    "        #find the location of the target citation\n",
    "        target_location = [idx for idx, sent in enumerate(text) if '#TARGET_REF' in sent]\n",
    "        if len(target_location) != 1: continue # ignore entries that have != 1 target ref\n",
    "        \n",
    "        # check the len of tokenized strin and restrict to < max_len\n",
    "        sent_len = [len(tokenizer.encode(sent)) for sent in text] # get len of token\n",
    "        total_len = sum(sent_len)\n",
    "        \n",
    "        # if total len > max len add sentences until got max len\n",
    "        if total_len > max_len:\n",
    "            # add citance\n",
    "            cumm_len = sent_len[target_location[0]] \n",
    "            res_sent_ids = [target_location[0]]\n",
    "            \n",
    "            # add sentences\n",
    "            while len(res_sent_ids) < len(text):\n",
    "                chance = randint(0,1) # randomly front or bag sentence\n",
    "                prev = res_sent_ids[0] - 1\n",
    "                next = res_sent_ids[-1] +1\n",
    "                if chance == 0 and prev >= 0:\n",
    "                    if cumm_len + sent_len[prev] <= max_len:\n",
    "                        res_sent_ids.insert(0, prev)\n",
    "                        cumm_len += sent_len[prev]\n",
    "                    else: break\n",
    "                if chance == 1 and next < len(text):\n",
    "                    if cumm_len + sent_len[next] <= max_len:\n",
    "                        res_sent_ids.append(next)\n",
    "                        cumm_len += sent_len[next]\n",
    "                    else: break   \n",
    "            text = [sent for idx, sent in enumerate(text) if idx in res_sent_ids]\n",
    "        assert len(tokenizer.tokenize(' '.join(text))) <= max_len\n",
    "        \n",
    "        #prepare labels\n",
    "        label = label_mapping(row['label'])\n",
    "        \n",
    "        \n",
    "        # add text and labels to result\n",
    "        res_df.loc[len(res_df)] = [text, label, row['section_title']]\n",
    "    return res_df\n",
    "\n",
    "def replace_citations(text):\n",
    "    #replace and fix #AUTHOR_TAG\n",
    "    text = re.sub(r'#AUTHOR_TAG', '#TARGET_REF', text)\n",
    "    rext = re.sub(r'(?:van |)(?:[A-Z][a-z]+-?)+ ?(?:and [A-Za-z-]{2,}|et al?t?\\.)?(?: |,)*?#TARGET_REF', '#TARGET_REF', text) #fix wrong parsing\n",
    "    \n",
    "    #replace all other citatons with #REF\n",
    "    text = re.sub(r'(?:van |)(?:[A-Z][a-z]+-?)+ ?(?:and [A-Za-z-]{2,}|et al?t?\\.)?(?: |,)*?(?:\\d{4}|9\\d|\\[ ?\\d{1,2} ?\\]|\\( ?\\d{4} ?\\))', '#REF', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def label_mapping(label):\n",
    "    if type(label) == int:\n",
    "        if label == 0: return 'BACKGROUND'\n",
    "        if label == 1: return 'USE'\n",
    "        if label == 2: return 'COMPARISON'\n",
    "        if label == 3: return 'MOTIVATION'\n",
    "        if label == 4: return 'EXTENSION'\n",
    "        if label == 5: return 'FUTURE'\n",
    "    else:\n",
    "        if label == 'BACKGROUND': return 0\n",
    "        if label == 'USE': return 1\n",
    "        if label == 'COMPARISON': return 2\n",
    "        if label == 'MOTIVATION': return 3\n",
    "        if label == 'EXTENSION': return 4\n",
    "        if label == 'FUTURE': return 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create different task shemata for acl-arc\n",
    "'''data schema\n",
    "    [{\n",
    "        \"gold\": {\n",
    "            \"text\": [<str>],\n",
    "            \"label\": [<str>]\n",
    "        },\n",
    "        \"input\": <str>\n",
    "        \"output\": <str>\n",
    "    }]\n",
    "'''\n",
    "\n",
    "def create_xml_data(df):\n",
    "    res_data = []\n",
    "    for idx, row in df.iterrows():\n",
    "        text = row['text']\n",
    "        label = row['label']\n",
    "        section_title = row['section_title']\n",
    "        input = f'{section_title}:' + ' '.join(row['text'])\n",
    "        \n",
    "        #set XML tags\n",
    "        tag_location = input.find('#TARGET_REF') + len('#TARGET_REF')\n",
    "        output = input[:tag_location] + f'<{label}/>' + input[tag_location:]\n",
    "        \n",
    "        # add example to response\n",
    "        res_data.append({\n",
    "            \"gold\": {\n",
    "                \"text\": text,\n",
    "                \"label\": [label]\n",
    "            },\n",
    "            \"input\": input,\n",
    "            \"output\": output\n",
    "        })\n",
    "    return res_data\n",
    "\n",
    "def create_json_data(df):\n",
    "    res_data = []\n",
    "    for idx, row in df.iterrows():\n",
    "        text = row['text']\n",
    "        label = row['label']\n",
    "        section_title = row['section_title']\n",
    "        input = f'{section_title}:' + ' '.join(row['text'])\n",
    "        \n",
    "        #create json object\n",
    "        output = {\"label\": [label]}\n",
    "        \n",
    "        # add example to response\n",
    "        res_data.append({\n",
    "            \"gold\": {\n",
    "                \"text\": text,\n",
    "                \"label\": [label]\n",
    "            },\n",
    "            \"input\": input,\n",
    "            \"output\": json.dumps(output)\n",
    "        })\n",
    "    return res_data\n",
    "\n",
    "# def create_json_2_data(df):\n",
    "#     res_data = []\n",
    "#     for idx, row in df.iterrows():\n",
    "#         text = row['text']\n",
    "#         label = row['label']\n",
    "#         input = [f'sent{idx}: {sent}\\n' for idx, sent in enumerate(text)]\n",
    "        \n",
    "#         #create json object\n",
    "#         output = {\"label\": [label]}\n",
    "        \n",
    "#         # add example to response\n",
    "#         res_data.append({\n",
    "#             \"gold\": {\n",
    "#                 \"text\": text,\n",
    "#                 \"label\": [label]\n",
    "#             },\n",
    "#             \"input\": ' '.join(input),\n",
    "#             \"output\": json.dumps(output)\n",
    "#         })\n",
    "#     return res_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "test_df_clean = preprocess_df(test_df, max_input_len)\n",
    "train_df_clean = preprocess_df(train_df, max_input_len)\n",
    "\n",
    "# create train data with size data_size\n",
    "if data_size and data_size < len(train_df_clean):\n",
    "    \n",
    "    #create label specific df (reset index, shuffle)\n",
    "    train_0, train_1, train_2, train_3, train_4, train_5 = train_df_clean[train_df_clean['label']== label_mapping(0)].sample(frac=1).reset_index(drop=True), train_df_clean[train_df_clean['label']== label_mapping(1)].sample(frac=1).reset_index(drop=True), train_df_clean[train_df_clean['label']== label_mapping(2)].sample(frac=1).reset_index(drop=True), train_df_clean[train_df_clean['label']== label_mapping(3)].sample(frac=1).reset_index(drop=True), train_df_clean[train_df_clean['label']== label_mapping(4)].sample(frac=1).reset_index(drop=True),train_df_clean[train_df_clean['label']== label_mapping(5)].sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    #create new train_df\n",
    "    train_df_clean = pd.DataFrame(columns=['text', 'label', 'section_title'])\n",
    "    \n",
    "    #add new samples until data_size is succeeded\n",
    "    i = 0\n",
    "    while len(train_df_clean) < data_size:\n",
    "        if i < len(train_0): train_df_clean.loc[len(train_df_clean)] = train_0.loc[i]\n",
    "        if i < len(train_1): train_df_clean.loc[len(train_df_clean)] = train_1.loc[i]\n",
    "        if i < len(train_2): train_df_clean.loc[len(train_df_clean)] = train_2.loc[i]\n",
    "        if i < len(train_3): train_df_clean.loc[len(train_df_clean)] = train_3.loc[i]\n",
    "        if i < len(train_4): train_df_clean.loc[len(train_df_clean)] = train_4.loc[i]\n",
    "        if i < len(train_5): train_df_clean.loc[len(train_df_clean)] = train_5.loc[i]\n",
    "        i += 1\n",
    "    train_df_clean = train_df_clean[:data_size]\n",
    "\n",
    "# create XML data\n",
    "test_xml = create_xml_data(test_df_clean)\n",
    "train_xml = create_xml_data(train_df_clean)\n",
    "XML_OUTPUT = OUTPUT + 'XML/'\n",
    "os.makedirs(XML_OUTPUT, exist_ok=True)\n",
    "with open(XML_OUTPUT+ 'train.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(train_xml, f, ensure_ascii=False, indent=4)\n",
    "with open(XML_OUTPUT+ 'test.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(test_xml, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# create JSON 1 data\n",
    "test_json1 = create_json_data(test_df_clean)\n",
    "train_json1 = create_json_data(train_df_clean)\n",
    "JSON1_OUTPUT = OUTPUT + 'JSON/'\n",
    "os.makedirs(JSON1_OUTPUT, exist_ok=True)\n",
    "with open(JSON1_OUTPUT + '/train.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(train_json1, f, ensure_ascii=False, indent=4)\n",
    "with open(JSON1_OUTPUT + 'test.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(test_json1, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# create JSON 2 data\n",
    "# test_json2 = create_json_2_data(test_df_clean)\n",
    "# train_json2 = create_json_2_data(train_df_clean)\n",
    "# JSON2_OUTPUT = OUTPUT + 'JSON2/'\n",
    "# os.makedirs(JSON2_OUTPUT, exist_ok=True)\n",
    "# with open(JSON2_OUTPUT + 'train.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(train_json2, f, ensure_ascii=False, indent=4)\n",
    "# with open(JSON2_OUTPUT + 'test.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(test_json2, f, ensure_ascii=False, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lasse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
