{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "from random import randint\n",
    "import json\n",
    "import os\n",
    "\n",
    "MODEL = 'mistral'\n",
    "\n",
    "INPUT = '../data/acl_arc/'\n",
    "OUTPUT = f'../data/acl_arc/{MODEL}/'\n",
    "\n",
    "model_mapping = {\n",
    "    \"mistral\":'mistralai/Mistral-7B-Instruct-v0.3',\n",
    "}\n",
    "model_id = model_mapping[MODEL]\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "#length limitations\n",
    "max_input_len = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "train_raw = pd.read_csv(INPUT + 'train_raw.txt',sep='\\t')\n",
    "test_raw = pd.read_csv(INPUT + 'test_raw.txt',sep='\\t')\n",
    "\n",
    "#extract relevant data\n",
    "train_df = train_raw[['citation_context','citation_class_label']]\n",
    "train_df.columns = ['text', 'label']\n",
    "test_df = test_raw[['citation_context','citation_class_label']]\n",
    "test_df.columns = ['text', 'label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper \n",
    "def preprocess_df(df, max_len):\n",
    "    res_df = pd.DataFrame(columns=['text', 'label'])\n",
    "    for idx, row in df.iterrows():\n",
    "        \n",
    "        #prepare text\n",
    "        #text = eval(row['text']) # for whole paragraph\n",
    "        text = [row['text']] # for citing sentence only\n",
    "        text = [replace_citations(sent) for sent in text]\n",
    "        \n",
    "        #find target location\n",
    "        target_location = [idx for idx, sent in enumerate(text) if '#TARGET_REF' in sent]\n",
    "        if len(target_location) != 1: \n",
    "            #print(f'it seams like there are no or more then one target in the sample: {target_location}')\n",
    "            continue\n",
    "        \n",
    "        # restrict text len to < max len\n",
    "        text_len = [len(tokenizer.encode(sent)) for sent in text]\n",
    "        cumm_len = text_len[target_location[0]]\n",
    "        res_sent_ids = [target_location[0]]\n",
    "        while len(res_sent_ids) < len(text):\n",
    "            chance = randint(0,1)\n",
    "            prev = res_sent_ids[0] - 1\n",
    "            next = res_sent_ids[-1] +1\n",
    "            if chance == 0 and prev >= 0:\n",
    "                if cumm_len + text_len[prev] <= max_len:\n",
    "                    res_sent_ids.insert(0, prev)\n",
    "                    cumm_len += text_len[prev]\n",
    "                else: break\n",
    "            if chance == 1 and next < len(text):\n",
    "                if cumm_len + text_len[next] <= max_len:\n",
    "                    res_sent_ids.append(next)\n",
    "                    cumm_len += text_len[next]\n",
    "                else: break   \n",
    "        text = [sent for idx, sent in enumerate(text) if idx in res_sent_ids]\n",
    "        #prepare labels\n",
    "        label = label_mapping(row['label'])\n",
    "        \n",
    "        # add text and labels to result\n",
    "        res_df.loc[len(res_df)] = [text, label]\n",
    "    return res_df\n",
    "\n",
    "def replace_citations(text):\n",
    "    #replace and fix #AUTHOR_TAG\n",
    "    text = re.sub(r'#AUTHOR_TAG', '#TARGET_REF', text)\n",
    "    rext = re.sub(r'(?:van |)(?:[A-Z][a-z]+-?)+ ?(?:and [A-Za-z-]{2,}|et al?t?\\.)?(?: |,)*?#TARGET_REF', '#TARGET_REF', text) #fix wrong parsing\n",
    "    \n",
    "    #replace all other citatons with #REF\n",
    "    text = re.sub(r'(?:van |)(?:[A-Z][a-z]+-?)+ ?(?:and [A-Za-z-]{2,}|et al?t?\\.)?(?: |,)*?(?:\\d{4}|9\\d|\\[ ?\\d{1,2} ?\\]|\\( ?\\d{4} ?\\))', '#REF', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def label_mapping(label):\n",
    "    if type(label) == int:\n",
    "        if label == 0: return 'BACKGROUND'\n",
    "        if label == 1: return 'USE'\n",
    "        if label == 2: return 'COMPARE_CONTRAST'\n",
    "        if label == 3: return 'MOTIVATION'\n",
    "        if label == 4: return 'EXTENSION'\n",
    "        if label == 5: return 'FUTURE'\n",
    "    else:\n",
    "        if label == 'BACKGROUND': return 0\n",
    "        if label == 'USE': return 1\n",
    "        if label == 'COMPARE_CONTRAST': return 2\n",
    "        if label == 'MOTIVATION': return 3\n",
    "        if label == 'EXTENSION': return 4\n",
    "        if label == 'FUTURE': return 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create different task shemata for acl-arc\n",
    "'''data schema\n",
    "    [{\n",
    "        \"gold\": {\n",
    "            \"text\": [<str>],\n",
    "            \"label\": [<str>]\n",
    "        },\n",
    "        \"input\": <str>\n",
    "        \"output\": <str>\n",
    "    }]\n",
    "'''\n",
    "\n",
    "def create_xml_data(df):\n",
    "    res_data = []\n",
    "    for idx, row in df.iterrows():\n",
    "        text = row['text']\n",
    "        label = row['label']\n",
    "        input = ' '.join(row['text'])\n",
    "        \n",
    "        #set XML tags\n",
    "        tag_location = input.find('#TARGET_REF') + len('#TARGET_REF')\n",
    "        output = input[:tag_location] + f'<{label}/>' + input[tag_location:]\n",
    "        \n",
    "        # add example to response\n",
    "        res_data.append({\n",
    "            \"gold\": {\n",
    "                \"text\": text,\n",
    "                \"label\": [label]\n",
    "            },\n",
    "            \"input\": input,\n",
    "            \"output\": output\n",
    "        })\n",
    "    return res_data\n",
    "\n",
    "def create_json_1_data(df):\n",
    "    res_data = []\n",
    "    for idx, row in df.iterrows():\n",
    "        text = row['text']\n",
    "        label = row['label']\n",
    "        input = ' '.join(row['text'])\n",
    "        \n",
    "        #create json object\n",
    "        output = {\"label\": label}\n",
    "        \n",
    "        # add example to response\n",
    "        res_data.append({\n",
    "            \"gold\": {\n",
    "                \"text\": text,\n",
    "                \"label\": [label]\n",
    "            },\n",
    "            \"input\": input,\n",
    "            \"output\": json.dumps(output)\n",
    "        })\n",
    "    return res_data\n",
    "\n",
    "def create_json_2_data(df):\n",
    "    res_data = []\n",
    "    for idx, row in df.iterrows():\n",
    "        text = row['text']\n",
    "        label = row['label']\n",
    "        input = [f'sent{idx}: {sent}\\n' for idx, sent in enumerate(text)]\n",
    "        \n",
    "        #create json object\n",
    "        output = {\"label\": label}\n",
    "        \n",
    "        # add example to response\n",
    "        res_data.append({\n",
    "            \"gold\": {\n",
    "                \"text\": text,\n",
    "                \"label\": [label]\n",
    "            },\n",
    "            \"input\": ' '.join(input),\n",
    "            \"output\": json.dumps(output)\n",
    "        })\n",
    "    return res_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "test_df_clean = preprocess_df(test_df, max_input_len)\n",
    "train_df_clean = preprocess_df(train_df, max_input_len)\n",
    "\n",
    "# create XML data\n",
    "test_xml = create_xml_data(test_df_clean)\n",
    "train_xml = create_xml_data(train_df_clean)\n",
    "XML_OUTPUT = OUTPUT + 'XML/'\n",
    "os.makedirs(XML_OUTPUT, exist_ok=True)\n",
    "with open(XML_OUTPUT+ 'train.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(train_xml, f, ensure_ascii=False, indent=4)\n",
    "with open(XML_OUTPUT+ 'test.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(test_xml, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# create JSON 1 data\n",
    "test_json1 = create_json_1_data(test_df_clean)\n",
    "train_json1 = create_json_1_data(train_df_clean)\n",
    "JSON1_OUTPUT = OUTPUT + 'JSON1/'\n",
    "os.makedirs(JSON1_OUTPUT, exist_ok=True)\n",
    "with open(JSON1_OUTPUT + '/train.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(train_json1, f, ensure_ascii=False, indent=4)\n",
    "with open(JSON1_OUTPUT + 'test.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(test_json1, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# create JSON 2 data\n",
    "test_json2 = create_json_2_data(test_df_clean)\n",
    "train_json2 = create_json_2_data(train_df_clean)\n",
    "JSON2_OUTPUT = OUTPUT + 'JSON2/'\n",
    "os.makedirs(JSON2_OUTPUT, exist_ok=True)\n",
    "with open(JSON2_OUTPUT + 'train.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(train_json2, f, ensure_ascii=False, indent=4)\n",
    "with open(JSON2_OUTPUT + 'test.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(test_json2, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lasse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
